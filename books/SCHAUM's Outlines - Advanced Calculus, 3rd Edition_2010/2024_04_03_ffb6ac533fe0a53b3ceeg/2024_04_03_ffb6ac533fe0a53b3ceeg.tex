\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{mathrsfs}

\begin{document}
\section*{CHAPTER 1}
\section*{Numbers}
Mathematics has its own language, with numbers as the alphabet. The language is given structure with the aid of connective symbols, rules of operation, and a rigorous mode of thought (logic). These concepts, which previously were explored in elementary mathematics courses such as geometry, algebra, and calculus, are reviewed in the following paragraphs.

\section*{Sets}
Fundamental in mathematics is the concept of a set, class, or collection of objects having specified characteristics. For example, we speak of the set of all university professors, the set of all letters $A, B, C, D, \ldots, Z$ of the English alphabet, and so on. The individual objects of the set are called members or elements. Any part of a set is called a subset of the given set, e.g., $A, B, C$ is a subset of $A, B, C, D, \ldots, Z$. The set consisting of no elements is called the empty set or null set.

\section*{Real Numbers}
The number system is foundational to the modern scientific and technological world. It is based on the symbols 1, 2, 3, 4, 5, 6, 7, 8, 9, 0. Thus, it is called a base ten system. (There is the implication that there are other systems. One of these, which is of major importance, is the base two system.) The symbols were introduced by the Hindus, who had developed decimal representation and the arithmetic of positive numbers by 600 A.D. In the eighth century, the House of Wisdom (library) had been established in Baghdad, and it was there that the Hindu arithmetic and much of the mathematics of the Greeks were translated into Arabic. From there, this arithmetic gradually spread to the later-developing western civilization.

The flexibility of the Hindu-Arabic number system lies in the multiple uses of the numbers. They may be used to signify: (a) order-the runner finished fifth; (b) quantity—there are six apples in the barrel; (c) construction-2 and 3 may be used to form any of $23,32, .23$ or .32; (d) place-0 is used to establish place, as is illustrated by 607, 0603, and .007.

Finally, note that the significance of the base ten terminology is enhanced by the following examples:

$$
\begin{gathered}
357=7\left(10^{0}\right)+5\left(10^{1}\right)+3\left(10^{2}\right) \\
.972=\frac{9}{10}+\frac{7}{10^{2}}+\frac{2}{10^{3}}
\end{gathered}
$$

The collection of numbers created from the basic set is called the real number system. Significant subsets of them are listed as follows. For the purposes of this text, it is assumed that the reader is familiar with these numbers and the fundamental arithmetic operations.

\begin{enumerate}
  \item Natural numbers $1,2,3,4, \ldots$, also called positive integers, are used in counting members of a set. The symbols varied with the times; e.g., the Romans used I, II, III, IV, . . . The sum $a+b$ and product $a \cdot b$ or $a b$ of any two natural numbers $a$ and $b$ is also a natural number. This is often expressed by\\
saying that the set of natural numbers is closed under the operations of addition and multiplication, or satisfies the closure property with respect to these operations.

  \item Negative integers and zero, denoted by $-1,-2,-3, \ldots$, and 0 , respectively, arose to permit solutions of equations such as $x+b=a$, where $a$ and $b$ are any natural numbers. This leads to the operation of subtraction, or inverse of addition, and we write $x=a-b$.

\end{enumerate}

The set of positive and negative integers and zero is called the set of integers.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Rational numbers or fractions such as $\frac{2}{3},-\frac{5}{4}, \ldots$ arose to permit solutions of equations such as $b x=a$ for all integers $a$ and $b$, where $b \neq 0$. This leads to the operation of division, or inverse of multiplication, and we write $x=a / b$ or $a \div b$, where $a$ is the numerator and $b$ the denominator.
\end{enumerate}

The set of integers is a subset of the rational numbers, since integers correspond to rational numbers where $b=1$.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Irrational numbers such as $\sqrt{2}$ and $\pi$ are numbers which are not rational; i.e., they cannot be expressed as $a / b$ (called the quotient of $a$ and $b$ ), where $a$ and $b$ are integers and $b \neq 0$.
\end{enumerate}

The set of rational and irrational numbers is called the set of real numbers.

\section*{Decimal Representation of Real Numbers}
Any real number can be expressed in decimal form, e.g., $17 / 10=1.7,9 / 100=0.09,1 / 6=0.16666 \ldots$. In the case of a rational number, the decimal expansion either terminates or if it does not terminate, one or a group of digits in the expansion will ultimately repeat, as, for example, in $\frac{1}{7}=0.142857142857142 \ldots$. In the case of an irrational number such as $\sqrt{2}=1.41423 \ldots$ or $\pi=3.14159 \ldots$ no such repetition can occur. We can always consider a decimal expansion as unending; e.g., 1.375 is the same as 1.37500000 . . or 1.3749999 . . To indicate recurring decimals we sometimes place dots over the repeating cycle of digits, e.g., $\frac{1}{7}=0.1 \dot{4} \dot{2} \dot{8} \dot{7}$, and $\frac{19}{6}=3.1 \dot{6}$.

It is possible to design number systems with fewer or more digits; e.g., the binary system uses only two digits, 0 and 1 (see Problems 1.32 and 1.33).

\section*{Geometric Representation of Real Numbers}
The geometric representation of real numbers as points on a line, called the real axis, as in Figure 1.1, is also well known to the student. For each real number there corresponds one and only one point on the line, and, conversely, there is a one-to-one (see Figure 1.1) correspondence between the set of real numbers and the set of points on the line. Because of this we often use point and number interchangeably.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-013}
\end{center}

Figure 1.1

While this correlation of points and numbers is automatically assumed in the elementary study of mathematics, it is actually an axiom of the subject (the Cantor Dedekind axiom) and, in that sense, has deep meaning.

The set of real numbers to the right of 0 is called the set of positive numbers, the set to the left of 0 is the set of negative numbers, while 0 itself is neither positive nor negative.

(Both the horizontal position of the line and the placement of positive and negative numbers to the right and left, respectively, are conventions.)

Between any two rational numbers (or irrational numbers) on the line there are infinitely many rational (and irrational) numbers. This leads us to call the set of rational (or irrational) numbers an everywhere dense set.

\section*{Operations with Real Numbers}
If $a, b, c$ belong to the set $R$ of real numbers, then:

\begin{center}
\begin{tabular}{lll}
1. & $a+b$ and $a b$ belong to $R$ & Closure law \\
2. & $a+b=b+a$ & Commutative law of addition \\
3. & $a+(b+c)=(a+b)+c$ & Associative law of addition \\
4. & $a b=b a$ & Commutative law of multiplication \\
5. & $a(b c)=(a b) c$ & Associative law of multiplication \\
6. & $a(b+c)=a b+a c$ & Distributive law \\
7. & $a+0=0+a=a, 1 \cdot a=a \cdot 1=a$ &  \\
\end{tabular}
\end{center}

0 is called the identity with respect to addition; 1 is called the identity with respect to multiplication.

\begin{enumerate}
  \setcounter{enumi}{7}
  \item For any $a$ there is a number $x$ in $R$ such that $x+a=0$.
\end{enumerate}

$x$ is called the inverse of $a$ with respect to addition and is denoted by $-a$.

\begin{enumerate}
  \setcounter{enumi}{8}
  \item For any $a \neq 0$ there is a number $x$ in $R$ such that $a x=1$.
\end{enumerate}

$x$ is called the inverse of a with respect to multiplication and is denoted by $a^{-1}$ or $1 / a$.

Convention: For convenience, operations called subtraction and division are defined by $a-b=a+(-b)$ and $\frac{a}{b}=a b^{-1}$, respectively.

These enable us to operate according to the usual rules of algebra. In general, any set, such as $R$, whose members satisfy the preceding is called a field.

\section*{Inequalities}
If $a-b$ is a nonnegative number, we say that $a$ is greater than or equal to $b$ or $b$ is less than or equal to $a$, and write, respectively, $a \geqq b$ or $b \leqq a$. If there is no possibility that $a=b$, we write $a>b$ or $b<a$. Geometrically, $a>b$ if the point on the real axis corresponding to $a$ lies to the right of the point corresponding to $b$.

\section*{Properties of Inequalities}
If $a, b$, and $c$ are any given real numbers, then:

\begin{enumerate}
  \item Either $a>b, a=b$ or $a<b$
\end{enumerate}

Law of trichotomy

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If $a>b$ and $b>c$, then $a>c$ Law of transitivity

  \item If $a>b$, then $a+c>b+c$

  \item If $a>b$ and $c>0$, then $a c>b c$

  \item If $a>b$ and $c<0$, then $a c<b c$

\end{enumerate}

EXAMPLES. $3<5$ or $5>3 ;-2<-1$ or $-1>-2 ; x \leqq 3$ means that $x$ is a real number which may be 3 or less than 3.

\section*{Absolute Value of Real Numbers}
The absolute value of a real number $a$, denoted by $|a|$, is defined as $a$ if $a>0,-a$ if $a<0$, and 0 if $a=0$.

\section*{Properties of Absolute Value}
\begin{enumerate}
  \item $|a b|=|a||b|$
\end{enumerate}

$$
\text { or }|a b c \ldots m|=|a||b||c| \ldots|m|
$$

\begin{enumerate}
  \setcounter{enumi}{1}
  \item $|a+b| \leqq|a|+|b|$
\end{enumerate}

or $|a+b+c+\cdots+m| \leqq|a|+|b|+|c|+\cdots|m|$

\begin{enumerate}
  \setcounter{enumi}{2}
  \item $|a-b| \geqq|a|-|b|$
\end{enumerate}

EXAMPLES. $|-5|=5,|+2|=2,\left|-\frac{3}{4}\right|=\frac{3}{4},|-\sqrt{2}|=\sqrt{2},|0|=0$.

The distance between any two points (real numbers) $a$ and $b$ on the real axis is $|a-b|=|b-a|$.

\section*{Exponents and Roots}
The product $a \cdot a \ldots a$ of a real number $a$ by itself $p$ times is denoted by $a^{p}$, where $p$ is called the exponent and $a$ is called the base. The following rules hold:

\begin{enumerate}
  \item $\quad a^{p} \cdot a^{q}=a^{p+q}$

  \item $\frac{a^{p}}{a^{q}}=a^{p-q}$

  \item $\left(a^{p}\right)^{r}=a^{p r}$

  \item $\left(\frac{a}{b}\right)^{p}=\frac{a^{p}}{b^{p}}$

\end{enumerate}

These and extensions to any real numbers are possible so long as division by zero is excluded. In particular, by using 2 , with $p=q$ and $p=0$, respectively, we are led to the definitions $a^{0}=1, a^{-q}=1 / a^{q}$.

If $a^{p}=N$, where $p$ is a positive integer, we call $a$ a $p$ th root of $N$, written $\sqrt[p]{N}$. There may be more than one real $p$ th root of $N$. For example, since $2^{2}=4$ and $(-2)^{2}=4$, there are two real square roots of $4-$ namely, 2 and -2 . For square roots it is customary to define $\sqrt{N}$ as positive; thus, $\sqrt{4}=2$ and then $-\sqrt{4}=-2$.

If $p$ and $q$ are positive integers, we define $a^{p / q}=\sqrt[q]{a^{p}}$.

\section*{Logarithms}
If $a^{p}=N, p$ is called the logarithm of $N$ to the base $a$, written $p=\log _{a} N$. If $a$ and $N$ are positive and $a \neq 1$, there is only one real value for $p$. The following rules hold:

\begin{enumerate}
  \item $\log _{a} M N=\log _{a} M+\log _{a} N$

  \item $\log _{a} \frac{M}{N}=\log _{a} M-\log _{a} N$

  \item $\log _{a} M^{r}=r \log _{a} M$

\end{enumerate}

In practice, two bases are used: base $a=10$, and the natural base $a=e=2.71828 \ldots$ The logarithmic systems associated with these bases are called common and natural, respectively. The common logarithm system is signified by $\log N$; i.e., the subscript 10 is not used. For natural logarithms, the usual notation is $\ln N$.

Common logarithms (base 10) traditionally have been used for computation. Their application replaces multiplication with addition and powers with multiplication. In the age of calculators and computers, this process is outmoded; however, common logarithms remain useful in theory and application. For example, the Richter scale used to measure the intensity of earthquakes is a logarithmic scale. Natural logarithms were introduced to simplify formulas in calculus, and they remain effective for this purpose.

\section*{Axiomatic Foundations of the Real Number System}
The number system can be built up logically, starting from a basic set of axioms or "self-evident" truths, usually taken from experience, such as statements 1 through 9 on Page 3.

If we assume as given the natural numbers and the operations of addition and multiplication (although it is possible to start even further back, with the concept of sets), we find that statements 1 through 6 , with $R$ as the set of natural numbers, hold, while 7 through 9 do not hold.

Taking 7 and 8 as additional requirements, we introduce the numbers $-1,-2,-3, \ldots$, and 0 . Then, by taking 9 , we introduce the rational numbers.

Operations with these newly obtained numbers can be defined by adopting axioms 1 through 6 , where $R$ is now the set of integers. These lead to proofs of statements such as $(-2)(-3)=6,-(-4)=4,(0)(5)=0$, and so on, which are usually taken for granted in elementary mathematics.

We can also introduce the concept of order or inequality for integers, and, from these inequalities, for rational numbers. For example, if $a, b, c, d$ are positive integers, we define $a / b>c / d$ if and only if $a d>b c$, with similar extensions to negative integers.

Once we have the set of rational numbers and the rules of inequality concerning them, we can order them geometrically as points on the real axis, as already indicated. We can then show that there are points on the line which do not represent rational numbers (such as $\sqrt{2}$, $\pi$, etc.). These irrational numbers can be defined in various ways, one of which uses the idea of Dedekind cuts (see Problem 1.34). From this we can show that the usual rules of algebra apply to irrational numbers and that no further real numbers are possible.

\section*{Point Sets, Intervals}
A set of points (real numbers) located on the real axis is called a one-dimensional point set.

The set of points $x$ such that $a \leqq x \leqq b$ is called a closed interval and is denoted by $[a, b]$. The set $a<$ $x<b$ is called an open interval, denoted by $(a, b)$. The sets $a<x \leqq b$ and $a \leqq x<b$, denoted by $(a, b]$ and $[a, b)$, respectively, are called half-open or half-closed intervals.

The symbol $x$, which can represent any number or point of a set, is called a variable. The given numbers $a$ or $b$ are called constants.

Letters were introduced to construct algebraic formulas around 1600. Not long thereafter, the philosophermathematician Rene Descartes suggested that the letters at the end of the alphabet be used to represent variables and those at the beginning to represent constants. This was such a good idea that it remains the custom.

EXAMPLE. The set of all $x$ such that $|x|<4$, i.e., $-4<x<4$, is represented by $(-4,4)$, an open interval.

The set $x>a$ can also be represented by $a<x<\infty$. Such a set is called an infinite or unbounded interval. Similarly, $-\infty<x<\infty$ represents all real numbers $x$.

\section*{Countability}
A set is called countable or denumerable if its elements can be placed in 1-1 correspondence with the natural numbers.

EXAMPLE. The even natural numbers $2,4,6,8, \ldots$ is a countable set because of the $1-1$ correspondence shown.

\begin{center}
\begin{tabular}{lccccc}
Given set & 2 & 4 & 6 & 8 & $\ldots$ \\
 & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ &  \\
Natural numbers & 1 & 2 & 3 & 4 & $\ldots$ \\
\end{tabular}
\end{center}

A set is infinite if it can be placed in 1-1 correspondence with a subset of itself. An infinite set which is countable is called countable infinite.

The set of rational numbers is countable infinite, while the set of irrational numbers or all real numbers is noncountably infinite (see Problems 1.17 through 1.20).

The number of elements in a set is called its cardinal number. A set which is countably infinite is assigned the cardinal number $\boldsymbol{\aleph}_{0}$ (the Hebrew letter aleph-null). The set of real numbers (or any sets which can be placed into 1-1 correspondence with this set) is given the cardinal number $C$, called the cardinality of the contimuит.

\section*{Neighborhoods}
The set of all points $x$ such that $|x-a|<\delta$, where $\delta>0$, is called a $\delta$ neighborhood of the point $a$. The set of all points $x$ such that $0<|x-a|<\delta$, in which $x=a$ is excluded, is called a deleted $\delta$ neighborhood of $a$ or an open ball of radius $\delta$ about $a$.

\section*{Limit Points}
A limit point, point of accumulation, or cluster point of a set of numbers is a number $l$ such that every deleted $\delta$ neighborhood of $l$ contains members of the set; that is, no matter how small the radius of a ball about $l$, there are points of the set within it. In other words, for any $\delta>0$, however small, we can always find a member $x$ of the set which is not equal to $l$ but which is such that $|x-l|<\delta$. By considering smaller and smaller values of $\delta$, we see that there must be infinitely many such values of $x$.

A finite set cannot have a limit point. An infinite set may or may not have a limit point. Thus, the natural numbers have no limit point, while the set of rational numbers has infinitely many limit points.

A set containing all its limit points is called a closed set. The set of rational numbers is not a closed set, since, for example, the limit point $\sqrt{2}$ is not a member of the set (Problem 1.5). However, the set of all real numbers $x$ such that $0 \leqq x \leqq 1$ is a closed set.

\section*{Bounds}
If for all numbers $x$ of a set there is a number $M$ such that $x \leqq M$, the set is bounded above and $M$ is called an upper bound. Similarly if $x \geqq m$, the set is bounded below and $m$ is called a lower bound. If for all $x$ we have $m \leqq x \leqq M$, the set is called bounded.

If $\underline{M}$ is a number such that no member of the set is greater than $\underline{M}$ but there is at least one member which exceeds $\underline{M}-\epsilon$ for every $\epsilon>0$, then $\underline{M}$ is called the least upper bound (1.u.b.) of the set. Similarly, if no member of the set is smaller than $\bar{m}+\epsilon$ for every $\epsilon>0$, then $\bar{m}$ is called the greatest lower bound (g.l.b.) of the set.

\section*{Bolzano-Weierstrass Theorem}
The Bolzano-Weierstrass theorem states that every bounded infinite set has at least one limit point. A proof of this is given in Problem 2.23.

\section*{Algebraic and Transcendental Numbers}
A number $x$ which is a solution to the polynomial equation


\begin{equation*}
a_{0} x^{n}+a_{1} x^{n-1}+a_{2} x^{n-2}+\cdots+a_{n-1} x+a_{n}=0 \tag{1}
\end{equation*}


where $a_{0} \neq 0, a_{1}, a_{2}, \ldots, a_{n}$ are integers and $n$ is a positive integer, called the degree of the equation, is called an algebraic number. A number which cannot be expressed as a solution of any polynomial equation with integer coefficients is called a transcendental number.

EXAMPLES. $\frac{2}{3}$ and $\sqrt{2}$, which are solutions of $3 x-2=0$ and $x^{2}-2=0$, respectively, are algebraic\\
numbers.

The numbers $\pi$ and $e$ can be shown to be transcendental numbers. Mathematicians have yet to determine whether some numbers such as $e \pi$ or $e+\pi$ are algebraic or not.

The set of algebraic numbers is a countably infinite set (see Problem 1.23), but the set of transcendental numbers is noncountably infinite.

\section*{The Complex Number System}
Equations such as $x^{2}+1=0$ have no solution within the real number system. Because these equations were found to have a meaningful place in the mathematical structures being built, various mathematicians of the late nineteenth and early twentieth centuries developed an extended system of numbers in which there were solutions. The new system became known as the complex number system. It includes the real number system as a subset.

We can consider a complex number as having the form $a+b i$, where $a$ and $b$ are real numbers called the real and imaginary parts, and $i=\sqrt{-1}$ is called the imaginary unit. Two complex numbers $a+b i$ and $c+d i$ are equal if and only if $a=c$ and $b=d$. We can consider real numbers as a subset of the set of complex numbers with $b=0$. The complex number $0+0 i$ corresponds to the real number 0 .

The absolute value or modulus of $a+b i$ is defined as $|a+b i|=\sqrt{a^{2}+b^{2}}$. The complex conjugate of $a+b i$ is defined as $a-b i$. The complex conjugate of the complex number $z$ is often indicated by $\bar{z}$ or $z^{*}$.

The set of complex numbers obeys rules 1 through 9 on Pages 3, and thus constitutes a field. In performing operations with complex numbers, we can operate as in the algebra of real numbers, replacing $i^{2}$ by -1 when it occurs. Inequalities for complex numbers are not defined.

From the point of view of an axiomatic foundation of complex numbers, it is desirable to treat a complex number as an ordered pair $(a, b)$ of real numbers $a$ and $b$ subject to certain operational rules which turn out to be equivalent to the aforementioned rules. For example, we define $(a, b)+(c, d)=(a+c, b+d),(a, b)(c, d)=$ $(a c-b d, a d+b c), m(a, b)=(m a, m b)$, and so on. We then find that $(a, b)=a(1,0)+b(0,1)$ and we associate this with $a+b i$, where $i$ is the symbol for $(0,1)$.

\section*{Polar Form of Complex Numbers}
If real scales are chosen on two mutually perpendicular axes $X^{\prime} O X$ and $Y^{\prime} O Y$ (the $x$ and $y$ axes), as in Figure 1.2 , we can locate any point in the plane determined by these lines by the ordered pair of numbers $(x, y)$ called\\
rectangular coordinates of the point. Examples of the location of such points are indicated by $P, Q, R, S$, and $T$ in Figure 1.2.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-019}
\end{center}

Figure 1.2

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-019(1)}
\end{center}

Figure 1.3

Since a complex number $x+i y$ can be considered as an ordered pair $(x, y)$, we can represent such numbers by points in an $x y$ plane called the complex plane or Argand diagram. Referring to Figure 1.3, we see that $x=\rho$ $\cos \phi, y=\rho \sin \phi$, where $\rho=\sqrt{x^{2}+y^{2}}=|x+i y|$ and $\phi$, called the amplitude or argument, is the angle which line $O P$ makes with the positive $x$ axis $O X$. It follows that


\begin{equation*}
z=x+i y=\rho(\cos \phi+i \sin \phi) \tag{2}
\end{equation*}


called the polar form of the complex number, where $\rho$ and $\phi$ are called polar coordinates. It is sometimes convenient to write cis $\phi$ instead of $\cos \phi+i \sin \phi$.

If $z_{1}=x_{1}+i y_{i}=\rho_{1}\left(\cos \phi_{1}+i \sin \phi_{1}\right)$ and $z_{2}=x_{2}+i y_{2}=\rho_{2}\left(\cos \phi_{2}+i \sin \phi_{2}\right)$ and by using the addition formulas for sine and cosine, we can show that


\begin{gather*}
z_{1} z_{2}=\rho_{1} \rho_{2}\left\{\cos \left(\phi_{1}+\phi_{2}\right)+i \sin \left(\phi_{1}+\phi_{2}\right)\right\}  \tag{3}\\
\frac{z_{1}}{z_{2}}=\frac{p_{1}}{p_{2}}\left\{\cos \left(\phi_{1}-\phi_{2}\right)+i \sin \left(\phi_{1}-\phi_{2}\right)\right\}  \tag{4}\\
z^{n}=\{\rho(\cos \phi+i \sin \phi)\}^{n}=\rho^{n}(\cos n \phi+i \sin n \phi) \tag{5}
\end{gather*}


where $n$ is any real number. Equation (5) is sometimes called De Moivre's theorem. We can use this to determine roots of complex numbers. For example, if $n$ is a positive integer,


\begin{align*}
z^{1 / n} & =\{p(\cos \phi+i \sin \phi)\}^{1 / n} \\
& =p^{1 / n}\left\{\cos \left(\frac{\phi+2 k \pi}{n}\right)+i \sin \left(\frac{\phi+2 k \pi}{n}\right)\right\} \quad k=0,1,2,3, \ldots, n-1 \tag{6}
\end{align*}


from which it follows that there are in general $n$ different values of $z^{1 / n}$. In Chapter 11 we will show that $e^{i \phi}=\cos \phi+i \sin \phi$ where $e=2.71828 \ldots$. This is called Euler's formula.

\section*{Mathematical Induction}
The principle of mathematical induction is an important property of the positive integers. It is especially useful in proving statements involving all positive integers when it is known, for example, that the statements are valid for $n=1,2,3$ but it is suspected or conjectured that they hold for all positive integers. The method of proof consists of the following steps:

\begin{enumerate}
  \item Prove the statement for $n=1$ (or some other positive integer).

  \item Assume the statement is true for $n=k$, where $k$ is any positive integer.

  \item From the assumption in 2 , prove that the statement must be true for $n=k+1$. This is part of the proof establishing the induction and may be difficult or impossible.

  \item Since the statement is true for $n=1$ (from Step 1) it must (from Step 3) be true for $n=1+1=2$ and from this for $n=2+1=3$, and so on, and so must be true for all positive integers. (This assumption, which provides the link for the truth of a statement for a finite number of cases to the truth of that statement for the infinite set, is called the axiom of mathematical induction.)

\end{enumerate}

\section*{SOLVED PROBLEMS}
\section*{Operations with numbers}
1.1. If $x=4, y=15, z=-3, p=\frac{2}{3}, q=-\frac{1}{6}$, and $r=$\\
(c) $p(q r), \quad$ (d) $(p q) r$, (e) $x(p+q)$.\\
(a) $x+(y+z)=4+[15+(-3)]=4+12=16$\\
(b) $(x+y)+z=(4+15)+(-3)=19-3=16$

The fact that $(a)$ and $(b)$ are equal illustrates the associative law of addition.

(c) $p(q r)=\frac{2}{3}\left\{\left(-\frac{1}{6}\right)\left(\frac{3}{4}\right)\right\}=\left(\frac{2}{3}\right)\left(-\frac{3}{24}\right)=\left(\frac{2}{3}\right)\left(-\frac{1}{8}\right)=-\frac{2}{24}=-\frac{1}{12}$

(d) $(p q) r=\left\{\left(\frac{2}{3}\right)\left(-\frac{1}{6}\right)\right\}\left(\frac{3}{4}\right)=\left(-\frac{2}{18}\right)\left(\frac{3}{4}\right)=\left(-\frac{1}{9}\right)\left(\frac{3}{4}\right)=-\frac{3}{36}=-\frac{1}{12}$

The fact that $(c)$ and $(d)$ are equal illustrates the associative law of multiplication.

(e) $x(p+q)=4\left(\frac{2}{3}-\frac{1}{6}\right)=4\left(\frac{4}{6}-\frac{1}{6}\right)=4\left(\frac{3}{6}\right)=\frac{12}{6}=2$

Another method: $x(p+q)=x p+x q=(4)\left(\frac{2}{3}\right)+(4)\left(-\frac{1}{6}\right)=\frac{8}{3}-\frac{4}{6}=\frac{8}{3}-\frac{2}{3}=\frac{6}{3}=2$ using the dis-\\
tributive law.

1.2. Explain why we do not consider (a) $\frac{0}{0}$ and (b) $\frac{1}{0}$ as numbers.

(a) If we define $a / b$ as that number (if it exists) such that $b x=a$, then $0 / 0$ is that number $x$ such that $0 x=0$. However, this is true for all numbers. Since there is no unique number which $0 / 0$ can represent, we consider it undefined.

(b) As in (a), if we define $1 / 0$ as that number $x$ (if it exists) such that $0 x=1$, we conclude that there is no such number.

Because of these facts we must look upon division by zero as meaningless.

1.3. Simplify $\frac{x^{2}-5 x+6}{x^{2}-2 x-3}$.

$\frac{x^{2}-5 x+6}{x^{2}-2 x-3}=\frac{(x-3)(x-2)}{(x-3)(x+1)}=\frac{x-2}{x+1}$ provided that the cancelled factor $(x-3)$ is not zero; i.e., $x \neq 3$. For $x=3$, the given fraction is undefined.

\section*{Rational and irrational numbers}
1.4. Prove that the square of any odd integer is odd.

Any odd integer has the form $2 m+1$. Since $(2 m+1)^{2}=4 m^{2}+4 m+1$ is 1 more than the even integer $4 m^{2}$ $+4 m=2\left(2 m^{2}+2 m\right)$, the result follows.

1.5. Prove that there is no rational number whose square is 2 .

Let $p$ / $q$ be a rational number whose square is 2 , where we assume that $p / q$ is in lowest terms; i.e., $p$ and $q$ have no common integer factors except $\pm 1$ (we sometimes call such integers relatively prime).

Then $(p / q)^{2}=2, p^{2}=2 q^{2}$ and $p^{2}$ is even. From Problem 1.4, $p$ is even, since if $p$ were odd, $p^{2}$ would be odd. Thus, $p=2 m$.

Substituting $p=2 m$ in $p^{2}=2 q^{2}$ yields $q^{2}=2 m^{2}$, so that $q^{2}$ is even and $q$ is even.

Thus, $p$ and $q$ have the common factor 2, contradicting the original assumption that they had no common factors other than $\pm 1$. By virtue of this contradiction there can be no rational number whose square is 2 .

1.6. Show how to find rational numbers whose squares can be arbitrarily close to 2 .

We restrict ourselves to positive rational numbers. Since $(1)^{2}=1$ and $(2)^{2}=4$, we are led to choose rational numbers between 1 and 2, e.g., 1.1, 1.2, 1.3, . ., 1.9.

Since $(1.4)^{2}=1.96$ and $(1.5)^{2}=2.25$, we consider rational numbers between 1.4 and 1.5, e.g., 1.41, $1.42, ., 1.49$.

Continuing in this manner we can obtain closer and closer rational approximations; e.g., $(1.414213562)^{2}$ is less than 2, while $(1.414213563)^{2}$ is greater than 2 .

1.7. Given the equation $a_{0} x^{n}+a_{1} x^{n-1}+\cdots+a_{n}=0$, where $a_{0}, a_{1}, \ldots a_{n}$ are integers and $a_{0}$ and $a_{n} \neq 0$, show that if the equation is to have a rational root $p / q$, then $p$ must divide $a_{n}$ and $q$ must divide $a_{0}$ exactly.

Since $p / q$ is a root we have, on substituting in the given equation and multiplying by $q^{n}$, the result is


\begin{equation*}
a_{0} p^{n}+a_{1} p^{n-1} q+a_{2} p^{n-2} q^{2}+\cdots+a_{n-1} p q^{n-1}+a_{n} q^{n}=0 \tag{1}
\end{equation*}


or dividing by $p$,


\begin{equation*}
a_{0} p^{n-1}+a_{1} p^{n-2} q+\cdots+a_{n-1} q^{n-1}=-\frac{a_{n} q^{n}}{p} \tag{2}
\end{equation*}


Since the left side of Equation (2) is an integer, the right side must also be an integer. Then, since $p$ and $q$ are relatively prime, $p$ does not divide $q^{n}$ exactly and so must divide $a_{n}$.

In a similar manner, by transposing the first term of Equation (1) and dividing by $q$, we can show that $q$ must divide $a_{0}$.

1.8. Prove that $\sqrt{2}+\sqrt{3}$ cannot be a rational number.

If $x=\sqrt{2}+\sqrt{3}$, then $x^{2}=5+2 \sqrt{6}, x^{2}-5=2 \sqrt{6}$, and, squaring, $x^{4}-10 x^{2}+1=0$. The only possible rational roots of this equation are $\pm 1$ by Problem 1.7, and these do not satisfy the equation. It follows that $\sqrt{2}+\sqrt{3}$, which satisfies the equation, cannot be a rational number.

1.9. Prove that between any two rational numbers there is another rational number.

The set of rational numbers is closed under the operations of addition and division (nonzero denominator). Therefore, $\frac{a+b}{2}$ is rational. The next step is to guarantee that this value is between $a$ and $b$. To this purpose, assume $a<b$. (The proof would proceed similarly under the assumption $b<a$.) Then $2 a<a+b$; thus, $a<$ $\frac{a+b}{2}$ and $a+b<2 b$; therefore, $\frac{a+b}{2}<b$.

\section*{Inequalities}
1.10. For what values of $x$ is $x+3(2-x) \geqq 4-x$ ?

$$
x+3(2-x) \geqq 4-x \text { when } x+6-3 x \geqq 4-x, 6-2 x \geqq 4-x, 6-4 \geqq 2 x-x \text {, and } 2 \geqq x \text {; i.e. } x \leqq 2
$$

1.11. For what values of $x$ is $x^{2}-3 x-2<10-2 x$ ?

The required inequality holds when $x^{2}-3 x-2-10+2 x<0, x^{2}-x-12<0$ or $(x-4)(x+3)<0$. This last inequality holds only in the following cases.

Case 1: $\quad x-4>0$ and $x+3<0$; i.e., $x>4$ and $x<-3$. This is impossible, since $x$ cannot be both greater than 4 and less than -3 .

Case 2: $\quad x-4<0$ and $x+3>0$; i.e., $x<4$ and $x>-3$. This is possible when $-3<x<4$. Thus, the inequality holds for the set of all $x$ such that $-3<x<4$.

1.12. $\quad$ If $a \geqq 0$ and $b \geqq 0$, prove that $\frac{1}{2}(a+b) \geqq \sqrt{a b}$.

The statement is self-evident in the following cases: (1) $a=b$, and (2) either or both of $a$ and $b$ zero. For both $a$ and $b$ positive and $a \neq b$. the proof is by contradiction.

Assume to the contrary of the supposition that $\frac{1}{2}(a+b)<\sqrt{a b}$, then $\frac{1}{4}\left(a^{2}+2 a b+b^{2}\right)<a b$.

That is, $a^{2}-2 a b+b^{2}=(a-b)^{2}<0$. Since the left member of this equation is a square, it cannot be less than zero, as is indicated. Having reached this contradiction, we may conclude that our assumption is incorrect and that the original assertion is true.

1.13. If $a_{1}, a_{2}, \ldots, a_{n}$ and $b_{1}, b_{2} \ldots b_{n}$ are any real numbers, prove Schwarz's inequality:

$$
\left(a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}\right)^{2} \leqq\left(a_{1}^{2}+a_{2}^{2}+\cdots+a_{n}^{2}\right)\left(b_{1}^{2}+b_{2}^{2}+\cdots+b_{2}^{2}\right)
$$

For all real numbers $\lambda$, we have

$$
\left(a_{1} \lambda+b_{1}\right)^{2}+\left(a_{2} \lambda+b_{2}\right)^{2}+\cdots+\left(a_{n} \lambda+b_{n}\right)^{2} \geqq 0
$$

Expanding and collecting terms yields


\begin{equation*}
A^{2} \lambda^{2}+2 C \lambda+B^{2} \geqq 0 \tag{1}
\end{equation*}


where


\begin{equation*}
A^{2}=a_{1}^{2}+a_{2}^{2}+\cdots+a_{n}^{2} . \quad B^{2}=b_{1}^{2}+b_{2}^{2}+\cdots+b_{n}^{2}, \quad C=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n} \tag{2}
\end{equation*}


The left member of Equation (1) is a quadratic form in $\lambda$. Since it never is negative, its discriminant, $4 C^{2}$ $-4 A^{2} B^{2}$, cannot be positive. Thus,

$$
C^{2}-A^{2} B^{2} \leq 0 \quad \text { or } \quad C^{2} \leq A^{2} B^{2}
$$

This is the inequality that was to be proved.

1.14. Prove that $\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\cdots+\frac{1}{2^{n-1}}<1$ for all positive integers $n>1$.

Let

$$
S_{n}=\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\cdots+\frac{1}{2^{n-1}}
$$

Then

$$
\frac{1}{2} S_{n}=\frac{1}{4}+\frac{1}{8}+\cdots+\frac{1}{2^{n-1}}+\frac{1}{2^{n}}
$$

Subtracting,

$$
\frac{1}{2} S_{n}=\frac{1}{2}-\frac{1}{2^{n}}
$$

Thus,

$$
S_{n}=1-\frac{1}{2^{n-1}}<1 \text { for all } n
$$

\section*{Exponents, roots, and logarithms}
1.15. Evaluate each of the following:

(a) $\frac{3^{4} \cdot 3^{8}}{3^{14}}=\frac{3^{4+8}}{3^{14}}=3^{4+8-14}=3^{-2}=\frac{1}{3^{2}}=\frac{1}{9}$

(b) $\sqrt{\frac{\left(5 \cdot 10^{-6}\right)\left(4 \cdot 10^{2}\right)}{8 \cdot 10^{5}}}=\sqrt{\frac{5 \cdot 4}{8}: \frac{10^{-6} \cdot 10^{2}}{10^{5}}}=\sqrt{2.5 \cdot 10^{-9}}=\sqrt{25 \cdot 10^{-10}}=5 \cdot 10^{-5}$ or 0.00005

(c) $\log _{2 / 3}\left(\frac{27}{8}\right)=x$. Then $\left(\frac{2}{3}\right)^{x}=\frac{27}{8}=\left(\frac{3}{2}\right)^{3}=\left(\frac{2}{3}\right)^{-3}$ or $x=-3$

(d) $\left(\log _{a} b\right)\left(\log _{b} a\right)=u$. Then $\log _{a} b=x, \log _{b} a=y$, assuming $a, b>0$ and $a, b \neq 1$.

Then $a^{x}=b, b^{y}=a$, and $u=x y$. Since $\left(a^{x}\right)^{y}=a^{x y}=b^{y}=a$, we have $a^{x y}=a^{1}$ or $x y=1$, the required value.

1.16. If $M>0, N>0$, and $a>0$ but $a \neq 1$, prove that $\log _{a} \frac{M}{N}=\log _{a} M-\log _{a} N$.

Let $\log _{a} M=x, \log _{a} N=y . \quad$ Then $a^{x}=M, a^{y}=N$ and so

$$
\frac{M}{N}=\frac{a^{x}}{a^{y}}=a^{x-y} \quad \text { or } \quad \log _{a} \frac{M}{N}=x-y=\log _{a} M-\log _{a} N
$$

\section*{Countability}
1.17. Prove that the set of all rational numbers between 0 and 1 inclusive is countable.

Write all fractions with denominator 2 , then $3, \ldots$, considering equivalent fractions such as $\frac{1}{2}, \frac{2}{4}$, $\frac{3}{6}, \ldots$ no more than once. Then the 1-1 correspondence with the natural numbers can be accomplished as follows:

\begin{center}
\begin{tabular}{lcccccccccc}
Rational numbers & 0 & 1 & $\frac{1}{2}$ & $\frac{1}{3}$ & $\frac{2}{3}$ & $\frac{1}{4}$ & $\frac{3}{4}$ & $\frac{1}{5}$ & $\frac{2}{5}$ & $\ldots$ \\
 & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ & $\downarrow$ &  \\
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & $\ldots$ \\
\end{tabular}
\end{center}

Thus, the set of all rational numbers between 0 and 1 inclusive is countable and has cardinal number $\boldsymbol{\aleph}_{0}$ (see Page 6).

1.18. If $A$ and $B$ are two countable sets, prove that the set consisting of all elements from $A$ or $B$ (or both) is also countable.

Since $A$ is countable, there is a 1-1 correspondence between elements of $A$ and the natural numbers so that we can denote these elements by $a_{1}, a_{2}, a_{3}, \ldots$

Similarly, we can denote the elements of $B$ by $b_{1}, b_{2}, b_{3}, \ldots$

Case 1: Suppose elements of $A$ are all distinct from elements of $B$. Then the set consisting of elements from

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
or $B$ &  &  & $a_{2}$ & ${ }_{2} b_{2}$ & ${ }_{2} a_{3}$ \\
\hline
\end{tabular}
\end{center}

$A$ or $B$ is countable, since we can establish the following 1-1 correspondence:

Case 2: If some elements of $A$ and $B$ are the same, we count them only once, as in Problem 1.17. Then the set of elements belonging to $A$ or $B$ (or both) is countable.

The set consisting of all elements which belong to $A$ or $B$ (or both) is often called the union of $A$ and $B$, denoted by $A \cup B$ or $A+B$.

The set consisting of all elements which are contained in both $A$ and $B$ is called the intersection of $A$ and $B$, denoted by $A \cap B$ or $A B$. If $A$ and $B$ are countable, so is $A \cap B$.

The set consisting of all elements in $A$ but not in $B$ is written $A-B$. If we let $[B$ be the set of elements which are not in $B$, we can also write $A-B=A \bar{B}$. If $A$ and $B$ are countable, so is $A-B$.

1.19. Prove that the set of all positive rational numbers is countable.

Consider all rational numbers $x>1$. With each such rational number we can associate one and only one rational number $1 / x$ in $(0,1)$; i.e., there is a one-to-one correspondence between all rational numbers $>1$ and all rational numbers in $(0,1)$. Since these last are countable by Problem 1.17, it follows that the set of all rational numbers $>1$ is also countable.

From Problem 1.18 it then follows that the set consisting of all positive rational numbers is countable, since this is composed of the two countable sets of rationals between 0 and 1 and those greater than or equal to 1 .

From this we can show that the set of all rational numbers is countable (see Problem 1.59).

1.20. Prove that the set of all real numbers in $[0,1]$ is noncountable.

Every real number in $[0,1]$ has a decimal expansion.$a_{1} a_{2} a_{3} \ldots$ where $a_{1}, a_{2}, \ldots$ are any of the digits 0 , $1,2, \ldots, 9$.

We assume that numbers whose decimal expansions terminate such as 0.7324 are written $0.73240000 \ldots$ and that this is the same as $0.73239999 \ldots$

If all real numbers in $[0,1]$ are countable we can place them in 1-1 correspondence with the natural numbers as in the following list:

$$
\begin{aligned}
& 1 \leftrightarrow 0 . a_{11} a_{12} a_{13} a_{14} \ldots \\
& 2 \leftrightarrow 0 . a_{21} a_{22} a_{23} a_{24} \ldots \\
& 3 \leftrightarrow 0 . a_{31} a_{32} a_{33} a_{34} \ldots
\end{aligned}
$$

We now form a number

$$
0 . b_{1} b_{2} b_{3} b_{4} \ldots
$$

where $b_{1} \neq a_{11}, b_{2} \neq a_{22}, b \neq a_{33}, b_{4} \neq a_{44}, \ldots$ and where all $b$ 's beyond some position are not all 9's.

This number, which is in [0.1], is different from all numbers in the preceding list and is thus not in the list, contradicting the assumption that all numbers in $[0,1]$ were included.

Because of this contradiction, it follows that the real numbers in $[0,1]$ cannot be placed in 1-1 correspondence with the natural numbers; i.e., the set of real numbers in $[0,1]$ is noncountable.

\section*{Limit points, bounds, Bolzano-Weierstrass theorem}
1.21. (a) Prove that the infinite set of numbers $1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \ldots$ is bounded. (b) Determine the least upper bound (l.u.b.) and greatest lower bound (g.l.b.) of the set. (c) Prove that 0 is a limit point of the set. (d) Is the set a closed set? (e) How does this set illustrate the Bolzano-Weierstrass theorem?

(a) Since all members of the set are less than 2 and greater than -1 (for example), the set is bounded; 2 is an upper bound; -1 is a lower bound.

We can find smaller upper bounds (e.g., 3/2) and larger lower bounds (e.g., $-\frac{1}{2}$ ).\\
(b) Since no member of the set is greater than 1 and since there is at least one member of the set (namely, 1) which exceeds $1-\varepsilon$ for every positive number $\varepsilon$, we see that 1 is the l.u.b. of the set.

Since no member of the set is less than 0 and since there is at least one member of the set which is less than $0+\varepsilon$ for every positive $\varepsilon$ (we can always choose for this purpose the number $1 / n$, where $n$ is a positive integer greater than $1 / \varepsilon$ ), we see that 0 is the g.1.b. of the set.

(c) Let $x$ be any member of the set. Since we can always find a number $x$ such that $0<|x|<\delta$ for any positive number $\delta$ (e.g., we can always pick $x$ to be the number $1 / n$, where $n$ is a positive integer greater than $1 / \delta$ ), we see that 0 is a limit point of the set. To put this another way, we see that any deleted $\delta$ neighborhood of 0 always includes members of the set, no matter how small we take $\delta>0$.

(d) The set is not a closed set, since the limit point 0 does not belong to the given set.

(e) Since the set is bounded and infinite, it must, by the Bolzano-Weierstrass theorem, have at least one limit point. We have found this to be the case, so that the theorem is illustrated.

\section*{Algebraic numbers}
1.22. Prove that $\sqrt[3]{2}+\sqrt{3}$ is an algebraic number.

Let $x=\sqrt[3]{2}+\sqrt{3}$. Then $x-\sqrt{3}=\sqrt[3]{2}$. Cubing both sides and simplifying, we find $x^{3}+9 x-2=3 \sqrt{3}$ $\left(x^{2}+1\right)$. Then, squaring both sides and simplifying, we find $x^{6}-9 x^{4}-4 x^{3}+27 x^{2}+36 x-23=0$.

Since this is a polynomial equation with integral coefficients, it follows that $\sqrt[3]{2}+\sqrt{3}$, which is a solution, is an algebraic number.

1.23. Prove that the set of all algebraic numbers is a countable set.

Algebraic numbers are solutions to polynomial equations of the form $a_{0} x^{n}+\mathrm{a}, x^{n-1}+\cdots+a_{n}=0$ where $a_{0}, a_{1}, \ldots, a_{n}$ are integers.

Let $P=\left|a_{0}\right|+\left|a_{1}\right|+\cdots+\left|a_{n}\right|+n$. For any given value of $P$ there are only a finite number of possible polynomial equations and thus only a finite number of possible algebraic numbers.

Write all algebraic numbers corresponding to $P=1,2,3,4, \ldots$, avoiding repetitions. Thus, all algebraic numbers can be placed into 1-1 correspondence with the natural numbers and so are countable.

\section*{Complex numbers}
1.24. Perform the indicated operations:

(a) $(4-2 i)+(-6+5 i)=4-2 i-6+5 i=4-6+(-2+5) i=-2+3 i$

(b) $(-7+3 i)-(2-4 i)=-7+3 i-2+4 i=-9+7 i$

(c) $(3-2 i)(1+3 i)=3(1+3 i)-2 i(1+3 i)=3+9 i-2 i-6 i^{2}=3+9 i-2 i+6=9+7 i$

(d) $\frac{-5+5 i}{4-3 i}=\frac{-5+5 i}{4-3 i} \cdot \frac{4+3 i}{4+3 i}=\frac{(-5+5 i)(4+3 i)}{16-9 i^{2}}=\frac{-20-15 i+20 i+15 i^{2}}{16+9}$ $=\frac{-35+5 i}{25}=\frac{5(-7+i)}{25}=\frac{-7}{5}+\frac{1}{5} i$

(e) $\frac{i+i^{2}+i^{3}+i^{4}+i^{5}}{1+i}=\frac{i-1+\left(i^{2}\right)(i)+\left(i^{2}\right)^{2}+\left(i^{2}\right)^{2} i}{1+i}=\frac{i-1-i+1+i}{1+i}$

$=\frac{i}{1+i} \cdot \frac{1-i}{1-i}=\frac{i-i^{2}}{1-i^{2}}=\frac{i+1}{2}=\frac{1}{2}+\frac{1}{2} i$

(f) $|3-4 i||4+3 i|=\sqrt{(3)^{2}+(-4)^{2}} \sqrt{(4)^{2}+(3)^{2}}=(5)(5)=25$\\
(g) $\left|\frac{1}{1+3 i}-\frac{1}{1-3 i}\right|=\left|\frac{1-3 i}{1-9 i^{2}}-\frac{1+3 i}{1-9 i^{2}}\right|=\left|\frac{-6 i}{10}\right|=\sqrt{(0)^{2}+\left(-\frac{6}{10}\right)^{2}}=\frac{3}{5}$

1.25. If $z_{1}$ and $z_{2}$ are two complex numbers, prove that $\left|z_{1} z_{2}\right|=\left|z_{1}\right|\left|z_{2}\right|$.

Let $z_{1}=x_{1}+i y_{1}, z_{2}=x_{2}+i y_{2}$. Then

$$
\begin{aligned}
\left|\mathrm{z}_{1} z_{2}\right| & =\left|\left(x_{1}+i y_{1}\right)\left(x_{2}+i y_{2}\right)\right|=\left|x_{1} x_{2}-y_{1} y_{2}+i\left(x_{1} y_{2}+x_{2} y_{1}\right)\right| \\
& =\sqrt{\left(x_{1} x_{2}-y_{1} y_{2}\right)^{2}+\left(x_{1} y_{2}+x_{2} y_{1}\right)^{2}}=\sqrt{x_{1}^{2} x_{2}^{2}+y_{1}^{2} y_{2}^{2}+x_{1}^{2} y_{2}^{2}+x_{2}^{2} y_{1}^{2}} \\
& =\sqrt{\left(x_{1}^{2}+y_{1}^{2}\right)\left(x_{2}^{2}+y_{2}^{2}\right)}=\sqrt{x_{1}^{2}+y^{2}} \sqrt{x_{2}^{2}+y_{2}^{2}}=\left|x_{1}+i y_{1}\right|\left|x_{2}+i y_{2}\right|=\left|z_{1}\right|\left|z_{2}\right| .
\end{aligned}
$$

1.26. Solve $x^{3}-2 x-4=0$.

The possible rational roots using Problem 1.7 are $\pm 1, \pm 2$, and $\pm 4$. By trial, we find $x=2$ is a root. Then the given equation can be written $(x-2)\left(x^{2}+2 x+2\right)=0$. The solutions to the quadratic equation $a x^{2}+b x+c=0$ are $x=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}$ For $a=1, b=2$, and $c=2$, this gives $x=\frac{-2 \pm \sqrt{4-8}}{2}=$ $\frac{-2 \pm \sqrt{-4}}{2}=\frac{-2 \pm 2 i}{2}=-1 \pm i$.

The set of solutions is $2,-1+i,-1-i$.

\section*{Polar form of complex numbers}
1.27. Express in polar form $(a) 3+3 i,(b)-1+\sqrt{3} i,(c)-1$, and $(d)-2-2 \sqrt{3} i$. See Figure 1.4.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-026(3)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-026(1)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-026}
\end{center}

(c)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-026(2)}
\end{center}

(d)

Figure 1.4

(a) Amplitude $\phi=45^{\circ}=\pi / 4$ radians. Modulus $\rho=\sqrt{3^{2}+3^{2}}=3 \sqrt{2}$.

Then $3+3 i=\rho(\cos \phi+i \sin \phi)=3 \sqrt{2}(\cos \pi / 4+i \sin \pi / 4)=3 \sqrt{2}$ $\operatorname{cis} \pi / 4=3 \sqrt{2} e^{\pi / 4}$

(b) Amplitude $\phi=120^{\circ}=2 \pi / 3$ radians. Modulus

$\rho=\sqrt{(-1)^{2}+(\sqrt{3})^{2}}=\sqrt{4}=2$. Then $-1+3 \sqrt{3} i=2(\cos 2 \pi / 3+$ $i \sin 2 \pi / 3)=2$ cis $2 \pi / 3=2 e^{2 \pi i / 3}$.

(c) Amplitude $\phi=180^{\circ}=\pi$ radians. Modulus $\rho=\sqrt{(-1)^{2}+(0)^{2}}=1$. Then $-1=1(\cos \pi+i \sin \pi)=\operatorname{cis} \pi=e^{\pi \mathrm{i}}$.

(d) Amplitude $\phi=240^{\circ}=4 \pi / 3$ radians. Modulus $\rho=\sqrt{(-2)^{2}+(-2 \sqrt{3})^{2}}=4$. Then $-2-2 \sqrt{3}=$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-026(4)}
\end{center}

Figure 1.5 $4(\cos 4 \pi / 3+i \sin 4 \pi / 3)=4 \operatorname{cis} 4 \pi / 3=4 e^{4 \pi i / 3}$.

1.28. Evaluate $(a)(-1+\sqrt{3} i)^{10}$ and $(b)(-1+i)^{1 / 3}$.

(a) By Problem 1.27(b) and De Moivre's theorem.

$$
\begin{aligned}
(-1+\sqrt{3} i)^{10} & =[2(\cos 2 \pi / 3+i \sin 2 \pi / 3)]^{10}=2^{10}(\cos 20 \pi / 3+i \sin 20 \pi / 3) \\
& =1024[\cos (2 \pi / 3+6 \pi)+i \sin (2 \pi / 3+6 \pi)]=1024(\cos 2 \pi / 3+i \sin 2 \pi / 3) \\
& =1024\left(-\frac{1}{2}+\frac{1}{2} \sqrt{3} i\right)=-512+512 \sqrt{3} i
\end{aligned}
$$

(b) $-1+i=\sqrt{2}\left(\cos 135^{\circ}+i \sin 135^{\circ}\right)=\sqrt{2}\left[\cos \left(135^{\circ}+k \cdot 360^{\circ}\right)+i \sin \left(135^{\circ}+k \cdot 360^{\circ}\right)\right]$. Then

$$
(-1+i)^{1 / 3}=(\sqrt{2})^{1 / 3}\left[\cos \left(\frac{135^{\circ}+k \cdot 360^{\circ}}{3}\right)+i \sin \left(\frac{135^{\circ}+k \cdot 360^{\circ}}{3}\right)\right]
$$

The results for $k=0,1,2$ are

$$
\begin{aligned}
& \sqrt[6]{2}\left(\cos 45^{\circ}+i \sin 45^{\circ}\right) \\
& \sqrt[6]{2}\left(\cos 165^{\circ}+i \sin 165^{\circ}\right) \\
& \sqrt[6]{2}\left(\cos 285^{\circ}+i \sin 285^{\circ}\right)
\end{aligned}
$$

The results for $k=3,4,5,6,7, \ldots$ give repetitions of these. These complex roots are represented geometrically in the complex plane by points $P_{1}, P_{2}, P_{3}$ on the circle of Figure 1.5.

\section*{Mathematical induction}
1.29. Prove that $1^{2}+2^{2}+3^{3}+4^{2}+\cdots+n^{2}=\frac{1}{6} n(n+1)(2 n+1)$.

The statement is true for $n=1$, since $1^{2}=\frac{1}{6}(1)(1+1)(2 \cdot 1+1)=1$.

Assume the statement is true for $n=k$. Then

Adding $(k+1)^{2}$ to both sides.

$$
1^{2}+2^{2}+3^{2}+\cdots+k^{2}=\frac{1}{6} k(k+1)(2 k+1)
$$

$$
\begin{aligned}
1^{2}+2^{2}+3^{2}+\cdots+k^{2}+(k+1)^{2} & =\frac{1}{6} k(k+1)(2 k+1)+(k+1)^{2}=(k+1)\left[\frac{1}{6} k(2 k+1)+k+1\right] \\
& =\frac{1}{6}(k+1)\left(2 k^{2}+7 k+6\right)=\frac{1}{6}(k+1)(k+2)(2 k+3)
\end{aligned}
$$

which shows that the statement is true for $n=k+1$ if it is true for $n=k$. But since it is true for $n=1$, it follows that it is true for $n=1+1=2$ and for $n=2+1=3, \ldots$; i.e., it is true for all positive integers $n$.

1.30. Prove that $x^{n}-y^{n}$ has $x-y$ as a factor for all positive integers $n$.

The statement is true for $n=1$, since $x^{1}-y^{1}=x-y$.

Assume the statement is true for $n=k$; i.e., assume that $x^{k}-y^{k}$ has $x-y$ as a factor. Consider

$$
\begin{aligned}
x^{k+1}-y^{k+1} & =x^{k+1}-x^{k} y+x^{k} y-y^{k+1} \\
& =x^{k}(x-y)+y\left(x^{k}-y^{k}\right)
\end{aligned}
$$

The first term on the right has $x-y$ as a factor, and the second term on the right also has $x-y$ as a factor because of the previous assumption.

Thus, $x^{k+1}-y^{k+1}$ has $x-y$ as a factor if $x^{k}-y^{k}$ does.

Then, since $x^{1}-y^{1}$ has $x-y$ as factor, it follows that $x^{2}-y^{2}$ has $x-y$ as a factor, $x^{3}-y^{3}$ has $x-y$ as a factor, etc.

1.31. Prove Bernoulli's inequality $(1+x)^{n}>1+n x$ for $n=2,3, \ldots$ if $x>-1, x \neq 0$.

The statement is true for $n=2$, since $(1+x)^{2}=1+2 x+x^{2}>1+2 x$.

Assume the statement is true for $n=k$; i.e., $(1+x)^{k}>1+k x$.

Multiply both sides by $1+x$ (which is positive, since $x>-1$ ). Then we have

$$
(1+x)^{k+1}>(1+x)(1+k x)=1+(k+1) x+k x^{2}>1+(k+1) x
$$

Thus, the statement is true for $n=k+1$ if it is true for $n=k$.

But since the statement is true for $n=2$, it must be true for $n=2+1=3 \ldots$ and is thus true for all integers greater than or equal to 2 .

Note that the result is not true for $n=1$. However, the modified result $(1+x)^{n} \geq 1+n x$ is true for $n=1,2,3, \ldots$

\section*{Miscellaneous problems}
1.32. Prove that every positive integer $P$ can be expressed uniquely in the form $P=a_{0} 2^{n}+a_{1} 2^{n-1}+a_{2} 2^{n-2}+\cdots+$ $a_{n}$ where the $a$ 's are 0 's or 1 's.

Dividing $P$ by 2 , we have $P / 2=a_{0} 2^{n-1}+a_{1} 2^{n-2}+\cdots+a_{n-1}+a_{n} / 2$.

Then $a_{n}$ is the remainder, 0 or 1 , obtained when $P$ is divided by 2 and is unique.

Let $P_{1}$ be the integer part of $P / 2$. Then $P_{1}=a_{0} 2^{n-1}+a_{1} 2^{n-2}+\cdots+a_{n-1}$.

Dividing $P_{1}$ by 2 , we see that $a_{n-1}$ is the remainder, 0 or 1 , obtained when $P_{1}$ is divided by 2 and is unique

By continuing in this manner, all the $a$ 's can be determined as 0's or 1's and are unique.

1.33. Express the number 23 in the form of Problem 1.32.

The determination of the coefficient can be arranged as follows:\\
2) $\underline{23}$\\
2) $11 \quad$ Remainder 1\\
2) $5 \quad$ Remainder 1\\
2) $2 \quad$ Remainder 1\\
2) $1 \quad$ Remainder 0

$0 \quad$ Remainder 1

The coefficients are 10111 . Check: $23=1 \cdot 2^{4}+0 \cdot 2^{3}+1 \cdot 2^{2}+1 \cdot 2+1$.

The number 10111 is said to represent 23 in the scale of two or binary scale.

1.34. Dedekind defined a cut, section, or partition in the rational number system as a separation of all rational numbers into two classes or sets called $L$ (the left-hand class) and $R$ (the right-hand class) having the following properties:

I. The classes are non-empty (i.e. at least one number belongs to each class).

II. Every rational number is in one class or the other.

III. Every number in $L$ is less than every number in $R$.

Prove each of the following statements:

(a) There cannot be a largest number in $L$ and a smallest number in $R$.

(b) It is possible for $L$ to have a largest number and for $R$ to have no smallest number. What type of number does the cut define in this case?\\
(c) It is possible for $L$ to have no largest number and for $R$ to have a smallest number. What type of number does the cut define in this case?

(d) It is possible for $L$ to have no largest number and for $R$ to have no smallest number. What type of number does the cut define in this case?

(a) Let $a$ be the largest rational number in $L$ and $b$ the smallest rational number in $R$. Then either $a=b$ or $a<b$.

We cannot have $a=b$, since, by definition of the cut, every number in $L$ is less than every number in $R$.

We cannot have $a<b$, since, by Problem 1.9, $\frac{1}{2}(a+b)$ is a rational number which would be greater than $a$ (and so would have to be in $R$ ) but less than $b$ (and so would have to be in $L$ ), and, by definition, a rational number cannot belong to both $L$ and $R$.

(b) As an indication of the possibility, let $L$ contain the number $\frac{2}{3}$ and all rational numbers less than $\frac{2}{3}$, while $R$, contains all rational numbers greater than $\frac{2}{3}$. In this case the cut defines the rational number $\frac{2}{3}$. A similar argument replacing $\frac{2}{3}$ by any other rational number shows that in such case the cut defines a ra-\\
tional number.

(c) As an indication of the possibility, let $L$ contain all rational numbers less than $\frac{2}{3}$, while $R$ contains all rational numbers greater than $\frac{2}{3}$. This cut also defines the rational number $\frac{2}{3}$. A similar argument shows that this cut always defines a rational number.

(d) As an indication of the possibility, let $L$ consist of all negative rational numbers and all positive rational numbers whose squares are less than 2 , while $R$ consists of all positive numbers whose squares are greater than 2. We can show that if $a$ is any number of the $L$ class, there is always a larger number of the $L$ class, while if $b$ is any number of the $R$ class, there is always a smaller number of the $R$ class (see Problem 1.106). A cut of this type defines an irrational number.

From $(b),(c)$, and $(d)$, it follows that every cut in the rational number system, called a Dedekind cut, defines either a rational or an irrational number. By use of Dedekind cuts we can define operations (addition, multiplication, etc.) with irrational numbers.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Operations with numbers}
1.35. Given $x=-3, y=2, z=5, a=\frac{3}{2}$, and $b=-\frac{1}{4}$, evaluate:\\
(a) $(2 x-y)(3 y+z)(5 x-2 z)$\\
(b) $\frac{x y-2 z^{2}}{2 a b-1}$\\
(c) $\frac{3 a^{2} b+a b^{2}}{2 a^{2} 2 b^{2}+1}$\\
(d) $\frac{(a x+b y)^{2}+(a y-b x)^{2}}{(a y+b x)^{2}+(a x-b y)^{2}}$

Ans. (a) 2200 (b) 32 (c) $-51 / 41$ (d) 1

1.36. Find the set of values of $x$ for which the following equations are true. Justify all steps in each case.

(a) $4\{(x-2)+3(2 x-1)\}+2(2 x+1)=12(x+2)-2$

(b) $\frac{1}{8-x}-\frac{1}{x-2}=\frac{1}{4}$

Ans. (a) 2 (b) $6,-4$ (c) $-1,1$ (d) $-\frac{1}{2}$ (c) $\sqrt{x^{2}+8 x+7}-\sqrt{2 x+2}=x+1$

(d) $\frac{1-x}{\sqrt{x^{2}-2 x+5}}=\frac{3}{5}$

1.37. Prove that $\frac{x}{(z-x)(x-y)}+\frac{y}{(x-y)(y-z)}+\frac{z}{(y-z)(z-x)}=0$, giving restrictions if any.

\section*{Rational and irrational numbers}
1.38. Find decimal expansions for (a) $\frac{3}{7}$ and (b) $\sqrt{5}$.\\
Ans. (a) $0 . \dot{428 \dot{8} \dot{7} \mathrm{i}}$\\
(b) $2.2360679 \ldots$

1.39. Show that a fraction with denominator 17 and with numerator $1,2,3, \ldots, 16$ has 16 digits in the repeating portion of its decimal expansion. Is there any relation between the orders of the digits in these expansions?

1.40. Prove that (a) $\sqrt{3}$ and (b) $\sqrt[3]{2}$ are irrational numbers.

1.41. Prove that (a) $\sqrt[3]{5}-\sqrt[4]{3}$ and (b) $\sqrt{2}+\sqrt{3}+\sqrt{5}$ are irrational numbers.

1.42. Determine a positive rational number whose square differs from 7 by less than .000001 .

1.43. Prove that every rational number can be expressed as a repeating decimal.

1.44. Find the values of $x$ for which (a) $2 x^{3}-5 x^{2}-9 x+18=0$, (b) $3 x^{3}+4 x^{2}-35 x+8=0$, and (c) $x^{4}-21 x^{2}+$ $4=0$.\\
Ans. (a) $3,-2,3 / 2$\\
(b) $8 / 3,-2 \pm \sqrt{5}$\\
(c) $\frac{1}{2}(5 \pm \sqrt{17}), \frac{1}{2}(-5 \pm \sqrt{17})$

1.45. If $a, b, c$, and $d$ are rational and $m$ is not a perfect square, prove that $a+b \sqrt{m}=c+d \sqrt{m}$ if and only if $a=c$ and $b=d$.

1.46. Prove that $\frac{1+\sqrt{3}+\sqrt{5}}{1-\sqrt{3}+\sqrt{5}}=\frac{12 \sqrt{5}-2 \sqrt{15}+14 \sqrt{3}-7}{11}$.

\section*{Inequalities}
1.47. Find the set of values of $x$ for which each of the following inequalities holds:

(a) $\frac{1}{x}+\frac{3}{2 x} \geqq 5$, (b) $x(x+2) \leqq 24$, (c) $|x+2|<|x-5|$, (d) $\frac{x}{x+2}>\frac{x+3}{3 x+1}$

$$
\text { Ans. (a) } 0<x \leqq \frac{1}{2} \text { (b) }-6 \leqq x \leqq 4 \text { (c) } x<3 / 2 \text { (d) } x>3,-1<x<-\frac{1}{3} \text {, or } x<-2
$$

1.48. Prove (a) $|x+y| \leqq|x|+|y|$, (b) $|x+y+z| \leqq|x|+|y|+|z|$, and (c) $|x|-y|\geqq| x|-| y \mid$.

1.49. Prove that for all real $x, y, z, x^{2}+y^{2}+z^{2} \geq x y+y z+z x$.

1.50. If $a^{2}+b^{2}=1$ and $c^{2}+d^{2}=1$, prove that $a c+b d \leqq 1$.

1.51. If $x>0$, prove that $x^{n+1}+\frac{1}{x^{n+1}}>x^{n}+\frac{1}{x^{n}}$ where $n$ is any positive integer.

1.52. Prove that for all real $a \neq 0,|a+1 / a| \geqq 2$.

1.53. Show that in Schwarz's inequality (Problem 1.13) the equality holds if and only if $a_{p}=k b_{p}, p=1,2$, $3, \ldots, n$, where $k$ is any constant.

1.54. If $a_{1}, a_{2}, a_{3}$ are positive, prove that $\frac{1}{3}\left(a_{1}+a_{2}+a_{3}\right) \geqq \sqrt[3]{a_{1} a_{2} a_{3}}$.

\section*{Exponents, roots, and logarithms}
1.55. Evaluate: (a) $4^{\log _{2} 8}$ (b) $\frac{3}{4} \log _{1 / 8}\left(\frac{1}{128}\right)$ (c) $\sqrt{\frac{(0.00004)(25,000)}{(0.02)^{5}(0.125)}}\left(\right.$ d) $3^{-2 \log _{3} 5}(e)\left(-\frac{1}{8}\right)^{4 / 3}-(-27)^{-2 / 3}$ Ans. (a) 64 (b) $7 / 4$ (c) 50,000 (d) $1 / 25$ (e) $-7 / 144$

1.56. Prove (a) $\log _{a} M N=\log _{a} M+\log _{a} N$ and (b) $\log _{a} M^{r}=r \log _{a} M$ indicating restrictions, if any.

1.57. Prove $b^{\log _{b} a}=a$ giving restrictions, if any.

\section*{Countability}
1.58. (a) Prove that there is a one-to-one correspondence between the points of the interval $0 \leqq x \leqq 1$ and $-5 \leqq$ $x \leqq-3$. (b) What is the cardinal number of the sets in $(a)$ ?

Ans. (b) $C$, the cardinal number of the continuum.

1.59. (a) Prove that the set of all rational numbers is countable. (b) What is the cardinal number of the set in $(a)$ ? Ans. (b) $\aleph_{0}$

1.60. Prove that the set of (a) all real numbers and (b) all irrational numbers is noncountable.

1.61. The intersection of two sets $A$ and $B$, denoted by $A \cap B$ or $A B$, is the set consisting of all elements belonging to both $A$ and $B$. Prove that if $A$ and $B$ are countable, so is their intersection.

1.62. Prove that a countable sets of countable sets is countable.

1.63. Prove that the cardinal number of the set of points inside a square is equal to the cardinal number of the sets of points on (a) one side and (b) all four sides. (c) What is the cardinal number in this case? (d) Does a corresponding result hold for a cube?

Ans. (c) $C$

\section*{Limit points, bounds, Bolzano-Weierstrass theorem}
1.64. Given the set of numbers $1,1.1, .9,1.01, .99,1.001, .999, \ldots$, , (a) is the set bounded? (b) Does the set have an l.u.b. and a g.l.b.? If so, determine them. (c) Does the set have any limit points? If so, determine them. (d) Is the set a closed set?

1.65. Given the set $-.9, .9,-.99, .99,-.999, .999$, answer the questions in Problem 1.64 .\\
Ans. (a) Yes (b) 1.u.b. = 1, g.l.b. $=-1$\\
(c) $1,-1$\\
(d) No

1.66. Give an example of a set which has (a) three limit points and (b) no limit points.

1.67. (a) Prove that every point of the interval $0<x<1$ is a limit point. (b) Are there limit points which do not belong to the set in (a)? Justify your answer.

1.68. Let $S$ be the set of all rational numbers in $(0,1)$ having denominator $2^{n}, n=1,2,3, \ldots$ (a) Does $S$ have any limit points? (b) Is $S$ closed?

1.69. (a) Give an example of a set which has limit points but which is not bounded. (b) Does this contradict the Bolzano-Weierstrass theorem? Explain.

\section*{Algebraic and transcendental numbers}
1.70. Prove that (a) $\frac{\sqrt{3}-\sqrt{2}}{\sqrt{3}+\sqrt{2}}$, (b) $\sqrt{2}+\sqrt{3}+\sqrt{5}$ are algebraic numbers.

1.71. Prove that the set of transcendental numbers in $(0,1)$ is not countable.

1.72. Prove that every rational number is algebraic but every irrational number is not necessarily algebraic.

\section*{Complex numbers, polar form}
1.73. Perform each of the indicated operations:

(a) $2(5-3 i)-3(-2+i)+5(i-3)$

(b) $(3-2 i)^{3}$

(c) $\frac{5}{3-4 i}+\frac{10}{4+3 i}$

(d) $\left(\frac{1-i}{1+i}\right)^{10}$

(e) $\left|\frac{2-4 i}{5+7 i}\right|^{2}$

(f) $\frac{(1+i)(2+3 i)(4-2 i)}{(1+2 i)^{2}(1-i)}$

Ans. (a) $1-4 \mathrm{i}(\mathrm{b})-9-46 \mathrm{i}$ (c) $\frac{11}{5}-\frac{2}{5} i$ (d) -1 (e) $\frac{10}{37}$ (f) $\frac{16}{5}-\frac{2}{5} i$

1.74. If $z_{1}$ and $z_{2}$ are complex numbers, prove (a) $\left|\frac{z_{1}}{z_{2}}\right|=\left|\frac{z_{1}}{z_{2}}\right|$ and (b) $\left|z_{1}^{2}\right|=\left|z_{1}\right|^{2}$, giving any restrictions.

1.75.

Prove (a) $\left|z_{1}+z_{2}\right| \leqq\left|z_{1}\right|+\left|z_{2}\right|$, (b) $\left|z_{1}+z_{2}+z_{3}\right| \leqq\left|z_{1}\right|+\left|z_{2}+z_{3}\right|$ and (c) $\left|z_{1}-z_{2}\right| \geqq\left|z_{1}\right|-\left|z_{2}\right|$.

1.76. Find all solutions of $2 x^{4}-3 x^{3}-7 x^{2}-8 x+6=0$.

$$
\text { Ans. } 3, \frac{1}{2},-1 \pm i
$$

1.77. Let $z_{1}$ and $z_{2}$ be represented by points $P_{1}$ and $P_{2}$ in the Argand diagram. Construct lines $O P_{1}$ and $O P_{2}$, where $O$ is the origin. Show that $z_{1}+z_{2}$ can be represented by the point $P_{3}$, where $O P_{3}$ is the diagonal of a parallelogram having sides $O P_{1}$ and $O P_{2}$. This is called the parallelogram law of addition of complex numbers. Because of this and other properties, complex numbers can be considered as vectors in two dimensions.

1.78. Interpret geometrically the inequalities of Problem 1.75.

1.79. Express in polar form (a) $3 \sqrt{3}+3 i$, (b) $-2-2 i$, (c) $1-\sqrt{3} i$, (d) 5 , and (e) $-5 i$. Ans. (a) 6 cis $\pi / 6$ (b) $2 \sqrt{2}$ cis $5 \pi / 4$ (c) 2 cis $5 \pi / 3$ (d) 5 cis 0 (e) 5 cis $3 \pi / 2$

1.80. Evaluate (a) $\left[2\left(\cos 25^{\circ}+i \sin 25^{\circ}\right)\right]\left[5\left(\cos 110^{\circ}+i \sin 110^{\circ}\right)\right]$ and (b) $\frac{12 \operatorname{cis} 16^{\circ}}{\left(3 \operatorname{cis} 44^{\circ}\right)\left(2 \operatorname{cis} 62^{\circ}\right)}$.

Ans. (a) $-5 \sqrt{2}+5 \sqrt{2} i$ (b) $-2 i$

1.81. Determine all the indicated roots and represent them graphically: (a) $(4 \sqrt{2}+4 \sqrt{2} i)^{1 / 3}$, (b) $(-1)^{1 / 5}$, (c) $(\sqrt{3}-i)^{1 / 3}$, and (d) $i^{1 / 4}$.

Ans. (a) 2 cis $15^{\circ}, 2$ cis $135^{\circ}, 2$ cis $255^{\circ}$

(b) cis $36^{\circ}$, cis $108^{\circ}$, cis $180^{\circ}=-1$, cis $252^{\circ}$, cis $324^{\circ}$

(c) $\sqrt[3]{2}$ cis $110^{\circ}, \sqrt[3]{2}$ cis $230^{\circ}, \sqrt[3]{2}$ cis $350^{\circ}$

(d) cis $22.5^{\circ}$, cis $112.5^{\circ}$, cis $202.5^{\circ}$, cis $292.5^{\circ}$

1.82. Prove that $-1+\sqrt{3} i$ is an algebraic number.

1.83. If $z_{1}=\rho_{1}$ cis $\phi_{1}$ and $z_{2}=\rho_{2}$ cis $\phi_{2}$, prove (a) $z_{1} z_{2}=\rho_{1} \rho_{2} \operatorname{cis}\left(\phi_{1}+\phi_{2}\right)$ and (b) $z_{1} / z_{2}=\left(\rho_{1} / \rho_{2}\right) \operatorname{cis}\left(\phi_{1}-\phi_{2}\right)$. Interpret geometrically.

\section*{Mathematical induction}
Prove each of the following.

1.84. $1+3+5+\ldots+(2 n-1)=n^{2}$

1.85.

$$
\frac{1}{1 \cdot 3}+\frac{1}{3 \cdot 5}+\frac{1}{5 \cdot 7}+\cdots+\frac{1}{(2 n-1)(2 n+1)}=\frac{n}{2 n+1}
$$

1.86 .

$$
a+(a+d)+(a+2 d)+\cdots+[a+(n-1) d]=\frac{1}{2} n[2 a+(n-1) d]
$$

1.87. $\frac{1}{1 \cdot 2 \cdot 3}+\frac{1}{2 \cdot 3 \cdot 4}+\frac{1}{3 \cdot 4 \cdot 5}+\cdots+\frac{1}{n(n+1)(n+2)}=\frac{n(n+3)}{4(n+1)(n+2)}$

1.88. $a+a r+a r^{2}+\cdots+a r^{n-1}=\frac{a\left(r^{n}-1\right)}{r-1}, r \neq 1$

1.89. $1^{3}+2^{3}+3^{3}+\cdots+n^{3}=\frac{1}{4} n^{2}(n+1)^{2}$

1.90. $1(5)+2(5)^{2}+3(5)^{3}+\cdots+n(5)^{n-1}=\frac{5+(4 n-1) 5^{n+1}}{16}$

1.91. $x^{2 n-1}+y^{2 n-1}$ is divisible by $x+y$ for $n=1,2,3, \ldots$

1.92. $(\cos \phi+i \sin \phi)^{n}=\cos n \phi+i \sin n \phi$. Can this be proved if $n$ is a rational number?

1.93. $\frac{1}{2}+\cos x+\cos 2 x+\cdots+\cos n x=\frac{\sin \left(n+\frac{1}{2}\right) x}{2 \sin \frac{1}{2} x}, x \neq 0, \pm 2 \pi, \pm 4 \pi, \ldots$

1.94. $\sin x+\sin 2 x+\cdots+\sin n x=\frac{\cos \frac{1}{2} x-\cos \left(n+\frac{1}{2}\right) x}{2 \sin \frac{1}{2} x}, x \neq 0, \pm 2 \pi, \pm 4 \pi \ldots$

1.95. $\quad(a+b)^{n}=a^{n}+{ }_{n} C_{1} a^{n-1} b+{ }_{n} C_{2} a^{n-2} b^{2}+\cdots+{ }_{n} C_{n-1} a b^{n-1}+b_{n}$ where ${ }_{n} C_{r}=\frac{n(n-1)(n-2) \ldots(n-r+1)}{r !}=\frac{n !}{r !(n-r) !}={ }_{n} C_{n-r}$. Here $p !=p(p-1) \ldots 1$ and $0 !$ is defined as 1. This is called the binomial theorem. The coefficients ${ }_{n} C_{0}=1,{ }_{n} C_{1}=n,{ }_{n} C_{2}=\frac{n(n-1)}{2 !}, \ldots,{ }_{n} C_{n}=1$ are called the binomial coefficients. ${ }_{n} C_{r}$ is also written $\left(\begin{array}{l}n \\ r\end{array}\right)$.

\section*{Miscellaneous problems}
1.96. Express each of the following integers (scale of 10) in the scale of notation indicated: (a) 87 (two), (b) 64 (three) (c) 1736 (nine). Check each answer.\\
Ans. (a) 1010111\\
b) 2101\\
(c) 2338

1.97. If a number is 144 in the scale of 5 . what is the number in the scale of (a) 2 and (b) 8 ?

1.98. Prove that every rational number $p / q$ between 0 and 1 can be expressed in the form

$$
\frac{p}{q}=\frac{a_{1}}{2}+\frac{a_{2}}{2^{2}}+\cdots+\frac{a_{n}}{2^{n}}+\cdots
$$

where the $a$ 's can be determined uniquely as 0 's or 1 's and where the process may or may not terminate. The representation $0 . a_{1} a_{2} \ldots a_{n} \ldots$ is then called the binary form of the rational number. (Hint: Multiply both sides successively by 2 and consider remainders.)

1.99. Express $\frac{2}{3}$ in the scale of (a) 2 , (b) 3 , (c) 8 , and (d) 10 .\\
Ans. (a) 0.1010101\\
(b) 0.2 or 0.2000\\
(c) 0.5252 .\\
(d) $0.6666 \ldots$

1.100. A number in the scale of 2 is 11.01001 . What is the number in the scale of 10 .

Ans. 3.28125

1.101. In what scale of notation is $3+4=12$ ?

Ans. 5

1.102. In the scale of 12 , two additional symbols, $t$ and $e$, must be used to designate the "digits" 10 and 11 , respectively. Using these symbols, represent the integer 5110 (scale of 10) in the scale of 12.

Ans. $2 e 5 t$

1.103. Find a rational number whose decimal expansion is $1.636363 \ldots$

Ans. 18/11

1.104. A number in the scale of 10 consists of six digits. If the last digit is removed and placed before the first digit, the new number is one-third as large. Find the original number.

Ans. 428571

1.105. Show that the rational numbers form a field (see Page 3).

1.106. Using as axioms the relations 1 through 9 on Page 3, prove that (a) $(-3)(0)=0,($ b) $(-2)(+3)=-6$, and $(\mathrm{c})(-2)(-3)=6$.

1.107. (a) If $x$ is a rational number whose square is less than 2 , show that $x+\left(2-x^{2}\right) / 10$ is a larger such number. (b) If $x$ is a rational number whose square is greater than 2, find in terms of $x$ a smaller rational number whose square is greater than 2 .

1.108. Illustrate how you would use Dedekind cuts to define (a) $\sqrt{5}+\sqrt{3}$, (b) $\sqrt{3}-\sqrt{2}$, (c) $(\sqrt{3})(\sqrt{2}$ ), and (d) $\sqrt{2} / \sqrt{3}$

\section*{CHAPTER 2}
\section*{Sequences}
\section*{Definition of a Sequence}
A sequence is a set of numbers $u_{1}, u_{2}, u_{3}, \ldots$ in a definite order of arrangement (i.e., a correspondence with the natural numbers or a subset thereof) and formed according to a definite rule. Each number in the sequence is called a term; $u_{n}$ is called the $n$th term. The sequence is called finite or infinite according as there are or are not a finite number of terms. The sequence $u_{1}, u_{2}, u_{3}, \ldots$ is is also designated briefly by $\left\{u_{n}\right\}$.

EXAMPLES. 1. The set of numbers $2,7,12,17, \ldots, 32$ is a finite sequence; the $n$th term is given by $u_{n}=$ $2+5(n-1)=5 n-3, n=1,2, \ldots, 7$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The set of numbers $1,1 / 3,1 / 5,1 / 7, \ldots$ is an infinite sequence with $n$th term $u_{n}=1 /(2 n-1)$, $n=1,2,3, \ldots$.
\end{enumerate}

Unless otherwise specified, we shall consider infinite sequences only.

\section*{Limit of a Sequence}
A number $l$ is called the limit of an infinite sequence $u_{1}, u_{2}, u_{3}, \ldots$ if for any positive number $\epsilon$ we can find a positive number $N$ depending on $\epsilon$ such that $\left|u_{n}-l\right|<\epsilon$ for all integers $n>N$. In such case we write $\lim _{n \rightarrow \infty}$ $u_{n}=l$.

EXAMPLE. If $u_{n}=3+1 / n=(3 n+1) / n$, the sequence is $4,7 / 2,10 / 3, \ldots$ and we can show that $\lim _{n \rightarrow \infty} u_{n}=3$.

If the limit of a sequence exists, the sequence is called convergent; otherwise, it is called divergent. A sequence can converge to only one limit; i.e., if a limit exists, it is unique. See Problem 2.8.

A more intuitive but unrigorous way of expressing this concept of limit is to say that a sequence $u_{1}, u_{2}$, $u_{3}, \ldots$ has a limit $l$ if the successive terms get "closer and closer" to $l$. This is often used to provide a "guess" as to the value of the limit, after which the definition is applied to see if the guess is really correct.

\section*{Theorems on Limits of Sequences}
If $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B$, then

\begin{enumerate}
  \item $\lim _{n \rightarrow \infty}\left(a_{n}+b_{n}\right)=\lim _{n \rightarrow \infty} a_{n}+\lim _{n \rightarrow \infty} b_{n}=A+B$
  \item $\lim _{n \rightarrow \infty}\left(a_{n}-b_{n}\right)=\lim _{n \rightarrow \infty} a_{n}-\lim _{n \rightarrow \infty} b_{n}=A-B$
  \item $\lim _{n \rightarrow \infty}\left(a_{n} \cdot b_{n}\right)=\left(\lim _{n \rightarrow \infty} a_{n}\right)\left(\lim _{n \rightarrow \infty} b_{n}\right)=A B$
  \item $\lim _{n \rightarrow \infty} \frac{a_{n}}{b_{n}}=\frac{\lim _{n \rightarrow \infty} a_{n}}{\lim _{n \rightarrow \infty} b_{n}}=\frac{A}{B} \quad$ if $\lim _{n \rightarrow \infty} b_{n}=B \neq 0$
\end{enumerate}

If $B=0$ and $A \neq 0, \lim _{n \rightarrow \infty} \frac{a_{n}}{b_{n}}$ does not exist.

If $B=0$ and $A=0, \lim _{n \rightarrow \infty} \frac{a_{n}}{b_{n}}$ may or may not exist.\\
5. $\lim _{n \rightarrow \infty} a_{n}^{p}=\left(\lim _{n \rightarrow \infty} a_{n}\right)^{p}=A^{p}$, for $p=$ any real number if $A^{p}$ exists.\\
6. $\lim _{n \rightarrow \infty} p^{a_{n}}=p^{\lim _{n \rightarrow \infty}^{a_{n}}}=p^{A}$, for $p=$ any real number if $p^{A}$ exists.

\section*{Infinity}
We write $\lim _{n \rightarrow \infty} a_{n}=\infty$ if for each positive number $M$ we can find a positive number $N$ (depending on $M$ ) such that $\mathrm{a}_{\mathrm{n}}>M$ for all $n>N$. Similarly, we write $\lim _{n \rightarrow \infty} a_{n}=-\infty$ if for each positive number $M$ we can find a positive number $N$ such that $a_{n}<-M$ for all $n>N$. It should be emphasized that $\infty$ and $-\infty$ are not numbers and the sequences are not convergent. The terminology employed merely indicates that the sequences diverge in a certain manner. That is, no matter how large a number in absolute value that one chooses, there is an $n$ such that the absolute value of $a_{n}$ is greater than that quantity.

\section*{Bounded, Monotonic Sequences}
If $u_{n} \leqq M$ for $n=1,2,3, \ldots$, where $M$ is a constant (independent of $n$ ), we say that the sequence $\left\{u_{n}\right\}$ is bounded above and $M$ is called an upper bound. If $u_{n} \geqq m$, the sequence is bounded below and $m$ is called a lower bound.

If $m \leqq u_{n} \leqq M$ the sequence is called bounded. Often this is indicated by $\left|u_{n}\right| \leqq P$. Every convergent sequence is bounded, but the converse is not necessarily true.

If $u_{n+1} \geqq u_{n}$ the sequence is called monotonic increasing; if $u_{n+1}>u_{n}$ it is called strictly increasing. Similarly, if $u_{n+1} \leqq u_{n}$ the sequence is called monotonic decreasing, while if $u_{n+1}<u_{n}$ it is strictly decreasing.

EXAMPLES. 1. The sequence $1,1.1,1.11,1.111, \ldots$ is bounded and monotonic increasing. It is also strictly increasing.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The sequence $1,-1,1,-1,1, \ldots$ is bounded but not monotonic increasing or decreasing.

  \item The sequence $-1,-1.5,-2,-2.5,-3, \ldots$ is monotonic decreasing and not bounded. However, it is bounded above.

\end{enumerate}

The following theorem is fundamental and is related to the Bolzano-Weierstrass theorem (Chapter 1, Page 7) which is proved in Problem 2.23.

Theorem Every bounded monotonic (increasing or decreasing) sequence has a limit.

\section*{Least Upper Bound and Greatest Lower Bound of a Sequence}
A number $\underline{M}$ is called the least upper bound (1.u.b.) of the sequence $\left\{u_{n}\right\}$ if $u_{n} \leqq \underline{M}, n=1,2,3, \ldots$ while at least one term is greater than $\underline{M}-\epsilon$ for any $\epsilon>0$.

A number $\bar{m}$ is called the greatest lower bound (g.l.b.) of the sequence $\left\{u_{n}\right\}$ if $u_{n} \geqq \bar{m}, n=1,2$, $3, \ldots$ while at least one term is less than $\bar{m}+\epsilon$ for any $\epsilon>0$.

Compare with the definition of l.u.b. and g.l.b. for sets of numbers in general (see Page 6).

\section*{Limit Superior, Limit Inferior}
A number $\bar{l}$ is called the limit superior, greatest limit, or upper limit (lim sup or $\overline{\lim }$ ) of the sequence $\left\{u_{n}\right\}$ if infinitely many terms of the sequence are greater than $\bar{l}-\epsilon$ while only a finite number of terms are greater than $\bar{l}+\epsilon$, where $\epsilon$ is any positive number.

A number $\underline{l}$ is called the limit inferior, least limit, or lower limit ( $\lim \inf$ or $\underline{\underline{l i m}}$ ) of the sequence $\left\{u_{n}\right\}$ if infinitely many terms of the sequence are less than $\underline{l}+\epsilon$ while only a finite number of terms are less than $\underline{l}-\epsilon$, where $\epsilon$ is any positive number.

These correspond to least and greatest limiting points of general sets of numbers.

If infinitely many terms of $\left\{u_{n}\right\}$ exceed any positive number $M$, we define $\lim \sup \left\{u_{n}\right\}=\infty$. If infinitely many terms are less than $-M$, where $M$ is any positive number, we define $\lim \inf \left\{u_{n}\right\}=-\infty$.

If $\lim _{n \rightarrow \infty} u_{n}=\infty$, we define $\lim \sup \left\{u_{n}\right\}=\lim \inf \left\{u_{n}\right\}=\infty$.

If $\lim _{n \rightarrow \infty} u_{n}=-\infty$, we define $\lim \sup \left\{u_{n}\right\}=\lim \inf \left\{u_{n}\right\}=-\infty$.

Although every bounded sequence is not necessarily convergent, it always has a finite lim sup and lim inf.

A sequence $\left\{u_{n}\right\}$ converges if and only if $\lim \sup u_{n}=\lim \inf u_{n}$ is finite.

\section*{Nested Intervals}
Consider a set of intervals $\left[a_{n}, b_{n}\right], n=1,2,3, \ldots$, where each interval is contained in the preceding one and $\lim _{n \rightarrow \infty}\left(a_{n}-b_{n}\right)=0$. Such intervals are called nested intervals.

We can prove that to every set of nested intervals there corresponds one and only one real number. This can be used to establish the Bolzano-Weierstrass theorem of Chapter 1. (See Problems 2.22 and 2.23.)

\section*{Cauchy's Convergence Criterion}
Cauchy's convergence criterion states that a sequence $\left\{u_{n}\right\}$ converges if and only if for each $\epsilon>0$ we can find a number $N$ such that $\left|u_{p}-u_{q}\right|<\epsilon$ for all $p, q>N$. This criterion has the advantage that one need not know the limit $l$ in order to demonstrate convergence.

\section*{Infinite Series}
Let $u_{1}, u_{2}, u_{3}, \ldots$ be a given sequence. Form a new sequence $S_{1}, S_{2}, S_{3}, \ldots$ where

$$
S_{1}=u_{1}, S_{2}=u_{1}+u_{2}, S_{3}=u_{1}+u_{2}+u_{3}, \ldots,+S_{n}=u_{1}+u_{2}+u_{3}+\cdots+u_{n}, \ldots
$$

where $S_{n}$, called the $n$th partial sum, is the sum of the first $n$ terms of the sequence $\left\{u_{n}\right\}$.

The sequence $S_{1}, S_{2}, S_{3}, \ldots$ is symbolized by

$$
u_{1}+u_{2}+u_{3}+\cdots=\sum_{n=1}^{\infty} u_{n}
$$

which is called an infinite series. If $\lim S_{n}=S$ exists, the series is called convergent and $S$ is its sum; otherwise, the series is called divergent. ${ }^{n \rightarrow \infty}$

Further discussion of infinite series and other topics related to sequences is given in Chapter 11.

\section*{SOLVED PROBLEMS}
\section*{Sequences}
2.1. Write the first five terms of each of the following sequences.

(a) $\left\{\frac{2 n-1}{3 n+2}\right\}$

(b) $\left\{\frac{1-(-1)^{n}}{n^{3}}\right\}$

(c) $\left\{\frac{(-1)^{n-1}}{2 \cdot 4 \cdot 6 \cdots 2 n}\right\}$

(d) $\left\{\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\cdots+\frac{1}{2^{n}}\right\}$

(e) $\left\{\frac{(-1)^{n-1} x^{2 n-1}}{(2 n-1) !}\right\}$

(a) $\frac{1}{5}, \frac{3}{8}, \frac{5}{11}, \frac{7}{14}, \frac{9}{17}$

(b) $\frac{2}{1^{3}}, 0, \frac{2}{3^{3}}, 0, \frac{2}{5^{3}}$

(c) $1 \frac{1}{2}, \frac{-1}{2 \cdot 4}, \frac{1}{2 \cdot 4 \cdot 6}, \frac{-1}{2 \cdot 4 \cdot 6 \cdot 8}, \frac{1}{2 \cdot 4 \cdot 6 \cdot 8 \cdot 10}$

(d) $\frac{1}{2}, \frac{1}{2}+\frac{1}{4}, \frac{1}{2}+\frac{1}{4}+\frac{1}{8}, \frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}, \frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\frac{1}{32}$

(e) $\frac{x}{1 !}, \frac{-x^{3}}{3 !}, \frac{x^{5}}{5 !}, \frac{-x^{7}}{7 !}, \frac{x^{9}}{9 !}$

Note that $n !=1 \cdot 2 \cdot 3 \cdot 4 \ldots n$. Thus, $1 !=1,3 !=1 \cdot 2 \cdot 3=6,5 !=1 \cdot 2 \cdot 3 \cdot 4 \cdot 5=120$, etc. We define $0 !=1$.

2.2. Two students were asked to write an $n$th term for the sequence $1,16,81,256, \ldots$ and to write the 5 th term of the sequence. One student gave the $n$th term as $u_{n}=n^{4}$. The other student, who did not recognize this simple law of formation, wrote $u_{n}=10 n^{3}-35 n^{2}+50 n-24$. Which student gave the correct 5 th term?

If $u_{n}=n^{4}$, then $u_{1}=1^{4}=1, u_{2}=2^{4}=16, u_{3}=3^{4}=81$, and $u_{4}=4^{4}=256$, which agrees with the first four terms of the sequence. Hence, the first student gave the 5 th term as $u_{5}=5^{4}=625$.

If $u_{n}=10 n^{3}-35 n^{2}+50 n-24$, then $u_{1}=1, u_{2}=16, u_{3}=81$, and $u_{4}=256$, which also agrees with the first four terms given. Hence, the second student gave the 5 th term as $u_{5}=601$.

Both students were correct. Merely giving a finite number of terms of a sequence does not define a unique $n$th term. In fact, an infinite number of $n$th terms is possible.

\section*{Limit of a sequence}
2.3. A sequence has its $n$th term given by $u_{n}=\frac{3 n-1}{4 n+5}$. (a) Write the 1 st, 5th, 10th, 100th, 1000th, 10,000th and 100,000 th, terms of the sequence in decimal form. Make a guess as to the limit of this sequence as $n \rightarrow \infty$.

(b) Using the definition of limit, verify that the guess in $(a)$ is actually correct.

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-040}
\end{center}

A good guess is that the limit is $.75000 \ldots=\frac{3}{4}$. Note that it is only for large enough values of $n$ that a possible limit may become apparent.

(b) We must show that for any given $\epsilon>0$ (no matter how small) there is a number $N$ (depending on $\epsilon$ ) such that $\left|u_{n}-\frac{3}{4}\right|<\epsilon$ for all $n>N$.

$$
\begin{aligned}
\text { Now } & {\left[\frac{3 n-1}{4 n+5}-\frac{3}{4}\right]=\left[\frac{-19}{4(4 n+5)}\right]<\varepsilon \quad \text { when } \frac{19}{4(4 n+5)}<\varepsilon \text { or } } \\
& \frac{4(4 n+5)}{19}>\frac{1}{\varepsilon}, \quad 4 n+5>\frac{19}{4 \varepsilon}, \quad n>\frac{1}{4}\left(\frac{19}{4 \varepsilon}-5\right)
\end{aligned}
$$

Choosing $N=\frac{1}{4}(19 / 4 \epsilon-5)$, we see that $\left|u_{n}-\frac{3}{4}\right|<\epsilon$ for all $n>N$, so that $\lim _{n \rightarrow \infty}=\frac{3}{4}$ and the proof is complete.

Note that if $\epsilon=.001$ (for example), $N=\frac{1}{4}(19000 / 4-5)=1186 \frac{1}{4}$. This means that all terms of the sequence beyond the 1186th term differ from 3/4 in absolute value by less than .001 .

2.4. Prove that $\lim _{n \rightarrow \infty} \frac{c}{n^{p}}=0$ where $c \neq 0$ and $p>0$ are constants (independent of $n$ ).

We must show that for any $\epsilon>0$ there is a number $N$ such that $\left|c / n^{p}-0\right|<\epsilon$ for all $n>N$.

Now $\left|\frac{c}{n^{p}}\right|<\epsilon$ when $\frac{|c|}{n^{p}}<\epsilon$; i.e., $n^{p}>\frac{|c|}{\varepsilon}$ or $n>\left(\frac{|c|}{\varepsilon}\right)^{1 / p}$. Choosing $N=\left(\frac{|c|}{\varepsilon}\right)^{1 / p}$ (depending on $\epsilon$ ), we see that $\left|c / n^{p}\right|<\epsilon$ for all $n>N$, proving that $\lim \left(c / n^{p}\right)=0$.

2.5. Prove that $\lim _{n \rightarrow \infty} \frac{1+2 \cdot 10^{n}}{5+3 \cdot 10^{n}}=\frac{2}{3}$.

We must show that for any $\epsilon>0$ there is a number $N$ such that $\left|\frac{1+2 \cdot 10^{n}}{5+3 \cdot 10^{n}}-\frac{2}{3}\right|<\varepsilon$ for all $n>N$.

Now $\left|\frac{1+2 \cdot 10^{n}}{5+3 \cdot 10^{n}}-\frac{2}{3}\right|=\left|\frac{-7}{3\left(5+3 \cdot 10^{n}\right)}\right|<\varepsilon$ when $\frac{7}{3\left(5+3 \cdot 10^{n}\right)}<\varepsilon$; i.e., when $\frac{3}{7}\left(5+3 \cdot 10^{n}\right)>1 / \varepsilon$,

$3 \cdot 10^{n}>7 / 3 \epsilon-5,10^{n}>\frac{1}{8}(7 / 3 \epsilon-5)$ or $n>\log _{10}\left\{\frac{1}{3}(7 / 3 \epsilon-5)\right\}=N$, proving the existence of $N$ and thus establishing the required result.

Note that the value of $N$ is real only if $7 / 3 \epsilon-5>0$; i.e., $0<\epsilon<7 / 15$. If $\epsilon \geqq 7 / 15$, we see that

$\left|\frac{1+2 \cdot 10^{n}}{5+3 \cdot 10^{n}}-\frac{2}{3}\right|<\varepsilon$ for all $n>0$

2.6. Explain exactly what is meant by the statements (a) $\lim _{n \rightarrow \infty} 3^{2 n-1}=\infty$ and (b) $\lim _{n \rightarrow \infty}(1-2 n)=-\infty$.

(a) If for each positive number $M$ we can find a positive number $N$ (depending on $M$ ) such that $a_{n}>M$ for all $n>N$, then we write $\lim _{n \rightarrow \infty} a_{n}=\infty$.

In this case, $3^{2 n-1}>M$ when $(2 n-1) \log 3>\log M$; i.e., $n>\frac{1}{2}\left(\frac{\log M}{\log 3}+1\right)=N$.\\
(b) If for each positive number $M$ we can find a positive number $N$ (depending on $M$ ) such that $a_{n}<-M$ for all $n>N$, then we write $\lim =-\infty$.

In this case, $1-2 n<-M$ when $2 n-1>M$ or $n>\frac{1}{2}(M+1)=N$.

It should be emphasized that the use of the notations $\infty$ and $-\infty$ for limits does not in any way imply convergence of the given sequences, since $\infty$ and $-\infty$ are not numbers. Instead, these are notations used to describe that the sequences diverge in specific ways.

2.7. Prove that $\lim _{n \rightarrow \infty} x^{n}=0$ if $|x|<1$.

Method 1: We can restrict ourselves to $x \neq 0$, since if $x=0$, the result is clearly true. Given $\epsilon>0$, we must show that there exists $N$ such that $\left|x^{n}\right|<\epsilon$ for $n>N$. Now $\left|x^{n}\right|=|x|^{n}<\epsilon$ when $n \log _{10}|x|<\log _{10} \epsilon$. Dividing by $\log _{10}|x|$, which is negative, yields $n>\frac{\log _{10} \varepsilon}{\log _{10}|x|}=N$, proving the required result.

Method 2: Let $|x|=1 /(1+p)$, where $p>0$. By Bernoulli's inequality (Problem 1.31), we have $\left|x^{n}\right|=$ $|x|^{n}=1 /(1+p)^{n}<1 /(1+n p)<\epsilon$ for all $n>N$. Thus, $\lim _{n \rightarrow \infty} x^{n}=0$.

\section*{Theorems on limits of sequences}
2.8. Prove that if $\lim _{n \rightarrow \infty} u_{n}$ exists, it must be unique.

We must show that if $\lim _{n \rightarrow \infty} u_{n}=l_{1}$ and $\lim _{n \rightarrow \infty} u_{n}=l_{2}$, then $l_{1}=l_{2}$.

By hypothesis, given any $\epsilon>0$ we can find $N$ such that

$$
\left|u_{n}-l_{1}\right|<\frac{1}{2} \varepsilon \text { when } n>N, \quad\left|u_{n}-l_{2}\right|<\frac{1}{2} \varepsilon \text { when } n>N
$$

Then

$$
\left|l_{1}-l_{2}\right|=\left|l_{1}-u_{n}+u_{n}-l_{2}\right| \leqq\left|l_{1}-u_{n}\right|+\left|u_{n}-l_{2}\right|<\frac{1}{2} \varepsilon+\frac{1}{2} \varepsilon=\varepsilon
$$

i.e., $\left|l_{1}-l_{2}\right|$ is less than any positive $\epsilon$ (however small) and so must be zero. Thus, $l_{1}=l_{2}$.

2.9. If $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B$, prove that $\lim _{n \rightarrow \infty}\left(a_{n}+b_{n}\right)=A+B$.

We must show that for any $\epsilon>0$, we can find $N>0$ such that $\left|\left(a_{n}+b_{n}\right)-(A+B)\right|<\epsilon$ for all $n>N$. From absolute value property 2 , Page 4 , we have


\begin{equation*}
\left|\left(a_{n}+b_{n}\right)-(A+B)\right|=\left|\left(a_{n}-A\right)+\left(b_{n}-B\right)\right| \leqq\left|a_{n}-A\right|+\left|b_{n}-B\right| \tag{1}
\end{equation*}


By hypothesis, given $\epsilon>0$ we can find $N_{1}$ and $N_{2}$ such that

\[
\begin{array}{ll}
\left|a_{n}-A\right|<\frac{1}{2} \varepsilon & \text { for all } n>N_{1} \\
\left|b_{n}-B\right|<\frac{1}{2} \varepsilon & \text { for all } n>N_{2} \tag{3}
\end{array}
\]

Then from Equations (1), (2), and (3),

$$
\left|\left(a_{n}+b_{n}\right)-(A+B)\right|<\frac{1}{2} \varepsilon+\frac{1}{2} \varepsilon=\varepsilon \quad \text { for all } n>N
$$

where $N$ is chosen as the larger of $N_{1}$ and $N_{2}$. Thus, the required result follows.

2.10. Prove that a convergent sequence is bounded. Now

Given $\lim _{n \rightarrow \infty} a_{n}=A$, we must show that there exists a positive number $P$ such that $\left|a_{n}\right|<P$ for all $n$.

$$
\left|a_{n}\right|=\left|a_{n}-A+A\right| \leqq\left|a_{n}-A\right|+|A|
$$

But by hypothesis we can find $N$ such that $\left|a_{n}-A\right|<\epsilon$ for all $n>N$, i.e.,

$$
\left|a_{n}\right|<\epsilon+|A| \quad \text { for all } n>N
$$

$\epsilon+|A|$.

It follows that $\left|a_{n}\right|<P$ for all $n$ if we choose $P$ as the largest one of the numbers $a_{1}, a_{2}, \ldots, a_{N}$,

2.11. If $\lim _{n \rightarrow \infty} b_{n}=B \neq 0$, prove there exists a number $N$ such that $\left|b_{n}\right|>\frac{1}{2}|B|$ for all $n>N$.

Since $B=B-b_{n}+b_{n}$, we have:


\begin{equation*}
|B| \leqq\left|B-b_{n}\right|+\left|b_{n}\right| \tag{1}
\end{equation*}


Now we can choose $N$ so that $\left|B-b_{n}\right|=\left|b_{n}-B\right|<\frac{1}{2}|B|$ for all $n>N$, since $\lim _{n \rightarrow \infty} b_{n}=B$ by hypothesis.

Hence, from Equation (1), $|B|<\frac{1}{2}|B|+\left|b_{n}\right|$ or $\left|b_{n}\right|>\frac{1}{2}|B|$ for all $n>N$.

2.12. If $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B$, prove that $\lim _{n \rightarrow \infty} a_{n} b_{n}=A B$.

Using Problem 2.10, we have


\begin{align*}
\left|a_{n} b_{n}-A B\right|=\left|a_{n}\left(b_{n}-B\right)+B\left(a_{n}-A\right)\right| & \leqq\left|a_{n}\right|\left|b_{n}-B\right|+|B|\left|a_{n}-A\right| \\
& \leqq P\left|b_{n}-B\right|+(|B|+1)\left|a_{n}-A\right| \tag{1}
\end{align*}


But since $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B$, given any $\epsilon>0$ we can find $N_{1}$ and $N_{2}$ such that

$$
\left|b_{n}-B\right|<\frac{\varepsilon}{2 P} \text { for all } n>N_{1} \quad\left|a_{n}-A\right|<\frac{\varepsilon}{2(|B|+1)} \text { for all } n>N_{2}
$$

Hence, from Equation (1), $\left|a_{n} b_{n}-A B\right|<\frac{1}{2} \epsilon+\frac{1}{2} \epsilon=\epsilon$ for all $n>N$, where $N$ is the larger of $N_{1}$ and $N_{2}$.\\
Thus, the result is proved.

2.13. If $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B \neq 0$, prove (a) $\lim _{n \rightarrow \infty} \frac{1}{b_{n}}=\frac{1}{B}$, (b) $\lim _{n \rightarrow \infty} \frac{a_{n}}{b_{n}}=\frac{A}{B}$.

(a) We must show that for any given $\epsilon>0$, we can find $N$ such that


\begin{equation*}
\left|\frac{1}{b_{n}}-\frac{1}{B}\right|=\frac{\left|B-b_{n}\right|}{|B|\left|b_{n}\right|}<\varepsilon \quad \text { for all } n>N \tag{1}
\end{equation*}


By hypothesis, given any $\epsilon>0$, we can find $N_{1}$, such that $\left|b_{n}-B\right|<\frac{1}{2} B^{2} \epsilon$ for all $n>N_{1}$. 2.11).

Also, since $\lim _{n \rightarrow \infty} b_{n}=B \neq 0$, we can find $N_{2}$ such that $\left|b_{n}\right|>\frac{1}{2}|B|$ for all $n>N_{2}$ (see Problem

Then if $N$ is the larger of $N_{1}$ and $N_{2}$, we can write Equation (1) as

$$
\left|\frac{1}{b_{n}}-\frac{1}{B}\right|=\frac{\left|b_{n}-B\right|}{|B|\left|b_{n}\right|}<\frac{\frac{1}{2} B^{2} \varepsilon}{|B| \cdot \frac{1}{2}|B|}=\varepsilon \quad \text { for all } n>N
$$

and the proof is complete.

(b) From (a) and Problem 2.12, we have

$$
\lim _{n \rightarrow \infty} \frac{a_{n}}{b_{n}}=\lim _{n \rightarrow \infty}\left(a_{n} \cdot \frac{1}{\mathrm{~b}_{n}}\right)=\lim _{n \rightarrow \infty} a_{n} \cdot \lim _{n \rightarrow \infty} \frac{1}{b_{n}}=A \cdot \frac{1}{B}=\frac{A}{B}
$$

This can also be proved directly (see Problem 2.41).

2.14. Evaluate each of the following, using theorems on limits.

(a) $\lim _{n \rightarrow \infty} \frac{3 n^{2}-5 n}{5 n^{2}+2 n-6}=\lim _{n \rightarrow \infty} \frac{3-5 / n}{5+2 / n-6 / n^{2}}=\frac{3+0}{5+0+0}=\frac{3}{5}$

(b) $\lim _{n \rightarrow \infty}\left\{\frac{n(n+2)}{n+1}-\frac{n^{3}}{n^{2}+1}\right\}=\lim _{n \rightarrow \infty}\left\{\frac{n^{3}+n^{2}+2 n}{(n+1)\left(n^{2}+1\right)}\right\}=\lim _{n \rightarrow \infty}\left\{\frac{1+1 / n+2 / n^{2}}{(1+1 / n)\left(1+1 / n^{2}\right)}\right\}$

$$
=\frac{1+0+0}{(1+0) \cdot(1+0)}=1
$$

(c) $\lim _{n \rightarrow \infty}(\sqrt{n+1}-\sqrt{n})=\lim _{n \rightarrow \infty}(\sqrt{n+1}-\sqrt{n}) \frac{\sqrt{n+1}+\sqrt{n}}{\sqrt{n+1}+\sqrt{n}}=\lim _{n \rightarrow \infty} \frac{1}{\sqrt{n+1}+\sqrt{n}}=0$

(d) $\lim _{n \rightarrow \infty} \frac{3 n^{2}+4 n}{2 n-1}=\lim _{n \rightarrow \infty} \frac{3=4 / n}{2 / n-1 / n^{2}}$

Since the limits of the numerator and the denominator are 3 and 0 , respectively, the limit does not exist.

Since $\frac{3 n^{2}+4 n}{2 n-1}>\frac{3 n^{2}}{2 n}=\frac{3 n}{2}$ can be made larger than any positive number $M$ by choosing $n>N$, we can write, if desired, $\lim _{n \rightarrow \infty} \frac{3 n^{2}+4 n}{2 n-1}=\infty$.

(e) $\lim _{n \rightarrow \infty}\left(\frac{2 n-3}{2 n+7}\right)^{4}=\left(\lim _{n \rightarrow \infty} \frac{2-3 / n}{3+7 / n}\right)^{4}=\left(\frac{2}{3}\right)^{4}=\frac{16}{18}$

(f) $\lim _{n \rightarrow \infty} \frac{2 n^{5}-4 n^{2}}{3 n^{7}+n^{3}-10}=\lim _{n \rightarrow \infty} \frac{2 / n^{2}-4 / n^{5}}{3+1 / n^{4}-10 / n^{7}}=\frac{0}{3}=0$

(g) $\lim _{n \rightarrow \infty} \frac{1+2 \cdot 10^{n}}{5+3 \cdot 10^{n}}=\lim _{n \rightarrow \infty} \frac{10^{-n}+2}{5 \cdot 10^{-n}+3}=\frac{2}{3} \quad$ (Compare with Problem 2.5.)

\section*{Bounded monotonic sequences}
2.15. Prove that the sequence with $n$th $u_{n}=\frac{2 n-7}{3 n+2}$ (a) is monotonic increasing, (b) is bounded above, (c) is bounded below, (d) is bounded, (e) has a limit.

(a) $\left\{u_{n}\right\}$ is monotonic increasing if $u_{n+1} \geq u_{n}, n=1,2,3, \ldots$ Now

$$
\frac{2(n+1)-7}{3(n+1)+2} \geqq \frac{2 n-7}{3 n+2} \text { if and only if } \frac{2 n-5}{2 n+5} \geqq \frac{2 n-7}{3 n+2}
$$

or $(2 n-5)(3 n+2) \geq(2 n-7)(3 n+5), 6 n^{2}-11 n-10 \geq 6 n^{2}-11 n-35$, i.e., $-10 \geq-35$, which is true. Thus, by reversal of steps in the inequalities, we see that $\left\{u_{n}\right\}$ is monotonic increasing. Actually, since $-10>-35$, the sequence is strictly increasing.

(b) By writing some terms of the sequence, we may guess that an upper bound is 2 (for example). To prove this we must show that $u_{n} \leq 2$. If $(2 n-7) /(3 n+2) \leq 2$, then $2 n-7 \leq 6 n+4$ or $-4 n<11$, which is true. Reversal of steps proves that 2 is an upper bound.

(c) Since this particular sequence is monotonic increasing, the first term -1 is a lower bound; i.e., $u_{n} \geqq-1$, $n=1,2,3, \ldots$ Any number less than -1 is also a lower bound.

(d) Since the sequence has an upper and a lower bound, it is bounded. Thus, for example, we can write $\left|u_{n}\right| \leqq 2$ for all $n$.

(e) Since every bounded monotonic (increasing or decreasing) sequence has a limit, the given sequence has a limit. In fact, $\lim _{n \rightarrow \infty} \frac{2 n-7}{3 n+2}=\lim _{n \rightarrow \infty} \frac{2-7 / n}{3+2 / n}=\frac{2}{3}$.

2.16. A sequence $\left\{u_{n}\right\}$ is defined by the recursion formula $u_{n+1} \sqrt{3 u_{n}}, u_{1}=1$. (a) Prove that $\lim u_{n}$ exists. (b) Find the limit in (a).

(a) The terms of the sequence are $u_{1}=1, u_{2}=\sqrt{3 u_{1}}=3^{1 / 2}, u_{3}=\sqrt{3 u_{2}}=3^{1 / 2+1 / 4}, \ldots$.

The $n$th term is given by $u_{n}=3^{1 / 2+1 / 4+\cdots+1 / 2 n-1}$, as can be proved by mathematical induction (Chapter 1 ).

Clearly, $u_{n+1} \geqq u_{n}$. Then the sequence is monotonic increasing.

By Problem 1.14, $u_{n} \leq 3^{1}=3$, i.e., $u_{n}$ is bounded above. Hence, $u_{n}$ is bounded (since a lower bound is zero).

Thus, a limit exists, since the sequence is bounded and monotonic increasing.

(b) Let $x=$ required limit. Since $\lim _{n \rightarrow \infty} u_{n+1}=\lim _{n \rightarrow \infty} \sqrt{3 u_{n}}$, we have $x=\sqrt{3 x}$ and $x=3$. (The other possibility, $x=0$, is excluded, since $u_{n} \geqq 1$.)

Another method: $\lim _{n \rightarrow \infty} 3^{1 / 2+1 / 4+\cdots+1 / 2^{n-1}}=\lim _{n \rightarrow \infty} 3^{1-1 / 2^{n}}=3 \lim _{n \rightarrow \infty}^{\left(1-1 / 2^{n}\right)}=3^{1}=3$

2.17. Verify the validity of the entries in the following table.

\begin{center}
\begin{tabular}{lllll}
\hline
SEQUENCE & BOUNDED & \begin{tabular}{l}
MONOTONIC \\
INCREASING \\
\end{tabular} & \begin{tabular}{l}
MONOTONIC \\
DECREASING \\
\end{tabular} & \begin{tabular}{l}
LIMIT \\
EXISTS \\
\end{tabular} \\
\hline
$2,1.9,1.8,1.7, \ldots, 2-(n-1) / 10 \ldots$ & No & No & Yes & No \\
$1,-1,1,-1, \ldots,(-1)^{n-1}, \ldots$ & Yes & No & No & No \\
$\frac{1}{2},-\frac{1}{3}, \frac{1}{4},-\frac{1}{5}, \ldots,(-1)^{n-1} /(n+1), \ldots$ & Yes & No & No & Yes $(0)$ \\
$.6, .66, .666, \ldots, \frac{2}{3}\left(1-1 / 10^{n}\right), \ldots$ & Yes & Yes & No & Yes $\left(\frac{2}{3}\right)$ \\
$-1,+2,-3,+4,-5, \ldots,(-1)^{n} n, \ldots$ & No & No & No & No \\
\end{tabular}
\end{center}

2.18. Prove that the sequence with the $n$th term $u_{n}=\left(1+\frac{1}{n}\right)^{n}$ is monotonic, increasing, and bounded, and thus a limit exists. The limit is denoted by the symbol $e$.

Note: $\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}=e$, where $e \cong 2.71828 \cdots$ was introduced in the eighteenth century by Leonhart Euler as the base for a system of logarithms in order to simplify certain differentiation and integration formulas.

By the binomial theorem, if $n$ is a positive integer (see Problem 1.95),

$$
(1+x)^{n}=1+n x+\frac{n(n-1)}{2 !} x^{2}+\frac{n(n-1)(n-2)}{3 !} x^{2}+\cdots+\frac{n(n-1) \cdots(n-n+1)}{n !} x^{n}
$$

Letting $x=1 / n$,

$$
\begin{aligned}
u^{n}=\left(1+\frac{1}{n}\right)^{n}= & 1+n \frac{1}{n}+\frac{n(n-1)}{2 !} \frac{1}{n^{2}}+\cdots+\frac{n(n-1) \cdots(n-n+1)}{n !} \frac{1}{n^{n}} \\
= & 1+1+\frac{1}{2 !}\left(1-\frac{1}{n}\right)+\frac{1}{3 !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \\
& +\cdots+\frac{1}{n !}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdots\left(1-\frac{n-1}{n}\right)
\end{aligned}
$$

Since each term beyond the first two terms in the last expression is an increasing function of $n$, it follows that the sequence $u_{n}$ is a monotonic increasing sequence.

It is also clear that

$$
\left(1+\frac{1}{n}\right)^{n}<1+1+\frac{1}{2 !}+\frac{1}{3 !}+\ldots+\frac{1}{n !}<1+1+\frac{1}{2}+\frac{1}{2^{2}}+\cdots+\frac{1}{2^{n-1}}<3
$$

by Problem 1.14.

Thus, $u_{n}$ is bounded and monotonic increasing, and so has a limit which we denote by $e$. The value of $e=$ 2.71828 . .

2.19. Prove that $\lim _{x \rightarrow \infty}\left(1+\frac{1}{x}\right)^{x}=e$, where $x \rightarrow \infty$ in any manner whatsoever (i.e., not necessarily along the positive integers, as in Problem 2.18).

If $n=$ largest integer $\leqq x$, then $n \leqq x \leqq n+1$ and $\left(1+\frac{1}{n+1}\right)^{n} \leqq\left(1+\frac{1}{x}\right)^{x} \leqq\left(1+\frac{1}{n}\right)^{n+1}$. Since $\lim _{n \rightarrow \infty}\left(1+\frac{1}{n+1}\right)^{n}=\lim _{n \rightarrow \infty}\left(1+\frac{1}{n+1}\right)^{n+1} /\left(1+\frac{1}{n+1}\right)=e$ and $\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n+1}=\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^{n}\left(1+\frac{1}{n}\right)=e$, it follows that $\lim _{x \rightarrow \infty}\left(1+\frac{1}{x}\right)^{x}=e$.

\section*{Least upper bound, greatest lower bound, limit superior, limit inferior}
2.20. Find the (a) l.u.b., (b) g.l.b., (c) $\lim \sup (\overline{\lim })$, and (d) $\lim \inf (\underline{\lim })$ for the sequence $2,-2,1,-1,1,-1,1$, $-1, \ldots$.

(a) 1.u.b. $=2$, since all terms are less than equal to 2 , while at least one term (the 1 st) is greater than $2-\epsilon$ for any $\epsilon>0$.

(b) g.1.b. $=-2$, since all terms are greater than or equal to -2 , while at least one term (the 2 nd) is less than $-2+\epsilon$ for any $\epsilon>0$.

(c) $\lim$ sup or $\overline{\lim }=1$, since infinitely many terms of the sequence are greater than $1-\epsilon$ for any $\epsilon>0$ (namely, all 1's in the sequence), while only a finite number of terms are greater than $1+\epsilon$ for any $\epsilon>0$ (namely, the 1st term).

(d) $\lim \inf$ or $\underline{\lim }=-1$, since infinitely many terms of the sequence are less than $-1+\epsilon$ for any $\epsilon>0$ (namely, all -1 's in the sequence), while only a finite number of terms are less than $-1-\epsilon$ for any $\epsilon>0$ (namely, the 2nd term).

2.21. Find the (a) l.u.b., (b) g.l.b., (c) $\lim \sup (\overline{\lim }$ ), and (d) $\lim \inf (\underline{\lim })$ for the sequences in Problem 2.17.

The results are shown in the following table.

\begin{center}
\begin{tabular}{lllll}
\hline
SEQUENCE & l.u.b. & g.l.b. & lim sup or lim & lim inf or lim \\
\hline
$2,1.9,1.8,1.7, \ldots, 2-(n-1) / 10 \ldots$ & 2 & none & $-\infty$ & $-\infty$ \\
$1,-1,1,-1, \ldots,(-1)^{n-1}, \ldots$ & 1 & -1 & 1 & -1 \\
$\frac{1}{2},-\frac{1}{3}, \frac{1}{4}-\frac{1}{5}, \ldots,(-1)^{n-1} /(n+1), \ldots$ & $\frac{1}{2}$ & $-\frac{1}{3}$ & 0 & 0 \\
$.6, .66, .666, \ldots, \frac{2}{3}\left(1-1 / 10^{n}\right), \ldots$ & $\frac{2}{3}$ & 6 & $\frac{2}{3}$ & $\frac{2}{3}$ \\
$-1,+2,-3,+4,-5, \ldots,(-1)^{n} n, \ldots$ & none & none & $+\infty$ & $-\infty$ \\
\hline
\end{tabular}
\end{center}

\section*{Nested intervals}
2.22. Prove that to every set of nested intervals $\left[a_{n}, b_{n}\right], n=1,2,3, \ldots$ there corresponds one and only one real number.

By definition of nested intervals, $a_{n+1} \geqq a_{n}, b_{n+1}$, $b_{n} n=1,2,3, \ldots$ and $\lim _{n \rightarrow \infty}\left(a_{n}-b_{n}\right)=0$.

Then $a_{1} \leqq a_{n} \leqq b_{n} \leqq b_{1}$, and the sequences $\left\{a_{n}\right\}$ and $\left\{b_{n}\right\}$ are bounded and, respectively, monotonic increasing and decreasing sequences and so converge to $a$ and $b$.

To show that $a=b$ and thus prove the required result, we note that


\begin{gather*}
b-a=\left(b-b_{n}\right)+\left(b_{n}-a_{n}\right)+\left(a_{n}-a\right)  \tag{1}\\
|b-a| \leqq\left|b-b_{n}\right|+\left|b_{n}-a_{n}\right|+\left|a_{n}-a\right| \tag{2}
\end{gather*}


Now, given any $\epsilon>0$, we can find $N$ such that for all $n>N$


\begin{equation*}
\left|b-b_{n}\right|<\epsilon / 3,\left|b_{n}-a\right|<\epsilon / 3 \text {, } \tag{3}
\end{equation*}


so that from Equation (2), $|b-a|<\epsilon$. Since $\epsilon$ is any positive number, we must have $b-a=0$ or $a=b$.

2.23. Prove the Bolzano-Weierstrass theorem (see Page 7).

Suppose the given bounded infinite set is contained in the finite interval $[a, b]$. Divide this interval into two equal intervals. Then at least one of these, denoted by $\left[a_{1}, b_{1}\right]$, contains infinitely many points. Dividing $\left[a_{1}, b_{1}\right]$ into two equal intervals, we obtain another interval-say, $\left[a_{2}, b_{2}\right]$-containing infinitely many points. Continuing this process, we obtain a set of intervals $\left[a_{n}, b_{n}\right], n=1,2,3, \ldots$, each interval contained in the preceding one and such that

$$
b_{1}-a_{1}=(b-a) / 2, b_{2}-a_{2}=\left(b_{1}-a_{1}\right) / 2=(b-a) / 2^{2}, \ldots, b_{n}-a_{n}=(b-a) / 2^{n}
$$

from which we see that $\lim _{n \rightarrow \infty}\left(b_{n}-a_{n}\right)=0$.

This set of nested intervals, by Problem 2.22, corresponds to a real number which represents a limit point and so proves the theorem.

\section*{Cauchy's convergence criterion}
2.24. Prove Cauchy's convergence criterion as stated on Page 27.

Necessity. Suppose the sequence $\left\{u_{n}\right\}$ converges to $l$. Then, given any $\epsilon>0$, we can find $N$ such that

$$
\left|u_{p}-l\right|<\epsilon / 2 \text { for all } p>N \text { and }\left|u_{q}-l\right|<\epsilon / 2 \text { for all } q>N
$$

Then, for both $p>N$ and $q>N$, we have

$$
\left|u_{p}-u_{q}\right|=\left|\left(u_{p}-l\right)+\left(l-u_{q}\right)\right| \leqq\left|u_{p}-l\right|+\left|l-u_{q}\right|<\epsilon / 2+\epsilon / 2=\epsilon
$$

Sufficiency. Suppose $\left|u_{p}-u_{q}\right|<\epsilon$ for all $p, q>N$ and any $\epsilon>0$. Then all the numbers $u_{N}, u_{N+1}, \ldots$ lie in a finite interval; i.e., the set is bounded and infinite. Hence, by the Bolzano-Weierstrass theorem there is at least one limit point—say, $a$.

If $a$ is the only limit point, we have the desired proof and $\lim _{n \rightarrow \infty} u_{n}=a$.

Suppose there are two distinct limit points—say, $a$ and $b$ - and suppose $b>a$ (see Figure 2.1). By definition of limit points, we have

$$
\begin{aligned}
& \left|u_{p}-a\right|<(b-a) / 3 \text { for infinitely many values of } p \\
& \left|u_{q}-b\right|<(b-a) / 3 \text { for infinitely many values of } q
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-046}
\end{center}

Figure 2.1

Then, since $b-a=\left(b-u_{q}\right)+\left(u_{q}-u_{p}\right)+\left(u_{p}-a\right)$, we have


\begin{equation*}
|b-a|=b-a \leqq\left|b-u_{q}\right|+\left|u_{p}-u_{q}\right|+\left|u_{p}-a\right| \tag{3}
\end{equation*}


Using Equations (1) and (2) in (3), we see that $\left|u_{p}-u_{q}\right|>(b-a) / 3$ for infinitely many values of $p$ and $q$, thus contradicting the hypothesis that $\left|u_{p}-u_{q}\right|<\epsilon$ for $p, q>N$ and any $\epsilon>0$. Hence, there is only one limit point and the theorem is proved.

\section*{Infinite series}
2.25. Prove that the infinite series (sometimes called the geometric series)

$$
a+a r+a r^{2}+\cdots=\sum_{n=1}^{\infty} a r^{n-1}
$$

(a) converges to $a /(1-r)$ if $|r|<1$, and (b) diverges if $|r| \geqq 1$.

Let

$$
S_{n}=a+a r+a r^{2}+\cdots+a r^{n-1}
$$

Then

$$
r S_{n}=a r+a r^{2}+\cdots+a r^{n-1}+a r^{n}
$$

Subtract

$$
\begin{array}{rlr}
(1-r) S_{n} & =a & -a r^{n} \\
s_{n} & =\frac{a\left(1-r^{n}\right)}{1-r}
\end{array}
$$

or

(a) If $|r|<1, \lim _{n \rightarrow \infty} S_{n}=\lim _{n \rightarrow \infty} \frac{a\left(1-r^{n}\right)}{1-r}=\frac{a}{1-r}$ by Problem 2.7.

(b) If $|r|>1, \lim _{n \rightarrow \infty} S_{n}$ does not exist (see Problem 2.44).

2.26. Prove that if a series converges, its $n$th term must necessarily approach zero.

Since $S_{n}=u_{1}+u_{2}+\cdots+u_{n}$ and $S_{n-1}=u_{1}+u_{2}+\cdots+u_{n-1}$, we have $u_{n}=S_{n}-S_{n-1}$.

If the series converges to $S$, then

$$
\lim _{n \rightarrow \infty} u_{n}=\lim _{n \rightarrow \infty}\left(S_{n}-S_{n-1}\right)=\lim _{n \rightarrow \infty} S_{n}-\lim _{n \rightarrow \infty} S_{n-1}=S-S=0
$$

2.27. Prove that the series $1-1+1-1+1-1+\cdots=\sum_{n=1}^{\infty}(-1)^{n-1}$ diverges. Method 1: $\quad \lim _{n \rightarrow \infty}(-1)^{n} \neq 0$; in fact, it doesn't exist. Then by Problem 2.26, the series cannot converge; i.e.,\\
it diverges.

Method 2: The sequence of partial sums is $1,1-1,1-1+1,1-1+1-1$, . .; i.e., 1, $0,1,0,1,0$, $1, \ldots$ Since this sequence has no limit, the series diverges.

\section*{Miscellaneous problems}
2.28. If $\lim _{n \rightarrow \infty} u_{n}=l$, prove that $\lim _{n \rightarrow \infty} \frac{u_{1}+u_{2}+\cdots+u_{n}}{n}=l$.

Let $u_{n}=v_{n}+l$. We must show that $\lim _{n \rightarrow \infty} \frac{v_{1}+v_{2}+\cdots+v_{n}}{n}=0$ if $\lim _{n \rightarrow \infty} v_{n}=0$. Now

$$
\frac{v_{1}+v_{2}+\cdots+v_{n}}{n}=\frac{v_{1}+v_{2}+\cdots+v_{p}}{n}+\frac{v_{p+1}+v_{p+2}+\cdots v_{n}}{n}
$$

so that


\begin{equation*}
\left|\frac{v_{1}+v_{2}+\cdots+v_{n}}{n}\right| \leqq \frac{\left|v_{1}+v_{2}+\cdots+v_{p}\right|}{n}+\frac{\left|v_{P+1}\right|+\left|v_{P+2}\right|+\cdots+\left|v_{n}\right|}{n} \tag{1}
\end{equation*}


Since $\lim _{n \rightarrow \infty} v_{n}=0$, we can choose $P$ so that $\left|v_{n}\right|<\epsilon / 2$ for $n>P$. Then


\begin{equation*}
\frac{\left|v_{P+1}\right|+\left|v_{P+2}\right|+\cdots+\left|v_{n}\right|}{n}<\frac{\varepsilon / 2+\varepsilon / 2+\cdots+\varepsilon / 2}{n}=\frac{(n-P) \varepsilon / 2}{n}<\frac{\varepsilon}{2} \tag{2}
\end{equation*}


After choosing $P$, we can choose $N$ so that for $n>N>P$,


\begin{equation*}
\frac{\left|v_{1}+v_{2}+\cdots+v_{P}\right|}{n}<\frac{\varepsilon}{2} \tag{3}
\end{equation*}


Then, using Equations (2) and (3), (1) becomes

$$
\left|\frac{v_{1}+v_{2}+\cdots+v_{n}}{n}\right|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon \quad \text { for } n>N
$$

thus proving the required result.

2.29. Prove that $\lim _{n \rightarrow \infty}\left(1+n+n^{2}\right)^{1 / n}=1$.

Let $\left(1+n+n^{2}\right)^{1 / n}=1+u_{n}$, where $u_{n} \geqq 0$. Now, by the binomial theorem,

$$
1+n+n^{2}=\left(1+u_{n}\right)^{n}=1+n u_{n}+\frac{n(n-1)}{2 !} u_{n}^{2}+\frac{n(n-1)(n-2)}{3 !} u_{n}^{3}+\cdots+u_{n}^{n}
$$

Then $1+n+n^{2}>1+\frac{n(n-1)(n-2)}{3 !} u_{n}^{3}$ or $0<u_{n}^{3}<\frac{6\left(n^{2}+n\right)}{n(n-1)(n-2)}$. Hence, $\lim _{n \rightarrow \infty} u_{n}^{3}=0$ and

$\lim _{n \rightarrow \infty} u_{n}=0$. Thus, $\lim _{n \rightarrow \infty}\left(1+n+n^{2}\right)^{1 / n}=\lim _{n \rightarrow \infty}\left(1+u_{n}\right)=1$.

2.30. Prove that $\lim _{n \rightarrow \infty} \frac{a^{n}}{n !}=0$ for all constants $a$.

The result follows if we can prove that $\lim _{n \rightarrow \infty} \frac{|a|^{n}}{n !}=0$ (see Problem 2.38). We can assume $a \neq 0$.

Let $u_{n}=\frac{|a|^{n}}{n !}$. Then $\frac{u_{n}}{u_{n-1}}=\frac{|a|}{n}$. If $n$ is large enough—say, $n>2|a|$-and if we call $N=[2|a|+1]$, i.e., the greatest integer $\leqq 2|a|+1$, then

$$
\frac{u_{N+1}}{u_{N}}<\frac{1}{2}, \frac{u_{N+2}}{u_{N+1}}<\frac{1}{2}, \ldots, \frac{u_{n}}{u_{n-1}}<\frac{1}{2}
$$

Multiplying these inequalities yields $\frac{u_{n}}{u_{N}}<\left(\frac{1}{2}\right)^{n-N}$ or $u_{n}<\left(\frac{1}{2}\right)^{n-N} u_{N}$. Since $\lim _{n \rightarrow \infty}\left(\frac{1}{2}\right)^{n-N}=0$ (using

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Sequences}
2.31. Write the first four terms of each of the following sequences:\\
(a) $\left\{\frac{\sqrt{n}}{n+1}\right\}$\\
(d) $\left\{\frac{(-1)^{n} x^{2 n-1}}{1 \cdot 3 \cdot 5 \cdots(2 n-1)}\right\}$\\
(b) $\left\{\frac{(-1)^{n+1}}{n !}\right\}$\\
(e) $\left\{\frac{\cos n x}{x^{2}+n^{2}}\right\}$\\
(c) $\left\{\frac{(2 x)^{n-1}}{(2 n-1)^{5}}\right\}$\\
Ans. (a) $\frac{\sqrt{1}}{2}, \frac{\sqrt{2}}{3}, \frac{\sqrt{3}}{4}, \frac{\sqrt{4}}{5}$\\
(d) $\frac{-x}{1}, \frac{x^{3}}{1 \cdot 3}, \frac{-x^{5}}{1 \cdot 3 \cdot 5}, \frac{x^{7}}{1 \cdot 3 \cdot 5 \cdot 7}$\\
(b) $\frac{1}{1 !},-\frac{1}{2 !}, \frac{1}{3 !},-\frac{1}{4 !}$\\
(e) $\frac{\cos x}{x^{2}+1^{2}}, \frac{\cos 2 x}{x^{2}+2^{2}}, \frac{\cos 3 x}{x^{2}+3^{2}}, \frac{\cos 4 x}{x^{2}+4^{2}}$\\
(c) $\frac{1}{1^{5}}, \frac{2 x}{3^{5}}, \frac{4 x^{2}}{5^{5}}, \frac{8 x^{3}}{7^{5}}$

2.32. Find a possible $n$th term for the sequences whose first 5 terms are indicated, and find the 6th term:\\
(a) $\frac{-1}{5}, \frac{3}{8}, \frac{-5}{11}, \frac{7}{14}, \frac{-9}{17}, \cdots$\\
(b) $1,0,1,0,1, \cdots$\\
(c) $\frac{2}{3}, 0, \frac{3}{4}, 0, \frac{4}{5}, \cdots$\\
Ans. (a) $\frac{(-1)^{n}(2 n-1)}{(3 n+2)}$\\
(b) $\frac{1-(-1)^{n}}{2}$\\
(c) $\frac{(n+3)}{(n+5)} \cdot \frac{1-(-1)^{n}}{2}$

2.33. The Fibonacci sequence is the sequence $\left\{u_{n}\right\}$ where $u_{n+2}=u_{n+1}+u_{n}$ and $u_{n}$ and $u_{1}=1, u_{2}=1$. (a) Find the first 6 terms of the sequence.(b) Show that the $n$th term is given by $u_{n}=\left(a^{n}-b^{n}\right) / \sqrt{5}$, where $a=\frac{1}{2}(1+$ $\sqrt{5}$ ) and $b=\frac{1}{2}(1-\sqrt{5})$.

Ans. (a) $1,1,2,3,5,8$

\section*{Limits of sequences}
2.34. Using the definition of limit, prove that\\
(a) $\lim _{n \rightarrow \infty} \frac{4-2 n}{3 n+2}=\frac{-2}{3}$\\
(b) $\lim _{n \rightarrow \infty} 2^{-1 / \sqrt{n}}=1$\\
(c) $\lim _{n \rightarrow \infty} \frac{n^{4}+1}{n^{2}}=\infty$\\
(d) $\lim _{n \rightarrow \infty} \frac{\sin N}{n}=0$

2.35. Find the least positive integer $N$ such that $|(3 n+2) /(n-1)-3|<\epsilon$ for all $n>N$ if (a) $\epsilon=.01$, (b) $\epsilon=.001$ and (c) $\epsilon=.0001$.\\
Ans. (a) 502\\
(b) 5002\\
(c) 50,002

2.36. Using the definition of limit, prove that $\lim _{n \rightarrow \infty}(2 n-1) /(3 n+4)$ cannot be $\frac{1}{2}$.

2.37. Prove that $\lim _{n \rightarrow \infty}(-1)^{n} n$ does not exist.

2.38. Prove that if $\lim _{n \rightarrow \infty}\left|u_{n}\right|=0$, then $\lim _{n \rightarrow \infty} u_{n}=0$. Is the converse true?

2.39. If $\lim _{n \rightarrow \infty} u_{n}=l$, prove that (a) $\lim _{n \rightarrow \infty}=c u_{n}=c l$ where $c$ is any constant, (b) $\lim _{n \rightarrow \infty} u^{2}{ }_{n}=l^{2}$, (c) $\lim _{n \rightarrow \infty} u^{p}{ }_{n}=l^{p}$ where $p$ is a positive integer, and (d) $\lim _{n \rightarrow \infty} \sqrt{u_{n}}=\sqrt{l}, l \geqq 0$.

2.40. Give a direct proof that $\lim _{n \rightarrow \infty} a_{n} / b_{n}=A / B$ if $\lim _{n \rightarrow \infty} a_{n}=A$ and $\lim _{n \rightarrow \infty} b_{n}=B \neq 0$.

2.41. Prove that (a) $\lim _{n \rightarrow \infty} 3^{1 / n}=1$, (b) $\lim _{n \rightarrow \infty}\left(\frac{2}{3}\right)^{1 / n}=1$ and (c) $\lim _{n \rightarrow \infty}\left(\frac{3}{4}\right)^{n}=0$.

2.42. If $r>1$, prove that $\lim _{n \rightarrow \infty} r^{n}=\infty$, carefully explaining the significance of this statement.

2.43. If $|r|>1$, prove that $\lim _{n \rightarrow \infty} r^{n}$ does not exist.

2.44. Evaluate each of the following, using theorems on limits:\\
(a) $\lim _{n \rightarrow \infty} \frac{4-2 n-3 n^{2}}{2 n^{2}+n}$\\
(d) $\lim _{n \rightarrow \infty} \frac{4 \cdot 10^{n}-3 \cdot 10^{2 n}}{3 \cdot 10^{n-1}+2 \cdot 10^{2 n-1}}$\\
(b) $\lim _{n \rightarrow \infty} \sqrt[3]{\frac{(3-\sqrt{n})(\sqrt{n}+2)}{8 n-4}}$\\
(e) $\lim _{n \rightarrow \infty}\left(\sqrt{n^{2}+n-n}\right)$\\
(c) $\lim _{n \rightarrow \infty} \frac{\sqrt{3 n^{2}-5 n+4}}{2 n-7}$\\
(f) $\lim _{n \rightarrow \infty}\left(2^{n}+3^{n}\right)^{1 / n}$\\
Ans. (a) $-3 / 2$\\
(b) $-1 / 2$\\
(c) $\sqrt{3} / 2$\\
(d) -15\\
(e) $1 / 2$\\
(f) 3

\section*{Bounded monotonic sequences}
2.45. Prove that the sequence with $n$th term $u_{n}=u_{n}=\sqrt{n} /(n+1)$. (a) is monotonic decreasing, (b) is bounded below, (c) is bounded above, and (d) has a limit.

2.46. If $u_{n}=\frac{1}{1+n}+\frac{1}{2+n}+\frac{1}{3+n}+\cdots+\frac{1}{n+n}$, prove that $\lim _{n \rightarrow \infty} u_{n}$ exists and lies between 0 and 1 .

2.47. If $u_{n}=\sqrt{u_{n}+1}, u_{1}=1$, prove that $\lim _{n \rightarrow \infty} u=\frac{1}{2}(1+\sqrt{5})$.

2.48. If $u_{n+1}=\frac{1}{2}\left(u_{n}+p / u_{n}\right)$ where $p>0$ and $u_{1}>0$, prove that $\lim _{n \rightarrow \infty} u_{n}=\sqrt{p}$. Show how this can be used to determine $\sqrt{2}$.

2.49. If $u_{n}$ is monotonic increasing (or monotonic decreasing), prove that $S_{n} / n$, where $S_{n}=u_{1}+u_{2}+\cdots+u_{n}$ is also monotonic increasing (or monotonic decreasing).

\section*{Least upper bound, greatest lower bound, limit superior, limit inferior}
2.50. Find the l.u.b., g.l.b., $\lim \sup (\overline{\lim })$, and $\lim \inf (\lim )$ for each sequence:\\
(a) $-1, \frac{1}{3},-\frac{1}{5}, \frac{1}{7}, \ldots,(-1)^{n} /(2 n-1), \ldots$\\
(c) $1,-3,5,-7, \ldots,(-1)^{n-1}(2 n-1), \ldots$\\
(b) $\frac{2}{3},-\frac{3}{4}, \frac{4}{5},-\frac{5}{6}, \ldots,(-1)^{n+1}(n+1) /(n+2), \ldots$\\
(d) $1,4,1,16,1,36, \ldots, n^{1+(-1) n}, \ldots$

Ans. (a) $\frac{1}{3},-1,0,0$

(b) $1,-1,1,-1$

(c) none, none, $+\infty,-\infty$

(d) none, $1,+\infty, 1$

2.51. Prove that a bounded sequence $\left\{u_{n}\right\}$ is convergent if and only if $\varlimsup u_{n}=\underline{\lim } u_{n}$.

\section*{Infinite series}
2.52. Find the sum of the series $\sum_{n=1}^{\infty}\left(\frac{2}{3}\right)^{n}$.

Ans. 2

2.53. Evaluate $\sum_{n=1}^{\infty}(-1)^{n-1} / 5^{n}$.

Ans. $\frac{1}{6}$

2.54. Prove that $\frac{1}{1 \cdot 2}+\frac{1}{2 \cdot 3}+\frac{1}{3 \cdot 4}+\frac{1}{4 \cdot 5}+\cdots=\sum_{n=1}^{\infty} \frac{1}{n(n+1)}=1$. (Hint: $\frac{1}{n(n+1)}=\frac{1}{n}-\frac{1}{n+1}$.

2.55. Prove that multiplication of each term of an infinite series by a constant (not zero) does not affect the convergence or divergence.

2.56. Prove that the series $1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}+\cdots$ diverges. (Hint: Let $_{n}=1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}$. Then prove that $\left|S_{2 n}-S_{n}\right|>\frac{1}{2}$, giving a contradiction with Cauchy's convergence criterion.)

\section*{Miscellaneous problems}
2.57. If $a_{n} \leqq u_{n} \leqq b_{n}$ for all $n>N$, and $\lim _{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} b_{n}=l$, prove that $\lim _{n \rightarrow \infty} u_{n}=l$.

2.58. If $\lim _{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} b_{n}=0$, and $\theta$ is independent of $n$, prove that $\left.\lim _{n \rightarrow \infty}\left(a_{n} \cos n \theta\right)+b_{n} \sin n \theta\right)=0$. Is the result true when $\theta$ depends on $n$ ?

2.59. Let $u_{n}=\frac{1}{2}\left\{1+(-1)^{n}\right\}, n=1,2,3, \ldots$ If $S_{n}=u_{1}+u_{2}+\cdots+u_{n}$, prove that $\lim _{n \rightarrow \infty} S_{n} / n=\frac{1}{2}$.

2.60. Prove that (a) $\lim _{n \rightarrow \infty} n^{1 / n}$ and (b) $\lim _{n \rightarrow \infty}(a+n)^{p / n}=1$ where $a$ and $p$ are constants.

2.61. If $\lim _{n \rightarrow \infty}\left|u_{n+1} / u_{n}\right|=|a|<1$, prove that $\lim _{n \rightarrow \infty} u_{n}=0$.

2.62. If $|a|<1$, prove that $\lim _{n \rightarrow \infty} n^{p} a^{n}=0$ where the constant $p>0$.

2.63. Prove that $\lim \frac{2^{n} n !}{n^{n}}=0$.

2.64. Prove that $\lim _{n \rightarrow \infty} n \sin 1 / n=1$. (Hint: Let the central angel $\theta$ of a circle be measured in radians.) Geometrically illustrate that $\sin \theta \leq \theta \leq \tan \theta, 0 \leq \theta \leq \pi$.

Let $\theta=1 / n$. Observe that since $n$ is restricted to positive integers, the angle is restricted to the first quadrant.

2.65. If $\left\{u_{n}\right\}$ is the Fibonacci sequence (Problem 2.33), prove that $\lim _{n \rightarrow \infty} u_{n+1} / u_{n}=\frac{1}{2}(1+\sqrt{5})$.

2.66. Prove that the sequence $u_{n}=(1+1 / n)^{n+1}, n=1,2,3, \ldots$ is a monotonic decreasing sequence whose limit is e. (Hint: Show that $u_{n} / u_{n-1} \leqq 1$.)

2.67. If $a_{n} \geqq b_{n}$ for all $n>N$ and $\lim _{n \rightarrow \infty} a_{n}=A, \lim _{n \rightarrow \infty} b_{n}=B$, prove that $A \geqq B$.

2.68. If $\left|u_{n}\right| \leqq \mid v_{n}$ and $\lim _{n \rightarrow \infty} v_{n}=0$, prove that $\lim _{n \rightarrow \infty} u_{n}=0$.

2.69. Prove that $\lim _{n \rightarrow \infty} \frac{1}{n}\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)=0$.

2.70. Prove that $\left[a_{n}, b_{n}\right]$, where $a_{n}=(1+1 / n)^{n}$ and $b_{n}=(1+1 / n)^{n+1}$ is a set of nested intervals defining the number $e$.

2.71. Prove that every bounded monotonic (increasing or decreasing) sequence has a limit.

2.72. Let $\left\{u_{n}\right\}$ be a sequence such that $u_{n+2}=a u_{n+1}+b u_{n}$ where $a$ and $b$ are constants. This is called a secondorder difference equation for $u_{n}$. (a) Assuming a solution of the form $u_{n}=r^{n}$ where $r$ is a constant, prove that $r$ must satisfy the equation $r^{2}-a r-b=0$. (b) Use (a) to show that a solution of the difference equation (called a general solution) is $u_{n}=A r^{n}{ }_{1}+B r^{n}{ }_{2}$, where $A$ and $B$ are arbitrary constants and $r_{1}$ and $r_{2}$ are the two solutions of $r^{2}-a r-b=0$ assumed different. (c) In case $r_{1}=r_{2}$ in $(b)$, show that a (general) solution is $u_{n}=$ $(A+B n) r^{n}$.

2.73. Solve the following difference equations subject to the given conditions: (a) $u_{n+2}=u_{n+1}+u_{n}, u_{1}=1, u_{2}=1$ (compare Problem 2.34); (b) $u_{n+2}=2 u_{n+1}+3 u_{n}, u_{2}=5$; (c) $u_{n+2}=4 u_{n+1}, 4 u_{n}, u_{1}=2, u_{2}=8$.\\
Ans. (a) Same as in Problem 2.34,\\
(b) $u_{n}=2(3)^{n-1}+(-1)^{n-1}$\\
(c) $u_{n}=n \cdot 2^{n}$

This page intentionally left blank

\section*{CHAPTER 3}
\section*{Functions, Limits, and Continuity}
The notions described in this chapter historically followed the introduction of differentiation and integration. These concepts were established, developed, and applied in the 1700s on a strong mechanical basis but a weak theoretical foundation. In the 1800s, the theoretical inadequacies were resolved with the mathematical invention of limits. Precise definitions of derivatives and integrals were formulated. Many mathematicians, including Bolzano, introduced rigorous proofs free of geometry. Elegant notation, such as the $\varepsilon-\delta$ form of Weierstrass, became available. As a bonus, clear definitions of irrational numbers were made. Also, unexpected properties of infinite sets of real numbers were found by Cantor and other mathematicians.

This chapter sets forth the notion of the limit of a function, concepts that followed, and how these ideas made possible the rigorization of analysis.

\section*{Functions}
A function is composed of a domain set, a range set, and a rule of correspondence that assigns exactly one element of the range to each element of the domain.

This definition of a function places no restrictions on the nature of the elements of the two sets. However, in our early exploration of the calculus, these elements are real numbers. The rule of correspondence can take various forms, but in advanced calculus it most often is an equation or a set of equations.

If the elements of the domain and range are represented by $x$ and $y$, respectively, and $f$ symbolizes the function, then the rule of correspondence takes the form $y=f(x)$.

The distinction between $f$ and $f(x)$ should be kept in mind. $f$ denotes the function as defined in the first paragraph. $y$ and $f(x)$ are different symbols for the range (or image) values corresponding to domain values $x$. However, a common practice that provides an expediency in presentation is to read $f(x)$ as "the image of $x$ with respect to the function $f$ " and then use it when referring to the function. (For example, it is simpler to write $\sin x$ than "the sine function, the image value of which is $\sin x$.") This deviation from precise notation appears in the text because of its value in exhibiting the ideas.

The domain variable $x$ is called the independent variable. The variable $y$ representing the corresponding set of values in the range, is the dependent variable.

Note: There is nothing exclusive about the use of $x, y$, and $f$ to represent domain, range, and function. Many other letters are employed.

There are many ways to relate the elements of two sets. (Not all of them correspond a unique range value to a given domain value.) For example, given the equation $y^{2}=x$, there are two choices of $y$ for each positive value of $x$. As another example, the pairs $(a, b),(a, c),(a, d)$, and $(a, e)$ can be formed, and again the correspondence to a domain value is not unique. Because of such possibilities, some texts, especially older ones, distinguish between multiple-valued and single-valued functions. This viewpoint is not consistent with our definition or modern presentations. In order that there be no ambiquity, the calculus and its applications require a single image associated with each domain value. A multiple-valued rule of correspondence gives rise to a collection of functions (i.e., single-valued). Thus, the rule $y^{2}=x$ is replaced by the pair of rules $y=-x^{1 / 2}$\\
and the functions they generate through the establishment of domains. (See the following section on graphs for pictorial illustrations.)

EXAMPLES. 1. If to each number in $-1 \leqq x \leqq 1$ we associate a number $y$ given by $x^{2}$, then the interval -1 $\leqq x \leqq 1$ is the domain. The rule $y=x^{2}$ generates the range $-1 \leqq y \leqq 1$. The totality is a function $f$.

The functional image of $x$ is given by $y=f(x)=x^{2}$. For example, $f\left(-\frac{1}{3}\right)=\left(-\frac{1}{3}\right)^{2}=\frac{1}{9}$ is the image of $-\frac{1}{3}$ with respect to the function $f$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The sequences of Chapter 2 may be interpreted as functions. For infinite sequences, consider the domain as the set of positive integers. The rule is the definition of $u_{n}$, and the range is generated by this rule. To illustrate, let $u_{n}=\frac{1}{n}$ with $n=1,2, \ldots$ Then the range contains the elements $1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \ldots$ If the function is denoted by $f$, then we may write $f(n)=\frac{1}{n}$.
\end{enumerate}

As you read this chapter, reviewing Chapter 2 will be very useful.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item With each time $t$ after the year 1800 we can associate a value $P$ for the population of the United States. The correspondence between $P$ and $t$ defines a function-say, $F$ - and we can write $P=F(t)$.

  \item For the present, both the domain and the range of a function have been restricted to sets of real numbers. Eventually this limitation will be removed. To get the flavor for greater generality, think of a map of the world on a globe with circles of latitude and longitude as coordinate curves. Assume there is a rule that corresponds this domain to a range that is a region of a plane endowed with a rectangular Cartesian coordinate system. (Thus, a flat map usable for navigation and other purposes is created.) The points of the domain are expressed as pairs of numbers $(\theta, \phi)$, and those of the range by pairs $(x, y)$. These sets and a rule of correspondence constitute a function whose independent and dependent variables are not single real numbers; rather, they are pairs of real numbers.

\end{enumerate}

\section*{Graph of a Function}
A function $f$ establishes a set of ordered pairs $(x, y)$ of real numbers. The plot of these pairs $[x, f(x)]$ in a coordinate system is the graph of $f$. The result can be thought of as a pictorial representation of the function.

For example, the graphs of the functions described by $y=x^{2},-1 \leqq x \leqq 1$, and $y^{2}=x, 0 \leqq x \leqq 1, y \geqq 0$ appear in Figure 3.1.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-055(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-055}
\end{center}

(b)

Figure 3.1

\section*{Bounded Functions}
If there is a constant $M$ such that $f(x) \leqq M$ for all $x$ in an interval (or other set of numbers), we say that $f$ is bounded above in the interval (or the set) and call $M$ an upper bound of the function.

If a constant $m$ exists such that $f(x) \geq m$ for all $x$ in an interval, we say that $f(x)$ is bounded below in the interval and call $m$ a lower bound.

If $m \leqq f(x) \leqq M$ in an interval, we call $f(x)$ bounded. Frequently, when we wish to indicate that a function is bounded, we write $|f(x)|<P$.

EXAMPLES. 1. $f(x)=3+x$ is bounded in $-1 \leqq x \leqq 1$. An upper bound is 4 (or any number greater than 4). A lower bound is 2 (or any number less than 2 ).\\
2. $f(x)=1 / x$ is not bounded in $0<x<4$, since, by choosing $x$ sufficiently close to zero, $f(x)$ can be made as large as we wish, so that there is no upper bound. However, a lower bound is given by $\frac{1}{4}$ (or any number less than $\frac{1}{4}$ ).

If $f(x)$ has an upper bound, it has a least upper bound (1.u.b.); if it has a lower bound, it has a greatest lower bound (g.l.b.). (See Chapter 1 for these definitions.)

\section*{Monotonic Functions}
A function is called monotonic increasing in an interval if for any two points $x_{1}$ and $x_{2}$ in the interval $x_{1}<x_{2}$, $f\left(x_{1}\right) \leqq f\left(x_{2}\right)$. If $\left(f\left(x_{1}\right)<f\left(x_{2}\right)\right.$, the function is called strictly increasing.

Similarly, if $f\left(x_{1}\right) \geqq f\left(x_{2}\right)$ whenever $x_{1}<x_{2}$, then $f(x)$ is monotonic decreasing, while if $f\left(x_{1}\right)>f\left(x_{2}\right)$, it is strictly decreasing.

\section*{Inverse Functions, Principal Values}
Suppose $y$ is the range variable of a function $f$ with domain variable $x$. Furthermore, let the correspondence between the domain and range values be one-to-one. Then a new function $f^{-1}$, called the inverse function of $f$, can be created by interchanging the domain and range of $f$. This information is contained in the form $x=f^{-1}(y)$.

As you work with the inverse function, it often is convenient to rename the domain variable as $x$ and use $y$ to symbolize the images; then the notation is $y=f^{-1}(x)$. In particular, this allows graphical expression of the inverse function with its domain on the horizontal axis.

Note: $f^{-1}$ does not mean $f$ to the negative one power. When used with functions, the notation $f^{-1}$ always designates the inverse function to $f$.

If the domain and range elements of $f$ are not in one-to-one correspondence (this would mean that distinct domain elements have the same image), then a collection of one-to-one functions may be created. Each of them is called a branch. It is often convenient to choose one of these branches, called the principal branch, and denote it as the inverse function $f^{-1}$. The range values of $f$ that compose the principal branch, and hence the domain of $f^{-1}$, are called the principal values. (As will be seen in the section on elementary functions, it is common practice to specify these principal values for that class of functions.)

EXAMPLE. Suppose $f$ is generated by $y=\sin x$ and the domain is $-\infty \leqq x \leqq \infty$. Then there are an infinite number of domain values that have the same image. (A finite portion of the graph is illustrated in Figure 3.2(a). In Figure 3.2(b) the graph is rotated about a line at $45^{\circ}$ so that the $x$ axis rotates into the $y$ axis. Then the variables are interchanged so that the $x$ axis is once again the horizontal one. We see that the image of an $x$ value is not unique. Therefore, a set of principal values must be chosen to establish an inverse function. A choice of a

branch is accomplished by restricting the domain of the starting function, $\sin x$. For example, choose $-\frac{\pi}{2} \leqq$ $x \leqq \frac{\pi}{2}$. Then there is a one-to-one correspondence between the elements of this domain and the images in\\
$-1 \leqq x \leqq 1$. Thus, $f^{-1}$ may be defined with this interval as its domain. This idea is illustrated in Figure 3.2(c) and $(d)$. With the domain of $f^{-1}$ represented on the horizontal axis and by the variable $x$, we write $y=\sin ^{-1} x$, $-1 \leqq x \leqq 1$.

If $x=-\frac{1}{2}$, then the corresponding range value is $y=-\frac{\pi}{6}$.

Note: In algebra, $b^{-1}$ means $\frac{1}{b}$ and the fact that $b b^{-1}$ produces the identity element 1 is simply a rule of algebra generalized from arithmetic. Use of a similar exponential notation for inverse functions is justified in that corresponding algebraic characteristics are displayed by $f^{-1}[f(x)]=x$ and $f\left[f^{-1}(x)\right]=x$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-057}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-057(1)}
\end{center}

(c)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-057(3)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-057(2)}
\end{center}

(d)

Figure 3.2

\section*{Maxima and Minima}
The seventeenth-century development of the calculus was strongly motivated by questions concerning extreme values of functions. Of most importance to the calculus and its applications were the notions of local extrema, called the relative maximum and relative minimum.

If the graph of a function were compared to a path over hills and through valleys, the local extrema would be the high and low points along the way. This intuitive view is given mathematical precision by the following definition.

Definition If there exists an open interval $(a, b)$ containing $c$ such that $f(x)<f(c)$ for all $x$ other than $c$ in the interval, then $f(c)$ is a relative maximum of $f$. If $f(x)>f(c)$ for all $x$ in $(a, b)$ other than $c$, then $f(c)$ is a relative minimum of $f$. (See Figure 3.3.)

Functions may have any number of relative extrema. On the other hand, they may have none, as in the case of the strictly increasing and decreasing functions previously defined.

Definition If $c$ is in the domain of $f$ and for all $x$ in the domain of the function $f(x) \leq f(c)$; then $f(c)$ is an absolute maximum of the function $f$. If for all $x$ in the domain $f(x) \geq f(c)$, then $f(c)$ is absolute minimum of $f$. (See Figure 3.3.)

Note: If defined on closed intervals, the strictly increasing and decreasing functions possess absolute extrema.

Absolute extrema are not necessarily unique. For example, if the graph of a function is a horizontal line, then every point is an absolute maximum and an absolute minimum.

Note: A point of inflection is also represented in Figure 3.3. There is an overlap with relative extrema in representation of such points through derivatives that will be addressed in the problem set of Chapter 4.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-058}
\end{center}

Figure 3.3

\section*{Types of Functions}
It is worth realizing that there is a fundamental pool of functions at the foundation of calculus and advanced calculus. These are called elementary functions. Either they are generated from a real variable $x$ by the fundamental operations of algebra, including powers and roots, or they have relatively simple geometric interpretations. As the title "elementary functions" suggests, there is a more general category of functions (which, in fact, are dependent on the elementary ones). Some of these will be explored later in this book. The elementary functions are described as follows.

\begin{enumerate}
  \item Polynomial functions have the form
\end{enumerate}


\begin{equation*}
f(x)=a_{0} x^{n}+a_{1} x^{n-1}+\cdots+a_{n-1} x+a_{n} \tag{1}
\end{equation*}


where $a_{0}, \ldots, a_{n}$ are constants and $n$ is a positive integer called the degree of the polynomial if $a_{0} \neq 0$.

The fundamental theorem of algebra states that in the field of complex numbers every polynomial equation has at least one root. As a consequence of this theorem, it can be proved that every $n$ th-degree polynomial has $n$ roots in the complex field. When complex numbers are admitted, the polynomial theoretically may be expressed as the product of $n$ linear factors; with our restriction to real numbers, it is possible that $2 k$ of the roots may be complex. In this case, the $k$ factors generating them will be quadratic. (The corresponding roots are in complex conjugate pairs.) The polynomial $x^{3}-5 x^{2}+11 x$ $-15=(x-3)\left(x^{2}-2 x+5\right)$ illustrates this thought.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Algebraic functions are functions $y=f(x)$ satisfying an equation of the form
\end{enumerate}


\begin{equation*}
p_{0}(x) y^{n}+p_{1}(x) y^{n-1}+\cdots+p_{n-1}(x) y+p_{n}(x)=0 \tag{2}
\end{equation*}


where $p_{0}(x), \ldots, p_{n}(x)$ are polynomials in $x$.

If the function can be expressed as the quotient of two polynomials, i.e., $P(x) / Q(x)$ where $P(x)$ and $Q(x)$ are polynomials, it is called a rational algebraic function; otherwise, it is an irrational algebraic function.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Transcendental functions are functions which are not algebraic; i.e., they do not satisfy equations of the form of Equation (2).
\end{enumerate}

Note the analogy with real numbers, polynomials corresponding to integers, rational functions to rational numbers, and so on.

\section*{Transcendental Functions}
The following are sometimes called elementary transcendental functions.

\begin{enumerate}
  \item Exponential function: $f(x)=a^{x}, a \neq 0,1$. For properties, see Page 4.

  \item Logarithmic function: $f(x)=\log _{a} x, a \neq 0,1$. This and the exponential function are inverse functions. If $a=e=2.71828 \ldots$, called the natural base of logarithms, we write $f(x)=\log _{e} x=\operatorname{In} x$, called the natural logarithm of $x$. For properties, see Page 4.

  \item Trigonometric functions (also called circular functions because of their geometric interpretation with respect to the unit circle):

\end{enumerate}

$$
\sin x, \cos x, \tan x=\frac{\sin x}{\cos x}, \csc x=\frac{1}{\sin x}, \sec x=\frac{1}{\cos x}, \cot x=\frac{1}{\tan x}=\frac{\cos x}{\sin x}
$$

The variable $x$ is generally expressed in radians $\left(\pi\right.$ radians $\left.=180^{\circ}\right)$. For real values of $x, \sin x$ and $\cos x$ lie between -1 and 1 inclusive.

The following are some properties of these functions:

$$
\begin{array}{ll}
\sin ^{2} x+\cos ^{2} x=1 \quad 1+\tan ^{2} x=\operatorname{sce}^{2} x & 1+\cot ^{2} x=\csc ^{2} x \\
\sin (x \pm y)=\sin x \cos y \pm \cos x \sin y & \sin (-x)=-\sin x \\
\cos (x \pm y)=\cos x \cos y \mp \sin x \sin y & \cos (-x)=\cos x \\
\tan (x \pm y)=\frac{\tan x \pm \tan y}{1 \mp \tan x \tan y} & \tan (-x)=-\tan x
\end{array}
$$

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Inverse trigonometric functions. The following is a list of the inverse trigonometric functions and their principal values:\\
(a) $y=\sin ^{-1} x,(-\pi / 2 \leqq y \leqq \pi / 2)$\\
(d) $y=\csc ^{-1} x=\sin ^{-1} 1 / x,(-\pi / 2 \leqq y \leqq \pi / 2)$\\
(b) $y=\cos ^{-1} x,(0 \leqq y \leqq \pi)$\\
(e) $y=\sec ^{-1} x=\cos ^{-1} 1 / x,(0 \leqq y \leqq \pi)$\\
(c) $y=\tan ^{-1} x,(-\pi / 2<y<\pi / 2)$\\
(f) $y=\cot ^{-1} x=\pi / 2-\tan ^{-1} x,(0<y<\pi)$

  \item Hyperbolic functions are defined in terms of exponential functions as follows. These functions may be interpreted geometrically, much as the trigonometric functions but with respect to the unit hyperbola.\\
(a) $\sinh x=\frac{e^{x}-e^{-x}}{2}$\\
(d) $\operatorname{csch} x=\frac{1}{\sinh x}=\frac{2}{e^{x}-e^{-x}}$\\
(b) $\cosh x=\frac{e^{x}+e^{-x}}{2}$\\
(e) $\operatorname{sech} x=\frac{1}{\cosh x}=\frac{2}{e^{x}+e^{-x}}$\\
(c) $\tanh x=\frac{\sinh x}{\cosh x}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\\
(f) $\operatorname{coth} x=\frac{\cosh x}{\sinh x}=\frac{e^{x}+e^{-x}}{e^{x}-e^{-x}}$

\end{enumerate}

The following are some properties of these functions:

$$
\begin{array}{ll}
\cosh ^{2} x-\sinh ^{2} x=1 \quad 1-\tanh ^{2} x=\operatorname{sech}^{2} x & \operatorname{coth}^{2} x-1=\operatorname{csch}^{2} x \\
\sinh (x \pm y)=\sinh x \cosh y \pm \cosh x \sinh y & \sinh (-x)=-\sinh x \\
\cosh (x \pm y)=\cosh x \cosh y \pm \sinh x \sinh y & \cosh (-x)=\cosh x \\
\tanh (x \pm y)=\frac{\tanh x \pm \tanh y}{1 \pm \tanh x \tanh y} & \tanh (-x)=-\tanh x
\end{array}
$$

\begin{enumerate}
  \setcounter{enumi}{5}
  \item Inverse hyperbolic functions. If $x=\sinh y$, then $y=\sinh ^{-1} x$ is the inverse hyperbolic sine of $x$. The following list gives the principal values of the inverse hyperbolic functions in terms of natural logarithms and the domains for which they are real.\\
(a) $\sinh ^{-1} x=\ln \left(x+\sqrt{x^{2}+1}\right)$, all $x$\\
(d) $\operatorname{csch}^{-1} x=\ln \left(\frac{1}{\mathrm{x}}+\frac{\sqrt{x^{2}+1}}{|x|}\right), x \neq 0$\\
(b) $\cosh ^{-1} x=\ln \left(x+\sqrt{x^{2}-1}\right), x \geqq 1$\\
(e) $\operatorname{sech}^{-1} x=\ln \left(\frac{1+\sqrt{1-x^{2}}}{x}\right), 0<x \leqq 1$\\
(c) $\tanh ^{-1} x=\frac{1}{2} \ln \left(\frac{1+x}{1-x}\right),|x|<1$\\
(f) $\operatorname{coth}^{-1} x=\frac{1}{2} \ln \left(\frac{x+1}{x-1}\right),|x|>1$
\end{enumerate}

\section*{Limits of Functions}
Let $f(x)$ be defined and single-valued for all values of $x$ near $x=x_{0}$ with the possible exception of $x=x_{0}$ itself (i.e., in a deleted $\delta$ neighborhood of $x_{0}$ ). We say that the number $l$ is the limit of $f(x)$ as $x$ approaches $x_{0}$ and write $\lim _{x \rightarrow x_{0}} f(x)=l$ if for any positive number $\epsilon$ (however small) we can find some positive number $\delta$ (usually depending on $\epsilon$ ) such that $|f(x)-l|<\epsilon$ whenever $0<\left|x-x_{0}\right|<\delta$. In such a case we also say that $f(x)$ approaches $l$ as $x$ approaches $x_{0}$ and write $f(x) \rightarrow l$ as $x \rightarrow x_{0}$.

In words, this means that we can make $f(x)$ arbitrarily close to $l$ by choosing $x$ sufficiently close to $x_{0}$.

EXAMPLE. Let $f(x)=\left\lvert\, \begin{aligned} & x^{2} \text { if } x \neq 2 \\ & 0 \text { if } x=2\end{aligned}\right.$. Then as $x$ gets closer to 2 (i.e., $x$ approaches 2), $f(x)$ gets closer to 4 . We thus suspect that $\lim _{x \rightarrow 2} f(x)=4$. To prove this we must see whether the preceding definition of limit (with $l=4$ ) is satisfied. For this proof, see Problem 3.10.

Note that $\lim _{x \rightarrow 2} f(x)=f(2)$; i.e., the limit of $f(x)$ as $x \rightarrow 2$ is not the same as the value of $f(x)$ at $x=2$, since $f(2)=0$ by definition. The limit would, in fact, be 4 even if $f(x)$ were not defined at $x=2$.

When the limit of a function exists, it is unique; i.e., it is the only one (see Problem 3.17).

\section*{Right- and Left-Hand Limits}
In the definition of limit, no restriction was made as to how $x$ should approach $x_{0}$. It is sometimes found convenient to restrict this approach. Considering $x$ and $x_{0}$ as points on the real axis where $x_{0}$ is fixed and $x$ is moving, then $x$ can approach $x_{0}$ from the right or from the left. We indicate these respective approaches by writing $x \rightarrow x_{0}+$ and $x \rightarrow x_{0}-$.

If $\lim _{x \rightarrow x_{0}}+f(x)=l_{1}$ and $\lim _{x \rightarrow x_{0^{-}}} f(x)=l_{2}$, we call $l_{1}$ and $l_{12}$, respectively, the right- and left-hand limits of $f$ at $x_{0}$ and denote them by $f\left(x_{0}+\right)$ or $f\left(x_{0}+0\right)$ and $f\left(x_{0}-\right)$ or $f\left(x_{0}-0\right)$. The $\epsilon, \delta$ definitions of limit of $f(x)$ as\\
$x \rightarrow x_{0}+$ or $x \rightarrow x_{0}$ are the same as those for $x \rightarrow x_{0}$ except for the fact that values of $x$ are restricted to $x>$ $x_{0}$ or $x<x_{0}$, respectively.

We have $\lim _{x \rightarrow x_{0}} f(x)=l$ if and only if $\lim _{x \rightarrow x_{0}+} f(x)=\lim _{x \rightarrow x_{0-}} f s(x)=l$.

\section*{Theorems on Limits}
If $\lim _{x \rightarrow x_{0}} f(x)=A$ and $\lim _{x \rightarrow x_{0}} g(x)=B$, then

\begin{enumerate}
  \item $\lim _{x \rightarrow x_{0}}(f(x)+g(x))=\lim _{x \rightarrow x_{0}} f(x)+\lim _{x \rightarrow x_{0}} g(x)=A+B$

  \item $\lim _{x \rightarrow x_{0}}(f(x)-g(x))=\lim _{x \rightarrow x_{0}} f(x)-\lim _{x \rightarrow x_{0}} g(x)=A-B$

  \item $\lim _{x \rightarrow x_{0}}(f(x) g(x))=\left(\lim _{x \rightarrow x_{0}} f(x)\right)\left(\lim _{x \rightarrow x_{0}} g(x)\right)=A B$

  \item $\quad \lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}=\frac{\lim _{x \rightarrow x_{0}} f(x)}{\lim _{x \rightarrow x_{0}} g(x)}=\frac{A}{B} \quad$ if $B \neq 0$

\end{enumerate}

Similar results hold for right- and left-hand limits.

\section*{Infinity}
It sometimes happens that as $x \rightarrow x_{0}, f(x)$ increases or decreases without bound. In such case it is customary to write $\lim _{x \rightarrow x_{0}} f(x)=+\infty$ or $\lim _{x \rightarrow x_{0}} f(x)=-\infty$, respectively. The symbols $+\infty$ (also written $\infty$ ) and $-\infty$ are read "plus infinity" (or "infinity") and "minus infinity," respectively, but it must be emphasized that they are not numbers.

In precise language, we say that $\lim _{x \rightarrow x_{0}} f(x)=\infty$ if for each positive number $M$ we can find a positive number $\delta$ (depending on $M$ in general) such that $f(x)>M$ whenever $0<\left|x-x_{0}\right|<\delta$. Similarly, we say that $\lim _{x \rightarrow x_{0}} f(x)=-\infty$ if for each positive number $M$ we can find a positive number $\delta$ such that $f(x)<-M$ whenever $0<\left|x-x_{0}\right|<\delta$. Analogous remarks apply in case $x \rightarrow x_{0}+$ or $x \rightarrow x_{0}-$

Frequently we wish to examine the behavior of a function as $x$ increases or decreases without bound. In such cases it is customary to write $x \rightarrow+\infty$ (or $\infty$ ) or $x \rightarrow-\infty$, respectively.

We say that $\lim _{x \rightarrow+\infty} f(x)=l$, or $f(x) \rightarrow l$ as $x \rightarrow+\infty$, if for any positive number $\epsilon$ we can find a positive number $N$ (depending on $\epsilon$ in general) such that $|f(x)-l|<\epsilon$ whenever $x>N$. A similar definition can be formulated for $\lim _{x \rightarrow-\infty} f(x)$.

\section*{Special Limits}
\begin{enumerate}
  \item $\lim _{x \rightarrow 0} \frac{\sin x}{x}=1 \quad \lim _{x \rightarrow 0} \frac{1-\cos x}{x}=0$

  \item $\lim _{x \rightarrow \infty}\left(1+\frac{1}{x}\right)^{x}=e \quad \lim _{x \rightarrow 0+}(1+x)^{1 / x}=e$

  \item $\lim _{x \rightarrow 0} \frac{e^{x}-1}{x}=1 \quad \lim _{x \rightarrow 1} \frac{x-1}{\ln x}=1$

\end{enumerate}

\section*{Continuity}
Let $f$ be defined for all values of $x$ near $x=x_{0}$ as well as at $x=x_{0}$ (i.e., in a $\delta$ neighborhood of $x_{0}$ ). The function $f$ is called continuous at $x=x_{0}$ if $\lim _{x \rightarrow x_{0}} f(x)=f\left(x_{0}\right)$. Note that this implies three conditions which must be met in order that $f(x)$ be continuous at $x=x_{0}$ :

\begin{enumerate}
  \item $\lim _{x \rightarrow x_{0}} f(x)=l$ must exist.

  \item $f\left(x_{0}\right)$ must exist; i.e., $f(x)$ is defined at $x_{0}$.

  \item $l=f\left(x_{0}\right)$.

\end{enumerate}

In summary, $\lim _{x \rightarrow x_{0}} f(x)$ is the value suggested for $f$ at $x=x_{0}$ by the behavior of $f$ in arbitrarily small neighborhoods of $x_{0}$. If, in fact, this limit is the actual value, $f\left(x_{0}\right)$, of the function at $x_{0}$, then $f$ is continuous there.

Equivalently, if $f$ is continuous at $x_{0}$, we can write this in the suggestive form $\lim _{x \rightarrow x_{0}} f(x)=f\left(\lim _{x \rightarrow x_{0}} x\right)$.

EXAMPLES. 1. If $f(x)=\left\{\begin{array}{ll}x^{2}, & x \neq 2 \\ 0, & x=2\end{array}\right.$ then from the example on Page $45 \lim _{x \rightarrow 2} f(x)=4$. But $f(2)=0$.

Hence, $\lim _{x \rightarrow 2} f(x) \neq f(2)$ and the function is not continuous at $x=2$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If $f(x)=x^{2}$ for all $x$, then $\lim _{x \rightarrow 2} f(x)=f(2)=4$ and $f(x)$ is continuous at $x=2$.
\end{enumerate}

Points where $f$ fails to be continuous are called discontinuities of $f$ and $f$ is said to be discontinuous at these points.

In constructing a graph of a continuous function, the pencil need never leave the paper, while for a discontinuous function this is not true, since there is generally a jump taking place. This is, of course, merely a characteristic property and not a definition of continuity or discontinuity.

Alternative to the preceding definition of continuity, we can define $f$ as continuous at $x=x_{0}$ if for any $\epsilon>0$ we can find $\delta>0$ such that $\left|f\left(x_{0}\right)-f\left(x_{0}\right)\right|<\epsilon$ whenever $\left|x-x_{0}\right|<\delta$. Note that this is simply the definition of limit with $l=f\left(x_{0}\right)$ and removal of the restriction that $x \neq x_{0}$.

\section*{Right- and Left-Hand Continuity}
If $f$ is defined only for $x \geqq x_{0}$, the preceding definition does not apply. In such case we call $f$ continuous (on the right) at $x=x_{0}$ if $\lim _{x \rightarrow x_{0+}} f(x)=f\left(x_{0}\right)$, i.e., if $f\left(x_{0}+\right)=f\left(x_{0}\right)$. Similarly, $f$ is continuous (on the left) at $x=x_{0}$ if $\lim _{x \rightarrow x_{0-}} f(x)=f(x)_{0}$, i.e., $f\left(x_{0}-\right)=f\left(x_{0}\right)$. Definitions in terms of $\epsilon$ and $\delta$ can be given.

\section*{Continuity in an Interval}
A function $f$ is said to be continuous in an interval if it is continuous at all points of the interval. In particular, if $f$ is defined in the closed interval $a \leqq x \leqq b$ or $[a, b]$, then $f$ is continuous in the interval if and only if $\lim _{x \rightarrow x_{o}}$ $f(x)=f\left(x_{0}\right)$ for $a<x_{0}<b, \lim _{x \rightarrow x_{a+}} f(x)=f(a)$, and $\lim _{x \rightarrow b-} f(x)=f(b)$.

\section*{Theorems on Continuity}
Theorem 1 If $f$ and $g$ are continuous at $x=x_{0}$, so also are the functions whose image values satisfy the relations $f(x)+g(x), f(x)-g(x), f(x) g(x)$, and $\frac{f(x)}{g(x)}$, the last only if $g\left(x_{0}\right) \neq 0$. Similar results hold for\\
continuity in an interval.

Theorem 2 Functions described as follows are continuous in every finite interval: (a) all polynomials; $(b)$ $\sin x$ and $\cos x$; and (c) $a^{x}, a>0$.

Theorem 3 Let the function $f$ be continuous at the domain value $x=x_{0}$. Also suppose that a function $g$, represented by $z=g(y)$, is continuous at $y_{0}$, where $y=f(x)$ (i.e., the range value of $f$ corresponding to $x_{0}$ is a domain value of $g$ ). Then a new function, called a composite function, $f(g)$, represented by $z=g[f(x)]$, may be created which is continuous at its domain point $x=x_{0}$. (One says that a continuous function of a continuous function is continuous.)

Theorem 4 If $f(x)$ is continuous in a closed interval, it is bounded in the interval.

Theorem 5 If $f(x)$ is continuous at $x=x_{0}$ and $f\left(x_{0}\right)>0\left[\right.$ or $f\left(x_{0}\right)<0$ ], there exists an interval about $x=x_{0}$ in which $f(x)>0[$ or $f(x)<0]$.

Theorem 6 If a function $f(x)$ is continuous in an interval and either strictly increasing or strictly decreasing, the inverse function $f^{-1}(x)$ is single-valued, continuous, and either strictly increasing or strictly decreasing.

Theorem 7 If $f(x)$ is continuous in $[a, b]$ and if $f(a)=A$ and $f(b)=B$, then corresponding to any number $C$ between $A$ and $B$ there exists at least one number $c$ in $[a, b]$ such that $f(c)=C$. This is sometimes called the intermediate value theorem.

Theorem 8 If $f(x)$ is continuous in $[a, b]$ and if $f(a)$ and $f(b)$ have opposite signs, there is at least one number $c$ for which $f(c)=0$ where $a<c<b$. This is related to Theorem 7 .

Theorem 9 If $f(x)$ is continuous in a closed interval, then $f(x)$ has a maximum value $M$ for at least one value of $x$ in the interval and a minimum value $m$ for at least one value of $x$ in the interval. Furthermore, $f(x)$ assumes all values between $m$ and $M$ for one or more values of $x$ in the interval.

Theorem 10 If $f(x)$ is continuous in a closed interval and if $M$ and $m$ are, respectively, the least upper bound (1.u.b.) and greatest lower bound (g.l.b.) of $f(x)$, there exists at least one value of $x$ in the interval for which $f(x)=M$ or $f(x)=m$. This is related to Theorem 9 .

\section*{Piecewise Continuity}
A function is called piecewise continuous in an interval $a \leqq x \leqq b$ if the interval can be subdivided into a finite number of intervals in each of which the function is continuous and has finite right- and left-hand limits. Such a function has only a finite number of discontinuities. An example of a function which is piecewise continuous in $a \leqq x \leq b$ is shown graphically in Figure 3.4. This function has discontinuities at $x_{1}, x_{2}, x_{3}$, and $x_{4}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-063}
\end{center}

Figure 3.4

\section*{Uniform Continuity}
Let $f$ be continuous in an interval. Then, by definition, at each point $x_{0}$ of the interval and for any $\epsilon>0$, we can find $\delta>0$ (which will in general depend on both $\epsilon$ and the particular point $x_{0}$ ) such that $\left|f(x)-f\left(x_{0}\right)\right|<$ $\epsilon$ whenever $\left|x-x_{0}\right|<\delta$. If we can find $\delta$ for each $\epsilon$ which holds for all points of the interval (i.e., if $\delta$ depends only on $\epsilon$ and not on $x_{0}$ ), we say that $f$ is uniformly continuous in the interval.

Alternatively, $f$ is uniformly continuous in an interval if for any $\epsilon>0$ we can find $\delta>0$ such that $\mid f\left(x_{1}\right)$ $-f\left(x_{2}\right) \mid<\epsilon$ whenever $\left|x_{1}-x_{2}\right|<\delta$ where $x_{1}$ and $x_{2}$ are any two points in the interval.

Theorem If $f$ is continuous in a closed interval, it is uniformly continuous in the interval.

\section*{SOLVED PROBLEMS}
\section*{Functions}
3.1. Let $f(x)=(x-2)(8-x)$ for $2 \leq x \leq 8$. (a) Find $f(6)$ and $f(-1)$. (b) What is the domain of definition of $f(x)$ ? (c) Find $f(1-2 t)$ and give the domain of definition. (d) Find $f[f(3)], f[f(5)]$. (e) Graph $f(x)$.

(a) $f(6)=(6-2)(8-6)=4 \cdot 2=8$

$f(-1)$ is not defined since $f(x)$ is defined only for $2 \leqq x \leqq 8$.

(b) The set of all $x$ such that $2 \leqq x \leqq 8$.

(c) $f(1-2 t)=\{(1-2 t)-2\}\{8-(1-2 t)\}=-(1+2 t)(7+2 t)$ where $t$ is such that $2 \leqq 1-2 t \leqq 8$; i.e., $-7 / 2 \leq t \leq-1 / 2$.

(d) $f(3)=(3-2)(8-3)=5, f[f(3)]=f(5)=(5-2)(8-5)=9$. $f(5)=9$ so that $f[f(5)]=f(9)$ is not defined.

(e) The following table shows $f(x)$ for various values of $x$.

\begin{center}
\begin{tabular}{llllllllll}
$x$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 2.5 & 7.5 \\
$f(x)$ & 0 & 5 & 8 & 9 & 8 & 5 & 0 & 2.75 & 2.75 \\
\end{tabular}
\end{center}

Plot points $(2,0),(3,5),(4,8),(5,9),(6,8),(7,5),(8,0)$, $(2.5,2.75),(7.5,2.75)$. These points are only a few of the infinitely many points on the required graph shown in the adjoining Figure 3.5. This set of points defines a curve which is part of a parabola.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-064}
\end{center}

Figure 3.5

3.2. Let $g(x)=(x-2)(8-x)$ for $2<x<8$. (a) Discuss the difference between the graph of $g(x)$ and that of $f(x)$ in Problem 3.1. (b) What are the 1.u.b. and g.1.b. of $g(x)$ ? (c) Does $g(x)$ attain its 1.u.b. and g.l.b. for any value of $x$ in the domain of definition? (d) Answer parts (b) and (c) for the function $f(x)$ of Problem 3.1.

(a) The graph of $g(x)$ is the same as that in Problem 3.1 except that the two points $(2,0)$ and 8,0 ) are missing, since $g(x)$ is not defined at $x=2$ and $x=8$.

(b) The 1.u.b. of $g(x)$ is 9. The g.l.b. of $g(x)$ is 0 .

(c) The l.u.b. of $g(x)$ is attained for the value of $x=5$. The g.l.b. of $g(x)$ is not attained, since there is no value of $x$ in the domain of definition such that $g(x)=0$.

(d) As in (b), the l.u.b. of $f(x)$ is 9 and the g.l.b. of $f(x)$ is 0 . The l.u.b. of $f(x)$ is attained for the value $x=5$ and the g.l.b. of $f(x)$ is attained at $x=2$ and $x=8$.

Note that a function, such as $f(x)$, which is continuous in a closed interval attains its l.u.b. and g.l.b. at some point of the interval. However, a function, such as $g(x)$, which is not continuous in a closed interval need not attain its 1.u.b. and g.1.b. See Problem 3.34.

3.3. Let

$$
f(x)= \begin{cases}1, & \text { if } x \text { is a rational number } \\ 0, & \text { if } x \text { is an irrational number }\end{cases}
$$

(a) Find $f\left(\frac{2}{3}\right), f(-5), f(1.41423), f(\sqrt{2})$. (b) Construct a graph of $f(x)$ and explain why it is misleading by\\
itself.

(a) $f\left(\frac{2}{3}\right) \quad=1$ since $\frac{2}{3}$ is a rational number

$f(-5) \quad=1$ since -5 is a rational number

$f(1.41423)=1$ since 1.41423 is a rational number

$f(\sqrt{2}) \quad=0$ since $\sqrt{2}$ is an irrational number

(b) The graph is shown in Figure 3.6. Because the sets of both rational numbers and irrational numbers are dense, the visual impression is that there are two images corresponding to each domain value. In actuality, each domain value has only one corresponding range value.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-065(1)}
\end{center}

Figure 3.6

3.4. Referring to Problem 3.1: (a) Draw the graph with axes interchanged, thus illustrating the two possible choices available for definition of $f^{-1}$. (b) Solve for $x$ in terms of $y$ to determine the equations describing the two branches, and then interchange the variables.

(a) The graph of $y=f(x)$ is shown in Figure 3.5 of Problem 3.1(a). By interchanging the axes (and the variables), we obtain the graphical form of Figure 3.7. This figure illustrates that there are two values of $y$ corresponding to each value of $x$, and, hence, two branches. Either may be employed to define $f^{-1}$.

(b) We have $y=(x-2)(8-x)$ or $x^{2}-10 x+16+y=0$. The solution of this quadratic equation is

$$
x=5 \pm \sqrt{9-y}
$$

After interchanging variables

$$
y=5 \pm \sqrt{9-x}
$$

In Figure 3.7, $A P$ represents $y=5+\sqrt{9-x}$, and $B P$ designates $y=5-\sqrt{9-x}$. Either branch may represent $f^{-1}$.

Note: The point at which the two branches meet is called a branch point.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-065}
\end{center}

Figure 3.7

3.5. (a) Prove that $g(x)=5+\sqrt{9-x}$ is strictly decreasing in $0 \leqq x \leqq 9$. (b) Is it monotonic decreasing in this interval? (c) Does $g(x)$ have a single-valued inverse?

(a) $g(x)$ is strictly decreasing if $g\left(x_{1}\right)>g\left(x_{2}\right)$ whenever $x_{1}<x_{2}$. If $x_{1}<x_{2}$, the $9-x_{1}>9-x_{2}, \sqrt{9-x_{1}}>$ $\sqrt{9-x_{2}}$, and $5+\sqrt{9-x_{1}}>5+\sqrt{9-x_{2}}$, showing that $g(x)$ is strictly decreasing.

(b) Yes, any strictly decreasing function is also monotonic decreasing, since if $g\left(x_{1}\right)>g\left(x_{2}\right)$ it is also true that $g\left(x_{1}\right) \geq g\left(x_{2}\right)$. However, if $g(x)$ is monotonic decreasing, it is not necessarily strictly decreasing.

(c) If $y=5+\sqrt{9-x}$, then $y-5=\sqrt{9-x}$ or, squaring, $x=-16+10 y-y^{2}=(y-2)(8-y)$ and $x$ is a single-valued function of $y$; i.e., the inverse function is single-valued.

In general, any strictly decreasing (or increasing) function has a single-valued inverse (see Theorem 6, Page 52).

The results of this problem can be interpreted graphically using Figure 3.7.

3.6. Construct graphs for the following functions:

$$
\text { (a) } f(x)= \begin{cases}x \sin 1 / x, & x>0 \\ 0, & x=0\end{cases}
$$

$$
\text { (b) } f(x)=[x]=\text { greatest integer } \leqq x
$$

(a) The required graph is shown in Figure 3.8. Since $|x \sin 1 / x| \leqq|x|$, the graph is included between $y=$ $x$ and $y=-x$. Note that $f(x)=0$ when $\sin 1 / x=0$ or $1 / x=, m \pi, m=1,2,3,4, \ldots$, i.e., where $x=1 / \pi, 1 / 2 \pi$, $1 / 3 \pi, \ldots$ The curve oscillates infinitely often between $x=1 / \pi$ and $x=0$.

(b) The required graph is shown in Figure 3.9. If $1 \leqq x<2$, then $[x]=1$. Thus, $[1.8]=1,[\sqrt{2}]=1$, $[1.99999]=1$. However, [2] = 2. Similarly, for $2 \leqq x<3,[x]=2$, etc. Thus, there are jumps at the integers.

The function is sometimes called the staircase function or step function.

3.7. (a) Construct the graph of $f(x)=\tan x$. (b) Construct the graph of some of the infinite number of branches available for a definition of $\tan ^{-1} x$. (c) Show graphically why the relationship of $x$ to $y$ is multivalued. (d) Indicate possible principal values for $\tan ^{-1} x$. (e) Using your choice, evaluate $\tan ^{-1}(-1)$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-066(1)}
\end{center}

Figure 3.8

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-066}
\end{center}

Figure 3.9

(a) The graph of $f(x)=\tan x$ appears in Figure 3.10.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-066(2)}
\end{center}

Figure 3.10

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-066(3)}
\end{center}

Figure 3.11

(b) The required graph is obtained by interchanging the $x$ and $y$ axes in the graph of (a). The result, with axes oriented as usual, appears in Figure 3.11.

(c) In Figure 3.11, any vertical line meets the graph in infinitely many points. Thus, the relation of $y$ to $x$ is multivalued and infinitely many branches are available for the purpose of defining $\tan ^{-1} x$.

(d) To define $\tan ^{-1} x$ as a single-valued function, it is clear from the graph that we can do so only by restricting its value to any of the following: $-\pi / 2<\tan ^{-1} x<\pi / 2, \pi / 2<\tan ^{-1} x<3 \pi / 2$, etc. We agree to take the first as defining the principal value.

Note that no matter which branch is used to define $\tan ^{-1} x$, the resulting function is strictly increasing.

(e) $\tan ^{-1}(-1)=-\pi / 4$ is the only value lying between $-\pi / 2$ and $\pi / 2$; i.e., it is the principal value according to our choice in $(d)$.

3.8. Show that $f(x)=\frac{\sqrt{x}+1}{x+1}, x \neq-1$, describes an irrational algebraic function.

If $y=\frac{\sqrt{x}+1}{x+1}$, then $(x+1) y-1=\sqrt{x}$ or, squaring, $(x+1)^{2} y^{2}-2(x+1) y+1-x=0$, a polynomial equation in $y$ whose coefficients are polynomials in $x$. Thus, $f(x)$ is an algebraic function. However, it is not the quotient of two polynomials, so that it is an irrational algebraic function.

3.9. If $f(x)=\cosh x=\frac{1}{2}\left(e^{x}+e^{-x}\right)$, prove that we can choose, as the principal value of the inverse function,

$\cosh ^{-1} x=\ln \left(x+\sqrt{x^{2}-1}\right), x \geqq 1$.

If $y=\frac{1}{2}\left(e^{x}+e^{-x}\right), e^{2 x}-2 y e^{x}+1=0$. Then, using the quadratic formula, $e^{x}=\frac{2 y \pm \sqrt{4 y^{2}-4}}{2}=y \pm \sqrt{y^{2}-1}$.

Thus, $x=\ln \left(y \pm \sqrt{y^{2}-1}\right)$.

Since $y-\sqrt{y^{2}-1}=\left(y-\sqrt{\left.y^{2}-1\right)}\left(\frac{y+\sqrt{y^{2}-1}}{y+\sqrt{y^{2}-1}}\right)=\frac{1}{y+\sqrt{y^{2}-1}}\right.$ we can also write $x= \pm \ln \left(y+\sqrt{y^{2}-1}\right)$ or $\cosh ^{-1} y= \pm \ln \left(y+\sqrt{y^{2}-1}\right)$.

Choosing the + sign as defining the principal value and replacing $y$ by $x$, we have $\cosh ^{-1} x=\ln (x+$ $\sqrt{y^{2}-1}$ ). The choice $x \geqq 1$ is made so that the inverse function is real.

\section*{Limits}
3.10. If (a) $f(x)=x^{2}$ and (b) $f(x)=\left\{\begin{array}{ll}x^{2}, & x \neq 2 \\ 0, & x=2\end{array}\right.$, prove that $\lim _{x \rightarrow 2} f(x)=4$.

(a) We must show that, given any $\epsilon>0$, we can find $\delta>0$ (depending on $\epsilon$ in general) such that $\left|x^{2}-4\right|<\epsilon$ when $0<|x-2|<\delta$.

Choose $\delta \leqq 1$ so that $0<|x-2|<1$ or $1<x<3, x \neq 2$. Then $\left|x^{2}-4\right|=|(x-2)(x+2)|=|x-2|$ $|x+2|<\delta|x+2|<5 \delta$.

Take $\delta$ as 1 or $\epsilon / 5$, whichever is smaller. Then we have $\left|x^{2}-4\right|<\epsilon$ whenever $0<|x-2|<\delta$, and the required result is proved.

It is of interest to consider some numerical values. If, for example, we wish to make $\left|x^{2}-4\right|<.05$, we can choose $\delta=\epsilon / 5=.05 / 5=.01$. To see that this is actually the case, note that if $0<|x-2|<.01$, then $1.99<x<2.01(x \neq 2)$, and so $3.9601<x^{2}<4.0401,-.0399<x^{2}-4<.0401$, and certainly $\left|x^{2}-4\right|$ $<.05\left(x^{2} \neq 4\right)$. The fact that these inequalities also happen to hold at $x=2$ is merely coincidental.

If we wish to make $\left|x^{2}-4\right|<6$, we can choose $\delta=1$, and this will be satisfied.

(b) There is no difference between the proof for this case and the proof in (a), since in both cases we exclude $x=2$.

3.11. Prove that $\lim _{x \rightarrow 1} \frac{2 x^{4}-6 x^{3}+x^{2}+3}{x-1}=-8$.

We must show that for any $\epsilon>0$ we can find $\delta>0$ such that $\left|\frac{2 x^{4}-6 x^{3}+x^{2}+3}{x-1}-(-8)\right|<\varepsilon$ when $0<$ $|x-1|<\delta$. Since $x \neq 1$, we can write $\frac{2 x^{4}-6 x^{3}+x^{2}+3}{x-1}=\frac{\left(2 x^{3}-4 x^{2}-3 x-3\right)(x-1)}{x-1}=2 x^{3}-4 x^{2}-3 x-3$ on cancelling the common factor $x-1 \neq 0$.

Then we must show that for any $\varepsilon>0$, we can find $\delta>0$ such that $\left|2 x^{3}-4 x^{2}-3 x+5\right|<\epsilon$ when $0<$ $|x-1|<\delta$. Choosing $\delta \leqq 1$, we have $0<x<2, x \neq 1$.

Now $\left|2 x^{3}-4 x^{2}-3 x+5\right|=|x-1|\left|2 x^{2}-2 x-5\right|<\delta\left|2 x^{2}-2 x-5\right|<\delta\left(\left|2 x^{2}\right|+|2 x|+5\right)<(8+4+5)$ $\delta=17 \delta$. Taking $\delta$ as the smaller of 1 and $\epsilon / 17$, the required result follows.

3.12. Let

$$
f(x)=\left\{\begin{array}{ll}
\frac{|x-3|}{x-3}, & x \neq x \\
0, & x=3
\end{array} .\right.
$$

(a) Graph the function. (b) Find $\lim _{x f(x)}$. (c) Find $\lim _{x \rightarrow 3+} f(x)$. (d) Find $\lim _{x \rightarrow 3} f(x)$.

(a) For $x>3, \frac{|x-3|}{x-3}=\frac{x-3}{x-3}=1$.

For $x>3, \frac{|x-3|}{x-3}=\frac{-(x-3)}{x-3}=1$.

Then the graph, shown in Figure 3.12, consists of the lines $y=1, x>3 ; y=-1, x<3$; and the point $(3,0)$.

(b) As $x \rightarrow 3$ from the right, $f(x) \rightarrow 1$; i.e., $\lim _{x \rightarrow 3+} f(x)=1$, as seems clear from the graph. To prove this we must show that given any $\epsilon>0$, we can find $\delta>0$ such that $|f(x)-1|<\epsilon$ whenever $0<x-1<\delta$.

Now, since $x>1, f(x)=1$ and so the proof consists in the triviality that $|1-1|<\epsilon$ whenever $0<x-1<\delta$.

(c) As $x \rightarrow 3$ from the left, $f(x) \rightarrow-1$; i.e., $\lim _{x \rightarrow 3-} f(x)=-1$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-068}
\end{center}

Figure 3.12 A proof can be formulated as in $(b)$.

(d) Since $\lim _{x \rightarrow 3+} f(x) \neq \lim _{x \rightarrow 3-} f(x), \neq \lim _{x \rightarrow 3-} f(x)$ does not exist.

3.13. Prove that $\lim _{x \rightarrow 0} x \sin 1 / x=0$.

We must show that given any $\epsilon>0$, we can find $\delta>0$ such that $|x \sin 1 / x-0|<\epsilon$ when $0<|x-0|$ $<\delta$.

If $0<|x|<\delta$, then $|x \sin 1 / x|=|x||\sin 1 / x| \leqq|x|<\delta$, since $|\sin 1 / x| \leqq 1$ for all $x \neq 0$.

Making the choice $\delta=\epsilon$, we see that $|x \sin 1 / x|<\epsilon$ when $0<|x|<\delta$, completing the proof.

3.14. Evaluate $\lim _{x \rightarrow 0+} \frac{2}{1+e^{-1 / x}}$.

As $x \rightarrow 0+$ we suspect that $1 / x$ increases indefinitely, $e^{1 / x}$ increases indefinitely, $e^{-1 / x}$ approaches 0 , and $1+e^{-1 / x}$ approaches 1 ; thus, the required limit is 2 .

To prove this conjecture we must show that, given $\epsilon>0$, we can find $\delta>0$ such that

$$
\left|\frac{2}{1+e^{-1 / x}}-2\right|<\varepsilon \text { when } 0<x<\delta
$$

Now

$$
\left|\frac{2}{1+e^{-1 / x}}-2\right|=\left|\frac{2-2-2 e^{-1 / x}}{1+e^{-1 / x}}\right|=\frac{2}{e^{1 / x}+1}
$$

Since the function on the right is smaller than 1 for all $x>0$, any $\delta>0$ will work when $e \geq 1$. If $0<\varepsilon<1$, then $\frac{2}{e^{1 / x}+1}<\varepsilon$ when $\frac{e^{1 / x}+1}{2}>\frac{1}{\varepsilon}, e^{1 / x}>\frac{2}{\varepsilon}-1, \frac{1}{x}>\operatorname{In}\left(\frac{2}{\varepsilon}-1\right)$; or $0<x<\frac{1}{\ln (2 / \varepsilon-1)}=\delta$.

3.15. Explain exactly what is meant by the statement $\lim _{x \rightarrow 1} \frac{1}{(x-1)^{4}}=\infty$ and prove the validity of this statement.

The statement means that for each positive number $M$, we can find a positive number $\delta$ (depending on $M$ in general) such that

$$
\frac{1}{(x-1)^{4}}>4 \quad \text { when } \quad 0<|x-1|<\delta
$$

To prove this, note that $\frac{1}{(x-1)^{4}}>M$ when $0<(x-1)^{4}<\frac{1}{M}$ or $0<|x-1|<\frac{1}{\sqrt[4]{M}}$.

Choosing $\delta=1 / \sqrt[4]{M}$, the required results follows.

3.16. Present a geometric proof that $\lim _{\theta \rightarrow 0} \frac{\sin \theta}{\theta}=1$.

Construct a circle with center at $O$ and radius $O A=O D=1$, as in Figure 3.13. Choose point $B$ on $O A$ extended and point $C$ on $O D$ so that lines $B D$ and $A C$ are perpendicular to $O D$.

It is geometrically evident that

Area of triangle $O A C<$ Area of sector $O A D<$ Area of triangle $O B D$

that is,

Dividing by $\frac{1}{2} \sin \theta$,

$$
\frac{1}{2} \sin \theta \cos \theta<\frac{1}{2} \theta<\frac{1}{2} \tan \theta
$$

$$
\cos \theta<\frac{\theta}{\sin \theta}<\frac{1}{\cos \theta}
$$

or

$$
\cos \theta<\frac{\sin \theta}{\theta}<\frac{1}{\cos \theta}
$$

As $\theta \rightarrow 0, \cos \theta \rightarrow 1$, and it follows that $\lim _{\theta \rightarrow 0} \frac{\sin \theta}{\theta}=1$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-069}
\end{center}

Figure 3.13

\section*{Theorems on limits}
3.17. If $\lim _{x \rightarrow x_{0}} f(x)$ exists, prove that it must be unique.

We must show that if $\lim _{x \rightarrow x_{0}} f(x)=l_{1}$ and $\lim _{x \rightarrow x_{0}} f(x)=l_{2}$, then $l_{1}=l_{2}$.

By hypothesis, given any $\epsilon>0$ we can find $\delta>0$ such that

$$
\begin{array}{lll}
\left|f(x)-l_{1}\right|<\epsilon / 2 & \text { when } & 0<\left|x-x_{0}\right|<\delta \\
\left|f(x)-l_{2}\right|<\epsilon / 2 & \text { when } & 0<\left|x-x_{0}\right|<\delta
\end{array}
$$

Then by the absolute value property 2 on Page 4 ,

$$
\left|l_{1}-l_{2}\right|=\left|l_{1}-f(x)+f(x)-l_{2}\right| \leqq\left|l_{1}-f(x)\right|+\left|f(x)-l_{2}\right|<\epsilon / 2+\epsilon / 2=\epsilon
$$

i.e., $\left|l_{1}-l_{2}\right|$ is less than any positive number $\epsilon$ (however small) and so must be zero. Thus, $l_{1}=l_{2}$.

3.18. If $\lim _{x \rightarrow x_{0}} g(x)=B \neq 0$, prove that there exists $\delta>0$ such that

$$
|g(x)|>\frac{1}{2}|B| \quad \text { for } \quad 0<\left|x-x_{0}\right|<\delta
$$

Since $\lim _{x \rightarrow x_{0}} g(x)=B$, we can find $\delta>0$ such that $|g(x)-B|<\frac{1}{2}|B|$ for $0<\left|x-x_{0}\right|<\delta$.

Writing $B=B-g(x)+g(x)$, we have

$$
|B| \leqq|B-g(x)|+|g(x)|<\frac{1}{2}|B|+|g(x)|
$$

i.e., $|B|<\frac{1}{2}|B|+|g(x)|$, from which $|g(x)|>\frac{1}{2}|B|$.

3.19. Given $\lim _{x \rightarrow x_{0}} f(x)=A$ and $\lim _{x \rightarrow x_{0}} g(x)=B$, prove (a) $\lim _{x \rightarrow x_{0}}[f(x)+g(x)]=A+B$, (b) $\lim _{x \rightarrow x_{0}} f(x) g(x)=A B$,

(c) $\lim _{x \rightarrow x_{0}} \frac{1}{g(x)}=\frac{1}{B}$ if $B \neq 0$, and (d) $\lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}=\frac{A}{B}$ if $B \neq 0$.

(a) We must show that for any $\epsilon>0$ we can find $\delta>0$ such that

$$
|[f(x)+g(x)]-(A+B)|<\varepsilon \quad \text { when } \quad 0<\left|x-x_{0}\right|<\delta
$$

Using absolute value property 2 , Page 4 , we have


\begin{equation*}
|[f(x)+g(x)]-(A+B)|=|[f(x)-A]+[g(x)-B]| \leqq|f(x)-A|+|g(x)-B| \tag{1}
\end{equation*}


By hypothesis, given $\epsilon>0$ we can find $\delta_{1}>0$ and $\delta_{2}>0$ such that

\[
\begin{array}{lll}
|f(x)-A|<\epsilon / 2 & \text { when } & 0<\left|x-x_{0}\right|<\delta_{1} \\
|g(x)-B|<\epsilon / 2 & \text { when } & 0<\left|x-x_{0}\right|<\delta_{2} \tag{3}
\end{array}
\]

Then, from Equations (1), (2), and (3),

$$
|[f(x)+g(x)]-(A+B)|<\epsilon / 2+\epsilon / 2=\epsilon \quad \text { when } \quad 0<\left|x-x_{0}\right|<\delta
$$

where $\delta$ is chosen as the smaller of $\delta_{1}$ and $\delta_{2}$.

(b) We have


\begin{align*}
|f(x) g(x)-A B| & =|f(x)[g(x)-B]+B[f(x)-A]|  \tag{4}\\
& \leqq|f(x)||g(x)-B|+|B||f(x)-A| \\
& \leqq|f(x)||g(x)-B|+(|B|+1)|f(x)-A|
\end{align*}


Since $\lim _{x \rightarrow x_{0}} f(x)=A$, we can find $\delta_{1}$ such that $|f(x)-A|<1$ for $0<\left|x-x_{0}\right|<\delta_{1}$, i.e., $A-1<f(x)<A$ +1 , so that $f(x)$ is bounded, i.e., $|f(x)|<P$ where $P$ is a positive constant.

Since $\lim _{x \rightarrow x_{0}} g(x)=B$, given $\epsilon>0$, we can find $\delta_{2}>0$ such that $|g(x)-B|<\epsilon / 2 P$ for $0<\left|x-x_{0}\right|<\delta_{2}$.

Since $\lim _{x \rightarrow x_{0}} f(x)=A$, given $\epsilon>0$, we can find $\delta_{3}>0$ such that $|f(x)-A|<\frac{\varepsilon}{2(|B|+1)}$ for $0<\left|x-x_{0}\right|<\delta_{2}$.

Using these in Equation (4), we have

$$
|f(x) g(x)-A B|<P \cdot \frac{\varepsilon}{2 P}+(|B|+1) \cdot \frac{\varepsilon}{2(|B|+1)}=\varepsilon
$$

for $0<\left|x-x_{0}\right|<\delta$, where $\delta$ is the smaller of $\delta_{1}, \delta_{1}, \delta_{2}, \delta_{3}$, and the proof is complete.

(c) We must show that for any $\epsilon>0$ we can find $\delta>0$ such that


\begin{equation*}
\left|\frac{1}{g(x)}-\frac{1}{B}\right|=\frac{|g(x)-B|}{|B||g(x)|}<\varepsilon \quad \text { when } \quad 0<\left|x-x_{0}\right|<\delta \tag{5}
\end{equation*}


By hypothesis, given $\epsilon>0$, we can find $\delta_{1}>0$ such that

$$
|g(x)-B|<\frac{1}{2} B^{2} \epsilon \quad \text { when } \quad 0<\left|x-x_{0}\right|<\delta_{1}
$$

By Problem 3.18, since $\lim _{x \rightarrow 0} g(x)=B \neq 0$, we can find $\delta_{2}>0$ such that

$$
|g(x)|>\frac{1}{2}|B| \quad \text { when } \quad 0<\left|x-x_{0}\right|<\delta_{2}
$$

Then, if $\delta$ is the smaller of $\delta_{1}$ and $\delta_{2}$, we can write

$$
\left|\frac{1}{g(x)}-\frac{1}{B}\right|=\frac{|g(x)-B|}{|B||g(x)|}<\frac{\frac{1}{2} B^{2} \varepsilon}{|B| \cdot \frac{1}{2}|B|}=\varepsilon \quad \text { whenever } \quad 0<\left|x-x_{0}\right|<\delta
$$

and the required result is proved.

(d) From parts $(b)$ and $(c)$,

$$
\lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}=\lim _{x \rightarrow x_{0}} f(x) \cdot \frac{1}{g(x)}=\lim _{x \rightarrow x_{0}} f(x) \cdot \lim _{x \rightarrow x_{0}} \frac{1}{g(x)}=A \cdot \frac{1}{B}=\frac{A}{B}
$$

This can also be proved directly (see Problem 3.69).

These results can also be proved in the cases $x \rightarrow x_{0}+, x \rightarrow x_{0}-, x \rightarrow \infty, x \rightarrow-\infty$.

Note: In the proof of $(a)$ we have used the results $|f(x)-A|<\epsilon / 2$ and $|g(x)-B|<\epsilon / 2$, so that the final result would come out to be $|f(x)+g(x)-(A+B)|<\epsilon$. Of course, the proof would be just as valid if we had used $2 \epsilon$ (or any other positive multiple of $\epsilon$ ) in place of $\epsilon$. A similar remark holds for the proofs of $(b),(c)$, and $(d)$.

3.20. Evaluate each of the following, using theorems on limits.

(a) $\lim _{x \rightarrow 2}\left(x^{2}-6 x+4\right) \quad=\lim _{x \rightarrow 2} x^{2}+\lim _{x \rightarrow 2}(-6 x)+\lim _{x \rightarrow 2} 4$

$$
\begin{aligned}
& =\left(\lim _{x \rightarrow 2} x\right)\left(\lim _{x \rightarrow 2} x\right)+\left(\lim _{x \rightarrow 2}-6\right)\left(\lim _{x \rightarrow 2} x\right)+\lim _{x \rightarrow 2} 4 \\
& =(2)(2)+(-6)(2)+4=-4
\end{aligned}
$$

In practice, the intermediate steps are omitted.

(b) $\lim _{x \rightarrow-1} \frac{(x+3)(2 x-1)}{x^{2}+3 x-2}=\frac{\lim _{x \rightarrow-1}(x+3) \lim _{x \rightarrow-1}(2 x-1)}{\lim _{x \rightarrow-1}\left(x^{2}+3 x-2\right)}=\frac{2 \cdot(-3)}{-4}=\frac{3}{2}$

(c) $\lim _{x \rightarrow \infty} \frac{2 x^{4}-3 x^{2}+1}{6 x^{4}+x^{3}-3 x}=\lim _{x \rightarrow \infty} \frac{2-\frac{3}{x^{2}}+\frac{1}{x^{4}}}{6+\frac{1}{x}-\frac{3}{x^{3}}}$

$$
=\frac{\lim _{x \rightarrow \infty} 2+\lim _{x \rightarrow \infty} \frac{-3}{2}+\lim _{x \rightarrow \infty} \frac{1}{x^{4}}}{\lim _{x \rightarrow \infty} 6+\lim _{x \rightarrow \infty} \frac{1}{x}+\lim _{x \rightarrow \infty} \frac{-3}{x^{3}}}=\frac{2}{6}=\frac{1}{3}
$$

by Problem 3.19.

(d) $\begin{aligned} \lim _{h \rightarrow 0} \frac{\sqrt{4+h}-2}{h} & =\lim _{h \rightarrow 0} \frac{\sqrt{4+h}-2}{h} \cdot \frac{\sqrt{4+h}+2}{\sqrt{4+h}+2} \\ & =\lim _{h \rightarrow 0} \frac{4+h-4}{h(\sqrt{4+h}+2}=\lim _{h \rightarrow 0} \frac{1}{\sqrt{4+h}+2}=\frac{1}{2+2}=\frac{1}{4}\end{aligned}$

(e) $\lim _{x \rightarrow 0+} \frac{\sin x}{\sqrt{x}}=\lim _{x \rightarrow 0+} \frac{\sin x}{x} \cdot \sqrt{x}=\lim _{x \rightarrow 0-} \frac{\sin x}{x} \cdot \lim _{x \rightarrow 0+} \sqrt{x}=1 \cdot 0=0$.

Note that in (c), (d), and (e) if we use the theorems on limits indiscriminately we obtain the so-called indeterminate forms $\infty / \infty$ and $0 / 0$. To avoid such predicaments, note that in each case the form of the limit is suitably modified. For other methods of evaluating limits, see Chapter 4.

\section*{Continuity}
(Assume that values at which continuity is to be demonstrated are interior domain values unless otherwise stated.)

3.21. Prove that $f(x)=x^{2}$ is continuous at $x=2$.

Method 1: By Problem 3.10, $\lim _{x \rightarrow 2} f(x)=f(2)=4$ and so $f(x)$ is continuous at $x=2$.

Method 2: We must show that, given any $\epsilon>0$, we can find $\delta>0$ (depending on $\epsilon$ ) such that $|f(x)-f(2)|$ $=\left|x^{2}-4\right|<\epsilon$ when $|x-2|<\delta$. The proof patterns are given in Problem 3.10.

3.22. (a) Prove that $f(x)=\left\{\begin{array}{ll}x \sin 1 / x, & x \neq 0 \\ 5, & x=0\end{array}\right.$ is not continuous at $x=0$. (b) Can we redefine $f(0)$ so that $f(x)$ is continuous at $x=0$ ?

(a) From Problem 3.13, $\lim _{x \rightarrow 0} f(x)=0$. But this limit is not equal to $f(0)=5$, so $f(x)$ is discontinuous at $x=0$.

(b) By redefining $f(x)$ so that $f(0)=0$, the function becomes continuous. Because the function can be made continuous at a point simply by redefining the function at the point, we call the point a removable discontinuity.

3.23. Is the function $f(x)=\frac{2 x^{4}-6 x^{3}+x^{2}+3}{x-1}$ continuous at $x=1$ ?

$f(1)$ does not exist, so $f(x)$ is not continuous at $x=1$. By redefining $f(x)$ so that $f(1)=\lim _{x \rightarrow 1} f(x)=-8$ (see Problem 3.11), it becomes continuous at $x=1$; i.e., $x=1$ is a removable discontinuity.

3.24. $\quad$ Prove that if $f(x)$ and $g(x)$ are continuous at $x=x_{0}$, so also are (a) $f(x),+g(x)$, (b) $f(x) g(x)$, and (c) $\frac{f(x)}{g(x)}$ if $f\left(x_{0}\right) \neq 0$.

These results follow at once from the proofs given in Problem 3.19 by taking $A=f\left(x_{0}\right)$ and $B=g\left(x_{0}\right)$ and rewriting $0<\left|x-x_{0}\right|<\delta$ as $\left|x-x_{0}\right|<\delta$, i.e., including $x=x_{0}$.

3.25. Prove that $f(x)=x$ is continuous at any point $x=x_{0}$.

We must show that, given any $\epsilon>0$, we can find $\delta>0$ such that $\left|f(x)-f\left(x_{0}\right)\right|=\left|x-x_{0}\right|<\epsilon$ when $\left|x-x_{0}\right|<\delta$. By choosing $\delta=\epsilon$, the result follows at once.

3.26. Prove that $f(x)=2 x^{3}+x$ is continuous at any point $x=x_{0}$.

Since $x$ is continuous at any point $x=x_{0}$ (Problem 3.25), so also is $x \cdot x=x^{2}, x^{2} \cdot x=x^{3}, 2 x^{3}$, and, finally, $2 x^{3}+x$, using the theorem (Problem 3.24) that sums and products of continuous functions are continuous.

3.27. Prove that if $f(x)=\sqrt{x-5}$ for $5 \leqq x \leqq 9$, then $f(x)$ is continuous in this interval.

If $x_{0}$ is any point such that $5<x_{0}<9$, then $\lim _{x \rightarrow x_{0}} f(x)=\lim _{x \rightarrow x_{0}} \sqrt{x-5}=\sqrt{x_{0}-5}=f\left(x_{0}\right)$. Also, $\lim _{x \rightarrow 5+} \sqrt{x-5}=0=f(5)$ and $\lim _{x \rightarrow 9-} \sqrt{x-5}=2 f(9)$. Thus the result follows.

Here we have used the result that $\lim _{x \rightarrow x_{0}} \sqrt{f(x)}=\sqrt{\lim _{x \rightarrow x_{0}} f(x)}=\sqrt{f\left(x_{0}\right)}$ if $f(x)$ is continuous at $x_{0}$. An $\epsilon, \delta$ proof, directly from the definition, can also be employed.

3.28. For what values of $x$ in the domain of definition is each of the following functions continuous?

(a) $f(x)=\frac{1}{x^{2}-1}$

(b) $f(x)=\frac{1+\cos x}{3+\sin x}$

(c) $f(x)=\frac{1}{\sqrt[4]{10+4}}$

(d) $f(x)=10^{-1 /(x-3) 2}$

(e) $f(x)= \begin{cases}10^{-1(x-3)^{2}}, & x \neq 3 \\ 0, & x=3\end{cases}$

(f) $f(x)=\frac{x-|x|}{x}$

(g) $f(x)= \begin{cases}\frac{x-|x|}{x}, & x<0 \\ 2, & x=0\end{cases}$

(h) $f(x)=x \csc x=\frac{x}{\sin x}$.

(i) $f(x)=x \csc x, f(0)=1$.

(a) All $x$ except $\cdot x= \pm 1$ (where the denominator is zero)

(b) All $x$

(c) All $x>-10$

(d) All $x \neq 3$ (see Problem 3.55)

(e) All $x$, since $\lim _{x \rightarrow 3} f(x)=f(3)$

(f) If $x>0, f(x)=\frac{x-x}{x}=0$. If $x<0, f(x)=\frac{x+x}{x}=2$. At $x=0, f(x)$ is undefined. Then $f(x)$ is continuous for all $x$ except $x=0$.

(g) As in $(f), f(x)$ is continuous for $x<0$. Then, since

$$
\lim _{x \rightarrow 0-} \frac{x-|x|}{x}=\lim _{x \rightarrow 0-} \frac{x+x}{x}=\lim _{x \rightarrow 0-} 2=2=f(0)
$$

it follows that $f(x)$ is continuous (from the left) at $x=0$.

Thus, $f(x)$ is continuous for all $x \leqq 0$, i.e., everywhere in its domain of definition.\\
(h) All $x$ except $0, \pm \pi, \pm 2 \pi, \pm 3 \pi, \ldots$

(i) Since $\lim _{x \rightarrow 0} x \csc x=\lim _{x \rightarrow 0} \frac{x}{\sin x}=1=f(0)$, we see that $f(x)$ is continuous for all $x$ except $\pm \pi, \pm 2 \pi$, $\pm 3 \pi, \ldots$ [compare $(h)]$.

\section*{Uniform continuity}
3.29. Prove that $f(x)=x^{2}$ is uniformly continuous in $0<x<1$.

Method 1: Using definition.

We must show that, given any $\epsilon>0$, we can find $\delta>0$ such that $\left|x^{2}-x_{0}^{2}\right|<\epsilon$ when $\left|x-x_{0}\right|<\delta$, where $\delta$ depends only on $\epsilon$ and not on $x_{0}$ where $0<x_{0}<1$.

If $x$ and $x_{0}$ are any points in $0<x<1$, then

$$
\left|x^{2}-x_{0}^{2}\right|=\left|x+x_{0}\right|\left|x-x_{0}\right|<2\left|x-x_{0}\right|
$$

Thus, if $\left|x-x_{0}\right|<\delta$, it follows that $\left|x^{2}-x^{2}{ }_{0}\right|<2 \delta$. Choosing $\delta=\epsilon / 2$, we see that $\left|x^{2}-x_{0}^{2}\right|<\epsilon$ when $\left|x-x_{0}\right|$ $<\delta$, where $\delta$ depends only on $\epsilon$ and not on $x_{0}$ Hence, $f(x)=x^{2}$ is uniformly continuous in $0<x<1$.

This can be used to prove that $f(x)=x^{2}$ is uniformly continuous in $0 \leqq x \leqq 1$.

Method 2: The function $f(x)=x^{2}$ is continuous in the closed interval $0 \leqq x \leqq 1$. Hence, by the theorem on Page 48, it is uniformly continuous in $0 \leqq x \leqq 1$ and thus in $0<x<1$.

3.30. Prove that $f(x)=1 / x$ is not uniformly continuous in $0<x<1$.

Method 1: Suppose $f(x)$ is uniformly continuous in the given interval. Then, for any $\epsilon>0$ we should be able to find $\delta$, say, between 0 and 1 , such that $\left|f(x)-f\left(x_{0}\right)\right|<\epsilon$ when $\left|x-x_{0}\right|<\delta$ for all $x$ and $x_{0}$ in the interval.

Let $x=\delta$ and $x_{0}=\frac{\delta}{1+\varepsilon}$. Then $\left|x-x_{0}\right|=\left|\delta-\frac{\delta}{1+\varepsilon}\right|=\frac{\varepsilon}{1+\varepsilon} \delta<\delta$.

However, $\left|\frac{1}{x}-\frac{1}{x_{0}}\right|=\left|\frac{1}{\delta}-\frac{1+\varepsilon}{\delta}\right|=\frac{\varepsilon}{\delta}>\varepsilon \quad($ since $0<\delta<1)$.

Thus, we have a contradiction, and it follows that $f(x)=1 / x$ cannot be uniformly continuous in $0<x<1$.

Method 2: Let $x_{0}$ and $x_{0}+\delta$ be any two points in $(0,1)$. Then,

$$
\left\lvert\, f\left(x_{0}\right)-f\left(\left.x_{0}+\delta|=| \frac{1}{x_{0}}-\frac{1}{x_{0}+\delta} \right\rvert\,=\frac{\delta}{x_{0}\left(x_{0}+\delta\right)}\right.\right.
$$

can be made larger than any positive number by choosing $x_{0}$ sufficiently close to 0 . Hence, the function cannot be uniformly continuous.

\section*{Miscellaneous problems}
3.31. If $y=f(x)$ is continuous at $x=x_{0}$, and $z=g(y)$ is continuous at $y=y_{0}$ where $y_{0}=f\left(x_{0}\right)$, prove that $z=g\{f(x)\}$ is continuous at $x=x_{0}$.

Let $h(x)=g\{f(x)\}$. Since, by hypothesis, $f(x)$ and $g(y)$ are continuous at $x_{0}$ and $y_{0}$, respectively, we have

$$
\begin{aligned}
& \lim _{x \rightarrow x_{0}} f(x)=f\left(\lim _{x \rightarrow x_{0}} x\right)=f\left(x_{0}\right) \\
& \lim _{y \rightarrow y_{0}} g(y)=g\left(\lim _{y \rightarrow y_{0}} y\right)=g\left(y_{0}\right)=g\left\{f\left(x_{0}\right)\right\}
\end{aligned}
$$

Then

$$
\lim _{x \rightarrow x_{0}} h(x)=\lim _{x \rightarrow x_{0}} g\{f(x)\}=g\left\{\lim _{x \rightarrow x_{0}} f(x)\right\}=g\left\{f\left(x_{0}\right)\right\}=h\left(x_{0}\right)
$$

which proves that $h(x)=g\{f(x)\}$ is continuous at $x=x_{0}$.

3.32. Prove Theorem 8, Page 52.

Suppose that $f(a)<0$ and $f(b)>0$. Since $f(x)$ is continuous, there must be an interval $(a, a+h), h>0$, for which $f(x)<0$. The set of points $(a, a+h)$ has an upper bound and so has a least upper bound, which we call $c$. Then $f(c) \leq 0$. Now we cannot have $f(c)<0$, because if $f(c)$ were negative we would be able to find an interval about $c$ (including values greater than $c$ ) for which $f(x)<0$; but since $c$ is the least upper bound, this is impossible, and so we must have $f(c)=0$ as required.

If $f(a)>0$ and $f(b)<0$, a similar argument can be used.

3.33. (a) Given $f(x)=2 x^{3}-3 x^{2}+7 x-10$, evaluate $f(1)$ and $f(2)$. (b) Prove that $f(x)=0$ for some real number $x$ such that $1<x<2$. (c) Show how to calculate the value of $x$ in (b).

(a) $f(1)=2(1)^{3}-3(1)^{2}+7(1)-10=-4, f(2)=2(2)^{3}-3(2)^{2}+7(2)-10=8$.

(b) If $f(x)$ is continuous in $a \leq x \leq b$ and if $f(a)$ and $f(b)$ have opposite signs, then there is a value of $x$ between $a$ and $b$ such that $f(\bar{x})=\overline{0}$ (Problem 3.32).

To apply this theorem, we need only realize that the given polynomial is continuous in $1 \leqq x \leqq 2$, since we have already shown in $(a)$ that $f(1)<0$ and $f(2)>0$. Thus, there exists a number $c$ between 1 and 2 such that $f(c)=0$.

(c) $f(1.5)=2(1.5)^{3}-3(1.5)^{2}+7(1.5)-10=0.5$. Then, applying the theorem of $(b)$ again, we see that the required root lies between 1 and 1.5 and is "most likely" closer to 1.5 than to 1 , since $f(1.5)=$ 0.5 has a value closer to 0 than $f(1)=-4$ (this is not always a valid conclusion but is worth pursuing in practice).

Thus, we consider $x=1.4$. Since $f(1.4)=2(1.4)^{3}-3(1.4)^{2}+7(1.4)-10=-0.592$, we conclude that there is a root between 1.4 and 1.5 which is most likely closer to 1.5 than to 1.4.

Continuing in this manner, we find that the root is 1.46 to 2 decimal places.

3.34. Prove Theorem 10, Page 52 .

Given any $\epsilon>0$, we can find $x$ such that $M-f(x)<\epsilon$ by definition of the 1.u.b. $M$.

Then $\frac{1}{M-f(x)}>\frac{1}{\varepsilon}$, so that $\frac{1}{M-f(x)}$ is not bounded and, hence, cannot be continuous in view of Theorem 4, Page 52. However, if we suppose that $f(x) \neq M$, then, since $M-f(x)$ is continuous, by hypothesis we must have $\frac{1}{M-f(x)}$ also continuous. In view of this contradiction, we must have $f(x)=M$ for at least one value of $x$ in the interval.

Similarly, we can show that there exists an $x$ in the interval such that $f(x)=m$ (Problem 3.93).

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Functions}
3.35. Give the largest domain of definition for which each of the following rules of correspondence supports the the construction of a function.\\
(a) $\sqrt{(3-x)(2 x+4)}$\\
(b) $(x-2) /\left(x^{2}-4\right)$\\
(c) $\sqrt{\sin 3 x}$\\
(d) $\log _{10}\left(x^{3}-3 x^{2}-4 x+12\right)$

Ans. (a) $-2 \leqq x \leqq 3$

(b) all $x \neq \pm 2$

(c) $2 m \pi / 3 \leqq x \leqq(2 m+1) \pi / 3, m=0, \pm 1, \pm 2, \ldots$

(d) $x>3,-2<x<2$

3.36. If $f(x)=\frac{3 x+1}{x-2}, x \neq 2$, find:\\
(a) $\frac{5 f(-1)-2 f(0)+3 f(5)}{6}$\\
(b) $\left\{f\left(-\frac{1}{2}\right)\right\}^{2}$\\
(c) $f(2 x-3)$\\
(d) $f(x)+f(4 / x), x \neq 0$\\
(e) $\frac{f(h)-f(0)}{h} h \neq 0$\\
(f) $f(\{f(x)\}$

Ans. (a) $\frac{61}{81}$

(b) $\frac{1}{25}$ (c) $\frac{6 x-8}{2 x-5}, x \neq 0, \frac{5}{2}, 2$

(d) $\frac{5}{2}, x \neq 0,2$

(e) $\frac{7}{2 h-4}, h \neq 0,2$ (f) $\frac{10 x+1}{x+5}, x \neq-5,2$

3.37. If $f(x)=2 x^{2}, 0<x \leqq 2$, find (a) the 1.u.b. and (b) the g.1.b. of $f(x)$. Determine whether $f(x)$ attains its l.u.b. and g.l.b.

Ans. (a) 8 (b) 0

3.38. Construct a graph for each of the following functions.\\
(a) $f(x)=|x|,-3 \leqq x \leqq 3$\\
(f) $\frac{x-[x]}{x}$ where $[x]=$ greatest integer $\leqq x$\\
(b) $f(x)=2-\frac{|x|}{x},-2 \leqq x \leqq 2$\\
(g) $f(x)=\cosh x$\\
(c) $f(x)= \begin{cases}0, & x<0 \\ \frac{1}{2}, & x=0 \\ 1, & x>0\end{cases}$\\
(h) $f(x)=\frac{\sin x}{x}$\\
(d) $f(x)=\left\{\begin{array}{cr}-x, & -2 \leqq x \leqq 0 \\ x, & 0 \leqq x \leqq 2\end{array}\right.$\\
(i) $f(x)=\frac{x}{(x-1)(x-2)(x-3)}$\\
(e) $f(x)=x^{2} \sin 1 / x, x \neq 0$\\
(j) $f(x)=\frac{\sin ^{2} x}{x^{2}}$

3.39. Construct graphs for $(a) x^{2} / a^{2}+y^{2} / b^{2}=1$, (b) $x^{2} / a^{2}-y^{2} / b^{2}=1$, (c) $y^{2}=2 p x$, and (d) $y=2 a x-x^{2}$, where $a, b$, and $p$ are given constants. In which cases, when solved for $y$, is there exactly one value of $y$ assigned to each value of $x$, thus making possible definitions of functions $f$ and enabling us to write $y=f(x)$ ? In which cases must branches be defined?

3.40. (a) From the graph of $y=\cos x$, construct the graph obtained by interchanging the variables and from which $\cos ^{-1} x$ will result by choosing an appropriate branch. Indicate possible choices of a principal value of $\cos ^{-1}$ $x$. Using this choice, find $\cos ^{-1}(1 / 2)-\cos ^{-1}(-1 / 2)$. Does the value of this depend on the choice? Explain.

3.41. Work parts (a) and (b) of Problem 3.40 for (a) $y=\sec ^{-1} x$ and (b) $y=\cot ^{-1} x$.

3.42. Given the graph for $y=f(x)$, show how to obtain the graph for $y=f(a x+b)$, where $a$ and $b$ are given constants. Illustrate the procedure by obtaining the graphs of (a) $y=\cos 3 x$, (b) $y=\sin (5 x+\pi / 3)$, and (c) $y=\tan (\pi / 6-2 x)$.

3.43. Construct graphs for (a) $y=e^{-|x|}$, (b) $y=\ln |x|$, and (c) $y=e^{-|x|} \sin x$.

3.44. Using the conventional principal values on Pages 45 and 46 , evaluate:\\
(a) $\sin ^{-1}(-\sqrt{3} / 2)$\\
(f) $\sin ^{-1} x+\cos ^{-1} x,-1 \leqq x \leqq 1$\\
(b) $\tan ^{-1}(1)-\tan ^{-1}(-1)$\\
(g) $\sin ^{-1}(\cos 2 x), 0 \leqq x \leqq \pi / 2$\\
(c) $\cot ^{-1}(1 / \sqrt{3})-\cot ^{-1}(-1 / \sqrt{3})$\\
(h) $\sin ^{-1}(\cos 2 x), \pi / 2 \leqq x \leqq 3 \pi / 2$\\
(d) $\cosh ^{-1} \sqrt{2}$\\
(i) $\tanh \left(\operatorname{csch}^{-1} 3 x\right), x \neq 0$\\
(e) $e^{- \text {coth-1 }}(25 / 7)(j) \cos \left(2 \tan ^{-1} x^{2}\right)$\\
Ans. (a) $-\pi / 3$\\
(f) $\pi / 2$\\
(b) $\pi / 2$\\
(g) $\pi / 2-2 x$\\
(c) $-\pi / 3$\\
(h) $2 x-3 \pi / 2$\\
(d) $\ln (1+\sqrt{2})$\\
(i) $\frac{|x|}{x \sqrt{9 x^{2}+1}}$\\
(e) $\frac{3}{4}$\\
(j) $\frac{1-x^{4}}{1+x^{4}}$

3.45. Evaluate (a) $\cos \{\pi \sinh (\ln 2)\}$ and (b) $\cosh ^{-1}\{\operatorname{coth}(\ln 3)\}$. Ans. (a) $-\sqrt{2} / 2$ (b) $\ln 2$

3.46. (a) Prove that $\tan ^{-1} x+\cot ^{-1} x=\pi / 2$ if the conventional principal values on Pages 45 and 46 are taken. (b) Is $\tan ^{-1} x+\tan ^{-1}(1 / x)=\pi / 2$ also? Explain.

3.47. If $f(x)=\tan ^{-1} x$, prove that $f(x)+f(y)=f\left(\frac{x+y}{1-x y}\right)$, discussing the case $x y=1$.

3.48. Prove that $\tan ^{-1} a-\tan ^{-1} b=\cot ^{-1} b-\cot ^{-1} a$.

3.49. Prove the identities: (a) $1-\tanh ^{2} x=\operatorname{sech}^{2} x$, (b) $\sin 3 x=3 \sin x-4 \sin ^{3} x$, (c) $\cos 3 x=4 \cos ^{3} x-3 \cos x$,

(d) $\tanh \frac{1}{2} x=(\sinh x) /(1+\cosh x)$, and (e) $\ln |\csc x-\cot x|=\ln \left|\tan \frac{1}{2} x\right|$.

3.50. Find the relative and absolute maxima and minima of (a) $f(x)=(\sin x) / x, f(0)=1$ and (b) $f(x)=\left(\sin ^{2} x\right) / x^{2}$, $f(0)=1$. Discuss the cases when $f(0)$ is undefined or $f(0)$ is defined but $\neq 1$.

\section*{Limits}
3.51. Evaluate the following limits, first by using the definition and then by using theorems on limits.\\
(a) $\lim _{x \rightarrow 3}\left(x^{2}-3 x+2\right)$\\
(d) $\lim _{x \rightarrow 4} \frac{\sqrt{x}-2}{4-x}$\\
(b) $\lim _{x \rightarrow-1} \frac{1}{2 x-5}$\\
(e) $\lim _{h \rightarrow 0} \frac{(2+h)^{4}-16}{h}$\\
(c) $\lim _{x \rightarrow 2} \frac{x^{2}-4}{x-2}$\\
(f) $\lim _{x \rightarrow 1} \frac{\sqrt{x}}{x+1}$\\
Ans. (a) 2\\
(b) $-\frac{1}{7}$\\
(c) 4\\
(d) $-\frac{1}{4}$\\
(e) 32\\
(f) $\frac{1}{2}$

3.52. Let $f(x)= \begin{cases}3 x-1, & x<0 \\ 0, & x=0 \\ 2 x+5, & x>0\end{cases}$

(a) Construct a graph of $f(x)$. Evaluate (b) $\lim _{x \rightarrow 2} f(x)$, (c) $\lim _{x \rightarrow-3} f(x)$, (d) $\lim _{x \rightarrow 0+} f(x)$, (e) $\lim _{x \rightarrow 0-} f(x)$, and

(f) $\lim _{x \rightarrow 0} f(x)$, justifying your answer in each case.\\
Ans. (b) 9 (c) -10\\
(d) 5\\
(e) -1\\
(f) does not exist

3.53. Evaluate (a) $\lim _{h \rightarrow 0+} \frac{f(h)-f(0+)}{h}$ and (b) $\lim _{h \rightarrow 0-} \frac{f(h)-f(0-)}{h}$, where $f(x)$ is the function of Problem 3.52. Ans. (a) 2 (b) 3

3.54. (a) If $f(x)=x^{2} \cos 1 / x$, evaluate $\lim _{x \rightarrow 0} f(x)$, justifying your answer. (b) Does your answer to (a) still remain the same if we consider $f(x)=x^{2} \cos 1 / x, x \neq 0, f(0)=2$ ? Explain.

3.55. Prove that $\lim _{x \rightarrow 3} 10^{-1 /(x-3) 2}=0$, using the definition.

3.56. Let $f(x)=\frac{1+10^{-1 / x}}{2-10^{-1 / x}}, x \neq 0, f(0)=\frac{1}{2}$. Evaluate (a) $\lim _{x \rightarrow 0+} f(x)$, (b) $\lim _{x \rightarrow 0-} f(x)$, and (c) $\lim _{x \rightarrow 0} f(x)$, justifying answers in all cases.\\
Ans. (a) $\frac{1}{2}$\\
(b) -1\\
(c) does not exist.

3.57. Find (a) $\lim _{x \rightarrow 0+} \frac{|x|}{x}$ and (b) $\lim _{x \rightarrow 0-} \frac{|x|}{x}$. Illustrate your answers graphically.

Ans. (a) 1 (b) -1

3.58. If $f(x)$ is the function defined in Problem 3.56, does $\lim _{x \rightarrow 0} f(|x|)$ exist? Explain.

3.59. Explain exactly what is meant when you write:\\
(a) $\lim _{x \rightarrow 3} \frac{2-x}{(x-3)^{2}}=-\infty$\\
(b) $\lim _{x \rightarrow 0+}\left(1-e^{1 / x}\right)=-\infty$\\
(c) $\lim _{x \rightarrow \infty} \frac{2 x+5}{3 x-2}=\frac{2}{3}$

3.60. Prove that (a) $\lim _{x \rightarrow \infty} 10^{-x}=0$ and (b) $\lim _{x \rightarrow-\infty} \frac{\cos x}{x+\pi}=0$.

3.61. Explain why (a) $\lim _{x \rightarrow \infty} \sin x$ does not exist and (b) $\lim _{x \rightarrow \infty} e^{-x} \sin x$ does not exist.

3.62. If $f(x)=\frac{3 x+|x|}{7 x-5|x|}$, evaluate (a) $\lim _{x \rightarrow \infty} f(x) \quad$ (b) $\lim _{x \rightarrow-\infty} f(x), \quad$ (c) $\lim _{x \rightarrow 0+} f(x)$, (d) $\lim _{x \rightarrow 0-} f(x)$, and (e) $\lim _{x \rightarrow 0} f(x)$.\\
Ans. (a) 2\\
(b) $\frac{1}{6}$\\
(c) 2\\
(d) $\frac{1}{6}$\\
(e) does not exist

3.63. If $[x]=$ largest integer $\leqq x$, evaluate (a) $\lim _{x \rightarrow 2+}\{x-[x]\}$ and (b) $\lim _{x \rightarrow 2-}\{x-[x]\}$. Ans. (a) 0 and (b) 1

3.64. If $\lim _{x \rightarrow x_{0}} f(x)=A$, prove that (a) $\lim _{x \rightarrow x_{0}}\{f(x)\}^{2}=A^{2}$ and (b) $\lim _{x \rightarrow x_{0}} \sqrt[3]{f(x)}=\sqrt[3]{A}$. What generalizations of these do you suspect are true? Can you prove them?

3.65. If $\lim _{x \rightarrow x_{0}} f(x),=\mathrm{A}$ and $\lim g(x),=B$, prove that (a) $\lim _{x \rightarrow x_{0}}\{f(x)-g(x)\}=A-B$ and (b) $\lim _{x \rightarrow x_{0}}\{a f(x)+$ $b g(x)\}=a A+b B$, where $a, b=$ any constants.

3.66. If the limits of $f(x), g(x)$, and $h(x)$ are $A, B$, and $C$ respectively, prove that (a) $\lim _{x \rightarrow x_{0}}\{f(x)+g(x)+h(x)\}=$ $A+B+C$ and (b) $\lim _{x \rightarrow x_{0}} f(x) g(x) h(x)=A B C$. Generalize these results

3.67. Evaluate each of the following using the theorems on limits.

(a) $\lim _{x \rightarrow 1 / 2}\left\{\frac{2 x^{2}-1}{(3 x+2)(5 x-3)}-\frac{2-3 x}{x^{2}-5 x+3}\right\}$

(b) $\lim _{x \rightarrow \infty} \frac{(3 x-1)(2 x+3)}{(5 x-3)(4 x+5)}$

(c) $\lim _{x \rightarrow-\infty}\left(\frac{3 x}{x-1}-\frac{2 x}{x+1}\right)$

(d) $\lim _{x \rightarrow 1} \frac{1}{x-1}\left(\frac{1}{x+3}-\frac{2 x}{3 x+5}\right)$

Ans. (a) $-8 / 21$ (b) $3 / 10$ (c) 1 (d) $1 / 32$

3.68. Evaluate $\lim _{h \rightarrow 0} \frac{\sqrt[3]{8+h}-2}{h}$. (Hint: Let $8+h=x^{3}$.) Ans. 1/12

3.69. If $\lim _{x \rightarrow x_{0}} f(x)=A$ and $\lim _{x \rightarrow x_{0}} g(x)=B \neq 0$, prove directly that $\lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}=\frac{A}{B}$.

3.70. Given $\lim _{x \rightarrow 0} \frac{\sin 3 x}{x}=1$, evaluate:\\
(a) $\lim _{x \rightarrow 0} \frac{\sin 3 x}{x}$\\
(e) $\lim _{x \rightarrow 0} \frac{6 x-\sin 2 x}{2 x+3 \sin 4 x}$\\
(b) $\lim _{x \rightarrow 0} \frac{1-\cos x}{x}$\\
(f) $\lim _{x \rightarrow 0} \frac{\cos a x-\cos b x}{x^{2}}$\\
(c) $\lim _{x \rightarrow 0} \frac{1-\cos x}{x^{2}}$\\
(g) $\lim _{x \rightarrow 0} \frac{1-2 \cos x+\cos 2 x}{x^{2}}$\\
(d) $\lim _{x \rightarrow 3}(x-3) \csc \pi x$\\
(h) $\lim _{x \rightarrow 1} \frac{3 \sin \pi x-\sin 3 \pi x}{x^{3}}$\\
Ans. (a)\\
$\begin{array}{ll}\text { (b) } 0 & \text { (c) } \frac{1}{2}\end{array}$\\
(d) $-1 / \pi$\\
(e) $\frac{2}{7}$\\
(f) $\frac{1}{2}\left(b^{2}-a^{2}\right)$\\
(g) -1\\
(h) $4 \pi^{3}$

3.71. If $\lim _{x \rightarrow 0} \frac{e^{x}-1}{x}=1$, prove that (a) $\lim _{x \rightarrow 0} \frac{e^{-a x}-e^{-b x}}{x}=b-a$, (b) $\lim _{x \rightarrow 0} \frac{a^{x}-b^{x}}{x}=\ln \frac{a}{b}, a, b>0$, and (c) $\lim _{x \rightarrow 0} \frac{\tanh a x}{x}=a$.

3.72. Prove that $\lim _{x \rightarrow x_{0}} f(x)=l$ if and only if $\lim _{x \rightarrow x_{0}^{+}} f(x)=l$.

\section*{Continuity}
In the following problems, assume the largest possible domain unless otherwise stated.

3.73. Prove that $f(x)=x^{2}-3 x+2$ is continuous at $x=4$.

3.74. $\quad$ Prove that $f(x)=1 / x$ is continuous (a) at $x=2$ and (b) in $1 \leqq x \leqq 3$.

3.75. Investigate the continuity of each of the following functions at the indicated points:\\
(a) $f(x)=\frac{\sin x}{x} ; x \neq 0, f(0)=0 ; x=0$\\
(c) $f(x)=\frac{x^{3}-8}{x^{2}-4} ; x \neq 2, f(2)=3 ; x=2$\\
(b) $f(x)=x-|x| ; x=0$\\
(d) $f(x)=\left\{\begin{array}{ll}\sin \pi x, & 0<x<1 \\ \ln & 1<x<2\end{array} ; x=1\right.$

Ans. (a) discontinuous, (b) continuous, (c) continuous, (d) discontinuous

3.76. If $[x]=$ greatest integer $\leqq x$, investigate the continuity of $f(x)=x-[x]$ in the interval (a) $1<x<2$ and (b) $1 \leqq x \leqq 2$.

3.77. Prove that $f(x)=x^{3}$ is continuous in every finite interval.

3.78. If $f(x) / g(x)$ and $g(x)$ are continuous at $x=x_{0}$, prove that $f(x)$ must be continuous at $x=x_{0}$.

3.79. Prove that $f(x)=\left(\tan ^{-1} x\right) / x, f(0)=1$ is continuous at $x=0$.

3.80. Prove that a polynomial is continuous in every finite interval.

3.81. If $f(x)$ and $g(x)$ are polynomials, prove that $f(x) / g(x)$ is continuous at each point $x=x_{0}$ for which $g\left(x_{0}\right) \neq 0$.

3.82. Give the points of discontinuity of each of the following functions.\\
(a) $f(x)=\frac{x}{(x-2)(x-4)}$\\
(c) $f(x)=\sqrt{(x-3)(6-x)}, 3 \leqq x \leqq 6$\\
(b) $f(x)=x^{2} \sin 1 / x, x \neq 0, f(0)=0$\\
(d) $f(x)=\frac{1}{1+2 \sin x}$

Ans. (a) $x=2,4$ (b) none (c) none (d) $x=7 \pi / 6 \pm 2 m \pi, 11 \pi / 6 \pm 2 m \pi, m=0,1,2, \ldots$

\section*{Uniform continuity}
3.83. Prove that $f(x)=x^{3}$ is uniformly continuous in (a) $0<x<2$ (b) $0 \leqq x \leqq 2$ and (c) any finite interval.

3.84. Prove that $f(x)=x^{2}$ is not uniformly continuous in $0<x<\infty$.

3.85. If $a$ is a constant, prove that $f(x)=1 / x^{2}$ is (a) continuous in $a<x<\infty$ if $a \geqq 0$, (b) uniformly continuous in $a<x<\infty$ if $a>0$, and (c) not uniformly continuous in $0<x<1$.

3.86. If $f(x)$ and $g(x)$ are uniformly continuous in the same interval, prove that $(a) f(x) \pm g(x)$ and $(b) f(x) g(x)$ are uniformly continuous in the interval. State and prove an analogous theorem for $f(x) / g(x)$.

\section*{Miscellaneous problems}
3.87. Give an " $\epsilon, \delta$ " proof of the theorem of Problem 3.31.

3.88. (a) Prove that the equation $\tan x=x$ has a real positive root in each of the intervals $\pi / 2<x<3 \pi / 2,3 \pi / 2<x$ $<5 \pi / 2,5 \pi / 2<x<7 \pi / 2, \ldots$

(b) Illustrate the result in (a) graphically by constructing the graphs of $y=\tan x$ and $y=x$ and locating their points of intersection.

(c) Determine the value of the smallest positive root of $\tan x=x$.

Ans. (c) 4.49 approximately

3.89. Prove that the only real solution of $\sin x=x$ is $x=0$.

3.90. (a) Prove that $\cos x \cosh x+1=0$ has infinitely many real roots. (b) Prove that for large values of $x$, the roots approximate those of $\cos x=0$.

3.91. Prove that $\lim _{x \rightarrow 0} \frac{x^{2} \sin (1 / x)}{\sin x}=0$.

3.92. Suppose $f(x)$ is continuous at $x=x_{0}$ and assume $f\left(x_{0}\right)>0$. Prove that there exists an interval $\left(x_{0}-h, x_{0}+h\right)$, where $h>0$, in which $f(x)>0$. (See Theorem 5, page 52.) (Hint: Show that we can make $\left|f(x)-f\left(x_{0}\right)\right|$ $<\frac{1}{2} f\left(x_{0}\right)$. Then show that $f(x) \geqq f\left(x_{0}\right)-\left|f(x)-f\left(x_{0}\right)\right|>\frac{1}{2} f\left(x_{0}\right)>0$.)

3.93. (a) Prove Theorem 10, Page 52, for the greatest lower bound $m$ (see Problem 3.34). (b) Prove Theorem 9, Page 52, and explain its relationship to Theorem 10.

\section*{CHAPTER 4}
\section*{Derivatives}
\section*{The Concept and Definition of a Derivative}
Concepts that shape the course of mathematics are few and far between. The derivative, the fundamental element of the differential calculus, is such a concept. That branch of mathematics called analysis, of which advanced calculus is a part, is the end result. There were two problems that led to the discovery of the derivative. The older one of defining and representing the tangent line to a curve at one of its points had concerned early Greek philosophers. The other problem of representing the instantaneous velocity of an object whose motion was not constant was much more a problem of the seventeenth century. At the end of that century, these problems and their relationship were resolved. As is usually the case, many mathematicians contributed, but it was Isaac Newton and Gottfried Wilhelm Leibniz who independently put together organized bodies of thought upon which others could build.

The tangent problem provides a visual interpretation of the derivative and can be brought to mind no matter what the complexity of a particular application. It leads to the definition of the derivative as the limit of a difference quotient in the following way. (See Figure 4.1.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-082}
\end{center}

Figure 4.1

Let $P_{0}\left(x_{0}\right)$ be a point on the graph of $y=f(x)$. Let $P(x)$ be a nearby point on this same graph of the function $f$. Then the line through these two points is called a secant line. Its slope, $m_{s}$, is the difference quotient


\begin{equation*}
m_{s}=\frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}=\frac{\Delta y}{\Delta x} \tag{1}
\end{equation*}


where $\Delta x$ and $\Delta y$ are called the increments in $x$ and $y$, respectively. Also this slope may be written


\begin{equation*}
m_{s}=\frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \tag{2}
\end{equation*}


where $h=x-x_{0}=\Delta x$. See Figure 4.2.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-083}
\end{center}

Figure 4.2

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-083(1)}
\end{center}

Figure 4.3

We can imagine a sequence of lines formed as $h \rightarrow 0$. It is the limiting line of this sequence that is the natural one to be the tangent line to the graph at $P_{0}$.

To make this mode of reasoning precise, the limit (when it exists), is formed as follows:


\begin{equation*}
f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \tag{3a}
\end{equation*}


As indicated, this limit is given the name $f^{\prime}\left(x_{0}\right)$. It is called the derivative of the function $f$ at its domain value $x_{0}$. If this limit can be formed at each point of a subdomain of the domain of $f$, then $f$ is said to be differentiable on that subdomain and a new function $f^{\prime}$ has been constructed.

This limit concept was not understood until the middle of the nineteenth century. A simple example illustrates the conceptual problem that faced mathematicians from 1700 until that time. Let the graph of $f$ be the parabola $y=x^{2}$; then a little algebraic manipulation yields


\begin{equation*}
m_{s}=\frac{2 x_{0} h+h^{2}}{h}=2 x_{0}+h \tag{3b}
\end{equation*}


Newton, Leibniz, and their contemporaries simply let $h=0$ and said that $2 x_{0}$ was the slope of the tangent line at $P_{0}$. However, this raises the ghost of a $\frac{0}{0}$ form in the middle term. True understanding of the calculus is in the comprehension of how the introduction of something new (the derivative, i.e., the limit of a difference quotient) resolves this dilemma.

Note 1: The creation of new functions from difference quotients is not limited to $f^{\prime}$. If, starting with $f^{\prime}$, the limit of the difference quotient exists, then $f^{\prime \prime}$ may be constructed, and so on.

Note 2: Since the continuity of a function is such a strong property, one might think that differentiability followed. This is not necessarily true, as is illustrated in Figure 4.3.

The following theorem puts the matter in proper perspective.

Theorem: If $f$ is differentiable at a domain value, then it is continuous at that value.

As indicated, the converse of this theorem is not true.

\section*{Right- and Left-Hand Derivatives}
The status of the derivative at endpoints of the domain of $f$, and in other special circumstances, is clarified by the following definitions.

The right-hand derivative of $f(x)$ at $x=x_{0}$ is defined as


\begin{equation*}
f_{+}^{\prime}\left(x_{0}\right)=\lim _{h \rightarrow 0+} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \tag{5}
\end{equation*}


if this limit exists. Note that in this case $h(=\Delta x)$ is restricted only to positive values as it approaches zero.

Similarly, the left-hand derivative of $f(x)$ at $x=x_{0}$ is defined as


\begin{equation*}
f_{-}^{\prime}\left(x_{0}\right)=\lim _{h \rightarrow 0-} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \tag{6}
\end{equation*}


if this limit exists. In this case $h$ is restricted to negative values as it approaches zero.

A function $f$ has a derivative at $x=x_{0}$ if and only if $f_{+}^{\prime}\left(x_{0}\right)=f_{-}^{\prime}\left(x_{0}\right)$.

\section*{Differentiability in an Interval}
If a function has a derivative at all points of an interval, it is said to be differentiable in the interval. In particular, if $f$ is defined in the closed interval $a \leqq x \leqq b$-i.e. $[a, b]$ - then $f$ is differentiable in the interval if and only if $f^{\prime}\left(x_{0}\right)$ exists for each $x_{0}$ such that $a<x_{0}<b$ and if both $f_{+}^{\prime}(a)$ and $f_{-}^{\prime}(b)$ exist.

If a function has a continuous derivative, it is sometimes called continuously differentiable.

\section*{Piecewise Differentiability}
A function is called piecewise differentiable or piecewise smooth in an interval $a \leqq x \leqq b$ if $f^{\prime}(x)$ is piecewise continuous. An example of a piecewise continuous function is shown graphically on Page 47.

An equation for the tangent line to the curve $y=f(x)$ at the point where $x=x_{0}$ is given by


\begin{equation*}
y-f\left(x_{0}\right)=f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right) \tag{7}
\end{equation*}


The fact that a function can be continuous at a point and yet not be differentiable there is shown graphically in Figure 4.3. In this case there are two tangent lines at $P$, represented by $P M$ and $P N$. $P N$. The slopes of these tangent lines are $f_{-}^{\prime}\left(x_{0}\right)$ and $f_{+}^{\prime}\left(x_{0}\right)$, respectively.

\section*{Differentials}
Let $\Delta x=d x$ be an increment given to $x$. Then


\begin{equation*}
\Delta y=f(x+\Delta x)-f(x) \tag{8}
\end{equation*}


is called the increment in $y=f(x)$. If $f(x)$ is continuous and has a continuous first derivative in an interval, then


\begin{equation*}
\Delta y=f^{\prime}(x) \Delta x+\epsilon \Delta x=f^{\prime}(x) d x+d x \tag{9}
\end{equation*}


where $\epsilon \rightarrow 0$ as $\Delta x \rightarrow 0$. The expression


\begin{equation*}
\mathrm{d} y=f^{\prime}(x) d x \tag{10}
\end{equation*}


is called the differential of $y$ or $f(x)$ or the principal part of $\Delta y$. Note that $\Delta y \neq d y$, in general. However, if $\Delta x=d x$ is small, then $d y$ is a close approximation of $\Delta y$ (see Problem 4.11). The quantities $d x$ (called the differential of $x$ ) and $d y$ need not be small.

Because of the definitions given by Equations (8) and (10), we often write


\begin{equation*}
\frac{d y}{d x}=f^{\prime}(x)=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x} \tag{11}
\end{equation*}


It is emphasized that $d x$ and $d y$ are not the limits of $\Delta x$ and $\Delta y$ as $\Delta x \rightarrow 0$, since these limits are zero, whereas $d x$ and $d y$ are not necessarily zero. Instead, given $d x$, we determine $d y$ from Equation (10); i.e., $d y$ is a dependent variable determined from the independent variable $d x$ for a given $x$.

Geometrically, $d y$ is represented in Figure 4.1, for the particular value $x=x_{0}$ by the line segment $S R$, whereas $\Delta y$ is represented by $Q R$.

The geometric interpretation of the derivative as the slope of the tangent line to a curve at one of its points is fundamental to its application. Also of importance is its use as representative of instantaneous velocity in the construction of physical models. In particular, this physical viewpoint may be used to introduce the notion of differentials.

Newton's second and first laws of motion imply that the path of an object is determined by the forces acting on it and that, if those forces suddenly disappear, the object takes on the tangential direction of the path at the point of release. Thus, the nature of the path in a small neighborhood of the point of release becomes of interest. With this thought in mind, consider the following idea.

Suppose the graph of a function $f$ is represented by $y=f(x)$. Let $x=x_{0}$ be a domain value at which $f^{\prime}$ exists (i.e., the function is differentiable at that value). Construct a new linear function

$$
d y=f^{\prime}\left(x_{0}\right) d x
$$

with $d x$ as the (independent) domain variable and $d y$ the range variable generated by this rule. This linear function has the graphical interpretation illustrated in Figure 4.4.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-085}
\end{center}

Figure 4.4

That is, a coordinate system may be constructed with its origin at $P_{0}$ and the $d x$ - and $d y$-axes parallel to the $x$ - and $y$-axes, respectively. In this system our linear equation is the equation of the tangent line to the graph at $P_{0}$. It is representative of the path in a small neighborhood of the point, and if the path is that of an object, the linear equation represents its new path when all forces are released.

$d x$ and $d y$ are called differentials of $x$ and $y$, respectively. Because the preceding linear equation is valid at every point in the domain of $f$ at which the function has a derivative, the subscript may be dropped and we can write

$$
d y=f^{\prime}(x) d x
$$

The following important observations should be made. $\frac{d y}{d x}=f^{\prime}(x)=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x}$, thus $\frac{d y}{d x}$ is not the same thing as $\frac{\Delta y}{\Delta x}$.

On the other hand, $d y$ and $\Delta y$ are related. In particular, $\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x}=f^{\prime}(x)$ means that for any $\epsilon>0$ there exists $\delta>0$ such that $-\varepsilon<\frac{\Delta y}{\Delta x}-\frac{d y}{d x}<\varepsilon$ whenever $|\Delta x|<\delta$. Now $d x$ is an independent variable and the axes of $x$ and $d x$ are parallel; therefore, $d x$ may be chosen equal to $\Delta x$. With this choice,

$$
-\varepsilon \Delta x<\Delta y-d y<\varepsilon \Delta x
$$

or

$$
d y-\varepsilon \Delta x<\Delta y<d y+\varepsilon \Delta x
$$

From this relation we see that $d y$ is an approximation to $\Delta y$ in small neighborhoods of $x, d y$ is called the principal part of $\Delta y$.

The representation of $f^{\prime}$ by $\frac{d y}{d x}$ has an algebraic suggestiveness that is very appealing and appears in much of what follows. In fact, this notation was introduced by Leibniz (without the justification provided by knowledge of the limit idea) and was the primary reason his approach to the calculus, rather than Newton's, was followed.

\section*{The Differentiation of Composite Functions}
Many functions are a composition of simpler ones. For example, if $f$ and $g$ have the rules of correspondence $u$ $=x^{3}$ and $y=\sin u$, respectively, then $y=\sin x^{3}$ is the rule for a composite function $F=g(f)$. The domain of $F$ is that subset of the domain of $F$ whose corresponding range values are in the domain of $g$. The rule of com-

posite function differentiation is called the chain rule and is represented by $\frac{d y}{d x}=\frac{d y}{d u} \frac{d u}{d x}\left[F^{\prime}(x)=g^{\prime}(u) f^{\prime}(x)\right]$.\\
In the example,

$$
\frac{d y}{d x} \equiv \frac{d\left(\sin x^{3}\right)}{d x}=\cos x^{3}\left(3 x^{2} d x\right)
$$

The importance of the chain rule cannot be too greatly stressed. Its proper application is essential in the differentiation of functions, and it plays a fundamental role in changing the variable of integration, as well as in changing variables in mathematical models involving differential equations.

\section*{Implicit Differentiation}
The rule of correspondence for a function may not be explicit. For example, the rule $y=f(x)$ is implicit to the equation $x^{2}+4 x y^{5}+7 x y+8=0$. Furthermore, there is no reason to believe that this equation can be solved for $y$ in terms of $x$. However, assuming a common domain (described by the independent variable $x$ ), the left-hand member of the equation can be construed as a composition of functions and differentiated accordingly. (The rules for differentiation are listed here for your review.)

In this example, differentiation with respect to $x$ yields

$$
2 x+4\left(y^{5}+5 x y^{4} \frac{d y}{d x}\right)+7\left(y+x \frac{d y}{d x}\right)=0
$$

Observe that this equation can be solved for $\frac{d y}{d x}$ as a function of $x$ and $y$ (but not of $x$ alone).

\section*{Rules for Differentiation}
If $f, g$, and $h$ are differentiable functions, the following differentiation rules are valid.

\begin{enumerate}
  \item $\frac{d}{d x}\{f(x)+g(x)\}=\frac{d}{d x} f(x)+\frac{d}{d x} g(x)=f^{\prime}(x)+g^{\prime}(x) \quad$ (Addition rule)

  \item $\frac{d}{d x}\{f(x)-g(x)\}=\frac{d}{d x} f(x)-\frac{d}{d x} g(x)=f^{\prime}(x)-g^{\prime}(x)$

  \item $\frac{d}{d x}\{C f(x)\}=C \frac{d}{d x} f(x)=C f^{\prime}(x)$ where $C$ is any constant

  \item $\quad \frac{d}{d x}\{f(x) g(x)\}=f(x) \frac{d}{d x} g(x)+g(x) \frac{d}{d x} f(x)=f(x) g^{\prime}(x)+g(x) f^{\prime}(x)$

\end{enumerate}

(Product rule)\\
5. $\frac{d}{d x}\left\{\frac{f(x)}{g(x)}\right\}=\frac{g(x) \frac{d}{d x} f(x)-f(x) \frac{d}{d x} g(x)}{[g(x)]^{2}}=\frac{g(x) f^{\prime}(x)-f(x) g^{\prime}(x)}{[g(x)]^{2}}$ if $g(x) \neq 0 \quad$ (Quotient rule)

\begin{enumerate}
  \setcounter{enumi}{5}
  \item If $y=f(u)$ where $u=g(x)$, then
\end{enumerate}


\begin{equation*}
\frac{d y}{d x}=\frac{d y}{d u} \cdot \frac{d u}{d x}=f^{\prime}(u) \frac{d u}{d x}=f^{\prime}\{g(x)\} g^{\prime}(x) \tag{12}
\end{equation*}


Similarly, if $y=f(u)$ where $u=g(v)$ and $v=h(x)$, then


\begin{equation*}
\frac{d y}{d x}=\frac{d y}{d u} \cdot \frac{d u}{d v} \cdot \frac{d v}{d x} \tag{13}
\end{equation*}


The results (12) and (13) are often called chain rules for differentiation of composite functions.

These rules probably are the most misused (or perhaps unused) rules in the application of the calculus.

\begin{enumerate}
  \setcounter{enumi}{6}
  \item If $y=f(x)$ and $x=f^{-1}(y)$, then $d y / d x$ and $d x / d y$ are related by
\end{enumerate}


\begin{equation*}
\frac{d y}{d x}=\frac{1}{d x / d y} \tag{14}
\end{equation*}


\begin{enumerate}
  \setcounter{enumi}{7}
  \item If $x=f(t)$ and $y=g(t)$, then
\end{enumerate}


\begin{equation*}
\frac{d y}{d x}=\frac{d y / d t}{d x / d t}=\frac{g^{\prime}(t)}{f^{\prime}(t)} \tag{15}
\end{equation*}


Similar rules can be formulated for differentials. For example,

$$
\begin{gathered}
d\{f(x)+g(x)\}=d f(x)+d g(x)=f^{\prime}(x) d x+g^{\prime}(x) d x=\left\{f^{\prime}(x)+g^{\prime}(x)\right\} d x \\
d\{f(x) g(x)\}=f(x) \operatorname{dg}(x)+d f(x)=\left\{f(x) g^{\prime}(x)+g(x) f^{\prime}(x)\right\} d x
\end{gathered}
$$

\section*{Derivatives of Elementary Functions}
In the following we assume that $u$ is a differentiable function of $x$; if $u=x, d u / d x=1$. The inverse functions are defined according to the principal values given in Chapter 3.

\begin{enumerate}
  \item $\frac{d}{d x}(C)=0$

  \item $\frac{d}{d x} u^{n}=n u^{n-1} \frac{d u}{d x}$

  \item $\frac{d}{d x} \sin u=\cos u \frac{d u}{d x}$

  \item $\frac{d}{d x} \cos u=-\sin u \frac{d u}{d x}$

  \item $\frac{d}{d x} \tan u=\sec ^{2} u \frac{d u}{d x}$

  \item $\frac{d}{d x} \cot u=-\csc ^{2} u \frac{d u}{d x}$

  \item $\frac{d}{d x} \sec u=\sec u \tan u \frac{d u}{d x}$

  \item $\frac{d}{d x} \cot ^{-1} u=-\frac{1}{1+u^{2}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \sec ^{-1} u= \pm \frac{1}{u \sqrt{u^{2}-1}} \frac{d u}{d x}\left\{\begin{array}{l}+ \text { if } u>1 \\ - \text { if } u<-1\end{array}\right.$

  \item $\frac{d}{d x} \csc ^{-1} u=\mp \frac{1}{u \sqrt{u^{2}-1}} \frac{d u}{d x}\left\{\begin{array}{l}- \text { if } u>1 \\ + \text { if } u<-1\end{array}\right.$

  \item $\frac{d}{d x} \sinh u=\cosh u \frac{d u}{d x}$

  \item $\frac{d}{d x} \cosh u=\sinh u \frac{d u}{d x}$

  \item $\frac{d}{d x} \tanh u=\sec ^{2} u \frac{d u}{d x}$

  \item $\frac{d}{d x} \operatorname{coth} u=-\operatorname{csch}^{2} u \frac{d u}{d x}$

  \item $\frac{d}{d x} \csc u=-\csc u \cot u \frac{d u}{d x}$

  \item $\frac{d}{d x} \log _{a} u=\frac{\log _{a} e}{u} \frac{d u}{d x} \quad a>0, a \neq 1$

  \item $\frac{d}{d x} \log _{e} u=\frac{d}{d x} \ln u=\frac{1}{u} \frac{d u}{d x}$

  \item $\frac{d}{d x} a^{u}=a^{u} \ln a \frac{d u}{d x}$

  \item $\frac{d}{d x} e^{u}=e^{u} \frac{d u}{d x}$

  \item $\frac{d}{d x} \sin ^{-1} u=\frac{1}{\sqrt{1-u^{2}}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \cos ^{-1} u=-\frac{1}{\sqrt{1-u^{2}}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \tan ^{-1} u=\frac{1}{\sqrt{1+u^{2}}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \operatorname{sech} u=-\operatorname{sech} u \tanh u \frac{d u}{d x}$

  \item $\frac{d}{d x} \operatorname{csch} u=-\operatorname{csch} u \operatorname{coth} u \frac{d u}{d x}$

  \item $\frac{d}{d x} \sinh ^{-1} u=\frac{1}{\sqrt{1+u^{2}}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \cosh ^{-1} u=\frac{1}{\sqrt{u^{2}-1}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \tanh ^{-1} u=\frac{1}{1-u^{2}} \frac{d u}{d x}, \quad|u|<1$

  \item $\frac{d}{d x} \operatorname{coth}^{-1} u=\frac{1}{1-u^{2}} \frac{d u}{d x}, \quad|u|>1$

  \item $\frac{d}{d x} \operatorname{sech}^{-1} u=\frac{1}{u \sqrt{1-u^{2}}} \frac{d u}{d x}$

  \item $\frac{d}{d x} \operatorname{csch}^{-1} u=-\frac{1}{u \sqrt{u^{2}+1}} \frac{d u}{d x}$

\end{enumerate}

\section*{Higher-Order Derivatives}
If $f(x)$ is differentiable in an interval, its derivative is given by $f^{\prime}(x), y^{\prime}$ or $d y / d x$, where $y=f(x)$. If $f^{\prime}(x)$ is also differentiable in the interval, its derivative is denoted by $f^{\prime \prime}(x), y^{\prime \prime}$ or $\frac{d}{d x}\left(\frac{d y}{d x}\right)=\frac{d^{2} y}{d x^{2}}$. Similarly, the $n$th derivative of $f(x)$, if it exists, is denoted by $f^{(n)}(x), y^{(n)}$ or $\frac{d^{n} y}{d x^{n}}$, where $n$ is called the order of the derivative. Thus, derivatives of the first, second, third, $\ldots$ orders are given by $f^{\prime}(x), f^{\prime \prime}(x), f^{\prime \prime \prime}(x), \ldots$ here.

Computation of higher-order derivatives follows by repeated application of the differentiation rules given

\section*{Mean Value Theorems}
These theorems are fundamental to the rigorous establishment of numerous theorems and formulas. (See Figure 4.5.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-088}
\end{center}

Figure 4.5

\begin{enumerate}
  \item Rolle's theorem. If $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$ and if $f(a)=f(b)=0$, then there exists a point $\xi$ in $(a, b)$ such that $f^{\prime}(\xi)=0$.
\end{enumerate}

Rolle's theorem is employed in the proof of the mean value theorem. It then becomes a special case of that theorem.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The mean value theorem. If $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$, then there exists a point $\xi$ in $(a, b)$ such that
\end{enumerate}


\begin{equation*}
\frac{f(b)-f(a)}{b-a}=f^{\prime}(\xi) \quad a<\xi<b \tag{16}
\end{equation*}


Rolle's theorem is the special case of this where $f(a)=f(b)=0$.

The result (16) can be written in various alternative forms; for example, if $x$ and $x_{0}$ are in $(a, b)$, then


\begin{equation*}
f(x)=f\left(x_{0}\right)+f^{\prime}(\xi)\left(x-x_{0}\right) \quad \xi \text { between } x_{0} \text { and } x \tag{17}
\end{equation*}


We can also write result (16) with $b=a+h$, in which case $\xi=a+\theta h$, where $0<\theta<1$.

The mean value theorem is also called the law of the mean.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Cauchy's generalized mean value theorem. If $f(x)$ and $g(x)$ are continuous in $[a, b]$ and differentiable in $(a, b)$, then there exists a point $\xi$ in $(a, b)$ such that
\end{enumerate}


\begin{equation*}
\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} \quad a<\xi<b \tag{18}
\end{equation*}


where we assume $g(a) \neq g(b)$ and $f^{\prime}(x), g^{\prime}(x)$ are not simultaneously zero. Note that the special case $g(x)=x$ yields (16).

\section*{L'Hospital's Rules}
If $\lim _{x \rightarrow x_{0}} f(x)=A$ and $\lim _{x \rightarrow x_{0}} g(x)=B$, where $A$ and $B$ are either both zero or both infinite, $\lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}$ is often called an indeterminate of the form $0 / 0$ or $\infty / \infty$, respectively, although such terminology is somewhat misleading since there is usually nothing indeterminate involved. The following theorems, called L'Hospital's rules, facilitate evaluation of such limits.

\begin{enumerate}
  \item If $f(x)$ and $g(x)$ are differentiable in the interval $(a, b)$ except possibly at a point $x_{0}$ in this interval, and if $g^{\prime}(x) \neq 0$ for $x \neq x_{0}$, then
\end{enumerate}


\begin{equation*}
\lim _{x \rightarrow x_{0}} \frac{f(x)}{g(x)}=\lim _{x \rightarrow x_{0}} \frac{f^{\prime}(x)}{g^{\prime}(x)} \tag{19}
\end{equation*}


whenever the limit on the right can be found. In case $f^{\prime}(x)$ and $g^{\prime}(x)$ satisfy the same conditions as $f(x)$ and $g(x)$ given above, the process can be repeated.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If $\lim _{x \rightarrow x_{0}} f(x)=\infty$ and $\lim _{x \rightarrow x_{0}} g(x)=\infty$, the result (19) is also valid.
\end{enumerate}

These can be extended to cases where $x \rightarrow \infty$ or $-\infty$, and to cases where $x_{0}=a$ or $x_{0}=b$ in which only one-sided limits, such as $x \rightarrow a+$ or $x \rightarrow b-$, are involved.

Limits represented by the indeterminate forms $0 \cdot \infty, \infty^{0}, 0^{0}, 1^{\infty}$, and $\infty-\infty$ can be evaluated on replacing them by equivalent limits for which the aforementioned rules are applicable (see Problem 4.29).

\section*{Applications}
\section*{Relative Extrema and Points of Inflection}
See Chapter 3, where relative extrema and points of inflection are described and a diagram is presented. In this chapter such points are characterized by the variation of the tangent line and then by the derivative, which represents the slope of that line.

Assume that $f$ has a derivative at each point of an open interval and that $P_{1}$ is a point of the graph of $f$ associated with this interval. Let a varying tangent line to the graph move from left to right through $P_{1}$. If the point is a relative minimum, then the tangent line rotates counterclockwise. The slope is negative to the left of $P_{1}$ and positive to the right. At $P_{1}$ the slope is zero. At a relative maximum a similar analysis can be made except that the rotation is clockwise and the slope varies from positive to negative. Because $f^{\prime \prime}$ designates the change of $f^{\prime}$, we can state the following theorem. (See Figure 4.6.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-090}
\end{center}

Counterclockwise rotating tangent

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-090(1)}
\end{center}

Clockwise rotating tangent

Figure 4.6

Theorem Assume that $x_{1}$ is a number in an open set of the domain of $f$ at which $f^{\prime}$ is continuous and $f^{\prime \prime}$ is defined. If $f^{\prime}\left(x_{1}\right)=0$ and $f^{\prime \prime}\left(x_{1}\right) \neq 0$, then $f\left(x_{1}\right)$ is a relative extreme of $f$. Specifically:

(a) If $f^{\prime \prime}\left(x_{1}\right)>0$, then $f\left(x_{1}\right)$ is a relative minimum.

(b) If $f^{\prime \prime}\left(x_{1}\right)<0$, then $f\left(x_{1}\right)$ is a relative maximum.

(The domain value $x_{1}$ is called a critical value.)

This theorem may be generalized in the following way. Assume existence and continuity of derivatives as needed and suppose that $f^{\prime}\left(x_{1}\right)=f^{\prime \prime}\left(x_{1}\right)=\ldots f^{(2 p-1)}\left(x_{1}\right)=0$ and $f^{(2 p)}\left(x_{1}\right) \neq 0$ ( $p$ a positive integer). Then:

(a) $f$ has a relative minimum at $x_{1}$ if $f^{(2 p)}\left(x_{1}\right)>0$.

(b) $f$ has a relative maximum at $x_{1}$ if $f^{(2 p)}\left(x_{1}\right)<0$.

(Notice that the order of differentiation in each succeeding case is two greater. The nature of the intermediate possibilities is suggested in the next paragraph.)

It is possible that the slope of the tangent line to the graph of $f$ is positive to the left of $P_{1}$, zero at the point, and again positive to the right. Then $P_{1}$ is called a point of inflection. In the simplest case this point of inflection is characterized by $f^{\prime}\left(x_{1}\right)=0, f^{\prime \prime}\left(x_{1}\right)=0$, and $f^{\prime \prime \prime}\left(x_{1}\right) \neq 0$.

\section*{Particle Motion}
The fundamental theories of modern physics are relativity, electromagnetism, and quantum mechanics. Yet Newtonian physics must be studied because it is basic to many of the concepts in these other theories, and because it is most easily applied to many of the circumstances found in everyday life. The simplest aspect of Newtonian mechanics is called kinematics, or the geometry of motion. In this model of reality, objects are idealized as points and their paths are represented by curves. In the simplest (one-dimensional) case, the curve is a straight line, and it is the speeding up and slowing down of the object that is of importance. The calculus applies to the study in the following way.

If $x$ represents the distance of a particle from the origin and $t$ signifies time, then $x=f(t)$ designates the position of a particle at time $t$. Instantaneous velocity (or speed in the one-dimensional case) is represented\\
by $\frac{d x}{d t}=\lim _{\Delta t \rightarrow 0} \frac{f(t+\Delta t)}{\Delta t}$ (the limiting case of the formula change in $\frac{\text { change in distance }}{\text { change in time }}$ in time for speed when the motion is constant). Furthermore, the instantaneous change in velocity is called acceleration and represented by $\frac{d^{2} x}{d t^{2}}$.

Path, velocity, and acceleration of a particle will be represented in three dimensions in Chapter 7, on vectors.

\section*{Newton's Method}
It is difficult or impossible to solve algebraic equations of higher degree than two. In fact, it has been proved that there are no general formulas representing the roots of algebraic equations of degree five and higher in terms of radicals. However, the graph $y=f(x)$ of an algebraic equation $f(x)=0$ crosses the $x$ axis at each singlevalued real root. Thus, by trial and error, consecutive integers can be found between which a root lies. Newton's method is a systematic way of using tangents to obtain a better approximation of a specific real root. The procedure is as follows. (See Figure 4.7.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-091}
\end{center}

Figure 4.7

Suppose that $f$ has as many derivatives as required. Let $r$ be a real root of $f(x)=0$; i.e., $f(r)=0$. Let $x_{0}$ be a value of $x$ near $r$-for example, the integer preceding or following $r$. Let $f^{\prime}\left(x_{0}\right)$ be the slope of the graph of $y=f(x)$ at $P_{0}\left[x_{0}, f\left(x_{0}\right)\right]$. Let $Q_{1}\left(x_{1}, 0\right)$ be the $x$-axis intercept of the tangent line at $P_{0}$; then

$$
\frac{0-f\left(x_{0}\right)}{x-x_{0}}=f^{\prime}\left(x_{0}\right)
$$

where the two representations of the slope of the tangent line have been equated. The solution of this relation for $x_{1}$ is

$$
x_{1}=x_{0}-\frac{f\left(x_{0}\right)}{f^{\prime}\left(x_{0}\right)}
$$

Starting with the tangent line to the graph at $P_{1}\left[x_{1}, f\left(x_{1}\right)\right]$ and repeating the process, we get

$$
x_{2}=x_{1}-\frac{f\left(x_{1}\right)}{f^{\prime}\left(x_{1}\right)}=x_{0}-\frac{f\left(x_{0}\right)}{f^{\prime}\left(x_{0}\right)}-\frac{f\left(x_{1}\right)}{f^{\prime}\left(x_{1}\right)}
$$

and, in general,

$$
x_{n}=x_{0}-\sum_{k=0}^{n} \frac{f\left(x_{k}\right)}{f^{\prime}\left(x_{k}\right)}
$$

Under appropriate circumstances, the approximation $x_{n}$ to the root $r$ can be made as good as desired.

Note: Success with Newton's method depends on the shape of the function's graph in the neighborhood of the root. There are various cases which have not been explored here.

\section*{SOLVED PROBLEMS}
\section*{Derivatives}
4.1. (a) Let $f(x)=\frac{3+x}{3-x}, x \neq 3$. Evaluate $f^{\prime}(2)$ from the definition.

$$
f^{\prime}(2)=\lim _{h \rightarrow 0} \frac{f(2+h)-f(2)}{h}=\lim _{h \rightarrow 0} \frac{1}{h}\left(\frac{5+h}{1-h}-5\right)=\lim _{h \rightarrow 0} \frac{1}{h} \cdot \frac{6 h}{1-h}=\lim _{h \rightarrow 0} \frac{6}{1-h}=6
$$

Note: By using rules of differentiation we find

$$
f^{\prime}(x)=\frac{(3-x) \frac{d}{d x}(3+x)-(3+x) \frac{d}{d x}(3-x)}{(3-x)^{2}}=\frac{(3-x)(1)-(3+x)(-1)}{(3-x)^{2}}=\frac{6}{(3-x)^{2}}
$$

at all points $x$ where the derivative exists. Putting $x=2$, we find $f^{\prime}(2)=6$. Although such rules are often useful, one must be careful not to apply them indiscriminately (see Problem 4.5).

(b) Let $f(x)=\sqrt{2 x-1}$. Evaluate $f^{\prime}(5)$ from the definition.

$$
\begin{aligned}
f^{\prime}(5) & =\lim _{h \rightarrow 0} \frac{f(5+h)-f(5)}{h}=\lim _{h \rightarrow 0} \frac{\sqrt{9+2 h}-3}{h} \\
& =\lim _{h \rightarrow 0} \frac{\sqrt{9+2 h}-3}{h} \cdot \frac{\sqrt{9+2 h}+3}{\sqrt{9+2 h}+3}=\lim _{h \rightarrow 0} \frac{9+2 h-9}{h(\sqrt{9+2 h}+3}=\lim _{h \rightarrow 0} \frac{2}{\sqrt{9+2 h}+3}=\frac{1}{3}
\end{aligned}
$$

By using rules of differentiation we find $f^{\prime}(x)=\frac{d}{d x}(2 x-1)^{1 / 2}=\frac{1}{2}(2 x-1)^{-1 / 2} \frac{d}{d x}(2 x-1)=$ $(2 x-1)^{-1 / 2}$. Then $f^{\prime}(5)=9^{-1 / 2}=\frac{1}{3}$.

4.2. (a) Show directly from definition that the derivative of $f(x)=x^{3}$ is $3 x^{2}$.

(b) Show from definition that $\left.\frac{d}{d x} \sqrt{x}\right)=\frac{1}{2 \sqrt{x}}$.

(a) $\frac{f(x+h)-f(x)}{h}=\frac{1}{h}\left[(x+h)^{3}-x^{3}\right]$

$$
\left.=\frac{1}{h}\left[x^{3}+3 x^{2} h+3 x h^{2}+h^{2}\right]-x^{3}\right]=3 x^{2}+3 x h+h^{2}
$$

Then

$$
f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}=3 x^{2}
$$

(b) $\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}=\lim _{h \rightarrow 0} \frac{\sqrt{x+h}-\sqrt{x}}{h}$

The result follows by multiplying numerator and denominator by $\sqrt{x+h}-\sqrt{x}$ and then letting $h \rightarrow 0$.

4.3. If $f(x)$ has a derivative at $x=x_{0}$, prove that $f(x)$ must be continuous at $x=x_{0}$.

$$
f\left(x_{0}+h\right)-f\left(x_{0}\right)=\frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \cdot h, \quad h \neq 0
$$

Then

$$
\lim _{h \rightarrow 0} f\left(x_{0}+h\right)-f\left(x_{0}\right)=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} \cdot \lim _{h \rightarrow 0} h=f^{\prime}\left(x_{0}\right) \cdot 0=0
$$

since $f^{\prime}\left(x_{0}\right)$ exists by hypothesis. Thus,

$$
\lim _{h \rightarrow 0} f\left(x_{0}+h\right)-f\left(x_{0}\right)=0 \quad \text { or } \quad \lim _{h \rightarrow 0} f\left(x_{0}+h\right)=f\left(x_{0}\right)
$$

showing that $f(x)$ is continuous at $x=x_{0}$.

4.4. Let $f(x)=\left\{\begin{array}{ll}x \sin 1 / x, & x \neq 0 \\ 0, & x=0\end{array}\right.$.

(a) Is $f(x)$ continuous at $x=0$ ? (b) Does $f(x)$ have a derivative at $x=0$ ?

(a) By Problem 3.22(b), $f(x)$ is continuous at $x=0$.

(b) $f^{\prime}(0)=\lim _{h \rightarrow 0} \frac{f(0+h)-f(0)}{h}=\lim _{h \rightarrow 0} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0} \frac{h \sin 1 / h-0}{h}=\lim _{h \rightarrow 0} \sin \frac{1}{h}$

which does not exist.

This example shows that even though a function is continuous at a point, it need not have a derivative at the point; i.e., the converse of the theorem in Problem 4.3 is not necessarily true.

It is possible to construct a function which is continuous at every point of an interval but has a derivative nowhere.

4.5. Let $f(x)=\left\{\begin{array}{ll}x^{2} \sin 1 / x, & x \neq 0 \\ 0, & x=0\end{array}\right.$.

(a) Is $f(x)$ differentiable at $x=0$ ? (b) Is $f^{\prime}(x)$ continuous at $x=0$ ?

(a) $\quad f^{\prime}(0)=\lim _{h \rightarrow 0} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0} \frac{h^{2} \sin 1 / h-0}{h}=\lim _{h \rightarrow 0} h \sin \frac{1}{h}=0$

by Problem 3.13. Then $f(x)$ has a derivative (is differentiable) at $x=0$ and its value is 0 .

(b) From elementary calculus differentiation rules, if $x \neq 0$,

$$
\begin{aligned}
f^{\prime}(x) & =\frac{d}{d x}\left(x^{2} \sin \frac{1}{x}\right)=x^{2} \frac{d}{d x}\left(\sin \frac{1}{x}\right)+\left(\sin \frac{1}{x}\right) \frac{d}{d x}\left(x^{2}\right) \\
& =x^{2}\left(\cos \frac{1}{x}\right)\left(-\frac{1}{x^{2}}\right)+\left(\sin \frac{1}{x}\right)(2 x)=-\cos \frac{1}{x}+2 x \sin \frac{1}{x}
\end{aligned}
$$

Since $\lim _{x \rightarrow 0} f^{\prime}(x)=\lim _{x \rightarrow 0}\left(-\cos \frac{1}{x}+2 x \sin \frac{1}{x}\right)$ does not exist (because $\lim _{x \rightarrow 0} \cos 1 / x$ does not exist). $f^{\prime}(x)$ cannot be continuous at $x=0$ in spite of the fact that $f^{\prime}(0)$ exists.

This shows that we cannot calculate $f^{\prime}(0)$ in this case by simply calculating $f^{\prime}(x)$ and and putting $x=0$, as is frequently supposed in elementary calculus. It is only when the derivative of a function is continuous at a point that this procedure gives the right answer. This happens to be true for most functions arising in elementary calculus.

4.6. Present an " $\epsilon, \delta$ " definition of the derivative of $f(x)$ at $x=x_{0}$.

$f(x)$ has a derivative $f^{\prime}\left(x_{0}\right)$ at $x=x_{0}$ if, given any $\epsilon>0$, we can find $\delta>0$ such that

$$
\left|\frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}-f^{\prime}\left(x_{0}\right)\right|<\varepsilon \quad \text { when } \quad 0<|h|<\delta
$$

\section*{Right- and left-hand derivatives}
4.7. Let $f(x)=|x|$. (a) Calculate the right-hand derivatives of $f(x)$ at $x=0$. (b) Calculate the left-hand derivative of $f(x)$ at $x=0$. (c) Does $f(x)$ have a derivative at $x=0$ ? (d) Illustrate the conclusions in (a), (b), and (c) from a graph.

(a) $f_{+}^{\prime}(0)=\lim _{h \rightarrow 0+} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0+} \frac{|h|-0}{h}=\lim _{h \rightarrow 0+} \frac{h}{h}=1$

since $|h|=-h$ for $h>0$.

(b) $f_{-}^{\prime}(0)=\lim _{h \rightarrow 0-} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0-} \frac{|h|-0}{h}=\lim _{h \rightarrow 0-} \frac{-h}{h}=-1$

since $|h|=-h$ for $h<0$.

(c) No. The derivative at 0 does not exist if the right- and lefthand derivatives are unequal.

(d) The required graph is shown in Figure 4.8. Note that the slopes of the lines $y=x$ and $y=-x$ are 1 and -1 , respectively, representing the right- and left-hand derivatives at $x=0$. However, the derivative at $x=0$ does not exist.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-094}
\end{center}

Figure 4.8

4.8. Prove that $f(x)=x^{2}$ is differentiable in $0 \leqq x \leqq 1$.

Let $x_{0}$ be any value such that $0<x_{0}<1$. Then

$$
f^{\prime}\left(x_{0}\right)=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0} \frac{\left(x_{0}+h\right)^{2}-x_{0}^{2}}{h}=\lim _{h \rightarrow 0}\left(2 x_{0}+h\right)=2 x_{0}
$$

At the endpoint $x=0$,

$$
f_{+}^{\prime}(0)=\lim _{h \rightarrow 0+} \frac{f(0+h)-f(0)}{h}=\lim _{h \rightarrow 0+} \frac{h^{2}-0}{h}=\lim _{h \rightarrow 0+} h=0
$$

At the end point $x=1$,

$$
f_{-}^{\prime}(1)=\lim _{h \rightarrow 0-} \frac{f(1+h)-f(1)}{h}=\lim _{h \rightarrow 0-} \frac{(1+h)^{2}-1}{h}=\lim _{h \rightarrow 0-}(2+h)=2
$$

Then $f(x)$ is differentiable in $0 \leqq x \leqq 1$. We may write $f^{\prime}(x)=2 x$ for any $x$ in this interval. It is customary to write $f_{+}^{\prime}(0)=f^{\prime}(0)$ and $f_{-}^{\prime}(1)=f^{\prime}(1)$ in this case.

4.9. Find an equation for the tangent line to $y=x^{2}$ at the point where (a) $x=1 / 3$ and (b) $x=1$.

(a) From Problem 4.8. $f^{\prime}\left(x_{0}\right)=2 x_{0}$ so that $f^{\prime}(1 / 3)=2 / 3$. Then the equation of the tangent line is

$$
y-f\left(x_{0}\right)=f\left(x_{0}\right)\left(x-x_{0}\right) \quad \text { or } \quad y-\frac{1}{9}=\frac{2}{3}\left(x-\frac{1}{3}\right) \text {. i.e., } \mathrm{y}=\frac{2}{3} x-\frac{1}{9}
$$

(b) As in part $(a), y-f(1)=f^{\prime}(1)(x-1)$ or $y-1=2(x-1)$, i.e., $y=2 x-1$.

\section*{Differentials}
4.10. If $y=f(x)=x^{3}-6 x$, find (a) $\Delta y$, (b) $d y$, and (c) $\Delta y-d y$.

(a) $\Delta y=f(x+\Delta x)-f(x)=\left\{(x+\Delta x)^{3}-6(x+\Delta x)\right\}-\left\{x^{3}-6 x\right\}$

$$
\begin{aligned}
& =x^{3}+3 x^{2} \Delta x+3 x(\Delta x)^{2}+(\Delta x)^{3}-6 x-6 \Delta x-x^{3}+6 x \\
& =\left(3 x^{2}-6\right) \Delta x+3 x(\Delta x)^{2}+(\Delta x)^{3}
\end{aligned}
$$

(b) $d y=$ principal part of $\Delta y=\left(3 x^{2}-6\right) \Delta x=\left(3 x^{2}-6\right) d x$, since by definition $\Delta x=d x$.

Note that $f^{\prime}(x)=3 x^{2}-6$ and $d y=\left(3 x^{2}-6\right) d x$, i.e.; $d y / d x=3 x^{2}-6$. It must be emphasized that $d y$ and $d x$ are not necessarily small.

(c) From (a) and (b), $\Delta y-d y=3 x(\Delta x)^{2}+(\Delta x)^{3}=\epsilon \Delta x$, where $\epsilon=3 x \Delta x+(\Delta x)^{2}$.

Note that $\epsilon \rightarrow 0$ as $\Delta x \rightarrow 0$; i.e., $\frac{\Delta y-d y}{\Delta x} \rightarrow 0$ as $\Delta x \rightarrow 0$. Hence, $\Delta y-d y$ is an infinitesimal of higher order than $\Delta x$ (see Problem 4.83).

In case $\Delta x$ is small, $d y$ and $\Delta y$ are approximately equal.

4.11. Evaluate $\sqrt[3]{25}$ approximately by use of differentials.

If $\Delta x$ is small, $\Delta y=f(x+\Delta x)-f(x)=f^{\prime}(x) \Delta x$ approximately.

Let $f(x)=\sqrt[3]{x}$. Then $\sqrt[3]{x+\Delta x}-\sqrt[3]{x} \approx \frac{1}{3} x^{-2 / 3} \Delta x$ (where $\approx$ denotes approximately equal to).

If $x=27$ and $\Delta x=-2$, we have

$$
\sqrt[3]{27-2}-\sqrt[3]{27} \approx \frac{1}{3}(27)^{-2 / 3}(-2), \quad \text { i.e., } \sqrt[3]{25}-3 \approx-2 / 27
$$

Then $\sqrt[3]{25} \approx 3-2 / 27$ or 2.926 .

It is interesting to observe that $(2.926)^{3}=25.05$, so the approximation is fairly good.

\section*{Differentiation rules: differentiation of elementary functions}
4.12. Prove the formula $\frac{d}{d x}\{f(x) g(x)\}=f(x) \frac{d}{d x} g(x)+g(x) \frac{d}{d x} f(x)$, assuming $f$ and $g$ are differentiable. By definition,

$$
\begin{aligned}
\frac{d}{d x}\{f(x) g(x)\} & =\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x) g(x+\Delta x)-f(x) g(x)}{\Delta x} \\
& =\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)\{g(x+\Delta x)-g(x)\}+g(x)\{f(x+\Delta x)-f(x)\}}{\Delta x} \\
& =\lim _{\Delta x \rightarrow 0} f(x+\Delta x)\left\{\frac{g(x+\Delta x)-g(x)}{\Delta x}\right\}+\lim _{\Delta x \rightarrow 0} g(x)\left\{\frac{f(x+\Delta x)-f(x)}{\Delta x}\right\} \\
& =f(x) \frac{d}{d x} g(x)+g(x) \frac{d}{d x} f(x)
\end{aligned}
$$

\section*{Another method:}
Let $u=f(x), v=g(x)$. Then $\Delta u=f(x+\Delta x)-f(x)$ and $\Delta v=g(x+\Delta x)-g(x)$; i.e., $f(x+\Delta x)=u+\Delta u$, $g(x+\Delta x)=v+\Delta v$. Thus,

$$
\begin{aligned}
\frac{d}{d x} u v & =\lim _{\Delta x \rightarrow 0} \frac{(u+\Delta u)(v+\Delta v)-w}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{u \Delta v+v \Delta u+\Delta u \Delta v}{\Delta x} \\
& =\lim _{\Delta x \rightarrow 0}\left(u \frac{\Delta v}{\Delta x}+v \frac{\Delta u}{\Delta x}+\frac{\Delta u}{\Delta x} \Delta v\right)=u \frac{d v}{d x}+v \frac{d u}{d x}
\end{aligned}
$$

where it is noted that $\Delta v \rightarrow 0$ as $\Delta x \rightarrow 0$, since $v$ is supposed differentiable and thus continuous.

4.13. If $y=f(u)$ where $u=g(x)$, prove that $\frac{d y}{d x}=\frac{d y}{d u} \cdot \frac{d u}{d x}$, assuming that $f$ and $g$ are differentiable.

Let $x$ be given an increment $\Delta x \neq 0$. Then, as a consequence, $u$ and $y$ take on increments $\Delta u$ and $\Delta y$, respectively, where


\begin{equation*}
\Delta y=f(u+\Delta u)-f(u), \quad \Delta u=g(x+\Delta x)-g(x) \tag{1}
\end{equation*}


Note that as $\Delta x \rightarrow 0, \Delta y \rightarrow 0$ and $\Delta u \rightarrow 0$.

If $\Delta u \neq 0$, let us write $\in=\frac{\Delta y}{\Delta u}-\frac{d y}{d u}$ so that $\epsilon \rightarrow 0$ as $\Delta u \rightarrow 0$ and


\begin{equation*}
\Delta y=\frac{d y}{d u} \Delta u+\in \Delta u \tag{2}
\end{equation*}


If $\Delta u=0$ for values of $\Delta x$, then Equation (1) shows that $\Delta y=0$ for these values of $\Delta x$. For such cases, we define $\epsilon=0$.

It follows that in both cases. $\Delta u \neq 0$ or $\Delta u=0$, Equation (2) holds. Dividing Equation (2) by $\Delta x \neq 0$ and taking the limit as $\Delta x \rightarrow 0$, we have


\begin{align*}
\frac{d y}{d x} & =\lim _{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x}=\lim _{\Delta x \rightarrow 0}\left(\frac{d y}{d u} \frac{\Delta u}{\Delta x}+\in \frac{\Delta u}{\Delta x}\right)=\frac{d y}{d u} \cdot \lim _{\Delta x \rightarrow 0} \frac{\Delta u}{\Delta x}+\lim _{\Delta x \rightarrow 0} \in \cdot \lim _{\Delta x \rightarrow 0} \frac{\Delta u}{\Delta x}  \tag{3}\\
& =\frac{d y}{d u} \frac{d u}{d x}+0 \cdot \frac{d u}{d x} \frac{d y}{d u} \cdot \frac{d u}{d x}
\end{align*}


4.14. Given $\frac{d}{d x}(\sin x)=\cos x$ and $\frac{d}{d x}(\cos x)=-\sin x$, derive the following formulas:\\
(a) $\frac{d}{d x}(\tan x)=\sec ^{2} x$\\
(b) $\frac{d}{d x}\left(\sin ^{-1} x\right)=\frac{1}{\sqrt{1-x^{2}}}$\\
(a) $\frac{d}{d x}(\tan x)=\frac{d}{d x}\left(\frac{\sin x}{\cos x}\right)=\frac{\cos x \frac{d}{d x}(\sin x)-\sin x \frac{d}{d x}}{\cos ^{2} x}$

$$
=\frac{(\cos x)(\cos x)-(\sin x)(-\sin x)}{\cos ^{2} x}=\frac{1}{\cos ^{2} x}={ }^{2} x
$$

(b) If $y=\sin ^{-1} x$, then $x=\sin y$. Taking the derivative with respect to $x$,

$$
1=\cos y \frac{d y}{d x} \quad \text { or } \quad \frac{d y}{d x}=\frac{1}{\cos y}=\frac{1}{\sqrt{1-\sin ^{2} y}}=\frac{1}{\sqrt{1-x^{2}}}
$$

We have supposed here that the principal value $-\pi / 2 \leqq \sin ^{-1} x \leqq \pi / 2$ is chosen so that $\cos y$ is positive, thus accounting for our writing $\cos y=\sqrt{1-\sin ^{2} y}$ rather than $\cos y= \pm \sqrt{1-\sin ^{2} y}$.

4.15. Derive the formula $\frac{d}{d x}\left(\log _{a} u\right)=\frac{\log _{a} e}{u} \frac{d u}{d x}(a>0, a \neq 1)$, where $u$ is a differentiable function of $x$.

Consider $y=f(u)=\log _{a} u$. By definition,

$$
\begin{aligned}
\frac{d y}{d u} & =\lim _{\Delta u \rightarrow 0} \frac{f(u+\Delta u)-f(u)}{\Delta u}=\lim _{\Delta u \rightarrow 0} \frac{\log _{a}(u+\Delta u)-\log _{a} u}{\Delta u} \\
& =\lim _{\Delta u \rightarrow 0} \frac{1}{\Delta u} \log _{a}\left(\frac{u+\Delta u}{u}\right)=\lim _{\Delta u \rightarrow 0} \frac{1}{u} \log _{a}\left(1+\frac{\Delta u}{u}\right)^{u / \Delta u}
\end{aligned}
$$

Since the logarithm is a continuous function, this can be written

$$
\frac{1}{u} \log _{a}\left\{\lim _{\Delta u \rightarrow 0}\left(1+\frac{\Delta u}{u}\right)^{u / \Delta u}\right\}=\frac{1}{u} \log _{a} e
$$

by Problem 2.19, with $x=u / \Delta u$.

Then by Problem 4.13, $\frac{d}{d x}\left(\log _{a} u\right)=\frac{\log _{a} e}{u} \frac{d u}{d x}$.

4.16. Calculate $d y / d x$ if (a) $x y^{3}-3 x^{2}=x y+5$ and (b) $e^{x y}+y \ln x=\cos 2 x$.

(a) Differentiate with respect to $x$, considering $y$ as a function of $x$. (We sometimes say that $y$ is an implicit function of $x$, since we cannot solve explicitly for $y$ in terms of $x$.) Then

$\frac{d}{d x}(x y)^{2}-\frac{d}{d x}\left(3 x^{2}\right)=\frac{d}{d x}(x y)+\frac{d}{d x}(5) \quad$ or $\quad(x)\left(3 y^{2} y^{\prime}\right)+\left(y^{3}\right)(1)-6 x=(x)\left(y^{\prime}\right)+(y)(1)+0$

where $y^{\prime}=d y / d x$. Solving,

$$
y^{\prime}=\left(6 x-y^{3}+y\right) /\left(3 x y^{2}-x\right)
$$

(b) $\frac{d}{d x}\left(e^{x y}\right)+\frac{d}{d x}(u \operatorname{In})=\frac{d}{d x}(\cos 2 x) . \quad e^{x y}\left(x y^{\prime}+y\right)+\frac{y}{x}+(\operatorname{In} x) y^{\prime}=-2 \sin 2 x$.

Solving,

$$
y^{\prime}=-\frac{2 x \sin 2 x+x y e^{x y}+y}{x^{2} e^{x y}+x \operatorname{In} x}
$$

4.17. If $y=\cosh \left(x^{2}-3 x+1\right)$, find (a) $d y / d x$ and (b) $d^{2} y / d x^{2}$.

(a) Let $y=\cosh u$, where $u=x^{2}-3 x+1$. Then $d y / d x=\sinh u, d u / d x=2 x-3$, and

$$
\frac{d y}{d x}=\frac{d y}{d u} \cdot \frac{d u}{d x}=(\sinh u)(2 x-3)=(2 x-3) \sinh \left(x^{2}-3 x+1\right)
$$

(b) $\frac{d^{2} y}{d x^{2}}=\frac{d}{d x}\left(\frac{d y}{d x}\right)=\frac{d}{d x}\left(\sinh u \frac{d u}{d x}\right)=\sinh u \frac{d^{2} u}{d x^{2}}+\cosh u\left(\frac{d u}{d x}\right)^{2}$

$$
=(\sinh u)(2)+(\cosh u)(2 x-3)^{2}=2 \sinh \left(x^{2}-3 x+1\right)+(2 x-3)^{2} \cosh \left(x^{2}-3 x+1\right)
$$

4.18. If $x^{2} y+y^{3}=2$, find (a) $y^{\prime}$ and (b) $y^{\prime \prime}$ at the point $(1,1)$.

(a) Differentiating with respect to $x, x^{2} y^{\prime}+2 x y+3 y^{2} y^{\prime}=0$ and

$$
y^{\prime}=\frac{-2 x y}{x^{2}+3 x y^{2}}=-\frac{1}{2} \text { at }(1,1)
$$

(b) $y^{\prime \prime}=\frac{d}{d x}\left(y^{\prime}\right)=\frac{d}{d x}\left(\frac{-2 x y}{x^{2}+3 y^{2}}\right)=-\frac{\left(x^{2}+3 y^{2}\right)\left(2 x y^{\prime}+2 y\right)-(2 x y)\left(2 x+6 y y^{\prime}\right)}{\left(x^{2}+3 y^{2}\right)^{2}}$

Substituting $x=1, y=1$, and $y^{\prime}=-\frac{1}{2}$, we find $y^{\prime \prime}=-\frac{3}{8}$.

\section*{Mean value theorems}
4.19. Prove Rolle's theorem.

Case 1: $f(x) \equiv 0$ in $[a, b]$. Then $f^{\prime}(x)=0$ for all $x$ in $(a, b)$.

Case 2: $f(x) \not \equiv 0$ in $[a, b]$. Since $f(x)$ is continuous, there are points at which $f(x)$ attains its maximum and minimum values, denoted by $M$ and $m$, respectively (see Problem 3.34).

Since $f(x) \not \equiv 0$, at least one of the values $M, m$ is not zero. Suppose, for example, $M \not \equiv 0$ and that $f(\xi)=$ $M$ (see Figure 4.9). For this case, $f(\xi+h) \leqq f(\xi)$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-098}
\end{center}

Figure 4.9

If $h>0$, then $\frac{f(\xi+h)-f(\xi)}{h} \leqq 0$ and


\begin{equation*}
\lim _{h \rightarrow 0+} \frac{f(\xi+h)-f(\xi)}{h} \leqq 0 \tag{1}
\end{equation*}


If $h<0$, then $\frac{f(\xi+h)-f(\xi)}{h} \geqq 0$ and


\begin{equation*}
\lim _{h \rightarrow 0-} \frac{f(\xi+h)-f(\xi)}{h} \geq 0 \tag{2}
\end{equation*}


But, by hypothesis, $f(x)$ has a derivative at all points in $(a, b)$. Then the right-hand derivative (1) must be equal to the left-hand derivative (2). This can happen only if they are both equal to zero, in which case $f^{\prime}(\xi)=0$ as required.

A similar argument can be used in case $M=0$ and $m \neq 0$.

4.20. Prove the mean value theorem.

Define $F(x)=f(x)-f(a)-(x-a) \frac{f(b)-f(a)}{b-a}$.\\
Then $f(a)=0$ and $f(b)=0$.

Also, if $f(x)$ satisfies the conditions on continuity and differentiability specified in Rolle's theorem, then $F(x)$ satisfies them also.

Then, applying Rolle's theorem to the function $F(x)$, we obtain

$$
F^{\prime}(\xi)=f^{\prime}(\xi)-\frac{f(b)-f(a)}{b-a}=0, \quad a<\xi<b \quad \text { or } \quad f^{\prime}(\xi)=\frac{f(b)-f(a)}{b-a}, \quad a<\xi<b
$$

4.21. Verify the mean value theorem for $f(x)=2 x^{2}-7 x+10, a=2, b=5$.

$f(2)=4, f(5)=25, f^{\prime}(\xi)=4 \xi-7$. Then the mean value theorem states that $4 \xi-7=(25-4) /(5-2)$ or $\xi=$ 3.5. Since $2<\xi<5$, the theorem is verified.

4.22. If $f^{\prime}(x)=0$ at all points of the interval $(a, b)$, prove that $f(x)$ must be a constant in the interval.

Let $x_{1}<x_{2}$ be any two different points in $(a, b)$. By the mean value theorem for $x_{1}<\xi<x_{2}$,

$$
\frac{f\left(x^{2}\right)-f\left(x_{1}\right)}{x^{2}-x^{2}}=f^{\prime}(\xi)=0
$$

Thus $f\left(x_{1}\right)=f\left(x_{2}\right)=$ constant. From this it follows that if two functions have the same derivative at all points of $(a, b)$, the functions can differ only by a constant.

4.23. If $f^{\prime}(x)>0$ at all points of the interval $(a, b)$, prove that $f(x)$ is strictly increasing.

Let $x_{1}<x_{2}$ be any two different points in $(a, b)$. By the mean value theorem for $x_{1}<\xi<x_{2}$,

$$
\frac{f\left(x_{2}\right)-f\left(x_{1}\right)}{x_{2}-x_{1}}=f^{\prime}(\xi)>0
$$

Then $f\left(x_{2}\right)>f\left(x_{1}\right)$ for $x_{2}>x_{1}$, and so $f(x)$ is strictly increasing.

4.24.

(a) Pr ove that $\frac{b-a}{1+b^{2}}<\tan ^{-1} b-\tan ^{-1} a<\frac{b-a}{1+a^{2}}$ if $a<b$.

(b) Show that $\frac{\pi}{4}+\frac{3}{25}<\tan ^{-1} \frac{4}{3}<\frac{\pi}{4}+\frac{1}{6}$.

(a) Let $f(x)=\tan ^{-1} x$. Since $f^{\prime}(x)=1 /\left(1+x^{2}\right)$ and $f^{\prime}(\xi)=1 /\left(1+\xi^{2}\right)$, we have by the mean value theorem

$$
\frac{\tan ^{-1} b-\tan ^{-1} a}{b-a}=\frac{1}{1+\xi^{2}} \quad a<\xi<b
$$

Since $\xi>a, 1 /\left(1+\xi^{2}\right)<1 /\left(1+a^{2}\right)$. Since $\xi<b, 1 /\left(1+\xi^{2}\right)>1 /\left(1+b^{2}\right)$. Then

$$
\frac{1}{1+b^{2}}<\frac{\tan ^{-1} b-\tan ^{-1} a}{b-a}<\frac{1}{1+a^{2}}
$$

and the required result follows on multiplying by $b-a$.

(b) Let $b=4 / 3$ and $a=1$ in the result of (a). Then, since $\tan ^{+} 1=\pi / 4$, we have

$$
\frac{3}{25}<\tan ^{-1} \frac{4}{3}-\tan ^{-1} 1<\frac{1}{6} \quad \text { or } \quad \frac{\pi}{4}+\frac{3}{25}<\tan ^{-1} \frac{4}{3}<\frac{\pi}{4}+\frac{1}{6}
$$

4.25. Prove Cauchy's generalized mean value theorem.

Consider $G(x)=f(x)-f(a)-\alpha\{g(x)-g(a)\}$, where $\alpha$ is a constant. Then $G(x)$ satisfies the conditions of Rolle's theorem, provided $f(x)$ and $g(x)$ satisfy the continuity and differentiability conditions of Rolle's theorem and if $G(a)=G(b)=0$. Both latter conditions are satisfied if the constant $\alpha=\frac{f(b)-f(a)}{g(b)-g(a)}$.

Applying Rolle's theorem, $G^{\prime}(\xi)=0$ for $a<\xi<b$, we have

$$
f^{\prime}(\xi)-a g^{\prime}(\xi)=0 \quad \text { or } \quad \frac{f^{\prime}(\xi)}{g^{\prime}(\xi)}=\frac{f(b)-f(a)}{g(b)-g(a)}, \quad a<\xi<b
$$

as required.

\section*{L'Hospital's rule}
4.26. Prove L'Hospital's rule for the case of the "indeterminate forms" (a) $0 / 0$ and (b) $\infty / \infty$.

(a) We shall suppose that $f(x)$ and $g(x)$ are differentiable in $a<x<b$ and $f\left(x_{0}\right)=0, g\left(x_{0}\right)=0$, where $a<x_{0}<b$.

By Cauchy's generalized mean value theorem (Problem 4.25),

$$
\frac{f(x)}{g(x)}=\frac{f(x)-f\left(x_{0}\right)}{g(x)-g\left(x_{0}\right)}=\frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} \quad x_{0}<\xi<x
$$

Then

$$
\lim _{x \rightarrow x_{0}+} \frac{f(x)}{g(x)}=\lim _{x \rightarrow x_{0}+} \frac{f^{\prime}(\xi)}{g^{\prime}(\xi)}=\lim _{x \rightarrow x_{0}+} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L
$$

since as $x \rightarrow x_{0}+, \xi \rightarrow x_{0}+$.

Modification of this procedure can be used to establish the result if $x \rightarrow x_{0}-, x \rightarrow x_{0}, x \rightarrow \infty$, or $x \rightarrow-\infty$.

(b) We suppose that $f(x)$ and $g(x)$ are differentiable in $a<x<b$, and $\lim _{x \rightarrow x_{0}+} f(x)=\infty, \lim _{x \rightarrow x_{0}+} g(x)=\infty$ where\\
$a<x_{0}<b$.

Assume $x_{1}$ is such that $a<x_{0}<x<x_{1}<b$. By Cauchy's generalized mean value theorem,

$$
\frac{f(x)-f\left(x_{1}\right)}{g(x)-g\left(x_{1}\right)}=\frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} \quad x<\xi<x_{1}
$$

Hence,

$$
\frac{f(x)-f\left(x_{1}\right)}{g(x)-g\left(x_{1}\right)}=\frac{f(x)}{g(x)} \cdot \frac{1-f\left(x_{1}\right) / f(x)}{1-g\left(x_{1}\right) / g(x)}=\frac{f^{\prime}(\xi)}{g^{\prime}(\xi)}
$$

from which we see that


\begin{equation*}
\frac{f(x)}{g(x)}=\frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} \cdot \frac{1-g\left(x_{1}\right) / g(x)}{1-f\left(x_{1}\right) / f(x)} \tag{1}
\end{equation*}


Let us now suppose that $\lim _{x \rightarrow x_{0}+} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L$ and write Equation (1) as


\begin{equation*}
\frac{f(x)}{g(x)}=\left(\frac{f^{\prime}(\xi)}{g^{\prime}(x)}-L\right)\left(\frac{1-g\left(x_{1}\right) / g(x)}{1-f\left(x_{1}\right) / f(x)}\right)+L\left(\frac{1-g\left(x_{1}\right) / g(x)}{1-f\left(x_{1}\right) / f(x)}\right) \tag{2}
\end{equation*}


We can choose $x_{1}$ so close to $x_{0}$ that $\left|f^{\prime}(\xi) / g^{\prime}(\xi)-L\right|<\epsilon$. Keeping $x_{1}$ fixed, we see that

$$
\lim _{x \rightarrow x_{0}+}\left(\frac{1-g\left(x_{1}\right) / g(x)}{1-f\left(x_{1}\right) / f(x)}\right)=1 \text { since } 1 \lim _{x \rightarrow x_{0}+} f(x)_{1}=\infty \text { and } \lim _{x \rightarrow x_{0}+} g(x)=\infty
$$

Then taking the limit as $x \rightarrow x_{0}+$ on both sides of (2), we see that, as required,

$$
\lim _{x \rightarrow x_{0}+} \frac{f(x)}{g(x)}=L=\lim _{x \rightarrow x_{0}+} \frac{f^{\prime}(x)}{g^{\prime}(x)}
$$

Appropriate modifications of this procedure establish the result if $x \rightarrow x_{0}-, x \rightarrow x_{0}, x \rightarrow \infty$, or $x \rightarrow-\infty$.

4.27. Evaluate (a) $\lim _{x \rightarrow 0} \frac{e^{2 x}-1}{x}$ and (b) $\lim _{x \rightarrow 1} \frac{1+\cos \pi x}{x^{2}-2 x+1}$.

All of these have the "indeterminate form" $0 / 0$

(a) $\lim _{x \rightarrow 0} \frac{e^{2 x}-1}{x}=\lim _{x \rightarrow 0} \frac{2 e^{2 x}}{1}=2$

(b) $\lim _{x \rightarrow 1} \frac{1+\cos \pi x}{x^{2}-2 x+1}=\lim _{x \rightarrow 1} \frac{-\pi \sin \pi x}{2 x-2}=\lim _{x \rightarrow 1} \frac{-\pi^{2}+\cos \pi x}{2}=\frac{\pi^{2}}{2}$

Note: Here L'Hospital's rule is applied twice, since the first application again yields the "indeterminate form" $0 / 0$ and the conditions for L'Hospital's rule are satisfied once more.

4.28. Evaluate (a) $\lim _{x \rightarrow \infty} \frac{3 x^{2}-x+5}{5 x^{2}-6 x-3}$ and (b) $\lim _{x \rightarrow \infty} x^{2} e^{-x}$.

All of these have or can be arranged to have the "indeterminate form" $\infty / \infty$.

(a) $\lim _{x \rightarrow \infty} \frac{3 x^{2}-x+5}{5 x^{2}-6 x-3}=\lim _{x \rightarrow \infty} \frac{6 x-1}{10 x+6}=\lim _{x \rightarrow \infty} \frac{6}{10}=\frac{3}{5}$

(b) $\lim _{x \rightarrow \infty} x^{2} e^{-x}=\lim _{x \rightarrow \infty} \frac{x^{2}}{e^{x}}=\lim _{x \rightarrow \infty} \frac{2 x}{e^{x}}=\lim _{x \rightarrow \infty} \frac{2}{e^{x}}=0$

4.29. Evaluate $\lim _{x \rightarrow 0+} x^{2} \ln x$.

$$
\lim _{x \rightarrow 0+} x^{2} \operatorname{In} x=\lim _{x \rightarrow 0+} \frac{\operatorname{In} x}{1 / x^{2}}=\lim _{x \rightarrow 0+} \frac{1 / x}{-2 / x^{3}} \lim _{x \rightarrow 0+} \frac{-x^{2}}{2}=0
$$

The given limit has the "indeterminate form" $0 \cdot \infty$. In the second step the form is altered so as to give the indeterminate form $\infty / \infty$, and L'Hospital's rule is then applied.

4.30. Find $\lim _{x \rightarrow 0}(\cos x)^{1 / x^{2}}$.

Since $\lim _{x \rightarrow 0} \cos x=1$ and $\lim _{x \rightarrow 0} 1 / x^{2}=\infty$, the limit takes the "indeterminate form" 1 .

Let $F(x)=(\cos x)^{1 / x 2}$. Then $\ln F(x)=(\ln \cos x) / x^{2}$, to which L'Hospital's rule can be applied. We have

$$
\lim _{x \rightarrow 0} \frac{\operatorname{In} \cos x}{x^{2}}=\lim _{x \rightarrow 0} \frac{(-\sin x) /(\cos x)}{2 x}=\lim _{x \rightarrow 0} \frac{-\sin x}{2 x \cos x}=\lim _{x \rightarrow 0} \frac{-\cos x}{-2 x \sin x+2 \cos x}=-\frac{1}{2} .
$$

Thus, $\lim _{x \rightarrow 0} \ln F(x)=-\frac{1}{2}$. But since the logarithm is a continuous function, $\lim _{x \rightarrow 0} \ln F(x)=\ln \left(\lim _{x \rightarrow 0} F(x)\right)$.\\
Then

$$
\ln \left(\lim _{x \rightarrow 0} F(x)\right)=-\frac{1}{2} \quad \text { or } \quad \lim _{x \rightarrow 0} F(x)=\lim _{x \rightarrow 0}(\cos x)^{1 / x^{2}}=e^{-1 / 2}
$$

4.31. If $F(x)=\left(e^{3 x}-5 x\right)^{1 / x}$, find $(a) \lim _{x \rightarrow 0} F(x)$ and $(b) \lim _{x \rightarrow 0} F(x)$.

The respective indeterminate forms in $(a)$ and $(b)$ are $\infty^{0}$ and $1^{\infty}$.

Let $G(x)=\ln F(x)=\frac{\left(\ln \left(e^{3 x}-5 x\right)\right.}{x}$. Then $\lim _{x \rightarrow \infty} G(x)$ and $\lim _{x \rightarrow 0} G(x)$ assume the indeterminate forms $\infty / \infty$ and $0 / 0$, respectively, and L'Hospital's rule applies. We have

(a) $\lim _{x \rightarrow 0} \frac{\ln \left(e^{3 x}-5 x\right)}{x}=\lim _{x \rightarrow \infty} \frac{3 e^{3 x}-5}{e^{3 x}-5 x}=\lim _{x \rightarrow 0} \frac{9 e^{3 x}}{3 e^{3 x}-5}=\lim _{x \rightarrow \infty} \frac{27 e^{3 x}}{9 e^{3 x}}=3$

Then, as in Problem 4.30, $\lim _{x \rightarrow \infty}\left(e^{3 x}-5 x\right)^{1 / x}=e^{3}$.

(b) $\lim _{x \rightarrow 0} \frac{\ln \left(e^{3 x}-5 x\right)}{x}=\lim _{x \rightarrow 0} \frac{3 e^{3 x}-5}{e^{3 x}-5 x}=-2$

4.32. Suppose the equation of motion of a particle is $x=\sin \left(c_{1} t+c_{2}\right)$, where $c_{1}$ and $c_{2}$ are constants (simple harmonic motion). (a) Show that the acceleration of the particle is proportional to its distance from the origin. (b) If $c_{1}=1, c_{2}=\pi$, and $t \geq 0$, determine the velocity and acceleration at the endpoints and at the midpoint of the motion.

(a) $\frac{d x}{d t} c_{1} \cos \left(c_{1} t+c_{2}\right), \frac{d^{2} x}{d t^{2}}=c_{1}^{2} \sin \left(c_{1} t+c_{2}\right)=-c_{1}^{2} x$

This relation demonstrates the proportionality of acceleration and distance.

(b) The motion starts at 0 and moves to -1 . Then it oscillates between this value and 1 . The absolute value of the velocity is zero at the endpoints, and that of the acceleration is maximum there. The particle coasts through the origin (zero acceleration), while the absolute value of the velocity is maximum there.

4.33. Use Newton's method to determine $\sqrt{3}$ to three decimal points of accuracy.

$\sqrt{3}$ is a solution of $x^{2}-3=0$, which lies between 1 and 2. Consider $f(x)=x^{2}-3$, then $f^{\prime}(x)=2 x$. The graph of $f$ crosses the $x$ axis between 1 and 2. Let $x_{0}=2$. Then $f\left(x_{0}\right)=1$ and $f^{\prime}\left(x_{0}\right)=1.75$. According to the Newton formula, $x_{1}=x_{0}-\frac{f\left(x_{0}\right)}{f^{\prime}\left(x_{0}\right)}=2-.25=1.75$.

Then $x_{2}=x_{1}-\frac{f\left(x_{1}\right)}{f^{\prime}\left(x_{1}\right)}=1.732$. To verify the three-decimal-point accuracy, note that $(1.732)^{2}=2.9998$ and $(1.7333)^{2}=3.0033$

\section*{Miscellaneous problems}
4.34. If $x=g(t)$ and $y=f(t)$ are twice differentiable, find (a) $d y / d x$ and (b) $d^{2} y / d x^{2}$.

(a) Letting primes denote derivatives with respect to $t$, we have

$$
\frac{d y}{d x}=\frac{d y / d t}{d x / d t}=\frac{f^{\prime}(t)}{g^{\prime}(t)} \text { if } g^{\prime}(t) \neq 0
$$

(b) $\frac{d^{2} y}{d x^{2}}=\frac{d}{d x}\left(\frac{d y}{d x}\right)=\frac{d}{d x}\left(\frac{f^{\prime}(t)}{g^{\prime}(t)}\right) \frac{\frac{d}{d t}\left(\frac{f^{\prime}(t)}{g^{\prime}(t)}\right)}{d x / d t}=\frac{\frac{d}{d t}\left(\frac{f^{\prime}(t)}{g^{\prime}(t)}\right)}{g^{\prime}(t)}$

$$
=\frac{1}{g^{\prime}(t)}\left\{\frac{g^{\prime}(t) f^{\prime \prime}(t)-f^{\prime}(t) g^{\prime \prime}(t)}{\left[g^{\prime}(t)\right]^{2}}\right\}=\frac{g^{\prime}(t) f^{n}(t)-f^{\prime}(t) g^{n}(t)}{\left[g^{\prime}(t)\right]^{3}} \text { if } g^{\prime}(t) \neq 0
$$

4.35. Let $f(x)=\left\{\begin{array}{ll}e^{-1 / x 2}, & x \neq 0 \\ 0, & x \neq 0\end{array}\right.$. Prove that (a) $f^{\prime}(0)=0$ and (b) $f^{\prime \prime}(0)=0$.

(a) $\quad f_{+}^{\prime}(0)=\lim _{h \rightarrow 0+} \frac{f(h)-f(0)}{h}=\lim _{h \rightarrow 0+} \frac{e^{-1 / h^{2}}-0}{h}=\lim _{h \rightarrow 0+} \frac{e^{-1 / h^{2}}}{h}$

If $h=1 / u$, using L'Hospital's rule this limit equals

$$
\lim _{u \rightarrow \infty} u e^{-u^{2}}=\lim _{u \rightarrow \infty} u / e^{u^{2}}=\lim _{u \rightarrow \infty} 1 / 2 u e^{u^{2}}=0
$$

Similarly, replacing $h \rightarrow 0+$ by $h \rightarrow 0-$ and $u \rightarrow \infty$ by $u \rightarrow-\infty$, we find $f_{-}^{\prime}(0)=0$. Thus, $f_{+}^{\prime}(0)=f_{-}^{\prime}(0)=0$, and $\operatorname{so} f^{\prime}(0)=0$.

(b) $f_{+}^{\prime \prime}(0)=\lim _{h \rightarrow 0+} \frac{f^{\prime}(h)-f^{\prime}(0)}{h}=\lim _{h \rightarrow 0+} \frac{e^{-1 / h^{2}} \cdot 2 h^{-3}-0}{h}=\lim _{h \rightarrow 0+} \frac{2 e^{-1 / h^{2}}}{h}=\lim _{u \rightarrow \infty} \frac{2 u^{4}}{e^{u^{2}}}=0$

by successive applications of L'Hospital's rule.

Similarly, $f_{-}^{\prime \prime}(0)=0$ and so $f^{\prime \prime}(0)=0$.

In general, $f^{(n)}(0)=0$ for $n=1,2,3, \ldots$

4.36. Find the length of the longest ladder which can be carried around the corner of a corridor whose dimensions are indicated in Figure 4.10, if it is assumed that the ladder is carried parallel to the floor.

The length of the longest ladder is the same as the shortest straight-line segment $A B$ (Figure 4.10), which touches both outer walls and the corner formed by the inner walls.

As seen from Figure 4.10, the length of the ladder $A B$ is $L=a$ sec $\theta+b \csc \theta$.

$L$ is a minimum when $d L / d \theta=a \sec \theta \tan \theta-b \csc \theta \cot \theta=0 ;$

i.e., $a \sin ^{3} \theta=b \cos ^{3} \theta$ or $\tan \theta=\sqrt[3]{b / a}$. Then $\sec \theta=\frac{\sqrt{a^{2 / 3}+b^{2 / 3}}}{a^{1 / 3}}$ and $\cos \theta=\frac{\sqrt{a^{2 / 3}+b^{2 / 3}}}{b^{1 / 3}}$ so that $L=a \sec \theta+b \csc \theta=\left(a^{2 / 3}+b^{2 / 3}\right)^{3 / 2}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-102}
\end{center}

Figure 4.10

Although it is geometrically evident that this gives the minimum length, we can prove this analytically by showing that $d^{2} L / d \theta^{2}$ for $\theta=\tan ^{-1} \sqrt[3]{b / a}$ is positive (see Problem 4.78).

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Derivatives}
4.37. Use the definition to compute the derivatives of each of the following functions at the indicated point: (a) ( $3 x$ $-4) /(2 x+3), x=1$, (b) $x^{3}-3 x^{2}+2 x-5, x=2$, (c) $\sqrt{x}, x=4$, and (d) $\sqrt[3]{6 x-4}, x=2$.

Ans. (a) $17 / 25$, (b) 2 , (c) $\frac{1}{4}$, (d) $\frac{1}{2}$

4.38. Show from definition that (a) $\frac{d}{d x} x^{4}=4 x^{3}$ and (b) $\frac{d}{d x} \frac{3+x}{3-x}=\frac{6}{(3-x)^{2}}, x \neq 3$.

4.39. Let $f(x)=\left\{\begin{array}{ll}x^{3} \sin 1 / x, & x \neq 0 \\ 0, & x=0\end{array}\right.$ Prove that (a) $f(x)$ is continuous at $x=0$. (b) $f(x)$ has a derivative at $x=0$, and (c) $f^{\prime}(x)$ is continuous at $x=0$.

4.40. Let $f(x)=\left\{\begin{array}{ll}x e^{-1 / x^{2}}, & x \neq 0 \\ 0, & x=0\end{array}\right.$. Determine whether $f(x)(\mathrm{a})$ is is continuous at $x=0$, and (b) has a derivative at Ans. (a) Yes (b) Yes, 0

4.41. Give an alternative proof of the theorem in Problem 4.3, using " $\epsilon, \delta$ " definitions.

4.42. If $f(x)=e^{x}$, show that $f^{\prime}\left(x_{0}\right)=e^{x 0}$ depends on the result $\lim _{h \rightarrow 0}\left(e^{h}-1\right) / h=1$.

4.43. Use the results $\lim _{h \rightarrow 0}(\sin h) / h=1 . \lim _{h \rightarrow 0}(1-\cos h) / h=0$ to prove that if $f(x)=\sin x, f^{\prime}\left(x_{0}\right)=\cos x_{0}$.

\section*{Right-and left-hand derivatives}
4.44. Let $f(x)=x|x|$. (a) Calculate the right-hand derivative of $f(x)$ at $x=0$. (b) Calculate the left-hand derivative of $f(x)$ at $x=0$. (c) Does $f(x)$ have a derivative at $x=0$ ? (d) Illustrate the conclusions in (a), (b), and (c) from a graph.

Ans. (a) 0 (b) 0 (c) Yes, 0

4.45. Discuss the (a) continuity and (b) differentiability of $f(x)=x^{p} \sin 1 / x, f(0)=0$, where $p$ is any positive number. What happens in case $p$ is any real number?

4.46. Let $f(x)=\left\{\begin{array}{ll}2 x-3, & 0 \leqq x \leqq 2 \\ x^{2}-3, & 2<x \leqq 4\end{array}\right.$. Discuss the $(a)$ continuity and $(b)$ differentiability of $f(x)$ in $0 \leqq x \leqq 4$.

4.47. $\quad$ Prove that the derivative of $f(x)$ at $x=x_{0}$ exists if and only if $f_{+}^{\prime}\left(x_{0}\right)=f_{-}^{\prime}\left(x_{0}\right)$.

4.48. (a) Prove that $f(x)=x^{3}-x^{2}++5 x-6$ is differentiable in $a \leq x \leq b$, where $a$ and $b$ are any constants.

(b) Find equations for the tangent lines to the curve $y=x^{3}-x^{2}+5 x-6$ at $x=0$ and $x=1$. Illustrate by means of a graph. (c) Determine the point of intersection of the tangent lines in (b). (d) Find $f^{\prime}(x), f^{\prime \prime}(x)$, $f^{\prime \prime \prime}(x), f^{(\mathrm{IV})}(x), \ldots$

Ans. (b) $y=5 x-6, y=6 x-7$ (c) $(1,-1)$ (d) $3 x^{2}-2 x+5,6 x-2,6,0,0,0, \ldots$

4.49. If $f(x)=x^{2}|x|$, discuss the existence of successive derivatives of $f(x)$ at $x=0$.

\section*{Differentials}
4.50. If $y=f(x)=x+1 / x$, find (a) $\Delta y$, (b) $d y$, (c) $\Delta y-d y$, (d) $(\Delta y-d y) / \Delta x$, and (e) $d y / d x$.

$$
\begin{aligned}
& \text { Ans. (a) } \Delta x-\frac{\Delta x}{x(x+\Delta x)} \text { (b) }\left(1-\frac{1}{x^{2}}\right) \Delta x \text { (c) } \frac{(\Delta x)^{2}}{x^{2}(x+\Delta x)} \text { (d) } \frac{\Delta x}{x^{2}(x+\Delta x)} \text { (e) } 1-\frac{1}{x^{2}}
\end{aligned}
$$

4.51. If $f(x)=x^{2}+3 x$, find (a) $\Delta y$, (b) $d y$, (c) $\Delta y / \Delta x$, (d) $d y / d x$, and (e) $(\Delta y-d y) / \Delta x$, if $x=1$ and $\Delta x=.01$.\\
Ans. (a) . 0501\\
1, (b) .05, (c) 5.01 ,\\
, (d) 5 , (e) . 01

4.52. Using differentials, compute approximate values for each of the following: (a) $\sin 31^{\circ}$, (b) $\ln (1.12)$, (c) $\sqrt[5]{36}$. Ans. (a) 0.515 , (b) 0.12 , (c) 2.0125

4.53. If $y=\sin x$, evaluate (a) $\Delta y$ and (b) $d y$. (c) Prove that $(\Delta y-d y) / \Delta x \rightarrow 0$ as $\Delta x \rightarrow 0$.

\section*{Differentiation rules and elementary functions}
4.54. Prove the following:

(a) $\frac{d}{d x}\{f(x)+g(x)\}=\frac{d}{d x} f(x)+\frac{d}{d x} g(x)$

(b) $\frac{d}{d x}\{f(x)-g(x)\}=\frac{d}{d x} f(x)-\frac{d}{d x} g(x)$

(c) $\frac{d}{d x}\left\{\frac{f(x)}{g(x)}\right\}=\frac{g(x) f^{\prime}(x)-f(x) g^{\prime}(x)}{[g(x)]^{2}}, \quad g(x) \neq 0$.

4.55. Evaluate (a) $\frac{d}{d x}\left\{x^{3} \ln \left(x^{2}-2 x+5\right)\right\}$ at $x=1$ and (b) $\frac{d}{d x}\left\{\sin ^{2}(3 x+\pi / 6\}\right.$ at $x=0$.

Ans. (a) $3 \ln 4$ (b) $\frac{3}{2} \sqrt{3}$

4.56. Derive these formulas: (a) $\frac{d}{d x} a^{u}=a^{u} \ln a \frac{d u}{d x}, a>0, a \neq 1 ; \frac{d}{d x} \csc u=-\csc u \cot u \frac{d u}{d x}$; and (c) $\frac{d}{d x} \tanh u=\operatorname{sech}^{2} u \frac{d u}{d x}$ where $u$ is a differentiable function of $x$.

4.57. Compute (a) $\frac{d}{d x} \tan ^{-1} x$, (b) $\frac{d}{d x} \csc ^{-1} x$, (c) $\frac{d}{d x} \sinh ^{-1} x$, and (d) $\frac{d}{d x} \operatorname{coth}^{-1} x$, paying attention to the use of principal values.

4.58. If $y=x^{x}$, compute $d y / d x$. (Hint: Take logarithms before differentiating.)

Ans. $x^{x}(1+\ln x)$

4.59.

$$
\text { If } y=\{\ln (3 x+2)\}^{\sin -1(2 x+.5)} \text {, find } d y / d x \text { at } x=0 \text {. }
$$

$$
\text { Ans. }\left(\frac{\pi}{41 \ln 2}+\frac{2 \ln \ln 2}{\sqrt{3}}\right)(\ln 2)^{\pi / 6}
$$

4.60. If $y=f(u)$, where $u=g(v)$ and $v=h(x)$, prove that $\frac{d y}{d x}=\frac{d y}{d u} \cdot \frac{d u}{d v} \cdot \frac{d v}{d x}$ assuming $f, g$, and $h$ are differentiable.

4.61. Calculate (a) $d y / d x$ and (b) $d^{2} y / d x^{2}$ if $x y-\ln y=1$.

$$
\text { Ans. (a) } y^{2} /(1-x y) \text { (b) }\left(3 y^{3}-2 x y^{4}\right) /(1-x y)^{3} \text {, provided } x y \neq 1
$$

4.62. If $y=\tan x$, prove that $y^{\prime \prime \prime}=2\left(1+y^{2}\right)\left(1+3 y^{2}\right)$.

4.63. If $x=\sec t$ and $y=\tan t$, evaluate (a) $d y / d x$, (b) $d^{2} y / d x^{2}$, and (c) $d^{3} y / d x^{3}$, at $t=\pi / 4$.

$$
\text { Ans. (a) } \sqrt{2} \text { (b) }-1 \text { (c) } 3 \sqrt{2}
$$

4.64. Prove that $\frac{d^{2} y}{d x^{2}}=-\frac{d^{2} x}{d y^{2}} /\left(\frac{d x}{d y}\right)^{3}$, stating precise conditions under which it holds.

4.65. Establish formulas (a) 7 and (b) 18 on Pages 73 and 78.

\section*{Mean value theorems}
4.66. Let $f(x)=1-(x-1)^{2 / 3}, 0 \leq x \leq 2$. (a) Construct the graph of $f(x)$. (b) Explain why Rolle's theorem is not applicable to this functions; i.e., there is no value $\xi$ for which $f^{\prime}(\xi)=0,0<\xi<2$.

4.67. Verify Rolle's theorem for $f(x)=x^{2}(1-x)^{2}, 0 \leqq x \leqq 1$.

4.68. Prove that between any two real roots of $e^{x} \sin x=1$ there is at least one real root of $e^{x} \cos x=-1$. (Hint: Apply Rolle's theorem to the function $e^{-x}-\sin x$.)

4.69. (a) If $0<a<b$, prove that $(1-a / b)<\ln b / a<(b / a-1)$. (b) Use the result of $(a)$ to show that $\frac{1}{6}<\ln 1.2 \frac{1}{5}$.

4.70. Prove that $(\pi / 6+\sqrt{3} / 15)<\sin ^{-1} .6<(\pi / 6+1 / 8)$ by using the mean value theorem.

4.71. Show that the function $F(x)$ in Problem 4.20 represents the difference in ordinates of curve $A C B$ and line $A B$ at any point $x$ in $(a, b)$.

4.72. (a) If $f^{\prime}(x) \leq 0$ at all points of $(a, b)$, prove that $f(x)$ is monotonic decreasing in $(a, b)$. (b) Under what conditions is $f(x)$ strictly decreasing in $(a, b)$ ?

4.73. (a) Prove that $(\sin x) / x$ is strictly decreasing in $(0, \pi / 2)$. (b) Prove that $0 \leqq \sin x \leqq 2 x / \pi$ for $0 \leqq x \leqq \pi / 2$.

4.74. (a) Prove that $\frac{\sin b-\sin a}{\cos a-\cos b}=\cot \xi$, where $\xi$ is between $a$ and $b$. (b) By placing $a=0$ and $b=x$ in $(a)$, show that $\xi=x / 2$. Does the result hold if $x<0$ ?

\section*{L'Hospital's Rule}
4.75. Evaluate each of the following limits.\\
(a) $\lim _{x \rightarrow 0} \frac{x-\sin x}{x^{3}}$\\
(i) $\lim _{x \rightarrow 0}(1 / x-\csc x)$\\
(b) $\lim _{x \rightarrow 0} \frac{e^{2 x}-2^{e x}+1}{\cos 3 x-2 \cos 2 x+\cos x}$\\
(j) $\lim _{x \rightarrow 0} x^{\sin x}$\\
(c) $\lim _{x \rightarrow 1}\left(x^{2}-1\right) \tan \pi x / 2$\\
(k) $\lim _{x \rightarrow \infty}\left(1 / x^{2}-\cot ^{2} x\right)$\\
(d) $\lim _{x \rightarrow \infty} x^{3} e^{-2 x}$\\
(1) $\lim _{x \rightarrow 0} \frac{\tan ^{-1} x-\sin ^{-1} x}{x(1-\cos x)}$\\
(e) $\lim _{x \rightarrow 0+} x^{3} \ln x$\\
(m) $\lim _{x \rightarrow \infty} x \ln \left(\frac{x+3}{x-3}\right)$\\
(f) $\lim _{x \rightarrow 0}\left(3^{x}-2^{x}\right) / x$\\
(n) $\lim _{x \rightarrow 0}\left(\frac{\sin x}{x}\right)^{1 / x^{2}}$\\
(g) $\lim _{x \rightarrow \infty}(1-3 / x)^{2 x}$\\
(o) $\lim _{x \rightarrow \infty}\left(x+e^{x}+e^{2 x}\right)^{1 / x}$\\
(h) $\lim _{x \rightarrow \infty}(1+2 x)^{1 / 3 x}$\\
(p) $\lim _{x \rightarrow 0+}(\sin x)^{1 / \ln x}$\\
Ans. (a) $\frac{1}{6}$ (b) -1 (c) $-4 / \pi$ (d) 0 (e) 0 (f) $\ln 3 / 2$ (g) $e^{-6}$ (h) 1 (i) 0 (j) 1 (k) $\frac{2}{3}$ (l) $\frac{1}{3}$ (m) 6 (n) $e^{-1 / 6}$ (o) $e^{2}$ (p) $e$

\section*{Miscellaneous problems}
4.76. Prove that $\sqrt{\frac{1-x}{1+x}}<\frac{\ln (1+x)}{\sin ^{-1} x}<1$ if $0<x<1$.

4.77. If $\Delta f(x)=f(x+\Delta x)-f(x)$, (a) prove that $\Delta\{\Delta f(x)\}=\Delta^{2} f(x)=f(x+2 \Delta x)-2 f(x+\Delta x)+f(x)$; (b) derive an expression for $\Delta^{n} f(x)$ where $n$ is any positive integer; and (c) show that $\lim _{\Delta x \rightarrow 0} \frac{\Delta^{n} f(x)}{(\Delta x)^{n}}=f^{(n)}(x)$ if this limit\\
exists.

4.78. Complete the analytic proof mentioned at the end of Problem 4.36.

4.79. Find the relative maximum and minima of $f(x)=x^{2}, x>0$.

Ans. $f(x)$ has a relative minimum when $x=e^{-1}$.

4.80. A train moves according to the rule $x=5 t^{3}+30 t$, where $t$ and $x$ are measured in hours and miles, respectively. (a) What is the acceleration after 1 minute? (b) What is the speed after 2 hours?

4.81. A stone thrown vertically upward has the law of motion $x=-16 t^{2}+96 t$. (Assume that the stone is at ground level at $t=0$, that $t$ is measured in seconds, and that $x$ is measured in feet.) (a) What is the height of the stone at $t=2$ seconds? (b) To what height does the stone rise? (c) What is the initial velocity, and what is the maximum speed attained?

4.82. A particle travels with constant velocities $v_{1}$ and $v_{2}$ in mediums I and II, respectively (see Figure 4.11). Show that in order to go from point $P$ to point $Q$ in the least time, it must follow path $P A Q$ where $A$ is such that

$$
\left(\sin \theta_{1}\right) /\left(\sin \theta_{2}\right)=v_{1} / v_{2}
$$

Note: This is Snell's law, a fundamental law of optics first discovered experimentally and then derived mathematically.

4.83. A variable $\alpha$ is called an infinitesimal if it has zero as a limit. Given two infinitesimals $\alpha$ and $\beta$, we say that $\alpha$ is an infinitesimal of higher order (or the same order) if $\lim \alpha \beta=0$ (or $\lim \alpha \beta=l \neq 0$ ). Prove that as $x \rightarrow 0$, (a) $\sin ^{2} 2 x$ and $(1-\cos 3 x)$ are infinitesimals of the same order, and (b) $\left(x^{3}-\right.$ $\left.\sin ^{3} x\right)$ is an infinitesimal of higher order than $\{x-\ln (1+x)-1+\cos x\}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-107}
\end{center}

Figure 4.11

4.85. Can we use L'Hospital's rule to evaluate the limit of the sequence $u_{n}=n^{3} e^{-n 2}, n=1,2,3, \ldots$ ? Explain.

4.86. (1) Determine decimal approximations with at least three places of accuracy for each of the following irrational numbers: (a) $\sqrt{2}$, (b) $\sqrt{5}$, and (c) $7^{1 / 3}$.

(2) The cubic equation $x^{3}-3 x^{2}+x-4=0$ has a root between 3 and 4. Use Newton's method to determine it to at least three places of accuracy.

4.87. Using successive applications of Newton's method, obtain the positive root of (a) $x^{3}-2 x^{2}-2 x-7=0$ and (b) $5 \sin x=4 x$ to three decimal places.\\
Ans. (a) 3.268\\
(b) 1.131

4.88. If $D$ denotes the operator $d / d x$ so that $D y \equiv d y / d x$ while $D^{k} y \equiv d^{k} y / d x^{k}$, prove Leibniz's formula

$$
D^{n}(u v)=\left(D^{n} u\right) v+{ }_{n} C_{1}\left(D^{n-1} u\right)(D v)+{ }_{n} C_{2}\left(D^{n-2} u\right)\left(D^{2} v\right)+\cdots+{ }_{n} C_{r}\left(D^{n-r} u\right) D^{r} v+\cdots+u D n v
$$

where ${ }_{n} C_{r}=\left(\begin{array}{l}h \\ r\end{array}\right)$ are the binomial coefficients (see Problem 1.95).

4.89. Prove that $\frac{d^{n}}{d x^{n}}\left(x^{2} \sin x\right)=\left\{x^{2}-n(n-1)\right\} \sin (x+n \pi / 2)-2 n x \cos (x+n \pi / 2)$.

4.90. If $f^{\prime}\left(x_{0}\right)=f^{\prime \prime}\left(x_{0}\right)=\ldots=f^{(2 n)}\left(x_{0}\right)=0$ but $f^{(2 n+1)}\left(x_{0}\right) \neq 0$, discuss the behavior of $f(x)$ in the neighborhood of $x=x_{0}$. The point $x_{0}$ in such case is often called a point of inflection. This is a generalization of the previously discussed case corresponding to $n=1$.

4.91. Let $f(x)$ be twice differentiable in $(a, b)$ and suppose that $f^{\prime}(a)=f^{\prime}(b)=0$. Prove that there exists at least one point $\xi$ in $(a, b)$ such that $\left|f^{\prime \prime}(\xi)\right| \leqq \frac{4}{(b-a)^{2}}\{f(b)-f(a)\}$. Give a physical interpretation involving the velocity and acceleration of a particle.

\section*{CHAPTER 5}
\section*{Integrals}
\section*{Introduction of the Definite Integral}
The geometric problems that motivated the development of the integral calculus (determination of lengths, areas, and volumes) arose in the ancient civilizations of northern Africa. Where solutions were found, they related to concrete problems such as the measurement of a quantity of grain. Greek philosophers took a more abstract approach. In fact, Eudoxus (around 400 в.с.) and Archimedes (250 в.с.) formulated ideas of integration as we know it today.

Integral calculus developed independently and without an obvious connection to differential calculus. The calculus became a "whole" in the last part of the seventeenth century when Isaac Barrow, Isaac Newton, and Gottfried Wilhelm Leibniz (with help from others) discovered that the integral of a function could be found by asking what was differentiated to obtain that function.

The following introduction of integration is the usual one. It displays the concept geometrically and then defines the integral in the nineteenth-century language of limits. This form of definition establishes the basis for a wide variety of applications.

Consider the area of the region bound by $y=f(x)$, the $x$ axis, and the joining vertical segments (ordinates) $x=a$ and $x=b$. (See Figure 5.1.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-108}
\end{center}

Figure 5.1

Subdivide the interval $a \leq x \leq b$ into $n$ subintervals by means of the points $x_{1}, x_{2}, \ldots, x_{n-1}$, chosen arbitrarily. In each of the new intervals $\left(a, x_{1}\right),\left(x_{1}, x_{2}\right), \ldots,\left(x_{n-1}, b\right)$ choose points $\xi_{1}, \xi_{2}, \ldots, \xi_{n}$ arbitrarily. Form the sum


\begin{equation*}
f\left(\xi_{1}\right)\left(x_{1}-a\right)+f\left(\xi_{2}\right)\left(x_{2}-x_{1}\right)+f\left(\xi_{3}\right)\left(x_{3}-x_{2}\right)+\cdots+f\left(\xi_{n}\right)\left(b-x_{n-1}\right) \tag{1}
\end{equation*}


By writing $x_{0}=a, x_{n}=b$, and $x_{k}-x_{k-1}=\Delta x_{k}$, this can be written


\begin{equation*}
\sum_{k=1}^{n} f\left(\xi_{k}\right)\left(x_{k}-x_{k-1}\right)=\sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k} \tag{2}
\end{equation*}


Geometrically, this sum represents the total area of all rectangles in Figure 5.1.

We now let the number of subdivisions $n$ increase in such a way that each $\Delta x_{k} \rightarrow 0$. If, as a result, the sum (1) or (2) approaches a limit which does not depend on the mode of subdivision, we denote this limit by


\begin{equation*}
\int_{a}^{b} f(x) d x=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k} \tag{3}
\end{equation*}


This is called the definite integral of $f(x)$ between $a$ and $b$. In this symbol, $f(x) d x$ is called the integrand and $[a, b]$ is called the range of integration. We call $a$ and $b$ the limits of integration, $a$ being the lower limit of integration and $b$ the upper limit.

The limit (3) exists whenever $f(x)$ is continuous (or piecewise continuous) in $a \leqq x \leqq b$ (see Problem 5.31). When this limit exists we say that $f$ is Riemann integrable or simply integrable in $[\overline{\bar{a}}, b]$.

The definition of the definite integral as the limit of a sum was established by Cauchy around 1825. It was named for Georg Friedrich Bernhard Riemann because he made extensive use of it in this 1850 exposition of integration.

Geometrically, the value of this definite integral represents the area bounded by the curve $y=f(x)$, the $x$ axis, and the ordinates at $x=a$ and $x=b$ only if $f(x) \geqq 0$. If $f(x)$ is sometimes positive and sometimes negative, the definite integral represents the algebraic sum of the areas above and below the $x$ axis, treating areas above the $x$ axis as positive and areas below the $x$ axis as negative.

\section*{Measure Zero}
A set of points on the $x$ axis is said to have measure zero if the sum of the lengths of intervals enclosing all the points can be made arbitrarily small (less than any given positive number $\varepsilon$ ). We can show (see Problem 5.6) that any countable set of points on the real axis has measure zero. In particular, the set of rational numbers which is countable (see Problems 1.17 and 1.59), has measure zero.

An important theorem in the theory of Riemann integration is the following:

Theorem. If $f(x)$ is bounded in $[a, b]$, then a necessary and sufficient condition for the existence of $\int_{a}^{b} f(x)$ $d x$ is that the set of discontinuities of $f(x)$ have measure zero.

\section*{Properties of Definite Integrals}
If $f(x)$ and $g(x)$ are integrable in $[a, b]$, then

\begin{enumerate}
  \item $\int_{a}^{b}\{f(x) \pm g(x)\} d x=\int_{a}^{b} f(x) d x \pm \int_{a}^{b} g(x) d x$

  \item $\int_{a}^{b} A f(x) d x=A \int_{a}^{b} f(x) d x$ where $A$ is any constant

  \item $\int_{a}^{b} f(x) d x=\int_{a}^{c} f(x) d x+\int_{c}^{b} f(x) d x$ provided $f(x)$ is integrable in $[a, c]$ and $[c, b]$

  \item $\int_{a}^{b} f(x) d x=-\int_{b}^{a} f(x) d x$

  \item $\int_{a}^{a} f(x) d x=0$

  \item If in $a \leqq x \leqq b, m \leqq f(x) \leqq M$ where $m$ and $M$ are constants, then $m(b-a) \leqq \int_{a}^{b} f(x) d x \leqq M(b-a)$

  \item If in $a \leqq x \leqq b, f(x) \leqq g(x)$, then $\int_{a}^{b} f(x) d x \leqq \int_{a}^{b} g(x) d x$

  \item $\left|\int_{a}^{b} f(x) d x\right| \leqq \int_{a}^{b}|f(x)| d x$ if $a<b$

\end{enumerate}

\section*{Mean Value Theorems for Integrals}
As in differential calculus, the mean value theorems listed here are existence theorems. The first one generalizes the idea of finding an arithmetic mean (i.e., an average value of a given set of values) to a continuous function over an interval. The second mean value theorem is an extension of the first one, which defines a weighted average of a continuous function.

By analogy, consider determining the arithmetic mean (i.e., average value) of temperatures at noon for a given week. This question is resolved by recording the seven temperatures, adding them, and dividing by 7 . To generalize from the notion of arithmetic mean and ask for the average temperature for the week is much more complicated because the spectrum of temperatures is now continuous. However, it is reasonable to believe that there exists a time at which the average temperature takes place. The manner in which the integral can be employed to resolve the question is suggested by the following example.

Let $f$ be continuous on the closed interval $a \leq x \leq b$. Assume the function is represented by the correspondence $y=f(x)$, with $f(x)>0$. Insert points of equal subdivision, $a=x_{0}, x_{1}, \ldots, x_{n}=b$. Then all $\Delta x_{k}=x_{k}$ $-x_{k-1}$ are equal and each can be designated by $\Delta x$. Observe that $b-a=n \Delta x$. Let $\xi_{k}$ be the midpoint of the interval $\Delta x_{k}$ and $f\left(\xi_{k}\right)$ the value of $f$ there. Then the average of these functional values is

$$
\frac{f\left(\xi_{1}\right)+\cdots+f\left(\xi_{n}\right)}{n}=\frac{\left[f\left(\xi_{1}\right)+\cdots+f\left(\xi_{n}\right) \Delta x\right.}{b-a}=\frac{1}{b-a} \sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta \xi_{k}
$$

This sum specifies the average value of the $n$ functions at the midpoints of the intervals. However, we may abstract the last member of the string of equalities (dropping the special conditions) and define

$$
\lim _{n \rightarrow \infty} \frac{1}{b-a} \sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta \xi_{k}=\frac{1}{b-a} \int_{a}^{b} f(x) d x
$$

as the average value of $f$ on $[a, b]$.

Of course, the question of for what value $x=\xi$ the average is attained is not answered; in fact, in general, only existence, not the value, can be demonstrated. To see that there is a point $x=\xi$ such that $f(\xi)$ represents the average value of $f$ on $[a, b]$, recall that a continuous function on a closed interval has maximum and minimum values $M$ and $m$, respectively. (Think of the integral as representing the area under the curve; see Figure 5.2.) Thus,

$$
m(b-a) \leqq \int_{a}^{b} f(x) d x \leqq M(b-a)
$$

or

$$
m \leqq \frac{1}{b-a} \int_{a}^{b} f(x) d x \leqq M
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-110}
\end{center}

Figure 5.2

Since $f$ is a continuous function on a closed interval, there exists a point $x=\xi$ in $(a, b)$ intermediate to $m$ and $M$ such that

$$
f(\xi)=\frac{1}{b-a} \int_{a}^{b} f(x) d x
$$

While this example is not a rigorous proof of the first mean value theorem, it motivates it and provides an interpretation. (See Chapter 3, Theorem 10.)

First Mean Value Theorem If $f(x)$ is continuous in $[a, b]$, there is a point $\xi$ in $(a, b)$ such that


\begin{equation*}
\int_{a}^{b} f(x) d x=(b-a) f(\xi) \tag{4}
\end{equation*}


Generalized First Mean Value Theorem If $f(x)$ and $g(x)$ are continuous in $[a, b]$, and $g(x)$ does not change sign in the interval, then there is a point $\xi$ in $(a, b)$ such that


\begin{equation*}
\int_{a}^{b} f(x) g(x) d x=f(\xi) \int_{a}^{b} g(x) d x \tag{5}
\end{equation*}


This reduces to Equation (4) if $g(x)=1$.

\section*{Connecting Integral and Differential Calculus}
In the late seventeenth century the key relationship between the derivative and the integral was established. The connection which is embodied in the fundamental theorem of calculus was responsible for the creation of a whole new branch of mathematics called analysis.

Definition Any function $F$ such that $F^{\prime}(\mathrm{x})=f(x)$ is called an antiderivative, primitive, or indefinite integral of $f$.

The antiderivative of a function is not unique. This is clear from the observation that for any constant $c$

$$
(F(x)+c)^{\prime}=F^{\prime}(x)=f(x)
$$

The following theorem is an even stronger statement.

Theorem Any two primitives (i.e., antiderivatives) $F$ and $G$ of $f$ differ at most by a constant; i.e., $F(x)-$ $G(x)=C$.

(See the problem set for the proof of this theorem.) EXAMPLE. If $F^{\prime}(x)=x^{2}$, then $F(x)=\int x^{2} d x=\frac{x^{3}}{3}+c$ is an indefinite integral (antiderivative or primitive)\\
of $x^{2}$.

The indefinite integral (which is a function) may be expressed as a definite integral by writing

$$
\int f(x) d x=\int_{c}^{x} f(t) d t
$$

The functional character is expressed through the upper limit of the definite integral which appears on the right-hand side of the equation.

This notation also emphasizes that the definite integral of a given function depends only on the limits of integration, and thus any symbol may be used as the variable of integration. For this reason, that variable is often called a dummy variable. The indefinite integral notation on the left depends on continuity of $f$ on a domain that is not described. One can visualize the definite integral on the right by thinking of the dummy variable $t$ as ranging over a subinterval $[c, x]$. (There is nothing unique about the letter $t$; any other convenient letter may represent the dummy variable.)

The previous terminology and explanation set the stage for the fundamental theorem. It is stated in two parts. Part 1 states that the antiderivative of $f$ is a new function, the integrand of which is the derivative of that function. Part 2 demonstrates how that primitive function (antiderivative) enables us to evaluate definite integrals.

\section*{The Fundamental Theorem of the Calculus}
Part 1. Let $f$ be integrable on a closed interval $[a, b]$. Let $c$ satisfy the condition $a \leqq c \leqq b$, and define a new function

$$
F(x)=\int_{c}^{x} f(t) d t \text { if } a \leqq x \leqq b
$$

Then the derivative $F^{\prime}(x)$ exists at each point $x$ in the open interval $(a, b)$, where $f$ is continuous and $F^{\prime}(x)$ $=f(x)$. (See Problem 5.10 for proof of this theorem.)

Part 2. As in Part 1, assume that $f$ is integrable on the closed interval $[a, b]$ and continuous in the open interval $(a, b)$. Let $F$ be any antiderivative so that $F^{\prime}(x)=f(x)$ for each $x$ in $(a, b)$. If $a<c<b$, then for any $x$ in $(a, b)$

$$
\int_{c}^{x} f(t) d t=F(x)-F(c)
$$

If the open interval on which $f$ is continuous includes $a$ and $b$, then we may write

$$
\int_{a}^{b} f(x) d x=F(b)-F(a) . \quad(\text { See Problem 5.11) }
$$

This is the usual form in which the theorem is used.

$$
\begin{aligned}
& \text { EXAMPLE. To evaluate } \int_{1}^{2} x^{2} d x \text { we observe that } F^{\prime}(x)=x^{2}, \quad F(x)=\frac{x^{3}}{3}+c \text {, and } \int_{1}^{2} x^{2} d x=\left(\frac{2^{3}}{3}+c\right)- \\
& \left(\frac{1^{3}}{3}+c\right)=\frac{7}{3} . \text { Since } c \text { subtracts out of this evaluation, it is convenient to exclude it and simply write } \frac{2^{3}}{3}-\frac{1^{3}}{3} .
\end{aligned}
$$

\section*{Generalization of the Limits of Integration}
The upper and lower limits of integration may be variables. For example:

$$
\int_{\sin x}^{\cos x} t d t=\left[\frac{t^{2}}{2}\right]_{\sin x}^{\cos x}=\left(\cos ^{2} x-\sin ^{2} x\right) / 2
$$

In general, if $F^{\prime}(x)=f(x)$, then

$$
\int_{u(x)}^{v(x)} f(t) d t=F[v(x)]=F[u(x)]
$$

\section*{Change of Variable of Integration}
If a determination of $\int f(x) d x$ is not immediately obvious in terms of elementary functions, useful results may be obtained by changing the variable from $x$ to $t$ according to the transformation $x=g(t)$. [This change\\
of integrand that follows is suggested by the differential relation $d x=g^{\prime}(t) d t$.] The fundamental theorem enabling us to do this is summarized in the statement


\begin{equation*}
\int f(x) d x=\int f\{g(t)\} g^{\prime}(t) d t \tag{6}
\end{equation*}


where, after obtaining the indefinite integral on the right, we replace $t$ by its value in terms of $x$; i.e., $t=g^{-1}$ $(x)$. This result is analogous to the chain rule for differentiation (see Page 76).

The corresponding theorem for definite integrals is


\begin{equation*}
\int_{a}^{b} f(x) d x=\int_{\alpha}^{\beta} f\{g(t)\} g^{\prime}(t) d t \tag{7}
\end{equation*}


where $g(\alpha)=a$ and $g(\beta)=b$; i.e., $\alpha=g^{-1}(a), \beta=g^{-1}(b)$. This result is certainly valid if $f(x)$ is continuous in $[a, b]$ and if $g(t)$ is continuous and has a continuous derivative in $\alpha \leqq t \leqq \beta$.

\section*{Integrals of Elementary Functions}
The following results can be demonstrated by differentiating both sides to produce an identity. In each case, an arbitrary constant $c$ (which has been omitted here) should be added.

\begin{enumerate}
  \item $\int u^{n} d u=\frac{u^{n+1}}{n+1} \quad n \neq-1$

  \item $\int \operatorname{coth} u d u=\ln |\sinh u|$

  \item $\int \frac{d u}{u}=\ln |u|$

  \item $\int \operatorname{sech} u d u=\tan ^{-1}(\sinh u)$

  \item $\int \sin u d u=-\cos u$

  \item $\int \operatorname{csch} u d u=-\operatorname{coth}^{-1}(\cosh u)$

  \item $\int \cos u d u=\sin u$

  \item $\int \operatorname{sech}^{2} u d u=\tanh u$

  \item $\quad \int \tan u d u=\ln |\sec u|$

\end{enumerate}

$$
=-\ln |\cos u|
$$

\begin{enumerate}
  \setcounter{enumi}{21}
  \item $\int \csc h^{2} u d u=-\operatorname{coth} u$

  \item $\int \cot u d u=\ln |\sin u|$

  \item $\int \operatorname{sech} u \tanh u d u=-\operatorname{sech} u$

  \item $\int \sec u d u=\ln |\sec u+\tan u|$

\end{enumerate}

$$
=\ln |\tan (u / 2+\pi / 4)|
$$

\begin{enumerate}
  \setcounter{enumi}{23}
  \item $\int \operatorname{csch} u \operatorname{coth} u d u=-\operatorname{csch} u$

  \item $\quad \int \csc u d u=\ln |\csc u-\cot u|$

  \item $\int \frac{d u}{\sqrt{s^{2}-u^{2}}}=\sin ^{-1} \frac{u}{a}$ or $-\cos ^{-1} \frac{u}{a}$

  \item $\int \sec ^{2} u d u=\tan u$

  \item $\int \frac{d u}{\sqrt{u^{2} \pm a^{2}}}=\ln \left|u+\sqrt{u^{2} \pm a^{2}}\right|$

  \item $\int \csc ^{2} u d u=-\cot u$

  \item $\int \frac{d u}{u^{2}+a^{2}}=\frac{1}{a} \tan ^{-1} \frac{u}{a}$ or $-\frac{1}{a} \cot ^{-1} \frac{u}{a}$

  \item $\int \sec u \tan u d u=\sec u$

  \item $\int \frac{d u}{u^{2}-a^{2}}=\frac{1}{2 a} \ln \left|\frac{u-a}{u+a}\right|$

  \item $\int \csc u \cot u d u=-\csc u$

  \item $\int \frac{d u}{u \sqrt{a^{2} \pm u^{2}}}=\frac{1}{a} \ln \left|\frac{u}{a+\sqrt{a^{2} \pm u^{2}}}\right|$

  \item $\int a^{u} d u=\frac{a^{u}}{\operatorname{In} a} \quad a>0, a \neq 1$

\end{enumerate}

$30 \int \frac{d u}{u \sqrt{u^{2}-a^{2}}}=\frac{1}{a} \cos ^{-1} \frac{a}{u}$ or $\frac{1}{a} \sec ^{-1} \frac{u}{a}$

\begin{enumerate}
  \setcounter{enumi}{13}
  \item $\int e^{u} d u=e^{u}$

  \item $\int \sinh u d u=\cosh u$

  \item $\int \cosh u d u=\sinh u$

  \item $\int \tanh u d u=\ln \cosh u$\\
31

\end{enumerate}

$$
\begin{aligned}
\int \sqrt{u^{2} \pm a^{2}} d u & =\frac{u}{2} \sqrt{u^{2} \pm a^{2}} \\
& \pm \frac{a^{2}}{2} \operatorname{In}\left|u+\sqrt{u^{2} \pm a^{2}}\right|
\end{aligned}
$$

\begin{enumerate}
  \setcounter{enumi}{31}
  \item $\int \sqrt{a^{2}-u^{2}} d u=\frac{u}{2} \sqrt{a^{2}-u^{2}}+\frac{a^{2}}{2} \sin ^{-1} \frac{u}{a}$

  \item $\int e^{a u} \sin b u d u=\frac{e^{a u}(a \sin b u-b \cos b u)}{a^{2}+b^{2}}$

  \item $\int e^{a u} \cos b u d u=\frac{e^{a u}(a \cos b u+b \sin b u)}{a^{2}+b^{2}}$

\end{enumerate}

\section*{Special Methods of Integration}
\begin{enumerate}
  \item Integration by Parts Let $u$ and $v$ be differentiable functions. According to the product rule for differentials,
\end{enumerate}

$$
d(w)=u d v+v d u
$$

Upon taking the antiderivative of both sides of the equation, we obtain

$$
w=\int u d v+\int v d u
$$

This is the formula for integration by parts when written in the form

$$
\int v d v=u v-\int v d u \quad \text { or } \quad \int f(x) g^{\prime}(x) d x=f(x) g(x)-\int f^{\prime}(x) g(x) d x
$$

where $u=f(x)$ and $v=g(x)$. The corresponding result for definite integrals over the interval $[a, b]$ is certainly valid if $f(x)$ and $g(x)$ are continuous and have continuous derivatives in $[a, b]$. See Problems 5.17 to 5.19.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Partial Fractions Any rational function $\frac{P(x)}{Q(x)}$ where $P(x)$ and $Q(x)$ are polynomials, with the degree of $P(x)$ less than that of $Q(x)$, can be written as the sum of rational functions having the form $\frac{A}{(a x+b)^{r}}, \frac{A x+B}{\left(a x^{2}+b x+c\right)^{r}}$ where $r=1,2,3, \ldots$, which can always be integrated in terms of elementary functions.
\end{enumerate}

\section*{EXAMPLE 1.}
$$
\frac{3 x-2}{(4 x-3)(2 x-5)^{3}}=\frac{A}{4 x-3}+\frac{B}{(2 x+5)^{3}}+\frac{C}{(2 x+5)^{2}}+\frac{D}{(2 x+5)}
$$

EXAMPLE 2.

$$
\frac{5 x^{2}-x+2}{\left(x^{2}+2 x+4\right)^{2}(x-1)}=\frac{A x+B}{\left(x^{2}+2 x+4\right)^{2}}+\frac{C x+D}{x^{2}+2 x+4}+\frac{E}{x-1}
$$

The constants, $A, B, C$, etc., can be found by clearing of fractions and equating coefficients of like powers of $x$ on both sides of the equation or by using special methods (see Problem 5.20).

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Rational Functions of $\sin x$ and $\cos x$ These can always be integrated in terms of elementary functions by the substitution $\tan x / 2=u$ (see Problem 5.21).

  \item Special Devices Depending on the particular form of the integrand, special devices are often employed (see Problems 5.22 and 5.23).

\end{enumerate}

\section*{Improper Integrals}
If the range of integration $[a, b]$ is not finite or if $f(x)$ is not defined or not bounded at one or more points of $[a, b]$, then the integral of $f(x)$ over this range is called an improper integral. By use of appropriate limiting operations, we may define the integrals in such cases.

EXAMPLE 1

$$
\int_{0}^{\infty} \frac{d x}{1+x^{2}}=\lim _{M \rightarrow \infty} \int_{0}^{M} \frac{d x}{1+x^{2}}=\left.\lim _{M \rightarrow \infty} \tan ^{-1} x\right|_{0} ^{M}=\lim _{M \rightarrow \infty} \tan ^{-1} M=\pi / 2
$$

\section*{EXAMPLE 2}
$$
\int_{0}^{1} \frac{d x}{\sqrt{x}}=\lim _{\epsilon \rightarrow 0+} \int_{\epsilon}^{1} \frac{d x}{\sqrt{x}}=\left.\lim _{\epsilon \rightarrow 0+} 2 \sqrt{x}\right|_{\epsilon} ^{1}=\lim _{\epsilon \rightarrow 0+}(2-2 \sqrt{\epsilon})=2
$$

EXAMPLE 3

$$
\int_{0}^{1} \frac{d x}{\sqrt{x}}=\lim _{\epsilon \rightarrow 0+} \int_{\epsilon}^{1} \frac{d x}{x}=\left.\lim _{\epsilon \rightarrow 0+} \ln x\right|_{\epsilon} ^{1}=\lim _{\epsilon \rightarrow 0+}(-\ln \in)
$$

Since this limit does not exist, we say that the integral diverges (i.e., does not converge).

For further examples, see Problems 5.29 and 5.74 through 5.76. For further discussion of improper integrals, see Chapter 12.

\section*{Numerical Methods for Evaluating Definite Integrals}
Numerical methods for evaluating definite integrals are available in case the integrals cannot be evaluated exactly. The following special numerical methods are based on subdividing the interval $[a, b]$ into $n$ equal parts of length $\Delta x=(b-a) / n$. For simplicity we denote $f(a+k \Delta x)=f\left(x_{k}\right)$ by $y_{k}$, where $k=0,1,2, \ldots, n$. The symbol $\approx$ means "approximately equal." In general, the approximation improves as $n$ increases.

\section*{1. Rectangular Rule}

\begin{equation*}
\int_{a}^{b} f(x) d x \approx \Delta x\left\{y_{0}+y_{1}+y_{2}+\cdots+y_{n-1}\right\} \text { or } \Delta x\left\{y_{1}+y_{2}+y_{3}+\cdots+y_{n}\right\} \tag{8}
\end{equation*}


The geometric interpretation is evident from Figure 5.1. When left endpoint function values $y_{0}, y_{1}, \ldots, y_{n-1}$ are used, the rule is called the left-hand rule. Similarly, when right endpoint evaluations are employed, it is called the right-hand rule.

\section*{2. Trapezoidal Rule}

\begin{equation*}
\int_{a}^{b} f(x) d x \approx \frac{\Delta x}{2}\left\{y_{0}+2 y_{1}+2 y_{2}+\cdots+2 y_{n-1}+y_{n}\right\} \tag{9}
\end{equation*}


This is obtained by taking the mean of the approximations in Equation (8). Geometrically, this replaces the curve $y=f(x)$ by a set of approximating line segments.

\section*{3. Simpson's Rule}

\begin{equation*}
\int_{a}^{b} f(x) d x \approx \frac{\Delta x}{3}\left\{y_{0}+4 y_{1}+2 y_{2}+4 y_{3}+2 y_{4}+4 y_{5}+\cdots+2 y_{n-2}+4 y_{n-1}+y_{n}\right\} \tag{10}
\end{equation*}


This formula is obtained by approximating the graph of $y=g(x)$ by a set of parabolic arcs of the form $y$ $=a x^{2}+b x+c$. The correlation of two observations lead to Equation (10). First,

$$
\int_{-h}^{h}\left[a x^{2}+b x+c\right] d x=\frac{h}{3}\left[2 a h^{2}+6 c\right]
$$

The second observation is related to the fact that the vertical parabolas employed here are determined by three nonlinear points. In particular, consider $\left(-h, y_{0}\right),\left(0, y_{1}\right),\left(h, y_{2}\right)$, then $y_{0}=a(-h)^{2}+b(-h)+c, y_{1}=c, y_{2}$ $=a h^{2}+b h+c$. Consequently, $y_{0}+4 y_{1}+y_{2}=2 a h^{2}+6 c$. Thus, this combination of ordinate values (corresponding to equally spaced domain values) yields the area bounded by the parabola, vertical segments, and the $x$ axis. Now these ordinates may be interpreted as those of the function $f$ whose integral is to be approximated. Then, as illustrated in Figure 5.3:

$$
\sum_{k=1}^{n} \frac{h}{3}\left[y_{k-1}+4 y_{k}+y_{k+1}\right]=\frac{\Delta x}{3}\left[y_{0}+4 y_{1}+2 y_{2}+4 y_{3}+2 y_{4}+4 y_{5}+\cdots+2 y_{n-2}+4 y_{n-1}+y_{n}\right]
$$

The Simpson rule is likely to give a better approximation than the others for smooth curves.

\section*{Applications}
The use of the integral as a limit of a sum enables us to solve many physical and geometrical problems such as determination of areas, volumes, arc lengths, moments of intertia, and centroids.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-116}
\end{center}

Figure 5.3

\section*{Arc Length}
As you walk a twisting mountain trail, it is possible to determine the distance covered by using a pedometer. To create a geometric model of this event, it is necessary to describe the trail and a method of measuring distance along it. The trail might be referred to as a path, but in more exacting geometric terminology the word curve is appropriate. That segment to be measured is an arc of the curve. The arc is subject to the following restrictions:

\begin{enumerate}
  \item It does not intersect itself (i.e., it is a simple arc).

  \item There is a tangent line at each point.

  \item The tangent line varies continuously over the arc.

\end{enumerate}

These conditions are satisfied with a parametric representation $x=f(t), y=g(t), z=h(t), a \leqq t \leqq b$, where the functions $f, g$, and $h$ have continuous derivatives that do not simultaneously vanish at any point. This arc is in Euclidean three-dimensional space and is discussed in Chapter 10. In this introduction to curves and their arc length, we let $z=0$, thereby restricting the discussion to the plane.

A careful examination of your walk would reveal movement on a sequence of straight segments, each changed in direction from the previous one. This suggests that the length of the arc of a curve is obtained as the limit of a sequence of lengths of polygonal approximations. (The polygonal approximations are characterized by the number of divisions $n \rightarrow \infty$ and no subdivision is bound from zero. (See Figure 5.4.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-117}
\end{center}

Figure 5.4

Geometrically, the measurement of the $k$ th segment of the arc $0 \leqq t \leqq s$ is accomplished by employing the Pythagorean theorem; thus, the measure is defined by

$$
\lim _{n \rightarrow \infty} \sum_{k=1}^{n}\left\{\left(\Delta x_{k}\right)^{2}+\left(\Delta y_{k}\right)^{2}\right\}^{1 / 2}
$$

or, equivalently,

$$
\lim _{n \rightarrow \infty} \sum_{k=1}^{n}\left\{1+\left(\frac{\Delta y_{k}}{\Delta x_{k}}\right)^{2}\right\}^{1 / 2}\left(\Delta x_{k}\right)
$$

where $\Delta x_{k}=x_{k}-x_{k-1}$ and $\Delta y_{k}=y_{k}-y_{k-1}$.

Thus, the length of the arc of a curve in rectangular Cartesian coordinates is

$$
L=\int_{a}^{b}\left\{\left[f^{\prime}(t)^{2}\right]+\left[g^{\prime}(t)\right]^{2}\right\}^{1 / 2} d t=\int\left\{\left(\frac{d x}{d t}\right)^{2}+\left(\frac{d y}{d t}\right)^{2}\right\}^{1 / 2} d t
$$

(This form may be generalized to any number of dimensions.)

Upon changing the variable of integration from $t$ to $x$ we obtain the planar form

$$
L=\int_{f(a)}^{f(b)}\left\{1+\left[\frac{d y}{d x}\right]^{2}\right\}^{1 / 2}
$$

(This form is appropriate only in the plane.)

The generic differential formula $d s^{2}=d x^{2}+d y^{2}$ is useful, in that various representations algebraically arise from it. For example,

$$
\frac{d s}{d t}
$$

expresses instantaneous speed.

\section*{Area}
Area was a motivating concept in introducing the integral. Since many applications of the integral are geometrically interpretable in the context of area, an extended formula is listed and illustrated here.

Let $f$ and $g$ be continuous functions whose graphs intersect at the graphical points corresponding to $x=a$ and $x=b, a<b$. If $g(x) \varepsilon f(x) \varepsilon f(x)$ on $[a, b]$, then the area bounded by $f(x)$ and $g(x)$ is

$$
A=\int_{a}^{b}\{g(x)-f(x)\} d x
$$

If the functions intersect in $(a, b)$, then the integral yields an algebraic sum. For example, if $g(x)=\sin x$ and $f(x)=0$ then

$$
\int_{0}^{2 \pi} \sin x d x=\left.\cos x\right|_{0} ^{2 \pi}=0
$$

\section*{Volumes of Revolution}
Disk Method Assume that $f$ is continuous on a closed interval $a \leq x \leq b$ and that $f(x) \varepsilon 0$. Then the solid realized through the revolution of a plane region $R$ [bound by $f(x)$, the $x$ axis, and $x=a$ and $x=b$ ] about the $x$ axis has the volume

$$
V=\pi \int_{a}^{b}[f(x)]^{2} d x
$$

This method of generating a volume is called the disk method because the cross sections of revolution are circular disks. See Figure 5.5(a).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-118(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-118}
\end{center}

(b)

Figure 5.5

EXAMPLE. A solid cone is generated by revolving the graph of $y=k x, k>0$, and $0 \leqq x \leqq b$ about the $x$ axis. Its volume is

$$
V=\pi \int_{0}^{b} k^{2} x^{2} d x=\left.\pi \frac{k^{3} x^{3}}{3}\right|_{0} ^{b}=\pi \frac{k^{3} b^{3}}{3}
$$

Shell Method Suppose $f$ is a continuous function on $[a, b], a \varepsilon 0$, satisfying the condition $f(x) \varepsilon 0$. Let $R$ be a plane region bounded by $f(x), x=a, x=b$, and the $x$ axis. The volume obtained by orbiting $R$ about the $y$ axis is

$$
V=\int_{a}^{b} 2 \pi x f(x) d x
$$

This method of generating a volume is called the shell method because of the cylindrical nature of the vertical lines of revolution. See Figure 5.5(b).

EXAMPLE. If the region bounded by $y=k x, 0 \leqq x \leqq b$, and $x=b$ (with the same conditions as in the previous example) is orbited about the $y$ axis, the volume obtained is

$$
V=2 \pi \int_{0}^{b} x(k x) d x=\left.2 \pi k \frac{x^{3}}{3}\right|_{0} ^{b}=2 \pi k \frac{b^{3}}{3}
$$

By comparing this example with that in the section on the disk method, it is clear that for the same plane region the disk method and the shell method produce different solids and, hence, different volumes.

Moment of Inertia Moment of inertia is an important physical concept that can be studied through its idealized geometric form. This form is abstracted in the following way from the physical notions of kinetic energy $K=1 / 2 m v^{2}$ and angular velocity $v=\omega r$ ( $m$ represents mass and $v$ signifies linear velocity). Upon substituting for $v$,

$$
K=\frac{1}{2} m \omega^{2} r^{2}=\frac{1}{2}\left(m r^{2}\right) \omega^{2}
$$

When this form is compared to the original representation of kinetic energy, it is reasonable to identify $m r^{2}$ as rotational mass. It is this quantity, $l=m r^{2}$, that we call the moment of inertia.

Then in a purely geometric sense, we denote a plane region $R$ described through continuous functions $f$ and $g$ on $[a, b]$, where $a>0$ and $f(x)$ and $g(x)$ intersect at $a$ and $b$ only. For simplicity, assume $g(x) \varepsilon f(x)>$ 0 . Then

$$
l=\int_{a}^{b} x^{2}[g(x)-f(x)] d x
$$

By idealizing the plane region $R$ as a volume with uniform density one, the expression $[f(x)-g(x)] d x$ stands in for mass and $r^{2}$ has the coordinate representation $x^{2}$. See Problem 5.25(b) for more details.

\section*{SOLVED PROBLEMS}
\section*{Definition of a definite integral}
5.1. If $f(x)$ is continuous in $[a, b]$, prove that $\lim _{n \rightarrow \infty} \frac{b-a}{n} \sum_{k=1}^{n} f\left(a+\frac{k(b-a)}{n}\right)=\int_{a}^{b} f(x) d x$.

Since $f(x)$ is continuous, the limit exists independent of the mode of subdivision (see Problem 5.31). Choose the subdivision of $[a, b]$ into $n$ equal parts of equal length $\Delta x=(b-a) / n$ see Figure 5.1. Let $\xi_{k}=a+$ $k(b-a) / n, k=1,2, \ldots, n$. Then

$$
\lim _{n \rightarrow \infty} \sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k}=\lim _{n \rightarrow \infty} \frac{b-a}{n} \sum_{k=1}^{n} f\left(a+\frac{k(b-a)}{n}\right)=\int_{a}^{b} f(x) d x
$$

5.2. Express $\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} f\left(\frac{k}{n}\right)$ as a definite integral.

Let $a=0, b=1$ in Problem 5.1. Then

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} f\left(\frac{k}{n}\right)=\int_{0}^{1} f(x) d x
$$

5.3. (a) Express $\int_{0}^{1} x^{2} d x$ as a limit of a sum, and use the result to evaluate the given definite integral. (b) Interpret the result geometrically.

(a) If $f(x)=x^{2}$, then $f(k / n)=(k / n)^{2}=k^{2} / n^{2}$. Thus, by Problem 5.2,

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} \frac{k^{2}}{n^{2}}=\int_{0}^{1} x^{2} d x
$$

This can be written, using Problem 1.29,

$$
\begin{aligned}
\int_{0}^{1} x^{2} d x & =\lim _{n \rightarrow \infty} \frac{1}{n}\left(\frac{1^{2}}{n^{2}}+\frac{2^{2}}{n^{2}}+\cdots+\frac{n^{2}}{n^{2}}\right)=\lim _{n \rightarrow \infty} \frac{1^{2}+2^{2}+\cdots+n^{2}}{n^{3}} \\
& =\lim _{n \rightarrow \infty} \frac{n(n+1)(2 n+1)}{6 n^{3}} \\
& =\lim _{n \rightarrow \infty} \frac{(1+1 / n)(2+1 / n)}{6}=\frac{1}{3}
\end{aligned}
$$

which is the required limit.

Note: By using the fundamental theorem of the calculus, we observe that $\int_{0}^{1} x^{2} d x=\left.\left(x^{3} / 3\right)\right|_{0} ^{1}=1^{3} / 3-0^{3} / 3$ $=1 / 3$.

(b) The area bounded by the curve $y=x^{2}$, the $x$ axis, and the line $x=1$ is equal to $\frac{1}{3}$.

5.4. Evaluate $\lim _{n \rightarrow \infty}\left\{\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{n+n}\right\}$.

The required limit can be written

$$
\lim _{n \rightarrow \infty} \frac{1}{n}\left\{\frac{1}{1+1 / n}+\frac{1}{1+2 / n}+\cdots+\frac{1}{1+n / n}\right\}=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} \frac{1}{1+k / n}=\int_{0}^{1} \frac{d x}{1+x}=\ln |1+x|_{0}^{1}=\ln 2
$$

using Problem 5.2 and the fundamental theorem of the calculus.

5.5. Prove that $\lim _{n \rightarrow \infty} \frac{1}{n}\left\{\sin \frac{t}{n}+\sin \frac{2 t}{n}+\cdots+\sin \frac{(n-1) t}{n}\right\}=\frac{1-\cos t}{t}$.

Let $a=0, b=t, f(x)=\sin x$ in Problem 1. Then

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} \sin \frac{k t}{n}=\int_{0}^{t} \sin x d x=1-\cos t
$$

and so

$$
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n-1} \sin \frac{k t}{n}=\frac{1-\cos t}{t}
$$

using the fact that $\lim _{n \rightarrow \infty} \frac{\sin t}{n}=0$.

\section*{Measure zero}
5.6. Prove that a countable point set has measure zero.

Let the point set be denoted by $x_{1}, x_{2}, x_{3}, x_{4}, \ldots$ and suppose that intervals of lengths less than $\varepsilon / 2, \varepsilon / 4$, $\varepsilon / 8, \varepsilon / 16, \ldots$, respectively, enclose the points, where $\varepsilon$ is any positive number. Then the sum of the lengths of the intervals is less than $\varepsilon / 2+\varepsilon / 4+\varepsilon / 8+\ldots=\varepsilon$ [let $a=\varepsilon / 2$ and $r=1 / 2$ in Problem 2.25(a)], showing that the set has measure zero.

\section*{Properties of definite integrals}
5.7. Prove that $\left|\int_{a}^{b} f(x) d x\right| \leqq \int_{a}^{b}|f(x)| d x$ if $a<b$.

By absolute value property 2, on Page 4,

$$
\left|\sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k}\right| \leqq \sum_{k=1}^{n}\left|f\left(\xi_{k}\right) \Delta x_{k}\right|=\sum_{k=1}^{n}\left|f\left(\xi_{k}\right)\right| \Delta x_{k}
$$

Taking the limit as $n \rightarrow \infty$ and each $\Delta x_{k} \rightarrow 0$, we have the required result.

5.8. Prove that $\lim _{n \rightarrow \infty} \int_{0}^{2 \pi} \frac{\sin n x}{x^{2}+n^{2}} d x=0$.

$$
\left|\int_{0}^{2 \pi} \frac{\sin n x}{x^{2}+n^{2}} d x\right| \leqq \int_{0}^{2 \pi}\left|\frac{\sin n x}{x^{2}+n^{2}}\right| d x \leqq \int_{0}^{2 \pi} \frac{d x}{n^{2}}=\frac{2 \pi}{n^{2}}
$$

Then $\lim _{n \rightarrow \infty}\left|\int_{0}^{2 \pi} \frac{\sin n x}{x^{2}+n^{2}} d x\right|=0$, and so the required result follows.

\section*{Mean value theorems for integrals}
5.9. Given the right triangle pictured in Figure 5.6: (a) Find the average value of $h$. (b) At what point does this average value occur? (c) Determine the average value of $f(x)=\sin ^{-1} x, 0 \leqq x \leqq \frac{1}{2}$. (Use integration by parts.) (d) Determine the average value of $f(x)=\cos ^{2} x, 0 \leqq x \leqq \frac{\pi}{2}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-121}
\end{center}

Figure 5.6

(a) $h(x)=\frac{H}{B} x$. According to the mean value theorem for integrals, the average value of the function $h$ on the interval $[0, B]$ is

$$
A=\frac{1}{B} \int_{0}^{B} \frac{H}{B} x d x=\frac{H}{2}
$$

(b) The point $\xi$, at which the average value of $h$ occurs, may be obtained by equating $f(\xi)$ with that average value, i.e., $\frac{H}{B} \xi=\frac{H}{2}$. Thus, $\xi=\frac{B}{2}$.

\section*{Fundamental theorem of the calculus}
5.10. If $F(x)=\int_{a}^{x} f(t) d t$ where $f(x)$ is continuous in $[a, b]$, prove that $F^{\prime}(x)=f(x)$.

$$
\begin{aligned}
\frac{F(x+h)-F(x)}{h} & =\frac{1}{h}\left\{\int_{a}^{x+h} f(t) d t-\int_{a}^{x} f(t) d t\right\} \\
& =\frac{1}{h} \int_{x}^{x+h} f(t) d t=f(\xi) \quad \xi \text { between } x \text { and } x+h
\end{aligned}
$$

by the first mean value theorem for integrals (Page 99).

Then if $x$ is any point interior to $[a, b]$,

$$
F^{\prime}(x)=\lim _{h \rightarrow 0} \frac{F(x+h)-F(x)}{h}=\lim _{h \rightarrow 0} f(\xi)=f(x)
$$

since $f$ is continuous.

If $x=a$ or $x=b$, we use right-or left-hand limits, respectively, and the result holds in these cases as well.

5.11. Prove the fundamental theorem of the calculus, Part 2 (Page 101).

By Problem 5.10, if $F(x)$ is any function whose derivative is $f(x)$, we can write

$$
F(x)=\int_{a}^{x} f(t) d t+c
$$

where $c$ is any constant (see the last line of Problem 4.22).

Since $F(a)=c$, it follows that $F(x)=\int_{a}^{b} f(t) d t+F(a)$ or $\int_{a}^{b} f(t) d t=F(b)-F(a)$

5.12. If $f(x)$ is continuous in $[a, b]$, prove that $F(x)=\int_{a}^{b} f(t) d t$ is continuous in $[a, b]$.

If $x$ is any point interior to $[a, b]$, then, as in Problem 5.10,

$$
\lim _{h \rightarrow 0} F(x+h)-F(x)=\lim _{h \rightarrow 0} h f(\xi)=0
$$

and $F(x)$ is continuous.

If $x=a$ and $x=b$, we use right- and left-hand limits, respectively, to show that $F(x)$ is continuous at $x=a$ and $x=b$.

Another Method: By Problems 5.10 and 4.3, it follows that $F^{\prime}(x)$ exists, and so $F(x)$ must be continuous.

\section*{Change of variables and special methods of integration}
5.13. Prove the result in Equation (7), Page 102, for changing the variable of integration.

Let $F(x)=\int_{a}^{x} f(x) d x$ and $G(t)=\int_{a}^{x} f\{g(t)\} g^{\prime}(t) d t$, where $x=g(t)$

Then $d F=f(x) d x, d G=f\{g(t)\} g^{\prime}(t) d t$.

Since $d x=g^{\prime}(t) d t$, it follows that $f(x) d x=f\{g(t)\} g^{\prime}(t) d t$ so that $d F(x)=d G(t)$, from which $F(x)=G(t)+c$.

Now, when $x=a, t=\alpha$ or $F(a)=G(\alpha)+c$. But $F(a)=G(\alpha)=0$, so that $c=0$. Hence, $F(x)=G(t)$. Since $x=b$ when $t=\beta$, we have

$$
\int_{a}^{b} f(x) d x=\int_{\alpha}^{\beta} f\{g(t)\} g^{\prime}(t) d t
$$

as required.

5.14. Evaluate:\\
(a) $\int(x+2) \sin \left(x^{2}+4 x-6\right) d x$\\
(c) $\int_{-1}^{1} \frac{d x}{\sqrt{(x+2)(3-x)}}$\\
(e) $\int_{0}^{1 / \sqrt{2}} \frac{x \sin ^{-1} x^{2}}{\sqrt{1-x^{4}}} d x$\\
(b) $\int \frac{\cot (\ln x)}{x} d x$\\
(d) $\int 2^{-x} \tanh 2^{1-x} d x$\\
(f) $\int \frac{x d x}{\sqrt{x^{2}+x+1}}$

(a) Method 1: Let $x^{2}+4 x-6=u$. Then $(2 x+4) d x=d u,(x+2) d x=\frac{1}{2} d u$, and the integral becomes $\frac{1}{2} \int \sin u d u=-\frac{1}{2} \cos u+c=-\frac{1}{2} \cos \left(x^{2}+4 x-6\right)+c$.

Method 2:

$$
\begin{aligned}
\int(x+2) \sin \left(x^{2}+4 x-6\right) d x & =\frac{1}{2} \int \sin \left(x^{2}+4 x-6\right) d\left(x^{2}+4 x-6\right) \\
& =-\frac{1}{2} \cos \left(x^{2}+4 x-6\right)+c
\end{aligned}
$$

(b) Let $\ln x=u$. Then $(d x) / x=d u$ and the integral becomes $\int \cot u d u=\ln |\sin u|+c=\ln |\sin (\ln x)|+c$.

(c) Method 1:

$$
\int \frac{d x}{\sqrt{(x+2)(3-x)}}=\int \frac{d x}{\sqrt{6+x-x^{2}}}=\int \frac{d x}{\sqrt{6+\left(x^{2}-x\right)}}=\int \frac{d x}{\sqrt{25 / 4-\left(x-\frac{1}{2}\right)^{2}}} .
$$

Letting $x-\frac{1}{2}=u$, this becomes $\int \frac{d u}{\sqrt{25 / 4-u^{2}}}=\sin ^{-1} \frac{u}{5 / 2}+c=\sin ^{-1}\left(\frac{2 x-1}{5}\right)+c$

Then

$\int_{-1}^{1} \frac{d x}{\sqrt{(x+2)(3-x)}}=\left.\sin ^{-1}\left(\frac{2 x-1}{5}\right)\right|_{-1} ^{1}=\sin ^{-1}\left(\frac{1}{5}\right)-\sin ^{-1}\left(-\frac{3}{5}\right)=\sin ^{-1} .2+\sin ^{-1} .6$

Method 2: Let $x-\frac{1}{2}=u$ as in Method 1. Now, when $x=-1, u=-\frac{3}{2}$, and when $x=1, u=\frac{1}{2}$. Thus, by Formula 25, Page 102,

$$
\begin{aligned}
\int_{-1}^{1} \frac{d x}{\sqrt{(x+2)(3-x)}} & =\int_{-1}^{1} \frac{d x}{\sqrt{25 / 4-\left(x-\frac{1}{2}\right)^{2}}}=\int_{-3 / 2}^{1 / 2} \frac{d u}{\sqrt{25 / 4-u^{2}}}=\left.\sin ^{-1} \frac{u}{5 / 2}\right|_{-3 / 2} ^{1 / 2} \\
& =\sin ^{1} .2+\sin ^{-1} .6
\end{aligned}
$$

(d) Let $2^{1-x}=u$. Then $-2^{1-x}(\ln 2) d x=d u$ and $2^{-x} d x=-\frac{d u}{2 \ln 2}$, so that the integral becomes

$$
\frac{1}{-2 \ln 2} \int \tanh u d u=-\frac{1}{2 \ln 2} \ln \cosh 2^{1-x}+c
$$

(e) Let $\sin ^{-1} x^{2}=u$. Then $d u=\frac{1}{\sqrt{1-\left(x^{2}\right)^{2}}} 2 x d x=\frac{2 x d x}{\sqrt{1-x^{4}}}$ and the integral becomes

$$
\frac{1}{2} \int u d u=\frac{1}{4} u^{2}+c=\frac{1}{4}\left(\sin ^{-1} x^{2}\right)^{2}+c
$$

Thus,

$$
\begin{aligned}
& \int_{0}^{1 / \sqrt{2}} \frac{x \sin ^{-1} x^{2}}{\sqrt{1-x^{4}}} d x=\left.\frac{1}{4}\left(\sin ^{-1} x^{2}\right)^{2}\right|_{0} ^{1 / \sqrt{2}}=\frac{1}{4}\left(\sin ^{-1} \frac{1}{2}\right)^{2} \frac{\pi^{2}}{144} . \\
& \text { (f) } \int \frac{x d x}{\sqrt{x^{2}+x+1}}=\frac{1}{2} \int \frac{2 x+1-1}{\sqrt{x^{2}+x+1}} d x=\frac{1}{2} \int \frac{2 x+1}{\sqrt{x^{2}+x+1}} d x-\frac{1}{2} \frac{d x}{\sqrt{x^{2}+x+1}} \\
& =\frac{1}{2} \int\left(x^{2}+x+1\right)^{-1 / 2} d\left(x^{2}+x+1\right)-\frac{1}{2} \int \frac{d x}{\sqrt{\left(x+\frac{1}{2}\right)^{2}+\frac{3}{4}}} \\
& =\sqrt{x^{2}+x+1}-\frac{1}{2} \ln \left|x+\frac{1}{2}+\sqrt{\left(x+\frac{1}{2}\right)^{2}+\frac{3}{4}}\right|+c
\end{aligned}
$$

5.15. Show that $\int_{1}^{2} \frac{d x}{\left(x^{2}-2 x+4\right)^{3 / 2}}=\frac{1}{6}$.

Write the integral as $\int_{1}^{2} \frac{d x}{\left[(x-1)^{2}+3\right]^{3 / 2}}$. Let $x-1=\sqrt{3}$ tan $u, d x=\sqrt{3} \sec ^{2} u d u$. When $x=1, u=$ $\tan ^{-1} 0=0$; when $x=2, u=\tan ^{-1} 1 / \sqrt{3}=\pi / 6$. Then the integral becomes

$$
\int_{0}^{\pi / 6} \frac{\sqrt{3} \sec ^{2} u d u}{\left[3+3 \tan ^{2} u\right]^{3 / 2}}=\int_{0}^{\pi / 6} \frac{\sqrt{3} \sec ^{2} u d u}{\left[3 \sec ^{2} u\right]^{3 / 2}}=\frac{1}{3} \int_{0}^{\pi / 6} \cos u d u=\left.\frac{1}{3} \sin u\right|_{0} ^{\pi / 6}=\frac{1}{6}
$$

5.16. Determine $\int_{e}^{e^{2}} \frac{d x}{x(\ln x)^{3}}$.

Let $\ln x=y,(d x) / x=d y$. When $x=e, y=1$; when $x=e^{2}, y=2$. Then the integral becomes

$$
\int_{1}^{2} \frac{d y}{y^{3}}=\left.\frac{y^{-2}}{-2}\right|_{1} ^{2}=\frac{3}{8}
$$

5.17. Find $\int x^{n} \ln x d x$ if (a) $n \neq-1$ and if (b) $n=-1$.

(a) Use integration by parts, letting $u=\ln x, d v=x^{n} d x$, so that $d u=(d x) / x, v=x^{n+1} /(n+1)$. Then

$$
\begin{aligned}
\int x^{n} \ln x d x & =\int u d v=w-\int v d u=\frac{x^{n+1}}{n+1} \ln x-\int \frac{x^{n+1}}{n+1} \cdot \frac{d x}{x} \\
& =\frac{x^{n+1}}{n+1} \ln x-\frac{x^{n+1}}{(n+1)^{2}}+c
\end{aligned}
$$

(b) $\int x^{-1} \ln x d x=\int \ln x d(\ln x)=\frac{1}{2}(\ln x)^{2}+c$.

5.18. Find $\int 3^{\sqrt{2 x+1}} d x$.

Let $\sqrt{2 x+1}=y, 2 x+1=y^{2}$. Then $d x=y d y$ and the integral becomes $\int 3^{y} \cdot y d y$.

Integrate by parts, letting $u=y, d v=3^{y} d y$; then $d u=d y, v=3^{y} /(\ln 3)$, and we have

$$
\int 3^{y} \cdot y d y=\int u d v=w-\int v d u=\frac{y \cdot 3^{y}}{\ln 3}-\int \frac{3^{y}}{\ln 3} d y=\frac{y \cdot 3^{y}}{\ln 3}-\frac{3^{y}}{(\ln 3)^{2}}+c
$$

5.19. Find $\int_{0}^{1} x \ln (x+3) d x$.

Let $u=\ln (x+3), d v=x d x$. Then $d u=\frac{d x}{x+3}, v=\frac{x^{2}}{2}$. Hence, on integrating by parts,

$$
\begin{aligned}
\int x \ln (x+3) d x & =\frac{x^{2}}{2} \ln (x+3)-\frac{1}{2} \int \frac{x^{2} d x}{x+3}=\frac{x^{2}}{2} \ln (x+3)-\frac{1}{2} \int\left(x-3+\frac{9}{x+3}\right) d x \\
& =\frac{x^{2}}{2} \ln (x+3)-\frac{1}{2}\left\{\frac{x^{2}}{2}-3 x+9 \ln (x+3)\right\}+c
\end{aligned}
$$

Then

$$
\int_{0}^{1} x \ln (x+3) d x=\frac{5}{4}-4 \ln 4+\frac{9}{2} \ln 3
$$

5.20. Determine $\int \frac{6-x}{(x-3)(2 x+5)} d x$.

Use the method of partial fractions. Let $\frac{6-x}{(x-3)(2 x+5)}=\frac{A}{x-3}+\frac{B}{2 x+5}$.

Method 1: To determine the constants $A$ and $B$, multiply both sides by $(x-3)(2 x+5)$ to obtain


\begin{equation*}
6-x=A(2 x+5)+B(x-3) \text { or } 6-x=5 A-3 B+(2 A+B) x \tag{1}
\end{equation*}


Since this is an identity, $5 A-3 B=6,2 A+B=-1$ and $A=3 / 11, B=-17 / 11$. Then

$$
\int \frac{6-x}{(x-3)(2 x+5)} d x=\int \frac{3 / 11}{x-3} d x+\int \frac{-17 / 11}{2 x+5} d x=\frac{3}{11} \ln |x-3|-\frac{17}{22} \ln |2 x+5|+c
$$

Method 2: Substitute suitable values for $x$ in the identity (1). For example, letting $x=3$ and $x=-5 / 2$ in (1), we find at once $A=3 / 11, B=-17 / 11$.

5.21. Evaluate $\int \frac{d x}{5+3 \cos x}$ by using the substitution $\tan x / 2=u$.

From Figure 5.7 we see that

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-125}
\end{center}

Figure 5.7

Then

$$
\cos x=\cos ^{2} x / 2-\sin ^{2} x / 2=\frac{1-u^{2}}{1+u^{2}}
$$

Also

$$
d u=\frac{1}{2} \sec ^{2} x / 2 d x \quad \text { or } \quad d x=2 \cos ^{2} x / 2 d u=\frac{2 d u}{1+u^{2}}
$$

Thus, the integral becomes

$$
\int \frac{d u}{u^{2}+4}=\frac{1}{2} \tan ^{-1} u / 2+c=\frac{1}{2} \tan ^{-1}\left(\frac{1}{2} \tan x / 2\right)+c .
$$

5.22. Evaluate $\int_{0}^{\pi} \frac{x \sin x}{1+\cos ^{2} x} d x$.

Let $x=\pi-y$. Then

$$
\begin{aligned}
I & =\int_{0}^{\pi} \frac{x \sin x}{1+\cos ^{2} x} d x=\int_{0}^{\pi} \frac{(\pi-y) \sin y}{1+\cos ^{2} y} d y=\pi \int_{0}^{\pi} \frac{\sin y}{1+\cos ^{2} y} d y-\int_{0}^{\pi} \frac{y \sin y}{1+\cos ^{2} y} d y \\
& =-\pi \int_{0}^{\pi} \frac{d(\cos y)}{1+\cos ^{2} y}-I=-\left.\pi \tan ^{-1}(\cos y)\right|_{0} ^{\pi}=-I=\pi^{2} / 2-I
\end{aligned}
$$

i.e., $I=\pi^{2} / 2-I$ or $I=\pi^{2} / 4$.

5.23. Prove that $\int_{0}^{\pi / 2} \frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}} d x=\frac{\pi}{4}$.

Letting $x=\pi / 2-y$, we have

$$
I=\int_{0}^{\pi / 2} \frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}} d x=\int_{0}^{\pi / 2} \frac{\sqrt{\cos y}}{\sqrt{\cos y}+\sqrt{\sin y}} d y=\int_{0}^{\pi / 2} \frac{\sqrt{\cos x}}{\sqrt{\cos x}+\sqrt{\sin x}} d x
$$

Then

$$
\begin{aligned}
I+I \int_{0}^{\pi / 2} & =\frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}} d x+\int_{0}^{\pi / 2} \frac{\sqrt{\cos x}}{\sqrt{\cos x}+\sqrt{\sin x}} d x \\
& =\int_{0}^{\pi / 2} \frac{\sqrt{\sin x}+\sqrt{\cos x}}{\sqrt{\sin x}+\sqrt{\cos x}} d x=\int_{0}^{\pi / 2}=\frac{\pi}{2}
\end{aligned}
$$

from which $2 I=\pi / 2$ and $I=\pi / 4$.

The same method can be used to prove that for all real values of $m$,

$$
\int_{0}^{\pi / 2} \frac{\sin ^{m} x}{\sin ^{m} x+\cos ^{m} x} d x=\frac{\pi}{4}
$$

(see Problem 5.89).

Note: This problem and Problem 5.22 show that some definite integrals can be evaluated without first finding the corresponding indefinite integrals.

\section*{Numerical methods for evaluating definite integrals}
5.24. Evaluate $\int_{0}^{1} \frac{d x}{1+x^{2}}$ approximately, using (a) the trapezodial rule, and (b) Simpson's rule, where the interval $[0,1]$ is divided into $n=4$ equal parts.

Let $f(x)=1 /\left(1+x^{2}\right)$. Using the notation on Page 104 , we find $\Delta x=(b-a) / n=(1-0) / 4=0.25$. Then, keeping four decimal places, we have $y_{0}=f(0)=1.0000, y_{1}=f(0.25)=0.9412, y_{2}=f(0.50)=0.8000, y_{3}=$ $f(0.75)=0.6400$, and $y_{4}=f(1)=0.50000$.

(a) The trapezoidal rule gives

$$
\begin{aligned}
\frac{\Delta x}{2}\left\{y_{0}+2 y_{1}+2 y_{2}+2 y_{3}+y_{4}\right\} & =\frac{0.25}{2}\{1.0000+2(0.9412)+2(0.8000)+2(0.6400)+0.500\} \\
& =0.7828
\end{aligned}
$$

(b) Simpson's rule gives

$$
\begin{aligned}
\frac{\Delta x}{3}\left\{y_{0}+4 y_{1}+2 y_{2}+4 y_{3}+y_{4}\right\} & =\frac{0.25}{2}\{1.0000+4(0.9412)+2(0.8000)+4(0.6400)+0.500\} \\
& =0.7854
\end{aligned}
$$

The true value is $\pi / 4 \approx 0.7854$.

\section*{Applications (area, arc length, volume, moment of inertia)}
5.25. Find (a) the area and (b) the moment of inertia about the $y$ axis of the region in the $x y$ plane bounded by $y=$ $4-x^{2}$ and the $x$ axis.

(a) Subdivide the region into rectangles as in Figure 5.1. A typical rectangle is shown in Figure 5.8. Then

$$
\begin{aligned}
\text { Required area } & =\lim _{n \rightarrow \infty} \sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k} \\
& =\lim _{n \rightarrow \infty} \sum_{k=1}^{n}\left(4-\xi_{k}^{2}\right) \Delta x_{k} \\
& =\int_{-2}^{2}\left(4-x^{2}\right) d x=\frac{32}{3}
\end{aligned}
$$

(b) Assuming unit density, the moment of inertia about the $y$ axis of the typical rectangle shown in Figure 5.8 is $\xi_{k}^{2} f\left(\xi_{k}\right) \Delta x_{k}$. Then

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-126}
\end{center}

Figure 5.8

Required moment of inertia $=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \xi_{k}^{2} f\left(\xi_{k}\right) \Delta x_{k}=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \xi_{k}^{2}\left(4-\xi_{k}^{2}\right) \Delta x_{k}$

$$
=\int_{-2}^{2} x^{2}\left(4-x^{2}\right) d x=\frac{128}{15}
$$

5.26. Find the length of arc of the parabola $y=x^{2}$ from $x=0$ to $x=1$.

Required arc length $=\int_{0}^{1} \sqrt{1+(d y / d x)^{2}} d x=\int_{0}^{1} \sqrt{1+(2 x)^{2} d x}$

$$
\begin{aligned}
& =\int_{0}^{1} \sqrt{1+4 x^{2}} d x=\frac{1}{2} \int_{0}^{2} \sqrt{1+u^{2}} d u \\
& =\left.\frac{1}{2}\left\{\frac{1}{2} u \sqrt{1+u^{2}}+\frac{1}{2} \ln \left(u+\sqrt{1+u^{2}}\right)\right\}\right|_{0} ^{2}=\frac{1}{2} \sqrt{5}+\frac{1}{4} \ln (2+\sqrt{5})
\end{aligned}
$$

5.27. (a) (Disk method.) Find the volume generated by revolving the region of Problem 5.25 about the $x$ axis.

$$
\text { Required volume }=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \pi y_{k}^{2} \Delta x_{k}=\pi \int_{-2}^{2}\left(4-x^{2}\right)^{2} d x=512 \pi / 15 .
$$

(b) (Disk method.) Find the volume of the frustrum of a paraboloid obtained by revolving $f(x)=\sqrt{k x}$, $0<a \leqq x \leqq b$ about the $x$ axis.

$$
V=\pi \int_{a}^{b} k x d x=\frac{\pi k}{2}\left(b^{2}-a^{2}\right)
$$

(c) (Shell method.) Find the volume obtained by orbiting the region of (b) about the $y$ axis. Compare this volume with that obtained in (b).

$$
V=2 \pi \int_{0}^{b} x(k x) d x=2 \pi k b^{3} / 3
$$

The solids generated by the two regions are different, as are the volumes.

\section*{Miscellaneous problems}
If $f(x)$ and $g(x)$ are continuous in $[a, b]$, prove Schwarz's inequality for integrals:

$$
\left(\int_{a}^{b} f(x) g(x) d x\right)^{2} \leqq \int_{a}^{b}\{f(x)\}^{2} d x \int_{a}^{b}\{g(x)\}^{2} d x
$$

We have

$$
\int_{a}^{b}\{f(x)+\lambda g(x)\}^{2} d x=\int_{a}^{b}\{f(x)\}^{2} d x+2 \lambda \int_{a}^{b} f(x) g(x) d x+\lambda^{2} \int_{a}^{b}\{g(x)\}^{2} d x \geqq 0
$$

for all real values of $\lambda$. Hence, using Equation (1) in Problem 1.13 with

$$
A^{2}=\int_{a}^{b}\{g(x)\}^{2} d x, \quad B^{2}=\int_{a}^{b}\{f(x)\}^{2} d x, \quad C=\int_{a}^{b} f(x) g(x) d x
$$

we find $C^{2} \leqq A^{2} B^{2}$, which gives the required result.

5.29. Prove that $\lim _{M \rightarrow \infty} \int_{0}^{M} \frac{d x}{x^{4}+4}=\frac{\pi}{8}$.

We have $x^{4}+4=x^{4}+4 x^{2}+4-4 x^{2}=\left(x^{2}+2\right)^{2}-(2 x)^{2}=\left(x^{2}+2+2 x\right)\left(x^{2}+2-2 x\right)$. According to the method of partial fractions, assume

$$
\frac{1}{x^{4}+4}=\frac{A x+B}{x^{2}+2 x+2}+\frac{C x+D}{x^{2}-2 x+2}
$$

Then $1=(A+C) x^{3}+(B-2 A+2 C+D) x^{2}+(2 A-2 B+2 C+2 D) x+2 B+2 D$, so that $A+C=0, B-2 A$ $+2 C+D=0,2 A-2 B+2 C+2 D=0,2 B+2 D=1$. Solving simultaneously, $A=\frac{1}{8}, \quad B=\frac{1}{4}, \quad C=-\frac{1}{8}, \quad D=\frac{1}{4}$.\\
Thus,

$$
\begin{aligned}
\int \frac{d x}{x^{4}+4} & =\frac{1}{8} \int \frac{x+2}{x^{2}+2 x+2} d x-\frac{1}{8} \int \frac{x-2}{x^{2}-2 x+2} d x \\
& =\frac{1}{8} \int \frac{x+1}{(x+1)^{2}+1} d x+\frac{1}{8} \int \frac{d x}{(x+1)^{2}+1}-\frac{1}{8} \int \frac{x-1}{(x-1)^{2}+1} d x+\frac{1}{8} \int \frac{d x}{(x-1)^{2}+1} \\
& =\frac{1}{16} \ln \left(x^{2}+2 x+2\right)+\frac{1}{8} \tan ^{-1}(x+1)-\frac{1}{16} \ln \left(x^{2}-2 x+2\right)+\frac{1}{8} \tan ^{-1}(x-1)+C
\end{aligned}
$$

Then

$$
\lim _{M \rightarrow \infty} \int_{0}^{M} \frac{d x}{x^{4}+4}=\lim _{M \rightarrow \infty}\left\{\frac{1}{16} \ln \left(\frac{M^{2}+2 M+2}{M^{2}-2 M+2}\right)+\frac{1}{8} \tan ^{-1}(M+1)+\frac{1}{8} \tan ^{-1}(M-1)\right\}=\frac{\pi}{8}
$$

We denote this limit by $\int_{0}^{\infty} \frac{d x}{x^{4}+4}$, called an improper integral of the first kind. Such integrals are considered further in Chapter 12. See also Problem 5.74.

5.30. Evaluate $\lim _{x \rightarrow 0} \frac{\int_{0}^{x} \sin t^{3} d t}{x^{4}}$.

The conditions of L'Hospital's rule are satisfied, so that the required limit is

$$
\lim _{x \rightarrow 0} \frac{\frac{d}{d x} \int_{0}^{x} \sin t^{3} d t}{\frac{d}{d x}\left(x^{4}\right)}=\lim _{x \rightarrow 0} \frac{\sin x^{3}}{4 x^{3}}=\lim _{x \rightarrow 0} \frac{\frac{d}{d x}\left(\sin x^{3}\right)}{\frac{d}{d x}\left(4 x^{3}\right)}=\lim _{x \rightarrow 0} \frac{3 x^{2} \cos x^{3}}{12 x^{2}}=\frac{1}{4}
$$

5.31. Prove that if $f(x)$ is continuous in $[a, b]$, then $\int_{a}^{b} f(x) d x$ exists.

Let $\sigma=\sum_{k=1}^{n} f\left(\xi_{k}\right) \Delta x_{k}$, using the notation of Page 99. Since $f(x)$ is continuous, we can find numbers $M_{k}$ and $m_{k}$, representing the l.u.b. and g.l.b. of $f(x)$ in the interval $\left[x_{k-1}, x_{k}\right]$, i.e., such that $m_{k} \leqq f(x) \leqq M_{k}$. We then have


\begin{equation*}
m(b-a) \leqq s=\sum_{k=1}^{n} m_{k} \Delta x_{k} \leqq \sigma \leqq \sum_{k=1}^{n} M_{k} \Delta x_{k}=S \leqq M_{k}(b-a) \tag{1}
\end{equation*}


where $m$ and $M$ are the g.l.b. and l.u.b. of $f(x)$ in $[a, b]$. The sums $s$ and $S$ are sometimes called the lower and upper sums, respectively.

Now choose a second mode of subdivision of $[a, b]$ and consider the corresponding lower and upper sums denoted by $s^{\prime}$ and $S^{\prime}$ respectively. We have must


\begin{equation*}
s^{\prime} \leqq S \quad \text { and } \quad S^{\prime} \varepsilon S \tag{2}
\end{equation*}


To prove this we choose a third mode of subdivision obtained by using the division points of both the first and second modes of subdivision and consider the corresponding lower and upper sums, denoted by $t$ and $T$, respectively. By Problem 5.84, we have


\begin{equation*}
s \leqq t \leqq T \leqq S^{\prime} \quad \text { and } \quad s^{\prime} \leqq t \leqq T \leqq S \tag{3}
\end{equation*}


which proves (2).

From (2) it is also clear that as the number of subdivisions is increased, the upper sums are monotonic decreasing and the lower sums are monotonic increasing. Since, according to Equation (1), these sums are also bounded, it follows that they have limiting values, which we shall call $\bar{s}$ and $\underline{S}$, respectively. By By Problem $5.85, \bar{s} \leqq \underline{S}$. In order to prove that the integral exists, we must show that $\bar{s}=\underline{S}$.

Since $f(x)$ is continuous in the closed interval $[a, b]$, it is uniformly continuous. Then, given any $\varepsilon>0$, we can take each $\Delta x_{k}$ so small that $M_{k}-m_{k}<\varepsilon /(b-a)$. It follows that


\begin{equation*}
S-s=\sum_{k=1}^{n}\left(M_{k}-m_{k}\right) \Delta x_{k}<\frac{\varepsilon}{b-a} \sum_{k=1}^{n} \Delta x_{k}=\varepsilon \tag{4}
\end{equation*}


Now $S-s=(S-\underline{S})+(\underline{S}-\bar{S})+(\bar{S}-s)$ and it follows that each term in parentheses is positive and so is less than $\varepsilon$, by Equation (4). In particular, since $\underline{S}-\bar{S}$ is a definite number, it must be zero; i.e., $\underline{S}=\bar{S}$. Thus, the limits of the upper and lower sums are equal and the proof is complete.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Definition of a definite integral}
5.32. (a) Express Express $\int_{0}^{1} x^{3} d x$ as a limit of a sum. (b) Use the result of (a) to evaluate the given definite integral. (c) Interpret the result geometrically.

$$
\text { Ans. (b) } \frac{1}{4}
$$

5.33. Using the definition, evaluate (a) $\int_{0}^{2}(3 x+1) d x$, and (b) $\int_{3}^{6}\left(x^{2}-4 x\right) d x$.

$$
\text { Ans. (a) } 8 \text { (b) } 9
$$

5.34. Prove that $\lim _{n \rightarrow \infty}\left\{\frac{n}{n^{2}+1^{2}}+\frac{n}{n^{2}+2^{2}}+\cdots+\frac{n}{n^{2}+n^{2}}\right\}=\frac{\pi}{4}$.

5.35. Prove that $\lim _{n \rightarrow \infty}\left\{\frac{1^{p}+2^{p}+3^{p}+\cdots+n^{p}}{n^{p+1}}=\frac{1}{p+1}\right\}$ if $p>-1$

5.36. Using the definition, prove that $\int_{a}^{b} e^{x} d x=e^{b}-e^{a}$.

5.37. Work Problem 5.5 directly, using Problem 1.94.

5.38. Prove that $\lim _{n \rightarrow \infty}\left\{\frac{1}{\sqrt{n^{2}+1^{2}}}+\frac{1}{\sqrt{n^{2}+2^{2}}}+\cdots+\frac{1}{\sqrt{n^{2}+n^{2}}}\right\}=\ln (1+\sqrt{2})$.

5.38. Prove that $\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \frac{n}{n^{2}+k^{2} x^{2}}=\frac{\tan ^{-1} x}{x}$ if $x \neq 0$.

\section*{Properties of definite integrals}
5.40. Prove (a) Property 2 and (b) Property 3, on Page 102.

5.41. If $f(x)$ is integrable in $(a, c)$ and $(c, b)$, prove that $\int_{a}^{b} f(x) d x=\int_{a}^{c} f(x) d x+\int_{c}^{b} f(x) d x$.

5.42. If $f(x)$ and $g(x)$ are integrable in $[a, b]$ and $f(x) \leqq g(x)$, prove that $\int_{a}^{b} f(x) d x \leqq \int_{a}^{b} g(x) d x$.

5.43. Prove that $1-\cos x \varepsilon x^{2} / \pi$ for $0 \leqq x \leqq \pi / 2$.

5.44. Prove that $\left|\int_{0}^{1} \frac{\cos n x}{x+1} d x\right| \leqq \ln 2$ for all $n$.

5.45. Prove that $\left|\int_{1}^{\sqrt{3}} \frac{e^{-x} \sin x}{x^{2}+1} d x\right| \leqq \frac{\pi}{12 e}$.

\section*{Mean value theorems for integrals}
5.46. Prove result (5), Page 100. [Hint: If $m \leqq f(x) \leqq M$, then $m g(x) \leqq f(x) g(x) \leqq M g(x)$. Now integrate and divide by $\int_{a}^{b} g(x) d x$. Then apply Theorem 9, from Chapter 3.]

5.47. Prove that there exist values $\xi_{1}$ and $\xi_{2}$ in $0 \leqq x \leqq 1$ such that

$$
\int_{0}^{1} \frac{\sin \pi x}{x^{2}+1} d x=\frac{2}{\pi\left(\xi_{1}^{2}+1\right)}=\frac{\pi}{4} \sin \pi \xi_{2}
$$

(Hint: Apply the first mean value theorem.)

5.48. (a) Prove that there is a value $\xi$ in $0 \leqq x \leqq \pi$ such that $\int_{0}^{\pi} e^{-x} \cos x d x=\sin \xi$. (b) Suppose a wedge in the shape of a right triangle is idealized by the region bounded by the $x$ axis, $f(x)=x$, and $x=L$. Let the weight distribution for the wedge be defined by $W(x)=x^{2}+1$. Use the generalized mean value theorem to show that the point at which the weighted value occurs is $\frac{3 L}{4} \frac{L^{2}+2}{L^{2}+3}$.

\section*{Change of variables and special methods of integration}
5.49. Evaluate:\\
(a) $\int x^{2} e^{\sin x^{3}} \cos x^{3} d x$ (b) $\int_{0}^{1} \frac{\tan ^{-1} t}{1+t^{2}} d t$\\
(c) $\int_{1}^{3} \frac{d x}{\sqrt{4 x-x^{2}}}$\\
(d) $\int \frac{\operatorname{csch}^{2} \sqrt{u}}{\sqrt{u}} d u$.\\
Ans. (a) $\frac{1}{3} e^{\sin x^{3}}+c$ (b) $\pi^{2} / 32$\\
(c) $\pi / 3(\mathrm{~d})-2 \operatorname{coth} \sqrt{u}+c$\\
(e) $\frac{1}{4} \ln 3$

5.50. Show that (a) $\int_{0}^{1} \frac{d x}{\left(3+2 x-x^{2}\right)^{3 / 2}}=\frac{\sqrt{3}}{12}$ and (b) $\int \frac{d x}{x^{2} \sqrt{x^{2}-1}}=\frac{\sqrt{x^{2}-1}}{x}+c$

5.51. Prove that (a) $\int \sqrt{u^{2} \pm a^{2}} d u=\frac{1}{2} u \sqrt{u^{2} \pm a^{2}} \pm \frac{1}{2} a^{2} \ln \left|u+\sqrt{u^{2} \pm a^{2}}\right|$ and

(b) $\int \sqrt{a^{2}-u^{2}} d u=\frac{1}{2} u \sqrt{a^{2}-u^{2}}+\frac{1}{2} a^{2} \sin ^{-1} u / a+c, \quad a>0$.

5.52. Find $\int \frac{x d x}{\sqrt{x^{2}+2 x+5}}$

$$
\text { Ans. } \sqrt{x^{2}+2 x+5}-\ln \left|x+1+\sqrt{x^{2}+2 x+5}\right|+c .
$$

5.53. Establish the validity of the method of integration by parts.

5.54. Evaluate (a) $\int_{0}^{\pi} x \cos 3 x d x$ and (b) $\int x^{3} e^{-2 x} d x$

$$
\text { Ans. (a) }-2 / 9 \text { (b) }-\frac{1}{3} e^{-2 x}\left(4 x^{3}+6 x^{2}+6 x+3\right)+c
$$

5.55. Show that (a) $\int_{0}^{1} x^{2} \tan ^{-1} x d x=\frac{1}{12} \pi-\frac{1}{6}+\frac{1}{6} \ln 2$ and (b)

$$
\int_{-2}^{2} \sqrt{x^{2}+x+1} d x=\frac{5 \sqrt{7}}{4}+\frac{3 \sqrt{3}}{4}+\frac{3}{8} \ln \left(\frac{5+2 \sqrt{7}}{2 \sqrt{3}-3}\right)
$$

5.56. (a) If $u=f(x)$ and $v=g(x)$ have continuous $n$th derivatives, prove that $\int w^{(n)} d x=w^{(n-1)}-u^{\prime} v^{(n-2)}+u^{\prime \prime} v^{(n-3)}-\cdots-(-1)^{n} \int u^{(n)} v d x$, called generalized integration by parts.

(b) What simplifications occur if $u^{(n)}=0$ ? Discuss. (c) Use (a) to evaluate $\int_{0}^{\pi} x^{4} \sin x d x$.

Ans. (c) $\pi^{4}-12 \pi^{2}+48$

5.57. Show that $\int_{0}^{1} \frac{x d x}{(x+1)^{2}\left(x^{2}+1\right)}=\frac{\pi-2}{8}$ [Hint: Use partial fractions; i.e., assume $\frac{x}{(x+1)^{2}\left(x^{2}+1\right)}=\frac{A}{(x+1)^{2}}+\frac{B}{x+1}+\frac{C x+D}{x^{2}+1}$ and find $A, B, C, D$.]

5.58. Prove that $\int_{0}^{\pi} \frac{d x}{\alpha-\cos x}=\frac{\pi}{\sqrt{\alpha^{2}-1}}, \quad \alpha>1$.

\section*{Numerical methods for evaluating definite integrals}
5.59. Evaluate $\int_{0}^{1} \frac{d x}{1+x}$ approximately, using (a) the trapezoidal rule and (b) Simpson's rule, taking $n=4$. Compare with the exact value, $\ln 2=0.6931$.

5.60. Using (a) the trapezoidal rule and (b) Simpson's rule, evaluate $\int_{0}^{\pi / 2} \sin ^{2} x d x$ by obtaining the values of $\sin ^{2} x$ at $x=0^{\circ}, 10^{\circ}, \ldots, 90^{\circ}$ and compare with the exact value $\pi / 4$.

5.61. Prove (a) the rectangular rule and (b) the trapezoidal rule, i.e., Equations (8) and (9), Page 104.

5.62. Prove Simpson's rule.

5.63. Evaluate the following to three decimal places using numerical integration:

(a) $\int_{1}^{2} \frac{d x}{1+x^{2}}$ (b) $\int_{0}^{1} \cosh x^{2} d x$

Ans. (a) 0.322 (b) 1.105

\section*{Applications}
5.64. Find (a) the area and (b) the moment of inertia about the $y$ axis of the region in the $x y$ plane bounded by $y=$ $\sin x, 0 \leqq x \leqq \pi$, and the $x$ axis, assuming unit density.

$$
\text { Ans. (a) } 2 \text { (b) } \pi^{2}-4
$$

5.65. Find the moment of inertia about the $x$ axis of the region bounded by $y=x^{2}$ and $y=x$, if the density is proportional to the distance from the $x$ axis.

$$
\text { Ans. } \frac{1}{8} M \text {, where } M=\text { mass of the region }
$$

5.66. (a) Show that the arc length of the catenary $y=\cosh x$ from $x=0$ to $x=\ln 2$ is $\frac{3}{4}$. (b) Show that the length of $\operatorname{arc}$ of $y=x^{3 / 2}, 2 \leqq x \leqq 5$ is $\frac{343}{27}-2 \sqrt{2} 11^{3 / 2}$.

5.67. Show that the length of one arc of the cycloid $x=a(\theta-\sin \theta), y=a(1-\cos \theta),(0 \leqq \tau \leqq 2 \pi)$ is $8 a$.

5.68. Prove that the area bounded by the ellipse $x^{2} / a^{2}+y^{2} / b^{2}=1$ is $\pi a b$.

5.69. (a) (Disk method.) Find the volume of the region obtained by revolving the curve $y=\sin x, 0 \leqq x \leqq \pi$, about the $x$ axis.

Ans. (a) $\pi^{2} / 2$

(b) (Disk method.) Show that the volume of the frustrum of a paraboloid obtained by revolving $f(x)=\sqrt{k x}, 0<$ $a \leqq x \leqq b$ about the $x$ axis is $\pi \int_{a}^{b} k x d x=\frac{\pi k}{2}\left(b^{2}-a^{2}\right)$. (c) Determine the volume obtained by rotating the region bound by $f(x)=3, g(x)=5-x^{2}$ on $-\sqrt{2} \leqq x \leqq \sqrt{2}$. (d) (Shell method.) A spherical bead of radius $a$ has a circular cylindrical hole of radius $b, b<a$, through the center. Find the volume of the remaining solid by the shell method. (e) (Shell method.) Find the volume of a solid whose outer boundary is a torus [i.e., the solid is generated by orbiting a circle $(x-a)^{2}-y^{2}=b^{2}$ about the $y$ axis $\left.(a>b)\right]$.

5.70. Prove that the centroid of the region bounded by $y=\sqrt{a^{2}-x^{2}},-a \leqq x \leqq a$, and the $x$ axis is located at $(0,4 a / 3 \pi)$.

5.71. (a) If $\rho=f(\phi)$ is the equation of a curve in polar coordinates, show that the area bounded by this curve and the lines $\phi=\theta$ and $\phi=\phi_{2}$ is $\frac{1}{2} \int_{\rho_{1}}^{\phi} \rho^{2} d \phi$. (b) Find the area bounded by one loop of the lemniscate $\rho^{2}=a^{2} \cos 2 \phi$.

Ans. (b) $a^{2}$

5.72. (a) Prove that the arc length of the curve in Problem 5.71(a) is $\int_{\rho_{1}}^{\phi_{2}} \sqrt{\rho^{2}+(d \rho / d \phi)^{2}} d \phi$. (b) Find the length of arc of the cardioid $\rho=a(1-\cos \phi)$.

Ans. (b) $8 a$

\section*{Miscellaneous problems}
5.73. Establish the mean value theorem for derivatives from the first mean value theorem for integrals. [Hint: Let $f(x)=F^{\prime}(x)$ in (4), Page 100.]

5.74. Prove that (a) $\lim _{\epsilon \rightarrow 0+} \int_{0}^{4-\epsilon} \frac{d x}{\sqrt{4-x}}=4$, (b) $\lim _{\epsilon \rightarrow 0+} \int_{\epsilon}^{3} \frac{d x}{\sqrt[3]{x}}=6$, (c) $\lim _{\epsilon \rightarrow 0+} \int_{0}^{1-\epsilon} \frac{d x}{\sqrt{1-x^{2}}}=\frac{\pi}{2}$, and give a geometric interpretation of the results.

(These limits, denoted usually by $\int_{0}^{4} \frac{d x}{\sqrt{4-x}}, \int_{0}^{3} \frac{d x}{\sqrt[3]{x}}$, and $\int_{0}^{1} \frac{d x}{\sqrt{1-x^{2}}}$, respectively, are called improper integrals of the second kind (see Problem 5.29), since the integrands are not bounded in the range of integration. For further discussion of improper integrals, see Chapter 12.)

5.75. Prove that (a) $\lim _{M \rightarrow \infty} \int_{0}^{M} x^{5} e^{-x} d x=4 !=24$ and (b) $\lim _{\epsilon \rightarrow 0+} \int_{1}^{2-\epsilon} \frac{d x}{\sqrt{x(2-x)}}=\frac{\pi}{2}$.

5.76. Evaluate (a) $\int_{0}^{\infty} \frac{d x}{1+x^{3}}$, (b) $\int_{0}^{\pi / 2} \frac{\sin 2 x}{(\sin x)^{4 / 3}} d x$, and (c) $\int_{0}^{\infty} \frac{d x}{x+\sqrt{x^{2}+1}}$.

Ans. (a) $\frac{2 \pi}{3 \sqrt{3}}$ (b) 3 (c) does not exist

5.77. Evaluate $\lim _{x \rightarrow \pi / 2} \frac{e x^{2} / \pi-e \pi / 4+\int_{x}^{\pi / 2} e^{\sin t} d t}{1+\cos 2 x}$.

Ans. $e / 2 \pi$

5.78. Prove: (a) $\frac{d}{d x} \int_{x^{2}}^{x^{3}}\left(t^{2}+t+1\right) d t=3 x^{3}+x^{5}-2 x^{3}+3 x^{2}-2 x$ and

(b) $\frac{d}{d x} \int_{x}^{x^{2}} \cos t^{2} d t=2 x \cos x^{4}-\cos x^{2}$.

5.79. Prove that (a) $\int_{0}^{\pi} \sqrt{1+\sin x} d x=4$ and (b) $\int_{0}^{\pi / 2} \frac{d x}{\sin x+\cos x}=\sqrt{2} \ln (\sqrt{2}+1)$.

5.80. Explain the fallacy $I=\int_{-1}^{1} \frac{d x}{1+x^{2}}=-\int_{-1}^{1} \frac{d y}{1+y^{2}}=-I$, using the transformation $x=1 / y$. Hence, $I=0$. But $I=\tan ^{-1}(1)-\tan ^{-1}(-1)=\pi / 4-(-\pi / 4)=\pi / 2$. Thus, $\pi / 2=0$.

5.81. Prove that $\int_{0}^{1 / 2} \frac{\cos \pi x}{\sqrt{1+x^{2}}} d x \leqq \frac{1}{4} \tan ^{-1} \frac{1}{2}$.

5.82. Evaluate $\lim _{n \rightarrow \infty}\left\{\frac{\sqrt{n+1}+\sqrt{n+2}+\cdots+\sqrt{2 n-1}}{n^{3 / 2}}\right\}$.

Ans. $\frac{2}{3}(2 \sqrt{2}-1)$

5.83. Prove that $f(x)=\left\{\begin{array}{c}1 \text { if } x \text { is irrational } \\ 0 \text { if } x \text { is rational }\end{array}\right.$ is not Riemann integrable in [0,1].

[Hint: In Equation (2), Page 98, let $\xi_{k}, k=1,2,3, \ldots, n$ be first rational and then irrational points of subdivision and examine the lower and upper sums of Problem 5.31.]

5.84. Prove the result (3) of Problem 5.31. (Hint: First consider the effect of only one additional point of subdivision.)

5.85. In Problem 5.31, prove that $\bar{s} \leqq \underline{S}$. (Hint: Assume the contrary and obtain a contradiction.)

5.86. If $f(x)$ is sectionally continuous in $[a, b]$, prove that $\int_{a}^{b} f(x) d x$ exists. (Hint: Enclose each point of discontinuity in an interval, noting that the sum of the lengths of such intervals can be made arbitrarily small. Then consider the difference between the upper and lower sums.)

5.87. If $f(x)=\left\{\begin{array}{ll}2 x & 0<x<1 \\ 3 & x=1 \\ 6 x-1 & 1<x<2\end{array}\right.$, find $\int_{0}^{2} f(x) d x$. Interpret the result graphically.

Ans. 9

5.88. Evaluate $\int_{0}^{3}\left\{x-[x]+\frac{1}{2}\right\} d x$ where $[x]$ denotes the greatest integer less than or equal to $x$. Interpret the result graphically.

Ans. 3

5.89. (a) Prove that $\int_{0}^{\pi / 2} \frac{\sin ^{m} x}{\sin ^{m} x+\cos ^{m} x} d x=\frac{\pi}{4}$ for all real values of $m$.

(b) Prove that $\int_{0}^{2 \pi} \frac{d x}{1+\tan ^{4} x}=\pi$.

5.90. Prove that $\int_{0}^{\pi / 2} \frac{\sin x}{x} d x$ exists.

5.91. Show that $\int_{0}^{0.5} \frac{\tan ^{-1} x}{x} d x=0.4872$ approximately.

5.92. Show that $\int_{0}^{\pi} \frac{x d x}{1+\cos ^{2} x}=\frac{\pi^{2}}{2 \sqrt{2}}$.

This page intentionally left blank

\section*{CHAPTER 6}
\section*{Partial Derivatives}
\section*{Functions of Two or More Variables}
The definition of a function was given in Chapter 3 (page 43). For us, the distinction for functions of two or more variables is that the domain is a set of $n$-tuples of numbers. The range remains one-dimensional and is referred to an interval of numbers. If $n=2$, the domain is pictured as a two-dimensional region. The region is referred to a rectangular Cartesian coordinate system described through number pairs $(x, y)$, and the range variable is usually denoted by $z$. The domain variables are independent, while the range variable is dependent.

We use the notation $f(x, y), F(x, y)$, etc., to denote the value of the function at $(x, y)$ and write $z=f(x, y)$, $z=F(x, y)$, etc. We also sometimes use the notation $z=z(x, y)$, although it should be understood that in this case $z$ is used in two senses, namely, as a function and as a variable.

EXAMPLE. If $f(x, y)=x^{2}+2 y^{3}$, then $f(3,-1)=(3)^{2}+2(-1)^{3}=7$.

The concept is easily extended. Thus, $w=F(x, y, z)$ denotes the value of a function at $(x, y, z)$ (a point in three-dimensional space), etc.

EXAMPLE. If $z=\sqrt{1-\left(x^{2}+y^{2}\right)}$, the domain for which $z$ is real consists of the set of points $(x, y)$ such that $x^{2}+y^{2} \leqq 1$, i.e., the set of points inside and on a circle in the $x y$ plane having center at $(0,0)$ and radius 1 .

A three-dimensional rectangular Cartesian coordinate system is obtained by constructing three mutually perpendicular axes (the $x, y$, and $z$ axes) intersecting in a point (designated by 0 and called the origin). This is a natural extension of the rectangular system $x, y$ in the plane. A point in the three-dimensional Cartesian system is represented by the triple of coordinates $(x, y, z)$. The collection of points $P(x, y, z)$, represented by the implicit equation $F(x, y, z)=0$, is a surface. The term surface is used in a very broad sense and requires refinement according to the context in which it is to be used. For example, $x^{2}+y^{2}+z^{2}=r^{2}$ is the algebraic representation of a surface in the large. This form might be employed in topology to indicate the property of being closed rather than open. In analysis, which is the subject of this outline of advanced calculus, the concern is with portions of a surface - that is, points and their neighborhoods. These may be obtained from implicit representations by imposing restrictions. For example,

$$
\mathrm{z}=\sqrt{r^{2}-\left(x^{2}+y^{2}\right)} \text { with } 1 x^{2}+y^{2} 1<r
$$

signifies an open upper hemisphere. Problems in surface theory employ partial derivatives and relate to a point of a surface, the collection of points about it, the tangent plane at the point, and the properties of continuity and differentiability binding this structure. These concepts will be discussed in the following pages.

For functions of more than two variables such geometric interpretation fails, although the terminology is still employed. For example, $(x, y, z, w)$ is a point in four-dimensional space, and $w=f(x, y, z)$ [or $F(x, y, z$, $w)=0]$ represents a hypersurface in four dimensions; thus, $x^{2}+y^{2}+z^{2}+w^{2}=a^{2}$ represents a hypersphere in four dimensions with radius $a>0$ and center at $(0,0,0,0) . w=\sqrt{a^{2}-\left(x^{2}+y^{2}+z^{2}\right)}, x^{2}+y^{2}+z^{2} \leqq a^{2}$ describes a function generated from the hypersphere.

\section*{Neighborhoods}
The set of all points $(x, y)$ such that $\left|x-x_{0}\right|<\delta,\left|y-y_{0}\right|<\delta$ where $\delta>0$ is called a rectangular $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$; the set $0<\left|x-x_{0}\right|<\delta, 0<\left|y-y_{0}\right|<\delta$, which excludes $\left(x_{0}, y_{0}\right)$, is called a rectangular deleted $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$. Similar remarks can be made for made for other neighborhoods; e.g., $(x$ $\left.-x_{0}\right)^{2}+\left(y-y_{0}\right)^{2}<\delta^{2}$ is a circular $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$. The term open ball is used to designate this circular neighborhood. This terminology is appropriate for generalization to more dimensions. Whether neighborhoods are viewed as circular or square is immaterial, since the descriptions are interchangeable. Simply notice that given an open ball (circular neighborhood) of radius $\delta$ there is a centered square whose side is of length less than $\sqrt{2} \delta$ that is interior to the open ball, and, conversely, for a square of side $\delta$ there is an interior centered circle of radius less than $\delta / 2$. (See Figure 6.1.)

A point $\left(x_{0}, y_{0}\right)$ is called a limit point, accumulation point, or cluster

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-137(1)}
\end{center}

Figure 6.1 point of a point set $S$ if every deleted $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$ contains points of $S$. As in the case of one-dimensional point sets, every bounded infinite set has at least one limit point (the Bolzano-Weierstrass theorem; see Chapter 1). A set containing all its limit points is called a closed set.

\section*{Regions}
A point $P$ belonging to a point set $S$ is called an interior point of $S$ if there exists a deleted $\delta$ neighborhood of $P$ all of whose points belong to $S$. A point $P$ not belonging to $S$ is called an exterior point of $S$ if there exists a deleted $\delta$ neighborhood of $P$ all of whose points do not belong to $\mathrm{S}$. A point $P$ is called a boundary point of $S$ if every deleted $\delta$ neighborhood of $P$ contains points belonging to $S$ and also points not belonging to $S$.

If any two points of a set $S$ can be joined by a path consisting of a finite number of broken line segments all of whose points belong to $S$, then $S$ is called a connected set. A region is a connected set which consists of interior points or interior and boundary points. A closed region is a region containing all its boundary points. An open region consists only of interior points. The complement of a set $S$ in the $x y$ plane is the set of all points in the plane not belonging to $S$. (See Figure 6.2.)

Examples of some regions are shown graphically in Figure 6.3(a), (b), and (c) . The rectangular region of Figure 6.1(a), including the boundary, represents the sets of points $a \leqq x \leqq b, c \leqq y \leqq d$ which

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-137(3)}
\end{center}

Figure 6.2 is a natural extension of the closed interval $a \leqq x \leqq b$ for one dimension. The set $a<x<b, c<y<d$ corresponds to the boundary being excluded.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-137}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-137(4)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-137(2)}
\end{center}

(c)

Figure 6.3

In the regions of Figure 6.3(a) and (b), any simple closed curve (one which does not intersect itself anywhere) lying inside the region can be shrunk to a point which also lies in the region. Such regions are called simply connected regions. In Figure 6.3(c), however, a simple closed curve $A B C D$ surrounding one of the "holes" in the region cannot be shrunk to a point without leaving the region. Such regions are called multiply connected regions.

\section*{Limits}
Let $f(x, y)$ be defined in a deleted $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$ [i.e., $f(x, y)$ may be undefined at $\left(x_{0}, y_{0}\right)$ ]. We say that $l$ is the limit of $f(x, y)$ as $x$ approaches $x_{0}$ and $y$ approaches $y_{0}\left[\right.$ or $(x, y)$ approaches $\left.\left(x_{0}, y_{0}\right)\right]$ and write $\lim _{x \rightarrow x_{0}} f(x, y)=l$ [or $\left.\lim _{(x, y) \rightarrow\left(x_{0}, y_{0}\right)} f(x, y)=l\right]$ if for any positive number $\delta$ we can find some positive number $y \rightarrow y_{0}$

$\delta$ [depending on $\delta$ and $\left(x_{0}, y_{0}\right)$, in general] such that $|f(x, y)-l|<\delta$ whenever $0<\left|x-x_{0}\right|<\delta$ and $0<$ $\left|y-y_{0}\right|<\delta$.

If desired, we can use the deleted circular neighborhood open ball $0<\left(x-x_{0}\right)^{2}+\left(y-y_{0}\right)^{2}<\delta^{2}$ instead of the deleted rectangular neighborhood.

EXAMPLE. Let $f(x, y)=\left\{\begin{array}{ll}3 x y & \text { if }(x, y) \neq(1,2) \\ 0 & \text { if }(x, y)=(1,2)\end{array}\right.$ : As $x \rightarrow 1$ and $y \rightarrow 2[$ or $(x, y) \rightarrow(1,2)], f(x, y)$ gets

closer to $3(1)(2)=6$ and we suspect that $\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} f(x, y)=6$. To prove this, we must show that the preceding defi-

nition of limit, with $l=6$, is satisfied. Such a proof can be supplied by a method similar to that of Problem 6.4.

Note that $\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} f(x, y) \neq f(1,2)$ since $f(1,2)=0$. The limit would, in fact, be 6 even if $f(x, y)$ were not defined at $(1,2)$. Thus, the existence of the limit of $f(x, y)$ as $(x, y) \rightarrow\left(x_{0}, y_{0}\right)$ is in no way dependent on the existence of a value of $f(x, y)$ at $\left(x_{0}, y_{0}\right)$.

Note that in order for $\lim _{(x, y) \rightarrow\left(x_{0}, y_{0}\right)} f(x, y)$ to exist, it must have the same value regardless of the approach of $(x, y)$ to $\left(x_{0}, y_{0}\right)$. It follows that if two different approaches give different values, the limit cannot exist (see Problem 6.7). This implies, as in the case of functions of one variable, that if a limit exists it is unique.

The concept of one-sided limits for functions of one variable is easily extended to functions of more than one variable.

EXAMPLE 1. $\lim _{\substack{x \rightarrow 0+\\ y \rightarrow 1}} \tan ^{-1}(y / x)=\pi / 2, \lim _{\substack{x \rightarrow 0-\\ y \rightarrow 1}} \tan ^{-1}(y / x)=-\pi / 2$

EXAMPLE 2. $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 1}} \tan ^{-1}(y / x)$ does not exist, as is clear from the fact that the two different approaches of Example 1 give different results.

In general, the theorems on limits, concepts of infinity, etc., for functions of one variable (see Page 25) apply as well, with appropriate modifications, to functions of two or more variables.

\section*{Iterated Limits}
The iterated limits $\lim _{x \rightarrow x_{0}}\left\{\lim _{y \rightarrow y_{0}} f(x, y)\right\}$ and $\lim _{y \rightarrow y_{0}}\left\{\lim _{x \rightarrow x_{0}} f(x, y)\right\}$ [also denoted by $\lim _{x \rightarrow x_{0}} \lim _{y \rightarrow y_{0}} f(x, y)$ and $\lim _{y \rightarrow y_{0}} \lim _{x \rightarrow x_{0}}$ $f(x, y)$, respectively] are not necessarily equal. Although they must be equal if $\lim f(x, y)$ is to exist, their equality does not guarantee the existence of this last limit.

$$
\begin{aligned}
& x \rightarrow x_{0} \\
& y \rightarrow y_{0}
\end{aligned}
$$

EXAMPLE. If $(x, y)=\frac{x-y}{x+y}$, then $\lim _{x \rightarrow 0}\left(\lim _{y \rightarrow 0} \frac{x-y}{x+y}\right)=\lim _{y \rightarrow 0}(1)=1$ and $\lim _{y \rightarrow 0}\left(\lim _{x \rightarrow 0} \frac{x-y}{x+y}\right)=\lim _{y \rightarrow 0}(-1)=-1$.

Thus, the iterated limits are not equal and so $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} f(x, y)$ cannot exist.

\section*{Continuity}
Let $f(x, y)$ be defined in a $\delta$ neighborhood of $\left(x_{0}, y_{0}\right)$ [i.e., $f(x, y)$ must be defined at $\left(x_{0}, y_{0}\right)$ as well as near it]. We say that $f(x, y)$ is continuous at $\left(x_{0}, y_{0}\right)$ if for any positive number $\delta$ we can find some positive number $\delta$ [depending on $\delta$ and $\left(x_{0}, y_{0}\right)$ in general] such that $\left|f(x, y)-f\left(x_{0}, y_{0}\right)\right|<\delta$ whenever $\left|x-x_{0}\right|<\delta$ and $\left|y-y_{0}\right|$ $<\delta$, or, alternatively, $\left(x-x_{0}\right)^{2}+\left(y-y_{0}\right)^{2}<\delta^{2}$.

Note that three conditions must be satisfied in order for $f(x, y)$ to be continuous at $\left(x_{0}, y_{0}\right)$ :

\begin{enumerate}
  \item $\quad \lim _{(x, y) \rightarrow\left(x_{0}, y_{0}\right)} f(x, y)=l$; i.e., the limit exists as $(x, y) \rightarrow\left(x_{0}, y_{0}\right)$.

  \item $f\left(x_{0}, y_{0}\right)$ must exist; i.e., $f(x, y)$ is defined at $\left(x_{0}, y_{0}\right)$.

  \item $l=f\left(x_{0}, y_{0}\right)$.

\end{enumerate}

If desired, we can write this in the suggestive form $\lim _{\substack{x \rightarrow x_{0} \\ y \rightarrow y_{0}}} f(x, y)=f\left(\lim _{x \rightarrow x_{0}} x, \lim _{y \rightarrow y_{0}} y\right)$.

EXAMPLE. If $f(x, y)=\left\{\begin{array}{ll}3 x y & (x, y) \neq(1,2) \\ 0 & (x, y)=(1,2)\end{array}\right.$, then $\lim _{(x, y) \rightarrow(1,2)} f(x, y)=6 \neq f(1,2)$. Hence, $f(x, y)$ is not

continuous at $(1,2)$. If we redefine the function so that $f(x, y)=6$ for $(x, y)=(1,2)$, then the function is continuous at $(1,2)$.

If a function is not continuous at a point $\left(x_{0}, y_{0}\right)$, it is said to be discontinuous at $\left(x_{0}, y_{0}\right)$, which is then called a point of discontinuity. If, as in the preceding example, it is possible to redefine the value of a function at a point of discontinuity so that the new function is continuous, we say that the point is a removable discontinuity of the old function. A function is said to be continuous in a region $\Re$ of the $x y$ plane if it is continuous at every point of $\Re$.

Many of the theorems on continuity for functions of a single variable can, with suitable modification, be extended to functions of two more variables.

\section*{Uniform Continuity}
In the definition of continuity of $f(x, y)$ at $\left(x_{0}, y_{0}\right), \delta$ depends on $\delta$ and also $\left(x_{0}, y_{0}\right)$ in general. If in a region $\Re$ we can find a $\delta$ which depends only on $\delta$ but not on any particular point $\left(x_{0}, y_{0}\right)$ in $\Re$ (i.e., the same $\delta$ will work for all points in $\Re$ ), then $f(x, y)$ is said to be uniformly continuous in $\Re$. As in the case of functions of one variable, it can be proved that a function which is continuous in a closed and bounded region is uniformly continuous in the region.

\section*{Partial Derivatives}
The ordinary derivative of a function of several variables with respect to one of the independent variables, keeping all other independent variables constant, is called the partial derivative of the function with respect to the variable. Partial derivatives of $f(x, y)$ with respect to $x$ and $y$ are denoted by $\frac{\partial}{\partial x}\left[\right.$ or $\left.f_{x}, f_{x}(x, y),\left.\frac{\partial f}{\partial x}\right|_{y}\right]$ and $\frac{\partial f}{\partial y}\left[\right.$ or $\left.f_{y}, f_{y}(x, y),\left.\frac{\partial f}{\partial y}\right|_{x}\right]$, respectively, the latter notations being used when needed to emphasize which variables are held constant.

By definition,


\begin{equation*}
\frac{\partial f}{\partial x}=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x, y)-f(x, y)}{\Delta x}, \quad \frac{\partial f}{\partial y}=\lim _{\Delta y \rightarrow 0} \frac{f(x, y+\Delta y)-f(x, y)}{\Delta y} \tag{1}
\end{equation*}


when these limits exist. The derivatives evaluated at the particular point $\left(x_{0}, y_{0}\right)$ are often indicated by $\left.\frac{\partial f}{\partial x}\right|_{\left(x_{0}, y_{0}\right)}=f_{x}\left(x_{0}, y_{0}\right)$ and $\left.\frac{\partial f}{\partial y}\right|_{\left(x_{0}, y_{0}\right)}=f_{x}\left(x_{0}, y_{0}\right)$, respectively.

EXAMPLE. If $f(x, y)=2 x^{3}+3 x y^{2}$, then $f_{x}=\partial f / \partial x=6 x^{2}+3 y^{2}$ and $f_{y}=\partial f / \partial y=6 x y$. Also, $f(1,2)=6(1)^{2}+3(2)^{2}$ $=18, f_{y}(1,2)=6(1)(2)=12$.

If a function $f$ has continuous partial derivatives $\partial f / \partial x, \partial f / \partial y$ in a region, then $f$ must be continuous in the region. However, the existence of these partial derivatives alone is not enough to guarantee the continuity of $f$ (see Problem 6.9).

\section*{Higher-Order Partial Derivatives}
If $f(x, y)$ has partial derivatives at each point $(x, y)$ in a region, then $\partial f / \partial x$ and $\partial f / \partial y$ are themselves functions of $x$ and $y$, which may also have partial derivatives. These second derivatives are denoted by are denoted by


\begin{equation*}
\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right)=\frac{\partial^{2} f}{\partial x^{2}}=f_{x x}, \quad \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right)=\frac{\partial^{2} f}{\partial y^{2}}=f_{y y}, \quad \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right)=\frac{\partial^{2} f}{\partial x \partial y}=f_{y x}, \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right)=\frac{\partial^{2} f}{\partial y \partial x}=f_{x y} \tag{2}
\end{equation*}


If $f_{x y}$ and $f_{y x}$ are continuous, then $f_{x y}=f_{y x}$ and the order of differentiation is immaterial; otherwise they may not be equal (see Problems 6.13 and 6.41).

EXAMPLE. If $f(x, y)=2 x^{3}+3 x y^{2}$ (see preceding example), then $f_{x x}=12 x, f_{y y}=6 x$, and $f_{x y}=6 y=f_{v x}$. In such case $f_{x x}(1,2)=12, f_{y y}(1,2)=6$, and $f_{x y}(1,2)=f_{y x}(1,2)=12$ In a similar manner, higher order derivatives are defined. For example, $\frac{\partial^{3} f}{\partial x^{2} \partial y}=f_{y x x}$ is the derivative of $f$\\
taken once with respect to $y$ and twice with respect to $x$.

\section*{Differentials}
(The section on differentials in Chapter 4 should be read before beginning this one.)

Let $\Delta x=d x$ and $\Delta y=d y$ be increments given to $x$ and $y$, respectively. Then


\begin{equation*}
\Delta z=f(x+\Delta x, y+\Delta y)-f(x, y)=\Delta f \tag{3}
\end{equation*}


is called the increment in $z=f(x, y)$. If $f(x, y)$ has continuous first partial derivatives in a region, then


\begin{equation*}
\Delta z=\frac{\partial f}{\partial x} \Delta x+\frac{\partial f}{\partial y} \Delta y+\epsilon_{1} \Delta x+\epsilon_{2} \Delta y=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y+\epsilon_{1} d x+\epsilon_{2} d y=\Delta f \tag{4}
\end{equation*}


where $\epsilon_{1}$ and $\epsilon_{2}$ approach zero as $\Delta x$ and $\Delta y$ approach zero (see Problem 6.14). The expression


\begin{equation*}
d z=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y \quad \text { or } \quad d f=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y \tag{5}
\end{equation*}


is called the total differential or simply the differential of $z$ or $f$, or the principal part of $\Delta z$ or $\Delta f$. Note that $\Delta z \neq d z$ in general. However, if $\Delta x=d x$ and $\Delta y=d y$ are "small," then $d z$ is a close approximation of $\Delta z$ (see Problem 6.15). The quantities $d x$ and $d y$ —called differentials of $x$ and $y$, respectively-need not be small.

The form $d z=f_{x}\left(x_{0}, y_{0}\right) d x+f_{y}\left(x_{0}, y_{0}\right) d y$ signifies a linear function with the independent variables $d x$ and $d y$ and the dependent range variable $d z$. In the one-variable case, the corresponding linear function represents the tangent line to the underlying curve. In this case, the underlying entity is a surface and the linear function generates the tangent plane at $P_{0}$. In a small enough neighborhood, this tangent plane is an approximation of the surface (i.e., the linear representation of the surface at $P_{0}$ ). If $y$ is held constant, then we obtain the curve\\
of intersection of the surface and the coordinate plane $y=y_{0}$. The differential form reduces to $d z=f_{x}\left(x_{0}, y_{0}\right)$ $d x$ (i.e., the one-variable case). A similar statement follows when $x$ is held constant. (See Figure 6.4.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-141}
\end{center}

Figure 6.4

If $f$ is such that $\Delta f$ (or $\Delta z$ ) can be expressed in the form of Equation (4) where $\epsilon_{1}$ and $\epsilon_{2}$ approach zero as $\Delta x$ and $\Delta y$ approach zero, we call $f$ differentiable at $(x, y)$. The mere existence of $f_{x}$ and $f_{y}$ does not in itself guarantee differentiability; however, continuity of $f_{x}$ and $f_{y}$ does (although this condition happens to be slightly stronger than necessary). In case $f_{x}$ and $f_{y}$ are continuous in a region $\mathfrak{R}$, we say that $f$ is continuously differentiable in $\Re$.

\section*{Theorems on Differentials}
In the following, we assume that all functions have continuous first partial derivatives in a region $\Re$; i.e., the functions are continuously differentiable in $\Re$.

\begin{enumerate}
  \item If $z=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, then
\end{enumerate}


\begin{equation*}
d f=\frac{\partial f}{\partial x_{1}} d x_{1}+\frac{\partial f}{\partial x_{2}} d x_{2}+\cdots+\frac{\partial f}{\partial x_{n}} d x_{n} \tag{6}
\end{equation*}


regardless of whether the variables $x_{1}, x_{2}, \ldots, x_{n}$ are independent or dependent on other variables (see Problem 6.20). This is a generalization of result in Equation (5). In Euqtion (6) we often use $z$ in place of $f$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)=c$, a constant, then $d f=0$. Note that in this case $x_{1}, x_{2}, \ldots, x_{n}$ cannot all be independent variables.

  \item The expression $P(x, y) d x+Q(x, y) d y$, or, briefly, $P d x+Q d y$, is the differential of $f(x, y)$ if and only if $\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}$. In such case, $P d x+Q d y$ is called an exact differential.

\end{enumerate}

Note: Observe that $\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}$ implies that $\frac{\partial^{2} f}{\partial y \partial x}=\frac{\partial^{2} f}{\partial x \partial y}$.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item The expression $P(x, y, z) d x+Q(x, y, z) d y+R(x, y, z) d z$, or, briefly, $P d x+Q d y+R d z$, is the differential of $f(x, y, z)$ if and only if $\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}, \frac{\partial Q}{\partial z}=\frac{\partial R}{\partial y}, \frac{\partial R}{\partial x}=\frac{\partial P}{\partial z}$. In such case, $P d x+Q d y+R d z$ is called an exact differential.
\end{enumerate}

Proofs of Theorems 3 and 4 are best supplied by methods of later chapters (see Problems 10.13 and 10.30).

\section*{Differentiation of Composite Functions}
Let $z=f(x, y)$ where $x=g(r, s), y=h(r, s)$ so that $z$ is a function of $r$ and $s$. Then


\begin{equation*}
\frac{\partial z}{\partial r}=\frac{\partial z}{\partial x} \frac{\partial x}{\partial r}+\frac{\partial z}{\partial y} \frac{\partial y}{\partial r}, \quad \frac{\partial z}{\partial s}=\frac{\partial z}{\partial x} \frac{\partial x}{\partial s}+\frac{\partial z}{\partial y} \frac{\partial y}{\partial s} \tag{7}
\end{equation*}


In general, if $u=F\left(x_{1}, \ldots, x_{n}\right)$ where $x_{1}=f_{1}\left(r_{1}, \ldots, r_{p}\right), \ldots, x_{n}=f_{n}\left(r_{1}, \ldots, r_{p}\right)$, then


\begin{equation*}
\frac{\partial u}{\partial r_{k}}=\frac{\partial u}{\partial x_{1}} \frac{\partial x_{1}}{\partial r_{k}}+\frac{\partial u}{\partial x_{2}} \frac{\partial x_{2}}{\partial r_{k}}+\cdots+\frac{\partial u}{\partial x_{n}} \frac{\partial u}{\partial r_{k}} \quad k=1,2, \ldots, p \tag{8}
\end{equation*}


If, in particular, $x_{1}, x_{2}, \ldots, x_{n}$ depend on only one variable $s$, then


\begin{equation*}
\frac{d u}{d s}=\frac{\partial u}{\partial x_{1}} \frac{d x_{1}}{d s}+\frac{\partial u}{\partial x_{2}} \frac{d x_{2}}{d s}+\cdots+\frac{\partial u}{\partial x_{n}} \frac{d x_{n}}{d s} \tag{9}
\end{equation*}


These results, often called chain rules, are useful in transforming derivatives from one set of variables to another.

Higher derivatives are obtained by repeated application of the chain rules.

\section*{Euler's Theorem on Homogeneous Functions}
A function represented by $F\left(x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{n}\right)$ is called homogeneous of degree $p$ if, for all values of the parameter $\lambda$ and some constant $p$, we have the identity


\begin{equation*}
F\left(\lambda x_{1}, \lambda x_{2}, \ldots, \lambda x_{n}\right)=\lambda^{p} F\left(x_{1}, x_{2}, \ldots, x_{n}\right) \tag{10}
\end{equation*}


EXAMPLE. $F(x, y)=x^{4}+2 x y^{3}-5 y^{4}$ is homogeneous of degree 4 , since

$$
F(\lambda x, \lambda y)=(\lambda x)^{4}+2(\lambda x)(\lambda y)^{3}-5(\lambda y)^{4}=\lambda^{4}\left(x^{4}+2 x y^{3}-5 y^{4}\right)=\lambda^{4} F(x, y)
$$

Euler's theorem on homogeneous functions states that if $F\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is homogeneous of degree $p$, then (see Problem 6.25)


\begin{equation*}
x_{1} \frac{\partial F}{\partial x_{1}}+x_{2} \frac{\partial F}{\partial x_{2}}+\cdots+x_{n} \frac{\partial F}{\partial x_{n}}=p F \tag{11}
\end{equation*}


\section*{Implicit Functions}
In general, an equation such as $F(x, y, z)=0$ defines one variable-say, z-as a function of the other two variables $x$ and $y$. Then $z$ is sometimes called an implicit function of $x$ and $y$, as distinguished from an explicit function $f$, where $z=f(x, y)$, which is such that $F[x, y, f(x, y)] \equiv 0$.

Differentiation of implicit functions requires considerable discipline in interpreting the independent and dependent character of the variables and in distinguishing the intent of one's notation. For example, suppose that in the implicit equation $F[x, y, f(x, z)]=0$, the independent variables are $x$ and $y$ and that $z=f(x, y)$. In order to find $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, we initially write the following [observe that $F(x, t, z)$ is zero for all domain pairs $(x, y)$; i.e., it is a constant]:

$$
0=d F=F_{x} d x+F_{y} d y+F_{z} d z
$$

and then compute the partial derivatives $F_{x}, F_{y}, F_{z}$ as though $y, y, z$ constituted an independent set of variables. At this stage we invoke the dependence of $z$ on $x$ and $y$ to obtain the differential form $d z=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y$. Upon substitution and some algebra (see Problem 6.30), the following results are obtained:

$$
\frac{\partial f}{\partial x}=-\frac{F_{x}}{F_{z}}, \quad \frac{\partial f}{\partial y}=-\frac{F_{y}}{F_{z}}
$$

EXAMPLE. If $0=F(x, y, z)=x^{2} z+y z^{2}+2 x y^{2}-z^{3}$ and $z=f(x, y)$, then $F_{x}=2 x z+2 y^{2}, F_{y}=z^{2}+4 x y$. $F_{z}=x^{2}+2 y z-3 z^{2}$. Then

$$
\frac{\partial f}{\partial x}=-\frac{\left(2 x z+2 y^{2}\right)}{x^{2}+2 y z-3 z^{2}}, \quad \frac{\partial f}{\partial y}=-\frac{\left(z^{2}+4 x y\right)}{x^{2}+2 y z-3 x^{2}}
$$

Observe that $f$ need not be known to obtain these results. If that information is available, then (at least theoretically) the partial derivatives may be expressed through the independent variables $x$ and $y$.

\section*{Jacobians}
If $F(u, v)$ and $G(u, v)$ are differentiable in a region, the Jacobian determinant, or the Jacobian, of $F$ and $G$ with respect to $u$ and $v$ is the second-order functional determinant defined by

\[
\frac{\partial(F, G)}{\partial(u, v)}=\left|\begin{array}{ll}
\frac{\partial F}{\partial u} & \frac{\partial F}{\partial v}  \tag{12}\\
\frac{\partial G}{\partial u} & \frac{\partial G}{\partial v}
\end{array}\right|=\left|\begin{array}{ll}
F_{u} & F_{v} \\
G_{u} & G_{v}
\end{array}\right|
\]

Similarly, the third-order determinant

$$
\frac{\partial(F, G, H)}{\partial(u, v, w)}=\left|\begin{array}{lll}
F_{u} & F_{v} & F_{w} \\
G_{u} & G_{v} & G_{w} \\
H_{u} & H_{v} & H_{w}
\end{array}\right|
$$

is called the Jacobian of $F, G$, and $H$ with respect to $u, v$, and $w$. Extensions easily made.

\section*{Partial Derivatives Using Jacobians}
Jacobians often prove useful in obtaining partial derivatives of implicit functions. Thus, for example, given the simultaneous equations

$$
F(x, y, u, v)=0, \quad G(x, y, u, v)=0
$$

we may, in general, consider $u$ and $v$ as functions of $x$ and $y$. In this case, we have (see Problem 6.31)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-143}
\end{center}

The ideas are easily extended. Thus, if we consider the simultaneous equations

$$
F(u, v, w, x, y)=0, \quad G(u, v, w, x, y) y)=0, \quad H(u, v, w, x, y)=0
$$

we may, for example, consider $u, v$, and $w$ as functions of $x$ and $y$. In this case,

$$
\frac{\partial u}{\partial x}=-\frac{\frac{\partial(F, G, H)}{\partial(x, v, w)}}{\frac{\partial(F, G, H)}{\partial(u, v, w)}}, \quad \frac{\partial w}{\partial y}=-\frac{\frac{\partial(F, G, H)}{\partial(u, v, y)}}{\frac{\partial(F, G, H)}{\partial(u, v, w)}}
$$

with similar results for the remaining partial derivatives (see Problem 6.33).

\section*{Theorems on Jacobians}
In the following we assume that all functions are continuously differentiable.

\begin{enumerate}
  \item A necessary and sufficient condition that the equations $F(u, v, x, y, z)=0$ and $G(u, v, x, y, z)=0$ can be solved for $u$ and $v$ (for example) is that $\frac{\partial(F, G)}{\partial(u, v)}$ is not identically zero in a region $\Re$.
\end{enumerate}

Similar results are valid for $m$ equations in $n$ variables, where $m<n$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If $x$ and $y$ are functions of $u$ and $v$ while $u$ and $v$ are functions of and $s$, then (see Problem 6.43)
\end{enumerate}


\begin{equation*}
\frac{\partial(x, y)}{\partial(r, s)}=\frac{\partial(x, y)}{\partial(u, v)} \frac{\partial(u, v)}{\partial(r, s)} \tag{13}
\end{equation*}


This is an example of a chain rule for Jacobians. These ideas are capable of generalization (see Problems 6.107 and 6.109 , for example).

\begin{enumerate}
  \setcounter{enumi}{2}
  \item If $u=f(x, y)$ and $v=g(x, y)$, then a necessary and sufficient condition that a functional relation of the form $\phi(u, v)=0$ exists between $u$ and $v$ is that $\frac{\partial(u, v)}{\partial(x, y)}$ be identically zero Similar results. hold for $n$ functions of $n$ variables.
\end{enumerate}

Further discussion of Jacobians appears in Chapter 7, where vector interpretations are employed.

\section*{Transformations}
The set of equations

\[
\left\{\begin{array}{l}
x=F(u, v)  \tag{14}\\
y=F(u, v)
\end{array}\right.
\]

defines, in general, a transformation or mapping which establishes a correspondence between points in the $u v$ and $x y$ planes. If to each point in the $u v$ plane there corresponds one and only one the $x y$ plane, and conversely, we speak of a one-to-one transformation or mapping. This will be so if $F$ and $G$ are continuously differentiable, with Jacobian not identically zero in a region. In such case (which we assume unless otherwise stated), Equations (14) are said to define a continuously differentiable transformation or mapping.

The words transformation and mapping describe the same mathematical concept in different ways. A transformation correlates one coordinate representation of a region of space with another. A mapping views this correspondence as a correlation of two distinct regions.

Under the transformation (14) a closed region $\Re$ of the $x y$ plane is, in general, mapped into a closed region $\Re^{\prime}$ of the $u v$ plane. Then if $\Delta A_{x y}$ and $\Delta A_{u v}$ denote, respectively, the areas of these regions, we can show that


\begin{equation*}
\lim \frac{\Delta A_{x y}}{\Delta A_{\iota 0}}=\left|\frac{\partial(x, y)}{\partial(u, v)}\right| \tag{15}
\end{equation*}


where lim denotes the limit as $\Delta A_{x y}$ (or $\Delta A_{u v}$ ) approaches zero. The Jacobian on the right of Equation (15) is often called the Jacobian of the transformation (14).

If we solve transformation (14) for $u$ and $v$ in terms of $x$ and $y$, we obtain the transformation $u=f(x, y), v=$ $g(x, y)$, often called the inverse transformation corresponding to (14). The Jacobians $\frac{\partial(u, v)}{\partial(x, y)}$ and $\frac{\partial(x, y)}{\partial(u, v)}$ of these transformations are reciprocals of each other (see Problem 6.43). Hence, if one Jacobian is different from zero in a region, the inverse exists and is not zero.

These ideas can be extended to transformations in three or higher dimensions. We deal further with these topics in Chapter 7, where use is made of the simplicity of vector notation and interpretation.

\section*{Curvilinear Coordinates}
Rectangular Cartesian coordinates in the Euclidean plane or in three-dimensional space were mentioned at the beginning of this chapter. Other coordinate systems, the coordinate curves of which are generated from families that are not necessarily linear, are useful. These are called curvilinear coordinates.

EXAMPLE 1. Polar coordinates $\rho, \Phi$ are related to rectangular Cartesian coordinates through the transformation $x=\rho \cos \Phi$ and $y=\rho \sin \Phi$. The curves $\Phi=\Phi_{0}$ are radical lines, while $\rho=r_{0}$ are concentric circles. Among the convenient representations yielded by these coordinates are circles with the center at the origin. A weakness is that representations may not be defined at the origin.

EXAMPLE 2. Spherical coordinates $\mathrm{r}, \Theta, \Phi_{0}$ for Euclidean three-dimensional space are generated from rectangular Cartesian coordinates by the transformation equations $x=\mathrm{r} \sin \Theta \cos \Phi, y=\mathrm{r} \sin \Theta \sin \Phi$, and $z=\mathrm{r} \cos$ $\Phi$. Again, certain problems lend themselves to spherical coordinates, and also uniqueness of representation can be a problem at the origin. The coordinate surfaces $r=\mathrm{r}_{0}, \Theta=\Theta_{0}$, and $\Phi=\Phi_{0}$ are spheres, planes, and cones, respectively. The coordinate curves are the intersections of these surfaces, i.e., circles, circles, and lines.

For curvilinear coordinates in higher dimensional spaces, see Chapter 7.

\section*{Mean Value Theorem}
If $f(x, y)$ is continuous in a closed region and if the first partial derivatives exist in the open region (i.e., excluding boundary points). then


\begin{equation*}
f\left(x_{0}+h, y_{0}+k\right)-f\left(x_{0}, y_{0}\right)=h f_{x}\left(x_{0}+\theta h, y_{0}+\theta k\right)+k f_{y}\left(x_{0}+\theta h, y_{0}+\theta k\right) \quad 0<\theta<1 \tag{16}
\end{equation*}


This is sometimes written in a form in which $h=\Delta x=x-x_{0}$ and $k=\Delta y=y-y_{0}$.

\section*{SOLVED PROBLEMS}
\section*{Functions and graphs}
6.1. If $f(x, y)=x^{3}-2 x y+3 y^{2}$, find:\\
(a) $f\left(\frac{1}{x}, \frac{2}{y}\right)$;\\
(c) $\frac{f(x, y+k)-f(x, y)}{k}, k \neq 0$.

(a) $f(-2,3)=(-2)^{3}-2(-2)(3)+3(3)^{2}=-8+12+27=31$

(b) $f\left(\frac{1}{x}, \frac{2}{y}\right)=\left(\frac{1}{x}\right)^{3}-2\left(\frac{1}{x}\right)\left(\frac{2}{y}\right)+3\left(\frac{2}{y}\right)^{2}=\frac{1}{x^{3}}-\frac{4}{x y}+\frac{12}{y^{2}}$

(c) $\frac{f(x, y+k)-f(x, y)}{k}=\frac{1}{k}\left\{\left[x^{3}-2 x(y+k)+3(y+k)^{2}\right]-\left[x^{3}-2 x y+3 y^{2}\right]\right\}$

$=\frac{1}{k}\left(x^{3}-2 x y-2 k x+3 y^{2}+6 k y+3 k^{2}-x^{2}+2 x y-3 y^{2}\right)$

$=\frac{1}{k}\left(-2 k x=6 k y+3 k^{2}\right)=-2 x+6 y+3 k$.

6.2. Give the domain of definition for which each of the following functions is defined and real, and indicate this domain graphically.

(a) $f(x, y)=\ln \left\{\left(16-x^{2}-y^{2}\right)\left(x^{2}+y^{2}-4\right)\right\}$

The function is defined and real for all points $(x, y)$ such that

$$
\left(16-x^{2}-y^{2}\right)\left(x^{2}+y^{2}-4\right)>0, \quad \text { i.e., } 4<x^{2}+y^{2}<16
$$

which is the required domain of definition. This point set consists of all points interior to the circle of radius 4 with center at the origin and exterior to the circle of radius 2 with center at the origin, as in Figure 6.5. The corresponding region, shown shaded in Figure 6.5, is an open region.

(b) $f(x, y)=\sqrt{6-(2 x+3 y)}$

The function is defined and real for all points $(x, y)$ such that $2 x+3 y \leq 6$, which is the required domain of definition.

The corresponding (unbounded) region of the $x y$ plane is shown shaded in Figure 6.6.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-146(1)}
\end{center}

Figure 6.5

6.3. Sketch and name the surface in three-dimensional space represented by each of the following. What are the traces on the coordinate planes?

(a) $2 x+4 y+3 z=12$.

Trace on $x y$ plane $(z=0)$ is the straight line $x+2 y=6, z=0$.

Trace on $y z$ plane $(x=0)$ is the straight line $4 y+3 z=12, x=0$.

Trace on $x z$ plane $(y=0)$ is the straight line $2 x+3 z=12, y=0$.

These are represented by $A B, B C$, and $A C$ in Figure 6.7.

The surface is a plane intersecting the $x, y$, and $z$ axes in the points $A(6,0,0), B(0,3,0)$, and $C(0,0,4)$. The lengths $\overline{O A}=6, \overline{O B}=3$, and $\overline{O C}=4$ are called the $x, y$, and $z$ intercepts, respectively.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-146}
\end{center}

Figure 6.7

(b) $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1$

Trace on $x y$ plane $(z=0)$ is the ellipse $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1, z=0$.

Trace on $y z$ plane $(x=0)$ is the hyperbola $\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1, x=0$.

Trace on $x z$ plane $(y=0)$ is the hyperbola $\frac{x^{2}}{a^{2}}-\frac{z^{2}}{c^{2}}=1, y=0$.

Trace on any plane $z=p$ parallel to the $x y$ plane is the ellipse $\frac{x^{2}}{a^{2}\left(1+p^{2} / c^{2}\right)}+\frac{y^{2}}{b^{2}\left(1+p^{2} / c^{2}\right)}=1$.

As $|p|$ increases from zero, the elliptic cross section increases in size.

The surface is a hyperboloid of one sheet (see Figure 6.8).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-146(2)}
\end{center}

Figure 6.8

\section*{Limits and continuity}
6.4. Prove that $\lim _{\substack{x \rightarrow 1 \\ y \rightarrow 2}}\left(x^{2}+2 y\right)=5$.

Method 1, using definition of limit.

We must show that, given any $\delta>0$, we can find $\delta>0$ such that $\left|x^{2}+2 y-5\right|<\delta$ when $0<|x-1|<\delta$, $0<|y-2|<\delta$.

If $0<|x-1|<\delta$ and $0<|y-2|<\delta$, then $1-\delta x<1+<\delta$ and $2-\delta<y<2+\delta$, excluding $x=1$, $y=2$.

Thus, $1-2 \delta+\delta^{2}<x^{2}<1+2 \delta+\delta^{2}$ and $4-2 \delta<2 y<4+2 \delta$. Adding

$$
5-4 \delta+\delta^{2}<x^{2}+2 y<5+4 \delta+\delta^{2} \text { or }-48+\delta^{2}<x^{2}+2 y-5<4 \delta+\delta^{2}
$$

Now, if $\delta \leq 1$, it certainly follows that $-5 \delta<x^{2}+2 y-5<5 \delta$; i.e., $\left|x^{2}+2 y-5\right|<5 \delta$ whenever $0<|x-1|$ $<\delta, 0<|y-2|<\delta$. Then, choosing $5 \delta=\epsilon$, i.e., $\delta=\epsilon / 5$ (or $\delta=1$, whichever is smaller), it follows that $\left|x^{2}+2 y-5\right|$ $<\epsilon$ when $0<|x-1|<\delta, 0<|y-2|<\delta$; i.e., $\lim _{x \rightarrow 1}\left(x^{2}+2 y\right)=5$.

Method 2, using theorems on limits.

$$
y \rightarrow 2
$$

$$
\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}}\left(x^{2}+2 y\right)=\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} x^{2}+\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} 2 y=1+4=5
$$

6.5. Prove that $f(x, y)=x^{2}+2 y$ is continuous at $(1,2)$.

By Problem 6.4, $\lim _{\substack{x \rightarrow 1 \\ y \rightarrow 2}} f(x, y)=5$. Also, $f(1,2)=1^{2}+2(2)=5$.

Then $\lim _{x \rightarrow 1} f(x, y)=f(1,2)$ and the function is continuous at $(1,2)$.

Alternatively, we can show, in much the same manner as in the first method of Problem 6.4, that given any $\delta>0$ we can find $\delta>0$ such that $|f(x, y)-f(1,2)|<\delta$ when $|x-1|<\delta,|y-2|<\delta$.

6.6. Determine whether $f(x, y)=\left\lvert\, \begin{array}{ll}x^{2}+2 y, & (x, y) \neq(1,2) \\ 0, & (x, y)=(1,2)\end{array}\right.$ (a) has a limit as $x \rightarrow 1$ and $y \rightarrow 2$, and (b) is continuous at $(1,2)$.

(a) By Problem 6.4, it follows that $\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} f(x, y)=5$, since the limit has nothing to do with the value at $(1,2)$.

(b) Since $\lim _{\substack{x \rightarrow 1 \\ x \rightarrow 2}} f(x, y)=5$ and $f(1,2)=0$, it follows that $\lim _{\substack{x \rightarrow 1 \\ y \rightarrow 2}} f(x, y) \neq f(1,2)$. Hence, the function is discontinuous at $(1,2)$.

6.7. Investigate the continuity of $f(x, y)=\left\{\begin{array}{ll}\frac{x^{2}-y^{2}}{x^{2}+y^{2}} & (x, y) \neq(0,0) \\ 0 & (x, y)=(0,0)\end{array}\right.$. at $(0,0)$.

Let $x \rightarrow 0$ and $y \rightarrow 0$ in such a way that $y=m x$ (a line in the $x y$ plane). Then, along this line,

$$
\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} \frac{x^{2}-y^{2}}{x^{2}+y^{2}}=\lim _{x \rightarrow 0} \frac{x^{2}-m^{2} x^{2}}{x^{2}+m^{2} x^{2}}=\lim _{x \rightarrow 0} \frac{x^{2}\left(1-m^{2}\right)}{x^{2}\left(1+m^{2}\right)}=\frac{1-m^{2}}{1+m^{2}}
$$

Since the limit of the function depends on the manner of approach to $(0,0)$ (i.e., the slope $m$ of the line), the function cannot be continuous at $(0,0)$.

\section*{Another method:}
Since $\lim _{x \rightarrow 0}\left\{\lim _{x \rightarrow 0} \frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right\}=\lim _{x \rightarrow 0} \frac{y^{2}}{x^{2}}=1$ and $\lim _{x \rightarrow 0}\left\{\lim _{x \rightarrow 0} \frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right\}=-1$ are not equal, $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} f(x, y)$ cannot exist. Hence, $f(x, y)$ cannot be continuous at $(0,0)$.

\section*{Partial derivatives}
6.8. If $f(x, y)=2 x^{2}-x y+y^{2}$, find (a) $\partial f / \partial x$ and (b) $\partial f / \partial y$ at $\left(x_{0}, y_{0}\right)$ directly from the definition.

(a) $\left.\frac{\partial f}{\partial x}\right|_{\left(x_{0}, y_{0}\right)}=f_{x}\left(x_{0}, y_{0}\right)=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h, y_{0}\right)-f\left(x_{0}, y_{0}\right)}{h}$

$$
\begin{aligned}
& =\lim _{h \rightarrow 0} \frac{\left[2\left(x_{0}+h\right)^{2}-\left(x_{0}+h\right) y_{0}+y_{0}^{2}\right]=\left[2 x_{0}^{2}-x_{0} y_{0}+y_{0}^{2}\right]}{h} \\
& =\lim _{h \rightarrow 0} \frac{4 h x_{0}+2 h^{2}-h y_{0}}{h}=\lim _{h \rightarrow 0}\left(4 x_{0}+2 h-y_{0}\right)=4 x_{0}-y_{0}
\end{aligned}
$$

(b) $\left.\frac{\partial f}{\partial y}\right|_{\left(x_{0}, y_{0}\right)}=f_{y}\left(x_{0}, y_{0}\right)=\lim _{k \rightarrow 0} \frac{f\left(x_{0}, y_{0}+k\right)-f\left(x_{0}, y_{0}\right)}{k}$

$$
\begin{aligned}
& =\lim _{k \rightarrow 0} \frac{\left[2 x_{0}^{2}-x_{0}\left(y_{0}+k\right)+\left(y_{0}+k\right)^{2}\right]-\left[2 x_{0}^{2}-x_{0} y_{0}+y_{0}^{2}\right]}{k} \\
& =\lim _{k \rightarrow 0} \frac{-k x_{0}+2 k y_{0}+k^{2}}{k}=\lim _{k \rightarrow 0}\left(-x_{0}+2 y_{0}+k\right)=-x_{0}+2 y_{0}
\end{aligned}
$$

Since the limits exist for all points $\left(x_{0}, y_{0}\right)$, we can write $f_{x}(x, y)=f_{x}=4 x-y, f_{y}(x, y)=f_{y}=-x+2 y$, which are themselves functions of $x$ and $y$.

Note that formally $f_{x}\left(x_{0}, y_{0}\right)$ is obtained from $f(x, y)$ by differentiating with respect to $x$, keeping $y$ constant and then putting $x=x_{0}, y=y_{0}$. Similarly, $f_{y}\left(x_{0}, y_{0}\right)$ is obtained by differentiating $f$ with respect to $y$, keeping $x$ constant. This procedure, while often lucrative in practice, need not always yield correct results (see Problem 6.9). It will work if the partial derivatives are continuous.

6.9. Let $f(x, y)=\left\{\begin{array}{ll}x y /\left(x^{2}+y^{2}\right) & (x, y) \neq(0,0) \\ 0 & \text { otherwise }\end{array}\right.$. Prove that (a) both $f_{x}(0,0)$ and $f_{y}(0,0)$ exist but that (b) $f(x, y)$ is discontinuous at $(0,0)$.

(a)

$$
\begin{aligned}
& f_{x}(0,0)=\lim _{h \rightarrow 0} \frac{f(h, 0)-f(0,0)}{h}=\lim _{h \rightarrow 0} \frac{0}{h}=0 \\
& f_{x}(0,0)=\lim _{k \rightarrow 0} \frac{f(0,0)-f(0,0)}{k}=\lim _{k \rightarrow 0} \frac{0}{k}=0
\end{aligned}
$$

(b) Let $(x, y) \rightarrow(0,0)$ along the line $y=m x$ in the $x y$ plane. Then $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} f(x, y)=\lim _{x \rightarrow 0} \frac{m x^{2}}{x^{2}+m^{2} x^{2}}=\frac{m}{1+m^{2}}$ so that the limit depends on $m$ and, hence, on the approach; therefore, it does not exist. Hence, $f(x, y)$ is not continuous at $(0,0)$.

Note that unlike the situation for functions of one variable, the existence of the first partial derivatives at a point does not imply continuity at the point.

Note also that if $(x, y) \neq(0,0), f_{x}=\frac{y^{2}-x^{2} y}{\left(x^{2}+y^{2}\right)^{2}}, f_{y}=\frac{x^{3}-x y^{2}}{\left(x^{2}+y^{2}\right)^{2}}$ and $f_{x}(0,0), f_{y}(0,0)$ cannot be computed from them by merely letting $x=0$ and $y=0$. See the remark at the end of Problem 4.5(b).

6.10. If $\phi(x, y)=x^{3} y+e^{x y^{2}}$, find (a) $\phi_{x}$, (b) $\phi_{y \phi x}$, (d) $\phi_{y y}$, (e) $\phi_{x y}$, and (f) $\phi_{y x}$.

(a) $\phi_{x}=\frac{\partial \phi}{\partial x}=\frac{\partial}{\partial x}\left(x^{3} y+e^{x y^{2}}\right)=3 x^{2} y+e^{x y^{2}} \cdot y^{2}=3 x^{2} y+y^{2} e^{x y^{2}}$

(b) $\phi_{y}=\frac{\partial \phi}{\partial y}=\frac{\partial}{\partial y}\left(x^{3} y+e^{x y^{2}}\right)=x^{3}+e^{x y^{2}} \cdot 2 x y=x^{3}+2 x y e^{x y^{2}}$

(c) $\phi_{x x}=\frac{\partial^{2} \phi}{\partial x^{2}}=\frac{\partial}{\partial x}\left(\frac{\partial \phi}{\partial x}\right)=\frac{\partial}{\partial x}\left(3 x^{2} y+y^{2} e^{x y^{2}}\right)=6 x y+y^{2}\left(e^{x y^{2}} \cdot y^{2}\right)=6 x y+y^{4} e^{x y^{2}}$\\
(d) $\phi_{y y}=\frac{\partial^{2} \phi}{\partial y^{2}}=\frac{\partial}{\partial y}\left(x^{3}+2 x y e^{x y^{2}}\right)=0+2 x y \cdot \frac{\partial}{\partial y}\left(e^{x y^{2}}\right)+e^{x y^{2}} \frac{\partial}{\partial y}(2 x y)$

$$
=2 x y \cdot e^{x y^{2}} \cdot 2 x y+e^{x y^{2}} \cdot 2 x=x^{2} y^{2} e^{x y^{2}}+2 x e^{x y^{2}}
$$

(e) $\phi_{x y}=\frac{\partial^{2} \phi}{\partial y \partial x}=\frac{\partial}{\partial y}\left(\frac{\partial \phi}{\partial x}\right)=\frac{\partial}{\partial y}\left(3 x^{2} y+y^{2} e^{x y^{2}}\right)=3 x^{2}+y^{2} \cdot e^{x y^{2}} \cdot 2 x y+e^{x y^{2}} \cdot 2 y$

$$
=3 x^{2}+2 x y^{3} e^{x y^{2}}+2 y e^{x y^{2}}
$$

(f) $\phi_{y x}=\frac{\partial^{2} \phi}{\partial x \partial y}=\frac{\partial}{\partial x}\left(\frac{\partial \phi}{\partial y}\right)=\frac{\partial}{\partial x}\left(x^{3}+2 x y e^{x y^{2}}\right)=3 x^{2}+y^{2} \cdot e^{x y^{2}} \cdot 2 x y+e^{x y^{2}} \cdot 2 y$

$$
=3 x^{2}+2 x y^{3} e^{x y^{2}}+2 y e^{x y^{2}}
$$

Note that $\phi_{x y}=\phi_{y x}$ in this case. This is because the second partial derivatives exist and are continuous for all $(x, y)$ in a region $\Re$. When this is not true, we may have $\phi_{x y} \neq \phi_{y x}$ (see Problem 6.41, for example).

6.11. Show that $U(x, y, z)=\left(x^{2}+y^{2}+z^{2}\right)^{-1 / 2}$ satisfies Laplace's partial differential equation $\frac{\partial^{2} U}{\partial x^{2}}+\frac{\partial^{2} U}{\partial y^{2}}+\frac{\partial^{2} U}{\partial z^{2}}=0$.

We assume here that $(x, y, z) \neq(0,0,0)$. Then

$$
\begin{aligned}
\frac{\partial U}{\partial x} & =-\frac{1}{2}\left(x^{2}+y^{2}+z^{2}\right)^{-3 / 2} \cdot 2 x=-x\left(x^{2}+y^{2}+z^{2}\right)^{-3 / 2} \\
\frac{\partial^{2} U}{\partial x^{2}} & =\frac{\partial}{\partial x}\left[-x\left(x^{2}+y^{2}+z^{2}\right)^{-3 / 2}\right]=(-x)\left[-\frac{3}{2}\left(x^{2}+y^{2}+z^{2}\right)^{-5 / 2} \cdot 2 x\right]+\left(x^{2}+y^{2}+z^{2}\right)^{-3 / 2} \cdot(-1) \\
& =\frac{3 x^{2}}{\left(x^{2}+y^{2}+z^{2}\right)^{5 / 2}}-\frac{\left(x^{2}+y^{2}+z^{2}\right)}{\left(x^{2}+y^{2}+z^{2}\right)^{5 / 2}}=\frac{2 x^{2}-y^{2}-z^{2}}{\left(x^{2}+y^{2}+z^{2}\right)^{5 / 2}}
\end{aligned}
$$

Similarly,

Adding,

$$
\frac{\partial^{2} U}{\partial y^{2}}=\frac{2 y^{2}-x^{2}-z^{2}}{\left(x^{2}+y^{2}+z^{2}\right)^{5 / 2}}, \quad \frac{\partial^{2} U}{\partial x^{2}}=\frac{2 z^{2}-x^{2}-y^{2}}{\left(x^{2}+y^{2}+z^{2}\right)^{5 / 2}}
$$

$$
\frac{\partial^{2} U}{\partial x^{2}}+\frac{\partial^{2} U}{\partial y^{2}}+\frac{\partial^{2} U}{\partial z^{2}}=0
$$

6.12. If $z=x^{2} \tan ^{-1} \frac{y}{x}$, find $\frac{\partial^{2} z}{\partial x \partial y}$ at $(1,1)$.

$$
\begin{gathered}
\frac{\partial z}{\partial y}=x^{2} \cdot \frac{1}{1+(y / x)^{2}} \frac{\partial}{\partial y}\left(\frac{y}{x}\right)=x^{2} \cdot \frac{x^{2}}{x^{2}+y^{2}} \cdot \frac{1}{x}=\frac{x^{3}}{x^{2}+y^{2}} \\
\frac{\partial^{2} z}{\partial x \partial y}=\frac{\partial}{\partial x}\left(\frac{\partial z}{\partial y}\right)=\frac{\partial}{\partial x}\left(\frac{x^{3}}{x^{2}+y^{2}}\right)=\frac{\left(x^{2}+y^{2}\right)\left(3 x^{2}\right)-\left(x^{3}\right)(2 x)}{\left(x^{2}+y^{2}\right)^{2}}=\frac{2 \cdot 3-1 \cdot 2}{2^{2}}=1 \text { at }(1,1)
\end{gathered}
$$

The result can be written $z_{x y}(1,1)=1$.

Note: In this calculation we are using the fact that $z_{x y}$ is continuous at $(1,1)$ (see the remark at the end of Problem 6.9).

6.13. If $f(x, y)$ is defined in a region $\Re$ and if $f_{x y}$ and $f_{y x}$ exist and are continuous at a point of $\Re$, prove that $f_{x y}=f_{y x}$ at this point.

Let $\left(x_{0}, y_{0}\right)$ be the point of $\Re$. Consider

$$
G=f\left(x_{0}+h, y_{0}+k\right)-f\left(x_{0}, y_{0}+k\right)-f\left(x_{0}-h, y_{0}\right)+f\left(x_{0}, y_{0}\right)
$$

Define


\begin{align*}
& \phi(x, y)=f(x+h, y)-f(x, y)  \tag{1}\\
& \psi(x, y)=f(x, y+k)-f(x, y) \tag{2}
\end{align*}


Then


\begin{align*}
& G=\phi\left(x_{0}, y_{0}+k\right)-\phi\left(x_{0}, y_{0}\right)  \tag{3}\\
& G=\psi\left(x_{0}+h, y_{0}\right)-\psi\left(x_{0}, y_{0}\right) \tag{4}
\end{align*}


Applying the mean value theorem for functions of one variable (see Page 78) to Equations (3) and (4), we have


\begin{gather*}
G=k \phi_{y}\left(x_{0}, y_{0}+\theta_{1} k\right)=k\left\{f_{y}\left(x_{0}+h, y_{0}+\theta_{1}-k\right)-f_{y}\left(x_{0}, y_{0}+\theta_{1} k\right)\right\} 0<\theta_{1}<1  \tag{5}\\
G=h \psi_{x}\left(x_{0},+\theta_{2} h, y_{0}\right)=h\left\{f_{x}\left(x_{0}+\theta_{2} h, y_{0}+k\right)-f_{x}\left(x_{0}+\theta_{2} h, y_{0}\right)\right\} 0<\theta_{2}<1 \tag{6}
\end{gather*}


Applying the mean value theorem again to Equations (5) and (6), we have

\[
\begin{array}{ll}
G=h k f_{y x}\left(x_{0}+\theta_{3} h, y_{0}+\theta_{1} k\right) & 0<\theta_{1}<1,0<\theta_{3}<1 \\
G=h k f_{x y}\left(x_{0}+\theta_{2} h, y_{0}+\theta_{4} k\right) & 0<\theta_{2}<1,0<\theta_{4}<1 \tag{8}
\end{array}
\]

From Equations (7) and (8) we have


\begin{equation*}
f_{y x}\left(x_{0}+\theta_{3} h, y_{0}+\theta_{1} k\right)=f_{x y}\left(x_{0}+\theta_{2} h, y_{0}+\theta_{4} k\right) \tag{9}
\end{equation*}


Letting $h \rightarrow 0$ and $k \rightarrow 0$ in (9) we have, since $f_{x y}$ and $f_{y x}$ are assumed continuous at $\left(x_{0}, y_{0}\right)$,

$$
f_{y x}\left(x_{0}, y_{0}\right)=f_{x y}\left(d_{0}, y_{0}\right)
$$

as required. For an example where this fails to hold, see Problem 6.41.

\section*{Differentials}
6.14. Let $f(x, y)$ have continuous first partial derivatives in a region $\Re$ of the $x y$ plane. Prove that

$$
\Delta f=f(x+\Delta x, y+\Delta y)-f(x, y)=f_{x} \Delta x+f_{y} \Delta y+\delta_{1} \Delta x+\delta_{2} \Delta y
$$

where $\epsilon_{1}$ and $\epsilon_{2}$ approach zero as $\Delta x$ and $\Delta y$ approach zero.

Applying the mean value theorem for functions of one variable (see Page 78), we have


\begin{gather*}
\Delta f=\{f(x+\Delta x, y+\Delta y)-f(x, y+\Delta y)\}+\{f(x, y+\Delta y)-f(x, y)\}  \tag{1}\\
=\Delta x f_{x}\left(x+\theta_{1} \Delta x, y+\Delta y\right)+\Delta f_{y}\left(x, y+\theta_{2} \Delta y\right) \quad 0<\theta_{1}<1,0<\theta_{2}<1
\end{gather*}


Since, by hypothesis, $f_{x}$ and $f_{y}$ are continuous, it follows that

$$
\begin{gathered}
f_{x}\left(x+\theta_{1} \Delta x, y+\Delta y\right)=f_{x}(x, y)+\delta_{1}, \quad f_{y}\left(x, y+\theta_{2} \Delta y\right)=f_{y}(x, y)+\delta_{2} \\
\text { where } \delta_{1} \rightarrow 0, \delta_{2} \rightarrow 0 \text { as } \Delta x \rightarrow 0 \text { and } \Delta y \rightarrow 0 .
\end{gathered}
$$

Thus, $\Delta f=f_{x} \Delta x+f_{y} \Delta y+\delta_{1} \Delta x+\delta_{2} \Delta y$ as required.

Defining $\Delta x=d x, \Delta y=d y$, we have $\Delta f=f_{x} d x+f_{y} d y+\delta_{1} d x+\delta_{2} d y$.

We call $d f=f_{x} d x+f_{y} d y$ the differential of $f$ (or $z$ ) or the principal part of $\Delta f$ (or $\Delta z$ ).

6.15. If $z=f(x, y),=x^{2} y-3 y$, find (a) $\Delta z$ and (b) $d z$. (c) Determine $\Delta z$ and $d z$ if $x=4, y=3, \Delta x=-0.01$, and $\Delta y=$ 0.02 . (d) How might you determine $f(5.12,6.85)$ without direct computation?

\section*{Solution:}
(a) $\Delta z=f(x+\Delta x, y)-f(x, y)$

$$
\begin{aligned}
& =\left[(x+\Delta x)^{2}(y+\Delta y)-3(y+\Delta y)\right]-\left\{x^{2} y-3 y\right\} \\
& =\underbrace{2 x y \Delta x+\left(x^{2}-3\right) \Delta y}_{(A)}+\underbrace{(\Delta x)^{2} y+2 x \Delta x \Delta y+(\Delta x)^{2} \Delta y}_{(B)}
\end{aligned}
$$

The sum (A) is the principal part of $\Delta z$ and is the differential of $z$, i.e., $d z$. Thus,

(b) $d z=2 x y \Delta x+\left(x^{2}-3\right) \Delta y=2 x y d x+\left(x^{2}-3\right) d y$

Another method: $d z=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y=2 x y d x+\left(x^{2}-3\right) d y$

(c) $\Delta z=f(x+\Delta x, y+\Delta y)-f(x, y)=f(4-0.01,3+0.02)-f(4,3)$

$$
=\left\{(3.99)^{2}(3.02)-3(3.02)\right\}-\left\{(4)^{2}(3)-3(3)\right\}=0.018702
$$

$d z=2 x y d x+\left(x^{2}-3\right) d y=2(4)(3)(-0.01)+\left(4^{3}-3\right)(0.02)=0.02$

Note that in this case $\Delta z$ and $d z$ are approximately equal: because $\Delta x=d x$ and $\Delta y=d y$ are sufficiently small.

(d) We must find $f(x+\Delta x, y+\Delta y)$ when $x+\Delta x=5.12$ and $y=\Delta y=6.85$. We can accomplish this by choosing $x=5, \Delta x=0.12, y=7, \delta y=-0.15$. Since $\Delta x$ and $\Delta y$ are small, we use the fact that $f(x+\Delta x, y+\Delta y)$ $=f(x, y)+\Delta z$ is approximately equal to $f(x, y)+d z$, i.e., $z+d z$.

Now

$$
\begin{aligned}
z & =f(x, y)=f(5,7)=(5)^{2}(7)-3(7)=154 \\
d z & =2 x y d x+\left(x^{2}-3\right) d y=2(5)(7)(0.12)+\left(5^{2}-3\right)(-0.15)=5.1
\end{aligned}
$$

Then the required value is $154+5.1=159.1$ approximately. The value obtained by direct computation is 159.01864 .

6.16. (a) Let $U=x^{2} e^{y / x}$. Find $d U$. (b) Show that $\left(3 x^{2} y-2 y^{2}\right) d x+\left(x^{3}-4 x y+6 y^{2}\right) d y$ can be written as an exact differential of a function $\phi(x, y)$ and find this function.

(a) Method 1:

Then

$$
\frac{\partial U}{\partial x}=x^{2} e^{y / x}\left(-\frac{y}{x^{2}}\right)+2 x e^{y / x}, \quad \frac{\partial U}{\partial y}=x^{2} e^{y / x}\left(\frac{1}{x}\right)
$$

\section*{Method 2:}
$$
d U=\frac{\partial U}{\partial x} d x+\frac{\partial U}{\partial y} d y=\left(2 x e^{y / x}-y e^{y / x}\right) d x+x e^{y / x} d y
$$

$$
\begin{aligned}
d U & =x^{2} d\left(e^{y / x}\right)+e^{y / x} d\left(x^{2}\right)=x^{2} e^{y / x} d(y / x)+2 x e^{y / x} d x \\
& =x^{2} e^{y / x}\left(\frac{x d y-y d x}{x^{2}}\right)+2 x e^{y / x} d x=\left(2 x e^{y / x}-y e^{y / x}\right) d x+x e^{y / x} d y
\end{aligned}
$$

(b) Method 1:

Suppose that

$$
\left(3 x^{2} y-2 y^{2}\right) d x+\left(x^{3}-4 x y+6 y^{2}\right) d y=d \phi=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y
$$

Then


\begin{gather*}
\frac{\partial \phi}{\partial x}=3 x^{2} y-2 y^{2}  \tag{1}\\
\frac{\partial \phi}{\partial y}=x^{3}-4 x y+6 y^{2} \tag{2}
\end{gather*}


From Equation (1), integrating with respect to $x$ keeping $y$ constant, we have

$$
\phi=x^{3} y=2 x y^{2}+f(y)
$$

where $f(y)$ is the "constant" of integration. Substituting this into Equation (2) yields

$$
x^{3}-4 x y+F^{\prime}(y)=x^{3}-4 x y+6 y^{2}
$$

from which $F^{\prime}(y)=6 y^{2}$, i.e., $f(y)=2 y^{3}+c$.

Hence, the required function is $\phi=x^{3} y-2 x y^{2}+2 y^{3}+c$, where $c$ is an arbitrary constant.

Note that by Theorem 3, Page 130, the existence of such a function is guaranteed, since if $P=3 x^{2} y-2 y^{2}$ and $Q=x^{3}-4 x y+6 y^{2}$, then $\partial P / \partial y=3 x^{2}-4 y=\partial Q / \partial x$ identically. If $\partial P / \partial y \neq \partial Q / \partial x$, this function would not exist and the given expression would not be an exact differential.

\section*{Method 2:}
$$
\begin{aligned}
\left(3 x^{2} y-2 y^{2}\right) d x+\left(x^{3}-4 x y+6 y^{2}\right) d y & =\left(3 x^{2} y d x+x^{3} d y\right)-\left(2 y^{2} d x+4 x y d y\right)+6 y^{2} d y \\
& =d\left(x^{3} y\right)-d\left(2 x y^{2}\right)+d\left(2 y^{3}\right)=d\left(x^{3} y-2 x y^{2}+2 y^{3}\right) \\
& =d\left(x^{3} y-2 x y^{2}+2 y^{3}+c\right)
\end{aligned}
$$

Then the required function is $x^{3} y-2 x y^{2}+2 y^{3}+c$.

This method, called the grouping method, is based on our ability to recognize exact differential combinations and is less than Method 1. Naturally, before attempting to apply any method, we should determine whether the given expression is an exact differential by using Theorem 3, Page 130. See Theorem 4, Page 130.

\section*{Differentiation of composite functions}
6.17. Let $z=f(x, y)$ and $x=\phi(t), y=\psi(t)$ where $f, \phi, \psi$ are assumed differentiable. Prove

$$
\frac{d z}{d t}=\frac{\partial z}{\partial x} \frac{d x}{d t}+\frac{\partial z}{\partial y} \frac{\partial y}{d t}
$$

Using the results of Problem 6.14, we have

$$
\frac{d z}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\Delta z}{\Delta t}=\lim _{\Delta t \rightarrow 0}\left\{\frac{\partial z}{\partial x} \frac{\Delta x}{\Delta t}+\frac{\partial z}{\partial y} \frac{\Delta y}{\Delta t}+\varepsilon_{1} \frac{\Delta x}{\Delta t}+\varepsilon_{2} \frac{\Delta y}{\Delta t}\right\}=\frac{\partial z}{\partial x} \frac{d x}{d t}+\frac{\partial z}{\partial y} \frac{d y}{d t}
$$

since, as $\Delta t \rightarrow 0$, we have $\Delta x \rightarrow 0, \Delta y \rightarrow 0, \varepsilon_{1} \rightarrow 0, \frac{\Delta x}{\Delta t} \rightarrow \frac{d x}{d t}, \frac{\Delta y}{\Delta t} \rightarrow \frac{d y}{d t}$.

6.18. If $z=e^{x y^{2}}, x=t \cos t, y=t \sin t$, compute $d z / d t$ at $t=\pi / 2$.

$$
\frac{d z}{d t}=\frac{\partial z}{\partial x} \frac{d x}{d t}+\frac{\partial z}{\partial y} \frac{d y}{d t}=\left(y^{2} e^{x y^{2}}\right)(-t \sin t+\cos t)+\left(2 x y e^{x y^{2}}\right)(t \cos t+\sin t)
$$

$$
\text { At } t=\pi / 2, x=0, y=\pi / 2 \text {. Then }\left.\frac{d z}{d t}\right|_{t=\pi / 2}=\left(\pi^{2} / 4\right)(-\pi / 2)+(0)(1)=-\pi^{3} / 8 \text {. }
$$

Another method: Substitute $x$ and $y$ to obtain $z=e^{t 3} \sin ^{2} t \cos t$ and then differentiate.

6.19. If $z=f(x, y)$ where $x=\phi(u, v)$ and $y=\psi(u, v)$, prove the following:

$$
\begin{array}{ll}
\text { (a) } \frac{\partial z}{\partial u}=\frac{\partial z}{\partial x} \frac{\partial x}{\partial u}+\frac{\partial z}{\partial y} \frac{\partial y}{\partial u} & \text { (b) } \frac{\partial z}{\partial v}=\frac{\partial z}{\partial x} \frac{\partial x}{\partial v}+\frac{\partial z}{\partial y} \frac{\partial y}{\partial v}
\end{array}
$$

(a) From Problem 6.14, assuming the differentiability of $f, \phi, \psi$, we have

$$
\frac{\partial z}{\partial u}=\lim _{\Delta u \rightarrow 0} \frac{\Delta z}{\Delta u}=\lim _{\Delta u \rightarrow 0}\left\{\frac{\partial z}{\partial x} \frac{\Delta x}{\Delta u}+\frac{\partial z}{\partial y} \frac{\Delta y}{\Delta u}+\varepsilon_{1} \frac{\Delta x}{\Delta u}+\varepsilon_{2} \frac{\Delta y}{\Delta u}\right\}=\frac{\partial z}{\partial x} \frac{\partial x}{\partial u}+\frac{\partial z}{\partial y} \frac{\partial y}{\partial u}
$$

(b) The result is proved as in (a) by replacing $\Delta u$ by $\Delta v$ and letting $\Delta v \rightarrow 0$.

6.20. Prove that $d z=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y$ even if $x$ and $y$ are dependent variables.

Suppose $x$ and $y$ depend on three variables $u, v, w$, for example. Then


\begin{align*}
d x & =x_{u} d u+x_{v} d v+x_{w} d w  \tag{1}\\
d y & =y_{u} d u+y_{v} d v+y_{w} d w \tag{2}
\end{align*}


Thus,

$z_{x} d x+z_{y} d y=\left(z_{x} x_{u}+z_{y} y_{u}\right) d u+\left(z_{x} x_{v}+z_{y y v}\right) d v+\left(z_{x} x_{w}+z_{y} y_{w}\right) d w=z_{u} d u+z_{v} d v+z_{w}=d z$ using obvious generalizations from Problem 6.19.

6.21. If $T=x^{3}-x y+y^{3}, x=\rho \cos \phi$, and $y=\rho \sin \phi$, find (a) $\partial T / \partial \rho, \partial T / \partial \rho$ and (b) $\partial T / \partial \phi$.

$$
\begin{aligned}
& \frac{\partial T}{\partial \rho}=\frac{\partial T}{\partial x} \frac{\partial x}{\partial \rho}+\frac{\partial T}{\partial y} \frac{\partial y}{\partial \rho}=\left(3 x^{2}-y\right)(\cos \phi)+\left(3 y^{2}-x\right)(\sin \phi) \\
& \frac{\partial T}{\partial \phi}=\frac{\partial T}{\partial x} \frac{\partial x}{\partial \phi}+\frac{\partial T}{\partial y} \frac{\partial y}{\partial \phi}=\left(3 x^{2}-y\right)(-\rho \sin \phi)+\left(3 y^{2}-x\right)(\rho \cos \phi)
\end{aligned}
$$

This may also be worked by direct substitution of $x$ and $y$ in $T$.

6.22. If $U=z \sin y / x$ where $x=3 r^{2}+2 s, y=4 r-2 s^{3}$, and $z=2 r^{2}-3 s^{2}$, find (a) $\partial U / \partial r$ and (b) $\partial U / \partial s$.

(a) $\frac{\partial U}{\partial r}=\frac{\partial U}{\partial x} \frac{\partial x}{\partial r}+\frac{\partial U}{\partial y} \frac{\partial y}{\partial r}+\frac{\partial U}{\partial z} \frac{\partial z}{\partial r}$

$$
\begin{aligned}
& =\left\{\left(z \cos \frac{y}{x}\right)\left(-\frac{y}{x^{2}}\right)\right\}(6 r)+\left\{\left(z \cos \frac{y}{x}\right)\left(\frac{1}{x}\right)\right\}(4)+\left(\sin \frac{y}{x}\right)(4 r) \\
& =-\frac{6 r y z}{x^{2}} \cos \frac{y}{x}+\frac{4 z}{x} \cos \frac{y}{x}+4 r \sin \frac{y}{x}
\end{aligned}
$$

(b) $\frac{\partial U}{\partial s}=\frac{\partial U}{\partial x} \frac{\partial x}{\partial s}+\frac{\partial U}{\partial y} \frac{\partial y}{\partial s}+\frac{\partial U}{\partial z} \frac{\partial z}{\partial s}$

$$
\begin{aligned}
& =\left\{\left(z \cos \frac{y}{x}\right)\left(-\frac{y}{x^{2}}\right)\right\}(2)+\left\{\left(z \cos \frac{y}{x}\right)\left(\frac{1}{x}\right)\right\}\left(-6 s^{2}\right)+\left(\sin \frac{y}{x}\right)(-6 s) \\
& =-\frac{2 y z}{x^{2}} \cos \frac{y}{x}-\frac{6 s^{2} z}{x} \cos \frac{y}{x}-6 s \sin \frac{y}{x}
\end{aligned}
$$

6.23. If $x=\rho \cos \phi, y=\rho \sin \phi$, shown that $\left(\frac{\partial V}{\partial x}\right)^{2}+\left(\frac{\partial V}{\partial y}\right)^{2}=\left(\frac{\partial V}{\partial \rho}\right)^{2}+\frac{1}{\rho^{2}}\left(\frac{\partial V}{\partial \phi}\right)^{2}$.

Using the subscript notation for partial derivatives, we have


\begin{align*}
& V_{\rho}=V_{x} x_{\rho}+V_{y} y_{\rho}=V_{x} \cos \phi+V_{y} \sin \phi  \tag{1}\\
& V_{\phi}=V_{x} x_{\phi}+V_{y} y_{\phi}=V_{x}(-\rho \sin \phi)+V_{y}(\rho \cos \phi) \tag{2}
\end{align*}


Dividing both sides of Equation (2) by $\rho$, we have


\begin{equation*}
\frac{1}{\rho} V_{\phi}=-V_{x} \sin \phi+V_{y} \cos \phi \tag{3}
\end{equation*}


Then from Equations (1) and (3), we have

$$
V_{\rho}^{2}+\frac{1}{\rho^{2}} V_{\phi}^{2}=\left(V_{x} \cos \phi+V_{y} \sin \phi\right)^{2}+\left(-V_{x} \sin \phi+V_{y} \cos \phi\right)^{2}=V_{x}^{2}+V_{y}^{2}
$$

6.24. Show that $z=f\left(x^{2} y\right)$, where $f$ is differentiable, satisfies $x(\partial z / \partial x)=2 y(\partial z / \partial y)$.

Let $x^{2} y=u$. Then $z=f(u)$. Thus,

$$
\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial x}=f^{\prime}(u) \cdot 2 x y, \quad \frac{\partial z}{\partial y}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial y}=f^{\prime}(u) \cdot x^{2}
$$

Then

$$
x \frac{\partial z}{\partial x}=f^{\prime}(u) \cdot 2 x^{2} y, \quad 2 y \frac{\partial z}{\partial y}=f^{\prime}(u) \cdot 2 x^{2} y \text { and so } x \frac{\partial z}{\partial x}=2 y \frac{\partial z}{\partial y}
$$

Another method: We have $d z=f^{\prime}\left(x^{2} y\right) d\left(x^{2} y\right)=f^{\prime}\left(x^{2} y\right)\left(2 x y d x+x^{2} d y\right)$.

Also,

$$
d z=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y
$$

Then

$$
\frac{\partial z}{\partial x}=2 x y f^{\prime}\left(x^{2} y\right) . \quad \frac{\partial z}{\partial y}=x^{3} f^{\prime}\left(x^{2} y\right)
$$

Elimination of $f^{\prime}\left(x^{2} y\right)$ yields $x \frac{\partial z}{\partial x}=2 y \frac{\partial z}{\partial y}$.

6.25. If for all values of the parameter $\lambda$ and for some constant $p, F(\lambda x, \lambda y)=\lambda^{p} F(x, y)$ identically, where $F$ is assumed differentiable, prove that $x(\partial F / \partial x)+y(\partial F / \partial y)=p F$.

Let $\lambda x=u, \lambda y=v$. Then


\begin{equation*}
F(u, v)=\lambda^{p} F(x, y) \tag{1}
\end{equation*}


The derivative with respect to $\lambda$ of the left side of Equation (1) is

$$
\frac{\partial F}{\partial \lambda}=\frac{\partial F}{\partial u} \frac{\partial u}{\partial \lambda}+\frac{\partial F}{\partial v} \frac{d v}{\partial \lambda}=\frac{\partial F}{\partial u} x+\frac{\partial F}{\partial v} y
$$

The derivative with respect to $\lambda$ of the right side of Equation (1) is $p \lambda^{p-1} F$. Then


\begin{equation*}
x \frac{\partial F}{\partial u}+y \frac{\partial F}{\partial v}=p \lambda^{p-1} F \tag{2}
\end{equation*}


Letting $\lambda=1$ in Equation (2), so that $u=x, v=y$, we have $x(\partial F / \partial x)+y(\partial F / \partial y)=p F$.

6.26. If $F(x, y)=x^{4} y^{2} \sin ^{-1} y / x$, show that $x(\partial F / \partial x)+y(\partial F / \partial y)=6 F$.

Since $F(\lambda x, \lambda y)=(\lambda x)^{4}(\lambda y)^{2} \sin ^{-1} \lambda y / \lambda x=\lambda^{6} x^{4} y^{2} \sin ^{-1} y / x=\lambda^{6} F(x, y)$, the result follows from Problem 6.25 with $p=6$. It can, of course, also be shown by direct differentiation.

6.27. Prove that $Y=f(x+a t)+g(x-a t)$ satisfies $\partial^{2} Y / \partial t^{2}=a^{2}\left(\partial^{2} Y / \partial x^{2}\right)$, where $f$ and $g$ are assumed to be at least twice differentiable and $a$ is any constant.

Let $u=x+a t, v=x-a t$ so that $Y=f(u)+g(v) . \quad$ Then if $f^{\prime}(u) \equiv d f / d u, g^{\prime}(v) \equiv d g / d v$,

$\frac{\partial Y}{\partial t}=\frac{\partial Y}{\partial u} \frac{\partial u}{\partial t}+\frac{\partial Y}{\partial v} \frac{\partial v}{\partial t}=a f^{\prime}(u)-a g^{\prime}(v), \quad \frac{\partial Y}{\partial x}=\frac{\partial Y}{\partial x} \frac{\partial u}{\partial x}+\frac{\partial Y}{\partial v} \frac{\partial v}{\partial x}=f^{\prime}(u)+g^{\prime}(v)$

By further differentiation, using the notation $f^{\prime \prime}(u) \equiv d^{2} f / d u^{2}, g^{\prime \prime}(v) \equiv d^{2} g / d v^{2}$, we have


\begin{align*}
\frac{\partial^{2} Y}{\partial t^{2}} & =\frac{\partial Y_{t}}{\partial t}=\frac{\partial Y_{t}}{\partial u} \frac{\partial u}{\partial t}+\frac{\partial Y_{t}}{\partial v} \frac{\partial v}{\partial t}=\frac{\partial}{\partial u}\left\{a f^{\prime}(u)-a g^{\prime}(v)\right\}(a)+\frac{\partial}{\partial v}\left\{a f^{\prime}(u)-a g^{\prime}(v)\right\}(-a)  \tag{1}\\
& =a^{2} f^{\prime \prime}(u)+a^{2} g^{\prime \prime}(v) \\
\frac{\partial^{2} Y}{\partial x^{2}}= & \left.\frac{\partial Y_{x}}{\partial x}=\frac{\partial Y_{x}}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial Y_{x}}{\partial v} \frac{\partial v}{\partial x}=\frac{\partial}{\partial u}\left\{f^{\prime}(u)+g^{\prime}(v)\right\}+\frac{\partial}{\partial v}\left\{f^{\prime}(u)+g^{\prime}\right)(v)\right\}=f^{\prime \prime}(u)+g^{\prime \prime}(v) \tag{2}
\end{align*}


Then from Equations (1) and (2), $\partial^{2} Y / \partial t^{2}=a^{2}\left(\partial^{2} Y / \partial x^{2}\right)$.

6.28. If $x=2 r-s$ and $y=r+2 s$, find $\frac{\partial^{2} U}{\partial y \partial x}$ in terms of derivatives with respect to $r$ and $s$.

Solving $x=2 r-s, y=r+2 s$ for $r$ and $s: r=(2 x+y) / 5, s=(2 y-x) / 5$.

Then $\partial r / \partial x=2 / 5, \partial s / \partial x=-1 / 5, \partial r / \partial y=1 / 5, \partial s / \partial y=2 / 5$. Hence, we have

$$
\begin{aligned}
& \frac{\partial U}{\partial x}=\frac{\partial U}{\partial r} \frac{\partial r}{\partial x}+\frac{\partial U}{\partial s} \frac{\partial s}{\partial x}=\frac{2}{5} \frac{\partial U}{\partial r}-\frac{1}{5} \frac{\partial U}{\partial s} \\
& \frac{\partial^{2} U}{\partial y \partial x}=\frac{\partial}{\partial y}\left(\frac{\partial U}{\partial x}\right)=\frac{\partial}{\partial r}\left(\frac{2}{5} \frac{\partial U}{\partial r}-\frac{1}{5} \frac{\partial U}{\partial s}\right) \frac{\partial r}{\partial y}+\frac{\partial}{\partial s}\left(\frac{2}{5} \frac{\partial U}{\partial r}-\frac{1}{5} \frac{\partial U}{\partial s}\right) \frac{\partial s}{\partial y} \\
&=\left(\frac{2}{5} \frac{\partial^{2} U}{\partial r^{2}}-\frac{1}{5} \frac{\partial^{2} U}{\partial r \partial s}\right)\left(\frac{1}{5}\right)+\left(\frac{2}{5} \frac{\partial^{2} U}{\partial s}-\frac{1}{5} \frac{\partial^{2} U}{\partial s^{2}}\right)\left(\frac{2}{5}\right) \\
&=\frac{1}{25}\left(2 \frac{\partial^{2} U}{\partial r^{2}}+3 \frac{\partial^{2} U}{\partial r \partial s}-2 \frac{\partial^{2} U}{\partial s^{2}}\right)
\end{aligned}
$$

assuming $U$ has continuous second partial derivatives.

\section*{Implicit functions and jacobians}
6.29. If $U=x^{3} y$, find $d U / d t$ if


\begin{gather*}
x^{5}+y=t \text { and }  \tag{1}\\
x^{2}+y^{3}=t^{2} \tag{2}
\end{gather*}


Equations (1) and (2) define $x$ and $y$ as (implicit) functions of $t$. Then differentiating with respect to $t$, we have


\begin{gather*}
5 x^{4}(d x / d t)+d y / t=1  \tag{3}\\
2 x(d x / d t)+3 y^{2}(d y / d t)=2 t \tag{4}
\end{gather*}


Solving Equations (3) and (4) simultaneously for $d x / d t$ and $d y / d t$,

$$
\frac{d x}{d t}=\frac{\left|\begin{array}{cc}
1 & 1 \\
2 t & 3 y^{2}
\end{array}\right|}{\left|\begin{array}{cc}
5 x^{4} & 1 \\
2 x & 3 y^{2}
\end{array}\right|}=\frac{3 y^{2}-2 t}{15 x^{4} y^{2}-2 x}, \quad \frac{d y}{d t}=\frac{\left|\begin{array}{cc}
5 x^{4} & 1 \\
2 x & 2 t
\end{array}\right|}{\left|\begin{array}{cc}
5 x^{4} & 1 \\
2 x & 3 y^{2}
\end{array}\right|}=\frac{10 x^{4} t-2 x}{15 x^{4} y^{2}-2 x}
$$

Then $\frac{d U}{d t}=\frac{\partial U}{\partial x} \frac{d x}{d t}+\frac{\partial U}{\partial y} \frac{d y}{d t}=\left(3 x^{2} y\right)\left(\frac{3 y^{2}-2 t}{15 x^{4} y^{2}-2 x}\right)+\left(x^{3}\right)\left(\frac{10 x^{4} t-2 x}{15 x^{4} y^{2}-2 x}\right)$.

6.30. If $F(x, y, z)=0$ defines $z$ as an implicit function of $x$ and $y$ in a region $\Re$ of the $x y$ plane, prove that (a) $\partial z / \partial x$ $=-F_{x} / F_{z}$ and (b) $\partial z / \partial y=-F_{y} / F_{z}$, where $F_{z} \neq 0$.

Since $z$ is a function of $x$ and $y, d z=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} d y$.

Then $d F=\frac{\partial F}{\partial x} d x+\frac{\partial F}{\partial y} d y+\frac{\partial F}{\partial z} d z=\left(\frac{\partial F}{\partial x}+\frac{\partial F}{\partial z} \frac{\partial z}{\partial x}\right) d x+\left(\frac{\partial F}{\partial y}+\frac{\partial F}{\partial z} \frac{\partial z}{\partial y}\right) d y=0$.

Since $x$ and $y$ are independent, we have


\begin{align*}
& \frac{\partial F}{\partial x}+\frac{\partial F}{\partial z} \frac{\partial z}{\partial x}=0  \tag{1}\\
& \frac{\partial F}{\partial y}+\frac{\partial F}{\partial z} \frac{\partial z}{\partial y}=0 \tag{2}
\end{align*}


from which the required results are obtained. If desired, equations (1) and (2) can be written directly.

6.31. If $F(x, y, u, v)=0$ and $\mathrm{G}(x, y, u, v)=0$, find (a) $\partial u / \partial x$, (b) $\partial u / \partial y$, (c) $\partial v / \partial x$, and (d) $\partial v / \partial y$.

The two equations in general define the dependent variables $u$ and $v$ as (implicit) functions of the independent variables $x$ and $y$. Using the subscript notation, we have


\begin{align*}
& d F=F_{x} d x+F_{y} d y+F_{u} d u+F_{v} d v=0  \tag{1}\\
& d G=G_{x} d x+G_{y} d y+G_{u} d u+G_{v} d v=0 \tag{2}
\end{align*}


Also, since $u$ and $v$ are functions of $x$ and $y$,


\begin{align*}
& d u=u_{x} d x+u_{y} d y  \tag{3}\\
& d v=v_{x} d x+v_{y} d y \tag{4}
\end{align*}


Substituting Equations (3) and (4) in (1) and (2) yields


\begin{align*}
& d F=\left(F_{x}+F_{u} u_{x}+F_{v} v_{x}\right) d x+\left(F_{y}+F_{u} u_{y}+F_{v} v_{y}\right) d y=0  \tag{5}\\
& d G=\left(G_{x}+G_{u} u_{x}+G_{v} v_{x}\right) d x+\left(G_{y}+G_{u} u_{y}+G_{v} v_{y}\right) d y=0 \tag{6}
\end{align*}


Since $x$ and $y$ are independent, the coefficients of $d x$ and $d y$ in Equations (5) and (6) are zero. Hence, we obtain


\begin{align*}
& \left\{\begin{array}{l}
F_{u} u_{x}+F_{v} v_{x}=-F_{x} \\
G_{u} u_{x}+G_{v} v_{x}=-G_{x}
\end{array}\right.  \tag{7}\\
& \left\{\begin{array}{l}
F_{u} u_{y}+F_{v} v_{y}=-F_{y} \\
G_{u} u_{y}+G_{v} v_{y}=-G_{y}
\end{array}\right. \tag{8}
\end{align*}


Solving Equations (7) and (8) gives\\
(a) $u_{x} \frac{\partial u}{\partial x}=\frac{\left|\begin{array}{cc}-F_{x} & F_{v} \\ -G_{x} & G_{v}\end{array}\right|}{\left|\begin{array}{cc}F_{u} & F_{v} \\ G_{u} & G_{v}\end{array}\right|}=-\frac{\frac{\partial(F, G)}{\partial(x, v)}}{\frac{\partial(F, G)}{\partial(u, v)}}$\\
(b) $v_{x}=\frac{\partial v}{\partial x}=\frac{\left|\begin{array}{ll}F_{u} & -F_{x} \\ G_{u} & -G_{x}\end{array}\right|}{\left|\begin{array}{ll}F_{u} & F_{v} \\ G_{u} & G_{v}\end{array}\right|}=-\frac{\frac{\partial(F, G)}{\partial(u, x)}}{\frac{\partial(F, G)}{\partial(u, v)}}$\\
(c) $u_{y}=\frac{\partial u}{\partial y}=\frac{\left|\begin{array}{ll}-F_{y} & F_{v} \\ -G_{y} & G_{v}\end{array}\right|}{\left|\begin{array}{cc}F_{u} & F_{v} \\ G_{u} & G_{v}\end{array}\right|}=-\frac{\frac{\partial(F, G)}{\partial(y, v)}}{\frac{\partial(F, G)}{\partial(u, v)}}$\\
(d) $v_{y}=\frac{\partial v}{\partial y}=\frac{\left|\begin{array}{ll}F_{u} & -F_{y} \\ G_{u} & -G_{y}\end{array}\right|}{\left|\begin{array}{ll}F_{u} & F_{v} \\ G_{u} & G_{v}\end{array}\right|}=-\frac{\frac{\partial(F, G)}{\partial(u, y)}}{\frac{\partial(F, G)}{\partial(u, v)}}$

The functional determinant $\left|\begin{array}{ll}F_{u} & F_{v} \\ G_{u} & G_{v}\end{array}\right|$, denoted by $\frac{\partial(F, G)}{\partial(u, v)}$ or $J\left(\frac{F, G}{u, v}\right)$, is the Jacobian of $F$ and $G$ with respect to $u$ and $v$ and is supposed $\neq 0$.

Note that it is possible to devise mnemonic rules for writing at once the required partial derivatives in terms of Jacobians (see also Problem 6.33).

6.32. If $u^{2}-v=3 x+y$ and $u-2 v^{2}=x-2 y$, find (a) $\partial u / \partial x$, (b) $\partial v / \partial x$, (c) $\partial u / \partial y$, and (d) $\partial v / \partial y$.

Method 1: Differentiate the given equations with respect to $x$, considering $u$ and $v$ as functions of $x$ and $y$. Then


\begin{align*}
& 2 u \frac{\partial u}{\partial x}-\frac{\partial v}{\partial x}=3  \tag{1}\\
& \frac{\partial v}{\partial x}-4 v \frac{\partial v}{\partial x}=1 \tag{2}
\end{align*}


Solving, $\frac{\partial u}{\partial x}=\frac{1-12 v}{1-8 w}, \frac{\partial v}{\partial x}=\frac{2 u-3}{1-8 w}$.

Differentiating with respect to $y$, we have


\begin{gather*}
2 u \frac{\partial u}{\partial y}-\frac{\partial v}{\partial y}=1  \tag{3}\\
\frac{\partial u}{\partial y}-4 v \frac{\partial v}{\partial y}=-2 \tag{4}
\end{gather*}


Solving, $\frac{\partial u}{\partial y}=\frac{-2-4 v}{1-8 w}, \frac{\partial v}{\partial y}=\frac{-4 u-1}{1-8 w}$.

We have, of course, assumed that $1-8 u v \neq 0$.

Method 2: The given equations are $F=u^{2}-v-3 x-y=0, G=u-2 v^{2}-x+2 y=0$. Then, by Problem 6.31,

$$
\frac{\partial u}{\partial x}=-\frac{\frac{\partial(F, G)}{\partial(x, v)}}{\frac{\partial(F, G)}{\partial(u, v)}}=-\left|\begin{array}{ll}
F_{x} & F_{v} \\
G_{x} & G_{v} \\
F_{u} & F_{v} \\
G_{u} & G_{v}
\end{array}\right|=-\left|\begin{array}{cc}
-3 & -1 \\
-1 & -4 v \\
2 u & -1 \\
1 & -4 v
\end{array}\right|=\frac{1-12 v}{1-8 w v}
$$

provided $1-8 u v \neq 0$. Similarly, the other partial derivatives are obtained.

6.33. If $F(u, v, w, x, y)=0, G(u, v, w, x, y)=0$, and $H(u, v, w, x, y)=0$, find (a) $\left.\frac{\partial v}{\partial y}\right|_{x}$, (b) $\left.\frac{\partial x}{\partial v}\right|_{w}$, (c) $\left.\frac{\partial w}{\partial u}\right|_{y}$.

From three equations in five variables, we can (theoretically at least) determine three variables in terms of the remaining two. Thus, three variables are dependent and two are independent. If we were asked to determine $\partial v / \partial y$, we would know that $v$ is a dependent variable and $y$ is an independent variable, but would not know the remaining independent variable. However, the particular notation $\left.\frac{\partial v}{\partial y}\right|_{x}$ serves to indicate that we are to obtain $\partial v / \partial y$, keeping $x$ constant; i.e., $x$ is the other independent variable.

(a) Differentiating the given equations with respect to $y$, keeping $x$ constant, gives


\begin{gather*}
F_{u} u_{y}+F_{v} v_{y}+F_{w} w_{y}+F_{y}=0  \tag{1}\\
G_{u} u_{y}+G_{v} v_{y}+G_{w} w_{y}+G_{y}=G_{y}=0  \tag{2}\\
H_{u} u_{y}+H_{v} v_{y}+H_{w} w_{y}+H_{y}=0 \tag{3}
\end{gather*}


Solving simultaneously for $v_{y}$, we have

$$
v_{y}=\left.\frac{\partial v}{\partial y}\right|_{x}=-\frac{\left|\begin{array}{lll}
F_{u} & F_{y} & F_{w} \\
G_{u} & G_{y} & G_{w} \\
H_{u} & H_{y} & H_{w}
\end{array}\right|}{\left|\begin{array}{lll}
F_{u} & F_{v} & F_{w} \\
G_{u} & G_{v} & G_{w} \\
H_{u} & H_{v} & H_{w}
\end{array}\right|}=-\frac{\frac{\partial(F, G, H)}{\partial(u, y, w)}}{\frac{\partial(F, G, H)}{\partial(u, v, w)}}
$$

Equations (1), (2), and (3) can also be obtained by using differentials as in Problem 6.31.

The Jacobian method is very suggestive for writing results immediately, as seen in this problem and Problem 6.31. Thus, observe that in calculating $\left.\frac{\partial v}{\partial y}\right|_{x}$ the result is the negative of the quotient of two Jacobians, the numerator containing the independent variable $y$ and the denominator containing the dependent variable $v$ in the same relative positions. Using this scheme, we have

$$
\text { (b) }\left.\frac{\partial x}{\partial v}\right|_{w}=-\frac{\frac{\partial(F, G, H)}{\partial(v, y, u)}}{\frac{\partial(F, G, H)}{\partial(x, y, u)}} \quad \text { (c) }\left.\frac{\partial w}{\partial u}\right|_{y}=-\frac{\frac{\partial(F, G, H)}{\partial(u, x, v)}}{\frac{\partial(F, G, H)}{\partial(w, x, v)}}
$$

6.34. If $z^{3}-x z-y=0$, prove that $\frac{\partial^{2} z}{\partial x \partial y}=-\frac{3 z^{2}+x}{\left(3 z^{2}-x\right)^{3}}$

Differentiating with respect to $x$, keeping $y$ constant, and remembering that $z$ is the dependent variable depending on the independent variables $x$ and $y$, we find

$$
3 z^{2} \frac{\partial z}{\partial x}-x \frac{\partial z}{\partial x}-z=0
$$

and


\begin{equation*}
\frac{\partial z}{\partial x}=\frac{z}{3 z^{2}-x} \tag{1}
\end{equation*}


Differentiating with respect to $y$, keeping $x$ constant, we find

$$
3 z^{2} \frac{\partial z}{\partial y}-x \frac{\partial z}{\partial y}-1=0
$$

and


\begin{equation*}
\frac{\partial z}{\partial x}=\frac{z}{3 z^{2}-x} \tag{2}
\end{equation*}


Differentiating Equation (2) with respect to $x$ and using Equation (1), we have

$$
\frac{\partial^{2}}{\partial x \partial y}=\frac{-1}{\left(3 z^{2}-x\right)^{2}}\left(6 z \frac{\partial z}{\partial x}-1\right)=\frac{1-6 z\left[z /\left(3 z^{2}-x\right)\right]}{\left(3 z^{2}-x\right)^{2}}=-\frac{3 z^{2}-x}{\left(3 z^{2}-x\right)^{3}}
$$

The result can also be obtained by differentiating Equation (1) with respect to y and using Equation (2).

6.35. Let $u=f(x, y)$ and $v=g(x, y)$, where $f$ and $g$ are continuously differentiable in some region $\Re$. Prove that a necessary and sufficient condition that there exists a functional relation between $u$ and $v$ of the form $\phi(u, v)$ $=0$ is the vanishing of the Jacobian; i.e., $\frac{\partial(u, v)}{\partial(x, y)}=0$ identically.\\
Necessity. We have to prove that if the functional relation $\phi(u, v)=0$ exists, then the Jacobian $\frac{\partial(u, v)}{\partial(x, y)}=0$\\
identically. To do this, we note that

$$
\begin{aligned}
d \phi & =\phi_{u} d u+\phi_{v} d v=\phi_{u}\left(u_{x} d x+u_{y} d y\right)+\phi_{v}\left(v_{x} d x+v_{y} d y\right) \\
& =\left(\phi_{u} u_{x}+\phi_{v} v_{x}\right) d x+\left(\phi_{u} u_{y}+\phi_{v} v_{y}\right) d y=0
\end{aligned}
$$

Then


\begin{align*}
& \phi_{u} u_{x}+\phi_{v} v_{x}=0  \tag{1}\\
& \phi_{u} u_{y}+\phi_{v} v_{y}=0 \tag{2}
\end{align*}


Now $\phi_{u}$ and $\phi_{v}$ cannot be identically zero, since if they were, there would be no functional relation, contrary to hypothesis. Hence, it follows from Equations (1) and (2) that $\left|\begin{array}{ll}u_{x} & v_{x} \\ u_{y} & v_{y}\end{array}\right|=\frac{\partial(u, v)}{\partial(x, y)}=0$ identically. Sufficiency. We have to prove that if the Jacobian $\frac{\partial(u, v)}{\partial(x, y)}=0$ identically, then there exists a functional\\
relation between $u$ and $v$; i.e., $\phi(u, v)=0$.

Let us first suppose that both $u_{x}=0$ and $u_{y}=0$. In this case the Jacobian is identically zero and $u$ is a constant $c_{1}$, so that the trival functional relation $u=c_{1}$ is obtained.

Let us now assume that we do not have both $u_{x}=0$ and $u_{y}=0$; for definiteness, assume $u_{x} \neq 0$. We may then, according to Theorem 1, Page 133, solve for $x$ in the equation $u=f(x, y)$ to obtain $x=F(u, y)$, from which it follows that


\begin{gather*}
u=f\{F(u, y), y\}  \tag{1}\\
v=g\{F(u, y), y\} \tag{2}
\end{gather*}


From these we have, respectively,


\begin{gather*}
d u=u_{x} d x+u_{y} d y=u_{x}\left(F_{u} d u+F_{y} d y\right)+u_{y} d y=u_{x} F_{u} d u+\left(u_{x} F_{y}+u_{y}\right) d y  \tag{3}\\
d v=v_{x} d x+v_{y} d y=v_{x}\left(F_{u} d u+F_{y} d y\right)+v_{y} d y=v_{x} F_{u} d u+\left(v_{x} F_{y}+v_{y}\right) d y \tag{4}
\end{gather*}


From Equation (3), $u_{x} F_{u}=1$ and $u_{x} F_{y}+u_{y}=0$ or (5) $F_{y}=-u_{y} / u_{x}$. Using this, Equation (4) becomes


\begin{equation*}
d v=v_{x} F_{u} d u+\left\{v_{x}\left(-u_{y} / u_{x}\right)+v_{y}\right\} d y=v_{x} F_{u} d u+\left(\frac{u_{x} v_{y}-u_{y} v_{x}}{u_{x}}\right) d y \tag{6}
\end{equation*}


But by hypothesis $\frac{\partial(u, v)}{\partial(x, y)}=\left|\begin{array}{ll}u_{x} & u_{y} \\ v_{x} & v_{y}\end{array}\right|=u_{x} v_{y}-u_{y} v_{x}=0$ identically, so that Equation (6) becomes $d \phi$ $=v_{x} F_{u} d u$. This means essentially that, referring to Equation (2), $\partial v / \partial y=0$, which means that $v$ is not dependent on $y$ but depends only on $u$; i.e., $v$ is a function of $u$, which is the same as saying that the functional relation $\phi(u, v)=0$ exists.

6.36 .

(a) If $u=\frac{x+y}{1-x y}$ and $v=\tan ^{-1} x+\tan ^{-1} y$, find $\frac{\partial(u, v)}{\partial(x, y w)}$. (b) Are $u$ and $v$ functionally related? If so, find the relationship.

(a) $\frac{\partial(u, v)}{\partial(x, y)}=\left|\begin{array}{ll}u_{x} & u_{y} \\ v_{x} & v_{y}\end{array}\right|=\left|\begin{array}{cc}\frac{1+y^{2}}{(1-x y)^{2}} & \frac{1+x^{2}}{(1-x y)^{2}} \\ \frac{1}{1+x^{2}} & \frac{1}{1+y^{2}}\end{array}\right|=0 \quad$ if $x y \neq 1$

(b) By Problem 6.35, since the Jacobian is identically zero in a region, there must be a functional relationship between $u$ and $v$. This is seen to be $\tan v=u$; i.e., $\phi(u, v)=u-\tan v=0$. We can show this directly by solving for $x$ (say) in one of the equations and then substituting in the other. Thus, for example, from $v=$ $\tan ^{-1} x+\tan ^{-1} y$, we find $\tan ^{-1} x=v-\tan ^{-1} y$ and so

$$
x=\tan \left(v-\tan ^{-1} y\right)=\frac{\tan v-\tan \left(\tan ^{-1} y\right)}{1+\tan v \tan \left(\tan ^{-1} y\right)}=\frac{\tan v-y}{1+y \tan v}
$$

Then substituting this in $u=(x+y) /(1-x y)$ and simplifying, we find $u=\tan v$. 6.37. (a) If $x=u-v+w, y=u^{2}-v^{2}-w^{2}$ and $z=u^{3}+v$, evaluate the Jacobian $\frac{\partial(x, y, z)}{\partial(u, v, w)}$, and (b) explain the\\
significance of the nonvanishing of this Jacobian.

(a) $\frac{\partial(x, y, z)}{\partial(u, v, w)}=\left|\begin{array}{lll}x_{u} & x_{v} & x_{w} \\ y_{u} & y_{v} & y_{w} \\ z_{u} & z_{v} & z_{w}\end{array}\right|=\left|\begin{array}{ccc}1 & -1 & 1 \\ 2 u & -2 v & -2 w \\ 3 u^{2} & 1 & 0\end{array}\right|=6 w u^{2}+2 u+6 u^{2} v+2 w$

(b) The given equations can be solved simultaneously for $u, v, w$ in terms of $x, y, z$ in a region $\Re$ if the Jacobian is not zero in $\Re$.

\section*{Transformations, curvilinear coordinates}
6.38. A region $\Re$ in the $x y$ plane is bounded by $x+y=6, x-y=2$, and $y=0$. (a) Determine the region $\Re^{\prime}$ in the $u v$ plane into which $\Re$ is mapped under the transformation $x=u+v, y=u-v$. (b) Compute $\frac{\partial(x, y)}{\partial(u, v)}$. (c) Compare the result of (b) with the ratio of the areas of $\Re$ and $\Re^{\prime}$.

(a) The region $\Re$ shown shaded in Figure 6.9 (a) is a triangle bounded by the lines $x+y=6, x-y=2$, and $y$ $=0$, which for distinguishing purposes are shown dotted, dashed, and heavy, respectively.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-160}
\end{center}

(a) $x y$ plane

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-160(1)}
\end{center}

(b) $u v$ plane

Figure 6.9

Under the given transformation, the line $x+y=6$ is transformed into $(u+v)+(u-v)=6$; i.e., $2 u=6$ or $u=3$, which is a line (shown dotted) in the $u v$ plane of Figure 6.9(b).

Similarly, $x-y=2$ becomes $(u+v)-(u-v)=2$ or $v=1$, which is a line (shown dashed) in the $u v$ plane. In like manner, $y=0$ becomes $u-v=0$ or $u=v$, which is a line shown heavy in the $u v$ plane. Then the required region is bounded by $u=3, v=1$, and $u=v$, and is shown shaded in Figure 6.9(b).

(b) $\frac{\partial(x, y)}{\partial(u, v)}=\left|\begin{array}{ll}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}\end{array}\right|=\left|\begin{array}{ll}\frac{\partial}{\partial u}(u+v) & \frac{\partial}{\partial u}(u+v) \\ \frac{\partial}{\partial u}(u-v) & \frac{\partial}{\partial v}(u-v)\end{array}\right|=\left|\begin{array}{cc}1 & 1 \\ 1 & -1\end{array}\right|=2$

(c) The area of triangular region $\Re$ is 4 , whereas the area of triangular region $\Re^{\prime}$ is 2 . Hence, the ratio is $4 / 2$ $=2$, agreeing with the value of the Jacobian in (b). Since the Jacobian is constant in this case, the areas of any regions $\Re$ in the $x y$ plane are twice the areas of corresponding mapped regions $\mathfrak{R}^{\prime}$ in the $u v$ plane.

6.39. A region $\Re$ in the $x y$ plane is bounded by $x^{2}+y^{2}=a^{2}, x^{2}+y^{2}=b^{2}, x=0$, and $y=0$, where $0<a<b$.

(a) Determine the region $\Re^{\prime}$ into which $\Re$ is mapped under the transformation $x=\rho \cos \phi, y=\rho \sin \phi$, where $\rho>0,0 \leqq \phi<2 \pi$. (b) Discuss what happens when $a=0$. (c) compute $\frac{\partial(x, y)}{\partial(\rho, \phi)}$. (d) compute $\frac{\partial(\rho, \phi)}{\partial(x, y)}$.

(a) The region $\Re$ [shaded in Figure 6.10(a)] is bounded by $x=0$ (dotted), $y=0$ (dotted and dashed), $x^{2}+y^{2}$ $=a^{2}$ (dashed), and $x^{2}+y^{2}=b^{2}$ (heavy).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-161}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-161(1)}
\end{center}

(b)

Figure 6.10

Under the given transformation, $x^{2}+y^{2}=a^{2}$ and $x^{2}+y^{2}=b^{2}$ become $\rho^{2}=a^{2}$ and $\rho^{2}=b^{2}$ or $\rho=a$ and $\rho=b$, respectively. Also, $x=0, a \leqq y \leqq b$ becomes $\phi=\pi / 2, a \leqq \rho \leqq b ; y=0, a \leqq x \leqq b$ becomes $\phi=0, a \leqq \rho \leqq b$.

The required region $\mathfrak{R}^{\prime}$ is shown shaded in Figure 6.10(b).

Another method: Using the fact that $\rho$ is the distance from the origin $O$ of the $x y$ plane and $\phi$ is the angle measured from the positive $x$ axis, it is clear that the required region is given by $a \leqq \rho \leqq b, 0 \leqq \phi \leqq \pi / 2$, as indicated in Figure 6.10(b).

(b) If $a=0$, the region $\Re$ becomes one-fourth of a circular region of radius $b$ (bounded by three sides), while $\mathfrak{R}^{\prime}$ remains a rectangle. The reason for this is that the point $x=0, y=0$ is mapped into $\rho=0, \phi=$ an indeterminate and the transformation is not one to one at this point, which is sometimes called a singular point.

(c) $\frac{\partial(x, y)}{\partial(\rho, \phi)}=\left|\begin{array}{ll}\frac{\partial}{\partial \rho}(\rho \cos \phi) & \frac{\partial}{\partial \phi}(\rho \cos \phi) \\ \frac{\partial}{\partial \rho}(\rho \sin \phi) & \frac{\partial}{\partial \phi}(\rho \sin \phi)\end{array}\right|=\left|\begin{array}{cc}\cos \phi & -\rho \sin \phi \\ \sin \phi & \rho \cos \phi\end{array}\right|$ $=\rho\left(\cos ^{2} \phi+\sin ^{2} \phi\right)=\rho$

(d) From Problem 6.43(b) we have, letting $u=\rho, v=\phi, \frac{\partial(x, y)}{\partial(\rho, \phi)} \frac{\partial(\rho, \phi)}{\partial(x, y)}=1$ so that, thing (c), $\frac{\partial(\rho, \phi)}{\partial(x, y)}=\frac{1}{\rho}$

This can also be obtained by direct differentiation.

Note that from the Jacobians of these transformations it is clear why $\rho=0$ (i.e., $x=0, y=0$ ) is a singular point.

\section*{Mean value theorem}
6.40. Prove the mean value theorem for functions of two variables.

Let $f(t)=f\left(x_{0}+h t, y_{0}+k t\right)$. By the mean value theorem for functions of one variable,


\begin{equation*}
F(1)=F(0)=F^{\prime}(\theta) 0<\theta<1 \tag{1}
\end{equation*}


If $x=x_{0}+h t, y=y_{0}+k t$, then $F(t)=f(x, y)$, so that by Problem 6.17,

$F^{\prime}(t)=f_{x}(d x / d t)+f_{y}(d y / d t)=h f_{x}+k f_{y}$ and $F^{\prime}(\theta)=h f_{x}\left(x_{0}+\theta h, y_{0}+\theta k\right)+k f_{y}\left(x_{0}+\theta h, y_{0}+\theta k\right)$

where $0<\theta<1$. Thus, (1) becomes


\begin{equation*}
f\left(x_{0}+h, y_{0}+k\right)-f\left(x_{0}, y_{0}\right)=h f_{x}\left(x_{0}+\theta h, y_{0}+\theta k\right)+k f_{y}\left(x_{0}+\theta h, y_{0}+\theta k\right) \tag{2}
\end{equation*}


where $0<\theta<1$ as required.

Note that Equation (2), which is analogous to Equation (1) of Problem 6.14, where $h=\Delta x$, has the advantage of being more symmetric (and also more useful), since only a single number $\theta$ is involved.

\section*{Miscellaneous problems}
6.41. Let $f(x, y)=\left\{\begin{array}{ll}x y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right) & (x, y) \neq(0,0) \\ 0 & (x, y)=(0,0)\end{array}\right.$. Compute (a) $f_{x}(0,0)$, (b) $f_{y}(0,0)$, (c) $f_{x x}(0,0)$ (d) $f_{y y}(0,0)$, (e) $f_{x y}(0,0)$, and (f) $f_{y x}(0,0)$.

(a) $\lim _{h \rightarrow 0} \frac{f(h, 0)-f(0,0)}{h}=\lim _{h \rightarrow 0} \frac{0}{h}=0$

(b) $\lim _{h \rightarrow 0} \frac{f(0, k)-f(0,0)}{k}=\lim _{k \rightarrow 0} \frac{0}{k}=0$

If $(x, y) \neq(0,0)$,

$$
\begin{aligned}
& f_{x}(x, y)=\frac{\partial}{\partial x}\left\{x y\left(\frac{x^{2}-y^{2}}{x^{2}-y^{2}}\right)\right\}=x y\left(\frac{4 x y^{2}}{\left(x^{2}+y^{2}\right)^{2}}\right)+y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right) \\
& f_{x}(x, y)=\frac{\partial}{\partial y}\left\{x y\left(\frac{x^{2}-y^{2}}{x^{2}-y^{2}}\right)\right\}=x y\left(\frac{-4 x y^{2}}{\left(x^{2}+y^{2}\right)^{2}}\right)+x\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right)
\end{aligned}
$$

Then

(c) $\lim _{h \rightarrow 0} \frac{f_{x}(h, 0)-f_{x}(0,0)}{h}=\lim _{h \rightarrow 0} \frac{0}{h}=0$

(d) $\lim _{k \rightarrow 0} \frac{f_{y}(0, k)-f_{y}(0,0)}{k}=\lim _{k \rightarrow 0} \frac{0}{k}=0$

(e) $\lim _{k \rightarrow 0} \frac{f_{x}(0, k)-f_{x}(0,0)}{k}=\lim _{k \rightarrow 0} \frac{-k}{k}=-1$

(f) $\lim _{h \rightarrow 0} \frac{f_{y}(h, 0)-f_{y}(0,0)}{h}=\lim _{h \rightarrow 0} \frac{h}{h}=1$

Note that $f_{x y} \neq f_{y x}$ at $(0,0)$. See Problem 6.13.

6.42. Show that under the transformation $x=\rho \cos \phi, y=\rho \sin \phi$ the equation $\frac{\partial^{2} V}{\partial x^{2}}+\frac{\partial^{2} V}{\partial y^{2}}=0$ become $\frac{\partial^{2} V}{\partial \rho^{2}}+\frac{1}{\rho} \frac{\partial V}{\partial \phi}+\frac{1}{\rho^{2}} \frac{\partial^{2} V}{\partial \phi^{2}}=0$.

We have


\begin{align*}
& \frac{\partial V}{\partial x}=\frac{\partial V}{\partial \rho} \frac{\partial \rho}{\partial x}+\frac{\partial V}{\partial \phi} \frac{\partial \phi}{\partial x}  \tag{1}\\
& \frac{\partial V}{\partial y}=\frac{\partial V}{\partial \rho} \frac{\partial \rho}{\partial y}+\frac{\partial V}{\partial \phi} \frac{\partial \phi}{\partial y} \tag{2}
\end{align*}


Differentiate $x=\rho \cos \phi, y=\rho \sin \phi$ with respect to $x$, remembering that $\rho$ and $\phi$ are functions of $x$ and $y$

$$
1=-\rho \sin \phi \frac{\partial \phi}{\partial x}+\cos \phi \frac{\partial \rho}{\partial x} . \quad 0=\rho \cos \phi \frac{\partial \phi}{\partial x}+\sin \phi \frac{\partial \rho}{\partial x}
$$

Solving simultaneously,


\begin{equation*}
\frac{\partial \rho}{\partial x}=\cos \phi, \quad \frac{\partial \phi}{\partial x}=-\frac{\sin \phi}{\rho} \tag{3}
\end{equation*}


Similarly, differentiate with respect to $y$. Then

$$
0=-\rho \sin \phi \frac{\partial \phi}{\partial y}+\cos \phi \frac{\partial \rho}{\partial y}, \quad 1=\rho \cos \phi \frac{\partial \phi}{\partial y}+\sin \phi \frac{\partial \rho}{\partial y}
$$

Solving simultaneously,


\begin{equation*}
\frac{\partial \rho}{\partial y}=\sin \phi, \quad \frac{\partial \phi}{\partial y}=\frac{\cos \phi}{\rho} \tag{4}
\end{equation*}


Then, from Equations (1) and (2),


\begin{align*}
& \frac{\partial V}{\partial x}=\cos \phi \frac{\partial V}{\partial \rho}-\frac{\sin \phi}{\rho} \frac{\partial V}{\partial \phi}  \tag{5}\\
& \frac{\partial V}{\partial y}=\sin \phi \frac{\partial V}{\partial \rho}+\frac{\cos \phi}{\rho} \frac{\partial V}{\partial \phi} \tag{6}
\end{align*}


Hence,

$$
\begin{aligned}
\frac{\partial^{2} V}{\partial x^{2}}= & \frac{\partial}{\partial x}\left(\frac{\partial V}{\partial x}\right)=\frac{\partial}{\partial \rho}\left(\frac{\partial V}{\partial x}\right) \frac{\partial \rho}{\partial x}+\frac{\partial}{\partial \phi}\left(\frac{\partial V}{\partial x}\right) \frac{\partial \phi}{\partial x} \\
= & \frac{\partial}{\partial \rho}\left(\cos \phi \frac{\partial V}{\partial \rho}-\frac{\sin \phi}{\rho} \frac{\partial V}{\partial \phi}\right) \frac{\partial \rho}{\partial x}+\frac{\partial}{\partial \phi}\left(\cos \phi \frac{\partial V}{\partial \rho}-\frac{\sin \phi}{\rho} \frac{\partial V}{\partial \phi}\right) \frac{\partial \phi}{\partial x} \\
= & \left(\cos \phi \frac{\partial^{2} V}{\partial \rho^{2}}+\frac{\sin \phi}{\rho^{2}} \frac{\partial V}{\partial \phi}-\frac{\sin \phi}{\rho} \frac{\partial^{2} V}{\partial \rho}\right)(\cos \phi) \\
& +\left(-\sin \phi \frac{\partial V}{\partial \rho}+\cos \phi \frac{\partial^{2} V}{\partial \rho \partial \phi}-\frac{\cos \phi}{\rho} \frac{\partial V}{\partial \phi}-\frac{\sin \phi}{\rho} \frac{\partial^{2} V}{\partial \phi^{2}}\right)\left(-\frac{\sin \phi}{\rho}\right)
\end{aligned}
$$

which simplifies to


\begin{equation*}
\frac{\partial^{2} V}{\partial x^{2}}=\cos ^{2} \phi \frac{\partial^{2} V}{\partial \rho^{2}}+\frac{2 \sin \phi \cos \phi}{\rho^{2}} \frac{\partial V}{\partial \phi}-\frac{2 \sin \phi \cos \phi}{\rho} \frac{\partial^{2} V}{\partial \rho \partial \phi}+\frac{\sin ^{2} \phi}{\rho} \frac{\partial V}{\partial \rho}+\frac{\sin ^{2} \phi}{\rho^{2}} \frac{\partial^{2} V}{\partial \phi^{2}} \tag{7}
\end{equation*}


Similarly,


\begin{equation*}
\frac{\partial^{2} V}{\partial y^{2}}=\sin ^{2} \phi \frac{\partial^{2} V}{\partial \rho^{2}}-\frac{2 \sin \phi \cos \phi}{\rho^{2}} \frac{\partial V}{\partial \phi}+\frac{2 \sin \phi \cos \phi}{\rho} \frac{\partial^{2} V}{\partial \rho \partial \phi}+\frac{\cos ^{2} \phi}{\rho} \frac{\partial V}{\partial \rho}+\frac{\cos ^{2} \phi}{\rho^{2}} \frac{\partial^{2} V}{\partial \phi^{2}} \tag{8}
\end{equation*}


Adding Equations (7) and (8), we find, as required, $\frac{\partial^{2} V}{\partial x^{2}}+\frac{\partial^{2} V}{\partial y^{2}}=\frac{\partial^{2} V}{\partial \rho^{2}}+\frac{1}{\rho} \frac{\partial V}{\partial \rho}+\frac{1}{\rho^{2}} \frac{\partial^{2} V}{\partial \phi^{2}}=0$.

6.43. (a) If $x=f(u, v)$ and $y=g(u, v)$, where $u=\phi(r, s)$ and $v=\psi(r, s)$, prove that $\frac{\partial(x, y)}{\partial(r, s)}=\frac{\partial(x, y)}{\partial(u, v)} \frac{\partial(u, v)}{\partial(r, s)}$.

(b) Prove that $\frac{\partial(x, y)}{\partial(u, v)} \frac{\partial(u, v)}{\partial(x, y)}=1$, provided $\frac{\partial(x, y)}{\partial(u, v)} \neq 0$, and interpret geometrically.\\
(a) $\frac{\partial(x, y)}{\partial(r, s)}=\left|\begin{array}{ll}x_{r} & x_{s} \\ y_{r} & y_{s}\end{array}\right|=\left|\begin{array}{ll}x_{u} u_{r}+x_{v} v_{r} & x_{u} u_{s}+x_{v} v_{s} \\ y_{u} u_{r}+y_{v} v_{r} & y_{u} u_{s}+y_{v} v_{s}\end{array}\right|$

$$
=\left|\begin{array}{ll}
x_{u} & x_{v} \\
y_{u} & y_{v}
\end{array}\right|\left|\begin{array}{cc}
u_{r} & u_{s} \\
v_{r} & v_{s}
\end{array}\right| w=\frac{\partial(x, y) \partial(u, v)}{\partial(u, v) \partial(r, s)}
$$

using a theorem on multiplication of determinants (see Problem 6.108). We have assumed here, of course, the existence of the partial derivatives involved.

(b) Place $r=x, s=y$ in the result of (a). Then $\frac{\partial(x, y)}{\partial(u, v)} \frac{\partial(u, v)}{\partial(x, y)}=\frac{\partial(x, y)}{\partial(x, y)}=1$.

The equations $x=f(u, v), y=g(u, v)$ define a transformation between points $(x, y)$ in the $x y$ plane and points $(u, v)$ in the $u v$ plane. The inverse transformation is given by $u=\phi(x, y), v=\psi(x, y)$. The result obtained states that the Jacobians of these transformations are reciprocals of each other.

6.44. Show that $F(x y, z-2 x)=0$ satisfies, under suitable conditions, the equation $x(\partial z / \partial x)-y(\partial z / \partial y)=2 x$. What are these conditions?

Let $u=x y, v=z-2 x$. Then $F(u, v)=0$ and


\begin{equation*}
d F=F_{u} d u+F_{v} d v=F_{u}(x d y+y d x)+F_{v}(d z-2 d x)=0 \tag{1}
\end{equation*}


Taking $z$ as dependent variable and $x$ and $y$ as independent variables, we have $d z=z_{x} d x+z_{y} d y$. Then substituting in Equation (1), we find

$$
\left(y F_{u}+F_{v} z_{x}-2\right) d x+\left(x F_{u}+F_{v} z_{y}\right) d y=0
$$

Hence, since $x$ and $y$ are independent, we have


\begin{gather*}
y F_{u}+F_{v} z_{x}-2=0  \tag{2}\\
x F_{u}+F_{v} z_{y}=0 \tag{3}
\end{gather*}


Solve for $F_{u}$ in Equation (3) and substitute in (2). Then we obtain the required result $x z_{x}-y z_{y}=2 x$ upon dividing by $F_{v}$ (supposed not equal to zero).

The result will certainly be valid if we assume that $F(u, v)$ is continuously differentiable and that $F_{v} \neq 0$.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Functions and graphs}
6.45. If $f(x, y)=\frac{2 x+y}{1-x y}$, find (a) $f(1-3)$, (b) $\frac{f(2+h, 3)-f(2,3)}{h}$, and (c) $f(x+y, x y)$.

$$
\text { Ans. (a) }-\frac{1}{4} \text { (b) } \frac{11}{5(3 h+5)} \text { (c) } \frac{2 x+2 y+x y}{1-x^{2} y-x y^{2}}
$$

6.46. If $g(x, y, z)=x^{2}-y z+3 x y$, find (a) $g(1,-2,2)$, (b) $g\left(x+1, y-1, z^{2}\right)$, and (c) $g(x y, x z, x+y)$.

$$
\text { Ans. (a) }-1 \text { (b) } x^{2}-x-2-y z^{2}+z^{2}+3 x y+3 y \text { (c) } x^{2} y^{2}-x^{2} z-x y z+3 x^{2} y z
$$

6.47. Give the domain of definition for which each of the following functional rules is defined and real, and indicate this domain graphically: (a) $f(x, y)=\frac{1}{x^{2}+y^{2}-1}$, (b) $f(x, y)=\ln (x+y)$, and (c) $f(x, y)=\sin ^{-1}\left(\frac{2 x-y}{x+y}\right)$.

$$
\text { Ans. (a) } x^{2}+y^{2} \neq 1 \text { (b) } x+y>0 \text { (c) }\left|\frac{2 x-y}{x+y}\right| \leqq 1
$$

6.48. (a) What is the domain of definition for which $f(x, y, z)=\sqrt{\frac{x+y+z-1}{x^{2}+y^{2}+z^{2}-1}}$ is defined and real?\\
(b) Indicate this domain graphically.

Ans. (a) $x+y+z \leqq 1, x^{2}+y^{2}+z^{2}<1$ and $x+y+z \geqq 1, x^{2}+y^{2}+z^{2}>1$

6.49. Sketch and name the surface in three-dimensional space represented by each of the following.\\
(a) $3 x+2 z=12$\\
(d) $x^{2}+z^{2}=y^{2}$\\
(g) $x^{2}+y^{2}=2 y$\\
(b) $4 z=x^{2}+y^{2}$\\
(e) $x^{2}+y^{2}+z^{2}=16$\\
(h) $z=x+y$\\
(c) $z=x^{2}-4 y^{2}$\\
(f) $x^{2}-4 y^{2}-4 z^{2}=36$\\
(i) $y^{2}=4 z$\\
(j) $x^{2}+y^{2}+z^{2}-4 x+6 y+2 z-2=0$

Ans. (a) plane (b) paraboloid of revolution (c) hyperbolic paraboloid (d) right circular cone (e) sphere (f) hyperboloid of two sheets (g) right circular cylinder (h) plane (i) arabolic cylinder (j) sphere, center at $(2,-3,-1)$ and radius 4 .

6.50. Construct a graph of the region bounded by $x^{2}+y^{2}=a^{2}$ and $x^{2}+z^{2}=a^{2}$, where $a$ is a constant.

6.51. Describe graphically the set of points $(x, y, z)$ such that (a) $x^{2}+y^{2}+z^{2}=1, x^{2}+y^{2}=z^{2}$ and (b) $x^{2}+y^{2}<z<x+y$.

6.52. The level curves for a function $z=f(x, y)$ are curves in the $x y$ plane defined by $f(x, y)=c$, where $c$ is any constant. They provide a way of representing the function graphically. Similarly, the level surfaces of $w=f(x, y, z)$ are the surfaces in a rectangular (xyz) coordinate system defined by $f(x, y, z)=c$, where $c$ is any constant. Describe and graph the level curves and surfaces for each of the following functions: (a) $f(x, y)=\ln \left(x^{2}+y^{2}-1\right)$, (b) $f(x, y)=4 x y$, (c) $f(x, y)=\tan ^{-1} y /(x+1)$, (d) $f(x, y)=x^{2 / 3}+y^{2 / 3}$, (e) $f(x, y, z)$ $=x^{2}+4 y^{2}+16 z^{2}$ and (f) $\sin (x+z) /(1-y)$.

\section*{Limits and continuity}
6.53. Prove that $(a) \lim _{\substack{x \rightarrow 4 \\ y \rightarrow-1}}(3 x-2 y)=14$ and $(b) \lim _{(x, y) \rightarrow(2,1)}(x y-3 x+4)=0$ by using the definition.

6.54. If $\lim f(x, y)=A$ and $\lim g(x, y)=B$, where $\lim$ denotes limit as $(x, y) \rightarrow\left(x_{0}, y_{0}\right)$, prove that (a) $\lim \{f(x, y)+$ $g(x, y)\}=A+B$ and (b) $\lim \{f(x, y) g(x, y)\}=A B$.

6.55. Under what conditions is the limit of the quotient of two functions equal to the quotient of their limits? Prove your answer.

6.56. Evaluate each of the following limits where they exist:\\
(a) $\lim _{\substack{x \rightarrow 1 \\ y \rightarrow 2}} \frac{3-x+y}{4+x-2 y}$\\
(c) $\lim _{\substack{x \rightarrow 4 \\ y \rightarrow \pi}} x^{2} \sin \frac{y}{x}$\\
(e) $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 1}} e^{-1 / x^{2(y-1)^{2}}}$\\
(g) $\lim _{\substack{x \rightarrow 0+\\ y \rightarrow 1-}} \frac{x+y-1}{\sqrt{x}-\sqrt{1-y}}$\\
(b) $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} \frac{3 x-2 y}{2 x-3 y}$\\
(d) $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} \frac{x \sin \left(x^{2}+y^{2}\right)}{x^{2+} y^{2}}$\\
(f) $\lim _{\substack{x \rightarrow 0 \\ y \rightarrow 0}} \frac{2 x-y}{x^{2}+y^{2}}$\\
(h) $\lim _{\substack{x \rightarrow 2 \\ y \rightarrow 1}} \frac{\sin ^{-1}(x y-2)}{\tan ^{-1}(3 x y-6)}$ Ans. (a) 4 (b) does not exist (c) $8 \sqrt{2}$ (d) 0 (e) 0 (f) does not exist (g) 0 (h) $\frac{1}{3}$

6.57. Formulate a definition of limit for functions of (a) 3 and (b) $n$ variables.

6.58. Does $\lim \frac{4 x+y-3 z}{2 x-5 y+2 z}$ as $(x, y, z) \rightarrow(0,0,0)$ exist? Justify your answer.

6.59. Investigate the continuity of each of the following functions at the indicated points: (a) $x^{2}+y^{2},\left(x_{0}, y_{0}\right)$; (b) $\frac{x}{3 x+5 y},(0,0)$; and (c) $\left(x^{2}+y^{2}\right) \sin \frac{1}{x^{2}+y^{2}}$ if $(x, y) \neq(0,0), 0$ if $(x, y)=(0,0),(0,0)$.

Ans. (a) continuous (b) discontinuous (c) continuous

6.60. Using the definition, prove that $f(x, y)=x y+6 x$ is continuous continuous at (a) $(1,2)$ and (b) $\left(x_{0}, y_{0}\right)$.

6.61. Prove that the function in Problem 6.60 is uniformly continuous in the square region defined by $0 \leqq x \leqq 1$, $0 \leqq y \leqq 1$.

\section*{Partial derivatives}
6.62. If $f(x, y)=\frac{x-y}{x+y}$, find (a) $\partial f / \partial x$ and (b) $\partial f / \partial y$ at $(2,-1)$ from the definition and verify your answer by differentiation rules.

Ans. (a) -2 (b) -4

6.63. If $f(x, y)=\left\{\begin{array}{ll}\left(x^{2}-x y\right) /(x+y) & \text { for }(x, y) \neq(0,0) \\ 0 & \text { for }(x, y)=(0,0)\end{array}\right.$ find (a) $f_{x}(0,0)$ and (b) $f_{y}(0,0)$.

Ans. (a) 1 (b) 0

6.64. Investigate $\lim _{(x, y) \rightarrow(0,0)} f_{x}(x, y)$ for the function in the preceding problem and explain why this limit (if it exists) is or is not equal to $f_{x}(0,0)$.

6.65. If $f(x, y)=(x-y) \sin (3 x+2 y)$, compute (a) $f_{x}$, (b) $f_{y}$, (c) $f_{x x}$, (d) $f_{y y}$, and (e) $f_{y x}$ at $(0, \pi / 3)$.\\
Ans. (a) $\frac{1}{2}(\pi+\sqrt{3})$\\
(b) $\frac{1}{6}(2 \pi-3 \sqrt{3})$\\
(c) $\frac{3}{2}(\pi \sqrt{3}-2)$\\
(d) $\frac{2}{3}(i \sqrt{3}+3)$\\
(e) $\frac{1}{2}(2 \pi \sqrt{3}+1)$

6.66. (a) Prove by direct differentiation that $z=x y \tan (y / x)$ satisfies the equation $x(\partial z / \partial x)+y(\partial z / \partial y)=2 z$ if $(x, y)$ $\neq(0,0)$. (b) Discuss part (a) for all other points $(x, y)$, assuming $z=0$ at $(0,0)$.

6.67. Verify that $f_{x y}=f_{y x}$ for the functions (a) $(2 x-y) /(x+y)$, (b) $x \tan x y$, and (c) $\cosh (y+\cos x)$, indicating possible exceptional points, and investigate these points.

6.68. Show that $z=\ln \left\{(x-a)^{2}+(y-b)^{2}\right\}$ satisfies $\partial^{2} z / \partial x^{2}+\partial^{2} z / \partial y^{2}=0$ except at $(a, b)$.

6.69. Show that $z=x \cos (y / x)+\tan (y / x)$ satisfies $x^{2} z_{x x}+2 x y z_{x y}+y^{2} z_{y y}=0$ except at points for which $x=0$.

6.70. Show that if $w=\left(\frac{x-y+z}{x+y-z}\right)^{n}$, then:

(a) $x \frac{\partial w}{\partial x}+y \frac{\partial w}{\partial y}+z \frac{\partial w}{\partial z}=0$

(b) $x^{2} \frac{\partial^{2} w}{\partial x^{2}}+y^{2} \frac{\partial^{2} w}{\partial y^{2}}+z^{2} \frac{\partial^{2} w}{\partial z^{2}}+2 x y \frac{\partial^{2} w}{\partial x \partial y}+2 x z \frac{\partial^{2} w}{\partial x \partial z}+2 y z \frac{\partial^{2} w}{\partial y \partial z}=0$

Indicate possible exceptional points.

\section*{Differentials}
6.71. If $z=x^{3}-x y+3 y^{2}$, compute (a) $\Delta z$ and (b) $d z$, where $x=5, y=4, \Delta x=-0.2, \Delta y=0.1$. Explain why $\Delta z$ and $d z$ are approximately equal. (c) Find $\Delta z$ and $d z$ if $x=5, y=4, \Delta x=-2, \Delta y=1$.

Ans. (a) -11.658 (b) -12.3 (c) $\Delta z=-66, d z=-123$

6.72. Compute $\sqrt[5]{(3.8)^{2}+2(2.1)^{3}}$ approximately, using differentials.

Ans. 2.01

6.73. Find $d F$ and $d G$ if (a) $F(x, y)=x^{3} y-4 x y^{2}+8 y^{3}$, (b) $G(x, y, z)=8 x y^{2} z^{3}-3 x^{2} y z$, and (c) $F(x, y)=x y^{2} \ln (y / x)$.

Ans. (a) $\left(3 x^{2} y-4 y^{2}\right) d x+\left(x^{3}-8 x y+24 y^{2}\right) d y$

(b) $\left(8 y^{2} z^{3}-6 x y z\right) d x+\left(16 x y z^{3}-3 x^{2} z\right) d y+\left(24 x y^{2} z^{2}-3 x^{2} y\right) d z$

(c) $\left\{y^{2} \ln (y / x)-y^{2}\right\} d x+\{2 x y \ln (y / x)+x y\} d y$

6.74. Prove that (a) $d(U V)=U d V+V d U$, (b) $d(U / V)=(V d U-U d V) / V^{2}$, (c) $d(\ln U)=(d U) / U$, and (d) $d\left(\tan ^{-1} V\right)$ $=(d V) /\left(1+v^{2}\right)$, where $U$ and $V$ are differentiable functions of two or more variables.

6.75. Determine whether each of the following is an exact differential of a function and, if so, find the function:

(a) $\left(2 x y^{2}+3 y \cos 3 x\right) d x+\left(2 x^{2} y+\sin 3 x\right) d y$

(b) $\left(6 x y-y^{2}\right) d x+\left(2 x e^{y}-x^{2}\right) d y$

(c) $\left(z^{3}-3 y\right) d x+\left(12 y^{2}-3 x\right) d y+3 x z^{2} d z$

Ans. (a) $x^{2} y^{2}+y \sin 3 x+c$ (b) not exact (c) $x z^{2}+4 y^{3}-3 x y+c$

\section*{Differentiation of composite functions}
6.76. (a) If $U(x, y, z)=2 x^{2}-y z+x z^{2}, x=2 \sin t, y=t^{2}-t+1$, and $z=3 e^{-1}$, find $d U / d t$ at $t=0$. (b) If $H(x, y)=$ $\sin (3 x-y), x^{3}+2 y=2 t^{3}$, and $x-y^{2}=t^{2}+3 t$, find $d H / d t$.

Ans. (a) 24 (b) $\left(\frac{36 t^{2} y+12 t+9 x^{2}-6 t^{2}+6 x^{2} t+18}{6 x^{2} y+2}\right) \cos (3 x-y)$

6.77. If $F(x, y)=(2 x+y) /(y-2 x), x=2 u-3 v$, and $y=u+2 v$, find (a) $\partial F / \partial u$, (b) $\partial F / \partial v$, (c) $\partial^{2} F / \partial u^{2}$, (d) $\partial^{2} F / \partial v^{2}$, and (e) $\partial^{2} F / \partial u \partial v$, where $u=2, v=1$.

Ans. (a) 7 (b) -14 (c) 21 (d) 112 (e) -49

6.78. If $U=x^{2} F(y / x)$, show that, under suitable restrictions on $F, x(\partial U / \partial x)+y(\partial U / \partial y)=2 U$.

6.79. If $x=u \cos \alpha-v \sin \alpha$ and $y=u \sin \alpha+v \cos \alpha$, where $\alpha$ is a constant, show that $(\partial V / \partial x)^{2}+(\partial V / \partial y)^{2}=$ $(\partial V / \partial u)^{2}+(\partial V / \partial v)^{2}$.

6.80. Show that if $x=\rho \cos \phi, y=\rho \sin \phi$, the equation $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}, \frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x}$ becomes $\frac{\partial u}{\partial \rho}=\frac{1}{\rho} \frac{\partial v}{\partial \phi}, \frac{\partial v}{\partial \rho}=-\frac{1}{\rho} \frac{\partial u}{\partial \phi}$.

6.81. Use Problem 6.80 to show that under the transformation $x=\rho \cos \phi, y=\rho \sin \phi$, the equation

$$
\frac{\partial^{2} u}{\partial x^{2}}=\frac{\partial^{2} u}{\partial y^{2}}=0 \text { becomes } \frac{\partial^{2} u}{\partial \rho^{2}}=\frac{1}{\rho} \frac{\partial u}{\partial \rho}+-\frac{1}{\rho^{2}} \frac{\partial^{2} u}{\partial \phi^{2}}=0 .
$$

\section*{Implicit functions and jacobians}
6.82. If $F(x, y)=0$, prove that $d y / d x=-F_{x} / F_{y}$.

6.83. Find (a) $d y / d x$ and (b) $d^{2} y / d x^{2}$ if $x^{3}+y^{3}-3 x y=0$.

$$
\text { Ans. (a) }\left(y-x^{2}\right) /\left(y^{2}-x\right) \text { (b) }-2 x y /\left(y^{2}-x\right)^{3}
$$

6.84. If $x u^{2}+v=y^{3}, 2 y u-x v^{3}=4 x$, find (a) $\frac{\partial u}{\partial x}$ and (b) $\frac{\partial v}{\partial y}$.\\
Ans. (a) $\frac{v^{3}-3 x u^{2} v^{2}+4}{6 x^{2}-w v^{2}+2 y}$\\
(b) $\frac{2 x u^{2}+3 y^{3}}{3 x^{2} w^{2}+y}$

6.85. If $u=f(x, y)$ and $v=g(x, y)$ are differentiable, prove that $\frac{\partial u}{\partial x} \frac{\partial x}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial x}{\partial v}=1$. Explain clearly which variables are considered independent in each partial derivative.

6.86. If $f(x, y, r, s)=0, g(x, y, r, s)=0$, prove that $\frac{\partial y}{\partial r} \frac{\partial r}{\partial x}+\frac{\partial y}{\partial s} \frac{\partial s}{\partial x}=0$, explaining which variables are independent. What notation could you use to indicate the independent variables considered?

6.87. If $F(x, y)=0$, show that $\frac{d^{2} y}{d x^{2}}=-\frac{F_{x x} F_{y}^{2}-2 F_{x y} F_{x} F_{y}+F_{y y} F_{x}^{2}}{F_{y}^{3}}$.

6.88. Evaluate $\frac{\partial(F, G)}{\partial(u, v)}$ if $F(u, v)=3 u^{2}-w$ and $G(u, v)=2 w^{2}+v^{3}$.

Ans. $24 u^{2} v+16 u v^{2}-3 v^{3}$

6.89. If $F=x+3 y^{2}-z^{3}, G=2 x^{2} y z$, and $H=2 z^{2}-x y$, evaluate $\frac{\partial(F, G, H)}{\partial(x, y, z)}$ at $(1,-1,0)$.

Ans. 10

6.90. If $u=\sin ^{-1} x+\sin ^{-1} y$ and $v=x \sqrt{1-y^{2}}+y \sqrt{1-x^{2}}$, determine whether there is a functional relationship between $u$ and $v$, and, if so, find it.

6.91. If $F=x y+y z+z x, G=x^{2}+y^{2}+z^{2}$, and $H=x+y+z$, determine whether there is a functional relationship connecting $F, G$, and $H$, and, if so, find it.

Ans. $H^{2}-G-2 F=0$

6.92. (a) If $x=f(u, v, w), y=g(u, v, w)$, and $z=h(u, v, w)$, prove that $\frac{\partial(x, y, z)}{\partial(x, v, z)} \frac{\partial(u, v, w)}{\partial(x, y, w)}=1$ provided $\frac{\partial(x, y, z)}{\partial(u, v, w)} \neq 0$. (b) Give an interpretation of the result of (a) in terms of transformations. 6.93. If $f(x, y, z)=0$ and $g(x, y, z)=0$, show that $\frac{d x}{\frac{\partial(f, g)}{\partial(y, z)}}=\frac{d y}{\frac{\partial(f, g)}{\partial(z, x)}}=\frac{d z}{\frac{\partial(f, g)}{\partial(x, y)}}$ giving conditions under which\\
the result is valid.

$$
\frac{\partial(J, z)}{\partial(y, z)} \quad \frac{\partial(J, z)}{\partial(z, x)} \quad \frac{\partial(y, z)}{\partial(x, y)}
$$

6.94. If $x+y^{2}=u, y+z^{2}=v, z+x^{2}=w$, find (a) $\frac{\partial x}{\partial u}$, (b) $\frac{\partial^{2} x}{\partial u^{2}}$, (c) $\frac{\partial^{2} x}{\partial u \partial v}$, assuming that the equations define $x$, $y$, and $z$ as twice differentiable functions of $u$, $v$, and $w$.\\
Ans. (a) $\frac{1}{1+8 x y z}$\\
(b) $\frac{16 x^{2} y-8 y z-32 x^{2} z^{2}}{(1+8 x y z)^{3}}$\\
(c) $\frac{16 y^{2} z-8 x z-32 x^{2} y^{2}}{(1+8 x y z)^{3}}$

6.95. State and prove a theorem similar to that in Problem 6.35, for the case where $u=f(x, y, z), v=g(x, y, z)$, $w=h(x, y, z)$.

\section*{Transformations, curvilinear coordinates}
6.96. Given the transformation $x=2 u+v, y=u-3 v$, (a) sketch the region $\mathfrak{R}^{\prime}$ of the $u v$ plane into which the region $\Re$ of the $x y$ plane bounded by $x=0, x=1, y=0, y=1$ is mapped under the transformation; (b) compute $\frac{\partial(x, y)}{\partial(u, v)}$; and (c) compare the result of (b) with the ratios of the areas of $\Re$ and $\Re^{\prime}$.

Ans. (b) -7

6.97. (a) Prove that under a linear transformation $x=a_{1} u+a_{2} v, y=b_{1} u+b_{2} v\left(a_{1} b_{2}-a_{2} b_{1} \neq 0\right)$ lines and circles in the $x y$ plane are mapped, respectively, into lines and circles in the $u v$ plane. (b) Compute the Jacobian $J$ of the transformation and discuss the significance of $J=0$.

6.98. Given $x=\cos u \cosh v, y=\sin u \sinh v$, (a) show that, in general, the coordinate curves $u=a$ and $v=b$ in the $u v$ plane are mapped into hyperbolas and ellipses, respectively, in the $x y$ plane; (b) compute $\left|\frac{\partial(x, y)}{\partial(u, v)}\right|$; (c) compute $\left|\frac{\partial(u, v)}{\partial(x, y)}\right|$.

Ans. (b) $\sin ^{2} u \cosh ^{2} v+\cos ^{2} u \sinh ^{2} v$ (c) $\left(\sin ^{2} u \cosh ^{2} v+\cos ^{2} u \sinh ^{2} v\right)^{-1}$

6.99. Given the transformation $x=2 u+3 v-w, y=2 v+w, z=2 u-2 v+w$, (a) sketch the region $\Re^{\prime}$ of the $u v w$ space into which the region $\Re$ of the $x y z$ space bounded by $x=0, x=8, y=0, y=4, z=0, z=6$ is mapped; (b) compute $\frac{\partial(x, y, z)}{\partial(u, v, w)}$; (c) compare the result of (b) with the ratios of the volumes of $\Re$ and $\Re^{\prime}$. Ans. (b) 1

6.100. Given the spherical coordinate transformation $x=r \sin \theta \cos \phi, y=r \sin \theta \sin \phi, z=r \cos \theta$, where $r \delta 0.0$ $\leq \theta \leq \pi, 0 \leq \phi<2 \pi$, describe the coordinate surfaces (a) $r=a$, (b) $\theta=b$, and (c) $\phi=c$, where $a, b, c$ are any constants.

Ans. (a) spheres (b) cones (c) planes

6.101. (a) Verify that for the spherical coordinate transformation of Problem $6.100, J=\frac{\partial(x, y, z)}{\partial(r, \theta, \phi)}=r^{2} \sin \theta$.\\
(b) Discuss the case where $J=0$.

\section*{Miscellaneous problems}
6.102. If $F(P, V, T)=0$, prove that (a) $\left.\left.\frac{\partial P}{\partial T}\right|_{V} \frac{\partial T}{\partial V}\right|_{p}=-\left.\frac{\partial P}{\partial V}\right|_{T}$ (b) $\left.\left.\left.\frac{\partial P}{\partial T}\right|_{V} \frac{\partial T}{\partial V}\right|_{p} \frac{\partial V}{\partial P}\right|_{T}=-1$.

These results are useful in thermodynamics, where $P, V$, and $T$ correspond to pressure, volume, and temperature of a physical system.

6.103. Show that $F(x / y, z / y)=0$ satisfies $x(\partial z / \partial x)+y(\partial z / \partial y)=z$.

6.104. Show that $F\left(x+y-z, x^{2}+y^{2}\right)=0$ satisfies $x(\partial z / \partial y)-y(\partial z / \partial x)=x-y$.

6.105. If $x=f(u, v)$ and $y=g(u, v)$, prove that $\frac{\partial v}{\partial x}=-\frac{1}{J} \frac{\partial y}{\partial u} \quad$ where $\quad J=\frac{\partial(\mathrm{x}, \mathrm{y})}{\partial(\mathrm{u}, \mathrm{v})}$.

6.106. If $x=f(u, v), y=g(u, v), z=h(u, v)$ and $F(x, y, z)=0$, prove that $\frac{\partial(y, z)}{\partial(u, v)} d x+\frac{\partial(z, x)}{\partial(u, v)} d y+\frac{\partial(x, y)}{\partial(u, v)} d z=0$

6.107. If $x=\phi(u, v, w), y=\psi(u, v, w), u=f(r, s), v=g(r, s)$, and $w=h(r, s)$, prove that

$$
\frac{\partial(x, y)}{\partial(r, s)}=\frac{\partial(x, y)}{\partial(u, v)} \frac{\partial(u, v)}{\partial(r, s)}+\frac{\partial(x, y)}{\partial(v, w)}=\frac{\partial(v, w)}{\partial(r, s)}+\frac{\partial(x, y)}{\partial(w, u)}+\frac{\partial(w, u)}{\partial(r, s)}
$$

6.108. (a) Prove that $\left|\begin{array}{ll}a & b \\ c & d\end{array}\right| \cdot\left|\begin{array}{ll}e & f \\ g & h\end{array}\right|=\left|\begin{array}{ll}a e+b g & a f+b h \\ c e+d g & c f+d h\end{array}\right|$, thus establishing the rule for the product of two secondorder determinants referred to in Problem 6.43. (b) Generalize the result of (a) to determinants of $3,4 \ldots$

6.109. If $x, y$, and $z$ are functions of $u$, $v$, and $w$, while $u$, $v$, and $w$ are functions of $r, s$, and $t$, prove that $\frac{\partial(x, y, z)}{\partial(r, s, t)}=\frac{\partial(x, y, z)}{\partial(u, v, w)} \cdot \frac{\partial(u, v, w)}{\partial(r, s, t)}$.

6.110. Given the equation $F_{j}\left(x_{1}, \ldots, x_{m}, y_{1}, \ldots, y_{n}\right)=0$ where $j=1,2, \ldots, n$, prove that, under suitable conditions on $F_{j}, \frac{\partial y_{r}}{\partial x_{s}}=\frac{\partial\left(F_{1}, F_{2}, \ldots, F_{r}, \ldots, F_{n}\right.}{\partial\left(y_{1}, y_{2}, \ldots, x_{s}, \ldots, y_{n}\right.} / \frac{\partial\left(F_{1}, F_{2}, \ldots, F_{n}\right.}{\partial\left(y_{1}, y_{2}, \ldots, y_{n}\right.}$

6.111. (a) If $F(x, y)$ is homogeneous of degree 2, prove that $x^{2} \frac{\partial^{2} F}{\partial x^{2}}+2 x y \frac{\partial^{2} F}{\partial x \partial y}+y^{2} \frac{\partial^{2} F}{\partial y^{2}}=2 F$.

(b) Illustrate by using the special case $F(x, y)=x^{2} \ln (y / x)$.

Note that the result can be written in operator form, using $D_{x} \equiv \partial / \partial x$ and $D_{y} \equiv \partial / \partial y$, as $\left(x D_{x}+y D_{y}\right)^{2}$ $F=2 F$. [Hint: Different Differentiate both sides of Problem 6.25, Equation (1), twice with respect to $\lambda$.]

6.112. Generalize the result of Problem 6.11 as follows. If $F\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is homogeneous of degree $p$, then for any positive integer $r$, if $D_{x j} \equiv \partial / \partial x_{j},\left(x_{1} D_{x 1}+x_{2} D_{x 2}+\cdots+x_{n} D_{x n}\right)^{r} F=p(p-1) \ldots(p-r+1) F$.

6.113. (a) Let $x$ and $y$ be determined from $u$ and $v$ according to $x+i y=(u+i v)^{3}$. Prove that under this transformation the equation $\frac{\partial^{2} \phi}{\partial x^{2}}+\frac{\partial^{2} \phi}{\partial y^{2}}=0 \quad$ is transformed into $\quad \frac{\partial^{2} \phi}{\partial u^{2}}+\frac{\partial^{2} \phi}{\partial v^{2}}=0$.

(b) Is the result in (a) true if $x+i y=F(u+i v)$ ? Prove your statements.

This page intentionally left blank

\section*{CHAPTER 7}
\section*{Vectors}
\section*{Vectors}
The foundational ideas for vector analysis were formed independently in the nineteenth century by William Rowan Hamilton and Herman Grassmann. We are indebted to the physicist John Willard Gibbs, who formulated the classical presentation of the Hamilton viewpoint in his Yale lectures, and his student E. B. Wilson, who considered the mathematical material presented in class worthy of organizing as a book (published in 1901). Hamilton was searching for a mathematical language appropriate to a comprehensive exposition of the physical knowledge of the day. His geometric presentation emphasizing magnitude and direction and compact notation for the entities of the calculus was refined in the following years to the benefit of expressing Newtonian mechanics, electromagnetic theory, and so on. Grassmann developed an algebraic and more philosophic mathematical structure which was not appreciated until it was needed for Riemanian (nonEuclidean) geometry and the special and general theories of relativity.

Our introduction to vectors is geometric. We conceive of a vector as a directed line segment $\overrightarrow{P Q}$ from one point $P$, called the initial point, to another point $Q$, called the terminal point. We denote vectors by boldfaced letters or letters with an arrow over them. Thus, $\overrightarrow{P Q}$ is denoted by $\mathbf{A}$ or $\vec{A}$, as in Figure 7.1. The magnitude or length of the vector is then denoted by $|\overrightarrow{P Q}|, \overline{P Q},|\mathbf{A}|$ or $|\vec{A}|$.

Vectors are defined to satisfy the geometric properties discussed in the next section.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-172}
\end{center}

Figure 7.1

\section*{Geometric Properties of Vectors}
\begin{enumerate}
  \item Two vectors $\mathbf{A}$ and $\mathbf{B}$ are equal if they have the same magnitude and direction regardless of their initial points. Thus, $\mathbf{A}=\mathbf{B}$ in Figure 7.1.
\end{enumerate}

In other words, a vector is geometrically represented by any one of a class of commonly directed line segments of equal magnitude. Since any one of the class of line segments may be chosen to represent it, the vector is said to be free. In certain circumstances (tangent vectors, forces bound to a point), the initial point is fixed; then the vector is bound. Unless specifically stated, the vectors in this discussion are free vectors.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item A vector having direction opposite to that of vector $\mathbf{A}$ but with the same magnitude is denoted by $-\mathbf{A}$ (see Figure 7.2).

  \item The sum or resultant of vectors $\mathbf{A}$ and $\mathbf{B}$ of Figure 7.3(a) is a vector

\end{enumerate}

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-172(1)}
\end{center}

Figure 7.2\\
$\mathbf{C}$ formed by placing the initial point of $\mathbf{B}$ on the terminal point of $\mathbf{A}$ and joining the initial point of $\mathbf{A}$ to the terminal point of $\mathbf{B}$ [see Figure 7.3(b)]. The sum $\mathbf{C}$ is written $\mathbf{C}=\mathbf{A}+\mathbf{B}$. The definition here is equivalent to the parallelogram law for vector addition, as indicated in Figure 7.3(c).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-173(3)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-173}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-173(1)}
\end{center}

(c)

Figure 7.3

Extensions to sums of more than two vectors are immediate. For example, Figure 7.4 shows how to obtain the sum or resultant $\mathbf{E}$ of the vectors $\mathbf{A}, \mathbf{B}, \mathbf{C}$, and $\mathbf{D}$.\\
\includegraphics[max width=\textwidth, center]{2024_04_03_ffb6ac533fe0a53b3ceeg-173(2)}

Figure 7.4

\begin{enumerate}
  \setcounter{enumi}{3}
  \item The difference of vectors $\mathbf{A}$ and $\mathbf{B}$, represented by $\mathbf{A}-\mathbf{B}$, is that vector $\mathbf{C}$ which added to $\mathbf{B}$ gives $\mathbf{A}$. Equivalently, $\mathbf{A}-\mathbf{B}$ may be defined as $\mathbf{A}+(-\mathbf{B})$. If $\mathbf{A}=\mathbf{B}$, then $\mathbf{A}-\mathbf{B}$ is defined as the null or zero vector and is represented by the symbol $\mathbf{0}$. This has a magnitude of zero, but its direction is not defined.
\end{enumerate}

The expression of vector equations and related concepts is facilitated by the use of real numbers and functions. In this context, these are called scalars. This special designation arises from application where the scalars represent objects that do not have direction, such as mass, length, and temperature.

\begin{enumerate}
  \setcounter{enumi}{4}
  \item Multiplication of a vector $\mathbf{A}$ by a scalar $m$ produces a vector $m \mathbf{A}$ with magnitude $|m|$ times the magnitude of $\mathbf{A}$ and direction the same as or opposite to that of $\mathbf{A}$ according as $m$ is positive or negative. If $m=0, m \mathbf{A}=0$, the null vector.
\end{enumerate}

\section*{Algebraic Properties of Vectors}
The following algebraic properties are consequences of the geometric definition of a vector. (See Problems 7.1 and 7.2.)

If $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ are vectors, and $m$ and $n$ are scalars, then

\begin{enumerate}
  \item $\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$

  \item $\mathbf{A}+(\mathbf{B}+\mathbf{C})=(\mathbf{A}+\mathbf{B})+\mathbf{C}$

  \item $m(n \mathbf{A})=(m n) \mathbf{A}=n(m \mathbf{A})$

  \item $(m+n) \mathbf{A}=m \mathbf{A}+n \mathbf{A}$

  \item $m(\mathbf{A}+\mathbf{B})=m \mathbf{A}+m \mathbf{B}$\\
Commutative law for addition

\end{enumerate}

Associative law for addition

Associative law for multiplication

Distributive law

Distributive law

Note that in these laws only multiplication of a vector by one or more scalars is defined. On Pages 164 and 165 we define products of vectors.

\section*{Linear Independence and Linear Dependence of a Set of Vectors}
That a set of vectors $\mathbf{A}_{1}, \mathbf{A}_{2}, \ldots, \mathbf{A}_{p}$ is linearly independent means that $a_{1} \mathbf{A}_{1}+a_{2} \mathbf{A}_{2}+\cdots a_{p} \mathbf{A}_{p}=\mathbf{0}$ if and only if $a_{1}=a_{2}=\ldots=a_{p}=0$ (i.e., the algebraic sum is zero if and only if all the coefficients are zero). The set of vectors is linearly dependent when it is not linearly independent.

\section*{Unit Vectors}
Unit vectors are vectors having unit length. If $\mathbf{A}$ is any vector with magnitude $\mathbf{A}=|\mathbf{A}|>0$, then $\mathbf{A} /|\mathbf{A}|$ is a unit vector. If $\mathbf{a}$ is a unit vector with the same direction and sense as $\mathbf{A}$, then $\mathbf{a}=\mathrm{A} /|\mathbf{A}|$.

\section*{Rectangular (Orthogonal) Unit Vectors}
The rectangular unit vectors $\mathbf{i}, \mathbf{j}$, and $\mathbf{k}$ are unit vectors having the direction of the positive $x, y$, and $z$ axes of a rectangular coordinate system (see Figure 7.5). The triple i, $\mathbf{j}, \mathbf{k}$ is said to be a basis of the collection of vectors. We use right-handed rectangular coordinate systems unless otherwise specified. Such systems derive their name from the fact that a right-threaded screw rotated through $90^{\circ}$ from $O x$ to $O y$ will advance in the positive $z$ direction. In general, three vectors $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ which have coincident initial points and are not coplanar are said to form a right-handed system or dextral system if a right-threaded screw rotated through an angle less than $180^{\circ}$ from $\mathbf{A}$ to $\mathbf{B}$ will advance in the direction $\mathbf{C}$ (see Figure 7.6).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-174(1)}
\end{center}

Figure 7.5

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-174}
\end{center}

Figure 7.6

\section*{Components of a Vector}
Any vector $\mathbf{A}$ in three dimensions can be represented with initial point at the origin $O$ of a rectangular coordinate system (see Figure 7.7). Let $\left(A_{1}, A_{2}, A_{3}\right)$ be the rectangular coordinates of the terminal point of vector A with initial point at $O$. The vectors $A_{1} \mathbf{i}, A_{2} \mathbf{j}$, and $A_{3} \mathbf{k}$ are called the rectangular component vectors, or simply component vectors, of $\mathbf{A}$ in the $x, y$, and $z$ directions, respectively. $A_{1}, A_{2}$, and $A_{3}$ are called the rectangular components, or simply components, of $\mathbf{A}$ in the $x, y$, and $z$ directions, respectively.

The vectors of the set $\{\mathbf{i}, \mathbf{j}, \mathbf{k}\}$ are perpendicular to one another, and they are unit vectors. The words orthogonal and normal, respectively, are used to describe these characteristics; hence, the set is what is called an orthonormal basis.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-175}
\end{center}

Figure 7.7

It is easily shown to be linearly independent. In an $n$-dimensional space, any set of $n$ linearly independent vectors is a basis. The further characteristic of a basis is that any vector of the space can be expressed through it. It is the basis representation that provides the link between the geometric and the algebraic expressions of vectors and vector concepts.

The sum or resultant of $A_{1} \mathbf{i}, A_{2} \mathbf{j}$, and $A_{3} \mathbf{k}$ is the vector $\mathbf{A}$, so that we can write


\begin{equation*}
\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k} \tag{1}
\end{equation*}


The magnitude of $\mathbf{A}$ is


\begin{equation*}
A=|\mathbf{A}|=\sqrt{A_{1}^{2}+A_{2}^{2}+A_{3}^{2}} \tag{2}
\end{equation*}


In particular, the position vector or radius vector $\mathbf{r}$ from $O$ to the point $(x, y, z$ is written


\begin{equation*}
\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k} \tag{3}
\end{equation*}


and has magnitude $\mathrm{r}=|\mathbf{r}|=\sqrt{x^{2}+y^{2}+z^{2}}$.

A theory of vectors would be of limited use without a process of multiplication. In fact, two binary processes, designated as dot product and cross product, were created to meet the geometric and physical needs to which vectors were applied.

The first of them, the dot product, was generated from consideration of the angle between two vectors.

\section*{Dot, Scalar, or Inner Product}
The dot or scalar product of two vectors $\mathbf{A}$ and $\mathbf{B}$, denoted by $\mathbf{A} \cdot \mathbf{B}$ (read: $\mathbf{A}$ dot $\mathbf{B}$ ) is defined as the product of the magnitudes of $\mathbf{A}$ and $\mathbf{B}$ and the cosine of the angle between them. In symbols,


\begin{equation*}
\mathbf{A} \cdot \mathbf{B}=A B \cos \theta, \quad 0 \leqq \theta \leqq \pi \tag{4}
\end{equation*}


Assuming that neither $\mathbf{A}$ nor $\mathbf{B}$ is the zero vector, an immediate consequence of the definition is that $\mathbf{A} \cdot$ $\mathbf{B}=0$ if and only if $\mathbf{A}$ and $\mathbf{B}$ are perpendicular. Note that $\mathbf{A} \cdot \mathbf{B}$ is a scalar and not a vector.

The following laws are valid:

\begin{enumerate}
  \item $\mathbf{A} \cdot \mathbf{B}=\mathbf{B} \cdot \mathbf{A} \quad$ Commutative law for dot products

  \item $\mathbf{A} \cdot(\mathbf{B}+\mathbf{C})=\mathbf{A} \cdot \mathbf{B}+\mathbf{A} \cdot \mathbf{C}$ Distributive Law

  \item $m(\mathbf{A} \cdot \mathbf{B})=(m \mathbf{A}) \cdot \mathbf{B}=\mathbf{A} \cdot(m \mathbf{B})=(\mathbf{A} \cdot \mathbf{B}) m$, where $m$ is a scalar

  \item $\quad \mathbf{i} \cdot \mathbf{i}=\mathbf{j} \cdot \mathbf{j}=\mathbf{k} \cdot \mathbf{k}=1, \mathbf{i} \cdot \mathbf{j}=\mathbf{j} \cdot \mathbf{k}=\mathbf{k} \cdot \mathbf{i}=0$

  \item If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$ and $\mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$, then $\mathbf{A} \cdot \mathbf{B}=A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}$

\end{enumerate}

The equivalence of this component form the dot product with the geometric definition 4 following from the law of cosines. (See Figure 7.8).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-176}
\end{center}

Figure 7.8

In particular,

$$
|\mathbf{C}|^{2}=|\mathbf{A}|^{2}+|\mathbf{B}|^{2}-2|\mathbf{A}||\mathbf{B}| \cos \theta
$$

Since $\mathbf{C}=\mathbf{B}-\mathbf{A}$, its components are $\mathrm{B}_{1}-\mathrm{A}_{1}, \mathrm{~B}_{2}-\mathrm{A}_{2}, \mathrm{~B}_{3}-\mathrm{A}_{3}$ and the square of its magnitude is

$$
|\mathbf{B}|^{2}+|\mathbf{A}|^{2}-2\left(\mathrm{~A}_{1} \mathrm{~B}_{1}+\mathrm{A}_{2} \mathrm{~B}_{2}+\mathrm{A}_{3} \mathrm{~B}_{3}\right.
$$

When this representation for $\left|\mathbf{C}^{2}\right|$ is placed in the original equation and cancellations are made, we obtain

$$
\mathrm{A}_{1} \mathrm{~B}_{1}+\mathrm{A}_{2} \mathrm{~B}_{2}+\mathrm{A}_{3} \mathrm{~B}_{3}=|\mathbf{A}||\mathbf{B}| \cos \theta .
$$

The second form of vector multiplication-that is, the cross product-emerged from Hamilton's theory of quaternions (1844). Algebraically, the cross product is an example of a noncommutative operation. Geometrically, it generates a vector perpendicular to the initial pair of vectors, and its physical value is illustrated in electromagnetic theory, where it aids in the representation of a magnetic field perpendicular to the direction of an electric current.

\section*{Cross or Vector Product}
The cross or vector product of $\mathbf{A}$ and $\mathbf{B}$ is a vector $\mathbf{C}=\mathbf{A} \times \mathbf{B}$ (read: $\mathbf{A}$ cross $\mathbf{B}$ ). The magnitude of $\mathbf{A} \times \mathbf{B}$ is defined as the product of the magnitudes of $\mathbf{A}$ and $\mathbf{B}$ and the sine of the angle between them. The direction of the vector $\mathbf{C}=\mathbf{A} \times \mathbf{B}$ is perpendicular to the plane of $\mathbf{A}$ and $\mathbf{B}$, and such that $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ form a righthanded system. In symbols,


\begin{equation*}
\mathbf{A} \times \mathbf{B}=A B \sin \theta \mathbf{u}, 0 \leqq \theta \leqq \pi \tag{5}
\end{equation*}


where $\mathbf{u}$ is a unit vector indicating the direction of $\mathbf{A} \times \mathbf{B}$. If $\mathbf{A}=\mathbf{B}$ or if $\mathbf{A}$ is parallel to $\mathbf{B}$, then $\sin \theta=0$ and $\mathbf{A} \times \mathbf{B}=0$.

The following laws are valid:

\begin{enumerate}
  \item $\mathbf{A} \times \mathbf{B}=-\mathbf{B} \times \mathbf{A}$
\end{enumerate}

(Commutative law for cross products fails)

\begin{enumerate}
  \setcounter{enumi}{1}
  \item $\mathbf{A} \times(\mathbf{B}+\mathbf{C})=\mathbf{A} \times \mathbf{B}+\mathbf{A} \times \mathbf{C}$
\end{enumerate}

Distributive Law

\begin{enumerate}
  \setcounter{enumi}{2}
  \item $m(\mathbf{A} \times \mathbf{B})=(m \mathbf{A}) \times \mathbf{B}=\mathbf{A} \times(m \mathbf{B})=(\mathbf{A} \times \mathbf{B}) m$, where $m$ is a scalar
\end{enumerate}

Also, the following consequences of the definition are important:

\begin{enumerate}
  \setcounter{enumi}{3}
  \item $\mathbf{i} \times \mathbf{i}=\mathbf{j} \times \mathbf{j}=\mathbf{k} \times \mathbf{k}=\mathbf{0}, \mathbf{i} \times \mathbf{j}=\mathbf{k}, \mathbf{j} \times \mathbf{k}=\mathbf{i}, \mathbf{k} \times \mathbf{i}=\mathbf{j}$

  \item If $\mathbf{A}=A_{1} \mathbf{i}+\mathbf{A}_{2} \mathbf{j}+\mathbf{A}_{3} \mathbf{k}$ and $\mathbf{B}=B_{1} \mathbf{i}+B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$, then

\end{enumerate}

$$
\mathrm{A} \times \mathrm{B}=\left|\begin{array}{ccc}
\mathrm{i} & \mathrm{j} & \mathrm{k} \\
A_{1} & A_{2} & A_{3} \\
B_{1} & B_{2} & B_{3}
\end{array}\right|
$$

The equivalence of this component representation (5) and the geometric definition may be seen as follows. Choose a coodinate system such that the direction of the $x$-axis is that of $\mathbf{A}$ and the $x y$ plane is the plane of the vectors $\mathbf{A}$ and B. (See Figure 7.9.) Then

$$
\mathbf{A} \times \mathbf{B}=\left|\begin{array}{ccc}
\mathrm{i} & \mathrm{j} & \mathrm{k} \\
A_{1} & 0 & 0 \\
B_{1} & B_{2} & 0
\end{array}\right|=A_{1} B_{2} \mathrm{k}=|\mathbf{A}||\mathbf{B}| \operatorname{sine} \theta k
$$

Since this choice of coordinate system places no restrictions on the vectors $\mathbf{A}$ and $\mathbf{B}$, the result is general and thus establishes the equivalence.\\
6. $|\mathbf{A} \times \mathbf{B}|=$ the area of a parallelogram with sides $\mathbf{A}$ and $\mathbf{B}$.

\begin{enumerate}
  \setcounter{enumi}{6}
  \item If $\mathbf{A} \times \mathbf{B}=0$ and neither $\mathbf{A}$ nor $\mathbf{B}$ is a null vector, then $\mathbf{A}$ and $\mathbf{B}$
\end{enumerate}

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-177}
\end{center}

Figure 7.9 are parallel.

\section*{Triple Products}
Dot and cross multiplication of three vectors, $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ may produce meaningful products of the form $(\mathbf{A} \cdot \mathbf{B}) \mathbf{C}, \mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})$, and $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})$. The following laws are valid:

\begin{enumerate}
  \item $\mathbf{( A} \cdot \mathbf{B}) \mathbf{C} \neq \mathbf{A}(\mathbf{B} \cdot \mathbf{C})$ in general

  \item $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=\mathbf{B} \cdot(\mathbf{C} \times \mathbf{A})=\mathbf{C} \cdot(\mathbf{A} \times \mathbf{B})=$ volume of a parallelepiped having $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ as edges, or the negative of this volume according as $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ do or do not form a right-handed handed system. If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}, \mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$ and $\mathbf{C}=C_{1} \mathbf{i}+C_{2} \mathbf{j}+C_{3} \mathbf{k}$, then

\end{enumerate}

\[
\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=\left|\begin{array}{lll}
A_{1} & A_{2} & A_{3}  \tag{6}\\
B_{1} & B_{2} & B_{3} \\
C_{1} & C_{2} & C_{3}
\end{array}\right|
\]

\begin{enumerate}
  \setcounter{enumi}{2}
  \item $\mathbf{A} \times(\mathbf{B} \times \mathbf{C}) \neq(\mathbf{A} \times \mathbf{B}) \times \mathbf{C}$
\end{enumerate}

(Associative law for cross products fails)

\begin{enumerate}
  \setcounter{enumi}{3}
  \item $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})=(\mathbf{A} \cdot \mathbf{C}) \mathbf{B}-(\mathbf{A} \cdot \mathbf{B}) \mathbf{C}$
\end{enumerate}

$$
(\mathbf{A} \times \mathbf{B}) \times \mathbf{C}=(\mathbf{A} \cdot \mathbf{C}) \mathbf{B}-(\mathbf{B} \cdot \mathbf{C}) \mathbf{A}
$$

The product $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})$ is called the scalar triple product or box product and may be denoted by $[\mathbf{A B C}]$. The product $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})$ is called the vector triple product.

In $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})$ parentheses are sometimes omitted and we write $\mathbf{A} \cdot \mathbf{B} \times \mathbf{C}$. However, parentheses must be used in $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})$ (see Problem 7.29). Note that $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=(\mathbf{A} \times \mathbf{B}) \cdot \mathbf{C}$. This is often expressed by stating that in a scalar triple product the dot and the cross can be interchanged without affecting the result (see Problem 7.26).

\section*{Axiomatic Approach to Vector Analysis}
From the preceding remarks it is seen that a vector $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$ is determined when its three components $(x, y, z)$ relative to some coordinate system are known. In adopting an axiomatic approach, it is thus quite natural for us to make the following assumptions.

Definition A three-dimensional vector is an ordered triplet of real numbers with the following properties. If $\mathbf{A}=\left(A_{1}, A_{2}, A_{3}\right)$ and $\mathbf{B}=\left(B_{1}, B_{2}, B_{3}\right)$, then

\begin{enumerate}
  \item $\mathbf{A}=\mathbf{B}$ if and only if $A_{1}=B_{1}, A_{2}=B_{2}, A_{3}=B_{3}$

  \item $\mathbf{A}+\mathbf{B}=\left(A_{1}+B_{1}, A_{2}+B_{2}, A_{3}+B_{3}\right)$

  \item $\mathbf{A}-\mathbf{B}=\left(A_{1}-B_{1}, A_{2}-B_{2}, A_{3}-B_{3}\right)$

  \item $\mathbf{0}=(0,0,0)$

  \item $\quad m \mathbf{A}=m\left(A_{1}, A_{2}, A_{3}\right)=\left(m A_{1}, m A_{2}, m A_{3}\right)$

\end{enumerate}

In addition, two forms of multiplication are established.

\begin{enumerate}
  \setcounter{enumi}{5}
  \item $\mathbf{A} \cdot \mathbf{B}=A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}$

  \item Length or magnitude of $\mathrm{A}=|\mathrm{A}|=\sqrt{\mathrm{A} \cdot \mathrm{A}}=\sqrt{A_{1}^{2}+A_{2}^{2}+A_{3}^{2}}$

  \item $\mathrm{A} \times \mathrm{B}=\left(A_{2} B_{3}-A_{3} B_{2}, A_{3} B_{1}-A_{1} B_{3}, A_{1} B_{3}, A_{1} B_{2}-A_{2} B_{1}\right)$

\end{enumerate}

Unit vectors are defined to be $(1,0,0),(0,1,0),(0,0,1)$ and then designated by $\mathbf{i}, \mathbf{j}, \mathbf{k}$, respectively, thereby identifying the components axiomatically introduced with the geometric orthonormal basis elements.

If one wishes, this axiomatic formulation (which provides a component representation for vectors) can be used to reestablish the fundamental laws previously introduced geometrically; however, the primary reason for introducing this approach was to formalize a component representation of the vectors. It is that concept that will be used in the remainder of this chapter.

Note 1 One of the advantages of component representation of vectors is the easy extension of the ideas to all dimensions. In an $n$-dimensional space, the component representation is

$$
\mathbf{A}\left(A_{1}, A_{2}, \ldots, A_{n}\right)
$$

An exception is the cross product which is specifically restricted to three-dimensional space. There are generalizations of the cross product to higher dimensional spaces, but there is no direct extension.)

Note 2 The geometric interpretation of a vector endows it with an absolute meaning at any point of space. The component representation (as an ordered triple of numbers) in Euclidean three space is not unique; rather, it is attached to the coordinate system employed. This follows because the components are geometrically interpreted as the projections of the arrow representation on the coordinate directions. Therefore, the projections on the axes of a second coordinate system (rotated, for example) from the first one will be different. (See Figure 7.10.) Therefore, for theories where groups of coordinate systems play a role, a more adequate component definition of a vector is as a collection of ordered triples of numbers, each one identified with a coordinate system of the group, and any two related by a coordinate transformation. This viewpoint is indispensable in Newtonian mechanics, electromagnetic theory, special relativity, and so on.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-178}
\end{center}

Figure 7.10

\section*{Vector Functions}
If corresponding to each value of a scalar $u$ we associate a vector $\mathbf{A}$, then $\mathbf{A}$ is called a function of $u$ denoted by $\mathbf{A}(u)$. In three dimensions we can write $\mathbf{A}(u)=A_{1}(u) \mathbf{i}+A_{2}(u) \mathbf{j}+A_{3}(u) \mathbf{k}$.

The function concept is easily extended. Thus, if to each point $(x, y, z)$ there corresponds a vector $\mathbf{A}$, then $\mathbf{A}$ is a function of $(x, y, z)$, indicated by $\mathbf{A}(x, y, z)=A_{1}(x, y, z) \mathbf{i}+\boldsymbol{A}_{2}(x, y, z) \mathbf{j},+A_{3}(x, y, z) \mathbf{k}$.

We sometimes say that a vector function $\mathbf{A}$ defines a vector field since it associates a vector with each point of a region. Similarly, $\phi(x, y, z)$ defines a scalar field since it associates a scalar with each point of a region.

\section*{Limits, Continuity, and Derivatives of Vector Functions}
Limits, continuity, and derivatives of vector functions follow rules similar to those for scalar functions already considered. The following statements show the analogy which exists.

\begin{enumerate}
  \item The vector function represented by $\mathbf{A}(u)$ is said to be continuous at $u_{0}$ if, given any positive number $\delta$, we can find some positive number $\delta$ such that $\left|\mathbf{A}(u)-\mathbf{A}\left(u_{0}\right)\right|<\delta$ whenever $\left|u-u_{0}\right|<\delta$. This is equivalent to the statement $\lim _{u \rightarrow u_{0}} \mathbf{A}(u)=\mathbf{A}\left(u_{0}\right)$.

  \item The derivative of $\mathbf{A}(u)$ is defined as

\end{enumerate}

$$
\frac{d \mathrm{~A}}{d u}=\lim _{\Delta u \rightarrow 0} \frac{\mathrm{A}(u+\Delta u)-\mathrm{A}(u)}{\Delta u}
$$

provided this limit exists. In case $\mathbf{A}(u)=A_{1}(u) \mathbf{i}+\boldsymbol{A}_{2} A_{2}(u) \mathbf{j}+A_{3}(u) \mathbf{k}$; then

$$
\frac{d \mathrm{~A}}{d u}=\frac{d A_{1}}{d u} \mathbf{i}+\frac{d A_{2}}{d u} \mathbf{j}+\frac{d A_{3}}{d u} \mathbf{k}
$$

Higher derivatives such as $d^{2} \mathbf{A} / d u^{2}$, etc., can be similarly defined.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item If $\mathbf{A}(x, y, z)=A_{1}(x, y, z) \mathbf{i}+A_{2}(x, y, z) \mathbf{j}+A_{3}(x, y, z) \mathbf{k}$; then
\end{enumerate}

$$
d \mathbf{A}=\frac{\partial \mathbf{A}}{\partial x} d x+\frac{\partial \mathbf{A}}{\partial y} d y+\frac{\partial \mathbf{A}}{\partial z} d z
$$

is the differential of $\mathbf{A}$.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Derivatives of products obey rules similar to those for scalar functions. However, when cross products are involved, the order is important. Some examples are
\end{enumerate}

(a) $\frac{d}{d u}(\phi \mathbf{A})=\phi \frac{d \mathbf{A}}{d u}+\frac{d \phi}{d u} \mathbf{A}$.

(b) $\frac{\partial}{\partial y}(\mathbf{A} \cdot \mathbf{B})=\mathbf{A} \cdot \frac{\partial \mathbf{B}}{\partial y}+\frac{\partial \mathbf{A}}{\partial y} \cdot \mathbf{B}$

(c) $\frac{\partial}{\partial z}(\mathbf{A} \times \mathbf{B})=\mathbf{A} \times \frac{\partial \mathbf{B}}{\partial z}+\frac{\partial \mathbf{A}}{\partial z} \times \mathbf{B}$ (maintain the order of $\mathbf{A}$ and $\left.\mathbf{B}\right)$

\section*{Geometric Interpretation of a Vector Derivative}
If $\mathbf{r}$ is the vector joining the origin $O$ of a coordinate system and the point $(x, y, z)$, then specification of the vector function $\mathbf{r}(u)$ defines $x, y$, and $z$ as functions of $u$ ( $\mathbf{r}$ is called a position vector). As $u$ changes, the terminal point of $\mathbf{r}$ describes a space curve (see Figure 7.11) having parametric equations $x=x(u), y=y(u)$, $z=z(u)$. If the parameter $u$ is the are length $s$ measured from some fixed point on the curve, then recall from the discussion of arc length that $d s^{2}=d \mathbf{r} \cdot d \mathbf{r}$. Thus,


\begin{equation*}
\frac{d \mathbf{r}}{d s}=\mathbf{T} \tag{7}
\end{equation*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-180}
\end{center}

Figure 7.11

is a unit vector in the direction of the tangent to the curve and is called the unit tangent vector. If $u$ is the time $t$, then


\begin{equation*}
\frac{d \mathbf{r}}{d t}=\mathbf{v} \tag{8}
\end{equation*}


is the velocity with which the terminal point of $\mathbf{r}$ describes the curve. We have


\begin{equation*}
\mathbf{v}=\frac{d \mathbf{r}}{d t}=\frac{d \mathbf{r}}{d s} \frac{d s}{d t}=\frac{d s}{d t} \mathbf{T}=v \mathbf{T} \tag{9}
\end{equation*}


from which we see that the magnitude of $\mathbf{v}$ is $v=d s / d t$. Similarly,


\begin{equation*}
\frac{d^{2} \mathbf{r}}{d t^{2}}=\mathbf{a} \tag{10}
\end{equation*}


is the acceleration with which the terminal point of $\mathbf{r}$ describes the curve. These concepts have important applications in mechanics and differential geometry.

A primary objective of vector calculus is to express concepts in an intuitive and compact form. Success is nowhere more apparent than in applications involving the partial differentiation of scalar and vector fields. [Illustrations of such fields include implicit surface representation $\Phi\{x, y, z(x, y)=0$, the electromagnetic potential function $\Phi(x, y, z)$, and the electromagnetic vector field $\mathbf{F}(x, y, z)$.] To give mathematics the capability of addressing theories involving such functions, William Rowan Hamilton and others of the nineteenth century introduced derivative concepts called gradient, divergence, and curl, and then developed an analytic structure around them.

An intuitive understanding of these entities begins with examination of the differential of a scalar field, i.e.,

$$
d \Phi=\frac{\partial \Phi}{\partial x} d x+\frac{\partial \Phi}{d y} d y+\frac{\partial \Phi}{\partial z} d z
$$

Now suppose the function $\Phi$ is constant on a surface $S$ and that $C ; x=f_{1}(t), y=f_{2}(t), z=f_{3}(t)$ is a curve on $S$. At any point of this curve, $\frac{d \mathrm{r}}{d t}=\frac{d x}{d t} \mathbf{i}+\frac{d x}{d t} \mathbf{j}+\frac{d z}{d t} \mathbf{k}$ lies in the tangent plane to the surface. Since this statement is true for every surface curve through a given point, the differential $d \mathbf{r}$ spans the tangent plane. Thus, the triple $\frac{\partial \Phi}{\partial x}, \frac{\partial \Phi}{\partial y}, \frac{\partial \Phi}{\partial z}$ represents a vector perpendicular to $S$. With this special geometric characteristic in mind we define

$$
\nabla \Phi=\frac{\partial \Phi}{\partial x} \mathbf{i}+\frac{\partial \Phi}{\partial y} \mathbf{j}+\frac{\partial \Phi}{\partial z} \mathbf{k}
$$

to be the gradient of the scalar field $\Phi$.

Furthermore, we give the symbol $\nabla$ a special significance by naming it $d e l$.

EXAMPLE 1. $\mathrm{f} \Phi(x, y, z)=0$ is an implicitly defined surface, then, because the function always has the value zero for points on it, the condition of constancy is satisfied and $\nabla \phi$ is normal to the surface at any of its points.

This allows us to form an equation for the tangent plane to the surface at any one of its points. See Problem 7.36 .

EXAMPLE 2. For certain purposes, surfaces on which $\Phi$ is constant are called level surfaces. In meteorology, surfaces of equal temperature or of equal atmospheric pressure fall into this category. From the previous development, opment, we see that $\nabla \Phi$ is perpendicular to the level surface at any one of its points and, hence, has the direction of maximum change at that point.

The introduction of the vector operator $\nabla$ and the interaction of it with the multiplicative properties of dot and cross come to mind. Indeed, this line of thought does lead to new concepts called divergence and curl. A summary follows.

\section*{Gradient, Divergence, and Curl}
Consider the vector operator $\nabla$ (del) defined by


\begin{equation*}
\nabla \equiv \mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z} \tag{11}
\end{equation*}


Then if $\phi(x, y, z)$ and $\mathbf{A}(x, y, z)$ have continuous first partial derivatives in a region (a condition which is in many cases stronger than necessary), we can define the following.

\begin{enumerate}
  \item Gradient. The gradient of $\phi$ is defined by
\end{enumerate}


\begin{align*}
\operatorname{grad} \phi=\nabla \phi & =\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \phi=\mathbf{i} \frac{\partial \phi}{\partial x}+\mathbf{j} \frac{\partial \phi}{\partial y}+\mathbf{k} \frac{\partial \phi}{\partial z}  \tag{12}\\
& =\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}
\end{align*}


\begin{enumerate}
  \setcounter{enumi}{1}
  \item Divergence. The divergence of $\mathbf{A}$ is defined by
\end{enumerate}


\begin{align*}
\operatorname{div} \mathbf{A}=\nabla \cdot \mathbf{A} & =\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \cdot\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right)  \tag{13}\\
& =\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}
\end{align*}


\begin{enumerate}
  \setcounter{enumi}{2}
  \item Curl. The curl of $\mathbf{A}$ is defined by
\end{enumerate}

$$
\begin{aligned}
\operatorname{curl} \mathbf{A}=\nabla \times \mathbf{A} & =\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \times\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \\
& =\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
A_{1} & A_{2} & A_{3}
\end{array}\right| \\
& =\mathbf{i}\left|\begin{array}{ll}
\frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
A_{2} & A_{3}
\end{array}\right|-\mathbf{j}\left|\begin{array}{ll}
\frac{\partial}{\partial x} & \frac{\partial}{\partial z} \\
A_{1} & A_{2}
\end{array}\right|+\mathbf{k}\left|\begin{array}{ll}
\frac{\partial}{\partial x} & \frac{\partial}{\partial y} \\
A_{1} & A_{2}
\end{array}\right| \\
& =\left(\frac{\partial A_{3}}{\partial y}-\frac{\partial A_{2}}{\partial z}\right) \mathbf{i}+\left(\frac{\partial A_{1}}{\partial z}-\frac{\partial A_{3}}{\partial x}\right) \mathbf{j}+\left(\frac{\partial A_{2}}{\partial x}-\frac{\partial A_{1}}{\partial y}\right) \mathbf{k}
\end{aligned}
$$

Note that in the expansion of the determinant, the operators $\partial / \partial x, \partial / \partial y, \partial / \partial z$ must precede $A_{1}, A_{2}, A_{3}$. In other words, $\nabla$ is a vector operator, not a vector. When employing it, the laws of vector algebra either do not\\
apply or at the very least must be validated. In particular, $\nabla \times \mathbf{A}$ is a new vector obtained by the specified partial differentiation on $\mathbf{A}$, while $\mathbf{A} \times \nabla$ is an operator waiting to act upon a vector or a scalar.

\section*{Formulas Involving $\nabla$}
If the partial derivatives of $\mathbf{A}, \mathbf{B}, U$, and $V$ are assumed to exist, then

\begin{enumerate}
  \item $\nabla(U+V)=\nabla U+\nabla V$ or $\operatorname{grad}(U+V)=\operatorname{grad} u+\operatorname{grad} V$

  \item $\quad \nabla \cdot(\mathbf{A}+\mathbf{B})=\nabla \cdot \mathbf{A}+\nabla \cdot \mathbf{B}$ or $\operatorname{div}(\mathbf{A}+\mathbf{B})+\operatorname{div} \mathbf{A}+\operatorname{div} \mathbf{B}$

  \item $\nabla \times(\mathbf{A}+\mathbf{B})=\nabla \times \mathbf{A}+\nabla \times \mathbf{B}$ or $\operatorname{curl}(\mathbf{A}+\mathbf{B})=\operatorname{curl} \mathbf{A}+\operatorname{curl} \mathbf{B}$

  \item $\quad \nabla \cdot(U \mathbf{A})=(\nabla U) \cdot \mathbf{A}+U(\nabla \cdot \mathbf{A})$

  \item $\nabla \times(U \mathbf{A})=(\nabla U) \times \mathbf{A}+U(\nabla \times \mathbf{A})$

  \item $\quad \nabla \cdot(\mathbf{A} \times \mathbf{B})=\mathbf{B} \cdot(\nabla \times \mathbf{A})-\mathbf{A} \cdot(\nabla \times \mathbf{B})$

  \item $\nabla \times(\mathbf{A} \times \mathbf{B})=(\mathbf{B} \cdot \nabla) \mathbf{A}-\mathbf{B}(\nabla \cdot \mathbf{A})-(\mathbf{A} \cdot \nabla) \mathbf{B}+\mathbf{A}(\nabla \cdot \mathbf{B})$

  \item $\quad \nabla(\mathbf{A} \cdot \mathbf{B})=(\mathbf{B} \cdot \nabla) \mathbf{A}+(\mathbf{A} \cdot \nabla) \mathbf{B}+\mathbf{B} \times(\nabla \times \mathbf{A})+\mathbf{A} \times(\nabla \times \mathbf{B})$

  \item $\quad \nabla .(\nabla U) \equiv \nabla^{2} U \equiv \frac{\partial^{2} U}{\partial x^{2}}+\frac{\partial^{2} U}{\partial y^{2}}+\frac{\partial^{2} U}{\partial z^{2}}$ is called the Laplacian of $\mathrm{U}$. and $\nabla^{2} \equiv \frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}+\frac{\partial^{2}}{\partial z^{2}}$ is called the Lapacian operator.

  \item $\nabla \times(\nabla U)=0$. The curl of the gradient of $U$ is zero.

  \item $\nabla \cdot(\nabla \times \mathbf{A})=0$. The divergence of the curl of $\mathbf{A}$ is zero.

  \item $\nabla \times(\nabla \times \mathbf{A})=\nabla(\nabla \cdot \mathbf{A})-\nabla^{2} \mathbf{A}$

\end{enumerate}

\section*{Vector Interpretation of Jacobians and Orthogonal Curvilinear Coordinates}
The transformation equations


\begin{equation*}
x=f\left(u_{1}, u_{2}, u_{3}\right), \quad y=g\left(u_{1}, u_{2}, u_{3}\right), \quad z=h\left(u_{1}, u_{2}, u_{3}\right) \tag{15}
\end{equation*}


(where we assume that $f, g, h$ are continuous, have continuous partial derivatives, and have a single-valued inverse) establish a one-to-one correspondence between points in an $x y z$ and $u_{1} u_{2} u_{3}$ rectangular coordinate system. In vector notation, the transformation (15) can be written


\begin{equation*}
\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}=f\left(u_{1}, u_{2}, u_{3}\right) \mathbf{i}+g\left(u_{1}, u_{2}, u_{3}\right) \mathbf{j}+h\left(u_{1}, u_{2}, u_{3}\right) \mathbf{k} \tag{16}
\end{equation*}


A point $P$ in Figure 7.12 can then be defined not only by rectangular coordinates $(x, y, z)$ but by coordinates $\left(u_{1}, u_{2}, u_{3}\right)$ as well. We call $\left(u_{1}, u_{2}, u_{3}\right)$ the curvilinear coordinates of the point.

If $u_{2}$ and $u_{3}$ are constant, then as $u_{1}$ varies, $\mathbf{r}$ describes a curve which we call the $u_{1}$ coordinate curve. Similarly, we define the $u_{2}$ and $u_{3}$ coordinate curves through $P$.

From Equation (16), we have


\begin{equation*}
d \mathbf{r}=\frac{\partial \mathbf{r}}{\partial u_{1}} d u_{1}+\frac{\partial \mathbf{r}}{\partial u_{2}} d u_{2}+\frac{\partial \mathbf{r}}{\partial u_{3}} d u_{3} \tag{17}
\end{equation*}


The collection of vectors $\frac{\partial \mathbf{r}}{\partial u_{1}}, \frac{\partial \mathbf{r}}{\partial u_{2}}, \frac{\partial \mathbf{r}}{\partial u_{3}}$ is a basis for the vec-

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-182}
\end{center}

Figure 7.12 tor structure associated with the curvilinear system. If the cur-\\
vilinear system is orthogonal, then so is this set; however, in general, the vectors are not unit vectors. he differential form for are length may be written

$$
d s^{2}=g_{11}\left(d u_{1}\right)^{2}+g_{22}\left(d u_{2}\right)^{2}+g_{33}\left(d u_{3}\right)^{2}
$$

where

$$
g_{11}=\frac{\partial \mathbf{r}}{\partial u_{1}} \cdot \frac{\partial \mathbf{r}}{\partial u_{1}}, \quad g_{22}=\frac{\partial \mathbf{r}}{\partial u_{2}} \cdot \frac{\partial \mathbf{r}}{\partial u_{2}}, \quad g_{33}=\frac{\partial \mathbf{r}}{\partial u_{3}} \cdot \frac{\partial \mathbf{r}}{\partial u_{3}}
$$

The vector $\partial \mathbf{r} / \partial u_{1}$ is tangent to the $u_{1}$ coordinate curve at $P$. If $\mathbf{e}_{1}$ is a unit vector at $P$ in this direction, we can write $\partial \mathbf{r} / \partial u_{1}=h_{1} \mathbf{e}_{1}$ where $h_{1}=\left|\partial \mathbf{r} / \partial u_{1}\right|$. Similarly, we can write $\partial \mathbf{r} / \partial u_{2}=h_{2} \mathbf{e}_{2}$ and $\partial \mathbf{r} / \partial u_{3}=h_{3} \mathbf{e}_{3}$, where $h_{2}=\left|\partial \mathbf{r} / \partial u_{2}\right|$ and $h_{3}=\left|\partial \mathbf{r} / \partial u_{3}\right|$, respectively. Then Equation (17) can be written


\begin{equation*}
d \mathbf{r}=h_{1} d u_{1} \mathbf{e}_{1}+h_{2} d u_{2} \mathbf{e}_{2}+h_{3} d u_{3} \mathbf{e}_{3} \tag{18}
\end{equation*}


The quantities $h_{1}, h_{2}, h_{3}$ are sometimes called scale factors.

If $\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}$ are mutually perpendicular at any point $P$, the curvilinear coordinates are called orthogonal. Since the basis elements are unit vectors as well as orthogonal, this is an orthonormal basis. In such case the element of arc length $d s$ is given by


\begin{equation*}
d s^{2}=d \mathbf{r} \cdot d \mathbf{r}=h^{2}{ }_{1} d u^{2}{ }_{1}+h_{2}^{2} d u_{2}^{2}+h_{3}^{2} d u^{2}{ }_{3} \tag{19}
\end{equation*}


and corresponds to the square of the length of the diagonal in the preceding parallelepiped.

Also, in the case of othogonal coordinates referred to the orthonormal basis $\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}$, the volume of the parallelepiped is given by


\begin{equation*}
d V=\left|g_{j k}\right| d u_{1} d u_{2} d u_{3}=\left|\left(h_{1} d u_{1} \mathbf{e}_{1}\right) \cdot\left(h_{2} d u_{2} \mathbf{e}_{2}\right) \times\left(h_{3} d u_{3} \mathbf{e}_{3}\right)\right|=h_{1} h_{2} h_{3} d u_{1} d u_{2} d u_{3} \tag{20}
\end{equation*}


which can be written as


\begin{equation*}
d V=\left|\frac{\partial \mathbf{r}}{\partial u_{1}} \cdot \frac{\partial \mathbf{r}}{\partial u_{2}} \times \frac{\partial \mathbf{r}}{\partial u_{3}}\right| d u_{1} d u_{2} d u_{3}=\left|\frac{\partial(x, y, z)}{\partial\left(u_{1}, u_{2}, u_{3}\right)}\right| d u_{1} d u_{2} d u_{3} \tag{21}
\end{equation*}


where $\partial(x, y, z) / \partial\left(u_{1}, u_{2}, u_{3}\right)$ is the Jacobian of the transformation.

It is clear that when the Jacobian vanishes there is no parallelepiped and this explains geometrically the significance of the vanishing of a Jacobian as treated in Chapter 6.

Note: The further significance of the Jacobian vanishing is that the transformation degenerates at the point.

\section*{Gradient Divergence, Curl, and Laplacian in Orthogonal Curvilinear Coordinates}
If $\Phi$ is a scalar function and $\mathbf{A}=A_{1} \mathbf{e}_{1}+A_{2} \mathbf{e}_{2}+A_{3} \mathbf{e}_{3}$ a vector function of orthogonal curvilinear coordinates $u_{1}, u_{2}, u_{3}$, we have the following results.

\begin{enumerate}
  \item $\nabla \Phi=\operatorname{grad} \Phi=\frac{1}{h_{1}} \frac{\partial \Phi}{\partial u_{1}} \mathbf{e}_{1}+\frac{1}{h_{2}} \frac{\partial \Phi}{\partial u_{2}} \mathbf{e}_{2}+\frac{1}{h_{3}} \frac{\partial \Phi}{\partial u_{3}} \mathbf{e}_{3}$

  \item $\nabla \cdot \mathbf{A}=\operatorname{div} \mathbf{A}=\frac{1}{h_{1} h_{2} h_{3}}\left[\frac{\partial}{\partial u_{1}}\left(h_{2}, h_{3} A_{1}\right)+\frac{\partial}{\partial u_{2}}\left(h_{3} h_{1} A_{2}\right)+\frac{\partial}{\partial u_{3}}\left(h_{1} h_{2} A_{3}\right)\right]$

  \item $\nabla \times \mathbf{A}=\operatorname{curl} \mathbf{A}=\frac{1}{h_{1} h_{2} h_{3}}\left|\begin{array}{ccc}h_{1} \mathbf{e}_{1} & h_{2} \mathbf{e}_{2} & h_{3} \mathbf{e}_{3} \\ \frac{\partial}{\partial u_{1}} & \frac{\partial}{\partial u_{2}} & \frac{\partial}{\partial u_{3}} \\ h_{1} A_{1} & h_{2} A_{2} & h_{3} A_{3}\end{array}\right|$

  \item $\quad \nabla^{2} \Phi=$ Laplacian of $\Phi=\frac{1}{h_{1} h_{2} h_{3}}\left[\frac{\partial}{\partial u_{1}}\left(\frac{h_{2} h_{3}}{h_{1}} \frac{\partial \Phi}{\partial u_{1}}\right)+\frac{\partial}{\partial u_{2}}\left(\frac{h_{3} h_{1}}{h_{2}} \frac{\partial \Phi}{\partial u_{2}}\right)+\frac{\partial}{\partial u_{3}}\left(\frac{h_{1} h_{2}}{h_{3}} \frac{\partial \Phi}{\partial u_{3}}\right)\right]$

\end{enumerate}

These reduce to the usual expressions in rectangular coordinates if we replace $\left(u_{1}, u_{2}, u_{3}\right)$ by $(x, y, z)$, in which case $\mathbf{e}_{1}, \mathbf{e}_{2}$, and $\mathbf{e}_{3}$ are replaced by $\mathbf{i}, \mathbf{j}$, and $\mathbf{k}$ and $h_{1}=h_{2}=h_{3}=1$.

\section*{Special Curvilinear Coordinates}
Cylindrical Coordinates $(\rho, \phi, \mathbf{z}) \quad$ See Figure 7.13.

Transformation equations:

$$
x=\rho \cos \phi, y=\rho \sin \phi, z=z
$$

where $\rho \geqq 0,0 \leqq \phi<2 \pi,-\infty<z<\infty$.

Scale factors: $h_{1}=1, h_{2}=\rho, h_{3}=1$

Element of arc length: $d s^{2}=d \rho^{2}+\rho^{2} \rho^{2} d \phi^{2}+d z^{2}$

Jacobian: $\frac{\partial(x, y, z)}{\partial(\rho, \phi, z)}=\rho$

Element of volume: $d V=\rho d \rho d \phi d z$

$$
\text { Laplacian: } \quad \begin{aligned}
\quad \nabla^{2} U & =\frac{1}{\rho} \frac{\partial}{\partial \rho}\left(\rho \frac{\partial U}{\partial \rho}\right)+\frac{1}{\rho^{2}}+\frac{\partial^{2} U}{\partial \phi^{2}}+\frac{\partial^{2} U}{\partial z^{2}} \\
& =\frac{\partial^{2} U}{\partial \rho^{2}}+\frac{1}{\rho} \frac{\partial U}{\partial \rho}+\frac{1}{\rho^{2}}+\frac{\partial^{2} U}{\partial \phi^{2}}+\frac{\partial^{2} U}{\partial z^{2}}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-184(1)}
\end{center}

Figure 7.13

Note that corresponding results can be obtained for polar coordinates in the plane by omitting $z$ dependence. In such case, for example, $d s^{2}=d \rho^{2}+\rho^{2} d \phi^{2}$, while the element of volume is replaced by the element of area, $d A=\rho d \rho d \phi$.

Spherical Coordinates $(\mathbf{r}, \boldsymbol{\theta}, \boldsymbol{\phi})$ See Figure 7.14.

Transformation equations:

$$
x=r \sin \theta \cos \phi, y=r \sin \theta \sin \phi, z=r \cos \theta
$$

where $r \geqq 0,0 \leqq \theta \leqq \pi, 0 \leqq \phi<2 \pi$.

Scale factors: $h_{1}=1, h_{2}=r, h_{3}=r \sin \theta$

Element of arc length: $d s^{2}=d r^{2}+r^{2} d \theta^{2}+r^{2} \sin ^{2} \theta d \phi^{2}$

Jacobian: $\frac{\partial(x, y, z)}{\partial(r, \theta, \phi)}=r^{2} \sin \theta$

Element of volume: $d V=r^{2} \sin \theta d r d \theta d \phi$

Laplacian: $\nabla^{2} U=\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} \frac{\partial U}{\partial r}\right)$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-184}
\end{center}

Figure 7.14

$$
+\frac{1}{r^{2} \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \frac{\partial U}{\partial \theta}\right)+\frac{1}{r^{2} \sin ^{2} \theta} \frac{\partial^{2} U}{\partial \phi^{2}}
$$

Other types of coordinate systems are possible.

\section*{SOLVED PROBLEMS}
\section*{Vector algebra}
7.1. Show that addition of vectors is commutative, i.e., $\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$. See Figure 7.15.

$$
\mathbf{O P}+\mathbf{P Q}=\mathbf{O Q} \text { or } \mathbf{A}+\mathbf{B}=\mathbf{C}
$$

and

$$
\mathbf{O R}+\mathbf{R Q}=\mathbf{O Q} \text { or } \mathbf{B}+\mathbf{A}=\mathbf{C}
$$

Then $\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-185}
\end{center}

Figure 7.15

7.2. Show that the addition of vectors is associative, i.e., $\mathbf{A}+(\mathbf{B}+\mathbf{C})=$ $(\mathbf{A}+\mathbf{B})+\mathbf{C}$. See Figure 7.16.

$\mathbf{O P}+\mathbf{P Q}=\mathbf{O Q}=(\mathbf{A}+\mathbf{B})$ and $\mathbf{P Q}+\mathbf{Q R}=\mathbf{P R}=(\mathbf{B}+\mathbf{C})$

Since

$$
\begin{aligned}
& \mathbf{O P}+\mathbf{P R}=\mathbf{O R}=\mathbf{D} \text {, i.e., } \mathbf{A}+(\mathbf{B}+\mathbf{C})=\mathbf{D} \\
& \mathbf{O Q}+\mathbf{Q R}=\mathbf{O R}=\mathbf{D} \text {, i.e., }(\mathbf{A}+\mathbf{B})+\mathbf{C}=\mathbf{D}
\end{aligned}
$$

we have $\mathbf{A}+(\mathbf{B}+\mathbf{C})=(\mathbf{A}+\mathbf{B})+\mathbf{C}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-185(1)}
\end{center}

Figure 7.16

Extensions of the results of Problems 7.1 and 7.2 show that the order of addition of any number of vectors is immaterial.

7.3. An automobile travels 3 miles due north, then 5 miles northeast as shown in Figure 7.17. Represent these displacements graphically and determine the resultant displacement (a) graphically and (b) analytically.

Vector OP or A represents displacement of 3 miles due north

Vector $\mathbf{P Q}$ or $\mathbf{B}$ represents displacement of 5 miles northeast.

Vector $\mathbf{O Q}$ or $\mathbf{C}$ represents the resultant displacement or sum of vectors $\mathbf{A}$ and $\mathbf{B}$, i.e., $\mathbf{C}=\mathbf{A}+\mathbf{B}$. This is the triangle law of vector addition.

The resultant vector $\mathbf{O Q}$ can also be obtained by constructing the diagonal of the parallelogram $\boldsymbol{O} \boldsymbol{P} \boldsymbol{Q} \boldsymbol{R}$ having vectors $\mathbf{O P}=\mathbf{A}$ and $\mathbf{O R}$ (equal to vector $\mathbf{P Q}$ or $\mathbf{B}$ ) as sides. This is the parallelogram law of vector addition.

(a) Graphical Determination of Resultant. Lay off the 1-mile unit

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-185(2)}
\end{center}

Figure 7.17 on vector $\mathbf{O Q}$ to find the magnitude 7.4 miles (approximately).

Angle $E O Q=61.5^{\circ}$, using a protractor. Then vector $\mathbf{O Q}$ has magnitude 7.4 miles and direction $61.5^{\circ}$ north of east.

(b) Analytical Determination of Resultant. From triangle $O P Q$, denoting the magnitudes of $\mathbf{A}, \mathbf{B}, \mathbf{C}$ by $A, B$, $C$, we have by the law of cosines

$$
C^{2}=A^{2}+B^{2}-2 A B \cos O P Q=3^{2}+5^{2}-2(3)(5) \cos 135^{\circ}=34+15 \sqrt{2}=55.21
$$

and $C=7.43$ (approximately).

By the law of sines, $\frac{A}{\sin \angle O Q P}=\frac{C}{\sin \angle O P Q}$. Then

$$
\sin \angle O P Q=\frac{A \sin \angle O P Q}{C}=\frac{3(0.707)}{7.43}=0.2855 \text { and } \angle O Q P=16^{\circ} 35^{\prime}
$$

Thus, vector OQ has magnitude 7.43 miles and direction $\left(45^{\circ}+16^{\circ} 35^{\prime}\right)=61^{\circ} 35^{\prime}$ north of east.

7.4. Prove that if $\mathbf{a}$ and $\mathbf{b}$ are noncollinear, then $x \mathbf{a}+y \mathbf{b}=0$ implies $x=y=0$. Is the set $\{\mathbf{a}, \mathbf{b}\}$ linearly independent or linearly dependent?

Suppose $x \neq 0$. Then $x \mathbf{a}+\mathbf{y b}=\mathbf{0}$ implies $x \mathbf{a}=-y \mathbf{b}$ or $\mathbf{a}=-(y / x) \mathbf{b}$; i.e., $\mathbf{a}$ and $\mathbf{b}$ must be parallel to the same line (collinear), contrary to hypothesis. Thus, $x=0$; then $y \mathbf{b}=\mathbf{0}$, from which $y=0$. The set is linearly independent.

7.5. Prove that $x_{1} \mathbf{a}+y_{1} \mathbf{b}=x_{2} \mathbf{a}+y_{2} \mathbf{b}$, where $\mathbf{a}$ and $\mathbf{b}$ are noncollinear, then $x_{1}=x_{2}$ and $y_{1}=y_{2}$.

$x_{1} \mathbf{a}+y_{1} \mathbf{b}=x_{2} \mathbf{a}+y_{2} \mathbf{b}$ can be written

$$
x_{1} \mathbf{a}+y_{1} \mathbf{b}-\left(x_{2} \mathbf{a}+y_{2} \mathbf{b}\right)=\mathbf{0} \text { or }\left(x_{1}-x_{2}\right) \mathbf{a}+\left(y_{1}-y_{2}\right) \mathbf{b}=\mathbf{0}
$$

Hence, by Problem 7.4, $x_{1}-x_{2}=0, y_{1}-y_{2}=0$, or $x_{1}=x_{2}, y_{1}=y_{2}$.

Extensions are possible (see Problem 7.49).

7.6. Prove that the diagonals of a parallelogram bisect each other.

Let $A B C D$ be the given parallelogram with diagonals intersecting at $P$, as shown in Figure 7.18 .

Since $\mathbf{B D}+\mathbf{a}=\mathbf{b}, \mathbf{B D}=\mathbf{b}-\mathbf{a}$. Then $\mathbf{B P}=x(\mathbf{b}-\mathbf{a})$.

Since $\mathbf{A C}=\mathbf{a}+\mathbf{b}, \mathbf{A P}=y(\mathbf{a}+\mathbf{b})$.

But $\mathbf{A B}=\mathbf{A P}+\mathbf{P B}=\mathbf{A P}-\mathbf{B P}$; i.e., $\mathbf{a}=y(\mathbf{a}+\mathbf{b})-x(\mathbf{b}-\mathbf{a})$ $=(x+y) \mathbf{a}+(y-x) \mathbf{b}$.

Since $\mathbf{a}$ and $\mathbf{b}$ are noncollinear, we have, by Problem 7.5, $x+y=1$ and $y-x=0$; i.e., $x=y=1 / 2$ and $P$ is the midpoint of both diagonals.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-186}
\end{center}

Figure 7.18

7.7. Prove that the line joining the midpoints of two sides of a triangle is parallel to the third side and has half its length.

From Figure 7.19, $\mathbf{A C}+\mathbf{C B}=\mathbf{A B}$ or $\mathbf{b}+\mathbf{a}=\mathbf{c}$.

Let $\mathbf{D E}=\mathbf{d}$ be the line joining the midpoints of sides $A C$ and $C B$. Then $\mathbf{d}=\mathrm{DC}+\mathrm{CE}=\frac{\mathbf{1}}{\mathbf{2}} \mathbf{b}+\frac{1}{\mathbf{2}} \mathbf{a}=\frac{1}{2}(\mathbf{b}+\mathbf{a})=\frac{1}{2} \mathbf{c}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-186(2)}
\end{center}

Figure 7.19

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-186(1)}
\end{center}

Figure 7.20 $(\overline{O Q})^{2}=(\overline{O R})^{2}+(\overline{R Q})^{2}$.

Then $(\overline{O P})^{2}=(\overline{O R})^{2}+(\overline{R Q})^{2}+(\overline{Q P})^{2}$ or $A^{2}=A_{1}^{2}+A_{2}^{2}+A_{3}^{2}$, i.e.. $A=\sqrt{A_{1}^{2}+A_{2}^{2}+A_{3}^{2}}$.

7.9. Determine the vector having initial point $P\left(x_{1}, y_{1}, z_{1}\right)$ and terminal point $Q\left(x_{2}, y_{2}, z_{2}\right)$ and find its magnitude. See Figure 7.21 .

The position vector of $P$ is $\mathbf{r}_{1}=x_{1} \mathbf{i}+y_{1} \mathbf{j}+z_{1} \mathbf{k}$.

The position vector of $Q$ is $\mathbf{r}_{2}=x_{2} \mathbf{i}+y_{2} \mathbf{j}+z_{2} \mathbf{k}$.

$\mathbf{r}_{1}=\mathbf{P Q}=\mathbf{r}_{2}$ or

$$
\begin{aligned}
\mathbf{P Q}=\mathbf{r}_{2}-\mathbf{r}_{1} & =\left(x_{2} \mathbf{i}+y_{2} \mathbf{j}+z_{2} \mathbf{k}\right)-\left(x_{1} \mathbf{i}+y_{1} \mathbf{j}+z_{1} \mathbf{k}\right) \\
& =\left(x_{2}-x_{1}\right) \mathbf{i}+\left(y_{2}-y_{1}\right) \mathbf{j}+\left(z_{2}-z_{1}\right) \mathbf{k}
\end{aligned}
$$

Magnitude of $\mathbf{P Q}=\overline{P Q}$

$=\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}+\left(z_{2}-z_{1}\right)^{2}}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-187(2)}
\end{center}

Figure 7.21

Note that this is the distance between points $P$ and $Q$.

\section*{The dot or scalar product}
7.10. Prove that the projection of $\mathbf{A}$ on $\mathbf{B}$ is equal to $\mathbf{A} \cdot \mathbf{b}$, where $\mathbf{b}$, where $\mathbf{b}$ is a unit vector in the direction of $\mathbf{B}$.

Through the initial and terminal points of A pass planes perpendicular to $\mathbf{B}$ at $G$ and $H$, respectively, as in Figure 7.22; then

Projection of $\mathbf{A}$ on $\mathbf{B}=\overline{G H}=\overline{E F}=A \cos \theta=\mathbf{A} \cdot \mathbf{b}$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-187(1)}
\end{center}

Figure 7.22

7.11. Prove $\mathbf{A} \cdot(\mathbf{B}+\mathbf{C})=\mathbf{A} \cdot \mathbf{B}+\mathbf{A} \cdot \mathbf{C}$. See Figure 7.23

Let $\mathbf{a}$ be a unit vector in the direction of $\mathbf{A}$; then

Projection of $(\mathbf{B}+\mathbf{C})$ on $\mathbf{A}=$ projection of $\mathbf{B}$ on $\mathbf{A}+$ projection of $\mathbf{C}$ on $\mathbf{A}$

$$
(\mathbf{B}+\mathbf{C}) \cdot \mathbf{a}=\mathbf{B} \cdot \mathbf{a}+\mathbf{C} \cdot \mathbf{a}
$$

Multiplying by $A$.

$$
(\mathbf{B}+\mathbf{C}) \cdot A \mathbf{a}=\mathbf{B} \cdot A \mathbf{a}+\mathbf{C} \cdot A \mathbf{a}
$$

and

$$
(\mathbf{B}+\mathbf{C}) \cdot \mathbf{A}=\mathbf{B} \cdot \mathbf{A}+\mathbf{C} \cdot \mathbf{A}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-187}
\end{center}

Figure 7.23

Then by the commutative law for dot products.

$$
\mathbf{A} \cdot(\mathbf{B}+\mathbf{C})=\mathbf{A} \cdot \mathbf{B}+\mathbf{A} \cdot \mathbf{C}
$$

and the distributive law is valid.

7.12. Prove that $(\mathbf{A}+\mathbf{B}) \cdot(\mathbf{C}+\mathbf{D})=\mathbf{A} \cdot \mathbf{C}+\mathbf{A} \cdot \mathbf{D}+\mathbf{B} \cdot \mathbf{C}+\mathbf{B} \cdot \mathbf{D}$.

By Problem 7.11, (A+B) $(\mathbf{C}+\mathbf{D})=\mathbf{A} \cdot(\mathbf{C}+(\mathbf{C}+\mathbf{D})+\mathbf{B} \cdot(\mathbf{C}+\mathbf{D})=\mathbf{A} \cdot \mathbf{C}+\mathbf{A} \cdot \mathbf{D}+\mathbf{B} \cdot \mathbf{C}+\mathbf{B} \cdot \mathbf{D}$. The ordinary laws of algebra are valid for dot products where the operations are defined.

7.13. Evaluate each of the following.

(a) $\mathbf{i} \cdot \mathbf{i}=|\mathbf{i}||\mathbf{i}| \cos 0^{\circ}=(1)(1)(1)=1$

(b) $\mathbf{i} \cdot \mathbf{k}=|\mathbf{i}||\mathbf{k}| \cos 90^{\circ}=(1)(1)(0)=0$

(c) $\mathbf{k} \cdot \mathbf{j}=|\mathbf{k}||\mathbf{j}| \cos 90^{\circ}=(1)(1)(0)=0$

(d) $\mathbf{j} \cdot(2 \mathbf{i}-3 \mathbf{j}+\mathbf{k})=2 \mathbf{j} \cdot \mathbf{i}-3 \mathbf{j} \cdot \mathbf{j}+\mathbf{j} \cdot \mathbf{k}=0-3+0=-3$

(e) $(2 \mathbf{i}-\mathbf{j}) \cdot(3 \mathbf{i}+\mathbf{k})=2 \mathbf{i} \cdot(3 \mathbf{i}+\mathbf{k})-\mathbf{j} \cdot(3 \mathbf{i}+\mathbf{k})=6 \mathbf{i} \cdot \mathbf{i}+2 \mathbf{i} \cdot \mathbf{k}-3 \mathbf{j} \cdot \mathbf{i}-\mathbf{j} \cdot \mathbf{k}=6+0-0-0=6$

7.14. If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$ and $\mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$, prove that $\mathbf{A} \cdot \mathbf{B}=A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}$.

$$
\begin{aligned}
\mathbf{A} \cdot \mathbf{B}= & \left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \cdot\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right) \\
= & A_{1} \mathbf{i} \cdot\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right)+A_{2} \mathbf{j} \cdot\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right)+A_{3} \mathbf{k} \cdot\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right) \\
= & A_{1} B_{1} \mathbf{i} \cdot \mathbf{i}+A_{1} B_{2} \mathbf{i} \cdot \mathbf{j}+A_{1} B_{3} \mathbf{i} \cdot \mathbf{k}+A_{2} B_{1} \mathbf{j} \cdot \mathbf{i}+A_{2} B_{2} \mathbf{j} \cdot \mathbf{j}+A_{2} B_{3} \mathbf{j} \cdot \mathbf{k} \\
& +A_{3} B_{1} \mathbf{k} \cdot \mathbf{i}+A_{3} B_{2} \mathbf{k} \cdot \mathbf{j}+A_{3} B_{3} \mathbf{k} \cdot \mathbf{k} \\
= & A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}
\end{aligned}
$$

since $\mathbf{i} \cdot \mathbf{j}=\mathbf{k} \cdot \mathbf{k}=1$ and all other dot products are zero.

7.15. If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$. show that $A=\sqrt{\mathrm{A} \cdot \mathrm{A}}=\sqrt{A_{1}^{2}+A_{2}^{2}+A_{3}^{2}}$.

$$
\begin{aligned}
\quad \mathbf{A} \cdot \mathbf{A}= & (A)(A) \cos 0=A^{2} \text {. Then } A=\sqrt{\mathbf{A} \cdot \mathbf{A}} \\
\text { Also, } \mathbf{A} \cdot \mathbf{A} & =\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \cdot\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \\
& =\left(A_{1}\right)\left(A_{1}\right)+\left(A_{2}\right)\left(A_{2}\right)+\left(A_{3}\right)\left(A_{3}\right)=A_{1}^{2}+A_{2}^{2}+A_{3}^{2}
\end{aligned}
$$

By Problem 7.14, taking $\mathbf{B}=\mathbf{A}$.

Then $A=\sqrt{\mathbf{A} \cdot \mathbf{A}}=\sqrt{A_{1}^{2}+A_{2}^{2}+A_{3}^{2}}$. is the magnitude of $\mathbf{A}$. Sometimes $\mathbf{A} \cdot \mathbf{A}$ is written $\mathbf{A}^{2}$.

\section*{The cross or vector product}
7.16. Prove $\mathbf{A} \times \mathbf{B}=-\mathbf{B} \times \mathbf{A}$.

$\mathbf{A} \times \mathbf{B}=\mathbf{C}$ has magnitude $A B \sin \theta$ and direction such that $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ form a right-handed system. See Figure 7.24(a).

$\mathbf{B} \times \mathbf{A}=\mathbf{D}$ has magnitude $B A \sin \theta$ and direction such that $\mathbf{B}, \mathbf{A}$, and $\mathbf{D}$ form a right-handed system. See Figure 7.24(b).

Then $\mathbf{D}$ has the same magnitude as $\mathbf{C}$ but is opposite in direction; i.e., $\mathbf{C}=-\mathbf{D}$ or $\mathbf{A} \times \mathbf{B}=-\mathbf{B} \times \mathbf{A}$.

The commutative law for cross products is not valid.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-188(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-188}
\end{center}

(b)

Figure 7.24

7.17. Prove that $\mathbf{A} \times(\mathbf{B}+\mathbf{C})=\mathbf{A} \times \mathbf{B}+\mathbf{A} \times \mathbf{C}$ for the case where $\mathbf{A}$ is perpendicular to $\mathbf{B}$ and also to $\mathbf{C}$.

Since $\mathbf{A}$ is perpendicular to $\mathbf{B}, \mathbf{A} \times \mathbf{B}$ is a vector perpendicular to the plane of $\mathbf{A}$ and $\mathbf{B}$ and having magnitude $A B \sin 90^{\circ}=A B$ or magnitude of $A \mathbf{B}$. This is equivalent to multiplying vector $\mathbf{B}$ by $A$ and rotating the resultant vector through $90^{\circ}$ to the position shown in Figure 7.25.

Similarly, $\mathbf{A} \times \mathbf{C}$ is the vector obtained by multiplying $\mathbf{C}$ by $A$ and rotating the resultant vector through $90^{\circ}$ to the position shown.

In like manner, $\mathbf{A} \times(\mathbf{B}+\mathbf{C})$ is the vector obtained by multiplying $\mathbf{B}+\mathbf{C}$ by $A$ and rotating the resultant vector through $90^{\circ}$ to the position shown.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-189}
\end{center}

Figure 7.25

Since $\mathbf{A} \times(\mathbf{B}+\mathbf{C})$ is the diagonal of the parallelogram with $\mathbf{A} \times \mathbf{B}$ and $\mathbf{A} \times \mathbf{C}$ as sides, we have $\mathbf{A} \times(\mathbf{B}+\mathbf{C})=\mathbf{A} \times \mathbf{B}+\mathbf{A} \times \mathbf{C}$.

7.18. Prove that $\mathbf{A} \times(\mathbf{B}+\mathbf{C})=\mathbf{A} \times \mathbf{B}+\mathbf{A} \times \mathbf{C}$ in the general case where $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ are noncoplanar. See Figure 7.26

Resolve $\mathbf{B}$ into two component vectors, one perpendicular to $\mathbf{A}$ and the other parallel to $\mathbf{A}$, and denote them by $\mathbf{B}_{\perp}$ and $\mathbf{B}_{\|}$respectively. Then $\mathbf{B}=\mathbf{B}_{\perp}+\mathbf{B}_{\|}$.

If $\theta$ is the angle between $\mathbf{A}$ and $\mathbf{B}$, then $B_{\perp}=B \sin \theta$. Thus, the magnitude of $\mathbf{A} \times \mathbf{B}_{\perp}$ is $A B \sin \theta$, the same as the magnitude of $\mathbf{A} \times \mathbf{B}$. Also, the direction of $\mathbf{A} \times \mathbf{B}_{\perp}$ is the same as the direction of $\mathbf{A} \times \mathbf{B}$. Hence, $\mathbf{A}$ $\times \mathbf{B}_{\perp}=\mathbf{A} \times \mathbf{B}$.

Similarly, if $\mathbf{C}$ is resolved into two component vectors $\mathbf{C}_{\|}$and $\mathbf{C}_{\perp}$, parallel and perpendicular, respectively, to $\mathbf{A}$, then $\mathbf{A} \times \mathbf{C}_{\perp}=\mathbf{A} \times \mathbf{C}$.

Also, since $\mathbf{B}+\mathbf{C}=\mathbf{B}_{\perp}+\mathbf{B}_{\|}+\mathbf{C}_{\perp}+\mathbf{C}_{\|}=\left(\mathbf{B}_{\perp}+\mathbf{C}_{\perp}\right)+\left(\mathbf{B}_{\|}+\mathbf{C}_{\|}\right)$, it follows that

$$
\mathbf{A} \times\left(\mathbf{B}_{\perp}+\mathbf{C}_{\perp}\right)=\mathbf{A} \times(\mathbf{B}+\mathbf{C})
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-189(1)}
\end{center}

Figure 7.26

Now $\mathbf{B}_{\perp}$ and $\mathbf{C}_{\perp}$ are vectors perpendicular to $\mathbf{A}$, and so by Problem 7.17,

$$
\mathbf{A} \times\left(\mathbf{B}_{\perp}+\mathbf{C}_{\perp}\right)=\mathbf{A} \times \mathbf{B}_{\perp}+\mathbf{A} \times \mathbf{C}_{\perp}
$$

Then

$$
\mathbf{A} \times(\mathbf{B}+\mathbf{C})=\mathbf{A} \times \mathbf{B}+\mathbf{A} \times \mathbf{C}
$$

and the distributive law holds. Multiplying by -1 , using Problem 7.16, this becomes $(\mathbf{B}+\mathbf{C}) \times \mathbf{A}=\mathbf{B} \times \mathbf{A}+\mathbf{C}$ $\times \mathbf{A}$. Note that the order of factors in cross products is important. The usual laws of algebra apply only if proper order is maintained.

7.19. (a) If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$ and $\mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$, prove that $\mathbf{A} \times \mathbf{B}=\mathbf{A} \times \mathbf{B}=\left|\begin{array}{ccc}\mathbf{i} & \mathbf{j} & \mathbf{k} \\ A_{1} & A_{2} & A_{3} \\ B_{1} & B_{2} & B_{3}\end{array}\right|$.

$$
\begin{aligned}
\mathbf{A} \times \mathbf{B}= & \left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \times\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right) \\
= & A_{1} \mathbf{i} \times\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right)+A_{2} \mathbf{j} \times\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right)+A_{3} \mathbf{k} \times\left(B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}\right) \\
= & A_{1} B_{1} \mathbf{i} \times \mathbf{i}+A_{1} B_{2} \mathbf{i} \times \mathbf{j}+A_{1} B_{3} \mathbf{i} \times \mathbf{k}+A_{2} B_{1} \mathbf{j} \times \mathbf{i}+A_{2} B_{2} \mathbf{j} \times \mathbf{j}+A_{2} B_{3} \mathbf{j} \times \mathbf{k} \\
& +A_{3} B_{1} \mathbf{k} \times \mathbf{i}+A_{3} B_{2} \mathbf{k} \times \mathbf{j}+A_{3} B_{3} \mathbf{k} \times \mathbf{k} \\
= & \left(A_{2} B_{3}-A_{3} B_{2}\right) \mathbf{i}+\left(A_{3} B_{1}-A_{1} B_{3}\right) \mathbf{j}+\left(A_{1} B_{2}-A_{2} B_{1}\right) \mathbf{k}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
A_{1} & A_{2} & A_{3} \\
B_{1} & B_{2} & B_{3}
\end{array}\right|
\end{aligned}
$$

(b) Use the determinant representation to prove the result of Problem 7.18.

7.20. If $\mathbf{A}=3 \mathbf{i}-\mathbf{j}+2 \mathbf{k}$ and $\mathbf{B}=2 \mathbf{i}+3 \mathbf{j}-\mathbf{k}$, find $\mathbf{A} \times \mathbf{B}$.

$$
\begin{aligned}
\mathbf{A} \times \mathbf{B} & =\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
3 & -1 & 2 \\
2 & 3 & -1
\end{array}\right|=\mathbf{i}\left|\begin{array}{cc}
-1 & 2 \\
3 & -1
\end{array}\right|-\mathbf{j}\left|\begin{array}{cc}
3 & 2 \\
2 & -1
\end{array}\right|+\mathbf{k}\left|\begin{array}{cc}
3 & -1 \\
2 & 3
\end{array}\right| \\
& =-5 \mathbf{i}+7 \mathbf{j}+11 \mathbf{k}
\end{aligned}
$$

7.21. Prove that the area of a parallelogram with sides $\mathbf{A}$ and $\mathbf{B}$ is

$|\mathbf{A} \times \mathbf{B}|$. See Figure 7.27.

$$
\begin{aligned}
\text { Area of parallelogram } & =h|\mathbf{B}| \\
& =|\mathbf{A}| \sin \theta|\mathbf{B}| \\
& =|\mathbf{A} \times \mathbf{B}|
\end{aligned}
$$

Note that the area of the triangle with sides $\mathbf{A}$ and $\mathbf{B}=$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-190}
\end{center}

Figure 7.27 $1 / 2|\mathbf{A} \times \mathbf{B}|$.

7.22. Find the area of the triangle with vertices at $\mathrm{P}(2,3,5), \mathrm{Q}(4,2,-1)$, and $\mathrm{R}(3,6,4)$.

$$
\begin{aligned}
& \mathrm{PQ}=(4-2) \mathbf{i}+(2-3) \mathbf{j}+(-1-5) \mathbf{k}=2 \mathbf{i}-\mathbf{j}-6 \mathbf{k} \\
& \mathrm{PR}=(3-2) \mathbf{i}+(6-3) \mathbf{j}+(4-5) \mathbf{k}=\mathbf{i}+3 \mathbf{j}-\mathbf{k} \\
& \text { Area of triangle }=\frac{1}{2}|\mathrm{PQ} \times \mathrm{PR}|=\frac{1}{2}|(2 \mathbf{i}-\mathbf{j}-6 \mathbf{k}) \times(\mathrm{i}+3 \mathbf{j}-\mathbf{k})| \\
& =\frac{1}{2}\left\|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
2 & -1 & -6 \\
1 & 3 & -1
\end{array}\right\|=\frac{1}{2}|19 \mathbf{i}-4 \mathbf{j}+7 \mathbf{k}| \\
& =\frac{1}{2} \sqrt{(19)^{2}+(-4)^{2}+(7)^{2}}=\frac{1}{2} \sqrt{426}
\end{aligned}
$$

\section*{Triple products}
7.23. Show that $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})$ is in absolute value equal to the volume of a parallelepiped with sides $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$. See Figure 7.28.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-191}
\end{center}

Figure 7.28

Let $\mathbf{n}$ be a unit normal to parallelogram $I$, having the direction of $\mathbf{B} \times \mathbf{C}$, and let $h$ be the height of the terminal point of $\mathbf{A}$ above the parallelogram $I$.

Volume of a parallelepiped $=($ height $h)($ area of parallelogram $I)$

$$
\begin{aligned}
& =(\mathbf{A} \cdot \mathbf{n})(|\mathbf{B} \times \mathbf{C}|) \\
& =\mathbf{A} \cdot\{|\mathbf{B} \times \mathbf{C}| \mathbf{n}\}=\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})
\end{aligned}
$$

If $\mathbf{A}, \mathbf{B}$ and $\mathbf{C}$ do not form a right-handed system, $\mathbf{A} \cdot \mathbf{n}<0$ and the volume $=|\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})|$.

7.24. If $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}, \mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}, \mathbf{C}=C_{1} \mathbf{i}+C_{2} \mathbf{j}+C_{3} \mathbf{k}$ show that

$$
\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=\left|\begin{array}{lll}
A_{1} & A_{2} & A_{3} \\
B_{1} & B_{2} & B_{3} \\
C_{1} & C_{2} & C_{3}
\end{array}\right|
$$

$$
\begin{aligned}
\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C}) & =\mathbf{A} \cdot\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
B_{1} & B_{2} & B_{3} \\
C_{1} & C_{2} & C_{3}
\end{array}\right| \\
& =\left(A_{1} \mathrm{i}+A_{2} \mathrm{j}+A_{3} \mathrm{k}\right) \cdot\left[\left(B_{2} C_{3}-B_{3} C_{2}\right) \mathrm{i}+\left(B_{3} C_{1}-B_{1} C_{3}\right) \mathrm{j}+\left(B_{1} C_{2}-B_{2} C_{1}\right) \mathrm{k}\right] \\
& =A_{1}\left(B_{2} C_{3}-B_{3} C_{2}\right)+A_{2}\left(B_{3} C_{1}-B_{1} C_{3}\right)+A_{3}\left(B_{1} C_{2}-B_{2} C_{1}\right)=\left|\begin{array}{lll}
A_{1} & A_{2} & A_{3} \\
B_{1} & B_{2} & B_{3} \\
C_{1} & C_{2} & C_{3}
\end{array}\right| .
\end{aligned}
$$

7.25. Find the volume of a parallelepiped with sides $\mathbf{A}=3 \mathbf{i}-\mathbf{j}, \mathbf{B}=\mathbf{j}+2 \mathbf{k}, \mathbf{C}=\mathbf{i}+5 \mathbf{j}+4 \mathbf{k}$.

By Problems 7.23 and 7.24, volume of parallelepiped $\left.=|\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})|=\left|\begin{array}{ccc}3 & -1 & 0 \\ 0 & 1 & 2 \\ 1 & 5 & 4\end{array}\right||=|-20 \right\rvert\,=20$.

7.26. Prove that $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=(\mathbf{A} \times \mathbf{B}) \cdot \mathbf{C}$, i.e., the dot and cross can be interchanged.

By Problem 7.24: $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=\left|\begin{array}{lll}A_{1} & A_{2} & A_{3} \\ B_{1} & B_{2} & B_{3} \\ C_{1} & C_{2} & C_{3}\end{array}\right|,(\mathbf{A} \times \mathbf{B}) \cdot \mathbf{C}=\mathbf{C} \cdot(\mathbf{A} \times \mathbf{B})=\left|\begin{array}{lll}C_{1} & C_{2} & C_{3} \\ A_{1} & A_{2} & A_{3} \\ B_{1} & B_{2} & B_{3}\end{array}\right|$

Since the two determinants are equal, the required result follows.

7.27. Let $\mathbf{r}_{1}=x_{1} \mathbf{i}+y_{1} \mathbf{j}+z_{1} \mathbf{k}, \mathbf{r}_{2}=x_{2} \mathbf{i}+y_{2} \mathbf{j}+z_{2} \mathbf{k}$ and $\mathbf{r}_{3}=x_{3} \mathbf{i}+y_{3} \mathbf{j}+z_{3} \mathbf{k}$ be the position vectors of points $\mathrm{P}_{1}\left(x_{1}, y_{1}\right.$, $\left.z_{1}\right), \mathrm{P}_{2}\left(x_{2}, y_{\mathrm{x}}, z_{2}\right)$, and $\mathrm{P}_{3}\left(x_{3}, y_{3}, z_{3}\right)$. Find an equation for the plane passing through $\mathrm{P}_{1}, \mathrm{P}_{2}$, and $\mathrm{P}_{3}$. See Figure 7.29.

We assume that $\mathrm{P}_{1}, \mathrm{P}_{2}$, and $\mathrm{P}_{3}$ do not lie in the same straight line; hence, they determine a plane.

Let $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$ denote the position vectors of any point $\mathrm{P}(x, y, z)$ in the plane. Consider vectors $\overrightarrow{\mathrm{P}}_{1} \vec{P}_{2}$ $=\mathbf{r}_{2}-\mathbf{r}_{1}, \overrightarrow{\mathrm{P}}_{1} \overrightarrow{\mathrm{P}}_{3}=\mathbf{r}_{3}-\mathbf{r}_{1}$ and $\overrightarrow{\mathrm{P}_{1} \mathrm{P}}=\mathbf{r}-\mathbf{r}_{1}$ which all lie in the plane. Then

$$
\mathrm{P}_{1} \mathrm{P} \cdot \mathrm{P}_{1} \mathrm{P}_{2} \times \mathrm{P}_{1} \mathrm{P}_{3}=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-192}
\end{center}

Figure 7.29

or

$$
\left(\mathbf{r}-\mathbf{r}_{1}\right) \cdot\left(\mathbf{r}_{2}-\mathbf{r}_{1}\right) \times\left(\mathbf{r}_{3}-\mathbf{r}_{1}\right)=0
$$

In terms of rectangular coordinates this becomes

$$
\begin{gathered}
{\left[\left(x-x_{1}\right) \mathbf{i}+\left(y-y_{1}\right) \mathbf{j}+\left(z-z_{1}\right) \mathbf{k}\right] \cdot\left[\left(x_{2}-x_{1}\right) \mathbf{i}+\left(y_{2}-y_{1}\right) \mathbf{j}+\left(z_{2}-z_{1}\right) \mathbf{k}\right]} \\
\times\left[\left(x_{3}-x_{1}\right) \mathbf{i}+\left(y_{3}-y_{1}\right) \mathbf{j}+\left(z_{3}-z_{1}\right) \mathbf{k}\right]=0
\end{gathered}
$$

or, using Problem 7.24,

$$
\left|\begin{array}{ccc}
x-x_{1} & y-y_{1} & z-z_{1} \\
x_{2}-x_{1} & y_{2}-y_{1} & z_{2}-z_{1} \\
x_{3}-x_{1} & y_{3}-y_{1} & z_{3}-z_{1}
\end{array}\right|=0
$$

7.28. Find an equation for the plane passing through the points $P_{1}(3,1,-2), P_{2}(-1,2,4), P_{3}(2,-1,1)$.

The positions vectors of $P_{1}, P_{2}, P_{3}$ and any point $P(x, y, z)$ on the plane are respectively

$$
\mathbf{r}_{1}=3 \mathbf{i}+\mathbf{j}-2 \mathbf{k}, \mathbf{r}_{2}=-\mathbf{i}+2 \mathbf{j}+4 \mathbf{k}, \mathbf{r}_{3}=2 \mathbf{i}-\mathbf{j}+\mathbf{k}, \mathbf{r}=x \mathbf{i}+j \mathbf{j}+z \mathbf{k}
$$

Then $\mathbf{P} \mathbf{P}_{1}=\mathbf{r}-\mathbf{r}_{1}, \mathbf{P}_{2} \mathbf{P}_{1}=\mathbf{r}_{2}-\mathbf{r}_{1}, \mathbf{P}_{3} \mathbf{P}_{1}=\mathbf{r}_{3}-\mathbf{r}_{1}$, all lie in the required plane and so the required equation is $\left(\mathbf{r}-\mathbf{r}_{1}\right) \cdot\left(\mathbf{r}_{2}-\mathbf{r}_{1}\right) \times\left(\mathbf{r}_{3}-\mathbf{r}_{1}\right)=0$, i.e.,

$$
\begin{gathered}
\{(x-3) \mathbf{i}+(y-1) \mathbf{j}+(z+2) \mathbf{k}\} \cdot\{-4 \mathbf{i}+\mathbf{j}+6 \mathbf{k}\} \times\{-\mathbf{i}-2 \mathbf{j}+3 \mathbf{k}\}=0 \\
\{(x-3) \mathbf{i}+(y-1) \mathbf{j}+(z+2) \mathbf{k}\} \cdot\{15 \mathbf{i}+6 \mathbf{j}+9 \mathbf{k}\}=0 \\
15(x-3)+6(y-1)+9(z+2)=0 \quad \text { or } \quad 5 x-2 y+3 z=11
\end{gathered}
$$

Another method: By Problem 7.27, the required equation is

$$
\left|\begin{array}{ccc}
x-3 & y-1 & z+2 \\
-1-3 & 2-1 & 4+2 \\
2-3 & -1-1 & 1+2
\end{array}\right|=0 \quad \text { or } \quad 5 x+2 y+3 z=11
$$

7.29 (1) If $\mathbf{A}=\mathbf{i}+\mathbf{j}, \mathbf{B}=2 \mathbf{i}-3 \mathbf{j}+\mathbf{k}$, and $\mathbf{C}=4 \mathbf{j}-3 \mathbf{k}$, find (a) $(\mathbf{A} \times \mathbf{B}) \times \mathbf{C}$, (b) $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})$.

(a) $\mathbf{A} \times \mathbf{B}=\left|\begin{array}{ccc}\mathrm{i} & \mathrm{j} & \mathrm{k} \\ 1 & 1 & 0 \\ 2 & -3 & 1\end{array}\right|=\mathbf{i}-\mathbf{j}-5 \mathbf{k}$. Then $(\mathbf{A} \times \mathbf{B}) \times \mathbf{C}=\left|\begin{array}{ccc}\mathbf{i} & \mathbf{j} & \mathbf{k} \\ 1 & -1 & -5 \\ 0 & 4 & -3\end{array}\right|=23 \mathbf{i}+3 \mathbf{j}+4 \mathbf{k}$.

(b) $\mathbf{B} \times \mathbf{C}=\left|\begin{array}{ccc}\mathrm{i} & \mathrm{j} & \mathrm{k} \\ 2 & -3 & 1 \\ 0 & 4 & -3\end{array}\right|=5 \mathbf{i}+6 \mathbf{j}+8 \mathbf{k}$. Then $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})=\left|\begin{array}{lll}\mathbf{i} & \mathbf{j} & \mathbf{k} \\ 1 & 1 & 0 \\ 5 & 6 & 8\end{array}\right|=8 \mathbf{i}-8 \mathbf{j}+\mathbf{k}$.

It can be proved that, in general, $(\mathbf{A} \times \mathbf{B}) \times \mathbf{C} \neq \mathbf{A} \times(\mathbf{B} \times \mathbf{C})$.

7.29 (2) $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})=\mathbf{B}(\mathbf{A}, \mathbf{C})-\mathbf{C}(\mathbf{A}, \mathbf{B})$. Use the same vectors as in Problem 7.29 (1). (Note: sometimes remembered with the phrase "back to cab.")

\section*{Derivatives}
7.30. If $\mathbf{r}=\left(t^{3}+2 t\right) \mathbf{i}-3 e^{-2 t} \mathbf{j}+2 \sin 5 t \mathbf{k}$, find (a) $\frac{d \mathbf{r}}{d t}$, (b) $\left|\frac{d \mathbf{r}}{d t}\right|$, (c) $\frac{d^{2} \mathbf{r}}{d t^{2}}$, and (d) $\left|\frac{d^{2} \mathbf{r}}{d t^{2}}\right|$ at $t=0$, and give a possible\\
physical significance.

(a) $\frac{d \mathrm{r}}{d t}=\frac{d}{d t}\left(t^{3}+2 t\right) \mathbf{i}+\frac{d}{d t}\left(-3 e^{-2 t}\right) \mathbf{j}+\frac{d}{d t}(2 \sin 5 t) \mathbf{k}=\left(3 t^{2}+2\right) \mathbf{i}+6 e^{-2 t} \mathbf{j}+10 \cos 5 t \mathbf{k}$

At $t=0, d \mathbf{r} / d t=2 \mathbf{i}+6 \mathbf{j}+10 \mathbf{k}$

(b) From (a), $|d \mathbf{r} / d t|=\sqrt{(2)^{2}+(6)^{2}+(10)^{2}}=\sqrt{140}=2 \sqrt{35}$ at $t=0$.

(c) $\frac{d^{2} \mathbf{r}}{d t^{2}}=\frac{d}{d t}\left(\frac{d \mathbf{r}}{d t}\right)=\frac{d}{d t}\left\{\left(3 t^{2}+2\right) \mathbf{i}+6 e^{-2 t} \mathbf{j}+10 \cos 5 t \mathbf{k}\right\}=6 t \mathbf{i}-12 e^{-2 t} \mathbf{j}-50 \sin 5 t \mathbf{k}$

At $t=0, d^{2} \mathbf{r} / d t^{2}=-12 \mathbf{j}$.

(d) From (c), $\quad\left|d^{2} \mathbf{r} / d t^{2}\right|=12$ at $t=0$.

If $t$ represents time, these represent, respectively, the velocity, magnitude of the velocity, acceleration, and magnitude of the acceleration at $t=0$ of a particle moving along the space curve $x=t^{3}+2 t, y=-3 e^{-2 t}, z=2$ $\sin 5 t$.

7.31. Prove that $\frac{d}{d u}(\mathbf{A} \cdot \mathbf{B})=\mathrm{A} \cdot \frac{d \mathbf{B}}{d u}+\frac{d \mathbf{A}}{d u} \cdot \mathbf{B}$ where $\mathbf{A}$ and $\mathbf{B}$ are differentiable functions of $u$.

Method 1:

$$
\begin{aligned}
\frac{d}{d u}(\mathbf{A} \cdot \mathbf{B}) & =\lim _{\Delta u \rightarrow 0} \frac{(\mathbf{A}+\Delta \mathbf{A}) \cdot(\mathbf{B}+\Delta \mathbf{B})-\mathbf{A} \cdot \mathbf{B}}{\Delta u} \\
& =\lim _{\Delta u \rightarrow 0} \frac{\mathbf{A} \cdot \Delta \mathbf{B}+\Delta \mathbf{A} \cdot \mathbf{B}+\Delta \mathbf{A} \cdot \Delta \mathbf{B}}{\Delta u} \\
& =\lim _{\Delta u \rightarrow 0}\left(\mathrm{~A} \cdot \frac{\Delta \mathbf{B}}{\Delta u}+\frac{\Delta \mathbf{A}}{\Delta u} \cdot \mathbf{B}+\frac{\Delta \mathbf{A}}{\Delta u} \cdot \Delta \mathbf{B}\right)=\mathrm{A} \cdot \frac{d \mathbf{B}}{d u}+\frac{d \mathbf{A}}{d u} \cdot \mathbf{B}
\end{aligned}
$$

Method 2:

Let $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}, \mathbf{B}+B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$. Then

$$
\begin{aligned}
\frac{d}{d u}(\mathbf{A} \cdot \mathbf{B}) & =\frac{d}{d u}\left(A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}\right) \\
& =\left(A_{1} \frac{d B_{1}}{d u}+A_{2} \frac{d B_{2}}{d u}+A_{3} \frac{d B_{3}}{d u}\right)+\left(\frac{d A_{1}}{d u} B_{1}+\frac{d A_{2}}{d u} B_{2}+\frac{d A_{3}}{d u} B_{3}\right) \\
& =\mathbf{A} \cdot \frac{d \mathbf{B}}{d u}+\frac{d \mathbf{A}}{d u} \cdot \mathbf{B}
\end{aligned}
$$

7.32. If $\phi(x, y, z)=x^{2} y z$ and $\mathbf{A}=3 x^{2} y \mathbf{i}+y z^{2} \mathbf{j}-x z \mathbf{k}$, find $\mathbf{A}=3 x^{2} y \mathbf{i}+y z^{2} \mathbf{j}-x z \mathbf{k}$, find $\frac{\partial^{2}}{\partial y \partial z}(\phi \mathbf{A})$ at the point\\
$(1,-2,-1)$.

$$
\begin{aligned}
& \phi \mathbf{A}=\left(x^{2} y z\right)\left(3 x^{2} y \mathbf{i}+y z^{2} \mathbf{j}-x z \mathbf{k}\right)=3 \mathbf{x}^{4} y^{2} z \mathbf{i}+x^{2} y^{2} z^{3} \mathbf{j}-x^{3} y z^{2} \mathbf{k} \\
& \frac{\partial}{\partial z}(\phi \mathbf{A})=\frac{\partial}{\partial z}\left(3 \mathbf{x}^{4} y^{2} z \mathbf{i}+x^{2} y^{2} z^{3} \mathbf{j}-x^{3} y^{2} z^{3} \mathbf{k}\right)=3 \mathbf{x}^{4} y^{2} \mathbf{i}+3 x^{2} y^{2} z^{2} \mathbf{j}-2 x^{3} y z \mathbf{k} \\
& \frac{\partial^{2}}{\partial y \partial z}(\phi \mathbf{A})=\frac{\partial}{\partial z}\left(3 \mathbf{x}^{4} y^{2} \mathbf{i}+3 x^{2} y^{2} z^{2} \mathbf{j}-2 x^{3} y z \mathbf{k}\right)=6 \mathbf{x}^{4} y \mathbf{i}+6 x^{2} y z^{2} \mathbf{j}-2 x^{3} z \mathbf{k} \\
& \text { If } x=1, y=-2, z=-1, \text { this becomes }-12 \mathbf{i}-12 \mathbf{j}+2 \mathbf{k} .
\end{aligned}
$$

7.33. If $\mathbf{A}=x^{2} \sin y \mathbf{i}+z^{2} \cos y \mathbf{j}-x y^{2} \mathbf{k}$, find $d \mathbf{A}$.

\section*{Method 1:}
$$
\begin{aligned}
\frac{\partial \mathbf{A}}{\partial x} & =2 x \sin y \mathbf{i}-y^{2} \mathbf{k}, \quad \frac{\partial \mathbf{A}}{\partial y} x^{2} \cos y \mathbf{i}-z^{2} \sin y \mathbf{j}-2 x y \mathbf{k}, \quad \frac{\partial \mathbf{A}}{\partial z}=2 z \cos y \mathbf{j} \\
d \mathbf{A} & =\frac{\partial \mathbf{A}}{\partial x} d x+\frac{\partial \mathbf{A}}{\partial y} d y+\frac{\partial \mathbf{A}}{\partial z} d z \\
& =\left(2 x \sin y \mathbf{i}-y^{2} \mathbf{k}\right) d x+\left(x^{2} \cos y \mathbf{i}-z^{2} \sin y \mathbf{j}-2 x y \mathbf{k}\right) d y+(2 z \cos y \mathbf{j}) d z \\
& =\left(2 x \sin y d x+x^{2} \cos y d y\right) \mathbf{i}+\left(2 z \cos y d z-z^{2} \sin y d y\right) \mathbf{j}-\left(y^{2} d x+2 x y d y\right) \mathbf{k}
\end{aligned}
$$

Method 2:

$$
\begin{aligned}
d \mathbf{A} & =d\left(x^{2} \sin y\right) \mathbf{i}+d\left(z^{2} \cos y\right) \mathbf{j}-d\left(x y^{2}\right) \mathbf{k} \\
& =\left(2 x \sin y d x+x^{2} \cos y d y\right) \mathbf{i}+\left(2 z \cos y d z-z^{2} \sin y d y\right) \mathbf{j}-\left(y^{2} d x+2 x y d y\right) \mathbf{k}
\end{aligned}
$$

\section*{Gradient, divergence, and curl}
7.34. If $\phi=x^{2} y z^{3}$ and $\mathbf{A}=x z \mathbf{i}-y^{2} \mathbf{j}+2 x^{2} y \mathbf{k}$, find (a) $\nabla \phi$, (b) $\nabla \cdot \mathbf{A}$, (c) $\nabla \times \mathbf{A}$, (d) $\operatorname{div}(\phi \mathbf{A})$, (e) $\operatorname{curl}(\phi \mathbf{A})$.

(a) $\nabla \phi=\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \phi=\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}=\frac{\partial}{\partial x}\left(x^{2} y z^{3}\right) \mathbf{i}+\frac{\partial}{\partial x}\left(x^{2} y z^{3}\right) \mathbf{j}+\frac{\partial}{\partial z}\left(x^{2} y z^{3}\right) \mathbf{k}$

$$
=2 x y z^{3} \mathbf{i}+x^{2} z^{3} \mathbf{j}+3 x^{2} y z^{2} \mathbf{k}
$$

(b) $\nabla \cdot \mathbf{A}=\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \cdot\left(x z \mathbf{i}-y^{2} \mathbf{j}+2 x^{2} y \mathbf{k}\right)$

$$
=\frac{\partial}{\partial x}(x z)+\frac{\partial}{\partial y}\left(-y^{2}\right)+\frac{\partial}{\partial z}\left(2 x^{2} y\right)=z-2 y
$$

(c) $\nabla \times \mathbf{A}=\left(\mathbf{i} \frac{\partial}{\partial x}+\mathbf{j} \frac{\partial}{\partial y}+\mathbf{k} \frac{\partial}{\partial z}\right) \times\left(x z \mathbf{i}-y^{2} \mathbf{j}+2 x^{2} y \mathbf{k}\right)$

$$
\begin{aligned}
& =\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\partial / \partial x & \partial / \partial y & \partial / \partial z \\
x z & -y^{2} & 2 x^{2} y
\end{array}\right| \\
& =\left(\frac{\partial}{\partial x}\left(2 x^{2} y\right)-\frac{\partial}{\partial z}\left(-y^{2}\right)\right) \mathbf{i}+\left(\frac{\partial}{\partial z}(x z)-\frac{\partial}{\partial x}\left(2 x^{2} y\right)\right) \mathbf{j}+\left(\frac{\partial}{\partial x}\left(-y^{2}\right)-\frac{\partial}{\partial y}(x z)\right) \mathbf{k} \\
& =2 x^{2} \mathbf{i}+(x-4 x y) \mathbf{j}
\end{aligned}
$$

(d) $\operatorname{div}(\phi \mathbf{A})=\nabla \cdot(\phi \mathbf{A})=\nabla \cdot\left(x^{3} y z^{4} \mathbf{i}-x^{2} y^{3} z^{3} \mathbf{j}+2 x^{4} y^{2} z^{3} \mathbf{k}\right)$

$$
\begin{aligned}
& =\frac{\partial}{\partial x}\left(x^{3} y z^{4}\right)+\frac{\partial}{\partial y}\left(-x^{2} y^{3} z^{3}\right)+\frac{\partial}{\partial z}\left(2 x^{4} y^{2} z^{3}\right) \\
& =3 x^{2} y z^{4}-3 x^{2} y^{2} z^{3}+6 x^{4} y^{2} z^{2}
\end{aligned}
$$

(e) $\operatorname{curl}(\phi \mathbf{A})=\nabla \times(\phi \mathbf{A})=\nabla \times\left(x^{3} y z^{4} \mathbf{i}-x^{2} y^{3} z^{3} \mathbf{j}+2 x^{4} y^{2} z^{3} \mathbf{k}\right)$

$$
\begin{aligned}
& =\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\partial / \partial x & \partial / \partial y & \partial / \partial z \\
x^{3} y z^{4} & -x^{2} y^{3} z^{3} & 2 x^{4} y^{2} z^{3}
\end{array}\right| \\
& =\left(4 x^{4} y z^{3}-3 x^{2} y^{3} z^{2}\right) \mathbf{i}+\left(4 x^{3} y z^{3}-8 x^{3} y^{2} z^{3}\right) \mathbf{j}-\left(2 x y^{3} z^{3}+x^{3} z^{4}\right) \mathbf{k}
\end{aligned}
$$

7.35.

Prove $\nabla \cdot(\phi \mathbf{A})=(\nabla \phi) \cdot \mathbf{A}+\phi(\nabla \cdot \mathbf{A})$.

$$
\begin{aligned}
\nabla \cdot(\phi \mathbf{A})= & \nabla \cdot\left(\phi A_{1} \mathbf{i}+\phi A_{2} \mathbf{j}+\phi A_{3} \mathbf{k}\right) \\
= & \frac{\partial}{\partial x}\left(\phi A_{1}\right)+\frac{\partial}{\partial y}\left(\phi A_{2}\right)+\frac{\partial}{\partial z}\left(\phi A_{3}\right) \\
& =\frac{\partial \phi}{\partial x} A_{1}+\frac{\partial \phi}{\partial y} A_{2}+\frac{\partial \phi}{\partial z} A_{3}+\phi\left(\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}\right) \\
& =\left(\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}\right) \cdot\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \\
& +\phi\left(\frac{\partial}{\partial x} \mathbf{i}+\frac{\partial}{\partial y} \mathbf{j}+\frac{\partial}{\partial z} \mathbf{k}\right) \cdot\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \\
= & (\nabla \phi) \cdot \mathbf{A}+\phi(\nabla \cdot \mathbf{A})
\end{aligned}
$$

7.36. Express a formula for the tangent plane to the surface $\phi(x, y, z)=0$ at one of its points $P_{0}\left(x_{0}, y_{0}, z_{0}\right)$.

$$
(\nabla \phi)_{0} \cdot\left(\mathbf{r}-\mathbf{r}_{0}\right)=0
$$

7.37. Find a unit normal to the surface $2 x^{2}+4 y z-5 z^{2}=-10$ at the point $P(3,-1,2)$.

By Problem 7.36, a vector normal to the surface is

$$
\nabla\left(2 x^{2}+4 y z-5 z^{2}\right)=4 x \mathbf{i}+4 z \mathbf{j}+(4 y-10 z) \mathbf{k}=12 \mathbf{i}+8 \mathbf{j}-24 \mathbf{k} \text { at }(3,-1,2)
$$

Them a unit normal to the surface at $P$ is

$$
\frac{12 \mathbf{i}+8 \mathbf{j}-24 \mathbf{k}}{\sqrt{(12)^{2}+(8)^{2}+(-24)^{2}}}=\frac{3 \mathbf{i}+2 \mathbf{j}-6 \mathbf{k}}{7}
$$

Another unit normal to the surface at $P$ is

$$
-\frac{3 \mathbf{i}+2 \mathbf{j}-6 \mathbf{k}}{7}
$$

7.38. If $\phi=2 x^{2} y-x z^{3}$, find (a) $\nabla \phi$ and (b) $\nabla^{2} \phi$.

(a) $\nabla \phi=\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}=\left(4 x y-z^{3}\right) \mathbf{i}+2 x^{2} \mathbf{j}-3 x z^{2} \mathbf{k}$

(b) $\nabla^{2} \phi=$ Laplacian of $\phi=\nabla \cdot \nabla \phi=\frac{\partial}{\partial x}\left(4 x y-z^{3}\right)+\frac{\partial}{\partial y}\left(2 x^{2}\right)+\frac{\partial}{\partial z}\left(-3 x z^{2}\right)=4 y-6 x z$

\section*{Another method:}
$$
\begin{aligned}
\nabla^{2} \phi & =\frac{\partial^{2} \phi}{\partial x^{2}}+\frac{\partial^{2} \phi}{\partial y^{2}}+\frac{\partial^{2} \phi}{\partial z^{2}}=\frac{\partial^{2}}{\partial x^{2}}\left(2 x^{2} y-x z^{3}\right)+\frac{\partial^{2}}{\partial y^{2}}\left(2 x^{2} y-x z^{3}\right)+\frac{\partial^{2}}{\partial z^{2}}\left(2 x^{2} y-x z^{3}\right) \\
& =4 y-6 x z
\end{aligned}
$$

7.39. Prove div curl $\mathbf{A}=0$

$$
\begin{aligned}
\operatorname{div} \text { curl } \mathbf{A} & =\nabla \cdot(\nabla \times \mathbf{A})=\nabla \cdot\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\partial / \partial x & \partial / \partial y & \partial / \partial z \\
A_{1} & A_{2} & A_{3}
\end{array}\right| \\
& =\nabla \cdot\left[\left(\frac{\partial A_{3}}{\partial y}-\frac{\partial A_{2}}{\partial z}\right) \mathbf{i}+\left(\frac{\partial A_{1}}{\partial z}-\frac{\partial A_{3}}{\partial x}\right) \mathbf{j}+\left(\frac{\partial A_{2}}{\partial x}-\frac{\partial A_{1}}{\partial y}\right) \mathrm{k}\right. \\
& =\frac{\partial}{\partial x}\left(\frac{\partial A_{3}}{\partial y}-\frac{\partial A_{2}}{\partial z}\right)+\frac{\partial}{\partial y}\left(\frac{\partial A_{1}}{\partial z}-\frac{\partial A_{3}}{\partial x}\right)+\frac{\partial}{\partial z}\left(\frac{\partial A_{2}}{\partial x}-\frac{\partial A_{1}}{\partial y}\right. \\
& =\frac{\partial^{2} A_{3}}{\partial x \partial y}-\frac{\partial^{2} A_{2}}{\partial x \partial z}+\frac{\partial^{2} A_{1}}{\partial y \partial z}-\frac{\partial^{2} A_{3}}{\partial y \partial x}+\frac{\partial^{2} A_{2}}{\partial z}-\frac{\partial^{2} A_{1}}{\partial z \partial y} \\
& =0
\end{aligned}
$$

assuming that $\mathbf{A}$ has continuous second partial derivatives so that the order of differentiation is immaterial.

\section*{Jacobians and curvilinear coordinates}
7.40. Find $d s^{2}$ in (a) cylindrical and (b) spherical coordinates and determine the scale factors.

(a) Method 1:

$$
\begin{gathered}
x=\rho \cos \phi, y=\rho \sin \phi,=z \\
d x=-\rho \sin \phi d \phi+\cos \phi d \phi \rho, \quad d y=\rho \cos \phi d \phi+\sin \phi d \rho, \quad d z=d z
\end{gathered}
$$

Then

$$
\begin{aligned}
d s^{2} & =d x^{2}+d y^{2}+d z^{2}=(-\rho \sin \phi d \phi+\cos \phi d \rho)^{2}+(\rho \cos \phi d \phi+\sin \phi d \rho)^{2}+(d z)^{2} \\
& =(d \rho)^{2}+\rho^{2}(d \phi)^{2}+(d z)^{2}=h_{1}^{2}(d \rho)^{2}+h_{2}^{2}(d \phi)^{2}+d_{3}^{2}(d z)^{2}
\end{aligned}
$$

and $h_{1}=h_{\rho}=1, h_{2}=h_{\phi}=\rho, h_{3}=h_{\mathrm{z}}=1$ are the scale factors.

Method 2: $\quad$ The position vector is $\mathbf{r}=\rho \cos \phi \mathbf{i}+\rho \sin \phi \mathbf{j}+z \mathbf{k}$. Then

$$
\begin{aligned}
d \mathbf{r} & =\frac{\partial \mathbf{r}}{\partial \rho} d \rho+\frac{\partial \mathbf{r}}{\partial \phi} d \phi+\frac{\partial \mathbf{r}}{\partial z} d z \\
& =(\cos \phi \mathbf{i}+\sin \phi \mathbf{j}) d \rho+(-\rho \sin \phi \mathbf{i}+\rho \cos \phi \mathbf{j}) d \phi+\mathbf{k} d z \\
& =(\cos \phi d \rho-\rho \sin \phi d \phi) \mathbf{i}+(\sin \phi d \rho+\rho \cos \phi d \phi) \mathbf{j}+\mathbf{k} d z
\end{aligned}
$$

Thus, $d s^{2}=d \mathbf{r} \cdot d \mathbf{r}=(\cos \phi d \rho-\rho \sin \phi d \phi)^{2}+(\sin \phi d \rho+\rho \cos \phi d \phi)^{2}+(d z)^{2}$

$$
=(d \rho)^{2}+\rho^{2}(d \phi)^{2}+(d z)^{2}
$$

(b) $x=r \sin \theta \cos \phi, \quad y=r \sin \theta \sin \phi, \quad z=r \cos \theta$

Then

$$
\begin{aligned}
& d x=-r \sin \theta \sin \phi d \phi+r \cos \theta \cos \phi d \theta+\sin \theta \cos \phi d r \\
& d y=r \sin \theta \cos \phi d \phi+r \cos \theta \sin \phi d \theta+\sin \theta \sin \phi d r \\
& d z=-r \sin \theta d \phi+\cos \theta d r
\end{aligned}
$$

and

$$
(d s)^{2}=(d x)^{2}+(d y)^{2}+(d z)^{2}=(d r)^{2}+r^{2}(d \theta)^{2}+r^{2} \sin ^{2} \theta(d \phi)^{2}
$$

The scale factors are $h_{1}=h_{r}=1, h_{2}=h_{\theta}=r, h_{3}=h_{\phi}=r \sin \theta$.

7.41. Find the volume element $d V$ in (a) cylindrical and (b) spherical coordinates and sketch.

The volume element in orthogonal curvilinear coordinates $u_{1}, u_{2}, u_{3}$ is

$$
d V=h_{1} h_{2} h_{3} d u_{1} d u_{2} d u_{3}=\left|\frac{\partial(x, y, z)}{\partial\left(u_{1}, u_{2}, u_{3}\right)}\right| d u_{1} d u_{2} d u_{3}
$$

(a) In cylindrical coordinates, $u_{1}=\rho, u_{2}=\phi, u_{3}=z, h_{1}=1, h_{2}=\rho, h_{3}=1$ [see Problem 7.40(a)]. Then

$$
d V=(1)(\rho)(1) d \rho d \phi d z=\rho d \rho d \phi d z
$$

This can also be observed directly from Figure 7.30(a).

(b) In spherical coordinates, $u_{1}=r, u_{2}=\theta, u_{3}=\phi, h_{1}=1, h_{2}=r, h_{3}=r \sin \theta$ [see Problem 7.40(b)]. Then

$$
d V=(1)(r)(r \sin \theta) d r d \theta d \phi=r^{2} \sin \theta d r d \theta d \phi
$$

This can also be observed directly from Figure 7.30(b).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-197}
\end{center}

(a) Volume element in cylindrical coordinates.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-197(1)}
\end{center}

(b) Volume element in spherical coordinates.

Figure 7.30

7.42. Express in cylindrical coordinates: (a) $\operatorname{grad} \Phi$, (b) div $\mathbf{A}$, and (c) $\nabla^{2} \Phi$.

Let $u_{1}=\rho, u_{2}=\phi, u_{3}=z, h_{1}=1, h_{2}=\rho, h_{3}=1$ [see Problem 7.40(a) and (b).] Then

(a) $\operatorname{grad} \Phi=\nabla \Phi=\frac{1}{1} \frac{\partial \Phi}{\partial \rho} \mathbf{e}_{1}+\frac{1}{\rho} \frac{\partial \Phi}{\partial \phi} \mathbf{e}_{2}+\frac{1}{1} \frac{\partial \Phi}{\partial z} \mathbf{e}_{3}=\frac{\partial \Phi}{\partial \rho} \mathbf{e}_{1}+\frac{1}{\rho} \frac{\partial \Phi}{\partial \phi} \mathbf{e}_{2}+\frac{\partial \Phi}{\partial z} \mathbf{e}_{3}$

where $\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}$ are the unit vectors in the directions of increasing $\rho, \phi, z$, respectively.

(b) $\operatorname{div} \mathbf{A}=\nabla \cdot \mathbf{A}=\frac{1}{(1)(\rho)(1)}\left[\frac{\partial}{\partial \rho}\left((\rho)(1) A_{1}\right)+\frac{\partial}{\partial \phi}\left((1)(1) A_{2}\right)+\frac{\partial}{\partial z}\left((1)(\rho) A_{3}\right)\right]$

$$
=\frac{1}{\rho}\left[\frac{\partial}{\partial \rho}\left(\rho A_{1}\right)+\frac{\partial A_{2}}{\partial \phi}+\frac{\partial A_{3}}{\partial z}\right]
$$

where $\mathbf{A}=A_{1} \mathbf{e}_{1}+A_{2} \mathbf{e}_{2}+A_{3} \mathbf{e}_{3}$.

(c) $\nabla^{2} \Phi=\frac{1}{(1)(\rho)(1)}\left[\frac{\partial}{\partial \rho}\left(\frac{(\rho)(1)}{(1)} \frac{\partial \phi}{\partial \rho}\right)+\frac{\partial}{\partial \phi}\left(\frac{(1)(1)}{(\rho)} \frac{\partial \Phi}{\partial \phi}\right)+\frac{\partial}{\partial z}\left(\frac{(1)(\rho)}{(1)} \frac{\partial \Phi}{\partial z}\right)\right]$

$=\frac{1}{\rho} \frac{\partial}{\partial \rho}\left(\rho \frac{\partial \Phi}{\partial \rho}\right)+\frac{1}{\rho^{2}} \frac{\partial^{2} \Phi}{\partial \phi^{2}}+\frac{\partial^{2} \Phi}{\partial z^{2}}$

\section*{Miscellaneous problems}
7.43. Prove that $\operatorname{grad} f(r)=\frac{f^{\prime}(r)}{r} \mathbf{r}$, where $r=\sqrt{x^{2}+y^{2}+z^{2}}$ and $f^{\prime}(r)=d f$ is $d f / d r$ is assumed to exist.

$$
\begin{aligned}
\operatorname{grad} f(r) & =\nabla f(r)=\frac{\partial}{\partial x} f(r) \mathbf{i}+\frac{\partial}{\partial y} f(r) \mathbf{j}+\frac{\partial}{\partial z} f(r) \mathbf{k} \\
& =f^{\prime}(r) \frac{\partial r}{\partial x} \mathbf{i}+f^{\prime}(r) \frac{\partial r}{\partial y} \mathbf{j}+f^{\prime}(r) \frac{\partial r}{\partial z} \mathbf{k} \\
& =f^{\prime}(r) \frac{x}{r} \mathbf{i}+f^{\prime}(r) \frac{y}{r} \mathbf{j}+f^{\prime}(r) \frac{z}{r} \mathbf{k}=\frac{f^{\prime}(r)}{r}(x \mathbf{i}+y \mathbf{j}+z \mathbf{k})=\frac{f^{\prime}(r)}{r} \mathbf{r}
\end{aligned}
$$

Another method: In orthogonal curvilinear coordinates $u_{1}, u_{2}, u_{3}$, we have


\begin{equation*}
\nabla \Phi=\frac{1}{h_{1}} \frac{\partial \Phi}{\partial u_{1}} \mathbf{e}_{1}+\frac{1}{h_{2}} \frac{\partial \Phi}{\partial u_{2}} \mathbf{e}_{2}+\frac{1}{h_{3}} \frac{\partial \Phi}{\partial u_{3}} \mathbf{e}_{3} \tag{1}
\end{equation*}


If, in particular, we use spherical coordinates, we have $u_{1}=r, u_{2}=\theta, u_{3}=\phi$. Then letting $\Phi=f(r)$, a function of $r$ alone, the last two terms on the right of Equation (1) are zero. Hence, on observing that $\mathbf{e}_{1}=\mathbf{r} / r$ and $h_{1},=1$, we have, the result


\begin{equation*}
\nabla f(r)=\frac{1}{1} \frac{\partial f(r)}{\partial r} \frac{\mathbf{r}}{r}=\frac{f^{\prime}(r)}{r} \mathbf{r} \tag{2}
\end{equation*}


7.44. (a) Find the Laplacian of $\phi=f(r)$. (b) Prove that $\phi=1 / r$ is a solution of Laplace's equation $\nabla^{2} \phi=0$.

(a) By Problem 7.43,

$$
\nabla \phi=\nabla f(r)=\frac{f^{\prime}(r)}{r} \mathbf{r}
$$

By Problem 7.35, assuming that $f(r)$ has continuous second partial derivatives, we have

$$
\begin{aligned}
\text { Laplacian of } \phi & =\nabla^{2} \phi=\nabla \cdot(\nabla \phi)=\nabla \cdot\left\{\frac{f^{\prime}(r)}{r} \mathbf{r}\right\} \\
& =\nabla\left\{\frac{f^{\prime}(r)}{r}\right\} \cdot \mathbf{r}+\frac{f^{\prime}(r)}{r}(\nabla \cdot \mathbf{r})=\frac{1}{r} \frac{d}{d r}\left\{\frac{f^{\prime}(r)}{r}\right\} \mathbf{r} \cdot \mathbf{r}+\frac{f^{\prime}(r)}{r}(3) \\
& =\frac{r f^{\prime \prime}(r)+f^{\prime}(r)}{r^{3}} r^{2}+\frac{3 f^{\prime}(r)}{r}=f^{\prime \prime}(r)+\frac{2}{r} f^{\prime}(r)
\end{aligned}
$$

Another method: In spherical coordinates, we have

$$
\nabla^{2} U=\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} \frac{\partial U}{\partial r}\right)+\frac{1}{r^{2} \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \frac{\partial U}{\partial \theta}\right)+\frac{1}{r^{2} \sin ^{2} \theta} \frac{\partial^{2} U}{\partial \phi^{2}}
$$

If $U=f(r)$, the last two terms on the right are zero and we find

$$
\nabla^{2} f(r)=\frac{1}{r^{2}} \frac{d}{d r}\left(r^{2} f^{\prime}(r)\right)=f^{\prime \prime}(r)+\frac{2}{r} f^{\prime}(r)
$$

(b) From the result in (a), we have

$$
\nabla^{2}\left(\frac{1}{r}\right)=\frac{d^{2}}{d r^{2}}\left(\frac{1}{r}\right)+\frac{2}{r} \frac{d}{d r}\left(\frac{1}{r}\right)=\frac{2}{r^{3}}-\frac{2}{r^{3}}=0
$$

showing that $1 / r$ is a solution of Laplace's equation.

7.45. A particle moves along a space curve $\mathbf{r}=\mathbf{r}(t)$, where $t$ is the time measured from some initial time. If $v=|d \mathbf{r} / d t|=d s / d t$ is the magnitude of the velocity of the particle ( $s$ is the arc length along the space curve measured from the initial position), prove that the acceleration a of the particle is given by

$$
\mathbf{a}=\frac{d v}{d t} \mathbf{T}+\frac{v^{2}}{\rho} \mathbf{N}
$$

where $\mathbf{T}$ and $\mathbf{N}$ are unit tangent and normal vectors to the space curve and

$$
\rho=\left|\frac{d^{2} \mathbf{r}}{d s^{2}}\right|^{-1}=\left\{\left(\frac{d^{2} x}{d s^{2}}\right)^{2}+\left(\frac{d^{2} y}{d s^{2}}\right)^{2}+\left(\frac{d^{2} z}{d s^{2}}\right)^{2}\right\}^{-1 / 2}
$$

The velocity of the particle is given by $\mathbf{v}=v \mathbf{T}$. Then the acceleration is given by


\begin{equation*}
\mathbf{a}=\frac{d \mathbf{v}}{d t}=\frac{d}{d t}(v \mathbf{T})=\frac{d v}{d t} \mathbf{T}+v \frac{d \mathbf{T}}{d t}=\frac{d v}{d t} \mathbf{T}+v \frac{d \mathbf{T}}{d s} \frac{d s}{d t}=\frac{d v}{d t} \mathbf{T}+v^{2} \frac{d \mathbf{T}}{d s} \tag{1}
\end{equation*}


Since $\mathbf{T}$ has a unit magnitude, we have $\mathbf{T} \cdot \mathbf{T}=1$. Then, differentiating with respect to $s$,

$$
\mathbf{T} \cdot \frac{d \mathbf{T}}{d s}+\frac{d \mathbf{T}}{d s} \cdot \mathbf{T}=0, \quad 2 \mathbf{T} \cdot \frac{d \mathbf{T}}{d s}=0 \quad \text { or } \quad \mathbf{T} \cdot \frac{d \mathbf{T}}{d s}=0
$$

from which it follows that $d \mathbf{T} / d s$ is perpendicular to $\mathbf{T}$. Denoting by $\mathbf{N}$ the unit vector in the direction of $d \mathbf{T} /$ $d s$, and called the principal normal to the space curve, we have


\begin{equation*}
\frac{d \mathbf{T}}{d s}=\kappa \mathbf{N} \tag{2}
\end{equation*}


where $k$ is the magnitude of $d \mathbf{T} / d s$. Now, since $\mathbf{T}=d / \mathbf{r} / d s$ [see Equation (7), Page 168], we have $d \mathbf{T} / d s=d^{2} \mathbf{r} /$ $d s^{2}$. Hence,

$$
\kappa=\left|\frac{d^{2} \mathbf{r}}{d s^{2}}\right|=\left\{\left(\frac{d^{2} x}{d s^{2}}\right)^{2}+\left(\frac{d^{2} y}{d s^{2}}\right)^{2}+\left(\frac{d^{2}}{d s}\right.\right.
$$

Defining $\rho=1 / k$, Equation (2) becomes $d \mathbf{T} / d s=\mathbf{N} / \rho$. Thus, from Equation (1) we have, as required,

$$
\mathbf{a}=\frac{d v}{d t} \mathbf{T}+\frac{v^{2}}{\rho} \mathbf{N}
$$

The components $d v / d t$ and $v^{2} / \rho$ in the direction of $\mathbf{T}$ and $\mathbf{N}$ are called the tangential and normal components of the acceleration, the latter being sometimes called the centripetal acceleration. The quantities $\rho$ and $k$ are, respectively, the radius of curvature and curvature of the space curve.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Vector algebra}
7.46. Given any two vectors $\mathbf{A}$ and $\mathbf{B}$, illustrate geometrically the equality $4 \mathbf{A}+3(\mathbf{B}-\mathbf{A})=\mathbf{A}+3 \mathbf{B}$.

7.47. A man travels 25 miles northeast, 15 miles due east, and 10 miles due south. By using an appropriate scale, determine graphically (a) how far and (b) in what direction he is from his starting position. Is it possible to determine the answer analytically?

Ans. 33.6 miles, $13.2^{\circ}$ north of east

7.48. If $\mathbf{A}$ and $\mathbf{B}$ are any two nonzero vectors which do not have the same direction, prove that $m \mathbf{A}+n \mathbf{B}$ is a vector lying in the plane determined by $\mathbf{A}$ and $\mathbf{B}$.

7.49. If $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ are non-coplanar vectors (vectors which do not all lie in the same plane) and $x_{1} \mathbf{A}+y_{1} \mathbf{B}+z_{1}$ $\mathbf{C}=x_{2} \mathbf{A}+y_{2} \mathbf{B}+z_{2} \mathbf{C}$, prove that, necessarily, $x_{1}=x_{2}, y_{1}=y_{2}, z_{1}=z_{2}$.

7.50. $\quad$ Let $A B C D$ be any quadrilateral and points $P, Q, R$, and $S$ the midpoints of successive sides. Prove that (a) $P Q R S$ is a parallelogram and (b) the perimeter of $P Q R S$ is equal to the sum of the lengths of the diagonals of $A B C D$.

7.51. Prove that the medians of a triangle intersect at a point which is a trisection point of each median.

7.52. Find a unit vector in the direction of the resultant of vectors $\mathbf{A}=2 \mathbf{i}-\mathbf{j}+\mathbf{k}, \mathbf{B}=\mathbf{i}+\mathbf{j}+2 \mathbf{k}, \mathbf{C}=3 \mathbf{i}-2 \mathbf{j}+4 \mathbf{k}$.

$$
\text { Ans. }(6 \mathbf{i}-2 \mathbf{j}+7 \mathbf{k}) / \sqrt{89}
$$

\section*{The dot or scalar product}
7.53.

Evaluate $|(\mathbf{A}+\mathbf{B}) \cdot(\mathbf{A}-\mathbf{B})|$ if $\mathbf{A}=2 \mathbf{i}-3 \mathbf{j}+5 \mathbf{k}$ and $\mathbf{B}=3 \mathbf{i}+\mathbf{j}-2 \mathbf{k}$.

Ans. 24

7.54. Verify the consistency of the law of cosines for a triangle. [Hint: Take the sides of $\mathbf{A}, \mathbf{B}, \mathbf{C}$ where $\mathbf{C}=\mathbf{A}-\mathbf{B}$. Then use $\mathbf{C} \cdot \mathbf{C}=(\mathbf{A}-\mathbf{B}) \cdot(\mathbf{A}-\mathbf{B})$.

7.55. Find $a$ so that $2 \mathbf{i}-3 \mathbf{j}+5 \mathbf{k}$ and $3 \mathbf{i}+a \mathbf{j}-2 \mathbf{k}$ are perpendicular.

Ans. $a=-4 / 3$

7.56. If $\mathbf{A}=2 \mathbf{i}+\mathbf{j}+\mathbf{k}, \mathbf{B}=\mathbf{i}-2 \mathbf{j}+2 \mathbf{k}$ and $\mathbf{C}=3 \mathbf{i}-4 \mathbf{j}+2 \mathbf{k}$, find the projection of $\mathbf{A}+\mathbf{C}$ in the direction of $\mathbf{B}$.

Ans. $17 / 3$

7.57. A triangle has vertices at $A(2,3,1), B(-1,1,2)$, and $C(1,-2,3)$. Find (a) the length of the median drawn from $B$ to side $A C$ and (b) the acute angle which this median makes with side $B C$.\\
Ans. (a) $\frac{1}{2} \sqrt{26}$\\
(b) $\cos ^{-1} \sqrt{91} / 14$

7.58. Prove that the diagonals of a rhombus are perpendicular to each other.

7.59. Prove that the vector $(A \mathbf{B}+B \mathbf{A}) /(A+B)$ represents the bisector of the angle between $\mathbf{A}$ and $\mathbf{B}$.

\section*{The cross or vector product}
7.60. If $\mathbf{A}=2 \mathbf{i}-\mathbf{j}+\mathbf{k}$ and $\mathbf{B}=\mathbf{i}+2 \mathbf{j}-3 \mathbf{k}$, find $|(2 \mathbf{A}+\mathbf{B}) \times(\mathbf{A}-2 \mathbf{B})|$.

Ans. $5 \sqrt{3}$

7.61. Find a unit vector perpendicular to the plane of the vectors $\mathbf{A}=3 \mathbf{i}-2 \mathbf{j}+4 \mathbf{k}$ and $\mathbf{B}=\mathbf{i}+\mathbf{j}-2 \mathbf{k}$.

$$
\text { Ans. } \pm(2 \mathbf{j}+\mathbf{k}) / \sqrt{5}
$$

7.62. If $\mathbf{A} \times \mathbf{B}=\mathbf{A} \times \mathbf{C}$, does $\mathbf{B}=\mathbf{C}$ necessarily?

7.63. Find the area of the triangle with vertices $(2,-3,1),(1,-1,2),(-1,2,3)$.

$$
\text { Ans. } \frac{1}{2} \sqrt{3}
$$

7.64 Find the shortest distance from the point $(3,2,1)$ to the plane determine by $(1,1,0),(3,-1,1),(-1,0,2)$. Ans. 2

\section*{Triple products}
7.65. If $\mathbf{A}=2 \mathbf{i}+\mathbf{j}-3 \mathbf{k}, \mathbf{B}=\mathbf{i}-2 \mathbf{j}+\mathbf{k}, \mathbf{C}=-\mathbf{i}+\mathbf{j}-4$, find (a) $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})$, (b) $\mathbf{C} \cdot(\mathbf{A} \times \mathbf{B}),(\mathrm{c}) \mathbf{A} \times(\mathbf{B} \times \mathbf{C})$, and (d) $(\mathbf{A} \times \mathbf{B}) \times \mathbf{C}$.

Ans. (a) 20 (b) 20 (c) $8 \mathbf{i}-19 \mathbf{j}-\mathbf{k}$ (d) $25 \mathbf{i}-15 \mathbf{j}-10 \mathbf{k}$

7.66. Prove that (a) $\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})=\mathbf{B} \cdot(\mathbf{C} \times \mathbf{A})=\mathbf{C} \cdot(\mathbf{A} \times \mathbf{B})$ and (b) $\mathbf{A} \times(\mathbf{B} \times \mathbf{C})=\mathbf{B}(\mathbf{A} \cdot \mathbf{C})-\mathbf{C}(\mathbf{A} \cdot \mathbf{B})$.

7.67. Find an equation for the plane passing through $(2,-1,-2),(-1,2,-3),(4,1,0)$.

$$
\text { Ans. } 2 x+y-3 z=9
$$

7.68. Find the volume of the tetrahedron with vertices at $(2,1,1),(1,-1,2),(0,1,-1),(1,-2,1)$.

$$
\text { Ans. } \frac{4}{3}
$$

7.69. Prove that $(\mathbf{A} \times \mathbf{B}) \cdot(\mathbf{C} \times \mathbf{D})+(\mathbf{B} \times \mathbf{C}) \cdot(\mathbf{A} \times \mathbf{D})+(\mathbf{C} \times \mathbf{A}) \cdot(\mathbf{B} \times \mathbf{D})=0$.

\section*{Derivatives}
7.70 A particle moves along the space curve $\mathbf{r}=e^{-t} \cos t \mathbf{i}+e^{-t} \sin t \mathbf{j}+e^{-t} \mathbf{k}$. Find the magnitude of the (a) the velocity and (b) the acceleration at any time $t$.

Ans. (a) $\sqrt{3} e^{-1}$ (b) $\sqrt{5} e^{-1}$

7.71. Prove that $\frac{d}{d u}(\mathbf{A} \times \mathbf{B})=A \times \frac{d \mathbf{B}}{d u}+\frac{d \mathbf{A}}{d u} \times \mathbf{B}$ where $\mathbf{A}$ and $\mathbf{B}$ are differentiable functions of $u$.

7.72. Find a unit vector tangent to the space curve $x=t, y=t^{2} z=t^{3}$ at the point where $t=1$.

$$
\text { Ans. }(\mathbf{i}+2 \mathbf{j}+3 \mathbf{k}) / \sqrt{14}
$$

7.73. If $\mathbf{r}=\mathbf{a} \cos \omega t+\mathbf{b} \sin \omega t$, where $\mathbf{a}$ and $\mathbf{b}$ are any constant noncollinear vectors and $\omega$ is a constant scalar. prove that (a) $\mathrm{r}=\frac{d \mathbf{r}}{d \mathbf{r}}=\omega(\mathrm{a} \times \mathrm{b})$ and (b) $\frac{d^{2} \mathbf{r}}{d t^{2}}+\omega^{2} \mathbf{r}=0$.

7.74. If $\mathbf{A}=x^{2} \mathbf{i}-y \mathbf{j}+x z \mathbf{k}, \mathbf{B}=y \mathbf{i}+x \mathbf{j}-x y z \mathbf{k}$, and $\mathbf{C}=\mathbf{i}-y \mathbf{j}+x^{3} z \mathbf{k}$, find (a) $\frac{\partial^{2}}{\partial x \partial y}(\mathbf{A}+\mathbf{B})$ and\\
(b) $d[\mathbf{A} \cdot(\mathbf{B} \times \mathbf{C})]$ at the point $(1,-1,2)$.

$$
\text { Ans. (a) }-4 \mathbf{i}+8 \mathbf{j} \text { (b) } 8 d x
$$

7.75. If $\mathrm{R}=x^{2} y \mathbf{i}-2 y^{2} z \mathbf{j}+x y^{2} z^{2} \mathbf{k}$, find $\left|\frac{\partial^{2} \mathbf{B}}{\partial x^{2}} \times \frac{\partial^{2} \mathbf{R}}{\partial y^{2}}\right|$ at the point $(2,1,-2)$.

Ans. $16 \sqrt{5}$

\section*{Gradient, divergence, and curl}
7.76. If $U, V, \mathbf{A}, \mathbf{B}$ have continuous partial derivatives, prove that (a) $\nabla(U+V)=\nabla U+\nabla V$, (b) $\nabla \cdot(\mathbf{A}+\mathbf{B})$ $=\nabla \cdot \mathbf{A}+\nabla \cdot \mathbf{B}$ and (c) $\nabla \times(\mathbf{A}+\mathbf{B})=\nabla \times \mathbf{A}+\nabla \times \mathbf{B}$.

7.77. If $\phi=x y+y z+z x$ and $\mathbf{A}=x^{2} y \mathbf{i}+y^{2} z \mathbf{j}+z^{2} x \mathbf{k}$, find (a) $\mathbf{A} \cdot \nabla \phi$, (b) $\phi \nabla \cdot \mathbf{A}$, and (c) $(\nabla \phi) \times \mathbf{A}$ at the point $(3,-1,2)$.

Ans. (a) 25 , (b) 2 , (c) $56 \mathbf{i}-30 \mathbf{j}+47 \mathbf{k}$

7.78. Show that $\nabla \times\left(r^{2} \mathbf{r}\right)=0$ where $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$ and $r=|\mathbf{r}|$.

7.79. Prove that (a) $\nabla \times(U \mathbf{A})=(\nabla U) \times \mathbf{A}+U(\nabla \times \mathbf{A})$ and (b) $\nabla \cdot(\mathbf{A} \times \mathbf{B})=\mathbf{B} \cdot(\nabla \times \mathbf{A})-\mathbf{A} \cdot(\nabla \times \mathbf{B})$.

7.80. Prove that curl grad $u=0$, stating appropriate conditions on $U$.

7.81. Find a unit normal to the surface $x^{2} y-2 x z+2 y^{2} z^{4}=10$ at the point $(2,1,-1)$.

$$
\text { Ans. } \pm(3 \mathbf{i}+4 \mathbf{j}-6 \mathbf{k}) \sqrt{61}
$$

7.82. If $\mathbf{A}=3 x z^{2} \mathbf{i}-y z \mathbf{j}+(x+2 z) \mathbf{k}$, find $\operatorname{curl} \mathbf{A}$.

$$
\text { Ans. }-6 x \mathbf{i}+(6 z-1) \mathbf{k}
$$

7.83. (a) Prove that $\nabla \times(\nabla \times \mathbf{A})=-\nabla^{2} \mathbf{A}+\nabla(\nabla \cdot \mathbf{A})$. (b) Verify the result in (a) if $\mathbf{A}$ is given as in problem 7.82.

\section*{Jacobians and curvilinear coordinates}
7.84. Prove that $\left|\frac{\partial(x, y, z)}{\partial\left(u_{1}, u_{2}, u_{3}\right)}\right|=\left|\frac{\partial \mathrm{r}}{\partial u_{1}} \cdot \frac{\partial \mathrm{r}}{\partial u_{2}} \times \frac{\partial \mathrm{r}}{\partial u_{3}}\right|$.

7.85. Express (a) $\operatorname{grad} \Phi$, (b) div $\mathbf{A}$, and (c) $\nabla^{2} \Phi$ in spherical coordinates.

Ans. (a) $\frac{\partial \Phi}{\partial r} \mathrm{e}_{1}+\frac{1}{r} \frac{\partial \Phi}{\partial \theta} \mathrm{e}_{2}+\frac{1}{r \sin \theta} \frac{\partial \Phi}{\partial \phi} \mathrm{e}_{3}$

(b) $\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} A_{1}\right)+\frac{1}{r \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta A_{2}\right)+\frac{1}{r \sin \theta} \frac{\partial A^{3}}{\partial \phi}$ where $\mathbf{A}=A_{1} \mathrm{e}_{1}+A_{2} \mathrm{e}_{2}+A_{3} \mathrm{e}_{3}$

(c) (c) $\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} \frac{\partial \Phi}{\partial r}\right)+\frac{1}{r^{2} \sin \theta} \frac{\theta}{\partial \theta}\left(\sin \theta \frac{\partial \Phi}{\partial \theta}\right)+\frac{1}{r^{2} \sin ^{2} \theta} \frac{\partial^{2} \Phi}{\partial \phi^{2}}$

7.86. The transformation from rectangular to parabolic cylindrical coordinates is defined by the equations $x=1 / 2$ $\left(u^{2}-v^{2}\right), y=u v, z=z$. (a) Prove that the system is orthogonal. (b) Find $d s^{2}$ and the scale factors. (c) Find the Jacobian of the transformation and the volume element.

Ans. (b) $\left.d s^{2}=\left(u^{2}+v^{2}\right) d u^{2}+(u)^{2}+v^{2}\right) d v^{2}+d z^{2}, h_{1}=h_{2}=\sqrt{u^{2}+v^{2}}, h_{3}=1$

(c) $u^{2}+v^{2},\left(u^{2}+v^{2}\right) d u d v d z$

7.87. Write (a) $\nabla^{2} \Phi$ and (b) div $\mathbf{A}$ in parabolic cylindrical coordinates.

Ans. (a) $\nabla^{2} \Phi=\frac{1}{u^{2}+v^{2}}\left(\frac{\partial^{2} \Phi}{\partial u^{2}}+\frac{\partial^{2} \Phi}{\partial v^{2}}\right)+\frac{\partial^{2} \Phi}{\partial z^{2}}$

(b) $\operatorname{div} \mathbf{A}=\frac{1}{u^{2}+v^{2}}\left\{\frac{\partial}{\partial u}\left(\sqrt{u^{2}+v^{2}} A_{1}\right)+\frac{\partial}{\partial v}\left(\sqrt{u^{2}+v^{2}} A_{2}\right)\right\}+\frac{\partial A_{3}}{\partial z}$

7.88. Prove that for orthogonal curvilinear coordinates,

$$
\nabla \Phi=\frac{\mathrm{e}_{1}}{h_{1}} \frac{\partial \Phi}{\partial u_{1}}+\frac{\mathrm{e}_{2}}{h_{2}} \frac{\partial \Phi}{\partial u_{2}}+\frac{\mathrm{e}_{3}}{h_{3}} \frac{\partial \Phi}{\partial u_{3}}
$$

(Hint: Let $\nabla \Phi=a_{1} \mathbf{e}_{1}+a_{2} \mathbf{e}_{2}+a_{3} \mathbf{e}_{3}$ and use the fact that $d \Phi=\nabla \Phi \cdot d \mathbf{r}$ must be the same in both rectangular and curvilinear coordinates.)

7.89. Give a vector interpretation to the theorem in Problem 6.35.

\section*{Miscellaneous problems}
7.90. If $\mathbf{A}$ is a differentiable function of $u$ and $|\mathbf{A}(u)|=1$, prove that $d \mathbf{A} / d u$ is perpendicular to $\mathbf{A}$.

7.91. Prove formulas 6, 7, and 8 on Page 171.

7.92. If $\rho$ and $\phi$ are polar coordinates and $A, B, n$ are any constants, prove that $U=\rho^{n}(A \cos n \phi+B \sin n \phi)$ satisfies Laplace's equation.

7.93. If $V=\frac{2 \cos \theta+3 \sin ^{3} \theta \cos \phi}{r^{2}}$, find $\nabla^{2} V$.

Ans. $\frac{6 \sin \theta \cos \phi\left(4-5 \sin ^{2} \theta\right)}{r^{4}}$

7.94. Find the most general function of (a) the cylindrical coordinate $\rho$, (b) the spherical coordinate $r$, and (c) the spherical coordinate $\theta$ which satisfies Laplace's equation.

Ans. (a) $A+B \ln \rho$ (b) $A+B / r$ (c) $A+B \ln (\csc \theta-\cot \theta)$ where $A$ and $B$ are any constants

7.95. Let $\mathbf{T}$ and $\mathbf{N}$ denote, respectively, the unit tangent vector and unit principal normal vector to a space curve $\mathbf{r}$ $=\mathbf{r}(u)$, where $\mathbf{r}(u)$ is assumed differentiable. Define a vector $\mathbf{B}=\mathbf{T}=\mathbf{T} \times \mathbf{N}$ called the unit binormal vector to the space curve. Prove that

$$
\frac{d \mathbf{T}}{d s}=\kappa \mathbf{N}, \frac{d \mathbf{B}}{d s}=-\tau \mathbf{N}, \quad \frac{d \mathbf{N}}{d s}=\tau \mathbf{B}-\kappa \mathbf{T}
$$

These are called the Frenet-Serret formulas and are of fundamental importance in differential geometry. In these formulas $k$ is called the curvature, $\tau$ is called the torsion; and the reciprocals of these, $\rho=1 / k$ and $\sigma=$ $1 / \tau$, are called the radius of curvature and radius of torsion, respectively.

7.96. (a) Prove that the radius of curvature at any point of the plane curve $y=f(x), z=0$ where $f(x)$ is differentiable, is given by

$$
\rho=\left|\frac{\left(1+y^{\prime 2}\right)^{3 / 2}}{y^{\prime \prime}}\right|
$$

(b) Find the radius of curvature at the point $(\pi / 2,1,0)$ of the curve $y=\sin x, z=0$.

Ans. (b) $2 \sqrt{2}$

7.97. Prove that the acceleration of a particle along a space curve is given respectively in (a) cylindrical and (b) spherical coordinates by

$$
\begin{gathered}
\left(\ddot{\rho}-\rho \dot{\phi}^{2}\right) \mathbf{e}_{\rho}+(\rho \ddot{\phi}+2 \dot{\rho} \dot{\phi}) \mathbf{e}_{\phi}+\ddot{z} \mathbf{e}_{z} \\
\left(\ddot{r}-r \dot{\theta}^{2}-r \dot{\phi}^{2} \sin ^{2} \theta\right) \mathbf{e}_{r}+\left(r \ddot{\theta}+2 \ddot{r} \ddot{\theta}-r \dot{\phi}^{2} \sin \theta \cos \theta\right) \mathbf{e}_{\theta}+(2 \dot{r} \dot{\phi} \sin \theta+2 r \dot{\theta} \dot{\phi} \cos \theta+r \ddot{\phi} \sin \theta) e_{\phi}
\end{gathered}
$$

where dots denote time derivatives and $\mathbf{e}_{\rho}, \mathbf{e}_{\phi}, \mathbf{e}_{z}, \mathbf{e}_{r}, \mathbf{e}_{\theta}, \mathbf{e}_{\phi}$ are unit vectors in the directions of increasing $\rho, \phi$, $z, r, \theta, \phi$, respectively.

7.98. Let $\mathbf{E}$ and $\mathbf{H}$ be two vectors assumed to have continuous partial derivatives (of second order at least) with respect to position and time. Suppose further that $\mathbf{E}$ and $\mathbf{H}$ satisfy the equations


\begin{equation*}
\nabla \cdot \mathbf{E}=0, \quad \nabla \cdot \mathbf{H}=0, \quad \nabla \times \mathbf{E}=-\frac{1}{c} \frac{\partial \mathbf{H}}{\partial t}, \quad \nabla \times \mathbf{H}=\frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \tag{1}
\end{equation*}


prove that $\mathbf{E}$ and $\mathbf{H}$ satisfy the equation


\begin{equation*}
\nabla^{2} \psi=\frac{1}{c^{2}} \frac{\partial^{2} \psi}{\partial t^{2}} \tag{2}
\end{equation*}


where $\psi$ is a generic meaning and, in particular, can represent any component of $\mathbf{E}$ or $\mathbf{H}$.

[The vectors $\mathbf{E}$ and $\mathbf{H}$ are called electric and magnetic field vectors in electromagnetic theory. Equations (1) are a special case of Maxwell's equations. The result (2) led Maxwell to the conclusion that light was an electromagnetic phenomena. The constant $c$ is the velocity of light.]

7.99. Use the relations in Problem 7.98 to show that

$$
\frac{\partial}{\partial t}\left\{\frac{1}{2}\left(E^{2}+H^{2}\right)\right\}+c \nabla \cdot(\mathbf{E} \times \mathbf{H})=0
$$

7.100. Let $A_{1}, A_{2}, A_{3}$ be the components of vector $\mathbf{A}$ in an $x y z$ rectangular coordinate system with unit vectors $\mathbf{i}_{1}, \mathbf{i}_{2}$, $\mathbf{i}_{3}$ (the usual $\mathbf{i}, \mathbf{j}$, $\mathbf{k}$ vectors), and $A^{\prime}{ }_{1}, A^{\prime}{ }_{2}, A^{\prime}{ }_{3}$ the components of $\mathbf{A}$ in an $x^{\prime} y^{\prime} z^{\prime}$ rectangular coordinate system which has the same origin as the $x y z$ system but is rotated with respect to it and has the unit vectors $\mathbf{i}_{1}^{\prime}, \mathbf{i}_{2}^{\prime}, \mathbf{i}_{3}^{\prime}$. Prove that the following relations (often called invariance relations) must hold:

$$
A_{n}=l_{1 n} A_{1}^{\prime}+l_{2 n} A_{2}^{\prime}+l_{3 n} A_{3}^{\prime} \quad n=1,2,3
$$

where $\mathbf{i}_{m}^{\prime} \cdot \mathbf{i}_{n}=l_{m n}$.

7.101. If $\mathbf{A}$ is the vector of Problem 7.100, prove that the divergence of $\mathbf{A}(\nabla \cdot \mathbf{A})$ is an invariant (often called a scalar invariant); i.e., prove that

$$
\frac{\partial A_{1}^{\prime}}{\partial x^{\prime}}+\frac{\partial A_{2}^{\prime}}{\partial y^{\prime}}+\frac{\partial A_{3}^{\prime}}{\partial z^{\prime}}=\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}
$$

The results of this and the preceding problem express an obvious requirement that physical quantities must not depend on coordinate systems in which they are observed. Such ideas when generalized lead to an important subject called tensor analysis, which is basic to the theory of relativity.

7.102. Prove that (a) $\mathbf{A} \cdot \mathbf{B}$, (b) $\mathbf{A} \times \mathbf{B}$, and (c) $\nabla \times \mathbf{A}$ are invariant under the transformation of Problem 7.100.

7.103. If $u_{1}, u_{2}, u_{3}$ are orthogonal curvilinear coordinates, prove that

$$
\text { (a) } \frac{\partial\left(u_{1}, u_{2}, u_{3}\right)}{\partial(x, y, z)}=\nabla u_{1} \cdot \nabla u_{2} \times \nabla u_{3} \text { (b) }\left(\frac{\partial \mathbf{r}}{\partial u_{1}} \cdot \frac{\partial \mathbf{r}}{\partial u_{2}} \times \frac{\partial \mathbf{r}}{\partial u_{3}}\right)\left(\nabla u_{1} \cdot \nabla u_{2} \times \nabla u_{3}\right)=1
$$

and give the significance of these in terms of Jacobians.

7.104. Use the axiomatic approach to vectors to prove relation 8 on Page 167.

7.105. A set of $n$ vectors $\mathbf{A}_{1}, \mathbf{A}_{2}, \ldots, \mathbf{A}_{n}$ is called linearly dependent if there exists a set of scalars $c_{1}, c_{2}, \ldots, c_{n}$ not all zero such that $c_{1} \mathbf{A}_{1}+c_{2} \mathbf{A}_{2}+\cdots+c_{n} \mathbf{A}_{n}=\mathbf{0}$ identically; otherwise, the set is called linearly independent. (a) Prove that the vectors $\mathbf{A}_{1}=2 \mathbf{i}-3 \mathbf{j}+5 \mathbf{k}, \mathbf{A}_{2}=\mathbf{i}+\mathbf{j}-2 \mathbf{k}, \mathbf{A}_{3}=3 \mathbf{i}-7 \mathbf{j}+12 \mathbf{k}$ are linearly dependent. (b) Prove that any four three-dimensional vectors are linearly dependent. (c) Prove that a necessary and sufficient condition that the vectors $\mathbf{A}_{1}=a_{1} \mathbf{i}+b_{1} \mathbf{j}+c_{1} \mathbf{k}, \mathbf{A}_{2}=a_{2} \mathbf{i}+b_{2} \mathbf{j}+c_{2} \mathbf{k}$, and $\mathbf{A}_{3}=a_{3} \mathbf{i}+b_{3} \mathbf{j}+c_{3} \mathbf{k}$ be linearly independent is that $\mathbf{A}_{1} \cdot \mathbf{A}_{2} \times \mathbf{A}_{3} \neq 0$. Give a geometrical interpretation of this.

7.106. A complex number can be defined as an ordered pair ( $a, b)$ of real numbers $a$ and $b$ subject to certain rules of operation for addition and multiplication. (a) What are these rules? (b) How can the rules in (a) be used to define subtraction and division? (c) Explain why complex numbers can be considered as two-dimensional vectors. (d) Describe similarities and differences between various operations involving complex numbers and the vectors considered in this chapter.

\section*{CHAPTER 8}
\section*{Applications of Partial Derivatives}
\section*{Applications to Geometry}
The theoretical study of curves and surfaces began more than two thousand years ago when Greek philosopher-mathematicians explored the properties of conic sections, helixes, spirals, and surfaces of revolution generated from them. While applications were not on their minds, many practical consequences evolved. These included representation of the elliptical paths of planets about the sun, employment of the focal properties of paraboloids, and use of the special properties of helixes to construct the double helical model of DNA.

The analytic tool for studying functions of more than one variable is the partial derivative. Surfaces are a geometric starting point, since they are represented by functions of two independent variables. Vector forms of many of these concepts were introduced in Chapter 7. In this chapter, corresponding coordinate equations are exhibited.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-206}
\end{center}

Figure 8.1

\begin{enumerate}
  \item Tangent Plane to a Surface Let $F(x, y, z)=0$ be the equation of a surface $S$ such as that shown in Figure 8.1. Assume that $F$, and all other functions in this chapter are continuously differentiable unless otherwise indicated. Suppose we wish to find the equation of a tangent plane to $S$ at the point $P\left(x_{0}, y_{0}, z_{0}\right)$. A vector normal to $S$ at this point is $\mathbf{N}_{0}=\left.\nabla F\right|_{P}$, the subscript $P$ indicating that the gradient is to be evaluated at the point $P\left(x_{0}, y_{0}, z_{0}\right)$.
\end{enumerate}

If $\mathbf{r}_{0}$ and $\mathbf{r}$ are tangent, the vectors drawn, respectively, from $O$ to $P\left(x_{0}, y_{0}, z_{0}\right)$ and $Q(x, y, z)$ on the tangent plane, the equation of the plane is


\begin{equation*}
\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \mathbf{N}_{0}=\left.\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \nabla F\right|_{P}=0 \tag{1}
\end{equation*}


since $\mathbf{r}-\mathbf{r}_{0}$ is perpendicular to $\mathbf{N}_{0}$.

In rectangular form this is


\begin{equation*}
\left.\frac{\partial F}{\partial x}\right|_{P}\left(x-x_{0}\right)+\left.\frac{\partial F}{\partial y}\right|_{P}\left(y-y_{0}\right)+\left.\frac{\partial F}{\partial z}\right|_{P}\left(z-z_{0}\right)=0 \tag{2}
\end{equation*}


In case the equation of the surface is given in orthogonal curvilinear coordinates in the form $F\left(u_{1}, u_{2}, u_{3}\right)$ $=0$, the equation of the tangent plane can be obtained using the result on Page 172 for the gradient in these coordinates. See Problem 8.4.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Normal Line to a Surface. Suppose we require equations for the normal line to the surface $S$ at $P\left(x_{0}\right.$, $\left.y_{0}, z_{0}\right)$, i.e., the line perpendicular to the tangent plane of the surface at $P$. If we now let $\mathbf{r}$ be the vector drawn from $O$ in Figure 8.1 to any point $(x, y, z)$ on the normal $\mathbf{N}_{0}$, we see that $\mathbf{r}-\mathbf{r}_{0}$ is collinear with $\mathbf{N}_{0}$, and so the required condition is
\end{enumerate}


\begin{equation*}
\left(\mathbf{r}-\mathbf{r}_{0} \times \mathbf{N}_{0}=\left(\mathbf{r}-\mathbf{r}_{0}\right) \times \times\left.\nabla F\right|_{P}=\mathbf{0}\right. \tag{3}
\end{equation*}


By expressing the cross product in the determinant form

we find that

$$
\left|\begin{array}{ccc}
i & j & k \\
x-x_{0} & y-y_{0} & z-z_{0} \\
\left.F_{x}\right|_{P} & \left.F_{y}\right|_{P} & \left.F_{z}\right|_{P}
\end{array}\right|
$$


\begin{equation*}
\frac{x-x_{0}}{\left.\frac{\partial F}{\partial x}\right|_{P}}=\frac{y-y_{0}}{\left.\frac{\partial F}{\partial y}\right|_{P}}=\frac{z-z_{0}}{\left.\frac{\partial F}{\partial z}\right|_{P}} \tag{4}
\end{equation*}


Setting each of these ratios equal to a parameter (such as $t$ or $u$ ) and solving for $x, y$, and $z$ yields the parametric equations of the normal line.

The equations for the normal line can also be written when the equation of the surface is expressed in orthogonal curvilinear coordinates. [See Problem 8.1(b).]

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Tangent Line to a Curve Let the parametric equations of curve $C$ of Figure 8.2 be $x=f(u), y=g(u)$, $z=h(u)$, where we shall suppose, unless otherwise indicated, that $f, g$, and $h$ are continuously differentiable. We wish to find equations for the tangent line to $C$ at the point $P\left(x_{0}, y_{0}, z_{0}\right)$ where $u=u_{0}$.
\end{enumerate}

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-207}
\end{center}

Fig. 8.2

If $\mathbf{R}=f(u) \mathbf{i}+g(u) \mathbf{j}+h(u) \mathbf{k}$, then a vector tangent to $C$ at the point $P$ is given by $\mathbf{T}_{0}=\left.\frac{d \mathbf{R}}{d u}\right|_{P}$. If $\mathbf{r}_{0}$ and $\mathbf{r}$ denote the vectors drawn respectively from $O$ to $P\left(x_{0}, y_{0}, z_{0}\right)$ and $Q(x, y, z)$ on the tangent line, then $\mathbf{r}-\mathbf{r}_{0}$ is collinear with $\mathbf{T}_{0}$. Thus,

In rectangular form this becomes


\begin{equation*}
\left(\mathbf{r}-\mathbf{r}_{0}\right) \times \mathbf{T}_{0}=\left(\mathbf{r}-\mathbf{r}_{0}\right) \times\left.\frac{d \mathbf{R}}{d u}\right|_{P}=0 \tag{5}
\end{equation*}



\begin{equation*}
\frac{x-x_{0}}{f^{\prime}\left(u_{0}\right)}=\frac{y-y_{0}}{g^{\prime}\left(u_{0}\right)}=\frac{z-z_{0}}{h^{\prime}\left(u_{0}\right)} \tag{6}
\end{equation*}


The parametric form is obtained by setting each ratio equal to $u$.

If the curve $C$ is given as the intersection of two surfaces with equations $F(x, y, z)=0$ and $G(x, y, z)=0$, observe that $\nabla F \times \nabla G$ has the direction of the line of intersection of the tangent planes; therefore, the corresponding equations of the tangent line are

\[
\frac{x-x_{0}}{\left|\begin{array}{cc}
F_{y} & F_{z}  \tag{7}\\
G_{y} & G_{z}
\end{array}\right|_{P}}=\frac{y-y_{0}}{\left|\begin{array}{cc}
F_{z} & F_{x} \\
G_{z} & G_{x}
\end{array}\right|_{P}}=\frac{z-z_{0}}{\left|\begin{array}{cc}
F_{x} & F_{y} \\
G_{x} & G_{y}
\end{array}\right|_{P}}
\]

Note that the determinants in Equation (7) are Jacobians. A similar result can be found when the surfaces are given in terms of orthogonal curvilinear coordinates.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Normal Plane to a Curve Suppose we wish to find an equation for the normal plane to curve $C$ at $P\left(x_{0}\right.$, $y_{0}, z_{0}$ ) in Figure 8.2 (i.e., the plane perpendicular to the tangent line to $C$ at this point). Letting $\mathbf{r}$ be the vector from $O$ to any point $(x, y, z)$ on this plane, it follows that $\mathbf{r}-\mathbf{r}_{0}$ is perpendicular to $\mathbf{T}_{0}$. Then the required equation is
\end{enumerate}


\begin{equation*}
\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \mathbf{T}_{0}=\left.\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \frac{d \mathbf{R}}{d u}\right|_{P}=0 \tag{8}
\end{equation*}


When the curve has parametric equations $x=f(u), y=g(u), z=h(u)$ this becomes


\begin{equation*}
f^{\prime}\left(u_{0}\right)\left(x-x_{0}\right)+g^{\prime}\left(u_{0}\right)\left(y-y_{0}\right)+h^{\prime}\left(u_{0}\right)\left(z-z_{0}\right)=0 \tag{9}
\end{equation*}


Furthermore, when the curve is the intersection of the implicitly defined surfaces

$$
F(x, y, z)=0 \text { and } G(x, y, z)=0
$$

then

\[
\left|\begin{array}{ll}
F_{y} & F_{z}  \tag{10}\\
G_{y} & G_{z}
\end{array}\right|_{\mathrm{P}}\left(x-x_{0}\right)+\left|\begin{array}{cc}
F_{z} & F_{x} \\
G_{z} & G_{x}
\end{array}\right|_{P}\left(y-y_{0}\right)+\left|\begin{array}{cc}
F_{x} & F_{y} \\
G_{x} & G_{y}
\end{array}\right|_{\mathrm{P}}\left(z-z_{0}\right)=0
\]

\begin{enumerate}
  \setcounter{enumi}{4}
  \item Envelopes Solutions of differential equations in two variables are geometrically represented by oneparameter families of curves. Sometimes such a family characterizes a curve called an envelope.
\end{enumerate}

For example, the family of all lines (see Problem 8.9) one unit from the origin may be represented by $x$ $\sin \alpha-y \cos \alpha-1=0$, where $\alpha$ is a parameter. The envelope of this family is the circle $x^{2}+y^{2}=1$.

If $\phi(x, y, z)=0$, is a one-parameter family of curves in the $x y$ plane, there may be a curve $E$ which is tangent at each point to some member of the family and such that each member of the family is tangent to $E$. If $E$ exists, its equation can be found by solving simultaneously the equations


\begin{equation*}
\phi(x, y, \alpha)=0, \phi\left({ }_{\alpha}(x, y, \alpha)=0\right. \tag{11}
\end{equation*}


and $E$ is called the envelope of the family.

The result can be extended to determine the envelope of a one-parameter family of surfaces $\phi(x, y, z, \alpha)$. This envelope can be found from


\begin{equation*}
\phi(x, y, z, \alpha)=0, \phi_{\alpha}(x, y, z, \alpha)=0 \tag{12}
\end{equation*}


Extensions to two- (or more) parameter families can be made.

\section*{Directional Derivatives}
Suppose $F(x, y, z)$ is defined at a point $(x, y, z)$ on a given space curve $C$. Let $F(x+\Delta x, y+\Delta y, z+\Delta z)$ be the value of the function at a neighboring point on $C$ and let $\Delta s$ denote the length of arc of the curve between those points. Then


\begin{equation*}
\lim _{\Delta s \rightarrow 0} \frac{\Delta F}{\Delta s}=\lim _{\Delta s \rightarrow 0} \frac{F(x+\Delta x, y+\Delta y, z+\Delta z)-F(x, y, z)}{\Delta s} \tag{13}
\end{equation*}


if it exists, is called the directional derivative of $F$ at the point $(x, y, z)$ along the curve $C$ and is given by


\begin{equation*}
\frac{d F}{d s}=\frac{\partial F}{\partial x} \frac{d x}{d s}+\frac{\partial F}{\partial y} \frac{d y}{d s}+\frac{\partial F}{\partial z} \frac{d z}{d s} \tag{14}
\end{equation*}


In vector form this can be written


\begin{equation*}
\frac{d F}{d s}=\left(\frac{\partial F}{\partial x} \mathbf{i}+\frac{\partial F}{\partial y} \mathbf{j}+\frac{\partial F}{\partial z} \mathbf{k}\right) \cdot\left(\frac{d x}{d s} \mathbf{i}+\frac{d y}{d s} \mathbf{j}+\frac{d z}{d s} \mathbf{k}\right)=\nabla F \cdot \frac{d r}{d s}=\nabla F \cdot \mathbf{T} \tag{15}
\end{equation*}


from which it follows that the directional derivative is given by the component of $\nabla F$ in the direction of the tangent to $C$.

Thus, the maximum value of the directional derivative is given by $|\nabla F|$ and these maxima occur in directions normal to the surfaces $F(x, y, z)=c$ is (where $c$ is any constant), which are sometimes called equipotential surfaces or level surfaces.

\section*{Differentiation Under the Integral Sign}
Let


\begin{equation*}
\phi(\alpha)=\int_{u_{1}}^{u_{2}} f(x, \alpha) d x \quad a \leqq \alpha \leqq b \tag{16}
\end{equation*}


where $u_{1}$ and $u_{2}$ may depend on the parameter $\alpha$. Then


\begin{equation*}
\frac{d \phi}{d \alpha}=\int_{u_{2}}^{u_{1}} \frac{\partial f}{\partial \alpha} d x+f\left(u_{2}, \alpha\right) \frac{d u_{2}}{d \alpha}-f\left(u_{1}, \alpha\right) \frac{d u_{1}}{d \alpha} \tag{17}
\end{equation*}


for $a \leqq \alpha \leqq b$, if $f(x, \alpha)$ and $\partial f / \partial \alpha$ are continuous in both $x$ and $\alpha$ in some region of the $x \alpha$ plane including $u_{1} \leqq x \leqq u_{2}, a \leqq \alpha \leqq b$ and if $u_{1}$ and $u_{2}$ are continuous and have continuous derivatives for $a \leqq \alpha \leqq b$.

In case $u_{1}$ and $u_{2}$ are constants, the last two terms of Equation (17) are zero.

The result (17), called Leibniz's rule, is often useful in evaluating definite integrals (see Problems 8.15 and 8.29).

\section*{Integration Under the Integral Sign}
If $\phi(\alpha)$ is defined by Equation (10) and $f(x, \alpha)$ is continuous in $x$ and $\alpha$ in a region including $u_{1} \leqq x \leqq u_{2}, a$ $\leqq x \leqq b$, then if $u_{1}$ and $u_{2}$ are constants,


\begin{equation*}
\int_{a}^{b} \phi(\alpha) d \alpha=\int_{a}^{b}\left\{\int_{u_{1}}^{u_{2}} f(x, \alpha) d x\right\} d \alpha=\int_{u_{1}}^{u_{2}}\left\{\int_{a}^{b} f(x, \alpha) d \alpha\right\} d x \tag{18}
\end{equation*}


The result is known as interchange of the order of integration or integration under the integral sign. (See Problem 8.18.)

\section*{Maxima and Minima}
In Chapter 4 we briefly examined relative extrema for functions of one variable. The general idea was that for points of the graph of $y=g(x)$ that were locally highest or lowest, the condition $g^{\prime}(x)=0$ was necessary. Such points $P_{0}\left(x_{0}\right)$ were called critical points. [See Figure 8.5 $(a$ and $b)$.] The condition $g^{\prime}(x)=0$ was useful in searching for relative maxima and minima, but it was not decisive. [See Figure 8.3(c).]

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-210}
\end{center}

Figure 8.3

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-210(1)}
\end{center}

Figure 8.4

To determine the exact nature of the function at a critical point, $\left.P_{0}, g^{\prime \prime}\left(x_{0}\right)\right)$ had to be examined.

$$
\begin{array}{rlrl}
>0 & & \text { counterclockwise rotation (relative minimum) } \\
g^{\prime \prime}\left(x_{0}\right)<0 & \text { implied } & & \text { a clockwise rotation (relative maximum) } \\
=0 & & & \text { need for further investigation }
\end{array}
$$

This section describes the $\mathrm{n}$ ecessary and sufficient conditions for relative extrema of functions of two variables. Geometrically, we think of surfaces $S$ represented by $z=f(x, y)$. If at a point $P_{0}\left(x_{0}, y_{0}\right)$ then $f_{x}(x$, $\left.y_{0}\right)=0$, means that the curve of intersection of $S$ and the plane $y=y_{0}$ has a tangent parallel to the $x$ axis. Similarly, $f_{y}\left(x_{0}, y_{0}\right)=0$ indicates that the curve of intersection of $S$ and the cross section $x=x_{0}$ has a tangent parallel the $y$ axis. (See Problem 8.20.)

Thus,

$$
f_{x}\left(x, y_{0}\right)=0, f_{y}\left(x_{0}, y\right)=0
$$

are necessary conditions for a relative extrema of $z=f(x, y)$ at $P_{0}$; however, they are not sufficient because there are directions associated with a rotation through $360^{\circ}$ that have not been examined. Of course, no differentiation between relative maxima and relative minima has been made. (See Figure 8.4.)

A very special form $f_{x y}-f_{x} f_{y}$, invariant under plane rotation and capable of characterizing the roots of a quadratic equation $A x^{2}+2 B x+C=0$, allows us to form sufficient conditions for relative extrema. (See Problem 8.21.)

A point $\left(x_{0}, y_{0}\right)$ is called a relative maximum point or relative minimum point of $f(x, y)$ respectively according as $f\left(x_{0}+h, y_{0}+k\right)<f\left(x_{0}, y_{0}\right)$ or $f\left(x_{0}+h, y_{0}+k\right)>f\left(x_{0}, y_{0}\right)$ for all $h$ and $k$ such that $0<|h|<\delta, 0<|k|$ $<\delta$ where $\delta$ is a sufficiently small positive number.

A necessary condition that a differentiable function $f(x, y)$ have a relative maximum or minimum is


\begin{equation*}
\frac{\partial f}{\partial x}=0, \quad \frac{\partial f}{\partial y}=0 \tag{19}
\end{equation*}


If $\left(x_{0}, y_{0}\right)$ is a point (called a critical point) satisfying Equations (19) and if $\Delta$ is defined by

then


\begin{equation*}
\Delta=\left.\left\{\left(\frac{\partial^{2} f}{\partial x^{2}}\right)\left(\frac{\partial^{2} f}{\partial y^{2}}\right)-\left(\frac{\partial^{2} f}{\partial x \partial y}\right)^{2}\right\}\right|_{\left(x_{0}, y_{0}\right)} \tag{20}
\end{equation*}


\begin{enumerate}
  \item $\quad\left(x_{0}, y_{0}\right)$ is a relative maximum point if $\Delta>0$ and $\left.\frac{\partial^{2} f}{\partial x^{2}}\right|_{\left(x_{0}, y_{0}\right)}<0\left(\right.$ or $\left.\left.\frac{\partial^{2} f}{\partial y^{2}}\right|_{\left(x_{0}, y_{0}\right)}<0\right)$.

  \item $\quad\left(x_{0}, y_{0}\right)$ is a relative minimum point of $\Delta>0$ and $\left.\frac{\partial^{2} f}{\partial x^{2}}\right|_{\left(x_{0}, y_{0}\right)}>0\left(\right.$ or $\left.\left.\frac{\partial^{2} f}{\partial y^{2}}\right|_{\left(x_{0}, y_{0}\right)}>0\right)$.

  \item $\left(x_{0}, y_{0}\right)$ is neither a relative maximum nor a relative minimum point if $\Delta<0$. If $\Delta<0,<0,\left(x_{0}, y_{0}\right)$ is sometimes called a saddle point.

  \item No information is obtained if $\Delta=0$ (in such case further investigation is necessary).

\end{enumerate}

\section*{Method of Lagrange Multipliers for Maxima and Minima}
A method for obtaining the relative maximum or minimum values of a function $F(x, y, z)$ subject to a constraint condition $\phi(x, y, z)=0$, consists of the formation of the auxiliary function


\begin{equation*}
G(x, y, z) \equiv F(x, y, z)+\lambda \phi(x, y, z) \tag{21}
\end{equation*}


subject to the conditions


\begin{equation*}
\frac{\partial G}{\partial x}=0, \quad \frac{\partial G}{\partial y}=0, \quad \frac{\partial G}{\partial z}=0 \tag{22}
\end{equation*}


which are necessary conditions for a relative maximum or minimum. The parameter $\lambda$, which is independent of $x, y, z$, is called a Lagrange multiplier.

The conditions in Equation (22) are equivalent to $\nabla G=0$, and, hence, $0=\nabla F+\lambda \nabla \phi$.

Geometrically, this means that $\nabla F$ and $\nabla \phi$ are parallel. This fact gives rise to the method of Lagrange multipliers in the following way.

Let the maximum value of $F$ on $\phi(x, y, z)=0$ be $A$ and suppose it occurs at $P_{0}\left(x_{0}, y_{0}, z_{0}\right)$. (A similar argument can be made for a minimum value of $F$.) Now consider a family of surfaces $F(x, y, z)=C$.

The member $F(x, y, z)=A$ passes through $P_{0}$, while those surface $F(x, y, z)=B$ with $B<A$ do not. (This choice of a surface, i.e., $f(x, y, z)=A$, geometrically imposes the condition $\phi(x, y, z)=0$ on $F$.) Since at $P_{0}$ the condition $0=\nabla F+\lambda \nabla \phi$ tells us that the gradients of $F(x, y, z)=A$ and $\phi(x, y, z)$ are parallel, we know that the surfaces have a common tangent plane at a point that is maximum for $F$. Thus, $\nabla G=0$ is a necessary condition for a relative maximum of $F$ at $P_{0}$. Of course, the condition is not sufficient. The critical point so determined may not be unique and it may not produce a relative extremum.

The method can be generalized. If we wish to find the relative maximum or minimum values of a function $F\left(x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right)$ subject to the constraint conditions $\phi\left(x_{1}, \ldots, x_{n}\right)=0, \phi_{2}\left(x_{1}, \ldots, x_{n}\right)=0, \ldots, \phi_{k}\left(x_{1}, \ldots\right.$, $\left.x_{n}\right)=0$, we form the we form the auxiliary function


\begin{equation*}
G\left(x_{1}, x_{2}, \ldots, x_{n}\right) \equiv F+\lambda_{1} \phi_{1}+\lambda_{2} \phi_{2}+\cdots+\lambda_{k} \phi_{k} \tag{23}
\end{equation*}


subject to the (necessary) conditions


\begin{equation*}
\frac{\partial G}{\partial x_{1}}=0, \quad \frac{\partial G}{\partial x_{2}}=0, \ldots \ldots \ldots, \frac{\partial G}{\partial x_{n}} \equiv 0 \tag{24}
\end{equation*}


where $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{k}$, which are independent of $x_{1}, x_{2}, \ldots, x_{n}$, are the Lagrange multipliers.

\section*{Applications to Errors}
The theory of differentials can be applied to obtain errors in a function of $x, y, z$, etc., when the errors in $x$, $y, z$, etc., are known. See Problem 8.28.

\section*{SOLVED PROBLEMS}
\section*{Tangent Plane And Normal Line To A Surface}
8.1. Find equations for the (a) tangent plane and (b) normal line to the surface $x^{2} y z+3 y^{2}=2 x z^{2}-8 z$ at the point $(1,2,-1)$.

(a) The equation of the surface is $F=x^{2} y z+3 y^{2}-2 x z^{2}+8 z=0$. A normal line to the surface at $(1,2,-1)$ is

$$
\begin{aligned}
\mathbf{N}_{0}=\left.\nabla F\right|_{(1,2,-1)} & =\left(2 x y z-2 z^{2}\right) \mathbf{i}+\left(x^{2} z+6 y\right) \mathbf{j}+\left.(x y-4 x z+8) \mathbf{k}\right|_{(1,2,-1)} \\
& =-6 \mathbf{i}+11 \mathbf{j}+14 \mathbf{k}
\end{aligned}
$$

Referring to Figure 8.1:

The vector from $O$ to any point $(x, y, z)$ on the tangent plane is $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$.

The vector from $O$ to the point $(1,2,-1)$ on the tangent plane is $\mathbf{r}_{0}=\mathbf{i}+2 \mathbf{j}-\mathbf{k}$.

The vector $\mathbf{r}-\mathbf{r}_{0}=(x-1) \mathbf{i}+(y-2) \mathbf{j}+(z+1) \mathbf{k}$ lies in the tangent plane and is thus perpendicular to $\mathbf{N}_{0}$.

Then the required equation is

$$
\begin{gathered}
\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \mathbf{N}_{0}=0 \quad \text { i.e., } \quad\{(x-1) \mathbf{i}+(y-2) \mathbf{j}+(z+1) \mathbf{k}\} \cdot\{-6 \mathbf{i}+11 \mathbf{j}+14 \mathbf{k}\}=0 \\
-6(x-1)+11(y-2)+14(z+1)=0 \text { or } 6 x-11 y-14 z+2=0
\end{gathered}
$$

(b) Let $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$ be the vector from $O$ to any point $(x, y, z)$ of the normal $\mathbf{N}_{0}$. The vector from $O$ to the point $(1,2,-1)$ on the normal is $\mathbf{r}_{0}=2 \mathbf{i}+2 \mathbf{j}-\mathbf{k}$. The vector $\mathbf{r}-\mathbf{r}_{0}=(x-1) \mathbf{i}+(y-2) \mathbf{j}+(z+1) \mathbf{k}$ is collinear with $\mathbf{N}_{0}$. Then

$$
\left(r-r_{0}\right) \times N_{0}=0 \quad \text { i.e., } \quad\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
x-1 & y-2 & x+1 \\
-6 & 11 & 14
\end{array}\right|=0
$$

which is equivalent to the equations

$$
11(x-1)=-6(y-2), 14(y-2)=11(z+1), 14(x-1)=-6(z+1)
$$

These can be written as

$$
\frac{x-1}{-6}=\frac{y-2}{11}=\frac{z+1}{14}
$$

often called the standard form for the equations of a line. By setting each of these ratios equal to the parameter $t$, we have

$$
x=1-6 t, \quad y=2+11 t, \quad z=14 t-1
$$

called the parametric equations for the line.

8.2. In what point does the normal line of Problem 8.1(b) meet the plane $x+3 y-2 z=10$ ?

Substituting the parametric equations of Problem 8.1(b), we have

$$
1-6 t+3(2+11 t)-2(14 t-1)=10 \text { or } t=-1
$$

Then $x=1-6 t=7, y=2+11 t=-9, z=14 t-1=-15$ and the required point is $(7,-9,-15)$.

8.3. Show that the surface $x^{2}-2 y z+y^{3}=4$ is perpendicular to any member of the family of surfaces $x^{2}+1=$ $(2-4 a) y^{2}+a z^{2}$ at the point of intersection $(1,-1,2)$.

Let the equations of the two surfaces be written in the form

$$
F=x^{2}-2 y z+y^{3}-4=0 \text { and } G=x^{2}+1-(2-4 a) y^{2}-a z^{2}=0
$$

Then

$$
\nabla F=2 x \mathbf{i}+\left(3 y^{2}-2 z\right) \mathbf{j}-2 y \mathbf{k}, \quad \nabla G=2 x \mathbf{i}-2(2-4 a) y \mathbf{j}-2 a z \mathbf{k}
$$

Thus, the normals to the two surfaces at $(1,-1,2)$ are given by

$$
\mathbf{N}_{1}=2 \mathbf{i}-\mathbf{j}+2 \mathbf{k}, \mathbf{N}_{2}=2 \mathbf{i}+2(2-4 a) \mathbf{j}-4 a \mathbf{k}
$$

Since $\mathbf{N}_{1} \cdot \mathbf{N}_{2}=(2)(2)-2(2-4 a)-(2)(4 a) \equiv 0$, it it follows that $\mathbf{N}_{1}$ and $\mathbf{N}_{2}$ are perpendicular for all $a$, and so the required result follows.

8.4. The equation of a surface is given in spherical coordinates by $F(r, \theta, \phi)=0$, where we suppose that $F$ is continuously differentiable. (a) Find an equation for the tangent plane to the surface at the point $\left(r_{0}, \theta_{0}, \phi_{0}\right)$. (b) Find an equation for the tangent plane to the surface $r=4 \cos \theta$ at the point $(2 \sqrt{2}, \pi / 4,3 \pi / 4)$. (c) Find a set of equations for the normal line to the surface in ( ) at the indicated point.

(a) The gradient of $\Phi$ in orthogonal curvilinear coordinates is

$$
\nabla \Phi=\frac{1}{h_{1}} \frac{\partial \Phi}{\partial u_{1}} \mathbf{e}_{1}+\frac{1}{h_{2}} \frac{\partial \Phi}{\partial u_{2}} \mathbf{e}_{2}+\frac{1}{h_{3}} \frac{\partial \Phi}{\partial u_{3}} \mathbf{e}_{3}
$$

where

$$
\mathbf{e}_{1}=\frac{1}{h_{1}} \frac{\partial \mathbf{r}}{\partial u_{1}}, \quad \mathbf{e}_{2}=\frac{1}{h_{2}} \frac{\partial \mathbf{r}}{\partial u_{2}}, \quad \mathbf{e}_{3}=\frac{1}{h_{3}} \frac{\partial \mathbf{r}}{\partial u_{3}}
$$

(see Pages 170 and 172).

In spherical coordinates $u_{1}=r, u_{2}=\theta, u_{3}=\phi, h_{1}=1, h_{2}=r, h_{3}=r \sin \theta$ and $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}=r \sin \theta \cos$ $\phi \mathbf{i}+r \sin \theta \sin \phi \mathbf{j}+r \cos \theta \mathbf{k}$.

Then

\[
\left\{\begin{array}{l}
\mathbf{e}_{1}=\sin \theta \cos \theta \mathbf{i}+\sin \theta \sin \theta \mathbf{j}+\cos \theta \mathbf{k}  \tag{1}\\
\mathbf{e}_{2}=\cos \theta \cos \phi \mathbf{i}+\cos \theta \sin \phi \mathbf{j}-\sin \theta \mathbf{k} \\
\mathbf{e}_{3}=-\sin \phi \mathbf{i}+\cos \phi \mathbf{j}
\end{array}\right.
\]

and


\begin{equation*}
\nabla F=\frac{\partial F}{\partial r} \mathrm{e}_{1}+\frac{1}{r} \frac{\partial F}{\partial \theta} \mathrm{e}_{2}+\frac{1}{r \sin \theta} \frac{\partial F}{\partial \phi} \mathrm{e}_{3} \tag{2}
\end{equation*}


As on Page 196, the required equation is $\left.\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \nabla F\right|_{P}=0$.

Now, substituting Equations (1) and (2), we have

$$
\begin{aligned}
\left.\nabla F\right|_{P}= & \left\{\left.\frac{\partial F}{\partial r}\right|_{P} \sin \theta_{0} \cos \phi_{0}+\left.\frac{1}{r_{0}} \frac{\partial F}{\partial \theta}\right|_{P} \cos \theta_{0} \cos \phi_{0}-\left.\frac{\sin \phi_{0}}{r_{0} \sin \theta_{0}} \frac{\partial F}{\partial \phi}\right|_{P}\right\} \mathbf{i} \\
& +\left\{\left.\frac{\partial F}{\partial r}\right|_{P} \sin \theta_{0} \sin \phi_{0}+\left.\frac{1}{r_{0}} \frac{\partial F}{\partial \theta}\right|_{P} \cos \theta_{0} \sin \phi_{0}+\left.\frac{\cos \phi_{0}}{r_{0} \sin \theta_{0}} \frac{\partial F}{\partial \phi}\right|_{P}\right\} \mathbf{j} \\
& +\left\{\left.\frac{\partial F}{\partial r}\right|_{P} \cos \theta_{0}-\left.\frac{1}{r_{0}} \frac{\partial F}{\partial \theta}\right|_{P} \sin \theta_{0}\right\} \mathbf{k}
\end{aligned}
$$

Denoting the expressions in braces by $A, B, C$, respectively, so that $\left.\nabla F\right|_{p}=A \mathbf{i}+B \mathbf{j}+C \mathbf{k}$, we see that the required equation is $A\left(x-x_{0}\right)+B\left(y-y_{0}\right)+C\left(z-z_{0}\right)=0$. This can be written in spherical coordinates by using the transformation equations for $x, y$, and $z$ in these coordinates.

(b) We have $F=r-4 \cos \theta=0$. Then $\partial F / \partial r=1, \partial F / \partial \theta=4 \sin \theta, \partial F / \partial \phi=0$.

Since $r_{0}=2 \sqrt{2}, \theta_{0}=\pi / 4, \phi_{0}=3 \pi / 4$, we have from (a), $\left.\nabla F\right|_{p}=A \mathbf{i}+B \mathbf{j}+C \mathbf{k}=-\mathbf{i}+\mathbf{j}$.

From the transformation equations, the given point has rectangular coordinates $(-\sqrt{2}, \sqrt{2}, 2)$, and so $r-r_{0}=(x+\sqrt{2}) \mathbf{i}+(y-\sqrt{2}) \mathbf{j}+(z-2) \mathbf{k}$.

The required equation of the plane is thus $-(x+\sqrt{2})+(y-\sqrt{2})=0$ or $y-x=2 \sqrt{2}$. In spherical coordinates this becomes $r \sin \theta \sin \phi-r \sin \theta \cos \phi=2 \sqrt{2}$.

In rectangular coordinates the equation $r=4 \cos \theta$ becomes $\left.x^{2}+y^{2}+(z-2)^{2}\right)=4$ and the tangent plane can be determined from this as in Problem 8.1. In other cases, however, it may not be so easy to obtain the equation in rectangular form, and in such cases the method of part $(a)$ is simpler to use.

(c) The equations of the normal line can be represented by

$$
\frac{x+\sqrt{2}}{-1}=\frac{y-\sqrt{2}}{1}=z=2
$$

\section*{Tangent Line and Normal Plane to a Curve}
8.5. Find equations for (a) the tangent line and (b) the normal plane to the curve $x=t-\cos t, y=3+\sin 2 t, z=$ $1+\cos 3 t$ at the point where $t=1 / 2 \pi$.

(a) The vector from origin $O$ (see Figure 8.2) to any point of curve $C$ is $\mathbf{R}=(t-\cos t) \mathbf{i}+(3+\sin 2 t) \mathbf{j}+(1+$ $\cos 3 t) \mathbf{k}$. Then a vector tangent to $C$ at the point where $t=\frac{1}{2} \pi$ is

$$
\mathbf{T}_{0}=\left.\frac{d \mathbf{R}}{d t}\right|_{t=1 / 2 \pi}=(1+\sin t) i+2 \cos 2 t \mathbf{j}-\left.3 \sin 3 t \mathbf{k}\right|_{t=1 / 2 \pi}=2 \mathbf{i}-2 \mathbf{j}+3 \mathbf{k}
$$

The vector from $O$ to the point where $t=1 / 2 \pi$ is $\mathbf{r}_{0}=1 / 2 \pi \mathbf{i}+3 \mathbf{j}+\mathbf{k}$.

The vector from $O$ to any point $(x, y, z)$ on the tangent line is $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$.

Then $\left.\mathbf{r}-\mathbf{r}_{0}=\left(x-\frac{1}{2} \pi\right) \mathbf{i}+y-3\right) \mathbf{j}+(z-1) \mathbf{k}$ is collinear with $\mathbf{T}_{0}$, so that the required equation is

$$
\left(\mathbf{r}-\mathbf{r}_{0}\right) \times \mathbf{T}_{0}=\mathbf{0}, \quad \text { i.e., } \quad\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
x-\frac{1}{2} \pi & y-3 & z-1 \\
2 & -2 & 3
\end{array}\right|=0
$$

and the required equations are $\frac{x-\frac{1}{2} \pi}{2}=\frac{y-3}{-2}=\frac{z-1}{3}$ or, in parametric form, $x=2 t+\frac{1}{2} \pi, y=3-2 t$,\\
$z=3 t+1$.

(b) Let $r=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$ be the vector from $O$ to any point $(x, y, z)$ of the normal plane. The vector from $O$ to the point where $t=\frac{1}{2} \pi$ is $\mathbf{r}_{0}=\frac{1}{2} \pi \mathbf{i}+3 \mathbf{j}+\mathbf{k}$. The vector $\mathbf{r}-\mathbf{r}_{0}=\left(x-\frac{1}{2} \pi\right) \mathbf{i}+(y-3) \mathbf{j}+(z-1) \mathbf{k}$ lies in the normal plane and, hence, is perpendicular to $\mathbf{T}_{0}$. Then the required equation is $\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \mathbf{T}_{0}=0$ or 2 $\left(x-\frac{1}{2} \pi\right)-2(y-3)+3(z-1)=0$.

8.6. Find equations for (a) the tangent line and (b) the normal plane to the curve $3 x^{2} y+y^{2} z=-2,2 x z-x^{2} y=3$ at the point $(1,-1,1)$.

(a) The equations of the surfaces intersecting in the curve are

$$
F=3 x^{2} y+y^{2} z+2=0, G=2 x z-x^{2} y-3=0
$$

The normals to each surface at the point $P(1,-1,1)$ are, respectively,

$$
\begin{aligned}
& \mathbf{N}_{1}=\left.\nabla F\right|_{P}=6 x y \mathbf{i}+\left(3 x^{2}+2 y z\right) \mathbf{j}+y^{2} \mathbf{k}=-6+\mathbf{j}+\mathbf{k} \\
& \mathbf{N}_{2}=\left.\nabla G\right|_{P}=(2 z-2 x y) \mathbf{i}-x^{2} \mathbf{j}+2 x \mathbf{k}=4 \mathbf{i}-\mathbf{j}+2 \mathbf{k}
\end{aligned}
$$

Then a tangent vector to the curve at $P$ is

$$
\mathbf{T}_{0}=\mathbf{N}_{1} \times \mathbf{N}_{2}=(-6 \mathbf{i}+\mathbf{j}+\mathbf{k}) \times(4-\mathbf{j}+2 \mathbf{k})=3 \mathbf{i}+16 \mathbf{j}+2 \mathbf{k}
$$

Thus, as in Problem 8.5(a), the tangent line is given by

$$
\left(\mathbf{r}-\mathbf{r}_{0}\right) \times \mathbf{T}_{0}=0 \text { or }\{(x-1) \mathbf{i}+(y+1) \mathbf{j}+(z-1) \mathbf{k}\} \times\{3 \mathbf{i}+16 \mathbf{j}+2 \mathbf{k}\}=\mathbf{0}
$$

i.e.,

$$
\frac{x-1}{3}=\frac{y+1}{16}=\frac{z-1}{2} \quad \text { or } \quad x=1+3 t, \quad y=16 t-1, \quad z=2 t+1
$$

(b) As in Problem 8.5(b), the normal plane is given by

$$
\left(r-\mathbf{r}_{0}\right) \cdot \mathbf{T}_{0}=0 \text { or }\{(x-1) \mathbf{i}+(y+1) \mathbf{j}+(z-1) \mathbf{k}\} \cdot\{3 \mathbf{i}+16 \mathbf{j}+2 \mathbf{k}\}=0
$$

i.e.,

$$
3(x-1)+16(y+1)+2(z-1)=0 \text { or } 3 x+16 y+2 z=-11
$$

The results in (a) and (b) can also be obtained by using Equations (7) and (10), respectively, from Page 197.

8.7. Establish equation (10), from Page 197.

Suppose the curve is defined by the intersection of two surfaces whose equations are $F(x, y, z)=0, G(x$, $y, z)=0$, where we assume $F$ and $G$ to be continuously differentiable.

The normals to each surface at point $P$ are given, respectively, by $\mathbf{N}_{1}=\left.\nabla F\right|_{P}$ and $\mathbf{N}_{2}=\left.\nabla G\right|_{P}$. Then a tangent vector to the curve at $P$ is $\mathbf{T}_{0}=\mathbf{N}_{1} \times \mathbf{N}_{2}=\left.\nabla F\right|_{P} \times\left.\nabla G\right|_{P}$. Thus, the equation of the normal plane is $\left(\mathbf{r}-\mathbf{r}_{0}\right) \cdot \mathbf{T}_{0}=0$. Now

$$
\begin{aligned}
\mathbf{T}_{0} & =\left.\nabla F\right|_{P} \times\left.\nabla G\right|_{P}=\left.\left\{\left(F_{x} i+F_{y} j+F_{z} k\right) \times\left(G_{x} i+G_{y} j+G_{z} k\right)\right\}\right|_{P} \\
& =\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
F_{x} & F_{y} & F_{z} \\
G_{x} & G_{y} & G_{z}
\end{array}\right|_{P}=\left|\begin{array}{cc}
F_{y} & F_{z} \\
G_{y} & G_{z}
\end{array}\right|_{P} \mathbf{i}+\left|\begin{array}{cc}
F_{x} & F_{x} \\
G_{x} & G_{x}
\end{array}\right|_{P} \mathbf{j}+\left|\begin{array}{cc}
F_{x} & F_{y} \\
G_{x} & G_{y}
\end{array}\right|_{P} \mathbf{k}
\end{aligned}
$$

and so the required equation is

$$
\left.\left(r-r_{0}\right) \cdot \nabla F\right|_{P}=0 \quad \text { or } \quad\left|\begin{array}{ll}
F_{y} & F_{z} \\
G_{y} & G_{z}
\end{array}\right|_{P}\left(x-x_{0}\right)+\left|\begin{array}{ll}
F_{z} & F_{x} \\
G_{z} & G_{x}
\end{array}\right|_{P}\left(y-y_{0}\right)+\left|\begin{array}{cc}
F_{x} & F_{y} \\
G_{x} & G_{y}
\end{array}\right|_{P}\left(z-z_{0}\right)=0
$$

\section*{Envelopes}
8.8. Prove that the envelope of the family $\phi(x, y, \alpha)=0$, if it exists, can be obtained by solving simultaneously the equations $\phi=0$ and $\phi_{\alpha}=0$.

Assume parametric equations of the envelope to be $x=f(\alpha), y=g(\alpha)$. Then $\phi(f(\alpha), g(\alpha), \alpha)=0$ identically, so, upon differentiating with respect to $\alpha$ (assuming that $\phi, f$, and $g$ have continuous derivatives), we have


\begin{equation*}
\phi_{x} f^{\prime}(\alpha)+\phi_{y} g^{\prime}(\alpha)+\phi_{\alpha}=0 \tag{1}
\end{equation*}


The slope of any member of the family $\phi(x, y, \alpha)=0$ at $(x, y)$ is given by $\phi_{x} d x+\phi_{y} d y=0$ or $\frac{d y}{d x}=-\frac{\phi_{x}}{\phi_{y}}$. The slope of the envelope at $(x, y)$ is $\frac{d y}{d x}=\frac{d y / d \alpha}{d x / d \alpha}=\frac{g^{\prime}(\alpha)}{f^{\prime}(\alpha)}$. Then at any point where the envelope and a member of the family are tangent, we must have


\begin{equation*}
-\frac{\phi_{x}}{\phi_{y}}=\frac{g^{\prime}(\alpha)}{f^{\prime}(\alpha)} \quad \text { or } \quad \phi_{x} f^{\prime}(\alpha)+\phi_{y} g^{\prime}(\alpha)=0 \tag{2}
\end{equation*}


Comparing Equations (2) with (1), we see that $\phi_{\alpha}=0$, and the required result follows.

8.9. (a) Find the envelope of the family $x \sin \alpha+y \cos \alpha=1$. (b) Illustrate the results geometrically.

(a) By Problem 8.8 the envelope, if it exists, is obtained by solving simultaneously the equations $\phi(x, y, \alpha)$ $=x \sin \alpha+y \cos \alpha-1=0$ and $\phi_{\alpha}(x, y, \alpha)=x \cos \alpha-y \cos \alpha=0$. From these equations we find $x=\sin$ $\alpha, y=\cos \alpha$ or $x^{2}+y^{2}=1$.

(b) The given family is a family of straight lines, some members of which are indicated in Figure 8.5. The envelope is the circle $x^{2}+y^{2}=1$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-216}
\end{center}

Fig. 8.5

8.10. Find the envelope of the family of surfaces $z=2 \alpha x-a^{2} y$.

By a generalization of Problem 8.8, the required envelope, if it exists, is obtained by solving simultaneously the equations


\begin{equation*}
\phi=2 \alpha x-\alpha^{2} y-z=0 \tag{1}
\end{equation*}


and


\begin{equation*}
\phi_{\alpha}=2 x-2 \alpha y=0 \tag{2}
\end{equation*}


From Equation (2), $\alpha=x / y$. Then substitution in Equation (1) yields $x^{2}=y z$, the required envelope.

8.11. Find the envelope of the two-parameter family of surfaces $z=\alpha x+\beta y-\alpha \beta$.

The envelope of the family $F(x, y, z, \alpha, \beta)=0$, if it exists, is obtained by eliminating $\alpha$ and $\beta$ between the equations $F=0, F_{\alpha}=0, F_{\beta}=0$ (see Problem 8.43). Now

$$
F=z-\alpha x-\beta y+\alpha \beta=0, F_{\alpha}=-x+\beta=0, F_{\beta}=-y+\alpha=0
$$

Then $\beta=x, \alpha=y$, and we have $z=x y$.

\section*{Directional derivatives}
8.12. Find the directional derivative of $F=x^{2} y z^{3}$ along the curve $x=e^{-u}, y=2 \sin u+1, z=u-\cos u$ at the point $P$ where $u=0$.

The point $P$ corresponding to $u=0$ is $(1,1,-1)$. Then

$$
\nabla F=2 x y z^{3} \mathbf{i}+x^{2} z^{3} \mathbf{j}+3 x^{2} y z^{2} \mathbf{k}=-2 \mathbf{i}-\mathbf{j}+3 \mathbf{k} \text { at } P
$$

A tangent vector to the curve is

$$
\begin{aligned}
\frac{d r}{d u} & =\frac{d}{d u}\left\{e^{-u} \mathbf{i}+(2 \sin u+1) \mathbf{j}+(u-\cos u) k\right\} \\
& =-e^{-u} \mathbf{i}+2 \cos u \mathbf{j}+(1+\sin u) \mathbf{k}=-\mathbf{i}+2 \mathbf{j}+\mathbf{k} \text { at } P
\end{aligned}
$$

and the unit tangent vector in this direction is

$$
\mathbf{T}_{0}=\frac{-\mathbf{i}+2 \mathbf{j}+\mathbf{k}}{\sqrt{6}}
$$

Then

$$
\text { Directional derivative }=\nabla F . \mathbf{T}_{0}=(-2 \mathbf{i}-\mathbf{j}+3 \mathbf{k}) \cdot\left(\frac{-\mathbf{i}+2 \mathbf{j}+\mathbf{k}}{\sqrt{6}}\right)=\frac{3}{\sqrt{6}}=\frac{1}{2} \sqrt{6}
$$

Since this is positive, $F$ is increasing in this direction.

8.13. Prove that the greatest rate of change of $F$, i.e., the maximum directional derivative, takes place in the direction of, and has the magnitude of, the vector $\nabla F$.

$$
\frac{d F}{d s}=\nabla F \cdot \frac{d \mathbf{r}}{d s} \text { is the projection of } \nabla F \text { in the direction } \frac{d \mathbf{r}}{d s} \text {. This projection is a maximum when } \nabla F \text { and }
$$

$d \mathbf{r} / d s$ have the same direction. Then the maximum value of $d F / d s$ takes place in the direction of $\nabla F$, and the magnitude is $|\nabla F|$.

8.14. (a) Find the directional derivative of $U=2 x^{3} y-3 y^{2} z$ at $P(1,2,-1)$ in a direction toward $Q(3,-1,5)$. (b) In what direction from $P$ is the directional derivative a maximum? (c) What is the magnitude of the maximum directional derivative?

(a) $\nabla U=6 x^{2} y \mathbf{i}+\left(2 x^{3}-6 y z\right) \mathbf{j}-3 y^{2} \mathbf{k}=12 \mathbf{i}+14 \mathbf{j}-12 \mathbf{k}$ at $P$.

The vector from $P$ to $Q=(3-1) \mathbf{i}+(-1-2) \mathbf{j}+[5-(-1)] \mathbf{k}=2 \mathbf{i}-3 \mathbf{j}+6 \mathbf{k}$

The unit vector from $P$ to $\mathrm{Q}=\mathrm{T}=\frac{2 \mathbf{i}-3 \mathbf{j}+6 \mathbf{k}}{\sqrt{(2)^{2}+(-3)^{2}+(6)^{2}}}=\frac{2 \mathbf{i}-3 \mathbf{j}+6 \mathbf{k}}{7}$

Then

Directional derivative at $\mathrm{P}=(12 \mathbf{i}+14 \mathbf{j}-12 \mathbf{k}) \cdot\left(\frac{2 \mathbf{i}-3 \mathbf{j}+6 \mathbf{k}}{7}\right)=-\frac{90}{7}$

i.e., $U$ is decreasing in this direction.

(b) From Problem 8.13, the directional derivative is a maximum in the direction $12 \mathbf{i}+14 \mathbf{j}-12 \mathbf{k}$.

(c) From Problem 8.13, the value of the maximum directional derivative is $|12 \mathbf{i}+14 \mathbf{j}-12 \mathbf{k}|=$ $\sqrt{144+196+144}=22$.

\section*{Differentiation under the integral sign}
8.15. Prove Leibnitz's rule for differentiating under the integral sign.

Let $\phi(\boldsymbol{\alpha})=\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f(x, \boldsymbol{\alpha}) d x$. Then

$$
\begin{aligned}
\Delta \phi & =\phi(\alpha+\Delta \alpha)-\phi(\alpha)=\int_{u_{1}(\alpha+\Delta \alpha}^{u_{2}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x-\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f(x, \alpha) d x \\
& =\int_{u_{1}(\alpha+\Delta \alpha}^{u_{1}(\alpha)} f(x, \alpha+\Delta \alpha) d x+\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f(x, \alpha+\Delta \alpha)+\int_{u_{1}(\alpha)}^{u_{2}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x-\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f(x, \alpha) d x \\
& =\int_{u_{1}(\alpha)}^{u_{2}(\alpha)}[f(x, \alpha+\Delta \alpha)-f(x, \alpha)] d x+\int_{u_{2}(\alpha)}^{u_{2}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x-\int_{u_{1}(\alpha)}^{u_{1}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x
\end{aligned}
$$

By the mean value theorem for integrals, we have


\begin{gather*}
\int_{u_{1}(\alpha)}^{u_{2}(\alpha)}[f(x, \alpha+\Delta \alpha)-f(x, \alpha)] d x=\Delta \alpha \int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f(x, \xi) d x  \tag{1}\\
\int_{u_{1}(\alpha)}^{u_{1}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x=f\left(\xi_{1}, \alpha+\Delta \alpha\right)\left[u_{1}(\alpha+\Delta \alpha)-u_{1}(\alpha)\right] \tag{2}
\end{gather*}



\begin{equation*}
\int_{u_{2}(\alpha)}^{u_{2}(\alpha+\Delta \alpha)} f(x, \alpha+\Delta \alpha) d x=f\left(\xi_{2}, \alpha+\Delta \alpha\right)\left[u_{1}(\alpha+\Delta \alpha)-u_{1}(\alpha)\right] \tag{3}
\end{equation*}


where $\xi$ is between $\alpha+\Delta \alpha$, $\xi_{1}$ is between $u_{1}(\alpha)$ and $u_{1}(\alpha+\Delta \alpha)$ and $\xi_{2}$ is between $u_{2}(\alpha)$ and $u_{2}(\alpha+\Delta \alpha)$.

Then

$$
\frac{\Delta \phi}{\Delta \phi}=\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f_{\alpha}(x, \xi) d x+f\left(\xi_{2}, \alpha+\Delta \alpha\right) \frac{\Delta u_{2}}{\Delta \alpha}-f\left(\xi_{1}, \alpha+\Delta \alpha\right) \frac{\Delta u_{1}}{\Delta \alpha}
$$

Taking the limit as $\Delta \alpha \rightarrow 0$, making use of the fact that the functions are assumed to have continuous derivatives, we obtain

$$
\frac{d \phi}{d \phi}=\int_{u_{1}(\alpha)}^{u_{2}(\alpha)} f_{\alpha}(x, \xi) d x+f\left(\xi_{2}, \alpha+\Delta \alpha\right) \frac{\Delta u_{2}}{\Delta \alpha}-f\left(\xi_{1}, \alpha+\Delta \alpha\right) \frac{\Delta u_{1}}{\Delta \alpha}
$$

8.16. If $\phi(\alpha)=\int_{\alpha}^{\alpha^{2}} \frac{\sin \alpha x}{x} d x$, find $\phi^{\prime}(\alpha)$ where $\alpha \neq 0$.

By Leibniz's rule,

$$
\begin{aligned}
\phi^{\prime}(\alpha) & =\int_{\alpha}^{\alpha^{2}} \frac{\partial}{\partial a}\left(\frac{\sin \alpha x}{x}\right) d x+\frac{\sin \left(\alpha \cdot \alpha^{2}\right)}{\alpha^{2}} \frac{d}{d \alpha}\left(\alpha^{2}\right)-\frac{\sin (\alpha \cdot \alpha)}{\alpha} \frac{d}{d \alpha}(\alpha) \\
& =\int_{\alpha}^{\alpha^{2}} \cos \alpha x d x+\frac{2 \sin \alpha^{3}}{\alpha}-\frac{\sin \alpha^{2}}{\alpha} \\
& =\left.\frac{\sin \alpha x}{\alpha}\right|_{\alpha} ^{a^{2}}+-\frac{2 \sin \alpha^{3}}{\alpha}-\frac{\sin \alpha^{2}}{\alpha}=\frac{3 \sin \alpha^{3}-2 \sin \alpha^{2}}{\alpha}
\end{aligned}
$$

8.17. If $\int_{0}^{\pi} \frac{d x}{\alpha-\cos x}=\frac{\pi}{\sqrt{\alpha^{2}-1}} \cdot \alpha>1$, find $\int_{0}^{\pi} \frac{d x}{(2-\cos x)^{2}}$ (See Problem 5.58.)

By Leibniz's rule, if $\phi(\alpha)=\int_{0}^{\pi} \frac{d x}{(\alpha-\cos x)}=\pi\left(\alpha^{2}-1\right)^{-1 / 2}$, then

$$
\phi(\alpha)=-\int_{0}^{\pi} \frac{d x}{(\alpha-\cos x)}=\frac{1}{2} \pi\left(\alpha^{2}-1\right)^{-3 / 2} 2 \alpha=\frac{-\pi \alpha}{\left(\alpha^{2}-1\right)^{3 / 2}}
$$

Thus, $\int_{0}^{\pi} \frac{d x}{(\alpha-\cos x)}=\frac{-\pi \alpha}{\left(\alpha^{2}-1\right)^{3 / 2}}$, from which $\int_{0}^{\pi} \frac{d x}{(2-\cos x)^{2}}=\frac{2 \pi}{3 \sqrt{3}}$.

\section*{Integration under the integral sign}
8.18. Prove the result (18), on Page 198, for integration under the integral sign.

Consider:

By Leibniz's rule,


\begin{equation*}
\psi(\alpha)=\int_{u_{1}}^{u_{2}}\left\{\int_{\alpha}^{\alpha} f(x, \alpha) d x\right\} d x \tag{2}
\end{equation*}


$$
\psi^{\prime}(\alpha)=\int_{u_{1}}^{u_{2}} \frac{\partial}{\partial \alpha}\left\{\int_{\alpha}^{\alpha} f(x, \alpha) d x\right\} d x=\int_{u_{1}}^{u_{2}} f(x, \alpha) d x=\phi(\alpha)
$$

Then, by integration,


\begin{equation*}
\psi(\alpha)=\int_{\alpha}^{\alpha} \phi(\alpha) d \alpha+c \tag{2}
\end{equation*}


Since $\psi(a)=0$ from Equation (1), we have $c=0$ in (2). Thus from Equation (1) and (2) with $c=0$, we find

$$
\int_{u_{1}}^{u_{2}}\left\{\int_{\alpha}^{\alpha} f(x, \alpha) d x\right\} d x=\int_{\alpha}^{\alpha}\left\{\int_{u_{1}}^{u_{2}} f(x, \alpha) d x\right\} d x
$$

Putting $\alpha=b$, the required result follows.

8.19. Prove that $\int_{0}^{\pi} \ln \left(\frac{b-\cos x}{a-\cos x}\right) d x=\pi \ln \left(\frac{b+\sqrt{b^{2}-1}}{a+\sqrt{a^{2}-1}}\right)$ if $a, b>1$.

From Problem 5.58, $\int_{0}^{\pi} \frac{d x}{\alpha-\cos x}=\frac{\pi}{\sqrt{\alpha^{2}-1}} \cdot \alpha>1$.

Integrating the left side with respect to $\alpha$ from $a$ to $b$ yields

$$
\int_{0}^{\pi}\left\{\int_{a}^{b} \frac{d x}{\alpha-\cos x}\right\} d x=\left.\int_{0}^{\pi} \operatorname{In}(\alpha-\cos x)\right|_{a} ^{b} d x \int_{0}^{\pi} \operatorname{In}\left(\frac{b-\cos x}{a-\cos x}\right) d x
$$

Integrating the right side with respect to $\alpha$ to $b$ yields

$$
\int_{0}^{\pi} \frac{\pi d \alpha}{\sqrt{\alpha^{2}-1}}=\pi \ln \left(\alpha+\left.\sqrt{\left.a^{2}-1\right)}\right|_{a} ^{b}=\pi \ln \left(\frac{b+\sqrt{b^{2}-1}}{a+\sqrt{a^{2}-1}}\right)\right.
$$

and the required result follows.

\section*{Maxima and minima}
8.20. Prove that a necessary condition for $f(x, y)$ to have a relative extremum (maximum or minimum) at $\left(x_{0}, y_{0}\right)$ is that $f_{x}\left(x_{0}, y_{0}\right)=0, f_{y}\left(x_{0}, y_{0}\right)=0$.

If $f\left(x_{0}, y_{0}\right)$ is to be an extreme value for $f(x, y)$, then it must be an extreme value for $f\left(x, y_{0}\right)$ and $f\left(x_{0}\right.$, $y$ ). But a necessary condition that these have extreme values at $x_{x}=0$ and $y=y_{0}$, respectively, is $f_{x}\left(x_{0}, y_{0}\right)=0$, $f_{y}\left(x_{0}, y_{0}\right)=0$ (using results for functions of one variable).

8.21. Let $f$ be continuous and have continuous partial derivatives of order two, at least, in a region $R$ with the critical point $P_{0}\left(x_{0}, y_{0}\right)$ an interior point. Determine the sufficient conditions for relative extrema at $P_{0}$.

In the case of one variable, sufficient conditions for a relative extrema were formulated through the second derivative [if positive then a relative minimum, if negative then a relative maximum, if zero a possible point of inflection but more investigation is necessary]. In the case of $z=f(x, y)$ that is before us we can expect the second partial derivatives to supply information. (See Figure 8.6.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-219}
\end{center}

Fig. 8.6

First observe that solutions of the quadratic equation $A t^{2}+2 B t+C=0$ are $t=\frac{-2 B \pm \sqrt{4 B^{2}-4 A C}}{2 A}$.

Further observe that the nature of these solutions is determined by $B^{2}-A C$. If the quantity is positive, the solutions are real and distinct; if negative, they are complex conjugate; and if zero, the two solutions are coincident.

The expression $B^{2}-A C$ also has the property of invariance with respect to plane rotations

$$
\begin{aligned}
& x=\bar{x} \cos \theta-\bar{y} \sin \theta \\
& y=\bar{x} \cos \theta-\bar{y} \sin \theta
\end{aligned}
$$

It has been discovered that with the identifications $A=f_{x x}, B=f_{x y}, C=f_{y y}$, we have the partial derivative form $f_{x y}^{2}-f_{x x} f_{y y}$ that characterizes relative extrema.

The demonstration of invariance of this form can be found in analytic geometric books. However, if you would like to put the problem in the context of the second partial derivative, observe that

$$
\begin{aligned}
& f_{\bar{x}}=f_{x} \frac{\partial x}{\partial \bar{x}}+f_{x} \frac{\partial y}{\partial \bar{x}}=f_{x} \cos \theta+f_{y} \sin \theta \\
& f_{\bar{y}}=f_{x} \frac{\partial x}{\partial \bar{x}}+f_{y} \frac{\partial y}{\partial \bar{y}}=-f_{x} \sin \theta+f_{y} \cos \theta
\end{aligned}
$$

Then, using the chain rule to compute the second partial derivatives and proceeding by straightforward but tedious calculation, we show that.

$$
f_{x y}^{2}=f_{x x} f_{y y}=f_{\bar{x} \bar{y}}^{2}-f_{\overline{x x}} f_{\overline{y y}}
$$

The following equivalences are a consequence of this invariant form (independently of direction in the tangent plane at $P_{0}$ ):

\[
\begin{array}{lll}
f_{x y}^{2}=f_{x x} f_{y y}<0 & \text { and } & f_{x x} f_{y y}>0 \\
f_{x y}^{2}=f_{x x} f_{y y}<0 & \text { and } & f_{x x} f_{y y}<0 \tag{2}
\end{array}
\]

The key relation is (1) because in order that this equivalence hold, both terms $f_{x} f_{y}$ must have the same sign. We can look to the one-variable case (make the same argument for each coordinate direction) and conclude that there is a relative minimum at $P_{0}$ if both partial derivatives are positive and a relative maximum if both are negative. We can make this argument for any pair of coordinate directions because of the invariance under rotation that was established.

If relation (2) holds, then the point is called a saddle point. If the quadratic form is zero, no information results.

Observe that this situation is analogous to the one-variable extreme value theory in which the nature of $f$ at $x$, and with $f^{\prime}(x)=0$, is undecided if $f^{\prime \prime}(x)=0$.

8.22. Find the relative maxima and minima of $f(x, y)=x^{3}+y^{3}-3 x-12 y+20$.

$f_{x}=3 x^{2}-3=0$ when $x= \pm 1, f_{y}=3 y^{2}-12=0$ when $y= \pm 2$. Then critical points are $P(1,2), Q(-1,2)$, $R(1,-2), S(-1,-2)$.

$f_{x x}=6 x, f_{y y}=0$. Then $\Delta=f_{x x} f_{y y}-f^{2}{ }_{x y}=36 x y$.

At $P(1,2), \Delta>0$ and $f_{x x}$ (or $f_{y y}$ ) $>0$; hence $P$ is a relative minimum point.

At $Q(-1,2), \Delta<0$ and $Q$ is neither a relative maximum or minimum point.

At $R(1,-2), \Delta<0$ and $R$ is neither a relative maximum or minimum point.

At $S(-1,-2), \Delta>0$ and $f_{x x}$ (or $f_{y y}$ ) <0 so $S$ is a relative maximum point.

Thus, the relative minimum value of $f(x, y)$ occurring at $P$ is 2 , while the relative maximum value occurring at $S$ is 38 . Points $Q$ and $R$ are saddle points.

8.23. A rectangular box, open at the top, is to have a volume of 32 cubic feet. What must be the dimensions so that the total surface is a minimum?

If $x, y$, and $z$ are the edges (see Fig. 8.7), then

Volume of box $=V=x y z=32$

Surface area of box $=S=x y+2 y z+2 x z$

or, since $z=32 / x y$ from Equation (1),

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-221}
\end{center}

Fig. 8.7


\begin{align*}
& \frac{\partial S}{\partial x}=y-\frac{64}{x^{2}}=0 \text { when } x^{2} y=64  \tag{3}\\
& \frac{\partial S}{\partial y}=x-\frac{64}{y^{2}}=0 \text { when } x y^{2}=64 \tag{4}
\end{align*}


Dividing Equations (3) and (4), we find $y=x$ so that $x^{3}=64$ or $x=y=4$ and $z=2$.

For $x=y=4, \Delta=S_{x x} S_{y y}-S_{x y}^{2}=\left(\frac{128}{x^{3}}\right)\left(\frac{128}{y^{3}}\right)-1>0$ and $\mathrm{s}_{x x}=\frac{128}{x^{3}}>0$. Hence, it follows that the dimensions 4 feet $\times 4$ feet $\times 2$ feet give the minimum surface.

\section*{Lagrange multipliers for maxima and minima}
8.24. Consider $F(x, y, z)$ subject to the constraint condition $G(x, y, z)=0$. Prove that a necessary condition that $F(x, y, z)$ have an extreme value is that $F_{x} G_{y}-F_{y} G_{x}=0$.

Since $G(x, y, z)=0$, we can consider $z$ as a function of $x$ and $y$-say, $z=f(x, y)$. A necessary condition that $F[x, y, f(x, y)]$ have an extreme value is that the partial derivatives with respect to $x$ and $y$ be zero. This gives


\begin{align*}
& F_{x}+F_{z} z_{x}=0  \tag{1}\\
& F_{y}+F_{z} Z_{y}=0 \tag{2}
\end{align*}


Since $G(x, y, z)=0$, we also have


\begin{align*}
& G_{x}+G_{x} z_{x}=0  \tag{3}\\
& G_{y}+G_{z} z_{y}=0 \tag{4}
\end{align*}


From Equations (1) and (3) we have


\begin{equation*}
F_{x} G_{x}-F_{x} G_{x}=0 \tag{5}
\end{equation*}


and from Equations (2) and (4) we have


\begin{equation*}
F_{y} G_{z}-F_{z} G_{y}=0 \tag{6}
\end{equation*}


Then from Equations (5) and (6) we find $F_{x} G_{y}-F_{y} G_{x}=0$.

These results hold only if $F_{z} \neq 0, G_{z} \neq 0$.

8.25. Referring to Problem 8.24, show that the stated condition is equivalent to the conditions $\phi_{x}=0, \phi_{y}=0$ where $\phi=F+\lambda G$ and $\lambda$ is a constant.

If $\phi_{x}=0, F_{x}+\lambda G_{x}=0$. If $\phi_{y}=0, F_{y}+\lambda G_{y}=0$. Elimination of $\lambda$ between these equations yields $F_{x} G_{y}-$ $F_{y} G_{x}=0$.

The multiplier $\lambda$ is the Lagrange multiplier. If desired, we can consider equivalently $\phi=\lambda F+G$ where $\phi_{x}=0, \phi_{y}=0$.

8.26. Find the shortest distance from the origin to the hyperbola $x^{2}+8 x y+7 y^{2}=225, z=0$.

We must find the minimum value of $x^{2}+y^{2}$ (the square of the distance from the origin to any point in the $x y$ plane) subject to the constraint $x^{2}+8 x y+7 y^{2}=225$.

According to the method of Lagrange multipliers, we consider $\phi=x^{2}+8 x y+7 y^{2}-225+\lambda\left(x^{2}+y^{2}\right)$. Then

$$
\begin{aligned}
& \phi_{x}=2 x+8 y+2 \lambda x=0 \quad \text { or } \quad(\lambda+1) x+4 y=0 \\
& \phi_{y}=8 x+14 y+2 \lambda y=0 \quad \text { or } \quad 4 x+(\lambda+7) y=0
\end{aligned}
$$

From Equations (1) and (2), since $(x, y) \neq(0,0)$, we must have

$$
\left|\begin{array}{cc}
\lambda+1 & 4 \\
4 & \lambda+7
\end{array}\right|=0 \text {, i.e., } \lambda^{2}+8 \lambda-9=\text { or } \lambda=1,-9
$$

Case 1: $\quad \lambda=1$. From Equation (1) or (2), $x=-2 y$, and substitution in $x^{2}+8 x y+7 y^{2}=225$ yields $-5 y^{2}=225$, for which no real solution exists.

Case 2: $\lambda=-9$. From Equation (1) or (2), $y=2 x$, and substitution in $x^{2}+8 x y+7 y^{2}=225$ yields $45 x^{2}=225$. Then $x^{2}=5, y^{2}=4 x^{2}=20$ and so $x^{2}+y^{2}=25$. Thus, the required shortest distance is $\sqrt{25}=5$.

8.27. (a) Find the maximum and minimum values of $x^{2}+y^{2}+z^{2}$ subject to the constraint conditions $x^{2} / 4+y^{2} / 5+$ $z^{2} / 25=1$ and $z=x+y$. (b) Give a geometric interpretation of the result in (a).

(a) We must find the extrema of $F=x^{2}+y^{2}+z^{2}$ subject to the constraint conditions $\phi_{1}=\frac{x^{2}}{4}+\frac{x^{2}}{5}+$ $\frac{z^{2}}{25}-1=0$ and $\phi_{2}=x+y-z=0$. In this case we use two Lagrange multipliers $\lambda_{1}, \lambda_{2}$ and consider the function

$$
G=F+\lambda_{1} \phi_{1}+\lambda_{2} \phi_{2}=x^{2}+y^{2}+z^{2}+\lambda_{1}\left(\frac{x^{2}}{4}+\frac{y^{2}}{5}+\frac{z^{2}}{25}-1\right)+\lambda_{2}(x+y-z)
$$

Taking the partial derivatives of $G$ with respect to $x, y, z$ and setting them equal to zero, we find


\begin{equation*}
G_{x}=2 x+\frac{\lambda_{1} x}{2}+\lambda_{2}=0, \quad G_{y}=2 y+\frac{2 \lambda_{1} y}{5}+\lambda_{2}=0, \quad G_{x}=2 z+\frac{2 \lambda_{1} z}{25}-\lambda_{2}=0 \tag{1}
\end{equation*}


Solving these equations for $x, y, z$, we find


\begin{equation*}
x=\frac{-2 \lambda_{2}}{\lambda_{1}+4}, \quad y=\frac{-5 \lambda_{2}}{2 \lambda_{1}+10}, \quad z=\frac{25 \lambda_{2}}{2 \lambda_{1}+50} \tag{2}
\end{equation*}


From the second constraint condition, $x+y-z=0$, we obtain, on division by $\lambda_{2}$, assumed different from zero (this is justified, since otherwise we would have $x=0, y=0, z=0$, which would not satisfy the first constraint condition), the result

$$
\frac{2}{\lambda_{1}+4}+\frac{5}{2 \lambda_{1}+10}+\frac{25}{2 \lambda_{1}+50}=0
$$

Multiplying both sides by $2\left(\lambda_{1}+4\right)\left(\lambda_{1}+5\right)\left(\lambda_{1}+5\right)\left(\lambda_{1}+25\right)$ and simplifying yields

$$
17 \lambda_{1}^{2}+245 \lambda_{1}+750=0 \text { or }\left(\lambda_{1}+10\right)\left(17 \lambda_{1}+75\right)=0
$$

from which $\lambda_{1}=-10$ or $-75 / 17$.

Case 1: $\lambda_{1}=-10$.

From (2), $x=\frac{1}{3} \lambda_{2}, y=\frac{1}{2} \lambda_{2}, z=5 / 6 \lambda_{2}$. Substituting in the first constraint condition, $x^{2} / 4+y^{2} / 5+z^{2} / 25$ $=1$, yields $\lambda_{2}^{2}=180 / 19$ or $\lambda_{2}= \pm 6 \sqrt{5 / 19}$. This gives the two critical points

$$
(2 \sqrt{5 / 19}, 3 \sqrt{5 / 19}, 5 \sqrt{5 / 19}) . \quad(-2 \sqrt{5 / 19},-3 \sqrt{5 / 19},-5 \sqrt{5 / 19})
$$

The value of $x^{2}+y^{2}+z^{2}$ corresponding to these critical points is $(20+45+125) / 19=10$.

Case 2: $\lambda_{1}=-75 / 17$.

From (2), $x=34 / 7 \lambda_{2}, y=-17 / 4 \lambda_{2}, z=17 / 28 \lambda_{2}$. Substituting in the first constraint condition, $x^{2} / 4+y^{2} / 5$ $+z^{2} / 25=1$, yields $\lambda_{2}= \pm 140 /(17 \sqrt{646})$ which give the critical points

$$
(40 / \sqrt{646},-35 \sqrt{646}, 5 / \sqrt{646}) . \quad(-40 / \sqrt{646},-35 \sqrt{646},-5 / \sqrt{646})
$$

The value of $x^{2}+y^{2}+z^{2}$ corresponding to these is $(1600+1225+25) / 646=75 / 17$.

Thus, the required maximum value is 10 and the minimum value is 75/17.

(b) Since $x^{2}+y^{2}+z^{2}$ represents the square of the distance of $(x, y, z)$ from the origin $(0,0,0)$, the problem is equivalent to determining the largest and smallest distances from the origin to the curve of intersection of the ellipsoid $x^{2} / 4+y^{2} / 5+z^{2} / 25=1$ and the plane $z=x+y$. Since this curve is an ellipse, we have the interpretation that $\sqrt{10}$ and $\sqrt{75 / 17}$ are the lengths of the semimajor and semiminor axes of this ellipse.

The fact that the maximum and minimum values happen to be given by $-\lambda_{1}$ in both Case 1 and Case 2 is more than a coincidence. It follows, in fact, on multiplying Equations (1) by $x, y$, and $z$ in succession and adding, for we then obtain

$$
2 x^{2}+\frac{\lambda_{1} x^{2}}{2}+\lambda_{2} x+2 y^{2}+\frac{2 \lambda_{1} y^{2}}{5}+\lambda_{2} y+2 z^{2}+\frac{2 \lambda_{1} z^{2}}{25}-\lambda_{2} z=0
$$

i.e.,

$$
x^{2}+y^{2}+z^{2}+\lambda_{1}\left(\frac{x^{2}}{4}+\frac{y^{2}}{5}+\frac{z^{2}}{25}\right)+\lambda_{2}(x+y-z)=0
$$

Then, using the constraint conditions, we find $x^{2}+y^{2}+z^{2}=-\lambda_{1}$.

For a generalization of this problem, see Problem 8.76.

\section*{Applications to errors}
8.28. The period $T$ of a simple pendulum of length $l$ is given by $T=2 \sqrt{l / g}$. Find (a) the error and (b) the percent error made in computing $T$ by using $l=2 \mathrm{~m}$ and $g=9.75 \mathrm{~m} / \mathrm{sec}^{2}$, if the true values are $l=19.5 \mathrm{~m}$ and $g=$ $9.81 \mathrm{~m} / \mathrm{sec}^{2}$.

(a) $T=2 \pi l^{1 / 2} g^{-1 / 2}$. Then


\begin{equation*}
d T=\left(2 \pi g^{-1 / 2}\left(\frac{1}{2} l^{-1 / 2} d l\right)+\left(2 \pi l^{1 / 2}\right)\left(-\frac{1}{2} g^{-3 / 2} d g\right)=\frac{\pi}{\sqrt{l g}} d l-\pi \sqrt{\frac{1}{g^{3}}} d g\right. \tag{1}
\end{equation*}


$$
\text { Error in } g=\Delta g=d g=+0.06 ; \text { error in } l=\Delta l=d l=-0.5
$$

The error in $T$ is actually $\Delta T$, which is in this case approximately equal to $d T$. Thus, we have from Equation (1),

$$
\text { Error in } T=d T=\frac{\pi}{\sqrt{(2)(9.75)}}(-0.05)-\pi \sqrt{\frac{2}{(9.75)^{3}}}(+0.06)=-0.0444 \mathrm{sec} \text { (approx.) }
$$

The value of $T$ for $l=2, g=9.75$ is $T=2 \pi \sqrt{\frac{2}{9.75}}=2.846 \mathrm{sec}$ (approx.)\\
(b) Percent error (or relative error) in $T=\frac{d T}{T}=\frac{-0.0444}{2.846}=-1.56 \%$.

Another method: $\quad$ Since $\ln T=\ln 2 \pi+\frac{1}{2} \ln l-\frac{1}{2} \ln g$,


\begin{equation*}
\frac{d T}{T}=\frac{1}{2} \frac{d l}{l}-\frac{1}{2} \frac{d g}{g}=\frac{1}{2}\left(\frac{-0.05}{2}\right)-\frac{1}{2}\left(\frac{+0.06}{9.75}\right)=-1.56 \% \tag{2}
\end{equation*}


as before. Note that Equation (2) can be written

$$
\text { Percent error in } T=\frac{1}{2} \text { Percent error in } l-\frac{1}{2} \text { Percent error in } g
$$

\section*{Miscellaneous problems}
8.29. Evaluate $\int_{0}^{1} \frac{x-1}{\operatorname{In} x} d x$.

In order to evaluate this integral, we resort to the following device. Define

$$
\phi(\alpha)=\int_{0}^{1} \frac{x^{\alpha}-1}{\operatorname{In} x} d x \quad \alpha>0
$$

Then by Leibniz's rule

$$
\phi^{\prime}(\alpha)=\int_{0}^{1} \frac{\partial}{\partial \alpha}\left(\frac{x^{\alpha}-1}{\operatorname{In} x}\right) d x=\int_{0}^{1} \frac{x^{\alpha} \operatorname{In} x}{\operatorname{In} x} d x=\int_{0}^{1} d x=\frac{1}{\alpha+1}
$$

Integrating with respect to $\alpha, \phi(\alpha)=\ln (\alpha+1)+c$. But since $\phi(0)=0, c=0$, and so $\phi(\alpha)=\ln (\alpha+1)$.

Then the value of the required integral is $\phi(1)=\ln 2$.

The applicability of Leibniz's rule can be justified here, since if we define $F(x, \alpha)=\left(x^{\alpha}-1\right) / \ln x, 0<x<$ $1, F(0, \alpha)=0, F(1, \alpha)=\alpha$, then $F(x, \alpha)$ is continuous in both $x$ and $\alpha$ for $0 \leqq x \leqq 1$ and all finite $\alpha>0$.

8.30. Find constants $a$ and $b$ for which $F(a, b)=\int_{0}^{\pi}\left\{\sin x-\left(a x^{2}+b x\right)\right\}^{2} d x$ is a minimum.

The necessary conditions for a minimum are $\partial F / \partial a=0$. Performing these differentiations, we obtain

$$
\begin{aligned}
& \frac{\partial F}{\partial a}=\int_{0}^{\pi} \frac{\partial}{\partial a}\left\{\sin x-\left(a x^{2}+b x\right)\right\}^{2} d x=-2 \int_{0}^{\pi} x^{2}\left\{\sin x-\left(a x^{2}+b x\right)\right\} d x=0 \\
& \frac{\partial F}{\partial b}=\int_{0}^{\pi} \frac{\partial}{\partial b}\left\{\sin x-\left(a x^{2}+b x\right)\right\}^{2} d x=-2 \int_{0}^{\pi} x\left\{\sin x-\left(a x^{2}+b x\right)\right\} d x=0
\end{aligned}
$$

From these we find

$$
\left\{\begin{array}{l}
\alpha \int_{0}^{\pi} x^{4} d x+b \int_{0}^{\pi} x^{3} d x=\int_{0}^{\pi} x^{2} \sin x d x \\
\alpha \int_{0}^{\pi} x^{3} d x+b \int_{0}^{\pi} x^{2} d x=\int_{0}^{\pi} x \sin x d x
\end{array}\right.
$$

or

$$
\left\{\begin{array}{l}
\frac{\pi^{5} a}{5}+\frac{\pi^{4} b}{4}=\pi^{2}-4 \\
\frac{\pi^{4} a}{4}+\frac{\pi^{3} b}{3}=\pi
\end{array}\right.
$$

Solving for $a$ and $b$, we find

$$
a=\frac{20}{\pi^{3}}-\frac{320}{\pi^{5}} \approx-0.40065 . \quad b=\frac{240}{\pi^{4}}-\frac{12}{\pi^{2}} \approx 1.24798
$$

We can show that for these values, $F(a, b)$ is indeed a minimum using the sufficiency conditions on Page 200.

The polynomial $a x^{2}+b x$ is said to be a least square approximation of $\sin x$ over the interval $(0, \pi)$. The ideas involved here are of importance in many branches of mathematics and their applications.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Tangent plane and normal line to a surface}
8.31. Find the equations of (a) the tangent plane and (b) the normal line to the surface $x^{2}+y^{2}=4 z$ at $(2,-4,5)$.

$$
\text { Ans. (a) } x-2 y-z=5 \text { (b) } \frac{x-2}{1}=\frac{y+4}{-2}=\frac{z-5}{-1}
$$

8.32. If $z=f(x, y)$, prove that the equations for the tangent plane and normal line at point $P\left(x_{0}, y_{0}, z_{0}\right)$ are given, respectively, by (a) $z-z_{0}=\left.f_{x}\right|_{p} ^{\prime}\left(x-x_{0}\right)+\left.f_{y}\right|_{p}\left(y-y_{0}\right)$ and (b) $\frac{x-x_{0}}{\left.f_{x}\right|_{p}}=\frac{y-y_{0}}{\left.f_{x}\right|_{p}}=\frac{z-z_{0}}{-1}$.

8.33. Prove that the acute angle $\gamma$ between the $z$ axis and the normal line to the surface $F(x, y, z)=0$ at any point is given by $\sec \gamma=\sqrt{F_{x}^{2}+F_{y}^{2} F_{2}^{2}} /\left|F_{z}\right|$.

8.34. The equation of a surface is given in cylindrical coordinates by $F(\rho, \phi, z)=0$, where $F$ is continuously differentiable. Prove that the equations of (a) the tangent plane and (b) the normal line at the point $P\left(\rho_{0}, \phi_{0}\right.$, $\left.z_{0}\right)$ are given, respectively, by $A\left(x-x_{0}\right)+B\left(y-y_{0}\right)+C\left(z-z_{0}\right)=0$ and $\frac{x-x_{0}}{A}=\frac{y-y_{0}}{B}=\frac{z-z_{0}}{C}$ where $x_{0}=\rho_{0} \cos \phi_{0}, y_{0}=\rho_{0} \sin \phi_{0}$ and $A=F_{\rho}\left|p \cos \phi_{0}-\frac{1}{\rho} F_{\phi}\right| p \sin \phi_{0}, B=F_{\rho}\left|p \sin \phi_{0}+\frac{1}{\rho} F_{\phi}\right| p \cos \phi_{0}$,\\
and $C=\left.F_{z}\right|_{p}$.

8.35. Use Problem 8.34 to find the equation of the tangent plane to the surface $\pi z=\rho \phi$ at the point where $\rho=2$, $\phi=\pi / 2, z=1$. To check your answer, work the problem using rectangular coordinates.

$$
\text { Ans. } 2 x-\pi y+2 \pi z=0
$$

\section*{Tangent line and normal plane to a curve}
8.36. Find the equations of (a) the tangent line and (b) the normal plane to the space curve $x=6 \sin t, y=4 \cos 3 t$. $z=2 \sin 5 t$ at the point where $t=\pi / 4$.

$$
\text { Ans. (a) } \frac{x-3 \sqrt{2}}{3}=\frac{y+2 \sqrt{2}}{-6}=\frac{z+\sqrt{2}}{-5} \text { (b) } 3 x-6 y-5 z=26 \sqrt{2}
$$

8.37. The surfaces $x+y+z=3$ and $x^{2}-y^{2}+2 z^{2}=2$ intersect in a space curve. Find the equations of (a) the tangent line and (b) the normal plane to this space curve at the point $(1,1,1)$.

$$
\text { Ans. (a) } \frac{x-1}{-3}=\frac{y-1}{1}=\frac{z-1}{2} \text { (b) } 3 x-y-2 z=0
$$

\section*{Envelopes}
8.38. Find the envelope of each of the following families of curves in the $x y$ plane: (a) $y=a x-\alpha^{2}$ and

(b) $\frac{x^{2}}{\alpha}+\frac{y^{2}}{1-\alpha}=1$. In each case construct a graph.\\
Ans.\\
(a) $x^{2}=4 y$\\
(b) $x+y= \pm 1, x-y= \pm 1$

8.39. Find the envelope of a family of lines having the property that the length intercepted between the $x$ and $y$ axes is a constant $a$.

$$
\text { Ans. } x^{2 / 3}+y^{2 / 3}=a^{2 / 3}
$$

8.40. Find the envelope of the family of circles having centers on the parabola $y=x^{2}$ and passing through its vertex. [Hint: Let $\left(\alpha, \alpha^{2}\right)$ be any point on the parabola.]

$$
\text { Ans. } x^{2}=-y^{3} /(2 y+1)
$$

8.41. Find the envelope of the normals (called an evolute) to the parabola $y=\frac{1}{2} x^{2}$ and construct a graph.

$$
\text { Ans. } 8(y-1)^{3}=27 x^{2}
$$

8.42. Find the envelope of the following families of surfaces: (a) $\alpha(x-y)-\alpha^{2} z=1$ and (b) $(x-\alpha)^{2}+y^{2}=2 \alpha z$.

Ans. (a) $4 z=(x-y)^{2}$, (b) $y^{2}=z^{2}+2 x z$

8.43. Prove that the envelope of the two-parameter family of surfaces $F(x, y, z, \alpha, \beta)=0$, if it exists, is obtained by eliminating $\alpha$ and $\beta$ in the equations $F=0, F_{\alpha}=0$, and $F_{\beta}=0$.

8.44. Find the envelope of the two-parameter families (a) $z=\alpha x+\beta y-\alpha^{2}-\beta^{2}$ and (b) $x \cos \alpha+y \cos \beta+z \cos \gamma$ $=a$ where $\cos ^{2} \alpha+\cos ^{2} \beta+\cos ^{2} \gamma=1$ and $a$ is a constant.

Ans. (a) $4 z=x^{2}+y^{2}$ (b) $x^{2}+y^{2}+z^{2}=a^{2}$

\section*{Directional derivatives}
8.45. (a) Find the directional derivative of $U=2 x y-z^{2}$ at $(2,-1,1)$ in a direction toward $(3,1,-1)$. (b) In what direction is the directional derivative a maximum? (c) What is the value of this maximum?

Ans. (a) $10 / 3$ (b) $-2 \mathbf{i}+4 \mathbf{j}-2 \mathbf{k}$ (c) $2 \sqrt{6}$

8.46. The temperature at any point $(x, y)$ in the $x y$ plane is given by $T=100 x y /\left(x^{2}+y^{2}\right)$. (a) Find the directional derivative at the point $(2,1)$ in a direction making an angle of $60^{\circ}$ with the positive $x$ axis. (b) In what direction from $(2,1)$ would the derivative be a maximum? (c) What is the value of this maximum?

Ans. (a) $12 \sqrt{3}-6$ (b) in a direction making an angle of $\pi-\tan ^{-1} 2$ with the positive $x$ axis. or in the direction $-\mathbf{i}+2 \mathbf{j}$ (c) $12 \sqrt{5}$

8.47. Prove that if $F(\rho, \phi, z)$ is continuously differentiable, the maximum directional derivative of $F$ at any point is given by $\sqrt{\left(\frac{\partial F}{\partial \rho}\right)^{2}+\frac{1}{\rho^{2}}\left(\frac{\partial F}{\partial \phi}\right)^{2}+\left(\frac{\partial F}{\partial z}\right)^{2}}$.

\section*{Differentiation under the integral sign}
8.48. If $\phi(\alpha)=\int_{\sqrt{\alpha}}^{1 / \alpha} \cos \alpha x^{2} d x$, find $\frac{d \phi}{d \alpha}$

$$
\text { Ans. } \int_{\sqrt{\alpha}}^{1 / \alpha} x^{2} \sin \alpha x^{2} d x-\frac{1}{\alpha^{2}} \cos \frac{1}{\alpha}-\frac{1}{2 \sqrt{\alpha}} \cos \alpha^{2}
$$

8.49. (a) If $F(\alpha)=\int_{0}^{\alpha^{2}} \tan ^{-1} \frac{x}{\alpha} d x$, find $\frac{d F}{d \alpha}$ by Leibniz's rule. (b) Check the result in (a) by direct\\
integration.

$$
\text { Ans. (a) } 2 \alpha \tan ^{-1} \alpha-\frac{1}{2} \operatorname{In}\left(\alpha^{2}+1\right)
$$

8.50. Given $\int_{0}^{1} x^{p} d x=\frac{1}{p+1}, p>-1$, prove that $\int_{0}^{1} x^{p}(\operatorname{In} x)^{m} d x=\frac{(-1)^{m} m !}{(p+1)^{m+1}}$

8.51. Prove that $\int_{0}^{\pi} \operatorname{In}(1+\alpha \cos x) d x=\pi \operatorname{In}\left(\frac{1+\sqrt{1-\alpha^{2}}}{2}\right),|\alpha|<1$.

8.52. Prove that $\int_{0}^{\pi} \operatorname{In}\left(1-2 \alpha \cos x+\alpha^{2}\right) d x=\left\{\begin{array}{ll}\pi \text { In } \alpha^{2}, & |\alpha|<1 \\ 0, & |\alpha|>1\end{array}\right.$. Discuss the case $|\alpha|=1$.

8.53. Show that $\int_{0}^{\pi} \frac{d x}{(5-3 \cos x)^{3}}=\frac{59 \pi}{2048}$.

\section*{Integration under the integral sign}
8.54. Verify that $\int_{0}^{1}\left\{\int_{1}^{2}\left(\alpha^{2}-x^{2}\right) d x\right\} d \alpha=\int_{1}^{2}\left\{\int_{0}^{1}\left(\alpha^{2}-x^{2}\right) d \alpha=\int_{1}^{2}\right\} d x$

8.55. Starting with the result $\int_{0}^{2 \pi}(\alpha-\sin x) d x=2 \pi \alpha$, prove that for all constants $a$ and $\left.b, x\right) d x=2 \pi \alpha$, prove that for all constants $a$ and $b$,

$$
\int_{0}^{2 \pi}\left\{(b-\sin x)^{2}-(a-\sin x)^{2}\right\} d x=2 \pi\left(b^{2}-a^{2}\right)
$$

8.56. Use the result $\int_{0}^{2 \pi} \frac{d x}{\alpha+\sin x}=\frac{2 \pi}{\sqrt{\alpha^{2}-1}}, \alpha>1$ to prove that $\int_{0}^{2 \pi} \operatorname{In}\left(\frac{5+3 \sin x}{5+4 \sin x}\right) d x=2 \pi \operatorname{In}\left(\frac{9}{8}\right)$

8.57. (a) Use the result $\int_{0}^{\pi / 2} \frac{d x}{1+\alpha \cos x}=\frac{\cos ^{-1} \alpha}{\sqrt{1-\alpha^{2}}}, 0 \leqq \alpha<1$ to show that for $0 \leqq a<1,0 \leqq b<1$,

$$
\int_{0}^{\pi / 2} \sec x \operatorname{In}\left(\frac{1+b \cos x}{1+a \cos x}\right) d x=\frac{1}{2}\left\{\left(\cos ^{-1} a\right)^{2}-\left(\cos ^{-1} b\right)^{2}\right\}
$$

(b) Show that $\int_{0}^{\pi / 2} \sec x \operatorname{In}\left(1+\frac{1}{2} \cos x\right) d x=\frac{5 \pi^{2}}{72}$.

\section*{Maxima and minima, lagrange multipliers}
8.58. Find the maxima and minima of $\left.F(x, y, z)=x y^{2} z\right)=x y^{2} z^{3}$ subject to the conditions $x+y+z=6, x>0, y>0$, $z>0$.

8.59. What is the volume of the largest rectangular parallelepiped which can be inscribed in the ellipsoid $x^{2} / 9+$ $y^{2} / 16+z^{2} / 36=1$ ?

Ans. $64 \sqrt{3}$

8.60. (a) Find the maximum and minimum values of $x^{2}+y^{2}$ subject to the condition $3 x^{2}+4 x y+6 y^{2}=140$. (b) Give a geometrical interpretation of the results in (a).

Ans. maximum value $=70$, minimum value $=20$

8.61. Solve Problem 8.23 using Lagrange multipliers.

8.62. Prove that in any triangle $A B C$ there is a point $P$ such that $\overline{P A}^{2}+\overline{P B}^{2}+\overline{P C}^{2}$ is a minimum and that $P$ is the intersection of the medians.

8.63. (a) Prove that the maximum and minimum values of $f(x, y)=x^{2}+x y+y^{2}$ in the unit square $0 \leqq x \leqq 1,0 \leqq$ $y \leqq 1$ are 3 and 0 , respectively. (b) Can the result of (a) be obtained by setting the partial derivatives of $f(x$, $y$ ) with respect to $x$ and $y$ equal to zero. Explain.

8.64. Find the extreme values of $z$ on the surface $2 x^{2}+3 y^{2}+z^{2}-12 x y+4 x z=35$.

Ans. maximum $=5$, minimum $=-5$

8.65. Establish the method of Lagrange multipliers in the case where we wish to find the extreme values of $F(x, y$, z) subject to the two constraint conditions $G(x, y, z)=0, H(x, y, z)=0$.

8.66. Prove that the shortest distance from the origin to the curve of intersection of the surfaces $x y z=a$ and $y=b x$, where $a>0, b>0$, is $3 \sqrt{a\left(b^{2}+1\right) / 2 b}$.

8.67. Find the volume of the ellipsoid $11 x^{2}+9 y^{2}+15 z^{2}-4 x y+10 y z-20 x z=80$.

Ans. $64 \pi \sqrt{2} / 3$

\section*{Applications to errors}
8.68. The diameter of a right circular cylinder is measured as $6.0 \pm 0.03$ inches, while its height is measured as 4.0 $\pm 0.02$ inches. What is the largest possible (a) error and (b) percent error made in computing the volume?

Ans. (a) $1.70 \mathrm{in}^{3}$ (b) 1.5 percent

8.69. The sides of a triangle are measured to be 12.0 and 15.0 feet, and the included angle is $60.0^{\circ}$. If the lengths can be measured to within 1 percent accuracy, while the angle can be measured to within 2 percent accuracy, find the maximum error and percent error in determining the (a) area and (b) the opposite side of the triangle.

Ans. (a) $2.501 \mathrm{ft}^{2}, 3.21$ percent (b) $0.287 \mathrm{ft}, 2.08$ percent

\section*{Miscellaneous problems}
8.70. If $\rho$ and $\phi$ are cylindrical coordinates, $a$ and $b$ are any positive constants, and $n$ is a positive integer, prove that the surface $\rho^{n} \sin n \phi=a$ and $\rho^{n} \cos n \phi=b$ are mutually perpendicular along their curves of intersection.

8.71. Find an equation for (a) the tangent plane and (b) the normal line to the surface $8 r \theta \phi=\pi^{2}$ at the point where $r=1, \theta=\pi / 4, \phi=\pi / 2,(r, \theta, \phi)$ being spherical coordinates.

$$
\text { Ans. (a) } 4 x-\left(\pi^{2}+4 \pi\right) y+\left(4 \pi-\pi^{2}\right) z=-\pi^{2} \sqrt{2} \text { (b) } \frac{x}{-4}=\frac{y-\sqrt{2} / 2}{\pi^{2}+4 \pi}=\frac{z-\sqrt{2} / 2}{\pi^{2}-4 \pi}
$$

8.72. (a) Prove that the shortest distance from the point $(a, b, c)$ to the plane $A x+B y+C z+D=0$ is

$$
\left|\frac{A a+B b+C c+D}{\sqrt{A^{2}+B^{2}+C^{2}}}\right|
$$

(b) Find the shortest distance from $(1,2,-3)$ to the plane $2 x-3 y+6 z=20$.

Ans. (b) 6

8.73. The potential $V$ due to a charge distribution is given in spherical coordinates $(r, \theta, \phi)$ by

$$
V=\frac{p \cos \theta}{r^{2}}
$$

where $p$ is a constant. Prove that the maximum directional derivative at any point is

$$
\frac{p \sqrt{\sin ^{2} \theta+4 \cos ^{2} \theta}}{r^{3}}
$$

8.74. Prove that $\int_{0}^{1} \frac{x^{m}-x^{n}}{\ln x} d x=\ln \left(\frac{m+1}{n+1}\right)$ if $m>0, n>0$. Can you extend the result to the case $m>-1, n>-1$ ?

8.75. (a) If $b^{2}-4 a c<0$ and $a>0, c>0$, prove that the area of the ellipse $a x^{2}+b x y+c y^{2}=1$ is $2 \pi / \sqrt{4 a c-b^{2}}$. (Hint: Find the maximum and minimum values of $x^{2}+y^{2}$ subject to the constraint $a x^{2}+b x y+c y^{2}=1$.)

8.76. Prove that the maximum and minimum distances from the origin to the curve of intersection defined by $x^{2} / a^{2}$ $+y^{2} / b^{2}+z^{2} / c^{2}=1$ and $A x+B y+C z=0$ can be obtained by solving for $d$ the equation

$$
\frac{A^{2} a^{2}}{a^{2}-d^{2}}+\frac{B^{2} b^{2}}{b^{2}-d^{2}}+\frac{C^{2} c^{2}}{c^{2}-d^{2}}=0
$$

8.77. Prove that the last equation in the preceding problem always has two real solutions $d_{1}^{2}$ and $d_{2}^{2}$ for any real nonzero constants $a, b, c$ and any real constants $A, B, C$ (not all zero). Discuss the geometrical significance of this.

8.78. (a) Prove that $I_{M}=\int_{0}^{M} \frac{d x}{\left(x^{2}+\alpha^{2}\right)^{2}}=\frac{1}{2 \alpha^{3}} \tan ^{-1} \frac{M}{\alpha}+\frac{M}{2 \alpha^{2}\left(\alpha^{2}+M^{2}\right)}$.

(b) Find $\lim _{M \rightarrow \infty} \mathrm{I}_{M}$. This can be denoted by $\int_{0}^{\mathrm{x}} \frac{d x}{\left(x^{2}+\alpha^{2}\right)^{2}}$.

(c) Is $\lim _{M \rightarrow \infty} \frac{d}{d \alpha} \int_{0}^{M} \frac{d x}{\left(x^{2}+\alpha^{2}\right)^{2}}=\frac{d}{d \alpha} \lim _{M \rightarrow \infty} \int_{0}^{M} \frac{d x}{\left(x^{2}+\alpha^{2}\right)^{2}}$ ?

8.79. Find the point on the paraboloid $z=x^{2}+y^{2}$ which is closest to the point $(3,-6,4)$.

Ans. $(1,-2,5)$

8.80. Investigate the maxima and minima of $f(x, y)=\left(x^{2}-2 x+4 y^{2}-8 y\right)^{2}$.

Ans. Minimum value $=0$

8.81. (a) Prove that

$$
\int_{0}^{\pi / 2} \frac{\cos x d x}{\alpha \cos x+\sin x}=\frac{\alpha \pi}{2\left(\alpha^{2}+1\right)}-\frac{\operatorname{In} \alpha}{\alpha^{2}+1} .
$$

(b) Use (a) to Prove that $\int_{0}^{\pi / 2} \frac{\cos ^{2} x d x}{(2 \cos x+\sin x)^{2}}=\frac{3 \pi+5-8 \ln 2}{50}$.

8.82. (a) Find sufficient conditions for a relative maximum or minimum of $w=f(x, y, z)$.

(b) Examine $w=x^{2}+y^{2}+z^{2}-6 x y+8 x z-10 y z$ for maxima and minima.

[Hint: For (a) use the fact that the quadratic form $A \alpha^{2}+B \beta^{2}+C \gamma^{2}+2 D \alpha \beta+2 E \alpha \gamma+2 F \beta \gamma>0$ (i.e., is positive definite) if

$$
A>0, \quad\left|\begin{array}{ll}
A & D \\
D & B
\end{array}\right|>0 . \quad\left|\begin{array}{lll}
A & D & F \\
D & B & E \\
F & E & C
\end{array}\right|>0
$$

This page intentionally left blank

\section*{CHAPTER 9}
\section*{Multiple Integrals}
Much of the procedure for double and triple integrals may be thought of as a reversal of partial differentiation and otherwise is analogous to that for single integrals. However, one complexity that must be addressed relates to the domain of definition. With single integrals, the functions of one variable were defined on intervals of real numbers. Thus, the integrals only depended on the properties of the functions. The integrands of double and triple integrals are functions of two and three variables, respectively, and as such are defined on two- and three-dimensional regions. These regions have a flexibility in shape not possible in the singlevariable cases. For example, with functions of two variables, and the corresponding double integrals, rectangular regions $a \leqq x \leqq b, c \leqq y \leqq d$ are common. However, in many problems the domains are regions bounded above and below by segments of plane curves. In the case of functions of three variables, and the corresponding triple integrals other than the regions $a \leqq x \leqq b, c \leqq y \leqq d, e \leqq z \leqq f$, there are those bounded above and below by portions of surfaces. In very special cases, double and triple integrals can be directly evaluated. However, the systematic technique of iterated integration is the usual procedure. It is here that the reversal of partial differentiation comes into play.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-232}
\end{center}

Figure 9.1

Definitions of double and triple integrals are given as follows. Also, the method of iterated integration is described.

\section*{Double Integrals}
Let $F(x, y)$ be defined in a closed region $\Re$ of the $x y$ plane (see Figure 9.1). Subdivide $\Re$ into $n$ subregions $\Delta \Re_{k}$ of area $\Delta A_{k}, k=1,2, \ldots, n$. Let $\left(\xi_{k}, \eta_{k}\right)$ be some point of $\Delta A_{k}$. Form the sum


\begin{equation*}
\sum_{k=1}^{n} F\left(\xi_{k}, \eta_{k}\right) \Delta A_{k} \tag{1}
\end{equation*}


Consider


\begin{equation*}
\lim _{n \rightarrow \infty} \sum_{k=1}^{n} F\left(\xi_{k}, \eta_{k}\right) \Delta A_{k} \tag{2}
\end{equation*}


where the limit is taken so that the number $n$ of subdivisions increases without limit and such that the largest linear dimension of each $\Delta A_{k}$ approaches zero. See Figure 9.2(a). If this limit exists, it is denoted by


\begin{equation*}
\int_{\mathfrak{R}} \int F(x, y) d A \tag{3}
\end{equation*}


and is called the double integral of $F(x, y)$ over the region $\Re$.

It can be proved that the limit does exist if $F(x, y)$ is continuous (or sectionally continuous) in $\Re$.

The double integral has a great variety of interpretations with any individual one dependent on the form of the integrand. For example, if $F(x, y)=\rho(x, y)$ represents the variable density of a flat iron plate, then the double integral $\int_{A} \rho d A$ of this function over a same-shaped plane region $A$ is the mass of the plate. In Figure 9.2(b) we assume that $F(x, y)$ is a height function [established by a portion of a surface $z=F(x, y)$ ] for a cylindrically shaped object. In this case the double integral represents a volume.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-233}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-233(1)}
\end{center}

(b)

Figure 9.2

\section*{Iterated Integrals}
If $\Re$ is such that any lines parallel to the $y$ axis meet the boundary of $\Re$ in, at most, two points (as is true in Figure 9.1), then we can write the equations of the curves $A C B$ and $A D B$ bounding $\Re$ as $y=f_{1}(x)$ and $y=$ $f_{2}(x)$, respectively, where $f_{1}(x)$ and $f_{2}(x)$ are single-valued and continuous in $a \leqq x \leqq b$. In this case we can evaluate the double integral (3) by choosing the regions $\Delta \Re_{k}$ as rectangles formed by constructing a grid of lines parallel to the $x$ and $y$ axes and $\Delta A_{k}$ as the corresponding areas. Then Equation (3) can be written


\begin{align*}
\iint_{\Re} F(x, y) d x d y & =\int_{x=a}^{b} \int_{y=f_{1}(x)}^{f_{2}(x)} F(x, y) d y d x  \tag{4}\\
& =\int_{x=a}^{b}\left\{\int_{y=f_{1}(x)}^{f_{2}(x)} F(x, y) d y\right\} d x
\end{align*}


where the integral in braces is to be evaluated first (keeping $x$ constant) and finally integrating with respect to $x$ from $a$ to $b$. The result (4) indicates how a double integral can be evaluated by expressing it in terms of two single integrals called iterated integrals.

The process of iterated integration is visually illustrated in Figure 9.3(a) and (b) and further illustrated as follows.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-234(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-234}
\end{center}

(b)

Figure 9.3

The general idea, as demonstrated with respect to a given three-space region, is to establish a plane section, integrate to determine its area, and then add up all the plane sections through an integration with respect to the remaining variable. For example, choose a value of $x$ (say, $x=x^{\prime}$ ). The intersection of the plane $x=x^{\prime}$ with the solid establishes the plane section. In it, $z=F\left(x^{\prime}, y\right)$ is the height function, and if $y=f_{1}(x)$ and $y=$ $f_{2}(x)$ for all $\left.z\right)$ are the bounding cylindrical surfaces of the solid, then the width is $f_{2}\left(x^{\prime}\right)-f_{1}\left(x^{\prime}\right)$, i.e., $y_{2}-y_{1}$. Thus, the area of the section is $A=\int_{y_{1}}^{y_{2}} F\left(x^{\prime}, y\right) d y$. Now establish slabs $A_{j} \Delta x_{j}$, where, for each interval $\Delta x_{j}=$ $x_{j}-x_{j-\mathrm{r}}$, there is an intermediate value $x_{j}^{\prime}$. Then sum these to get an approximation to the target volume. Adding the slabs and taking the limit yields

$$
V=\lim _{x \rightarrow \infty} \sum_{j=1}^{n} A_{j} \Delta x_{j}=\int_{a}^{b}\left(\int_{y_{1}}^{y_{2}} F(x, y) d x\right) d x
$$

In some cases the order of integration is dictated by the geometry. For example, if $\Re$ is such that any lines parallel to the $x$ axis meet the boundary of $\Re$ in, at most, two points (as in Figure 9.1), then the equations of curves $C A D$ and $C B D$ can be written $x=g_{1}(y)$ and $x=g_{2}(y)$, respectively, and we find, similarly,


\begin{align*}
\iint_{\Re} F(x, y) d x d y & =\int_{y=c}^{d} \int_{x=g_{1}(y)}^{g_{2}(y)} F(x, y) d x d y  \tag{5}\\
& =\int_{y=c}^{d}\left\{\int_{x=g_{1}(y)}^{g_{2}(y)} F(x, y) d x\right\} d y
\end{align*}


If the double integral exists, Equations (4) and (5) yield the same value. (See, however, Problem 9.21.) In writing a double integral, either of the forms (4) or (5), whichever is appropriate, may be used. We call one form an interchange of the order of integration with respect to the other form.

In case $\Re$ is not of the type shown in Figure 9.3 , it can generally be subdivided into regions $\Re_{1}, \Re_{2}, \ldots$, which are of this type. Then the double integral over $\Re$ is found by taking the sum of the double integrals over $\Re_{1}, \Re_{2}, \ldots$

\section*{Triple Integrals}
These results are easily generalized to closed regions in three dimensions. For example, consider a function $F(x, y, z)$ defined in a closed three-dimensional region $\Re$. Subdivide the region into $n$ subregions of volume $\Delta V_{k}, k=1,2, \ldots, n$. Letting $\left(\xi_{k}, \eta_{k}, \zeta_{k}\right)$ be some point in each subregion, we form


\begin{equation*}
\lim _{x \rightarrow \infty} \sum_{k=1}^{n} F\left(\xi_{k}, \eta_{k}, \xi_{k}\right) \Delta V_{k} \tag{6}
\end{equation*}


where the number $n$ of subdivisions approaches infinity in such a way that the largest linear dimension of each subregion approaches zero. If this limit exists, we denote it by


\begin{equation*}
\iiint_{\Re} F(x, y, z) d V \tag{7}
\end{equation*}


called the triple integral of $F(x, y, z)$ over $\Re$. The limit does exist if $F(x, y, z)$ is continuous (or piecemeal continuous) in $\Re$.

If we construct a grid consisting of planes parallel to the $x y, y z$, and $x z$ planes, the region $\Re$ is subdivided into subregions which are rectangular parallelepipeds. In such case we can express the triple integral over $\Re$ given by (7) as an iterated integral of the form


\begin{equation*}
\int_{x=a}^{b} \int_{y=g_{1}(x)}^{g_{2}(a)} \int_{z=f_{1}(x, y)}^{f_{2}(x, y)} F(x, y, z) d x d y d z=\int_{x=a}^{b}\left[\int_{y=g_{1}(x)}^{g_{2}(x)}\left\{\int_{z=f_{1}(x, y)}^{f_{2(x, Y)}} F(x, y, z) d z\right\} d y\right] d x \tag{8}
\end{equation*}


(where the innermost integral is to be evaluated first) or the sum of such integrals. The integration can also be performed in any other order to give an equivalent result.

The interated triple integral is a sequence of integrations, first from surface portion to surface portion, then from curve segment to curve segment, and finally from point to point. (See Figure 9.4.)

Extensions to higher dimensions are also possible.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-235}
\end{center}

Figure 9.4

\section*{Transformations of Multiple Integrals}
In evaluating a multiple integral over a region $\Re$, it is often convenient to use coordinates other than rectangular, such as the curvilinear coordinates considered in Chapters 6 and 7.

If we let $(u, v)$ be curvilinear coordinates of points in a plane, there will be a set of transformation equations $x=f(u, v), y=g(u, v)$ mapping points $(x, y)$ of the $x y$ plane into points $(u, v)$ of the $u v$ plane.

In such case the region $\Re$ of the $x y$ plane is mapped into a region $\Re^{\prime}$ of the $u v$ plane. We then have


\begin{equation*}
\iint F(x, y) d x d y=\iint G(u, v)\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v \tag{9}
\end{equation*}


where $G(u, v), \equiv F\{f(u, v), g(u, v)\}$ and

\[
\frac{\partial(x, y)}{\partial(u, v)} \equiv\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}  \tag{10}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|
\]

is the Jacobian of $x$ and $y$ with respect to $u$ and $v$ (see Chapter 6).

Similarly, if $(u, v, w)$ are curvilinear coordinates in three dimensions, there will be a set of transformation equations $x=f(u, v, w), y=g(u, v, w), z=h(u, v, w)$ and we can write


\begin{equation*}
\iiint_{\Re} F(x, y, z) d x d y d z=\iiint_{\Re} G(u, v, w)\left|\frac{\partial(x, y, z)}{\partial(u, v, w)}\right| d u d v d w \tag{11}
\end{equation*}


where $G(u, v, w) \equiv F\{(f(u, v, w), g(u, v, w), h(u, v, w)\}$ and

\[
\frac{\partial(x, y, z)}{\partial(u, v, w)} \equiv\left|\begin{array}{lll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \frac{\partial x}{\partial w}  \tag{12}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} & \frac{\partial y}{\partial w} \\
\frac{\partial z}{\partial u} & \frac{\partial z}{\partial v} & \frac{\partial z}{\partial w}
\end{array}\right|
\]

is the Jacobian of $x, y$, and $z$ with respect to $u$, $v$, and $w$.

The results (9) and (11) correspond to change of variables for double and triple integrals. Generalizations to higher dimensions are easily made.

\section*{The Differential Element of Area in Polar Coordinates, Differential Elements of Area in Cylindral and Spherical Coordinates}
Of special interest is the differential element of area $d A$ for polar coordinates in the plane, and the differential elements of volume $d V$ for cylindrical and spherical coordinates in three-space. With these in hand, the double and triple integrals as expressed in these systems are seen to take the following forms. (See Figure 9.5.)

The transformation equations relating cylindrical coordinates to rectangular Cartesian ones appear in Chapter 7, in particular,

$$
x=\rho \cos \phi, y=\rho \sin \phi, z=z
$$

The coordinate surfaces are circular cylinders, planes, and planes. (See Figure 9.5.)

At any point of the space (other than the origin), the set of vectors $\left\{\frac{\partial \mathrm{r}}{\partial \rho}, \frac{\partial \mathrm{r}}{\partial \phi}, \frac{\partial \mathrm{r}}{\partial z}\right\}$ constitutes an orthogo-\\
al basis.\\
\includegraphics[max width=\textwidth, center]{2024_04_03_ffb6ac533fe0a53b3ceeg-237}

Figure 9.5

In the cylindrical case, $\mathbf{r}=\rho \cos \phi \mathbf{i}+\rho \sin \phi \mathbf{j}+z \mathbf{k}$ and the set is

$$
\frac{\partial \mathbf{r}}{\partial \rho}=\cos \phi i+\sin \phi \mathbf{j}, \quad \frac{\partial \mathbf{r}}{\partial \rho}=-\rho \sin \phi i+\rho \cos \phi \mathbf{j}, \quad \frac{\partial \mathbf{r}}{\partial z}=\mathbf{k}
$$

Therefore, $\frac{\partial \mathbf{r}}{\partial \rho} \cdot \frac{\partial \mathbf{r}}{\partial \phi} \times \frac{\partial \mathbf{r}}{\partial z}=\rho$.

That the geometric interpretation of $\frac{\partial \mathbf{r}}{\partial \rho} \cdot \frac{\partial \mathbf{r}}{\partial \phi} \times \frac{\partial \mathbf{r}}{\partial z} d \rho d \phi d z$ is an infinitesimal rectangular parallelepiped suggests that the differential element of volume in cylindrical coordinates is

$$
d V=\rho d \rho d \phi d z
$$

Thus, for an integrable but otherwise arbitrary function $F(\rho, \phi, z)$ of cylindrical coordinates, the iterated triple integral takes the form

$$
\int_{z_{1}}^{z_{2}} \int_{g_{1}(z)}^{g_{2}(z)} \int_{f_{1}(\phi, z)}^{f_{2}(\phi, z)} F(\rho, \phi, z) \rho d \rho d \phi d z
$$

The differential element of area for polar coordinates in the plane results by suppressing the $z$ coordinate. It is

$$
d A=\left|\frac{\partial \mathbf{r}}{\partial \rho} \times \frac{\partial \mathbf{r}}{\partial \phi}\right| d \rho d \phi
$$

and the iterated form of the double integral is

$$
\int_{\rho_{1}}^{\rho_{2}} \int_{\phi_{1}(\rho)}^{\phi_{2}(\rho)} F(\rho, \phi) \rho d \rho d \phi
$$

The transformation equations relating spherical and rectangular Cartesian coordinates are

$$
x=r \sin \theta \cos \phi, y=r \sin \theta \sin \phi, z=r \cos \theta
$$

In this case the coordinate surfaces are spheres, cones, and planes. (See Figure 9.5.)

Following the same pattern as with cylindrical coordinates we discover that

$$
d V=r^{2} \sin \theta d r d \theta d \phi
$$

and the iterated triple integral of $F(r, \theta, \phi)$ has the spherical representation

$$
\int_{r_{1}}^{r_{2}} \int_{\theta_{1}(\phi)}^{\theta_{2}(\phi)} \int_{\phi_{1}(r, \theta)}^{\phi_{2}(r, \theta)} F(r, \theta, \phi) r^{2} \sin \theta d r d \theta d \phi
$$

Of course, the order of these integrations may be adapted to the geometry.

The coordinate surfaces in spherical coordinates are spheres, cones, and planes. If $r$ is held constant—say, $r=a$-then we obtain the differential element of surface area

$$
d A=a^{2} \sin \theta d \theta d \phi
$$

The first octant surface area of a sphere of radius $a$ is

$$
\int_{0}^{\pi / 2} \int_{0}^{\pi / 2} a^{2} \sin \theta d \theta d \phi=\int_{0}^{\pi / 2} a^{2}(-\cos \theta)_{0}^{\frac{\pi}{2}} d \phi=\int_{0}^{\pi / 2} a^{2} d \phi=a^{2} \frac{\pi}{2}
$$

Thus, the surface area of the sphere is $4 \pi a^{2}$.

\section*{SOLVED PROBLEMS}
\section*{Double integrals}
9.1. (a) Sketch the region $\Re$ in the $x y$ plane bounded by $y=x^{2}, x=2, y=1$. (b) Give a physical interpretation to $\iint_{\Re}\left(x^{2}+y^{2}\right) d x d y$. (c) Evaluate the double integral in (b).

(a) The required region $\Re$ is shown shaded in Figure 9.6.

(b) Since $x^{2}+y^{2}$ is the square of the distance from any point $(x, y)$ to $(0,0)$, we can consider the double integral as representing the polar moment of inertia (i.e., moment of intertia with respect to the origin) of the region $\Re$ (assuming unit density).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-238}
\end{center}

Figure 9.6

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-238(1)}
\end{center}

Figure 9.7

We can also consider the double integral as representing the mass of the region $\Re$, assuming a density varying as $x^{2}+y^{2}$.

(c) Method 1: The double integral can be expressed as the iterated integral

$$
\begin{aligned}
\int_{x=1}^{2} \int_{y=1}^{x^{2}}\left(x^{2}+y^{2}\right) d y d x & =\int_{x=1}^{2}\left\{\int_{y=1}^{x^{2}}\left(x^{2}+y^{2}\right) d y\right\} d x=\int_{x=1}^{2} x^{2} y+\left.\frac{y^{2}}{3}\right|_{y=1} ^{x^{2}} \\
d x & =\int_{x=1}^{2}\left(x^{4}+\frac{x^{6}}{3}-x^{2}-\frac{1}{3}\right) d x=\frac{1006}{105}
\end{aligned}
$$

The integration with respect to $y$ (keeping $x$ constant) from $y=1$ to $y=x^{2}$ corresponds formally to summing in a vertical column (see Figure 9.6). The subsequent integration with respect to $x$ from $x=1$ to $x=2$ corresponds to addition of contributions from all such vertical columns between $x=1$ and $x=2$.

Method 2: The double integral can also be expressed as the iterated integral

$$
\begin{aligned}
\int_{y=1}^{4} \int_{x=\sqrt{y}}^{2}\left(x^{2}+y^{2}\right) d x d y & =\int_{y=1}^{4}\left\{\int_{x=\sqrt{y}}^{2}\left(x^{2}+y^{2}\right) d x\right\} d y=\int_{y=1}^{4} \frac{x^{3}}{3}+\left.x y^{2}\right|_{x=\sqrt{y}} ^{2} d y \\
& =\int_{x=1}^{2}\left(x^{4}+\frac{x^{6}}{3}-x^{2}-\frac{1}{3}\right) d x=\frac{1006}{105}
\end{aligned}
$$

In this case the vertical column of region $\Re$ in Figure 9.6 is replaced by a horizontal column, as in Figure 9.7. Then the integration with respect to $x$ (keeping $y$ constant) from $x=\sqrt{y}$ to $x=2$ corresponds to summing in this horizontal column. Subsequent integration with respect to $y$ from $y=1$ to $y=4$ corresponds to addition of contributions for all such horizontal columns between $y=1$ and $y=4$.

9.2. Find the volume of the region bounded by the elliptic paraboloid $z=4-x^{2}-\frac{1}{4} y^{2}$ and the plane $z=0$.

Because of the symmetry of the elliptic paraboloid, the result can be obtained by multiplying the first octant volume by 4 .

Letting $z=0$ yields $4 x^{2}+y^{2}=16$. The limits of integration are determined from this equation. The required volume is

$$
4 \int_{0}^{2} \int_{0}^{2 \sqrt{4-x^{2}}}\left(4-x^{2}-\frac{1}{4} y^{2}\right) d y d x=4 \int_{0}^{2}\left(4 y-x^{2} y-\frac{1}{4} \frac{y^{3}}{3}\right)_{0}^{2 \sqrt{4-x^{2}}} d x=16 \Pi
$$

Hint: Use trigonometric substitutions to complete the integrations.

9.3. The geometric model of a material body is a plane region $R$ bounded by $y=x^{2}$ and $y=\sqrt{2-x^{2}}$ on the interval $0 \leqq x \leqq 1$, and with a density function $\rho=x y$. (a) Draw the graph of the region. (b) Find the mass of the body. (c) Find the coordinates of the center of mass.

(a) See Figure 9.8.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-239}
\end{center}

Figure 9.8

(b)

$$
\begin{aligned}
M & =\int_{a}^{b} \int_{f_{1}}^{f_{2}} \rho d y d x=\int_{0}^{1} \int_{x^{2}}^{\sqrt{2-x^{2}}} y x d y d x=\int_{0}^{1}\left[\frac{y^{2}}{2}\right]_{x^{2}}^{\sqrt{2-x^{2}}} x d x \\
& =\int_{0}^{1} \frac{1}{2} x\left(2-x^{2}-x^{4}\right) d x=\left[\frac{x^{2}}{2}-\frac{x^{4}}{8}-\frac{x^{6}}{12}\right]_{0}^{1}=\frac{7}{24}
\end{aligned}
$$

(c) The coordinates of the center of mass are defined to be

$$
\bar{x}=\frac{1}{M} \int_{a}^{b} \int_{f_{1}(x)}^{f_{2}(x)} x \rho d y d x \text { and } \bar{y}=\frac{1}{M} \int_{a}^{b} \int_{f_{1}(x)}^{f_{2}(x)} y \rho d y d x
$$

where

$$
M=\int_{a}^{b} \int_{f_{1}(x)}^{f_{2}(x)} \rho d y d x
$$

Thus,

$$
\begin{aligned}
M \bar{x} & =\int_{0}^{1} \int_{x^{2}}^{\sqrt{2 x-x^{2}}} x x y d y d x=\int_{0}^{1} x^{2}\left[\frac{y^{2}}{2}\right]_{x^{2}}^{\sqrt{2-x^{2}}} d x=\int_{0}^{1} x^{2} \frac{1}{2}\left[2-x^{2}-x^{4}\right] d x \\
& =\left[\frac{x^{3}}{3}-\frac{x^{5}}{10}-\frac{x^{7}}{14}\right]_{0}^{1}=-\frac{1}{3}-\frac{1}{10} \frac{1}{14}=\frac{17}{105} \\
M \bar{y} & =\int_{0}^{1} \int_{x^{2}}^{\sqrt{2 x-5}} y x d y d x=-\frac{13}{120}+4 \frac{\sqrt{2}}{15}
\end{aligned}
$$

9.4. Find the volume of the region common to the intersecting cylinders $x^{2}+y^{2}=a^{2}$ and $x^{2}+z^{2}=a^{2}$.

Required volume $=8$ times volume of region shown in Figure 9.9

$$
\begin{aligned}
& =8 \int_{x=0}^{a} \int_{y=0}^{\sqrt{a^{2}-x^{2}}} z d y d x \\
& =8 \int_{x=0}^{a} \int_{y=0}^{\sqrt{a^{2}-x^{2}}}=\sqrt{a^{2}-x^{2}} d y d x \\
& =8 \int_{x=0}^{a}\left(a^{2}-x^{2}\right) d x=\frac{16 a^{3}}{3}
\end{aligned}
$$

As an aid in setting up this integral, note that $z d y d x$ corresponds to the volume of a column such as shown darkly shaded in Figure 9.9. Keeping $x$ constant and integrating with respect to $y$ from $y=0$ to $y=\sqrt{a^{2}-x^{2}}$ corresponds to adding the volumes of all such columns in a slab parallel to the $y z$ plane, thus giving the volume of this slab. Finally, integrating with respect to $x$ from $x=0$ to $x=a$ corresponds to adding the volumes of all such slabs in the region, thus giving the required volume.

9.5. Find the volume of the region bounded by $z=x+y, z=6, x=0, y=0, z=0$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-240(1)}
\end{center}

Figure 9.9

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-240}
\end{center}

Figure 9.10

Required volume $=$ volume of region shown in Figure 9.10

$$
\begin{aligned}
& =\int_{x=0}^{6} \int_{y=0}^{6-x}\{6-(x+y)\} d y d x \\
& =\int_{x=0}^{6}(6-x) y-\left.\frac{1}{2} y^{2}\right|_{y=0} ^{6-x} d x \\
& =\int_{x=0}^{6} \frac{1}{2}(6-x)^{2} d x=36
\end{aligned}
$$

In this case the volume of a typical column (shown darkly shaded) corresponds to $\{6-(x+y)\} d y d x$. The limits of integration are then obtained by integrating over the region $\Re$ of Figure 9.10. Keeping $x$ constant and integrating with respect to $y$ from $y=0$ to $y=6-x$ (obtained from $z=6$ and $z=x+y$ ) corresponds to summing all columns in a slab parallel to the $y z$ plane. Finally, integrating with respect to $x$ from $x=0$ to $x=6$ corresponds to adding the volumes of all such slabs and gives the required volume.

\section*{Transformation of double integrals}
9.6. Justify Equation (9), Page 225, for changing variables in a double integral.

In rectangular coordinates, the double integral of $F(x, y)$ over the region $\Re$ (shaded in Figure 9.11) is $\iint_{\Re}(F(x, y) d x d y$. We can also evaluate this double integral by considering a grid formed by a family of $u$ and $v$ curvilinear coordinate curves constructed on the region $\mathfrak{R}$, as shown in Figure 9.11.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-241}
\end{center}

Figure 9.11

Let $P$ be any point with coordinates $(x, y)$ or $(u, v)$, where $x=f(u, v)$ and $y=g(u, v)$. Then the vector $\mathbf{r}$ from $O$ to $p$ is given by $\mathbf{r}=x \mathbf{i}+y \mathbf{j}=f(u, v) \mathbf{i}+g(u, v) \mathbf{j}$. The tangent vectors to the coordinate curves $u=c_{1}$ and $v=c_{2}$, where $c_{1}$ and $c_{2}$ are constants, are $\partial \mathbf{r} / \partial v$ and $\partial \mathbf{r} / \partial u$, respectively. Then the area of region $\Delta \Re$ of Figure 9.11 is given approximately by $\left|\frac{\partial \mathbf{r}}{\partial u} \times \frac{\partial \mathbf{r}}{\partial v}\right| \Delta u \Delta v$.

But

$$
\frac{\partial \mathbf{r}}{\partial u} \times \frac{\partial \mathbf{r}}{\partial v}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} & 0 \\
\frac{\partial x}{\partial v} & \frac{\partial y}{\partial v} & 0
\end{array}\right|=\left|\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} \\
\frac{\partial x}{\partial v} & \frac{\partial y}{\partial v}
\end{array}\right| \mathbf{k}=\frac{\partial(x, y)}{\partial(u, v)} \mathbf{k}
$$

so that

$$
\left|\frac{\partial \mathbf{r}}{\partial u} \times \frac{\partial \mathbf{r}}{\partial v}\right| \Delta u \Delta v=\left|\frac{\partial(x, y)}{\partial(u, v)}\right| \Delta u \Delta v
$$

The double integral is the limit of the sum

$$
\sum F\{f(u, v), g(u, v)\}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| \Delta u \Delta v
$$

taken over the entire region $\Re$. An investigation reveals that this limit is

$$
\iint_{\Re} F\{f(u, v), g(u, v)\}=\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v
$$

where $\Re^{\prime}$ is the region in the $u v$ plane into which the region $\Re$ is mapped under the transformation $x=f(u, v)$, $y=g(u, v)$.

Another method of justifying this method of change of variables makes use of line integrals and Green's theorem in the plane (see Problem 10.32).

9.7. If $u=x^{2}-y^{2}$ and $v=2 x y$, find $\partial(u, v)$ in terms of $u$ and $v$.

$$
\frac{\partial(u, v)}{\partial(x, y)}=\left|\begin{array}{ll}
u_{x} & u_{y} \\
v_{x} & v_{y}
\end{array}\right|=\left|\begin{array}{cc}
2 x & -2 y \\
2 y & 2 x
\end{array}\right|=4\left(x^{2}+y^{2}\right)
$$

From the identity $\left(x^{2}+y^{2}\right)^{2}=\left(x^{2}-y^{2}\right)^{2}+(2 x y)^{2}$, we have

$$
\left(x^{2}+y^{2}\right)^{2}=u^{2}+v^{2} \quad \text { and } \quad x^{2}+y^{2}=\sqrt{u^{2}+v^{2}}
$$

Then, by Problem 6.43,

$$
\frac{\partial(x, y)}{\partial(u, v)}=\frac{1}{\partial(u, v) / \partial(x, y)}=\frac{1}{4\left(x^{2}+y^{2}\right)}=\frac{1}{4 \sqrt{u^{2}+v^{2}}}
$$

Another method: Solve the given equations for $x$ and $y$ in terms of $u$ and $v$ and find the Jacobian directly.

9.8. Find the polar moment of inertia of the region in the $x y$ plane bounded by $x^{2}-y^{2}=1, x^{2}-y^{2}=9, x y=2, x y=$ 4 , assuming unit density.

Under the transformation $x^{2}-y^{2}=u, 2 x y=v$, the required region $\Re$ in the $x y$ plane, shaded in Figure 9.12(a), is mapped into region $\Re^{\prime}$ of the $u v$ plane, shaded in Figure 9.12(b). Then:

$$
\begin{aligned}
\text { Required polar moment of inertia } & =\iint_{\Re}\left(x^{2}+y^{2}\right) d x d y=\iint_{\Re^{\prime}}\left(x^{2}+y^{2}\right)\left|\begin{array}{l}
\partial(x, y) \\
\partial(u, v)
\end{array}\right| d u d v \\
& =\iint_{\Re^{\prime}} \sqrt{u^{2}+v^{2}} \frac{d u d v}{4 \sqrt{u^{2}+v^{2}}}=\frac{1}{4} \int_{u=1}^{9} \int_{v=4}^{8} d u d v=8
\end{aligned}
$$

where we have used the results of Problem 9.7.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-242(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-242}
\end{center}

(b)

Figure 9.12

Note that the limits of integration for the region $\Re^{\prime}$ can be constructed directly from the region $\Re$ in the $x y$ plane without actually constructing the region $\mathfrak{R}^{\prime}$. In such case we use a grid, as in Problem 9.6. The coordinates $(u, v)$ are curvilinear coordinates, in this case called hyperbolic coordinates.

9.9 Evaluate $\iint_{\Re} \sqrt{x^{2}+y^{2}} d x d y$, where $\Re$ is the region in the $x y$ plane bounded by $x^{2}+y^{2}=4$ and $x^{2}+y^{2}=9$.

The presence of $x^{2}+y^{2}$ suggests the use of polar coordinates $(\rho, \phi)$, where $x=\rho \cos \phi, y=\rho \sin \phi$ (see Problem 6.39). Under this transformation the region $\Re$ [Figure 9.13(a)] is mapped into the region $\Re^{\prime}$ [Figure $9.13(b)]$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-243}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-243(1)}
\end{center}

(b)

Figure 9.13

Since $\frac{\partial(x, y)}{\partial(\rho, \phi)}=\rho$, it follows that

$$
\begin{aligned}
\iint_{\Re} \sqrt{x^{2}+y^{2}} d x d y & =\iint_{\mathcal{R}^{\prime}} \sqrt{x^{2}+y^{2}}\left|\frac{\partial(x, y)}{\partial(\rho, \phi)}\right| d \rho d \phi=\iint_{\mathcal{K}^{\prime}} \rho \cdot \rho d d \phi \\
& =\int_{\phi=0}^{2 \pi} \int_{\rho=2}^{3} \rho^{2} d \rho d \phi=\left.\int_{\phi=0}^{2 \pi} \frac{\rho^{3}}{3}\right|_{2} ^{3} d \phi=\int_{\phi=0}^{2 \pi} \frac{19}{3} d \phi=\frac{38 \pi}{3}
\end{aligned}
$$

We can also write the integration limits for $\mathfrak{R}^{\prime}$ immediately on observing the region $\mathfrak{R}$, since for fixed $\phi$. $\rho$ varies from $\rho=2$ to $\rho=3$ within the sector shown dashed in Figure 9.13(a). An integration with respect to $\phi$ from $\phi=0$ to $\phi=2 \pi$ then gives the contribution from all sectors. Geometrically, $\rho d \rho d \phi$ represents the area $d A$, as shown in Figure 9.13(a).

9.10. Find the area of the region in the $x y$ plane bounded by the lemniscate $\rho^{2}=a^{2} \cos 2 \phi$.

Here the curve is given directly in polar coordinates $(\rho, \phi)$. By assigning various to $\phi$ and finding corresponding values of $\rho$, we obtain the graph shown in Figure 9.14. The required area (making use of symmetry) is

$$
\begin{aligned}
4 \int_{\phi=0}^{\pi / 4} \int_{\rho=0}^{a \sqrt{\cos 2 \phi}} \rho d \rho d \phi & =\left.4 \int_{\phi=0}^{\pi / 4} \frac{\rho^{3}}{2}\right|_{\rho=0} ^{a \sqrt{\cos 2 \phi}} d \phi \\
& =2 \int_{\phi=0}^{\pi / 4} a^{2} \cos 2 \phi d \phi=\left.a^{2} \sin 2 \phi\right|_{\phi=0} ^{\pi / 4}=a^{2}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-244}
\end{center}

Figure 9.14

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-244(1)}
\end{center}

Figure 9.15

\section*{Triple integrals}
9.11. (a) Sketch the three-dimensional region $\Re$ bounded by $x+y+z=a(a>0), x=0, y=0, z=0$. (b) Give a physical interpretation to

$$
\iiint_{\Re}\left(x^{2}+y^{2}+z^{2}\right) d x d y d z
$$

(c) Evaluate the triple integral in (b).

(a) The required region $\Re$ is shown in Figure 9.15.

(b) Since $x^{2}+y^{2}+z^{2}$ is the square of the distance from any point $(x, y, z)$ to $(0,0,0)$, we can consider the triple integral as representing the polar moment of inertia (i.e., moment of inertia with respect to the origin) of the region $\Re$ (assuming unit density).

We can also consider the triple integral as representing the mass of the region if the density varies as $x^{2}+y^{2}+z^{2}$.

(c) The triple integral can be expressed as the iterated integral

$$
\begin{aligned}
\int_{x=0}^{a} \int_{y=0}^{a-x} \int_{z=0}^{a-x-y}\left(x^{2}\right. & \left.+y^{2}+z^{2}\right) d z d y d x \\
& =\int_{x=0}^{a} \int_{y=0}^{a-x} x^{2} z+y^{2} z+\left.\frac{z^{3}}{3}\right|_{z=0} ^{a-x-y} d y d x \\
& =\int_{x=0}^{a} \int_{y=0}^{a-x}\left\{x^{2}(a-x)-x^{2} y+(a-x) y^{2}-y^{3}+\frac{(a-x-y)^{3}}{3}\right\} d y d x \\
& =\int_{x=0}^{a} x^{2}(a-x) y-\frac{x^{2} y^{2}}{2}+\frac{(a-x) y^{3}}{3}-\frac{y^{4}}{4}-\left.\frac{(a-x-y)^{4}}{12}\right|_{y=0} ^{a-x} d x \\
& =\int_{0}^{a}\left\{x^{2}(a-x)^{2}-\frac{x^{2}(a-x)^{2}}{2}+\frac{(a-x)^{4}}{3}-\frac{(a-x)^{4}}{4}+\frac{(a-x)^{4}}{12}\right\} d x \\
& =\int_{0}^{a}\left\{\frac{x^{2}(a-x)^{2}}{2}+\frac{(a-x)^{4}}{6}\right\} d x=\frac{a^{5}}{20}
\end{aligned}
$$

The integration with respect to $z$ (keeping $x$ and $y$ constant) from $z=0$ to $z=a-x-y$ corresponds to summing the polar moments of inertia (or masses) corresponding to each cube in a vertical column. The subsequent integration with respect to $y$ from $y=0$ to $y=a-x$ (keeping $x$ constant) corresponds to addition of contributions from all vertical columns contained in a slab parallel to the $y z$ plane. Finally, integration with respect to $x$ from $x=0$ to $x=a$ adds up contributions from all slabs parallel to the $y z$ plane.

Although this integration has been accomplished in the order $z, y, x$, any other order is is clearly possible and the final answer should be the same.

9.12. Find (a) the volume and (b) the centroid of the region $\Re$ bounded by the parabolic cylinder $z=4-x^{2}$ and the planes $x=0, y=6, z=0$, assuming the density to be a constant $\sigma$.

The region $\mathfrak{R}$ is shown in Figure 9.16.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-245}
\end{center}

Figure 9.16

(a) Required volume $=\iiint \mathrm{dx} d y d z$

$$
\begin{aligned}
& =\int_{x=0}^{2} \int_{y=0}^{6} \int_{z=0}^{4-x^{2}} d z d y d x \\
& =\int_{x=0}^{2} \int_{y=0}^{6}\left(4-x^{2}\right) d y d x \\
& =\left.\int_{x=0}^{2}\left(4-x^{2}\right) y\right|_{y=0} ^{6} d x \\
& =\int_{x=0}^{2}\left(2-6 x^{2}\right) d x=32
\end{aligned}
$$

(b) Total mass $=\int_{x=0}^{2} \int_{y=0}^{6} \int_{z=0}^{4-x^{2}} \sigma d z d y d x=32 \sigma$ by (a), since $\sigma$ is constant. Then

$$
\begin{aligned}
& \bar{x}=\frac{\text { Total moment about } y z \text { plane }}{\text { Total mass }}=\frac{\int_{\mathrm{x}=0}^{2} \int_{\mathrm{y}=0}^{6} \int_{\mathrm{z}=0}^{4-\mathrm{x}^{2}} \sigma x d z d y d x}{\text { Total mass }}=\frac{24}{32 \sigma}=\frac{3}{4} \\
& \bar{y}=\frac{\text { Total moment about } x z \text { plane }}{\text { Total mass }}=\frac{\int_{\mathrm{x}=0}^{2} \int_{\mathrm{y}=0}^{6} \int_{\mathrm{z}=0}^{4-\mathrm{x}^{2}} \sigma y d z d y d x}{\text { Total mass }}=\frac{96 \sigma}{32 \sigma}=3 \\
& \bar{z}=\frac{\text { Total moment about } x y \text { plane }}{\text { Total mass }}=\frac{\int_{\mathrm{x}=0}^{2} \int_{\mathrm{y}=0}^{6} \int_{\mathrm{z}=0}^{4-\mathrm{x}^{2}} \sigma z d z d y d x}{\text { Total mass }}=\frac{256 \sigma / 5}{32 \sigma}=\frac{8}{5}
\end{aligned}
$$

Thus, the centroid has coordinates $(3 / 4,3,8 / 5)$.

Note that the value for $\bar{y}$ could have been predicted because of symmetry.

\section*{Transformation of triple integrals}
9.13. Justify Equation (11), Page 225, for changing variables in a triple integral.

By analogy with Problem 9.6, we construct a grid of curvilinear coordinate surfaces which subdivide the region $\Re$ into subregions, a typical one of which is $\Delta \Re$ (see Figure 9.17).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-246}
\end{center}

Figure 9.17

The vector $\mathbf{r}$ from the origin $O$ to point $P$ is

$$
r=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}=f(u, v, w) \mathbf{i}+g(u, v, w) \mathbf{j}+h(u, v, w) \mathbf{k}
$$

assuming that the transformation equations are $x=f(u, v, w), y=g(u, v, w)$, and $z=h(u, v, w)$.

Tangent vectors to the coordinate curves corresponding to the intersection of pairs of coordinate surfaces are given by $\partial \mathbf{r} / \partial u, \partial \mathbf{r} / \partial v, \partial \mathbf{r} / \partial w$. Then the volume of the region $\Delta \Re$ of Figure 9.17 is given approximately by

$$
\left|\frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial v} \times \frac{\partial \mathbf{r}}{\partial w}\right| \Delta u \Delta v \Delta w=\left|\frac{\partial(x, y, z)}{\partial(u, v, w)}\right| \Delta u \Delta v \Delta w
$$

The triple integral of $F(x, y, z)$ over the region is the limit of the sum

$$
\sum F\{f(u, v, w), g(u, v, w), h(u, v, w)\}\left|\frac{\partial(x, y, z)}{\partial(u, v, w)}\right| \Delta u \Delta v \Delta w
$$

An investigation reveals that this limit is

$$
\iiint F\{f(u, v, w), g(u, v, w), h(u, v, w)\}\left|\frac{\partial(x, y, z)}{\partial(u, v, w)}\right| d u d v d w
$$

where $\Re^{\prime}$ is the region in the $u v w$ space into which the region $\Re$ is mapped under the transformation.

Another method for justifying this change of variables in triple integrals makes use of Stokes's theorem (see Problem 10.84).

9.14. What is the mass of a circular cylindrical body represented by the region $0 \leqq \rho \leqq c, 0 \leqq \phi \leqq 2 \pi, 0 \leqq z \leqq$ $h$, and with the density function $\mu=z \sin ^{2} \phi$ ?

$$
M=\int_{0}^{h} \int_{0}^{2 \pi} \int_{0}^{c} z \sin ^{2} \phi \rho d \rho d \phi d z=\pi
$$

9.15. Use spherical coordinates to calculate the volume of a sphere of radius $a$.

$$
V=8 \int_{0}^{a} \int_{0}^{\pi / 2} \int_{0}^{\pi / 2} a^{2} \sin \theta d r d \theta d \phi=\frac{4}{3} \pi a^{3}
$$

9.16. Express $\iiint_{\Re^{\prime}} F(x, y, z) d x d y d z$ in (a) cylindrical and (b) spherical coordinates.

(a) The transformation equations in cylindrical coordinates are $x=\rho \cos \phi, y=\rho \sin \phi, z=z$.

As in Problem 6.39, $\partial(x, y, z) / \partial(\rho, \phi, z)=\rho$. Then, by Problem 9.13, the triple integral becomes

$$
\iiint_{\Re^{\prime}} G(\rho, \phi, z) \rho d \rho d \phi d z
$$

where $\Re^{\prime}$ is the region in the $\rho, \phi, z$ space corresponding to $\Re$ and where $G(\rho, \phi, z \equiv F(\rho \cos \phi, \rho \sin \phi, z)$.\\
(b) The transformation equations in spherical coordinates are $x=r \sin \theta \cos \phi, y=r \sin \theta \sin \phi, z=r \cos \theta$.

By Problem 6.101, $\partial(x, y, z) / \partial(r, \theta, \phi)=r^{2} \sin \theta$. Then, by Problem 9.13, the triple integral becomes

$$
\iiint_{\Re^{\prime}} H(r, \theta, \phi) r^{2} \sin \theta d r d \theta d \phi
$$

where $\Re^{\prime}$ is the region in the $r, \theta, \phi$ space corresponding to $\Re$, and where $H(r, \theta, \phi) \equiv F(r \sin \theta \cos \phi$, $r \sin \theta \sin \phi, r \cos \theta)$.

9.17. Find the volume of the region above the $x y$ plane bounded by the paraboloid $z=x^{2}+y^{2}$ and the cylinder $x^{2}+y^{2}=a^{2}$.

The volume is most easily found by using cylindrical coordinates. In these coordinates the equations for the paraboloid and cylinder are, respectively, $z=\rho^{2}$ and $\rho=a$. Then

Required volume $=4$ times volume shown in Figure 9.18

$$
\begin{aligned}
& =4 \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{a} \int_{z=0}^{\rho^{2}} \rho d z d \rho d \phi \\
& =4 \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{a} \rho^{3} d \rho d \phi \\
& =\left.4 \int_{h i=0}^{\pi / 2} \frac{\rho^{4}}{4}\right|_{=0} ^{a} d \phi=\frac{\pi}{2} a^{4}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-247}
\end{center}

Figure 9.18

The integration with respect to $z$ (keeping $\rho$ and $\phi$ constant) from $z=0$ to $z=\rho^{2}$ corresponds to summing the cubical volumes (indicated by $d V$ ) in a vertical column extending from the $x y$ plane to the paraboloid. The subsequent integration with respect to $\rho$ (keeping $\phi$ constant) from $\rho=0$ to $\rho=a$ corresponds to addition of volumes of all columns in the wedge-shaped region. Finally, integration with respect to $\phi$ corresponds to adding volumes of all such wedge-shaped regions.

The integration can also be performed in other orders to yield the same result.

We can also set up the integral by determining the region $\Re^{\prime}$ in $\rho, \phi, z$ space into which $\Re$ is mapped by the cylindrical coordinate transformation.

9.18. (a) Find the moment of inertia about the $z$ axis of the region in Problem 9.17, assuming that the density is the constant $\sigma$. (b) Find the radius of gyration.

(a) The moment of inertia about the $z$ axis is

$$
\begin{aligned}
I_{z} & =4 \int_{\phi_{0}}^{\pi / 2} \int_{\rho=0}^{a} \int_{z=0}^{\rho^{2}} \rho^{2} \sigma \rho d z d \rho d \phi \\
& =4 \sigma \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{a} \rho^{5} d \rho d \phi=\left.4 \sigma \int_{\phi=0}^{\pi / 2} \frac{\rho^{6}}{6}\right|_{\rho=0} ^{a} d \phi=\frac{\pi a^{6} \sigma}{3}
\end{aligned}
$$

The result can be expressed in terms of the mass $M$ of the region, since, by Problem 9.17,

$$
M=\text { volume } \times \text { desnity }=\frac{\pi}{2} a^{4} \sigma \quad \text { so that } \quad I_{z}=\frac{\pi a^{6} \sigma}{3}=\frac{\pi a^{6}}{3} \cdot \frac{2 M}{\pi a^{4}}=\frac{2}{3} M a^{2}
$$

Note that in setting up the integral for $\boldsymbol{I}_{z}$ we can think of $\sigma \rho d z d \rho d \phi d z d \rho d \phi$ as being the mass of the cubical volume element, $\rho^{2} \sigma \rho d z d \rho d \phi$ as the moment of inertia of this mass with respect to the $z$ axis, and $\iiint_{\Re^{\prime}} \rho^{2} \sigma \rho d z d \rho d \phi$ as the total moment of inertia about the $z$ axis. The limits of integration are determined as in Problem 9.17.

(b) The radius of gyration is the value $K$ such that $M K^{2}=\frac{2}{3} M a^{2}$; i.e., $K^{2}=\frac{2}{3} a^{2}$ or $K=a \sqrt{2 / 3}$.

The physical significance of $K$ is that if all the mass $M$ were concentrated in a thin cylindrical shell of radius $K$, then the moment of inertia of this shell about the axis of the cylinder would be $I_{z}$.

9.19. (a) Find the volume of the region bounded above by the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ and below by the cone $z^{2}$ $\sin ^{2} \alpha=\left(x^{2}+y^{2}\right) \cos ^{2} \alpha$, where $\alpha$ is a constant such that $0 \leqq \alpha \leqq \pi$. (b) From the result in (a), find the volume of a sphere of radius $a$.

In spherical coordinates the equation of the sphere is $r=a$ and that of the cone is $\theta=\alpha$. This can been directly or by using the transformation equations $x=r \sin \theta \cos \phi, y=r \sin \theta \sin \phi, z=r \cos \theta$. For example, $z^{2} \sin ^{2} \alpha=\left(x^{2}+y^{2}\right) \cos ^{2} \alpha$ becomes, on using these equations, $r^{2} \cos ^{2} \theta \sin ^{2} \alpha=\left(r^{2} \sin ^{2} \theta \cos ^{2} \phi+r^{2} \sin ^{2} \theta\right.$ $\left.\sin ^{2} \phi\right) \cos ^{2} \alpha$, i.e., $r^{2} \cos ^{2} \theta \sin ^{2} \alpha=r^{2} \sin ^{2} \theta \cos ^{2} \alpha$, from which $\tan \theta= \pm \tan \alpha$ and $\operatorname{so} \theta=\alpha$ or $\theta=\pi-\alpha$. It is sufficient to consider one of these-say, $\theta=\alpha$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-248}
\end{center}

Figure 9.19

(a) Required volume $=4$ times volume (shaded) in Figure 9.19

$$
\begin{aligned}
& =4 \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \int_{r=0}^{\rho^{2}} r^{2} \sin \theta d r d \theta d \phi \\
& =\left.4 \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \frac{r^{3}}{3} \sin \theta\right|_{r=0} ^{\alpha} d \theta d \phi \\
& =\frac{4 a^{3}}{3} \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \sin \theta d \theta d \phi \\
& =\frac{4 a^{3}}{3} \int_{\phi=0}^{\pi / 2}-\left.\cos \theta\right|_{\theta=0} ^{\alpha} d \phi \\
& =\frac{2 \pi a^{3}}{3}(1-\cos \alpha)
\end{aligned}
$$

The integration with respect to $r$ (keeping $\theta$ and $\phi$ constant) from $r=0$ to $r=a$ corresponds to summing the volumes of all cubical elements (such as indicated by $d V$ ) in a column extending from $r=0$ to $r=a$. The subsequent integration with respect to $\theta$ (keeping $\phi$ constant) from $\theta=0$ to $\theta=\pi / 4$ corresponds to summing the volumes of all columns in the wedge-shaped region. Finally, integration with respect to $\phi$ corresponds to adding volumes of all such wedge-shaped regions.

(b) Letting $\alpha=\pi$, the volume of the sphere thus obtained is

$$
\frac{2 \pi a^{3}}{3}(1-\cos \pi)=\frac{4}{3} \pi a^{3}
$$

9.20. (a) Find the centroid of the region in Problem 9.19. (b) Use the result in (a) to find the centroid of a hemisphere.

(a) The centroid ( $\bar{x}, \bar{y}, \bar{z})$ is, due to symmetry, given by $\bar{x}=\bar{y}=0$ and

$$
\bar{z}=\frac{\text { Total moment about } x y \text { plane }}{\text { Total mass }}=\frac{\iiint z \sigma d V}{\iiint \sigma d V}
$$

Since $z=r \cos \theta$ and $\sigma$ is constant, the numerator is

$$
\begin{aligned}
4 \sigma \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \int_{r=0}^{\rho^{2}} r \cos \theta \cdot r^{2} \sin \theta d r d \theta d \phi & =\left.4 \sigma \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \frac{r^{4}}{4}\right|_{r=0} ^{a} \sin \theta \cos \theta d \theta d \phi \\
& =\sigma a^{4} \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\alpha} \sin \theta \cos \theta d \theta d \phi \\
& =\left.\sigma a^{4} \int_{\phi=0}^{\pi / 2} \frac{\sin ^{2} \theta}{2}\right|_{\theta=0} ^{a} d \phi=\frac{\pi \sigma a^{4} \sin ^{2} \alpha}{4}
\end{aligned}
$$

Then

The denominator, obtained by multiplying the result of Problem 9.19(a) by $\sigma$, is $\frac{2}{3} \pi \sigma a^{3}(1-\cos \alpha)$.

$$
\bar{z}=\frac{\frac{1}{4} \pi \sigma a^{4} \sin ^{2} \alpha}{\frac{2}{3} \pi \sigma a^{3}(1-\cos \alpha)}=\frac{3}{8} a(1+\cos \alpha)
$$

(b) Letting $\alpha=\pi / 2, \bar{z}=\frac{3}{8} \mathrm{a}$.

\section*{Miscellaneous problems}
9.21. Prove that (a) $\int_{0}^{1}\left\{\int_{0}^{1} \frac{x-y}{(x+y)^{3}} d y\right\} d x=\frac{1}{2}$ and (b) $\int_{0}^{1}\left\{\int_{0}^{1} \frac{x-y}{(x+y)^{3}} d x\right\} d y=-\frac{1}{2}$,

(a) $\int_{0}^{1}\left\{\int_{0}^{1} \frac{x-y}{(x+y)^{3}} d y\right\} d x=\int_{0}^{1}\left\{\int_{0}^{1} \frac{2 x-(x+y)}{(x+y)^{3}} d y\right\} d x$

$$
=\int_{0}^{1}\left\{\int_{0}^{1}\left(\frac{2 x}{(x+y)^{3}}-\frac{1}{(x+y)^{2}}\right) d y\right\} d x
$$

$$
\begin{aligned}
& =\left.\int_{0}^{1}\left(\frac{-x}{(x+y)^{2}}-\frac{1}{x+y}\right)\right|_{y=0} ^{1} d x \\
& =\int_{0}^{1} \frac{d x}{(x+y)^{2}}-\left.\frac{-1}{x+1}\right|_{0} ^{1}=\frac{1}{2}
\end{aligned}
$$

(b) This follows at once on formally interchanging $x$ and $y$ in (a) to obtain

$\iint_{\Re} \frac{x-y}{(x+y)^{3}} d x d y, \int_{0}^{1}\left\{\int_{0}^{1} \frac{x-y}{(x+y)^{3}} d x\right\} d y=-\frac{1}{2}$ and then multiplying both sides by -1.

This example shows that interchange in order of integration may not always produce equal results. A sufficient condition under which the order may be interchanged is that the double integral over the corresponding region exists. In this case $\iint_{\Re} \frac{x-y}{(x+y)^{3}} d x d y$, where $\Re$ is the region $0 \leqq x \leqq 1,0 \leqq y \leqq 1$, fails to exist because of the discontinuity of the integrand at the origin. The integral is actually an improper double integral (see Chapter 12).

9.22. Prove that $\int_{0}^{x}\left\{\int_{0}^{t} F(u) d u\right\} d t=\int_{0}^{x}(x-u) F(u) d u$

Let $I(x)=\int_{0}^{x}\left\{\int_{0}^{t} F(u) d u\right\} d t, \quad J(x)=\int_{0}^{z}(x-u) F(u) d u . \quad$ Then

$$
I^{\prime}(x)=\int_{0}^{z} F(u) d u, \quad J^{\prime}(x)=\int_{0}^{z} F(u) d u
$$

using Leibniz's rule, Page 198. Thus, $I^{\prime}(x)=J^{\prime}(x)$, and so $I(x)=J(x)=c$, where $c$ is a constant. Since $I(0)=$ $J(0)=0, c=0$, and so $I(x)=J(x)$.

The result is sometimes written in the form

$$
\int_{0}^{x} \int_{0}^{x} F(x) d x^{2}=\int_{0}^{x}(x-u) F(u) d u
$$

The result can be generalized to give (see Problem 9.58)

$$
\int_{0}^{x} \int_{0}^{x} \cdots \int_{0}^{x} F(x) d x^{n}=\frac{1}{(n-1) !} \int_{0}^{x}(x-u)^{n-1} F(u) d u
$$

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Double integrals}
9.23. (a) Sketch the region $\Re$ in the $x y$ plane bounded by $y^{2}=2 x$ and $y=x$. (b) Find the area of $\Re$. (c) Find the polar moment of inertia of $\Re$, assuming constant density $\sigma$.

Ans. (b) $\frac{2}{3}$ (c) $48 \sigma / 35=72 M / 35$, where $M$ is the mass of $\Re$

9.24. Find the centroid of the region in problem 9.23.

Ans. $\bar{x}=\frac{4}{5}, \bar{y}=1$

9.25. Given $\int_{y=0}^{3} \int_{x=1}^{\sqrt{4-y}}(x+y) d x d y$, (a) sketch the region and give a possible physical interpretation of the double integral, (b) interchange the order of integration, and (c) evaluate the double integral.

Ans. (b) $\int_{x=1}^{2} \int_{y=1}^{4-x^{2}}(x+y) d y d x \quad$ (c) $241 / 60$

9.26. Show that $\int_{x=1}^{2} \int_{y=\sqrt{x}}^{x} \sin \frac{\pi x}{2 y} d x+\int_{x=2}^{4} \int_{y=\sqrt{x}}^{2} \sin \frac{\pi x}{2 y} d y d x=\frac{4(\pi+2)}{\pi^{3}}$.

9.27. Find the volume of the tetrahedron bounded by $x / a+y / b+z / c=1$ and the coordinate planes.

Ans. $a b c / 6$

9.28. Find the volume of the region bounded by $z=x^{3}+y^{2}, z=0, x=-a, x=-a, y=-a, y=a$.

Ans. $8 a^{4} / 3$

9.29. Find (a) the moment of inertia about the $z$ axis and (b) the centroid of the region in Problem 9.28, assuming a constant density $\sigma$.

Ans. (a) $\frac{112}{45} a^{6} \sigma=\frac{14}{15} M a^{2}, \quad$ where $M=$ mass $\quad$ (b) $\bar{x}=\bar{y}=0, \bar{z}=\frac{7}{15} a^{2}$

\section*{Transformation of double integrals}
9.30. Evaluate $\iint_{\Re} \sqrt{x^{2}+y^{2}} d x d y$, where $\Re$ is the region $x^{2}+y^{2} \leqq a^{2}$.

Ans. $\frac{2}{3} \pi a^{3}$

9.31. If $\Re$ is the region of Problem 9.30, evaluate $\iint_{\Re} e^{-\left(x^{2}+y^{2}\right)} d x d y$.

Ans. $\pi\left(1-e^{-a^{2}}\right)$

9.32. By using the transformation $x+y=u, y=u v$, show that $\int_{x=0}^{1} \int_{y=0}^{1-x} e^{y /(x+y)} d y d x=\frac{e-1}{2}$.

9.33. Find the area of the region bounded by $x y=4, x y=8, x y^{3}=5, x y^{3}=15$. (Hint: Let $x y=u, x y^{3}=v$.)

Ans. $2 \ln 3$

9.34. Show that the volume generated by revolving the region in the first quadrant bounded by the parabolas $y^{2}=x$, $y^{2}=8 x, x^{2}=y, x^{2}=8 y$ about the $x$ axis is $279 \pi / 2$. (Hint: Let $y^{2}=u x, x^{2}=v y$.)

9.35. Find the area of the region in the first quadrant bounded by $y=x^{3}, y=4 x^{3}, x=y^{3}, x=4 y^{3}$.

$$
\text { Ans. } \frac{1}{8}
$$

9.36. Let $\Re$ be the region bounded by $x+y=1, x=0, y=0$. Show that $\iint_{\Re} \cos \left(\frac{x-y}{x+y}\right) d x d y=\frac{\sin 1}{2}$. (Hint: Let\\
$x-y=u, x+y=v$.

\section*{Triple integrals}
9.37. (a) Evaluate $\int_{x=0}^{1} \int_{y=0}^{1} \int_{z=\sqrt{x^{2}+y^{2}}}^{2} x y z d z d y d x$. (b) Give a physical interpretation to the integral in (a). Ans. (a) $\frac{3}{8}$

9.38. Find (a) the volume and (b) the centroid of the region in the first octant bounded by $x / a+y / b+z / c=1$, where $a, b, c$ are positive.

Ans. (a) $a b c / 6$ (b) $\bar{x}=a / 4, \bar{y}=b / 4, \bar{z}=c / 4$

9.39. Find (a) the moment of inertia and (b) the radius of gyration about the $z$ axis of the region in Problem 9.38.

Ans. (a) $M(a 2+b 2) / 10$ (b) $\sqrt{(a 2+b 2) / 10}$

9.40. Find the mass of the region corresponding to $x 2+y 2+z 2 \leqq 4, x \varepsilon 0, y \varepsilon 0, z \varepsilon 0$, if the density is equal to $x y z$.

$$
\text { Ans. 4/3 }
$$

9.41. Find the volume of the region bounded by $z=x 2+y 2$ and $z=2 x$.

Ans. $\pi / 2$

\section*{Transformation of triple integrals}
9.42. Find the volume of the region bounded by $z=4-x^{2}-y^{2}$ and the $x y$ plane.

\section*{Ans. $8 \pi$}
9.43. Find the centroid of the region in Problem 9.42, assuming constant density $\sigma$.

Ans. $\bar{x}=\bar{y}=0, \bar{z}=\frac{4}{3}$

9.44. (a) Evaluate $\iiint_{\Re} \int \sqrt{x^{2}+y^{2}+z^{2}} d x d y d z$, where $\Re$ is the region bounded by the plane $z=3$ and the cone $z=z=\sqrt{x^{2}+y^{2}}$. (b) Give a physical interpretation of the integral in (a). (Hint: Perform the integration in cylindrical coordinates in the order $\rho, z, \phi$.

$$
\text { Ans. } 27 \pi(2 \sqrt{2}-1) / 2
$$

9.45. Show that the volume of the region bonded by the cone $z=\sqrt{x^{2}+y^{2}}$ and the paraboloid $z=x^{2}+y^{2}$ is $\pi / 6$.

9.46. Find the moment of inertia of a right circular cylinder of radius $a$ and height $b$, about its axis if the density is proportional to the distance from the axis.

$$
\text { Ans. } \frac{3}{5} M a^{2}
$$

9.47. (a) Evaluate $\iiint_{\Re} \frac{d x d y d z}{\left(x^{2}+y^{2}+z^{2}\right)^{32}}$, where $\Re$ is the region bounded by the spheres $x^{2}+y^{2}+z^{2}=a^{2}$ and $x^{2}+y^{2}+z^{2}=b^{2}$ where $a>b>0$. (b) Give a physical interpretation of the integral in (a).

Ans. (a) $4 \pi \ln (a / b)$

9.48. (a) Find the volume of the region bounded above by the sphere $r=2 a \cos \theta$ and below by the cone $\phi=\alpha$, where $0<\alpha<\pi / 2$. (b) Discuss the case $\alpha=+\pi / 2$.

$$
\text { Ans. } \frac{4}{3} \pi a^{3}\left(1-\cos ^{4} \alpha\right)
$$

9.49. Find the centroid of a hemispherical shell having outer radius $a$ and inner radius $b$ if the density (a) is constant and (b) varies as the square of the distance from the base. Discuss the case $a=b$.

Ans. Taking the $z$ axis as the axis of symmetry: (a) $\bar{x}=\bar{y}=0, \bar{z}=\frac{3}{8}\left(a^{4}-b^{4}\right) /\left(a^{3}-b^{3}\right)$

(b) $\bar{x}=\bar{y}=0, \bar{z}=\frac{5}{8}\left(a^{6}-b^{6}\right) /\left(a^{5}-b^{5}\right)$

\section*{Miscellaneous problems}
9.50. Find the mass of a right circular cylinder of radius $a$ and height $b$ if the density varies as the square of the distance from a point on the circumference of the base.

$$
\text { Ans. } \frac{1}{6} \pi a^{2} b k\left(9 a^{2}+2 b^{2}\right) \text {, where } k=\text { constant of proportionality }
$$

9.51. Find (a) the volume and (b) the centroid of the region bounded above by the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ and below by the plane $z=b$ where $a>b>0$, assuming constant density.

$$
\text { Ans. (a) } \frac{1}{3} \pi\left(2 a^{3}-3 a^{2} b+b^{3}\right) \text { (b) } \bar{x}=\bar{y}=0, \bar{z}=\frac{3}{4}(a+b)^{2} /(2 a+b)
$$

9.52. A sphere of radius $a$ has a cylindrical hole of radius $b$ bored from it, the axis of the cylinder coinciding with a diameter of the sphere. Show that the volume of the sphere which remains is $\frac{4}{3} \pi\left[a^{3}-\left(a^{2}-b^{2}\right)^{3 / 2}\right]$.

9.53. A simple closed curve in a plane is revolved about an axis in the plane which does not intersect the curve. Prove that the volume generated is equal to the area bounded by the curve multiplied by the distance traveled by the centroid of the area (Pappus's theorem).

9.54. Use Problem 9.53 to find the volume generated by revolving the circle $x^{2}+(y-b)^{2}=a^{2}, b>a>0$ about the $x$ axis.

Ans. $2 \pi^{2} a^{2} b$

9.55. Find the volume of the region bounded by the hyperbolic cylinders $x y=1, x y=9, x z=4, x z=36, y z=25$, $y z=49$. (Hint: Let $x y=u, x z=v, y z=w$.)

Ans. 64

9.56 Evaluate $\iiint_{\Re} \sqrt{1-\left(x^{2} / a^{2}+y^{2} / b^{2}+z^{2} / c^{2}\right)} d x d y d z$ where $\Re$ is the region interior to the ellipsoid $x^{2} / a^{2}+$ $y^{2} / b^{2}+z^{2} / c^{2}=1$. (Hint: Let $x=a u, y=b v, z=c w$. Then use spherical coordinates.)

Ans. $\frac{1}{4} \pi^{2} a b c$

9.57. If $\Re$ is the region $x^{2}+x y+y^{2} \leqq 1$, prove that $\iint_{\Re} e^{-\left(x^{2}+x y+y^{2}\right)} d x d y=\frac{2 \pi}{e \sqrt{3}}(e-1)$. (Hint: Let $x=u \cos \alpha-v$ $\sin \alpha, y=u \sin \alpha+v \cos \alpha$ and choose $\alpha$ so as to eliminate the $x y$ term in the integrand. Then let $u=a \rho \cos$ $\phi, v=b \rho \sin \phi$ where $a$ and $b$ are appropriately chosen.)

9.58. Prove that $\int_{0}^{x} \int_{0}^{x} \cdots \int_{0}^{x} F(x) d x^{n}=\frac{1}{(n-1) !} \int_{0}^{x}(x-u)^{n-1} F(u) d u$ for $n=1,2,3, \ldots$ (see problem 9.22).

\section*{CHAPTER 10}
\section*{Line Integrals, Surface Integrals, and Integral Theorems}
Construction of mathematical models of physical phenomena requires functional domains of greater complexity than the previously employed line segments and plane regions. This section makes progress in meeting that need by enriching integral theory with the introduction of segments of curves and portions of surfaces as domains. Thus, single integrals as functions defined on curve segments take on new meaning and are then called line integrals. Stokes's theorem exhibits a striking relation between the line integral of a function on a closed curve and the double integral of the surface portion that is enclosed. The divergence theorem relates the triple integral of a function on a three-dimensional region of space to its double integral on the bounding surface. The elegant language of vectors best describes these concepts; therefore, it would be useful to reread the introduction to Chapter 7, where the importance of vectors is emphasized. (The integral theorems also are expressed in coordinate form.)

\section*{Line Integrals}
The objective of this section is to geometrically view the domain of a vector or scalar function as a segment of a curve. Since the curve is defined on an interval of real numbers, it is possible to refer the function to this primitive domain, but to do so would suppress much geometric insight.

A curve $C$ in three-dimensional space may be represented by parametric equations:


\begin{equation*}
x=f_{1}(t), y=f_{2}(t), z=f_{3}(t), a \leqq t \leqq b \tag{1}
\end{equation*}


or in vector notation:


\begin{equation*}
\mathbf{x}=\mathbf{r}(\mathrm{t}) \tag{2}
\end{equation*}


where

$$
\mathbf{r}(t)=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}
$$

(see Figure 10.1).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-255}
\end{center}

Figure 10.1

For this discussion it is assumed that $\mathbf{r}$ is continuously differentiable. While (as we are doing) it is convenient to refer the Euclidean space to a rectangular Cartesian coordinate system, it is not necessary. (For example, cylindrical and spherical coordinates sometimes are more useful.) In fact, one of the objectives of the vector language is to free us from any particular frame of reference. Then, a vector $\mathbf{A}[x(t), y(t, z(t)]$ or a scalar $\Theta$ is pictured on the domain $C$, which, according to the parametric representation, is referred to the real number interval $a \leqq t \leqq b$.

The integral


\begin{equation*}
\int_{C} \mathbf{A} \cdot d \mathbf{r} \tag{3}
\end{equation*}


of a vector field $\mathbf{A}$ defined on a curve segment $C$ is called a line integral. The integrand has the representation

$$
A_{1} d x+A_{2} d y+A_{3} d z
$$

obtained by expanding the dot product.

The scalar and vector integrals


\begin{align*}
& \int_{C} \Theta(t) d t=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \Theta\left(\xi_{k}, \eta_{k}, \zeta_{k}\right) \Delta t_{k}  \tag{4}\\
& \int_{C} \mathbf{A}(t) d t=\lim _{n \rightarrow \infty} \sum_{k=1}^{n} \mathbf{A}\left(\xi_{k}, \eta_{k}, \zeta_{k}\right) \Delta t_{k} \tag{5}
\end{align*}


can be interpreted as line integrals; however, they do not play a major role [except for the fact that the scalar integral (3) takes the form (4)].

The following three basic ways are used to evaluate the line integral (3):

\begin{enumerate}
  \item The parametric equations are used to express the integrand through the parameter $t$. Then
\end{enumerate}

$$
\int_{C} \boldsymbol{A} \cdot d \mathbf{r}=\int_{t_{1}}^{t_{2}} \mathbf{A} \cdot \frac{d \mathbf{r}}{d t} d t
$$

\begin{enumerate}
  \setcounter{enumi}{1}
  \item If the curve $C$ is a plane curve (for example, in the $x y$ plane) and has one of the representations $y=f(x)$ or $x=g(y)$, then the two integrals that arise are evaluated with respect to $x$ or $y$, whichever is more convenient.

  \item If the integrand is a perfect differential, then it may be evaluated through knowledge of the endpoints (that is, without reference to any particular joining curve). (See the section on independence of path on Page 246; also see Page 251.)

\end{enumerate}

These techniques are further illustrated for plane curves in the next section and for three-space in the problems.

\section*{Evaluation of Line Integrals for Plane Curves}
If the equation of a curve $C$ in the plane $z=0$ is given as $y=f(x)$, the line integral (2) is evaluated by placing $y=f(x), d y=f^{\prime}(x) d x$ in the integrand to obtain the definite integral


\begin{equation*}
\int_{a_{1}}^{a_{2}} P\{x, f(x)\} d x+Q\{x, f(x)\} f^{\prime}(x) d x \tag{6}
\end{equation*}


which is then evaluated in the usual manner.

Similarly, if $C$ is given as $x=g,(y)$, then $d x=g^{\prime}(y) d y$ and the line integral becomes


\begin{equation*}
\int_{b_{1}}^{b_{2}} P\{g(y), y\} g^{\prime}(y) d y+Q\{g(y), y\} d y \tag{7}
\end{equation*}


If $C$ is given in parametric form $x=\phi(t), y=\psi(t)$, the line integral becomes


\begin{equation*}
\int_{t_{1}}^{t_{2}} P\{\phi(t), \psi(t)\} \phi^{\prime}(t) d t+Q\{\phi(t), \psi(t)\}, \psi^{\prime}(t) d t \tag{8}
\end{equation*}


where $t_{1}$ and $t_{2}$ denote the values of $t$ corresponding to points $A$ and $B$, respectively.

Combinations of these methods may be used in the evaluation. If the integrand $\mathbf{A} \cdot d \mathbf{r}$ is a perfect differential $d \Theta$, then


\begin{equation*}
\int_{C} \mathbf{A} \cdot d \mathbf{r}=\int_{(a, b)}^{(c, d)} d \Theta=\Theta(c, d)-\Theta(a, b) \tag{9}
\end{equation*}


Similar methods are used for evaluating line integrals along space curves.

\section*{Properties of Line Integrals Expressed for Plane Curves}
Line integrals have properties which are analogous to those of ordinary integrals. For example:

\begin{enumerate}
  \item $\int_{C} P(x, y) d x+Q(x, y) d y=\int_{C} P(x, y) d x+\int_{C} Q(x, y) d y$

  \item $\int_{\left(a_{1}, b_{1}\right)}^{\left(a_{2}, b_{2}\right)} P d x+Q d y=-\int_{\left(a_{2}, b_{2}\right)}^{\left(a_{1}, b_{1}\right)} P d x+q d y$

\end{enumerate}

Thus, reversal of the path of integration changes the sign of the line integral.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item $\int_{\left(a_{1}, b_{1}\right)}^{\left(a_{2}, b_{2}\right)} P d x+Q d y=\int_{\left(a_{1}, b_{1}\right)}^{\left(a_{3}, b_{3}\right)} P d x+Q d y+\int_{\left(a_{3}, b_{3}\right)}^{\left(a_{2}, b_{2}\right)} P d x+Q d y$
\end{enumerate}

where $\left(a_{3}, b_{3}\right)$ is another point on $C$.

Similar properties hold for line integrals in space.

\section*{Simple Closed Curves, Simply and Multiply Connected Regions}
A simple closed curve is a closed curve which does not intersect itself anywhere. Mathematically, a curve in the $x y$ plane is defined by the parametric equations $x=\phi(t), y=\psi(t)$ where $\phi$ and $\psi$ are single-valued and continuous in an interval $t_{1} \leqq t \leqq t_{2}$. If $\phi\left(t_{1}\right)=\phi\left(t_{2}\right)$ and $\psi\left(t_{1}\right)=\psi\left(t_{2}\right)$, the curve is said to be closed. If $\phi(u)=\phi(v)$ and $\psi(u)=$ $\psi(v)$ only when $\bar{u}=\bar{v}$ (except in the special case where $u=t_{1}$ and $v=t_{2}$ ), the curve is closed and does not intersect itself, and so is a simple closed curve. We shall also assume, unless otherwise stated, that $\phi$ and $\psi$ are piecewise differentiable in $t_{1} \leqq t \leqq t_{2}$.

If a plane region has the property that any closed curve in it can be continuously shrunk to a point without leaving the region, then the region is called simple connected; otherwise, it is called multiply connected

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-256(1)}
\end{center}

Simple closed curve

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-256}
\end{center}

Multiple connected\\
Figure 10.2

As the parameter $t$ varies from $t_{1}$ to $t_{2}$, the plane curve is described in a certain sense or direction. For curves in the $x y$ plane, we arbitrarily describe this direction as positive or negative according as a person traversing the curve in this direction with his head pointing in the positive $z$ direction has the region enclosed by the curve always toward his left or right, respectively. If we look down upon a simple closed curve in the xy plane, this amounts to saying that traversal of the curve in the counterclockwise direction is taken as positive, while traversal in the clockwise direction is taken as negative.

\section*{Green's Theorem in the Plane}
This theorem is needed to prove Stokes's theorem (Page 251). Then it becomes a special case of that theorem.

Let $P, Q, \partial P / \partial y, \partial Q / \partial x$ be single-valued and continuous in a simple connected region $\Re$ bounded by a simple closed curve $C$. Then


\begin{equation*}
\oint_{C} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y \tag{10}
\end{equation*}


where $\oint_{C}$ is used to emphasize that $C$ is closed and that it is described in the positive direction.

This theorem is also true for regions bounded by two or more closed curves (i.e., multiply connected regions). See Problem 10.10.

\section*{Conditions for a Line Integral to Be Independent of the Path}
The line integral of a vector field $\mathbf{A}$ is independent of path if its value is the same regardless of the (allowable) path from initial to terminal point. (Thus, the integral is evaluated from knowledge of the coordinates of these two points.)

For example, the integral of the vector field $\mathbf{A}=y \mathbf{i}+x \mathbf{j}$ is independent of path since

$$
\int_{C} \mathbf{A} \cdot d \mathbf{r}=\int_{C} y d x+x d y=\int_{x_{1} y_{1}}^{x_{2} y_{2}} d(x y)=x_{2} y_{2}-x_{1} y_{1}
$$

Thus, the value of the integral is obtained without reference to the curve joining $P_{1}$ and $P_{2}$.

This notion of the independence of path of line integrals of certain vector fields, important to theory and application, is characterized by the following three theorems.

Theorem 1 A necessary and sufficient condition that $\int_{C} \mathbf{A} \cdot d \mathbf{r}$ be independent of path is that there exists a scalar function $\Theta$ such that $\mathbf{A}=\nabla \Theta$.

Theorem 2 A necessary and sufficient condition that the line integral $\int_{C} \mathbf{A} \cdot d \mathbf{r}$ be independent of path is that $\nabla \times \mathbf{A}=\mathbf{0}$.

Theorem 3 If $\nabla \times \mathbf{A}=\mathbf{0}$, then the line integral of $\mathbf{A}$ over an allowable closed path is 0 ; i.e., $\oint_{C} \mathbf{A} \cdot d \mathbf{r}=0$.

If $C$ is a plane curve, then Theorem 3 follows immediately from Green's theorem, since in the plane case $\nabla \times \mathbf{A}$ reduces to

$$
\frac{\partial A_{1}}{\partial y}=\frac{\partial A_{2}}{\partial x}
$$

EXAMPLE. Newton's second law for forces is $\mathbf{F}=\frac{d(m \mathbf{v})}{d t}$, where $m$ is the mass of an object and $\mathbf{v}$ is its\\
velocity.

When $\mathbf{F}$ has the representation $\mathbf{F}=-\nabla \Theta$, it is said to be conservative. The previous theorems tell us that the integrals of conservative fields of force are independent of path. Furthermore, showing that $\nabla \times \mathbf{F}=\mathbf{0}$ is the preferred way of showing that $\mathbf{F}$ is conservative, since it involves differentiation, while demonstrating that $\Theta$ exists such that $\mathbf{F}=-\nabla \Theta$ requires integration.

\section*{Surface Integrals}
Our previous double integrals have been related to a very special surface, the plane. Now we consider other surfaces. yet, the approach is quite similar. Surfaces can be viewed intrinsically, i.e., as non-Euclidean spaces: however, we do not do that. Rather, the surface is thought of as embedded in a three-dimensional Euclidean space and expressed through a two-parameter vector representation:

$$
\mathbf{x}=\mathbf{r}\left(v_{1}, v_{2}\right)
$$

While the purpose of the vector representation is to be general (that is, interpretable through any allowable three-space coordinate system), it is convenient to initially think in terms of rectangular Cartesian coordinates: therefore, assume

$$
\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}
$$

and that there is a parametric representation


\begin{equation*}
x=r\left(v_{1}, v_{2}\right), y=r\left(v_{1}, v_{2}\right), z=r\left(v_{1}, v_{2}\right) \tag{11}
\end{equation*}


The functions are assumed to be continuously differentiable.

The parameter curves $v_{2}=$ const and $v_{1}$ const establish a coordinate system on the surface (just as $y=$ const and $x=$ const form such a system in the plane). The key to establishing the surface integral of a function is the differential element of surface area. (For the plane, that element is $d A=d x, d y$.)

At any point $P$ of the surface

$$
d \mathbf{x}=\frac{\partial \mathbf{r}}{\partial v_{1}} d v_{1}+\frac{\partial \mathbf{r}}{\partial v_{2}} d v_{2}
$$

spans the tangent plane to the surface. In particular, the directions of the coordinate curves $v_{2}=$ const and $v_{1}$ $=$ const are designated by $d \mathbf{x}_{1}=\frac{\partial \mathbf{r}}{\partial v_{1}} d v_{1}$ and $d \mathbf{x}_{2} \frac{\partial \mathbf{r}}{\partial v_{2}} d v_{2}$, respectively (see Figure 10.3).

The cross product

$$
d \mathbf{x}_{1} \times d \mathbf{x}_{2}=\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}} d v_{1} d v_{2}
$$

is normal to the tangent plane at $P$, and its magnitude $\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right|$ is the area of a differential coordinate\\
parallelogram.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-258}
\end{center}

Figure 10.3

(This is the usual geometric interpreation of the cross product abstracted to the differential level.) This strongly suggests the following definition:

Definition The differential element of surface area is


\begin{equation*}
d S=\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right| d v_{1} d v_{2} \tag{12}
\end{equation*}


For a function $\Theta\left(v_{1}, v_{2}\right)$ that is everywhere integrable on $S$,


\begin{equation*}
\iint_{S} \Theta d S=\iint_{S} \Theta\left(v_{1}, v_{2}\right)\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right| d v_{1} d v_{2} \tag{13}
\end{equation*}


is the surface integral of the function $\Theta$.

In general, the surface integral must be referred to three-space coordinates to be evaluated. If the surface has the Cartesian representation $z=f(x, y)$ and the identifications

$$
v_{1}=x, v_{2}=y, z=f\left(v, v_{2}\right)
$$

are made, then

$$
\frac{\partial \mathbf{r}}{\partial v_{1}}=\mathbf{i}+\frac{\partial z}{\partial x} \mathbf{k}, \frac{\partial \mathbf{r}}{\partial v_{2}}=\mathbf{j}+\frac{\partial z}{\partial y} \mathbf{k}
$$

and

$$
\frac{\partial \mathbf{r}}{\partial v_{2}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}=\mathbf{k}-\frac{\partial z}{\partial x} \mathbf{j}-\frac{\partial z}{\partial x} \mathbf{i}
$$

Therefore,

$$
\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right|=\left[1+\left(\frac{\partial z}{\partial x}\right)^{2}+\left(\frac{\partial z}{\partial y}\right)^{2}\right]^{1 / 2}
$$

Thus, the surface integral of $\Theta$ has the special representation


\begin{equation*}
S=\iint_{S} \Theta(x, y, z)\left[1+\left(\frac{\partial z}{\partial x}\right)^{2}+\left(\frac{\partial z}{\partial y}\right)^{2}\right]^{1 / 2} d x d y \tag{14}
\end{equation*}


If the surface is given in the implicit form $F(x, y, z)=0$, then the gradient may be employed to obtain another representation. To establish it, recall that, at any surface point $P$, the gradient $\nabla F$ is perpendicular (normal) to the tangent plane (and, hence, to $S$ ).

Therefore, the following equality of the unit vectors holds (up to sign):


\begin{equation*}
\frac{\nabla F}{|\nabla F|}= \pm\left(\frac{\partial \mathbf{r}}{\partial x} \times \frac{\partial \mathbf{r}}{\partial y}\right) /\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right| \tag{15}
\end{equation*}


[A conclusion of the theory of implicit functions is that from $F(x, y, z)=0$ (and under appropriate conditions) there can be produced an explicit representation $z=f(x, y)$ of a portion of the surface. This is an existence statement. The theorem does not say that this representation can be explicitly produced.] With this fact in hand, we again let $v_{1}=x, v_{2}=y, z=f\left(v_{1}, v_{2}\right)$. Then

$$
\nabla F=F_{x} \mathbf{i}+f_{y} \mathbf{j}+F_{z} \mathbf{k}
$$

Taking the dot product of both sides of Equation (15), $K$ yields

$$
\frac{F_{z}}{|\nabla F|}= \pm \frac{1}{\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right|}
$$

The ambiguity of sign can be eliminated by taking the absolute value of both sides of the equation. Then

$$
\left|\frac{\partial \mathbf{r}}{\partial v_{1}} \times \frac{\partial \mathbf{r}}{\partial v_{2}}\right|=\frac{|\nabla F|}{\left|F_{z}\right|}=\frac{\left[\left(F_{x}\right)^{2}+\left(F_{y}\right)^{2}+\left(F_{z}\right)^{2}\right]^{1 / 2}}{\left|F_{z}\right|}
$$

and the surface integral of $\Theta$ takes the form


\begin{equation*}
\iint_{S} \frac{\left[\left(F_{x}\right)^{2}+\left(F_{y}\right)^{2}+\left(F_{z}\right)^{2}\right]^{1 / 2}}{\left|F_{z}\right|} d x d y \tag{16}
\end{equation*}


The formulas (14) and (16) also can be introduced in the following nonvectorial manner.

Let $S$ be a two-sided surface having projection $\Re$ on the $x y$ plane, as in Figure 10.4. Assume that an equation for $S$ is $z=f(x, y)$, where $f$ is single-valued and continuous for all $x$ and $y$ in $\Re$. Divide $\Re$ into $n$ subregions of area $\Delta A_{p}, p=1,2, \ldots, n$, and erect a vertical column on each of these subregions to intersect $S$ in an area $\Delta S_{p}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-260}
\end{center}

Figure 10.4

Let $\phi(x, y, z)$ be single-valued and continuous at all points of $S$. Form the sum


\begin{equation*}
\sum_{p=1}^{n} \phi\left(\xi_{p}, \eta_{p}, \zeta_{p}\right) \Delta S_{p} \tag{17}
\end{equation*}


where $\left(\xi_{p}, \eta_{p}, \zeta_{p}\right)$ is some point of $\Delta S_{p}$. If the limit of this sum as $n \rightarrow \infty$ in such a way that each $\Delta S_{p} \rightarrow 0$ exists, the resulting limit is called the surface integral of $\phi(x, y, z)$ over $S$ and is designated by


\begin{equation*}
\iint_{S} \phi(x, y, z) d S \tag{18}
\end{equation*}


Since $\Delta S_{p}=\left|\sec \gamma_{p}\right| \Delta A_{p}$ approximately, where $\gamma_{p}$ is the angle between the normal line to $S$ and the positive $z$ axis, the limit of the sum (17) can be written


\begin{equation*}
\iint_{\Re} \phi(x, y, z)|\sec \gamma| d A \tag{19}
\end{equation*}


The quantity $|\sec \gamma|$ is given by


\begin{equation*}
|\sec \gamma|=\frac{1}{\left|\mathbf{n}_{p} \cdot \mathbf{k}\right|}=\sqrt{1+\left(\frac{\partial z}{\partial x}\right)^{2}+\left(\frac{\partial z}{\partial y}\right)^{2}} \tag{20}
\end{equation*}


Then, assuming that $z=f(x, y)$ has continuous (or sectionally continuous) derivatives in $\Re$, (19) can be written in rectangular form as


\begin{equation*}
\iint_{\Re} \phi(x, y, z) \sqrt{1+\left(\frac{\partial z}{\partial x}\right)^{2}+\left(\frac{\partial z}{\partial y}\right)^{2}} d x d y \tag{21}
\end{equation*}


In case the equation for $S$ is given as $F(x, y, z)=0,(21)$ can also be written


\begin{equation*}
\iint_{\Re} \phi(x, y, z) \frac{\sqrt{\left(F_{x}\right)^{2}+\left(F_{y}\right)^{2}+\left(F_{z}\right)^{2}}}{\left|F_{z}\right|} d x d y \tag{22}
\end{equation*}


The results (21) or (22) can be used to evaluate (18).

In the preceding we have assumed that $S$ is such that any line parallel to the $z$ axis intersects $S$ in only one point. In case $S$ is not of this type, we can usually subdivide $S$ into surfaces $S_{1}, S_{2}, \ldots$ which are of this type. Then the surface integral over $S$ is defined as the sum of the surface integrals over $S_{1}, S_{2}, \ldots$.

The results stated hold when $S$ is projected onto a region $\Re$ on the $x y$ plane. In some cases it is better to project $S$ onto the $y z$ or $x z$ planes. For such cases, (18) can be evaluated by appropriately modifying (21) and (22).

\section*{The Divergence Theorem}
The divergence theorem establishes equality between a triple integral (volume integral) of a function over a region of three-dimensional space and the double integral of the function over the surface that bounds that region. This relation is very important in the expression of physical theory. (See Figure 10.5.)

\section*{Divergence (or Gauss) Theorem}
Let $\mathbf{A}$ be a vector field that is continuously differentiable on a closed-space region $V$ bounded by a smooth surface $S$. Then


\begin{equation*}
\iiint_{V} \nabla \cdot \mathbf{A} d V=\iint_{S} \mathbf{A} \cdot \mathbf{n} d S \tag{23}
\end{equation*}


where $\mathbf{n}$ is an outwardly drawn normal.

If $\mathbf{n}$ is expressed through direction $\operatorname{cosines}$, i.e., $\mathbf{n}=\mathbf{i} \cos \alpha+\mathbf{j} \cos \beta+\mathbf{k} \cos \gamma$, then Equation (23) may be written

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-261}
\end{center}

Figure 10.5


\begin{equation*}
\iiint\left(\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}\right) d V=\iint_{S}\left(A_{1} \cos \alpha+A_{2} \cos \beta+A_{3} \cos \gamma\right) d S \tag{24}
\end{equation*}


The rectangular Cartesian component form of Equation (23) is


\begin{equation*}
\iiint_{V}\left(\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}\right) d V=\iint_{S}\left(A_{1} d y d z+A_{2} d z d x+A_{3} d x d y\right) \tag{25}
\end{equation*}


EXAMPLE. If $\mathbf{B}$ is the magnetic field vector, then one of Maxwell's equations of electromagnetic theory is $\nabla \cdot \mathbf{B}=0$. When this equation is substituted into the left member of Equation (23), the right member tells us that the magnetic flux through a closed surface containing a magnetic field is zero. A simple interpretation of this fact results by thinking of a magnet enclosed in a ball. All magnetic lines of force that flow out of the ball must return (so that the total flux is zero). Thus, the lines of force flow from one pole to the other, and there is no dispersion.

\section*{Stokes's Theorem}
Stokes's theorem establishes the equality of the double integral of a vector field over a portion of a surface and the line integral of the field over a simple closed curve bounding the surface portion. (See Figure 10.6.)

Suppose a closed curve $C$ bounds a smooth surface portion $S$. If the component functions of $\mathbf{x}=\mathbf{r}\left(v_{1}, v_{2}\right)$ have continuous mixed partial derivatives, then for a vector field $\mathbf{A}$ with continuous partial derivatives on $S$


\begin{equation*}
\oint_{C} \mathbf{A} \cdot d \mathbf{r}=\iint_{S} \mathbf{n} \cdot \nabla \times \mathbf{A} d S \tag{26}
\end{equation*}


where $\mathbf{n}=\cos \alpha \mathbf{i}+\cos \beta \mathbf{j}+\cos \gamma \mathbf{k}$ with $\alpha, \beta$, and $\gamma$ represeting the angles made by the outward normal $\mathbf{n}$ and $\mathbf{i}, \mathbf{j}$, and $\mathbf{k}$, respectively.

Then the component form of Equation (26) is


\begin{equation*}
\oint_{C}\left(A_{1} d x+A_{2} d y+A_{3} d z\right)=\iint_{S}\left[\left(\frac{\partial A_{3}}{\partial y}-\frac{\partial A_{2}}{\partial z}\right) \cos \alpha+\left(\frac{\partial A_{1}}{\partial z}-\frac{\partial A_{3}}{\partial x}\right) \cos \beta+\left(\frac{\partial A_{2}}{\partial x}-\frac{\partial A_{1}}{\partial y}\right) \cos \gamma\right] d S \tag{27}
\end{equation*}


If $\nabla \times \mathbf{A}=\mathbf{0}$, Stokes's theorem tells us that $\oint_{C} \mathbf{A} \cdot d \mathbf{r}=0$. This is Theorem 3 on Page 233.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-262}
\end{center}

Figure 10.6

\section*{SOLVED PROBLEMS}
\section*{Line integrals}
10.1. Evaluate $\int_{(0,1)}^{(1,2)}\left(x^{2}-y\right) d x+\left(y^{2}+x\right) d y$ along (a) a straight line from $(0,1)$ to $(1,2)$, (b) a straight lines from $(0,1)$ to $(1,1)$ and then from $(1,1)$ to $(1,2)$, and (c) the parabola $x=t, y=t^{2}+1$.

(a) An equation for the line joining $(0,1)$ and $(1,2)$ in the $x y$ plane is $y=x+1$. Then $d y=d x$ and the line integral equals

$$
\int_{x=0}^{1}\left\{x^{2}-(x+1)\right\} d x+\left\{(x+1)^{2}+x\right\} d x=\int_{0}^{1}\left(2 x^{2}+2 x\right) d x=5 / 3
$$

(b) Along the straight line from $(0,1)$ to $(1,1), y=1, d y=0$ and the line integral equals

$$
\int_{x=0}^{1}\left(x^{2}-1\right) d x+(1+x)(0)=\int_{0}^{1}\left(x^{2}-1\right) d x=-2 / 3
$$

Along the straight line from $(1,1)$ to $(1,2), x=1, d x=0$ and the line integral equals

$$
\int_{y=1}^{2}(1-y)(0)+\left(y^{2}+1\right) d y=\int_{t}^{2}\left(y^{2}+1\right) d y=10 / 3
$$

Then the required value $=-2 / 3+10 / 3=8.3$.

(c) Since $t=0$ at $(0,1)$ and $t=1$ at $(1,2)$, the line integral equals

$$
\int_{t=0}^{1}\left\{t^{2}-\left(t^{2}+1\right) d t+\left\{\left(t^{2}+1\right)^{2}+t\right\} 2 t d t=\int_{0}^{1}\left(2 t^{5}+4 t^{2}+2 t^{2}+2 t-1\right) d t=2\right.
$$

10.2. If $\mathbf{A}=\left(3 x^{2}-6 y z\right) \mathbf{i}+(2 y+3 x z) \mathbf{j}+\left(1-4 x y z^{2}\right) \mathbf{k}$, evaluate $\int_{C} \mathbf{A} \cdot d \mathbf{r}$ from $(0,1,1)$ to $(1,1,1)$ along the following paths $C$ :

(a) $x=t, y=t^{2}, z=t^{3}$

(b) The straight lines from $(0,0,0)$ to $(0,0,1)$, then to $(0,1,1)$, and then to $(1,1,1)$

(c) The straight line joining $(0,0,0)$ and $(1,1,1)$

$$
\begin{aligned}
\int_{C} \mathbf{A} \cdot d \mathbf{r} & \left.=\int_{C}\left\{\left(3 x^{2}-6 y z\right) \mathbf{i}+(2 y+3 x z)\right\} \mathbf{j}+\left(1-4 x y z^{2}\right) \mathbf{k}\right\} \cdot(d x \mathbf{i}+d y \mathbf{j}+d z \mathbf{k}) \\
& =\int_{C}\left\{\left(3 x^{2}-6 y z\right) d x+(2 y+3 x z) d y+\left(1-4 x y z^{2}\right) d z\right.
\end{aligned}
$$

(a) If $x=t, y=t^{2}, z=t^{3}$, points $(0,0,0)$ and $(1,1,1)$ correspond to $t=0$ and $t=1$, respectively. Then

$$
\begin{aligned}
\int_{C} \mathbf{A} \cdot d \mathbf{r} & =\int_{t=0}^{1}\left\{3 t^{2}-6\left(t^{2}\right)\left(t^{3}\right)\right\} d t+\left\{2 t^{2}+3(t)\left(t^{3}\right)\right\} d\left(t^{2}\right)+\left\{1-4(t)\left(t^{2}\right)\left(t^{3}\right)^{2}\right\} d\left(t^{3}\right) \\
& \left.=\int_{t=0}^{1}\left\{3 t^{2}-6 t^{5}\right\} d t+\left(4 t^{3}+6 t^{5}\right)\right\} d t+\left(3 t^{2}-12 t^{11}\right) d t=2
\end{aligned}
$$

Another method: Along $C, \mathbf{A}=\left(3 t^{2}-6 t^{5}\right) \mathbf{i}+\left(2 t^{2}+3 t^{4}\right) \mathbf{j}+\left(1-4 t^{9}\right) \mathbf{k}$ and $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}=t \mathbf{i}+t^{2} \mathbf{j}+t^{3} \mathbf{k}$, $d \mathbf{r}=\left(\mathbf{i}+2 t \mathbf{j}+3 t^{2} \mathbf{k}\right) d t$. Then

$$
\int_{C} \mathbf{A} \cdot d \mathbf{r}=\int_{0}^{1}\left(3 t^{2}-6 t^{5}\right) d t+\left(4 t^{3}+6 t^{5}\right) d t+\left(3 t^{2}-12 t^{11}\right) d t=2
$$

(b) Along the straight line from $(0,0,0)$ to $(0,1,1), x=0, y=0, d x=0, d y=0$, while $z$ varies from 0 to 1 . Then the integral over this part of the path is

$$
\int_{z=0}^{1}\left\{3(0)^{2}-6(0)(z)\right\} 0+\left\{2(0)+3(0)(z) 0+\left\{1-4(0)(0)\left(z^{2}\right)\right\} d z=\int_{z=0}^{1} d z=1\right.
$$

Along the straight line from $(0,0,1)$ to $(0,1,1), x=0, z=1, d x=0, d z=0$, while $y$ varies from 0 to 1 . Then the integral over this part of the path is

$$
\int_{y=0}^{1}\left\{3(0)^{2}-6(y)(1)\right\} 0+\{2 y+3(0)(1)\} d y+\left\{1-4(0)(y)(1)^{2}\right\} 0=\int_{y=0}^{1} 2 y d y=1
$$

Along the straight line from $(0,1,1)$, to $(1,1,1), v=1, z=1, d y=0, d z=0$, while $x$ varies from 0 to 1 . Then the integral over this part of the path is

$$
\int_{x=0}^{1}\left\{3 x^{2}-6(1)(1)\right\} d x+\{2(1)+3 x(1)\} 0+\left\{1-4 x(1)(1)^{2}\right\} 0=\int_{x=0}^{1}\left(3 x^{2}-6\right) d x=-5
$$

Adding,

$$
\int_{C} \mathbf{A} \cdot d x=1+1-5=-3
$$

(c) The straight line joining $(0,0,0)$ and $(1,1,1)$ is given in parametric form by $x=t, y=t, z=t$. Then

$$
\int_{C} \mathbf{A} \cdot d \mathbf{r}=\int_{t=0}^{1}\left(3 t^{2}-6 t^{2}\right) d t+\left(2 t+3 t^{2}\right) d t+\left(1-4 t^{4}\right) d t=6 / 5
$$

10.3. Find the work done in moving a particle once around an ellipse $C$ in the $x y$ plane, if the ellipse has its center at the origin with semimajor and semiminor axes 4 and 3, respectively, as indicated in Figure 10.7, and if the force field is given by

$$
\mathbf{F}=(3 x-4 y+2 z) \mathbf{i}+\left(4 x+2 y-3 z^{2}\right) \mathbf{j}+\left(2 x z-4 y^{2}+z^{3}\right) \mathbf{k}
$$

In the plane $z=0, \mathbf{F}=(3 x-4 y) \mathbf{i}+(4 x+2 y) \mathbf{j}-4 y^{2} \mathbf{k}$, and $d \mathbf{r}=d x \mathbf{i}+d y \mathbf{j}$, so that the work done is

$$
\begin{aligned}
\oint_{C} \mathbf{F} \cdot d \mathbf{r} & =\int_{C}\left\{(3 x-4 y) \mathbf{i}+(4 x+2 y) \mathbf{j}-4 y^{2} \mathbf{k}\right\} \cdot(d x \mathbf{i}+d y \mathbf{j}) \\
& =\oint_{C}(3 x-4 y) d x+(4 x+2 y) d y
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-263}
\end{center}

Figure 10.7

Choose the parametric equations of the ellipse as $x=4 \cos t, y=3 \sin t$, where $t$ varies from 0 to $2 \pi$ (see Figure 10.7). Then the line integral equals

$$
\begin{aligned}
& \int_{t=0}^{2 \pi}\{3(4 \cos t)-4(3 \sin t)\}\{-4 \sin t\} d t+\{4(4 \cos t)+2(3 \sin t)\}\{3 \cos t\} d t \\
& =\int_{t=0}^{2 \pi}(48-30 \sin t \cos t) d t=\left.\left(48 t-15 \sin ^{2} t\right)\right|_{0} ^{2 \pi}=96 \pi
\end{aligned}
$$

In traversing $C$ we have chosen the counterclockwise direction indicated in Figure 10.7. We call this the positive direction or say that $C$ has been traversed in the positive sense. If $C$ were traversed in the clockwise (negative) direction, the value of the integral would be $-96 \pi$.

10.4. Evaluate $\int_{C} y d s$ along the curve $C$ given by $y=2 \sqrt{x}$ from $x=3$ to $x=24$.

Since $d s=\sqrt{d x^{2}+d y^{2}}=\sqrt{1+\left(y^{\prime}\right)^{2}} d x=\sqrt{1+1 / x d x}$, we have

$$
\int_{C} y d s=\int_{2}^{24} 2 \sqrt{x} \sqrt{1+1 / x} d x=2 \int_{3}^{24} \sqrt{x+1} d x=\left.\frac{4}{3}(x+1)^{3 / 2}\right|_{3} ^{24}=156
$$

\section*{Green's theorem in the plane}
10.5. Prove Green's theorem in the plane if $C$ is a closed curve which has the property that any straight line parallel to the coordinate axes cuts $C$ in, at most, two points.

Let the equations of the curves $A E B$ and $A F B$ (see Figure 10.8) be $y=Y_{1}(x)$ and $y=Y_{2}(x)$, respectively. If $\Re$ is the region bounded by $C$, we have

$$
\begin{aligned}
\iint_{\Re} \frac{\partial P}{\partial y} d x d y & =\int_{x=a}^{b}\left[\int_{y=Y_{1}(x)}^{Y_{2}(x)} \frac{\partial P}{\partial y} d y\right] d x \\
& =\left.\int_{x=a}^{b} P(x, y)\right|_{y=Y_{1}(x)} ^{Y_{2}(x)} d x=\int_{a}^{b}\left[P\left(x, Y_{2}\right)-P\left(x, Y_{1}\right)\right] d x \\
& =-\int_{a}^{b} P\left(x, Y_{1}\right) d x-\int_{b}^{a} P\left(x, Y_{2}\right) d x=-\oint_{C} P d x
\end{aligned}
$$

Then


\begin{equation*}
\oint_{C} P d x=-\iint_{\Re} \frac{\partial P}{\partial y} d x d y \tag{1}
\end{equation*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-264}
\end{center}

Figure 10.8

Similarly, let the equations of curves $E A F$ and $E B F$ be $x=X_{1}(y)$ and $x=X_{2}(y)$, respectively. Then

$$
\begin{aligned}
\iint_{\Re} \frac{\partial Q}{\partial x} d x d y & =\int_{y=c}^{f}\left[\int_{x=x_{1}(y)}^{x_{2}(y)} \frac{\partial Q}{\partial x} d x\right] d y=\int_{c}^{f}\left[Q\left(X_{2}, y\right)-Q\left(X_{1}, y\right)\right] d y \\
& =\int_{f}^{c} Q\left(X_{1}, y\right) d y+\int_{c}^{f} Q\left(X_{2}, y\right) d y=\oint_{C} Q d y
\end{aligned}
$$

Then


\begin{equation*}
\oint_{C} Q d y=\iint_{\Re} \frac{\partial Q}{\partial x} d x d y \tag{2}
\end{equation*}


Adding Equations (1) and (2),

$$
\oint_{C} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y
$$

10.6. Verify Green's theorem in the plane for

$$
\oint_{C}\left(2 x y-x^{2}\right) d x+\left(x+y^{2}\right) d y
$$

where $C$ is the closed curve of the region bounded by $y=x^{2}$ and $y^{2}=x$.

The plane curve $y=x^{2}$ and $y^{2}=x$ intersect at $(0,0)$ and $(1,1)$. The positive direction in traversing $C$ is as shown in Figure 10.9.

Along $y=x^{2}$, the line integral equals

$$
\int_{x=0}^{1}\left\{(2 x)\left(x^{2}\right)-x^{2}\right\} d x+\left\{x+\left(x^{2}\right)^{2}\right\} d\left(x^{2}\right)=\int_{0}^{1}\left(2 x^{3}+x^{2}+2 x^{5}\right) d x=7 / 6
$$

Along $y^{2}=x$, the line integral equals

$$
\int_{y=1}^{0}\left\{(2)\left(y^{2}\right)(y)-\left(y^{2}\right)^{2}\right\} d\left(y^{2}\right)+\left\{y^{2}+y^{2}\right\} d y=\int_{1}^{0}\left(4 y^{4}-2 y^{5}+2 y^{2}\right) d y=-17 / 15
$$

Then the required line integral $=7 / 6-17 / 15=1 / 30$.

$$
\begin{aligned}
\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y & =\iint_{\Re}\left\{\frac{\partial}{\partial x}\left(x+y^{2}\right)-\frac{\partial}{\partial y}\left(2 x y-x^{2}\right)\right\} d x d y \\
& =\iint_{\Re}(1-2 x) d x d y=\int_{x=0}^{1} \int_{y=x^{2}}^{\sqrt{x}}(1-2 x) d y d x \\
& =\left.\int_{x=0}^{1}(y-2 x y)\right|_{y=x^{2}} ^{\sqrt{x}} d x \\
& =\int_{0}^{1}\left(x^{1 / 2}-2 x^{3 / 2}-x^{2}+2 x^{3}\right) d x=1 / 30
\end{aligned}
$$

Hence, Green's theorem is verified.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-265}
\end{center}

Figure 10.9

10.7. Extend the proof of Green's theorem in the plane given in Problem 10.5 to the curves $C$ for which lines parallel to the coordinate axes may cut $C$ in more than two points.

Consider a closed curve $C$ such as is shown in Figure 10.10, in which lines parallel to the axes may meet $C$ in more than two points. By constructing line $S T$, the region is divided into two regions $\Re_{1}$ and $\Re_{2}$, which are of the type considered in Problem 10.5 and for which Green's theorem applies, i.e.,


\begin{equation*}
\int_{S T U S} P d x+Q d y=\iint_{\Re_{1}}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y \tag{1}
\end{equation*}



\begin{equation*}
\int_{S V T S} P d x+Q d y=\iint_{\Re_{1}}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y \tag{2}
\end{equation*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-266}
\end{center}

Figure 10.10

Adding the left-hand sides of Equations (1) and (2), and omitting the integrand $P d x+Q d y$ in each case, we have

$$
\int_{S T U S}+\int_{S V T S}=\int_{S T}+\int_{T U S}+\int_{S V T}+\int_{T S}=\int_{T U S}+\int_{S V T}=\int_{T U S V T}
$$

using the fact that $\int_{S T}=-\int_{T S}$. Adding the right-hand sides of Equations (1) and (2), omitting the integrand, $\iint_{\Re_{1}}+\iint_{\Re_{2}}=\iint_{\Re}$, where $\Re$ con-\\
sists of regions $\Re_{1}$ and $\Re_{2}$.

Then $\int_{T U S V T} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y$, and the theorem is proved.

A region $\Re$ such as is considered here and in Problem 10.5, for which any closed lying in $\Re$ can be continuously shrunk to a point without leaving $\Re$, is called a simply connected region. A region which is not simply connected is called multiply connected. We have shown here that Green's theorem in the plane applies to simply connected regions bounded by closed curves. In Problem 10.10 the theorem is extended to multiply connected regions.

For more complicated simply connected regions, it may be necessary to construct more lines, such as $S T$, to establish the theorem.

10.8. Show that the area bounded by a simple closed curve $C$ is given by $\frac{1}{2} \oint_{C} x d y-y d x$.

In Green's theorem, put $P=-y, Q=x$. Then

$$
\oint_{C} x d y-y d x=\iint_{\Re}\left(\frac{\partial}{\partial x}(x)-\frac{\partial}{\partial y}(-y)\right) d x d y=2 \iint_{\Re} d x d y=2 A
$$

where $A$ is the required area. Thus, $A=\frac{1}{2} \oint_{C} x d y-y d x$.

10.9. Find the area of the ellipse $x=a \cos \theta, y=b \sin \theta$.

$$
\begin{aligned}
\text { Area } & =\frac{1}{2} \oint_{C} x d y-y d x=\frac{1}{2} \int_{0}^{2 \pi}(a \cos \theta)(b \cos \theta) d \theta-(b \sin \theta)(-a \sin \theta) d \theta \\
& =\frac{1}{2} \int_{0}^{2 \pi} a b\left(\cos ^{2} \theta+\sin ^{2} \theta\right) d \theta=\frac{1}{2} \int_{0}^{2 \pi} a b d \theta=\pi a b
\end{aligned}
$$

10.10. Show that Green's theorem in the plane is also valid for a multiply connected region $\Re$ such as is shown in Figure 10.11.

The shaded region $\Re$, shown in Figure 10.11, is multiply connected, since not every closed curve lying in $\Re$ can be shrunk to a point without leaving $\Re$, as is observed by considering a curve surrounding $D E F G D$, for example. The boundary of $\Re$, which consists of the exterior boundary AHJKLA and the interior boundary $D E F G D$, is to be traversed in the positive direction, so that a person traveling in this direction always has the region on his left. It is seen that the positive directions are those indicated Figure 10.11.

In order to establish the theorem, construct a line such as $A D$, called a crosscut, connecting the exterior and interior boundaries. The region bounded by ADEFGDALKJHA is simply connected, and so Green's theorem is valid. Then

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-267}
\end{center}

Figure 10.11

$$
\oint_{A D E F G D A L K J H A} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y
$$

But the integral on the left, leaving out the integrand, is equal to

$$
\int_{A D}+\int_{D E F G D}+\int_{D A}+\int_{A L K J H A}=\int_{D E F G D}+\int_{A L K J H A}
$$

since $\int_{A D}=-\int_{D A}$. Thus, if $C_{1}$ is the curve ALKJHA, $C_{2}$ is the curve DEFGD, and $C$ is the boundary of $\Re$ consisting of $C_{1}$ and $C_{2}$ (traversed in the positive directions), then $\int_{C_{1}}+\int_{C_{2}}=\int_{C}$ and so

$$
\oint_{C} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y
$$

\section*{Independence of the path}
10.11. Let $P(x, y)$ and $Q(x, y)$ be continuous and have continuous first partial derivatives at each point of a simply connected region $\Re$. Prove that a necessary and sufficient condition that $\oint_{C} P d x+Q d y=0$ around every closed path $C$ in $\Re$ is that $\partial P / \partial y=\partial Q / \partial x$ identically in $\Re$.

Sufficiency. Suppose $\partial P / \partial y=\partial Q / \partial x$. Then, by Green's theorem,

$$
\oint_{C} P d x+Q d y=\iint_{\Re}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y=0
$$

where $\Re$ is the region bounded by $C$.

Necessity. Suppose $\oint_{C} P d x+Q d y=0$ around every closed path $C$ in $\Re$ and that $\partial P / \partial y \neq \partial Q / \partial x$ at some point of $\Re$. In particular, suppose $\partial P / \partial y-\partial Q / \partial x>0$ at the point $\left(x_{0}, y_{0}\right)$.

By hypothesis, $\partial P / \partial y$ and $\partial Q$ are continuous in $\Re$, so that there must be some region $\tau$ containing $\left(x_{0}, y_{0}\right)$ as an interior point for which $\partial P / \partial y-\partial Q / \partial x>0$. If $\Gamma$ is the boundary of $\tau$, then by Green's theorem,

$$
\oint_{C} P d x+Q d y=\iint_{\tau}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y>0
$$

contradicting the hypothesis that $\oint_{\Gamma} P d x+Q d y=\iint_{\tau}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y>0$ for all closed curves in in $\Re$. Thus, $\partial Q / \partial x-\partial P / \partial y$ cannot be positive.

Similarly, we can show that $\partial Q / \partial x-\partial P / \partial y$ cannot be negative, and it follows that it must be identically zero; i.e., $\partial P / \partial y=\partial Q / \partial x$ identically in $\Re$.

10.12 Let $P$ and $Q$ be defined as in Problem 10.11. Prove that a necessary and sufficient condition that $\int_{A}^{B} P d x+Q d y$ be independent of the path in $\Re$ joining points $A$ and $B$ is that $\partial P / \partial y=\partial Q / \partial x$ identically in $\Re$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-268}
\end{center}

Figure 10.12

Sufficiency. If $\partial P / \partial y=\partial Q / \partial x$, then by Problem 10.11,

$$
\int_{A D B E A} P d x+Q d y=0
$$

(See Figure 10.12.) From this, omitting for brevity the integrand $P d x+Q d y$, we have

$$
\int_{A D B}+\int_{B E A}=0, \quad \int_{A D B}=-\int_{B E A}=\int_{A E B} \text { and so } \int_{C_{1}}=\int_{C_{2}}
$$

i.e., the integral is independent of the path.

Necessity. If the integral is independent of the path, then for all paths $C_{1}$ and $C_{2}$ in $\Re$ we have

$$
\int_{C_{1}}=\int_{C_{2}}, \quad \int_{A D B}=\int_{A E B} \text { and } \int_{\text {ADBEA }}=0
$$

From this it follows that the line integral around any closed path in $\Re$ is zero, and, hence, by Problem 10.11 that $\partial P / \partial y=\partial Q / \partial x$.

10.13. Let $P$ and $Q$ be as in Problem 10.11. (a) Prove that a necessary and sufficient condition that $P d x+Q d y$ be an exact differential of a function $\varphi(x, y)$ is that $\partial P / \partial y=\partial Q / \partial x$. (b) Show that in such case

$\int_{A}^{B} P d x+Q d y=\int_{A}^{B} d \phi=\phi(B)-\phi(A)$ where $\mathrm{A}$ and $\mathrm{B}$ are any two points.

(a) Necessity. If $P d x+Q d y=d \phi=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y$, an exact differential, then


\begin{align*}
& \partial \phi / \partial x=P  \tag{1}\\
& \partial \phi / \partial y=0 \tag{2}
\end{align*}


Thus, by differentiating Equations (1) and (2) with respect to $y$ and $x$, respectively, $\partial P / \partial y=\partial Q / \partial x$, since we are assuming continuity of the partial derivatives.

Sufficiency. By Problem 10.12, if $\partial P / \partial y=\partial Q / \partial x$, then $\int P d x+Q d y$ is independent of the path joining two points. In particular, let the two points be $(a, b)$ and $(x, y)$ and define

$$
\phi(x, y)=\int_{(a, b)}^{(x, y)} P d x+Q d y
$$

Then

$$
\begin{aligned}
\phi(x+\Delta x, y)-\phi(x, y) & =\int_{(a, b)}^{(x+\Delta x, y)} P d x+Q d y-\int_{(a, b)}^{(x, y)} P d x+Q d y \\
& =\int_{(x, y)}^{(x+\Delta x, y)} P d x+Q d y
\end{aligned}
$$

Since the last integral is independent of the path joining $(x, y)$ and $(x+\Delta x, y)$, we can choose the path to be a straight line joining these points (see Figure 10.13) so that $d y=0$. Then, by the mean value theorem for integrals,

$$
\frac{\phi(x+\Delta x, y)-\phi(x, y)}{\Delta x}=\frac{1}{\Delta x} \int_{(x, y)}^{(x+\Delta x, y)} P d x=P(x+\theta \Delta x, y) \quad 0<\theta<1
$$

Taking the limit as $\Delta x \rightarrow 0$, we have $\partial \phi / \partial x=P$.

Similarly, we can show that $\partial \phi / \partial y=Q$.

Thus, it follows that $P d x+Q d y=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y=d \phi$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-269}
\end{center}

Figure 10.13

(b) Let $A=\left(x_{1}, y_{1}\right)$ and $B=\left(x_{2}, y_{2}\right)$. From (a),

$$
\phi(x, y)=\int_{(a, b)}^{(x, y)} P d x+Q d y
$$

Then, omitting the integrand $P d x+Q d y$, we have

$$
\int_{A}^{B}=\int_{\left(x_{1}, y_{1}\right)}^{\left(x_{2}, y_{2}\right)}=\int_{(a, b)}^{\left(x_{2}, y_{2}\right)}-\int_{(a, b)}^{\left(x_{1}, y_{1}\right)}=\phi\left(x_{2}, y_{2}\right)-\phi\left(x_{1}, y_{1}\right)=\phi(B)-\phi(A)
$$

10.14. (a) Prove that $\int_{(1,2)}^{(3,4)}\left(6 x y^{2}-y^{3}\right) d x+\left(6 x^{2} y-3 x y^{2}\right) d y d y$ is independent of the path joining $(1,2)$ and $(3,4)$. (b) Evaluate the integral in (a).

(a) $P=6 x y 2-y 3, Q=6 x 2 y-3 x y 2$. Then $\partial P / \partial y=12 x y-3 y 2=\partial Q / \partial x$ and, by Problem 10.12, the line integral is independent of the path.

(b) Method 1: Since the line integral is independent of the path, choose any path joining $(1,2)$ and $(3,4)$, for example, that consisting of lines from $(1,2)$ to $(3,2)$ (along which $y=2, d y=0$ ) and then $(3,2)$ to $(3,4)$ (along which $x=3, d x=0$ ). Then the required integral equals

$$
\int_{x=1}^{3}(24 x-8) d x+\int_{y=2}^{4}\left(54 y-9 y^{2}\right) d y=80+156=236
$$

Method 2: Since $\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}$, we must have


\begin{align*}
& \frac{\partial \phi}{\partial y}=6 x^{2} y-3 x y^{2}  \tag{1}\\
& \frac{\partial \phi}{\partial y}=6 x^{2} y-3 x y^{2} \tag{2}
\end{align*}


From Equation (1), $\phi=3 x^{2} y^{2}-x y^{3}+f(y)$. From Equation (2), $\phi=3 x^{2} y^{2}-x y^{3}+g(x)$. The only way in which these two expressions for $\phi$ are equal is if $f(y)=g(x)=c$, a constant. Hence, $\phi=3 x^{2} y^{2}-x y^{3}+c$. Then, by Problem 10.13.

$$
\begin{aligned}
\int_{(1.2)}^{(3.4)}\left(6 x y^{2}-y^{3}\right) d x+\left(6 x^{2} y-3 x y^{2}\right) d y & =\int_{(1.2)}^{(3.4)} d\left(3 x^{2} y^{2}-x y^{3}+c\right) \\
& =3 x^{2} y^{2}-x y^{3}+\left.c\right|_{(1,2)} ^{(3,4)}=236
\end{aligned}
$$

Note that in this evaluation the arbitrary constant $c$ can be omitted. See also Problem 6.16.

We could also have noted by inspection that

$$
\begin{aligned}
\left(6 x y^{2}-y^{3}\right) d x+\left(6 x^{2} y-3 x y^{2}\right) d y & =\left(6 x y^{2} d x+6 x^{2} y d y\right)-\left(y^{3} d x+3 x y^{2} d y\right) \\
& =d\left(3 x^{2} y^{2}\right)-d\left(x y^{3}\right)=d\left(3 x^{2} y^{2}-x y^{3}\right)
\end{aligned}
$$

from which it is clear that $\phi=3 x^{2} y^{2}-x y^{3}+c$.

10.15. Evaluate $\oint\left(x^{2} y \cos x+2 x y \sin x-y^{2} e^{x}\right) d x+\left(x^{2} \sin x-2 y e^{x}\right) d y$ around the hypocycloid $x^{2 / 3}+$\\
$y^{2 / 3}=a^{2 / 3}$.

$$
P=x^{2} y \cos x+2 x y \sin x-y^{2} e^{x}, Q=x^{2} \sin x-2 y e^{x}
$$

Then $\partial P / \partial y=x^{2} \cos x+2 x \sin x-2 y e^{x}=\partial Q / \partial x$, so that, by Problem 10.11, the line integral around any closed path—in particular, $x^{2 / 3}+y^{2 / 3}=a^{2 / 3}$-is zero.

\section*{Surface integrals}
10.16. If $\gamma$ is the angle between the normal line to any point $(x, y, z)$ of a surface $S$ and the positive $z$ axis, prove that

$$
|\sec \gamma|=\sqrt{1+z_{x}^{2}+z_{y}^{2}}=\frac{\sqrt{F_{x}^{2}+F_{y}^{2}+F_{z}^{2}}}{\left|F_{z}\right|}
$$

according as the equation for $S$ is $z=f(x, y)$ or $F(x, y, z)=0$.

If the equation for $S$ is $F(x, y, z)=0$, a normal to $S$ at $(x, y, z)$ is $\nabla F=F_{x} \mathbf{i}+F_{y} \mathbf{j}+F_{z} \mathbf{k}$. Then

$$
\nabla F \cdot \mathrm{K}=|\nabla F||\mathrm{k}| \cos \gamma \quad \text { or } \quad F_{z}=\sqrt{F_{x}^{2}+F_{y}^{2}+F_{z}^{2}} \cos \gamma
$$

from which $|\sec \gamma|=\frac{\sqrt{F_{x}^{2}+F_{y}^{2}+F_{z}^{2}}}{\left|F_{z}\right|}$ as required. as required.

In case the equation is $z=f(x, y)$, we can write $F(x, y)=0$, from which $F_{x}=-z_{x}, F_{y}-z_{y}, F_{z}=1$ and we find $|\sec \gamma|=\sqrt{1+z_{x}^{2}+z_{y}^{2}}$.

10.17. Evaluate $\iint_{S} U(x, y, z) d S$, where $S$ is the surface of the paraboloid $z=2-\left(x^{2}+y^{2}\right)$ above the $x y$ plane and $U(x, y, z)$ is equal to (a) 1 , (b) $x^{2}+y^{2}, y^{2}$, and (c) $3 z$. Give a physical interpretation in each case. (See Figure 10.14.)

The required integral is equal to


\begin{equation*}
\iint_{\Re} U(x, y, z) \sqrt{1+z_{x}^{2}+z_{y}^{2}} d x d y . \tag{1}
\end{equation*}


where $\mathfrak{R}$ is the projection of $S$ on the $x y$ plane given by $x^{2}+y^{2}=2, z=0$.

Since $z_{x}=-2 x, z_{y}=-2 y,(1)$ can be written

$\iint_{\Re} U(x, y, z) \sqrt{1+4 x^{2}+4 y^{2}} d x d y$

(a) If $\mathrm{U}(\mathrm{x}, \mathrm{y}, \mathrm{z})=1$, (2) becomes

$$
\iint_{\Re} \sqrt{1+4 x^{2}+4 y^{2}} d x d y
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-270}
\end{center}

Figure 10.14

To evaluate this, transform to polar coordinates $(\rho, \phi)$. Then the integral becomes

$$
\int_{\phi=0}^{2 \pi} \int_{\rho=0}^{\sqrt{2}} \sqrt{1+4 \rho^{2}} \rho d \rho d \phi=\left.\int_{\phi=0}^{2 \pi} \frac{1}{12}\left(1+4 \rho^{2}\right)^{3 / 2}\right|_{\rho=0} ^{\sqrt{2}} d \phi=\frac{13 \pi}{3}
$$

Physically, this could represent the surface area of $S$ or the mass of $S$ assuming unit density.

(b) If $\mathrm{U}(\mathrm{x}, \mathrm{y}, \mathrm{z})=\mathrm{x} 2+\mathrm{y} 2$, (2) becomes $\iint_{\Re}\left(x^{2}+y^{2}\right) \sqrt{1+4 x^{2}+4 y^{2}} d x d y$ or, in polar coordinates,

$$
\int_{\phi=0}^{2 \pi} \int_{\rho=0}^{\sqrt{2}} \rho^{3} \sqrt{1+4 \rho^{2}} d \rho d \phi=\frac{149 \pi}{30}
$$

where the integration with respect to $\rho$ is accomplished by the substitution $\sqrt{1+4 \rho^{2}}=u$.

Physically, this could represent the moment of inertia of $S$ about the $z$ axis assuming unit density, or the mass of $S$ assuming a density $=x^{2}+y^{2}$.

(c) If $\mathrm{U}(\mathrm{x}, \mathrm{y}, \mathrm{z})=3 \mathrm{z}$, (2) becomes

$$
\iint_{\Re} 3 z \sqrt{1+4 x^{2}+4 y^{2}} d x d y=\iint_{\Re} 3\left\{2-\left(x^{2}+y^{2}\right)\right\} \sqrt{1+4 x^{2}+4 y^{2}} d x d y
$$

or, in polar coordinates,

$$
\int_{\phi=0}^{2 \pi} \int_{\rho=0}^{\sqrt{2}} 3 \rho\left(2-\rho^{2}\right) \sqrt{1+4 \rho^{2}} d \rho d \phi=\frac{111 \pi}{10}
$$

Physically, this could represent the mass of $S$ assuming a density $=3 z$, or three times the first moment of $S$ about the $x y$ plane.

10.18. Find the surface area of a hemisphere of radius $a$ cut off by a cylinder having this radius as diameter.

Equations for the hemisphere and cylinder (see Figure 10.15) are given, respectively, by $x^{2}+y^{2}+z^{2}=a^{2}$ (or $z \sqrt{a^{2}-x^{2}-y^{2}}$ ) and $(x-a / 2)^{2}+y^{2}=a^{2} / 4\left(\right.$ or $\left.x^{2}+y^{2}=a x\right)$.

Since

$$
z_{x}=\frac{-x}{\sqrt{a^{2}-x^{2}-y^{2}}} \quad \text { and } \quad z_{y}=\frac{-y}{\sqrt{a^{2}-x^{2}-y^{2}}}
$$

we have

$$
\text { Required surface area }=2 \iint_{\Re} \sqrt{1+z_{x}^{2}+z_{y}^{2}} d x d y=2 \iint_{\Re} \frac{a}{\sqrt{a^{2}-x^{2}-y^{2}}} d x d y
$$

Two methods of evaluation are possible.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-271}
\end{center}

Figure 10.15

Method 1: Using polar coordinates.

Since $x^{2}+y^{2}=a x$ in polar coordinates is $\rho=a \cos \phi$, the integral becomes

$$
\begin{aligned}
2 \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{a \cos \phi} \frac{a}{\sqrt{a^{2}-\rho^{2}}} \rho d \rho d \phi & =2 a \int_{\phi=0}^{\pi / 2}-\left.\sqrt{a^{2}-\rho^{2}}\right|_{\rho=0} ^{a \cos \phi} d \phi \\
& =2 a^{2} \int_{0}^{\pi / 2}(1-\sin \phi) d \phi=(\pi-2) a^{2}
\end{aligned}
$$

Method 2: The integral is equal to

$$
\begin{aligned}
2 \int_{x=0}^{a} \int_{y=0}^{\sqrt{a x-x^{2}}} \frac{a}{\sqrt{a^{2}-x^{2}-y^{2}}} d x d y & =\left.2 a \int_{x=0}^{a} \sin ^{-1} \frac{y}{\sqrt{a x-x^{2}}}\right|_{y=0} ^{\sqrt{a x-x^{2}}} d x \\
& =2 a \int_{0}^{a} \sin ^{-1} \sqrt{\frac{x}{a+x}} d x
\end{aligned}
$$

Letting $x=a \tan ^{2} \theta$, this integral becomes

$$
\begin{aligned}
4 a^{2} \int_{0}^{\pi / 4} \theta \tan \theta \sec ^{2} \theta d \theta & =4 a^{2}\left\{\left.\frac{1}{2} \theta \tan ^{2} \theta\right|_{0} ^{\pi / 4}-\frac{1}{2} \int_{0}^{\pi / 4} \tan ^{2} \theta d \theta\right\} \\
& =2 a^{2}\left\{\left.\theta \tan ^{2} \theta\right|_{0} ^{\pi / 4}-\int_{0}^{\pi / 4}\left(\sec ^{2} \theta-1\right) d \theta\right\} \\
& =2 a^{2}\left\{\pi / 4-\left.(\tan \theta-\theta)\right|_{0} ^{\pi / 4}\right\}=(\pi-2) a^{2}
\end{aligned}
$$

Note that these integrals are actually improper and should be treated by appropriate limiting procedures (see Problem 5.74 and Chapter 12).

10.19. Find the centroid of the surface in Problem 10.17.

$$
\text { By symmetry, } \bar{x}=\bar{y}=0 \quad \text { and } \quad \overline{\mathrm{z}}=\frac{\iint_{S} z d S}{\iint_{S} z d S}=\frac{\iint_{\Re} z \sqrt{1+4 x^{2}+4 y^{2}} d x d y}{\iint_{\Re} \sqrt{1+4 x^{2}+4 y^{2}} d x d y}
$$

The numerator and denominator can be obtained from the results of Problems 10.17(c) and 10.17(a), respectively, and we thus have $\bar{z}=\frac{37 \pi / 10}{13 \pi / 3}=\frac{111}{130}$.

10.20. Evalute $\iint_{S} \mathbf{A} \cdot \mathbf{n} d S$, where $\mathbf{A}=x y \mathbf{i}-x^{2} \mathbf{j}+(x+z) \mathbf{k}, S$ is that portion of the plane $2 x+2 y+z=6$ included in the first octant, and $\mathbf{n}$ is a unit normal to $S$. (See Figure 10.16.)

A normal to $S$ is $\nabla(2 x+2 y+z-6)=2 \mathbf{i}+2 \mathbf{j}+\mathbf{k}$, and so

$$
\mathbf{n}=\frac{2 \mathbf{i}+2 \mathbf{j}+\mathbf{k}}{\sqrt{2^{2}+2^{2}+1^{2}}}=\frac{2 \mathbf{i}+2 \mathbf{j}+\mathbf{k}}{3}
$$

Then

$$
\begin{aligned}
\mathbf{A} \cdot \mathbf{n} & =\left\{x y \mathbf{i}-x^{2} \mathbf{j}+(x+z) \mathbf{k}\right\} \cdot\left(\frac{2 \mathbf{i}+2 \mathbf{j}+\mathbf{k}}{3}\right) \\
& =\frac{2 x y-2 x^{2}+(x+z)}{3} \\
& =\frac{2 x y-2 x^{2}+(x+6-2 x-2 y)}{3} \\
& =\frac{2 x y-2 x^{2}-x-2 y+6}{3}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-272}
\end{center}

Figure 10.16

The required surface integral is, therefore,

$$
\begin{aligned}
\iint_{S}\left(\frac{2 x y-2 x^{2}-x-2 y+6}{3}\right) d S & =\iint_{\Re}\left(\frac{2 x y-2 x^{2}-x-2 y+6}{3}\right) \sqrt{1+z_{x}^{2}+z_{y}^{2}} d x d y \\
& =\iint_{\Re}\left(\frac{2 x y-2 x^{2}-x-2 y+6}{3}\right) \sqrt{1^{2}+2^{2}+2^{2}} d x d y \\
& =\int_{x=0}^{3} \int_{y=0}^{3-x}\left(2 x y-2 x^{2}-x-2 y+6\right) d y d x \\
& =\left.\int_{x=0}^{3}\left(x y^{2}-2 x^{2} y-x y-y^{2}+6 y\right)\right|_{0} ^{3-x} d x=27 / 4
\end{aligned}
$$

10.21. In dealing with surface integrals we have restricted ourselves to surface which are two-sided. Give an example of a surface which is not two-sided.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-273}
\end{center}

Figure 10.17

Take a strip of paper such as $A B C D$, as shown in Figure 10.17. Twist the strip so that points $A$ and $B$ fall on $D$ and $C$, respectively, as in the figure. If $\mathbf{n}$ is the positive normal at point $P$ of the surface, we find that as n moves around the surface, it reverses its original direction when it reaches $P$ again. If we tried to color only one side of the surface, we would find the whole thing colored. This surface, called a Möbius strip, is an example of a one-sided surface. This is sometimes called a nonorientable surface. A two-sided surface is orientable.

\section*{The divergence theorem}
10.22. Prove the divergence theorem. (See Figure 10.18.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-273(1)}
\end{center}

Figure 10.18

Let $S$ be a closed surface which is such that any line parallel to the coordinate axes cuts $S$ in, at most, two points. Assume the equations of the lower and upper portions $S_{1}$ and $S_{2}$ to be $z=f_{1}(x, y)$ and $z=f_{2}(x, y)$ respectively. Denote the projection of the surface on the $x y$ plane by $\Re$. Consider

$$
\begin{aligned}
\iiint_{V} \frac{\partial A_{3}}{\partial z} d V & =\iiint_{V} \frac{\partial A_{3}}{\partial z} d z d y d x=\iint_{\Re}\left[\int_{z=f_{1}(x, y)}^{f_{2}(x, y)} \frac{\partial A_{3}}{\partial z} d z\right] d y d x \\
& =\left.\iint_{\Re} A_{3}(x, y, z)\right|_{z=f_{1}} ^{f_{2}} d y d x=\iint_{\Re}\left[A_{3}\left(x, y, f_{2}\right)-A_{3}\left(x, y, f_{1}\right)\right] d y d x
\end{aligned}
$$

with $\mathbf{k}$.

For the upper portion $S_{2}, d y d x=\cos \gamma_{2} d S_{2}=\mathbf{k} \cdot \mathbf{n}_{2} d S_{2}$ since the normal $\mathbf{n}_{2}$ to $S_{2}$ makes an acute angle $\gamma_{2}$

For the lower portion $S_{1}, d y d x=-\cos \gamma_{1} d S_{1}=-\mathbf{k} \cdot \mathbf{n}_{1} d S_{1}$ since the normal $\mathbf{n}_{1}$ to $S_{1}$ makes an obtuse angle $\gamma_{1}$ with $\mathbf{k}$.

Then

$$
\begin{aligned}
& \iint_{\Re} A_{3}\left(x, y, f_{2}\right) d y d x=\iint_{S_{2}} A_{3} \mathbf{k} \cdot \mathbf{n}_{2} d S_{2} \\
& \iint_{\Re} A_{3}\left(x, y, f_{1}\right) d y d x=-\iint_{S_{1}} A_{3} \mathbf{k} \cdot \mathbf{n}_{1} d S_{1}
\end{aligned}
$$

and

$$
\begin{aligned}
\iint_{\Re} A_{3}\left(x, y, f_{2}\right) d y d x-\iint_{\Re} A_{3}\left(x, y, f_{1}\right) d y d x & =\iint_{S_{2}} A_{3} \mathbf{k} \cdot \mathbf{n}_{2} d S_{2}+\iint_{S_{1}} A_{3} \mathbf{k} \cdot \mathbf{n}_{1} d S_{1} \\
& =\iint_{S} A_{3} \mathbf{k} \cdot \mathbf{n} d S
\end{aligned}
$$

so that


\begin{equation*}
\iiint_{V} \frac{\partial A_{3}}{\partial z} d V=\iint_{S} A_{3} \mathbf{k} \cdot \mathbf{n} d S \tag{1}
\end{equation*}


Similarly, by projecting $S$ on the other coordinate planes,


\begin{align*}
& \iiint_{V} \frac{\partial A_{1}}{\partial x} d V=\iint_{S} A_{3} \mathbf{i} \cdot \mathbf{n} d S  \tag{2}\\
& \iiint_{V} \frac{\partial A_{2}}{\partial y} d V=\iint_{S} A_{3} \mathbf{j} \cdot \mathbf{n} d S \tag{3}
\end{align*}


Adding Equations (1), (2), and (3),

$$
\iiint_{V}\left(\frac{\partial A_{1}}{\partial x}+\frac{\partial A_{2}}{\partial y}+\frac{\partial A_{3}}{\partial z}\right) d V=\iint_{S}\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right) \mathbf{n} d S
$$

or

$$
\iiint_{V} \nabla \cdot \mathbf{A} d V=\iint_{S} \mathbf{A} \cdot \mathbf{n} d S
$$

The theorem can be extended to surfaces which are such that lines parallel to the coordinate axes meet them in more than two points. To establish this extension, subdivide the region bounded by $S$ into subregions whose surfaces do satisfy this condition. The procedure is analogous to that used in Green's theorem for the plane.

10.23. Verify the divergence theorem for $\mathbf{A}=(2 x-z) \mathbf{i}+x^{2} y \mathbf{j}-x z^{2} \mathbf{k}$ taken over the region bounded by $x=0, x=1, y$ $=0, y=1, z=0, z=1$.

We first evaluate $\iint_{S} \mathbf{A} \cdot \mathbf{n} d S$, where $S$ is the surface of the cube in Figure 10.19.

Face DEFG: $\mathbf{n}=\mathbf{i}, x=1$. Then

$$
\begin{aligned}
\iint_{D E F G} \mathbf{A} \cdot \mathbf{n} d S & =\int_{0}^{1} \int_{0}^{1}\left\{(2-z) \mathbf{i}+\mathbf{j}-z^{2} \mathbf{k}\right\} \cdot \mathbf{i} d y d z \\
& =\int_{0}^{1} \int_{0}^{1}(2-z) d y d z=3 / 2
\end{aligned}
$$

Face ABCO: $\mathbf{n}=-\mathbf{i}, x=0$. Then

$$
\begin{aligned}
\iint_{A B C O} \mathbf{A} \cdot \mathbf{n} d S & =\int_{0}^{1} \int_{0}^{1}(-z \mathbf{i}) \cdot(-\mathbf{i}) d y d z \\
& =\int_{0}^{1} \int_{0}^{1} z d y d z=1 / 2
\end{aligned}
$$

Face ABEF: $\mathbf{n}=\mathbf{j}, y=1$. Then

$$
\iint_{A B E F} \mathbf{A} \cdot \mathbf{n} d S=\int_{0}^{1} \int_{0}^{1}\left\{(2-z) \mathbf{i}+x^{2} \mathbf{j}-x z^{2} \mathbf{k}\right\} \cdot \mathbf{j} d x d z=\int_{0}^{1} \int_{0}^{1} x^{2} d x d z=1 / 3
$$

Face OGDC: $\mathbf{n}=-\mathbf{j}, y=0$. Then

$$
\iint_{O G D C} \mathbf{A} \cdot \mathbf{n} d S=\int_{0}^{1} \int_{0}^{1}\left\{(2 x-z) \mathbf{i}-x z^{2} \mathbf{k}\right\} \cdot(-\mathbf{j}) d x d z=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-275}
\end{center}

Figure 10.19

Face BCDE: $\mathbf{n}=\mathbf{k}, z=1$. Then

$$
\iint_{B C D E} \mathbf{A} \cdot \mathbf{n} d S=\int_{0}^{1} \int_{0}^{1}\left\{(2 x-1) \mathbf{i}+x^{2} \mathbf{y} \mathbf{j}-x \mathbf{k}\right\} \cdot \mathbf{k} d x d y=\int_{0}^{1} \int_{0}^{1}-x d x d y-1 / 2
$$

Face AFGO: $\mathbf{n}=-\mathbf{k}, z=0$. Then

$$
\iint_{A F G O} \mathbf{A} \cdot \mathbf{n} d S=\int_{0}^{1} \int_{0}^{1}\left\{2 x \mathbf{i}-x^{2} \mathbf{y} \mathbf{j}\right\} \cdot(-\mathbf{k}) d x d y=0
$$

Adding $\iint_{S} \mathbf{A} \cdot \mathbf{n} d S=\frac{3}{2}+\frac{1}{2}+\frac{1}{3}+0-\frac{1}{2}+0=\frac{11}{6}$. Since

$$
\iiint_{V} \nabla \cdot \mathbf{A} d V=\int_{0}^{1} \int_{0}^{1} \int_{0}^{1}\left(2+x^{2}-2 x z\right) d x d y d z=\frac{11}{6}
$$

the divergence theorem is verified in this case.

10.24. Evaluate $\iint_{S} \mathbf{A} \cdot \mathbf{n} d s$, where $S$ is a closed surface.

By the divergence theorem,

$$
\begin{aligned}
\iint_{S} \mathbf{r} \cdot \mathbf{n} d S & =\iiint_{V} \nabla \cdot \mathbf{r} d V \\
& =\iiint_{V}\left(\frac{\partial}{\partial x} \mathbf{i}+\frac{\partial}{\partial y} \mathbf{j}+\frac{\partial}{\partial z} \mathbf{k}\right) \cdot(x \mathbf{i}+y \mathbf{j}+z \mathbf{k}) d V \\
& =\iiint_{V}\left(\frac{\partial x}{\partial x} \mathbf{i}+\frac{\partial y}{\partial y} \mathbf{j}+\frac{\partial z}{\partial z} \mathbf{k}\right) d V=3 \iiint_{V} d V=3 V
\end{aligned}
$$

where $V$ is the volume enclosed by $S$.

10.25. Evaluate $\iint_{S} x z^{2} d y d z+\left(x^{2} y-z^{3}\right) d z d x+\left(2 x y+y^{2} z\right) d x d y$, where $S$ is the entire surface of the hemispherical region bounded by $z=\sqrt{a^{2}-x^{2}-y^{2}}$ and $z=0$ (a) by the divergence theorem (Green's theorem in space) and (b) directly.

(a) Since $d y d z=d S \cos \alpha, d z d x=d S d S \cos \beta$, and $d x d y=d S \cos \gamma$, the integral can be written

$$
\iint_{S}\left\{x z^{2} \cos \alpha+\left(x^{2} y-z^{3}\right) \cos \beta+\left(2 x y+y^{2} z\right) \cos \gamma\right\} d S=\iint_{S} \mathbf{A} \cdot \mathbf{n} d S
$$

where $\mathbf{A}=x z^{2} \mathbf{i}+\left(x^{2} y-z^{3}\right) \mathbf{j}+\left(2 x y+y^{2} z\right) \mathbf{k}$ and $\mathbf{n}=\cos \alpha \mathbf{i}+\cos \beta \mathbf{j}+\cos \gamma \mathbf{k}$, the outward drawn unit normal.

Then, by the divergence theorem the integral equals

$$
\iiint_{V} \nabla \cdot \mathbf{A} d V=\iiint_{V}\left\{\frac{\partial}{\partial x}\left(x z^{2}\right)+\frac{\partial}{\partial y}\left(x^{2} y-z^{3}\right)+\frac{\partial}{\partial z}\left(2 x y+y^{2} z\right)\right\} d V=\iiint_{V}\left(x^{2}+y^{2}+z^{2}\right) d V
$$

where $V$ is the region bounded by the hemisphere and the $x y$ plane.

By use of spherical coordinates, as in Problem 9.19, this integral is equal to

$$
4 \int_{\phi=0}^{\pi / 2} \int_{\theta=0}^{\pi / 2} \int_{r=0}^{\alpha} r^{2} \cdot r^{2} \sin \theta d r d \theta d \phi=\frac{2 \pi a^{5}}{5}
$$

(b) If $S_{1}$ is the convex surface of the hemispherical region and $S_{2}$ is the base $(z=0)$, then

$$
\begin{aligned}
& \iint_{S_{1}} x z^{2} d y d z=\int_{y=-a}^{a} \int_{z=0}^{\sqrt{a^{2}-y^{2}}} z^{2} \sqrt{a^{2}-y^{2}-z^{2}} d z d y-\int_{y=-a}^{a} \int_{z=0}^{\sqrt{a^{2}-x^{2}}}-z^{2} \sqrt{a^{2}-y^{2}-z^{2}} d z d x \\
& \iint_{S_{1}}\left(x^{2} y-z^{3}\right) d y d x=\int_{x=-a}^{a} \int_{x=0}^{\sqrt{a^{2}-x^{2}}}\left\{x^{2} \sqrt{a^{2}-y^{2}-z^{2}}-z^{3}\right\} d z d x \\
& \quad-\int_{x=-a}^{a} \int_{z=0}^{\sqrt{a^{2}-x^{2}}}\left\{-x^{2} \sqrt{a^{2}-x^{2}-z^{2}}-z^{3}\right\} d z d x \\
& \iint_{S_{1}}\left(2 x y-y^{2} z\right) d x d y=\int_{x=-a}^{a} \int_{y=-\sqrt{a^{2}-x^{2}}}^{\sqrt{a^{2}-x^{2}}}\left\{2 x y+y^{2} \sqrt{a^{2}-y^{2}-z^{2}}\right\} d y d x \\
& \iint_{S_{2}} x z^{2} d y d z=0, \quad \iint_{S_{2}}\left(x^{2} y-z^{3}\right) d z d x=0, \\
& \iint_{S_{2}}\left(2 x y-y^{2} z\right) d x d y=\iint_{S_{2}}\left\{2 x y-y^{2}(0)\right\} d x d y=\int_{x=-a}^{a} \int_{y=-\sqrt{a^{2}-x^{2}}}^{\sqrt{a^{2}-x^{2}}} 2 x y d y d x=0
\end{aligned}
$$

By addition of the preceding, we obtain

$$
\begin{aligned}
4 \int_{y=0}^{a} \int_{x=0}^{\sqrt{a^{2}-y^{2}}} z^{2} \sqrt{a^{2}-y^{2}-z^{2}} d z d y+4 \int_{x=0}^{a} \int_{z=0}^{\sqrt{a^{2}-x^{2}}} x^{2} \sqrt{a^{2}-x^{2}-z^{2}} d z d x \\
+4 \int_{x=0}^{a} \int_{y=0}^{\sqrt{a^{2}-x^{2}}} y^{2} \sqrt{a^{2}-x^{2}-y^{2}} d y d x
\end{aligned}
$$

Since by symmetry all these integrals are equal, the result, on using polar coordinates, is

$$
12 \int_{x=0}^{a} \int_{y=0}^{\sqrt{a^{2}-x^{2}}} y^{2} \sqrt{a^{2}-x^{2}-y^{2}} d y d x=12 \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{a} \rho^{2} \sin ^{2} \phi \sqrt{a^{2}-\rho^{2}} \rho d \rho d \phi=\frac{2 \pi a^{5}}{5}
$$

\section*{Stokes's theorem}
10.26. Prove Stokes's theorem.

Let $S$ be a surface which is such that its projections on the $x y, y z$, and $x z$ planes are are regions bounded by simple closed curves, as indicated in Figure 10.20. Assume $S$ to have representation $z=f(x, y)$ or $x=$ $g(y, z)$ or $y=h(x, z)$, where $f, g$, and $h$ are single-valued, continuous, and differentiable functions. We must show that

$$
\begin{aligned}
\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S & =\iint_{S}\left[\nabla \times\left(A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}\right)\right] \cdot \mathbf{n} d S \\
& =\int_{C} A \cdot d \mathbf{r}
\end{aligned}
$$

where $C$ is the boundary of $S$.

$$
\begin{aligned}
& \text { Consider first } \iint_{S}\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S \\
& \text { Since } \nabla \times\left(A_{1} \mathbf{i}\right)=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
A_{1} & 0 & 0
\end{array}\right|=\frac{\partial A_{1}}{\partial z} \mathbf{j}-\frac{\partial A_{1}}{\partial y} \mathbf{k}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-277}
\end{center}

Figure 10.20


\begin{equation*}
\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S=\left(\frac{\partial A_{1}}{\partial z} \mathbf{n} \cdot \mathbf{j}-\frac{\partial A_{1}}{\partial y} \mathbf{n} \cdot \mathbf{k}\right) d S \tag{1}
\end{equation*}


If $z=f(x, y)$ is taken as the equation of $S$, then the position vector to any point of $S$ is $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}=x \mathbf{i}+$ $y \mathbf{j}+f(x, y) \mathbf{k}$ so that $\frac{\partial \mathbf{r}}{\partial y}=\mathbf{j}+\frac{\partial z}{\partial y} \mathbf{k}=\mathbf{j}+\frac{\partial f}{\partial y} \mathbf{k}$. But $\frac{\partial \mathbf{r}}{\partial y}$ is a vector tangent to $S$ and thus perpendicular to $\mathbf{n}$, so that

$$
\mathbf{n} \cdot \frac{\partial \mathbf{r}}{\partial y}=\mathbf{n} \cdot \mathbf{j}+\frac{\partial z}{\partial y} \mathbf{n} \cdot \mathbf{k}=0 \quad \text { or } \quad \mathbf{n} \cdot \mathbf{j}=-\frac{\partial z}{\partial y} \mathbf{n} \cdot \mathbf{k}
$$

Substitute in Equation (1) to obtain

$$
\left(\frac{\partial A_{1}}{\partial z} \mathbf{n} \cdot \mathbf{j}-\frac{\partial A_{1}}{\partial y} \mathbf{n} \cdot \mathbf{k}\right) d S=\left(\frac{\partial A_{1}}{\partial z} \frac{\partial z}{\partial y} \mathbf{n} \cdot \mathbf{k}-\frac{\partial A_{1}}{\partial y} \mathbf{n} \cdot \mathbf{k}\right) d S
$$

or


\begin{equation*}
\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S=-\left(\frac{\partial A_{1}}{\partial z}+\frac{\partial A_{1}}{\partial y} \frac{\partial z}{\partial y}\right) \mathbf{n} \cdot \mathbf{k} d S \tag{2}
\end{equation*}


Now on $S, A_{1}(x, y, z)=A_{1}[x, y, f(x, y)]=F(x, y)$; hence, $\frac{\partial A_{1}}{\partial y}+\frac{\partial A_{1}}{\partial z} \frac{\partial z}{\partial y}=\frac{\partial F}{\partial y}$ and Equation (2) becomes

$$
\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S=-\frac{\partial F}{\partial y} \mathbf{n} \cdot \mathbf{k} d S=-\frac{\partial F}{\partial y} d x d y
$$

Then

$$
\iint_{S}\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S=\iint_{\Re}-\frac{\partial F}{\partial y} d x d y
$$

where $\Re$ is the projection of $S$ on the $x y$ plane. By Green's theorem for the plane, the last integral equals $\oint_{\Gamma} F d x$ where $\Gamma$ is the boundary of $\Re$. Since at each point $(x, y)$ of $\Gamma$ the value of $F$ is the same as the value of $A_{1}$ at each point $(x, y, z)$ of $C$, and since $d x$ is the same for both curves, we must have

$$
\oint_{\Gamma} F d x=\oint_{C} A_{1} d x
$$

or

$$
\iint_{S}\left[\nabla \times\left(A_{1} \mathbf{i}\right)\right] \cdot \mathbf{n} d S=\oint_{C} A_{1} d x
$$

Similarly, by projections on the other coordinate planes,

$$
\iint_{S}\left[\nabla \times\left(\mathbf{A}_{2} \mathbf{j}\right)\right] \cdot \mathbf{n} \mathrm{d} S=\oint_{C} A_{2} d y, \quad \iint_{S}\left[\nabla \times\left(A_{3} \mathbf{k}\right)\right] \cdot \mathbf{n} d S=\oint_{C} A_{3} d z
$$

Thus, by addition,

$$
\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S=\oint_{C} \mathbf{A} \cdot d \mathbf{r}
$$

The theorem is also valid for surfaces $S$ which may not satisfy these imposed restrictions. Assume that $S$ can be subdivided into surfaces $S_{1}, S_{2}, \ldots S_{k}$ with boundaries $C_{1}, C_{2}, \ldots, C_{k}$, which do satisfy the restrictions. Then Stokes's theorem holds for each such surface. Adding these surface integrals, the total surface integral over $S$ is obtained. Adding the corresponding line integrals over $C_{1}, C_{2} \ldots, C_{k}$, the line integral over $C$ is obtained.

10.27. Verify Stokes's theorem for $\mathbf{A}=3 y \mathbf{i}-x z \mathbf{j}+y z^{2} \mathbf{k}$, where $S$ is the surface of the paraboloid $2 z=x^{2}+y^{2}$ bounded by $z=2$ and $C$ is its boundary. See Figure 10.21 .

The boundary $C$ of $S$ is a circle with equations $x^{2}+y^{2}=4, z=2$ and parametric equations $x=2 \cos t, y=$ $2 \sin t, z=2$, where $0 \leqq t<2 \pi$. Then

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-278}
\end{center}

Figure 10.21

$$
\begin{aligned}
\oint_{C} \mathbf{A} \cdot d \mathbf{r} & =\oint_{C} 3 y d x-x z d y+y z^{2} d z \\
& =\int_{2 \pi}^{0} 3(2 \sin t)(-2 \sin t) d t-(2 \cos t)(2)(2 \cos t) d t \\
& =\int_{0}^{2 \pi}\left(12 \sin ^{2} t+8 \cos ^{2} t\right) d t=20 \pi
\end{aligned}
$$

Also,

$$
\nabla \times \mathbf{A}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
3 y & -x z & y z^{2}
\end{array}\right|=\left(z^{2}+x\right) \mathbf{i}-(z+3) \mathbf{k}
$$

and

$$
\mathbf{n}=\frac{\nabla\left(x^{2}+y^{2}-2 z\right)}{\left|\nabla\left(x^{2}+y^{2}-2 z\right)\right|}=\frac{x \mathbf{i}+y \mathbf{j}-\mathbf{k}}{\sqrt{x^{2}+y^{2}+1}}
$$

Then

$$
\begin{aligned}
\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S & =\iint_{\Re}(\nabla \cdot A) \cdot \mathbf{n} \frac{d x d y}{|\mathbf{n} \cdot \mathbf{k}|}=\iint_{\Re}\left(x z^{2}+x^{2}+z+3\right) d x d \\
& =\iint_{\Re}\left\{x\left(\frac{x^{2}+y^{2}}{2}\right)^{2}+x^{2}+\frac{x^{2}+y^{2}}{2}+3\right\} d x d y
\end{aligned}
$$

In polar coordinates this becomes

$$
\int_{\phi=0}^{2 \pi} \int_{\rho=0}^{2}\left\{(\rho \cos \phi)\left(\rho^{4} / 2\right)+\rho^{2} \cos ^{2} \phi+\rho^{2} / 2+3\right\} \rho d \rho d \phi=20 \pi
$$

10.28. Prove that a necessary and sufficient condition that $\oint_{C} \mathbf{A} \cdot d \mathbf{r}=0$ for every closed curve $C$ is that $\nabla \times \mathbf{A}=0$ identically.

Sufficiency. Suppose $\nabla \times \mathbf{A}=\mathbf{0}$. Then, by Stokes's theorem,

$$
\oint_{C} \mathbf{A} \cdot d \mathbf{r}=\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S=0
$$

Necessity. Suppose $\oint_{C} \mathbf{A} \cdot d \mathbf{r}=0$ around every closed path $C$, and assume $\nabla \times \mathbf{A} \neq \mathbf{0}$ at some point $P$. Then, assuming $\nabla \times \mathbf{A}$ is continuous, there will be a region with $P$ as an interior point, where $\nabla \times \mathbf{A} \neq \mathbf{0}$. Let $S$ be a surface contained in this region whose normal $\mathbf{n}$ at each point has the same direction as $\nabla \times \mathbf{A} ;$ i.e., $\nabla \times \mathbf{A}=$ $\alpha \mathbf{n}$ where $\alpha$ is a positive constant. Let $C$ be the boundary of $S$. Then, by Stokes's theorem,

$$
\oint_{C} \mathbf{A} \cdot d \mathbf{r}=\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S=\alpha \iint_{S} \mathbf{n} \cdot \mathbf{n} d S>0
$$

which contradicts the hypothesis that $\oint_{C} \mathbf{A} \cdot d \mathbf{r}=0$ and shows that $\nabla \times \mathbf{A}=0$.

It follows that $\nabla \times \mathbf{A}=\mathbf{0}$ is also a necessary and sufficient condition for a line integral $\int_{P_{1}}^{P_{2}} \mathbf{A} \cdot d \mathbf{r}$ to be independent of the path joining points $P_{1}$ and $P_{2}$.

10.29. Prove that a necessary and sufficient condition that $\nabla \times \mathbf{A}=\mathbf{0}$ is that $\mathbf{A}=\nabla \phi$.

Sufficiency. If $\mathbf{A}=\nabla \phi$, then $\nabla \times \mathbf{A}=\nabla \times \nabla \phi=\mathbf{0}$ by Problem 7.80.

Necessity. If $\nabla \times \mathbf{A}=\mathbf{0}$, then by Problem $10.28, \oint_{C} \mathbf{A} \cdot d \mathbf{r}=\mathbf{0}$ around every closed path and $\int_{P_{1}}^{P_{2}} \mathbf{A} \cdot d \mathbf{r}$ is independent of the path joining two points, which we take as $(a, b, c)$ and $(x, y, z)$. Let us define

$$
\phi(x, y, z)=\int_{(a, b, c)}^{(x, y, z)} \mathbf{A} \cdot d \mathbf{r}=\int_{(a, b, c)}^{(x, y, z)} A_{1} d x+A_{2} d y+A_{3} d z
$$

Then

$$
\phi(x+\Delta x, y, z)-\phi(x, y, z)=\int_{(x, y, z)}^{(x+\Delta x, y, z)} A_{1} d x+A_{2} d y+A_{3} d z
$$

Since the last integral is independent of the path joining $(x, y, z)$ and $(x+\Delta x, y, z)$, we can choose the path to be a straight line joining these points so that $d y$ and $d z$ are zero. Then

$$
\frac{\phi(x+\Delta x, y, z)-\phi(x, y, z)}{\Delta x}=\frac{1}{\Delta x} \int_{(x, y, z)}^{(x+\Delta x, y, z)} A_{1} d x=A_{1}(x+\theta \Delta x, y, z) \quad 0<\theta<1
$$

where we have applied the law of the mean for integrals.

Taking the limit of both sides as $\Delta x \rightarrow 0$ gives $\partial \phi / \partial x=A_{1}$.

Similarly, we can show that $\partial \phi / \partial y=A_{2}, \partial \phi / \partial z=A_{3}$. Thus,

$$
\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}=\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}=\nabla \phi
$$

10.30. (a) Prove that a necessary and sufficient condition that $A_{1} d x+A_{2} d y+A_{3} d z=d \phi$, an exact differential, is that $\nabla \times \mathbf{A}=\mathbf{0}$ where $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$. (b) Show that in such case,

$$
\int_{\left(x_{1}, y_{1}, z_{1}\right)}^{\left(x_{2}, y_{2}, z_{2}\right)} A_{1} d x+A_{2} d y+A_{3} d z=\int_{\left(x_{1}, y_{1}, z_{1}\right)}^{\left(x_{2}, y_{2}, z_{2}\right)} d \phi=\phi\left(x_{2}, y_{2}, z_{2}\right)-\phi\left(x_{1}, y_{1}, z_{1}\right)
$$

(a) Necessity. If $A_{1} d x+A_{2} d y+A_{3} d z=d \phi=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y+\frac{\partial \phi}{\partial z} d z$, then


\begin{align*}
& \frac{\partial \phi}{\partial x}=A_{1}  \tag{1}\\
& \frac{\partial \phi}{\partial y}=A_{2}  \tag{2}\\
& \frac{\partial \phi}{\partial z}=A_{3} \tag{3}
\end{align*}


Then, by differentiating, and assuming continuity of the partial derivatives, we have

$$
\frac{\partial A_{1}}{\partial y}=\frac{\partial A_{2}}{\partial x}, \quad \frac{\partial A_{2}}{\partial z}=\frac{\partial A_{3}}{\partial y}, \quad \frac{\partial A_{1}}{\partial z}=\frac{\partial A_{3}}{\partial x}
$$

which is precisely the condition $\nabla \times \mathbf{A}=\mathbf{0}$.

Another method: If $A_{1} d x+A_{2} d y+A_{3} d z=d \phi$, then

$$
\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}=\frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k}=\nabla \phi
$$

from which $\nabla \times \mathbf{A}=\nabla \times \nabla \phi=\mathbf{0}$.

Sufficiency. If $\nabla \times \mathbf{A}=\mathbf{0}$, then by Problem $10.29, \mathbf{A}=\nabla \phi$ and

$$
A_{1} d x+A_{2} d y+A_{3} d z=\mathrm{A} \cdot d r=\nabla \phi \cdot d r=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y+\frac{\partial \phi}{\partial z} d z=d \phi
$$

(b) From (a),

$$
\phi(x, y, z)=\int_{(a, b, c)}^{(x, y, z)} A_{1} d x+A_{2} d y+A_{3} d z
$$

Then, omitting the integrand $A_{1} d x+A_{2} d y+A_{3} d z$, we have

$$
\int_{\left(x_{1}, y_{1}, z_{1}\right)}^{\left(x_{2}, y_{2}, z_{2}\right)}=\int_{(a, b, c)}^{\left(x_{2}, y_{2}, z_{2}\right)}--\iint_{(a, b, c)}^{\left(x_{1}, y_{1}, z_{1}\right)}=\left(\not\left(x_{2}, y_{2}, z_{2}\right) \cdot-\mid \not\left(x_{1}, y_{1}, z_{1}\right)\right.
$$

10.31. (a) Prove that $\mathbf{F}=\left(2 x z^{3}+6 y\right) \mathbf{i}+(6 x-2 y z) \mathbf{j}+\left(3 x^{2} z^{2}-y^{2}\right) \mathbf{k}$ is a conservative force field. (b) Evaluate $\int_{C} \mathbf{F}$. $d \mathbf{r}$ where $C$ is any path from $(1,-1,1)$ to $(2,1,-1)$. (c) Give a physical interpretation of the results.

(a) A force field $\mathbf{F}$ is conservative if the line integral $\int_{C} \mathbf{F} \cdot d \mathbf{r}$ is independent of the path $C$ joining any two points. A necessary and sufficient condition that $\mathrm{F}$ be conservative is that $\nabla \times \mathbf{F}=\mathbf{0}$.

Since here $\nabla \times \mathrm{F}=\left|\begin{array}{ccc}\mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ 2 x z^{3}+6 y & 6 x-2 y z & 3 x^{2} z^{2}-y^{2}\end{array}\right|=\mathbf{0}, \quad \mathbf{F}$ is conservative\\
(b) Method 1: By Problem 10.30, F $\cdot d \mathbf{r}=\left(2 x z^{3}+6 y\right) d x+(6 x-2 y z) d y+\left(3 x^{2} z^{2}-y^{2}\right) d z$ is an exact differential $d \phi$, where $\phi$ is such that


\begin{align*}
& \frac{\partial \phi}{\partial x}=2 x z^{3}+6 y  \tag{1}\\
& \frac{\partial \phi}{\partial y}=6 x-2 y z  \tag{2}\\
& \frac{\partial \phi}{\partial z}=3 x^{2} z^{2}-y^{2} \tag{3}
\end{align*}


From these we obtain, respectively,

$$
\begin{gathered}
\phi=x^{2} z^{3}+6 x y+f_{1}(y, z) \\
\phi=6 x y-y^{2} z+f_{2}(x, z) \\
\phi=x^{2} y^{2}-y^{2} z+f_{3}(x, y)
\end{gathered}
$$

These are consistent if $f_{1}(y, z)=-y^{2} z+c, f_{2}(x, z)=x^{2} z^{3}+c$, and $f_{3}(x, y)=6 x y+c$, in which case $\phi=x^{2} z^{3}$ $+6 x y-y^{2} z+c$. Thus, by Problem 10.30,

$$
\int_{(1,-1,1)}^{(2,1,-1)} \mathrm{F} \cdot d \mathrm{r}=x^{2} z^{3}+6 x y-y^{2} z+\left.c\right|_{(1,-1,1)} ^{(2,1,-1)}=15
$$

Alternatively, we may notice by inspection that

$$
\begin{aligned}
\mathbf{F} \cdot d \mathbf{r} & =\left(2 x z^{3} d x+3 x^{2} z^{2} d z\right)+(6 y d x+6 x d y)-\left(2 y z d y+y^{2} d z\right) \\
& =d\left(x^{2} z^{3}\right)+d(6 x y)-d\left(y^{2} z\right)=d\left(x^{2} z^{3}+6 x y-y^{2} z+c\right) \text { from which } \phi \text { is determined. }
\end{aligned}
$$

Method 2: Since the integral is independent of the path, we can choose any path to evaluate it; in particular, we can choose the path consisting of straight lines from $(1,-1,1)$ to $(2,-1,1)$, then to $(2,1,1)$ and then to $(2,1,-1)$. The result is

$$
\int_{x=1}^{2}(2 x-6) d x+\int_{y=1}^{1}(12-2 y) d y+\int_{z=1}^{-1}\left(12 z^{2}-1\right) d z=15
$$

where the first integral is obtained from the line integral by placing $y=-1, z=1, d y=0, d z=0$; the second integral, by placing $x=2, z=1, d x=0, d z=0$; and the third integral, by placing $x=2, y=1, d x=0, d y=0$.

(c) Physically, $\int_{C} \mathbf{F} \cdot d \mathbf{r}$ represents the work done in moving an object from $(1,-1,1)$ to $(2,1,-1)$ along $C$. In a conservative force field, the work done is independent of the path $C$ joining these points.

\section*{Miscellaneous problems}
10.32. (a) If $x=f(u, v), y=g(u, v)$ defines a transformation which maps a region $\Re$ of the $x y$ plane into a region $\Re^{\prime}$ of the $u v$ plane, prove that

$$
\iint_{\Re} d x d y=\iint_{\Re^{\prime}}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v
$$

(b) Interpret geometrically the result in (a).

(a) If $C$ (assumed to be a simple closed curve) is the boundary of $\Re$, then by Problem 10.8 ,


\begin{equation*}
\iint_{\Re} d x d y=\frac{1}{2} \oint_{C} x d y-y d x \tag{1}
\end{equation*}


Under the given transformation, the integral on the right of Equation (1) becomes


\begin{equation*}
\frac{1}{2} \oint_{C^{\prime}} x\left(\frac{\partial y}{\partial u} d u+\frac{\partial y}{\partial v} d v\right)-y\left(\frac{\partial y}{\partial u} d u+\frac{\partial x}{\partial v} d v\right)=\frac{1}{2} \int_{C^{\prime}}\left(x \frac{\partial y}{\partial u}-y \frac{\partial x}{\partial u}\right) d u+\left(x \frac{\partial y}{\partial v}-y \frac{\partial x}{\partial v}\right) d v \tag{2}
\end{equation*}


where $C^{\prime}$ is the mapping of $C$ in the $u v$ plane (we suppose the mapping to be such that $C^{\prime}$ is a simple closed curve also).

By Green's theorem, if $\Re^{\prime}$ is the region in the $u v$ plane bounded by $C^{\prime}$, the right side of Equation (2) equals

$$
\begin{aligned}
\frac{1}{2} \iint_{\Re^{\prime}} \left\lvert\, \frac{\partial}{\partial u}\left(x \frac{\partial y}{\partial v}-y \frac{\partial x}{\partial v}\right)-\frac{\partial}{\partial v}\left(x \frac{\partial y}{\partial u}-y \frac{\partial x}{\partial u}\right) d u d v\right. & =\iint_{\Re^{\prime}}\left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v}-\frac{\partial x}{\partial v} \frac{\partial y}{\partial u}\right| d u d v \\
& =\iint_{\Re^{\prime}}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v
\end{aligned}
$$

where we have inserted absolute value signs so as to ensure that the result is nonnegative, as is $\iint_{\mathfrak{R}} d x d y$.

In general, we can show (see Problem 10.83) that


\begin{equation*}
\iint_{\Re} F(x, y) d x d y=\iint_{\Re^{\prime}} F\{f(u, v), g(u, v)\}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v \tag{3}
\end{equation*}


(b) $\iint_{\Re} d x d y$ and $\iint_{\Re^{\prime}}\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v$ represent the area of region $\mathrm{R}$, the first expressed in rectangular coordinates, the second in curvilinear coordinates. See Page 225, and the introduction of the differential element of surface area for an alternative to (a).

10.33. Let $\mathbf{F}=\frac{-y \mathbf{i}+x \mathbf{j}}{x^{2}+y^{2}}$. (a) Calculate $\nabla \times \mathbf{F}$. (b) Evaluate $\oint \mathbf{F} \cdot d \mathbf{r}$ around any closed path and explain the results.

(a) $\nabla \times \mathbf{F}=\left|\begin{array}{ccc}\mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ \frac{-y}{x^{2}+y^{2}} & \frac{x}{x^{2}+y^{2}} & 0\end{array}\right|=\mathbf{0}$ in any region excluding $(0,0)$.

(b) $\oint \mathbf{F} \cdot d \mathbf{r}=\oint_{C} \frac{-y d x=x d y}{x^{2}+y^{2}}$. Let $\mathrm{x}=\rho \cos \phi, \mathrm{y}=\rho \sin \phi$, where $(\rho, \phi)$ are polar coordinates. Then

$$
d x=-\rho \sin \phi d \phi+d \rho \cos \phi, \quad d y=\rho \cos \phi d \phi+d \rho \sin \phi
$$

and so

$$
\frac{-y d x=x d y}{x^{2}+y^{2}}=d \phi=d\left(\operatorname{are} \tan \frac{y}{x}\right)
$$

For a closed curve $A B C D A$ [see Figure 10.22 (a)] surrounding the origin, $\phi=0$ at $A$ and $\phi=2 \pi$ after a complete circuit back to $A$. In this case the line integral equals $\int_{0}^{2 \pi}{ }_{0} d \phi=2 \pi$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-282(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-282}
\end{center}

(b)

Figure 10.22

For a closed curve PQRSP [see Figure 10.22(b)] not surrounding the origin, $\phi=\phi_{0}$ at $P$ and $\phi=\phi_{0}$ after a complete circuit back to $P$. In this case the line integral equals $\int_{\phi_{0}}^{\phi_{0}} d \phi=0$.

Since $\mathbf{F}=P \mathbf{i}+Q \mathbf{j}, \nabla \times \mathbf{F}=\mathbf{0}$ is equivalent to $\partial P / \partial y=\partial Q / \partial x$, the results would seem to contradict those of Problem 10.11. However, no contradiction exists, since $P=\frac{-y}{x^{2}+y^{2}}$ and $Q=\frac{x}{x^{2}+y^{2}}$ do not have continuous derivatives throughout any region including $(0,0)$, and this was assumed in Problem 10.11.

10.34. If div $\mathbf{A}$ denotes the divergence of a vector field $\mathbf{A}$ at a point $P$, show that

$$
\operatorname{div} \mathbf{A}=\lim _{\Delta V \rightarrow 0} \frac{\iint_{\Delta S} \mathbf{A} \cdot \mathbf{n} d S}{\Delta V}
$$

where $\Delta V$ is the volume enclosed by the surface $\Delta S$ and the limit is obtained by shrinking $\Delta V$ to the point $P$.

By the divergence theorem,

$$
\iiint_{\Delta V} \operatorname{div} \mathbf{A} d V=\iint_{\Delta S} \mathbf{A} \cdot \mathbf{n} d S
$$

By the mean value theorem for integrals, the left side can be written

$$
\overline{\operatorname{div} \mathbf{A}} \iiint_{\Delta V} d V=\overline{\operatorname{div} \mathbf{A}} \Delta V
$$

where $\overline{\operatorname{div} \mathbf{A}}$ is some value intermediate between the maximum and minimum of $\operatorname{div} \mathbf{A}$ throughout $\Delta V$. Then

$$
\operatorname{div} \mathbf{A}=\frac{\iint_{\Delta S} \mathbf{A} \cdot \mathbf{n} d S}{\Delta V}
$$

Taking the limit as $\Delta V \rightarrow 0$ such that $P$ is always interior to $\Delta V, \overline{\operatorname{div} \mathbf{A}}$ approaches the value $\operatorname{div} \mathbf{A}$ at point $P$ : hence,

$$
\operatorname{div} \mathrm{A}=\lim _{\Delta V \rightarrow 0} \frac{\iint_{\Delta S} \mathrm{~A} \cdot \mathrm{n} d S}{\Delta V}
$$

This result can be taken as a starting point for defining the divergence of $\mathbf{A}$, and from it all the properties may be derived, including proof of the divergence theorem. We can also use this to extend the concept of divergence to coordinate systems other than rectangular (see Page 170).

Physically, $\left(\iiint_{\Delta S} \mathrm{~A} \cdot \mathrm{n} d S\right) / \Delta V$ represents the flux or net outflow per unit volume of the vector $\mathbf{A}$ from the surface $\Delta S$. If div $\mathbf{A}$ is positive in the neighborhood of a point $P$, it means that the outflow from $P$ is positive, and we call $P$ a source. Similarly, if $\operatorname{div} \mathbf{A}$ is negative in the neighborhood of $P$, the outflow is really an inflow, and $P$ is called a sink. If in a region there are no sources or sinks, then $\operatorname{div} \mathbf{A}=0$, and we call $\mathbf{A}$ a solenoidal vector field.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Line Integrals}
10.35. Evaluate $\int_{(1,1)}^{(4,2)}(x+y) d x+(y-x) d y$ along (a) the parabola $y^{2}=x$, (b) a straight line, (c) straight lines from $(1,1)$ to $(1,2)$ and then to $(4,2)$, and (d) the curve $x=2 t^{2}+t+1, y=t^{2}+1$.\\
Ans.\\
(a) $34 / 3$\\
(b) 11 (c) 14 (d)\\
d) $32 / 3$

10.36. Evaluate $\oint(2 x-y+4) d x+(5 y+3 x-6) d y$ around a triangle in the $x y$ plane with vertices at $(0,0),(3,0)$, $(3,2)$ traversed in a counterclockwise direction.

Ans. 12

10.37. Evaluate the line integral in Problem 10.36 around a circle of radius 4 with center at $(0,0)$.

Ans. $64 \pi$

10.38. (a) If $\mathbf{F}=\left(x^{2}-y^{2}\right) \mathbf{i}+2 x y \mathbf{j}$, evaluate $\int_{C} \mathbf{F} \cdot d \mathbf{r}$ along the curve $C$ in the $x y$ plane given by $y=x^{2}-x$ from the point $(1,0)$ to $(2,2)$. (b) Interpret physically the result obtained.

Ans. (a) $124 / 15$

10.39. Evaluate $\int_{C}(2 x+y) d s$, where $C$ is the curve in the $x y$ plane given by $x^{2}+y^{2}=25$ and $s$ is the are length parameter, from the point $(3,4)$ to $(4,3)$ along the shortest path.

Ans. 15

10.40. If $\mathbf{F}=(3 x-2 y) \mathbf{i}+(y+2 z) \mathbf{j}-x^{2} \mathbf{k}$, evaluate $\int_{C} \mathbf{F} \cdot d \mathbf{r}$ from $(0,0,0)$ to $(1,1,1)$, where $C$ is a path consisting of (a) the curve $x=t, y=t^{2}, z=t^{3}$, (b) a straight line joining these points, (c) the straight lines from $(0,0,0)$ to $(0,1,0)$, then to $(0,1,1)$ and then to $(1,1,1)$, and (d) the curve $x=z^{2}, z=y^{2}$.\\
Ans.\\
(a) $23 / 15$\\
(b) $5 / 3$\\
(c) 0 (d)\\
(d) $13 / 15$

10.41. If $\mathbf{T}$ is the unit tangent vector to a curve $C$ (plane or space curve) and $\mathbf{F}$ is a given force field, prove that under appropriate conditions $\int_{C} \mathbf{F} \cdot d \mathbf{r}=\int_{C} \mathbf{F} \cdot \mathbf{T} d s$, where $s$ is the arc length parameter. Interpret the result physically and geometrically.

\section*{Green's theorem in the plane, independence of the path}
10.42. Verify Green's theorem in the plane for $\oint_{C}\left(x^{2}-x y^{3}\right) d x+\left(y^{2}-2 x y\right) d y$, where $C$ is a square with vertices at $(0,0),(2,0),(2,2),(0,2)$ and counterclockwise orientation.

Ans. Common value $=8$

10.43. Evaluate the line integrals of (a) Problem 10.36 and (b) Problem 10.37 by Green's theorem.

10.44. (a) Let $C$ be any simple closed curve bounding a region having area $A$. Prove that if $a_{1}, a_{2}, a_{3}, b_{1}, b_{2}, b_{3}$ are constants, $\oint_{C}\left(a_{1} x+a_{2} y+a_{3}\right) d x+\left(b_{1} x+b_{2} y+b_{3}\right) d y=\left(b_{1}-a_{2}\right) A$ (b) Under what conditions will the line integral around any path $\mathrm{C}$ be zero?

Ans. (b) $a_{2}=b_{1}$

10.45. Find the area bounded by the hypocycloid $x^{2 / 3}+y^{2 / 3}=a^{2 / 3}$. (Hint: Parametric equations are $x=a \cos ^{3} t$, $y=a \sin ^{3} t, 0 \leqq t \leqq 2 \pi$.

\section*{Ans. $3 \pi a^{2} / 8$}
10.46. If $x=\rho \cos \phi, y=\rho \sin \phi$, prove that $\frac{1}{2} \oint x d y-y d x=\frac{1}{2} \int \rho^{2} d \phi$ and interpret.

10.47. Verify Green's theorem in the plane for $\oint_{C}\left(x^{3}-x^{2} y\right) d x+x y^{2} d y$, where $C$ is the boundary of the region enclosed by the circles $x^{2}+y^{2}=4$ and $x^{2}+y^{2}=16$.

Ans. Common value $=120 \pi$

10.48. (a) Prove that $\int_{(1,0)}^{(2,1)}\left(2 x y-y^{4}+3\right) d x+\left(x^{2}-4 x y^{3}\right) d y$ is independent of the path joining $(1,0)$ and $(2,1)$.

(b) Evaluate the integral in (a).

Ans. (b) 5

10.49. Evaluate $\int_{C}\left(2 x y^{3}-y^{2} \cos x\right) d x+\left(1-2 y \sin x+3 x^{2} y^{2}\right) d y$ along the parabola $2 x=\pi y^{2}$ from $(0,0)$ to $(\pi / 2,1)$.

Ans. $\pi^{2} / 4$

10.50. Evaluate the line integral in Problem 10.49 around a parallelogram with vertices at $(0,0),(3,0),(5,2),(2,2)$.

Ans. 0

10.51. (a) Prove that $G=\left(2 x^{2}+x y-2 y^{2}\right) d x+\left(3 x^{2}+2 x y\right) d y$ is not an exact differential. (b) Prove that $e^{y / x} G / x$ is an exact differential of $\phi$ and find $\phi$. (c) Find a solution of the differential equation $\left(2 x^{2}+x y-2 y^{2}\right) d x+\left(3 x^{2}+2 x y\right) d y=0$.

Ans. (b) $\phi=e^{y / x}\left(x^{2}+2 x y\right)+c$ (c) $x^{2}+2 x y+c e^{-y / x}=0$

\section*{Surface integrals}
10.52. (a) Evaluate $\iint_{S}\left(x^{2}+y^{2}\right) d S$, where $S$ is the surface of the cone $z^{2}=3\left(x^{2}+y^{2}\right)$ bounded by $z=0$ and $z=3$.

(b) Interpret physically the result in (a).

Ans. (a) $9 \pi$

10.53. Determine the surface area of the plane $2 x+y+2 z=16$ cut off by (a) $x=0, y=0, x=2, y=3$ and (b) $x=0, y=0$, and $x^{2}+y^{2}=64$.

Ans. (a) 9 (b) $24 \pi$

10.54. Find the surface area of the paraboloid $2 z=x^{2}+y^{2}$ which is outside the cone $z=\sqrt{x^{2}+y^{2}}$.

Ans. $\frac{2}{3} \pi(5 \sqrt{5}-1)$

10.55. Find the area of the surface of the cone $z^{2}=3\left(x^{2}+y^{2}\right)$ cut out by the paraboloid $z=x^{2}+y y^{2}$.

Ans. $6 \pi$

10.56. Find the surface area of the region common to the intersecting cylinders $x^{2}+y^{2}=a^{2}$ and $x^{2}+z^{2}=a^{2}$.

Ans. $16 a^{2}$

10.57. (a) Obtain the surface area of the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ contained within the cone $z \tan \alpha=\sqrt{x^{2}+y^{2}}$, $0<\alpha<\pi / 2$. (b) Use the result in (a) to find the surface area of a hemisphere. (c) Explain why formally placing $\alpha=\pi$ in the result of (a) yields the total surface area of a sphere.

Ans. (a) $2 \pi a^{2}\left(1-\cos \alpha\right.$ ) (b) $2 \pi a^{2}$ (consider the limit as $\alpha \rightarrow \pi / 2$ )

10.58. Determine the moment of inertia of the surface of a sphere of radius $a$ about a point on the surface. Assume a constant density $\sigma$.

Ans. $2 M a^{2}$, where mass $M=4 \pi a^{2} \sigma$

10.59. (a) Find the centroid of the surface of the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ contained within the cone $z \tan$ $\alpha=\sqrt{x^{2}+y^{2}}, 0<\alpha<\pi / 2$. (b) From the result in (a) obtain the centroid of the surface of a hemisphere.\\
Ans. (a) $1 / 2 a(1+\cos \alpha)$\\
(b) $a / 2$

\section*{The divergence theorem}
10.60. Verify the divergence theorem for $\mathbf{A}=(2 x y+z) \mathbf{i}+y^{2} \mathbf{j}-(x+3 y) \mathbf{k}$ taken over the region bounded by $2 x+2 y$ $+z=6, x=0, y=0, z=0$.

Ans. common value $=27$

10.61. Evaluate $\iint_{S} \mathbf{F} \cdot n d S$. where $\mathbf{F}=\left(z^{2}-x\right) \mathbf{i}-x y \mathbf{j}+3 z \mathbf{k}$ and $S$ is the surface of the region bounded by $z=4-y^{2}$, $x=0, x=3$ and the $x y$ plane.

Ans. 16

10.62. Evaluate $\iint_{S} \mathbf{A} \cdot n d S$. where $\mathbf{A}=(2 x+3 z) \mathbf{i}-(x z+y) \mathbf{j}+\left(y^{2}+2 z\right) \mathbf{k}$ and $S$ is the surface of the sphere having center at $(3,-1,2)$ and radius 3 .

Ans. $108 \pi$

10.63. Determine the value of $\iint_{S} x d y d z+y d z d x+z d x d y$, where $S$ is the surface of the region bounded by the cylinder $x^{2}+y^{2}=9$ and the planes $z=0$ and $z=3$, (a) by using the divergence theorem and (b) directly.

Ans. $81 \pi$

10.64. Evaluate $\iint_{S} 4 x z d y d z-y^{2} d z d x+y z d x d y$, where $S$ is the surface of the cube bounded by $x=0, y=0$, $z=0, x=1, y=1, z=1$, (a) directly and (b) By Green's theorem in space (divergence theorem).

Ans. 3/2

10.65. Prove that $\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S$ for any closed surface $S$.

10.66. Prove that $\iint_{S} \mathbf{n} d S=0$. where $n$ is the outward drawn normal to any closed surface $S$. (Hint: Let $\mathbf{A}=\Theta \mathbf{c}$, where $\mathbf{c}$ is an arbitrary vector constant.) Express the divergence theorem in this special case. Use the arbitrary property of $\mathbf{c}$.

10.67. If $\mathbf{n}$ is the unit outward drawn normal to any closed surface $S$ bounding the region $V$, prove that

$$
\iiint_{V} \operatorname{div} \mathbf{n} d V=S
$$

\section*{Stokes's theorem}
10.68. Verify Stokes's theorem for $\mathbf{A}=2 y \mathbf{i}+3 x \mathbf{j}-z^{2} \mathbf{k}$, where $S$ is the upper half surface of the sphere $x^{2}+y^{2}+z^{2}=$ 9 and $C$ is its boundary.

Ans. Common value $=9 \pi$

10.69. Verify Stokes's theorem for $\mathbf{A}=(y+z) \mathbf{i}-x z \mathbf{j}+y^{2} \mathbf{k}$, where $S$ is the surface of the region in the first octant bounded by $2 x+z=6$ and $y=2$ which is not included in the (a) $x y$ plane, (b) plane $y=2$, and (c) plane $2 x+$ $z=6$ and $C$ is the corresponding boundary.

Ans. The common value is (a) -6 , (b) -9 , and (c) -18 .

10.70. Evaluate $\iint_{S}(\nabla \times \mathbf{A}) \cdot \mathbf{n} d S$. where $\mathbf{A}=(x-z) \mathbf{i}+\left(x^{3}+y z\right) \mathbf{j}-3 x y^{2} \mathbf{k}$ and $S$ is the surface of the cone $z=2-\sqrt{x^{2}+y^{2}}$ above the $x y$ plane.

Ans. $12 \pi$

10.71. If $V$ is a region bounded by a closed surface $S$ and $\mathbf{B}=\nabla \times \mathbf{A}$, prove that $\iint_{S} \mathbf{B} \cdot \mathbf{n} d S=0$

10.72. (a) Prove that $\mathbf{F}=(2 x y+3) \mathbf{i}+\left(x^{2}-4 z\right) \mathbf{j}-4 y \mathbf{k}$ is a conservative force field. (b) Find $\phi$ such that $\mathbf{F}=\nabla \phi$.

(c) Evaluate $\int_{C} \mathbf{F} \cdot d \mathbf{r}$, where $\mathrm{C}$ is any path from $(3,-1,2)$ to $(2,1,-1)$.

Ans. (b) $\phi=x^{2} y-4 y z+3 x+$ constant (c) 6

10.73. Let $C$ be any path joining any point on the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ to any point on the sphere $x^{2}+y^{2}+z^{2}=$ $b^{2}$. Show that if $\mathbf{F}=5 r^{3} \mathbf{r}$, where $\mathbf{r}=x \mathbf{i}+y \mathbf{j}+z \mathbf{k}$, then $\int_{C} \mathbf{F} \cdot d \mathbf{r}=b^{5}-a^{5}$.

10.74. In Problem 10.73 evaluate $\int_{C} \mathbf{F} \cdot d \mathbf{r}$ is $\mathbf{F}=f(\mathrm{r}) \mathbf{r}$, where $f(\mathrm{r})$ is assumed to be continuous. Ans. $\int_{a}^{b} r f(r) d r$

10.75. Determine whether there is a function $\phi$ such that $\mathbf{F}=\nabla \phi$, where (a) $\mathbf{F}=(x z-y) \mathbf{i}+\left(x^{2} y+z^{3}\right) \mathbf{j}+\left(3 x z^{2}-x y\right)$ $\mathbf{k}$, and (b) $\mathbf{F}=2 \mathrm{xe}^{-\mathrm{y}} \mathbf{i}+\left(\cos \mathrm{z}-\mathrm{x}^{2} \mathrm{e}^{-\mathrm{y}}\right) \mathbf{j}-\mathrm{y} \sin \mathrm{zk}$. If so, find it.

Ans. (a) $\phi$ does not exist (b) $\phi=x^{2} e^{-y}+y \cos z+$ constant

10.76. Solve the differential equation $\left(z^{3}-4 x y\right) d x+\left(6 y-2 x^{2}\right) d y+\left(3 x z^{2}+1\right) d z=0$.

Ans. $x z^{3}-2 x^{2} y+3 y^{2}+z=$ constant

\section*{Miscellaneous problems}
10.77. Prove that a necessary and sufficient condition that $\oint_{C} \frac{\partial U}{\partial x} d y-\frac{\partial U}{\partial y} d x$ be zero around every simple closed path $C$ in a region $\Re$ (where $U$ is continuous and has continuous partial derivatives of order two, at least) is that $\frac{\partial^{2} U}{\partial x^{2}}+\frac{\partial^{2} U}{\partial y^{2}}=0$.

10.78. Verify Green's theorem for a multiply connected region containing two "holes" (see Problem 10.10).

10.79. If $P d x+Q d y$ is not an exact differential but $\mu(P d x+Q d y)$ is an exact differential where $\mu$ is some function of $x$ and $y$, then $\mu$ is called an integrating factor. (a) Prove that if $F$ and $G$ are functions of $x$ alone then $(F y+G) d x+d y$ has an integrating factor $\mu$ which is a function of $x$ alone, and find $\mu$. What must be assumed about $F$ and $G$ ? (b) Use (a) to find solutions of the differential equation $x y^{\prime}=2 x+3 y$.

Ans. (a) $\mu=e^{\int F(x) d x}$ (b) $y=c x^{3}-x$, where $c$ is any constant

10.80. Find the surface area of the sphere $x^{2}+y^{2}+(z-a)^{2}=a^{2}$ contained within the paraboloid $z=x^{2}+y^{2}$.

Ans. $2 \pi a$

10.81. If $f(\mathrm{r})$ is a continuously differentiable function of $r=\sqrt{x^{2}+y^{2}+z^{2}}$. prove that

$$
\iint_{S} f(r) \mathbf{n} d S=\iiint_{V} \frac{f^{\prime}(r)}{r} \mathbf{r} d V
$$

10.82. Prove that $\iint \nabla \times(\phi \mathbf{n}) d S=\mathbf{0}$, where $\phi$ is any continuously differentiable scalar function of position and $\mathbf{n}$ is a unit outward drawn normal to a closed surface $S$. (See Problem 10.66.)

10.83. Establish Equation (3). Problem 10.32, by using Green's theorem in the plane. [Hint: Let the closed region $\Re$ in the $x y$ plane have boundary $C$ and suppose that under the transformation $x=f(u, v), y=g(u, v)$, these are transformed into $\Re^{\prime}$ and $C^{\prime}$ in the $u v$ plane, respectively.] First prove that $\iint_{\Re} F(x, y) d x d y=\int_{C} Q(x, y) d y$ where $\partial Q / \partial y=F(x, y)$. Then show that, apart from sign, this last integral is equal to $\int_{C} Q[f(u, v), g(u, v)]\left|\frac{\partial g}{\partial u} d u+\frac{\partial g}{\partial v} d v\right|$. Finally, use Green's theorem to transform this into $\iint_{\Re^{\prime}} F[f(u, v), g(u, v)]\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v$.

10.84. If $x=f(u, v, w), y=g(u, v, w), z=h(u, v, w)$ defines a transformation which maps a region $\Re$ of $x y z$ space into a region $\Re^{\prime}$ of $u v w$ space, prove, using Stokes's theorem, that

$$
\iiint_{\Re} F(x, y, z) d x d y d z=\iiint_{\Re^{\prime}} G(u, v, w)\left|\frac{\partial(x, y, z)}{\partial(u, v, w)}\right| d u d v d w
$$

where $G(u, v, w) \equiv F[f(u, v, w), g(u, v, w), h(u, v, w)]$. State sufficient conditions under which the result is valid. See Problem 10.83. Alternatively, employ the differential element of volume $d V=\frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial v} \times \frac{\partial \mathbf{r}}{\partial w} d u d v d w$ (recall the geometric meaning).

10.85. (a) Show that, in general, the equation $\mathbf{r}=\mathbf{r}(u, v)$ geometrically represents a surface. (b) Discuss the geometric significance of $u=c_{1}, v=c_{2}$, where $c_{1}$ and $c_{2}$ are constants. (c) Prove that the element of arc length on this surface is given by

$$
d s^{2}=E d u^{2}+2 F d u d v+G d v^{2}
$$

where $E=\frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial u}, \quad F=\frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial v}$, and $G=\frac{\partial \mathbf{r}}{\partial v} \cdot \frac{\partial \mathbf{r}}{\partial v}$.

10.86. (a) Referring to Problem 10.85, show that the element of surface area is given by $d S=\sqrt{E G-F^{2}} d u d v$. (b) Deduce from (a) that the area of a surface $\mathrm{r}=\mathrm{r}(\mathrm{u}, v)$ is $\iint_{S} \sqrt{E G-F^{2}} d u d v$. [Hint: Use the fact that $\left|\frac{\partial \mathrm{r}}{\partial u} \times \frac{\partial \mathrm{r}}{\partial v}\right|=\sqrt{\left(\frac{\partial \mathrm{r}}{\partial u} \times \frac{\partial \mathrm{r}}{\partial v}\right) \cdot\left(\frac{\partial \mathrm{r}}{\partial u}\right) \times\left(\frac{\partial \mathrm{r}}{\partial v}\right)}$ and then use the identity $(\mathbf{A} \times \mathbf{B}) \cdot(\mathbf{C} \times \mathbf{D})=(\mathbf{A} \cdot \mathbf{C})(\mathbf{B} \cdot \mathbf{D})-$ $(\mathbf{A} \cdot \mathbf{D})(\mathbf{B} \cdot \mathbf{C})$.

10.87. (a) Prove that $\mathbf{r}=a \sin u \cos v \mathbf{i}+a \sin u \sin v \mathbf{j}+a \cos u, 0 \leqq u \leqq \pi, 0 \leqq v<2 \pi$ represents a sphere of radius $a$. (b) Use Problem 10.86 to show that the surface area of this sphere is $4 \pi a^{2}$.

10.88. Use the result of Problem 10.34 to obtain div $\mathbf{A}$ in (a) cylindrical and (b) spherical coordinates. See Page 173 .

This page intentionally left blank

\section*{CHAPTER 11}
\section*{Infinite Series}
The early developers of the calculus, including Newton and Leibniz, were well aware of the importance of infinite series. The values of many functions such as sine and cosine were geometrically obtainable only in special cases. Infinite series provided a way of developing extensive tables of values for them.

This chapter begins with a statement of what is meant by infinite series, then the question of when these sums can be assigned values is addressed. Much information can be obtained by exploring infinite sums of constant terms; however, the eventual objective in analysis is to introduce series that depend on variables. This presents the possibility of representing functions by series. Afterward, the question of how continuity, differentiability, and integrability play a role can be examined.

The question of dividing a line segment into infinitesimal parts has stimulated the imaginations of philosophers for a very long time. In a corruption of a paradox introduced by Zeno of Elea (in the fifth century в.с.) a dimensionless frog sits on the end of a one-dimensional log of unit length. The frog jumps halfway, and then halfway and halfway ad infinitum. The question is whether the frog ever reaches the other end. Mathematically, an unending sum,

$$
\frac{1}{2}+\frac{1}{4}+\cdots+\frac{1}{2^{n}}+\cdots
$$

is suggested. Common sense tells us that the sum must approach 1 even though that value is never attained. We can form sequences of partial sums

$$
S_{1}=\frac{1}{2}, S_{2}=\frac{1}{2}+\frac{1}{4}, \ldots, S_{n}=\frac{1}{2}+\frac{1}{4}+\cdots+\frac{1}{2^{n}} S_{n+1} \cdots
$$

and then examine the limit. This returns us to Chapter 2 and the modern manner of thinking about the infinitesimal.

In this chapter, consideration of such sums launches us on the road to the theory of infinite series.

\section*{Definitions of Infinite Series and Their Convergence and Divergence}
Definition The sum


\begin{equation*}
S=\sum_{n=1}^{\infty} u_{n}=u_{1}+u_{2}+\cdots+u_{n}+\cdots \tag{1}
\end{equation*}


is an infinite series. Its value, if one exists, is the limit of the sequence of partial sums $\left\{S_{n}\right\}$


\begin{equation*}
S=\lim _{n \rightarrow \infty} S_{n} \tag{2}
\end{equation*}


If there is a unique value, the series is said to converge to that sum $S$. If there is not a unique sum, the series is said to diverge.

Sometimes the character of a series is obvious. For example, the series $\sum_{n=1}^{\infty} \frac{1}{2^{n}}$ generated by the frog on the $\log$ surely converges, while $\sum_{n=1}^{\infty} n$ is divergent. On the other hand, the variable series $1-x+x^{2}-x^{3}+x^{4}-x^{5}+\cdots$\\
raises questions.

This series may be obtained by carrying out the division $\frac{1}{1-x}$. If $-1<x<1$, the sum $S_{n}$ yields an approximation to $\frac{1}{1-x}$ and Equation (2) is the exact value. The indecision arises for $x=-1$. Some very great mathematicians, including Leonhard Euler, thought that $S$ should be equal to $\frac{1}{2}$, as is obtained by substituting -1 into $\frac{1}{1-x}$. The problem with this conclusion arises with examination of $1-1+1-1+1-1+\cdots$ and observation that appropriate associations can produce values of 1 or 0 . Imposition of the condition of uniqueness for convergence puts this series in the category of divergent and eliminates such possibility of ambiguity in other cases.

\section*{Fundamental Facts Concerning Infinite Series}
\begin{enumerate}
  \item If $\Sigma u_{n}$ converges, then $\lim _{n \rightarrow \infty} u_{n}=0$ (see Problem 2.26). The converse, however, is not necessarily true; i.e., if $\lim _{n \rightarrow \infty} u_{n}=0, \Sigma u_{n}$ may or may not converge. It follows that if the $n$th term of a series does not approach zero, the series is divergent.

  \item Multiplication of each term of a series by a constant different from zero does not affect the convergence or divergence.

  \item Removal (or addition) of a finite number of terms from (or to) a series does not affect the convergence or divergence.

\end{enumerate}

\section*{Special Series}
\begin{enumerate}
  \item Geometric series $\sum_{n=1}^{\infty} a r^{n-1}=a+a r+a r^{2}+\cdots$, where $a$ and $r$ are constants, converges to $S=\frac{a}{1-r}$ if $|r|<1$ and diverges if $|r| \geqq 1$. The sum of the first $n$ terms is $S_{n}=\frac{a\left(1-r^{n}\right)}{1-r}$ (see Problem 2.25).

  \item The $p$ series $\sum_{n=1}^{\infty} \frac{1}{n^{p}}=\frac{1}{1^{p}}+\frac{1}{2^{p}}+\frac{1}{3^{p}}+\cdots$, where $p$ is a constant, converges for $p>1$ and diverges for $p \leqq 1$. The series with $p=1$ is called the harmonic series.

\end{enumerate}

\section*{Tests for Convergence and Divergence of Series of Constants}
More often than not, exact values of infinite series cannot be obtained. Thus, the search turns toward information about the series. In particular, its convergence or divergence comes into question. The following tests aid in discovering this information.

\begin{enumerate}
  \item The comparison test for series of nonnegative terms.
\end{enumerate}

(a) Convergence. Let $v_{n} \geqq 0$ for all $n>N$ and suppose that $\Sigma v_{n}$ converges. Then if $0 \leqq u_{n} \leqq v_{n}$ for all $n>N, \Sigma u_{n}$ also converges. Note that $n>N$ means from some term onward. Often, $N=1$.

EXAMPLE. Since $\frac{1}{2^{n}+1} \leqq \frac{1}{2^{n}}$ and $\sum \frac{1}{2^{n}}$ converges, $\sum \frac{1}{2^{n+1}}$ also converges.\\
(b) Divergence. Let $v_{n} \geqq 0$ for all $n>N$ and suppose that $\Sigma v_{n}$ diverges. Then if $u_{n} \geqq v_{n}$ for all $n>N, \Sigma u_{n}$ also diverges.

EXAMPLE. Since $\frac{1}{\ln n}>\frac{1}{n}$ and $\sum_{n=2}^{\infty} \frac{1}{n}$ diverges, $\sum_{n=2}^{\infty} \frac{1}{\ln n}$ also diverges.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The limit-comparison or quotient test for series of nonnegative terms. (a) If $u_{n} \geqq 0$ and $v_{n} \geqq 0$ and if $\lim _{n \rightarrow \infty} \frac{u_{n}}{v_{n}}=A \neq 0$ or $\infty$, then $\Sigma u_{n}$ and $\Sigma v_{n}$ either both converge or both di-\\
verge.
\end{enumerate}

(b) If $A=0$ in (a) and $\Sigma v_{n}$ converges, then $\Sigma u_{n}$ converges.

(c) If $A=\infty$ in (a) and $\Sigma v_{n}$ diverges, then $\Sigma u_{n}$ diverges.

This test is related to the comparison test and is often a very useful alternative to it. In particular, taking $v_{n}=1 / n^{p}$, we have the following theorems from known facts about the $p$ series.

Theorem 1 Let $\lim _{n \rightarrow \infty} n^{p} u_{n}=A$. Then

(i) $\Sigma u_{n}$ conyerges if $p>1$ and $A$ is finite.

(ii) $\Sigma u_{n}$ diverges if $p \leqq 1$ and $A \neq 0$ ( $A$ may be infinite).

EXAMPLES 1. $\sum \frac{n}{4 n^{3}-2}$ converges since $\lim _{n \rightarrow \infty} n^{2} \cdot \frac{n}{4 n^{3}-2}=\frac{1}{4}$.\\
2. $\quad \sum \frac{\text { In } n}{\sqrt{n+1}}$ diverges since $\lim _{n \rightarrow \infty} n^{1 / 2} \cdot \frac{\operatorname{In} n}{(n+1)^{1 / 2}}=\infty$.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Integral test for series of non-negative terms.
\end{enumerate}

If $f(x)$ is positive, continuous, and monotonic decreasing for $x \geq N$ and is such that $f(n)=u_{n}, n=$ $N, N+1, N+2, \ldots$, then $\Sigma u_{n}$ converges or diverges according as $\int_{N}^{\infty} f(x) d x=\lim _{M \rightarrow \infty} \int_{n}^{M} f(x) d x$ converges or diverges. In particular, we may have $N=1$, as is often true in practice.

This theorem borrows from Chapter 12, since the integral has an unbounded upper limit. (It is an improper integral. The convergence or divergence of these integrals is defined in much the same way as for infinite series.)

EXAMPLE: $\sum_{n=1}^{\infty} \frac{1}{n^{2}}$ converges since $\lim _{M \rightarrow \infty} \int_{1}^{M} \frac{d x}{x^{2}}=\lim _{M \rightarrow \infty}\left(1-\frac{1}{M}\right)$ exists.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Alternating series test. An alternating series is one whose successive terms are alternately positive and negative.
\end{enumerate}

An alternating series converges if the following two conditions are satisfied (see Problem 11.15).

(a) $\left|u_{n+1}\right| \leq\left|u_{n}\right|$ for $n \geq N$. (Since a fixed number of terms does not affect the convergence or divergence of a series, $N$ may be any positive integer. Frequently it is chosen to be 1.)

(b) $\lim _{n \rightarrow \infty} u_{n}=0\left(\right.$ or $\left.\lim _{n \rightarrow \infty}\left|u_{n}\right|=0\right)$

EXAMPLE. For the series $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\cdots=\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}$, we have $u_{n}=\frac{(-1)^{n-1}}{n},\left|u_{n}\right|=\frac{1}{n}$, $\left|u_{n+1}\right|=\frac{1}{n+1}$. Then for $n \geqq 1,\left|u_{n+1}\right| \leqq\left|u_{n}\right|$. Also $\lim _{n \rightarrow \infty}\left|u_{n}\right|=0$. Hence, the series converges.

Theorem 2 The numerical error made in stopping at any particular term of a convergent alternating series which satisfies conditions (a) and (b) is less than the absolute value of the next term.

EXAMPLE. If we stop at the fourth term of the series $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\cdots$, the error made is less than $\frac{1}{5}=0.2$.

\begin{enumerate}
  \setcounter{enumi}{4}
  \item Absolute and conditional convergence. The series $\Sigma u_{n}$ is called absolutely convergent if $\Sigma\left|u_{n}\right|$ converges. If $\Sigma u_{n}$ converges but $\Sigma\left|u_{n}\right|$ diverges, then $\Sigma u_{n}$ is called conditionally convergent.
\end{enumerate}

Theorem 3 If $\Sigma\left|u_{n}\right|$ converges, then $\Sigma u_{n}$ converges. In words, an absolutely convergent series is convergent (see Problem 11.17).

EXAMPLE 1. $\frac{1}{1^{2}}+\frac{1}{2^{2}}-\frac{1}{3^{2}}-\frac{1}{4^{2}}+\frac{1}{5^{2}}+\frac{1}{6^{2}}-\cdots$ is absolutely convergent and thus convergent, since the series of absolute values $\frac{1}{1^{2}}+\frac{1}{2^{2}}+\frac{1}{3^{2}}+\frac{1}{4^{2}}+\cdots$ converges.

EXAMPLE 2. $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$ converges, but $1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots$ diverges. Thus, $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$ is conditionally convergent.

Any of the tests used for series with nonnegative terms can be used to test for absolute convergence. Also, tests that compare successive terms are common. Tests 6,8 , and 9 are of this type.

\begin{enumerate}
  \setcounter{enumi}{5}
  \item Ratio test. Let $\lim _{n \rightarrow \infty}\left|\frac{u_{n-1}}{u_{n}}\right|=L$. Then the series $\Sigma u_{n}$
\end{enumerate}

(a) converges (absolutely) if $L<1$.

(b) diverges if $L>1$.

If $L=1$ the test fails.

\begin{enumerate}
  \setcounter{enumi}{6}
  \item The $\boldsymbol{n}$ th root test. Let $\lim _{n \rightarrow \infty} \sqrt[n]{\left|u_{n}\right|}=L$. Then the series $\Sigma u_{n}$
\end{enumerate}

(a) converges (absolutely) if $L<1$

(b) diverges if $L>1$.

If $L=1$ the test fails.

\begin{enumerate}
  \setcounter{enumi}{7}
  \item Raabe's test. Let $\lim _{n \rightarrow \infty} n\left(1-\left|\frac{u_{n}+1}{u_{n}}\right|\right)=L$. Then the series $\Sigma u_{n}$
\end{enumerate}

(a) converges (absolutely) if $L>1$.

(b) diverges or converges conditionally if $L<1$.

If $L=1$ the test fails.

This test is often used when the ratio tests fails.

\begin{enumerate}
  \setcounter{enumi}{8}
  \item Gauss's test. If $\left|\frac{u_{n}+1}{u_{n}}\right|=1-\frac{L}{n}+\frac{c_{n}}{n^{q}}$, where $\left|c_{n}\right|<P$ for all $n>N$ the sequence $c_{n}$ is bounded, then the
\end{enumerate}

(a) converges (absolutely) if $L>1$.

(b) diverges or converges conditionally if $L \leqq 1$.

This test is often used when Raabe's test fails.

\section*{Theorems on Absolutely Convergent Series}
Theorem 4 (Rearrangement of terms.) The terms of an absolutely convergent series can be rearranged in any order, and all such rearranged series will converge to the same sum. However, if the terms of a conditionally convergent series are suitably rearranged, the resulting series may diverge or converge to any desired sum (see Problem 11.80).

Theorem 5 (Sums, differences, and products.) The sum, difference, and product of two absolutely convergent series is absolutely convergent. The operations can be performed as for finite series.

\section*{Infinite Sequences and Series of Functions, Uniform Convergence}
We opened this chapter with the thought that functions could be expressed in series form. Such representation is illustrated by

where

$$
\sin x=x-\frac{x^{3}}{3 !}+\frac{x^{3}}{5 !}-+\cdots+(-1)^{n-1} \frac{x^{2 n-1}}{(2 n-) !}+\cdots
$$

$$
\sin x=\lim _{n \rightarrow \infty} S_{n} . \quad \text { with } \quad S_{1}=x, S_{2}=x-\frac{x^{3}}{3 !}, \cdots S_{n}=\sum_{k=1}^{n}(-1)^{k-1} \frac{x^{2 k-1}}{(2 k-1) !}
$$

Observe that until this section the sequences and series depended on one element, $n$. Now there is variation with respect to $x$ as well. This complexity requires the introduction of a new concept called uniform convergence, which, in turn, is fundamental in exploring the continuity, differentiation, and integrability of series.

Let $\left\{u_{n}(x)\right\}, n=1,2,3, \ldots$ be a sequence of functions defined in $[a, b]$. The sequence is said to converge to $F(x)$, or to have the limit $F(x)$ in $[a, b]$, if for each $\epsilon>0$ and each $x$ in $[a, b]$ we can find $N>0$ such that $\left|u_{n}(x)-F(x)\right|<\epsilon$ for all $n>N$. In such case we write $\lim _{n \rightarrow \infty} u_{n}(x)=F(x)$. The number $N$ may depend on $x$ as well as $\epsilon$. If it depends only on $\epsilon$ and not on $x$, the sequence is said to converge to $F(x)$ uniformly in $[a, b]$ or to be uniformly convergent in $[a, b]$.

The infinite series of functions


\begin{equation*}
\sum_{n}^{\infty} u_{n}(x)=u_{1}(x)+u_{2}(x)+u_{3}(x)+\cdots \tag{3}
\end{equation*}


is said to be convergent in $[a, b]$ if the sequence of partial sums $\left\{S_{n}(x)\right\}, n=1,2,3, \ldots$, where $S_{n}(x)=u_{1}(x)$ $+u_{2}(x)+\cdots+u_{n}(x)$, is convergent in $[a, b]$. In such case we write $\lim _{n \rightarrow \infty} S_{n}(x)=S(x)$ and call $S(x)$ the sum of the series.

It follows that $\Sigma u_{n}(x)$ converges to $S(x)$ in $[a, b]$ if for each $\epsilon>0$ and each $x$ in $[a, b]$ we can find $N>0$ such that $\left|S_{n}(x)-S(x)\right|<\epsilon$ for all $n>N$. If $N$ depends only on $\epsilon$ and not on $x$, the series is called uniformly convergent in $[a, b]$.

Since $S(x)-S_{n}(x)=R_{n}(x)$, the remainder after $n$ terms, we can equivalently say that $\Sigma u_{n}(x)$ is uniformly convergent in $[a, b]$ if for each $\epsilon>0$ we can find $N$ depending on $\epsilon$ but not on $x$ such that $\left|R_{n}(x)\right|<\epsilon$ for all $n>N$ and all $x$ in $[a, b]$.

These definitions can be modified to include other intervals besides $a \leq x \leq b$, such as $a<x<b$, and so on.

The domain of convergence (absolute or uniform) of a series is the set of values of $x$ for which the series of functions converges (absolutely or uniformly).

EXAMPLE 1. Suppose $u_{n}=x^{n} / n$ and $-1 / 2 \leqq x \leqq 1$. Now think of the constant function $F(x)=0$ on this interval. For any $\epsilon>0$ and any $x$ in the interval, there is $N$ such that for all $n>N\left|u_{n}-F(x)\right|<\epsilon$, i.e., $\left|x^{n} / n\right|<\epsilon$. Since the limit does not depend on $x$, the sequence is uniformly convergent.

EXAMPLE 2. If $u_{n}=x^{n}$ and $0 \leqq x \leqq 1$, the sequence is not uniformly convergent because [think of the function $F(x)=0,0 \leqq x<1, F(1)=1$ ]

$$
\left|x^{n}-0\right|<\varepsilon \text { when } x^{n}<\varepsilon
$$

thus

$$
n \ln x<\ln \varepsilon
$$

On the interval $0 \leqq x<1$, and for $0<\epsilon<1$, both members of the inequality are negative; therefore, $n>\frac{\ln \varepsilon}{\ln x}$. Since $\frac{\ln \varepsilon}{\ln x}=\frac{\ln 1-\ln \varepsilon}{\ln 1-\ln x}=\frac{\ln (/ \varepsilon)}{\ln (1 / x)}$, it follows that we must choose $N$ such that

$$
n>N>\frac{\ln 1 / \varepsilon}{\ln 1 / x}
$$

From this expression we see that $\epsilon \rightarrow 0$, then $\ln \ln \frac{1}{\varepsilon} \rightarrow \infty$, and also as $x \rightarrow 1$ from the left $\ln \frac{1}{x} \rightarrow 0$ from the right; thus, in either case, $N$ must increase without bound. This dependency on both $\epsilon$ and $x$ demonstrates that the sequence is not uniformly convergent. For a pictorial view of this example, see Figure 11.1.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-295}
\end{center}

Figure 11.1

\section*{Special Tests for Uniform Convergence of Series}
\begin{enumerate}
  \item Weierstrass $M$ test. If a sequence of positive constants $M_{1}, M_{2}, M_{3}, \ldots$ can be found such that in some interval
\end{enumerate}

(a) $\left|u_{n}(x)\right| \leqq M_{n} \quad n=1,2,3, \ldots$

(b) $\Sigma M_{n}$ converges

then $\Sigma u_{n}(x)$ is uniformly and absolutely convergent in the interval.

EXAMPLE.\\
converges. $\quad \sum_{n=1}^{\infty} \frac{\cos n x}{n^{2}}$ is uniformly and absolutely convergent in $[0,2 \pi]$ since $\left|\frac{\cos n x}{n^{2}}\right| \leqq \frac{1}{n^{2}}$ and $\sum \frac{1}{n^{2}}$

This test supplies a sufficient but not a necessary condition for uniform convergence, i.e., a series may be uniformly convergent even when the test cannot be made to apply.

Because of this test, we may be led to believe that uniformly convergent series must be absolutely convergent, and conversely. However, the two properties are independent; i.e., a series can be uniformly convergent without being absolutely convergent, and conversely. See Problems 11.30 and 11.127.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Dirichlet's test. Suppose that
\end{enumerate}

(a) the sequence $\left\{a_{n}\right\}$ is a monotonic decreasing sequence of positive constants having limit zero.

(b) there exists a constant $P$ such that for $a \leq x \leq b\left|u_{1}(x)+u_{2}(x)+\cdots+u_{n}(x)\right|<P, \quad$ for all $n>N$.

Then the series $a_{1} u_{1}(x)+a_{2} u_{2}(x)+\cdots=\sum_{n=1}^{\infty} a_{n} u_{n}(x)$ is uniformly convergent in $a \leqq x \leqq b$.

\section*{Theorems on Uniformly Convergent Series}
If an infinite series of functions is uniformly convergent, it has many of the properties possessed by sums of finite series of functions, as indicated in the following theorems.

Theorem 6 If $\left\{u_{n}(x)\right\}, n=1,2,3, \ldots$ are continuous in $[a, b]$ and if $\Sigma u_{n}(x)$ converges uniformly to the sum $S(x)$ in $[a, b]$, then $S(x)$ is continuous in $[a, b]$.

Briefly, this states that a uniformly convergent series of continuous functions is a continuous function. This result is often used to demonstrate that a given series is not uniformly convergent by showing that the sum function $S(x)$ is discontinuous at some point (see Problem 11.30).

In particular, if $x_{0}$ is in $[a, b]$, then the theorem states that

$$
\lim _{x \rightarrow x_{0}} \sum_{n=1}^{\infty} u_{n}(x)=\sum_{n=1}^{\infty} \lim _{x \rightarrow x_{0}} u_{n}(x)=\sum_{n=1}^{\infty} u_{n}\left(x_{0}\right)
$$

where we use right- or left-hand limits in case $x_{0}$ is an endpoint of $[a, b]$.

Theorem 7 If $\left\{u_{n}(x)\right\}, n=1,2,3, \ldots$ are continuous in $[a, b]$ and if $\Sigma u_{n}(x)$ converges uniformly to the sum $S(x)$ in $[a, b]$, then

or


\begin{equation*}
\int_{a}^{b} S(x) d x=\sum_{n=1}^{\infty} \int_{a}^{b} u_{n}(x) d x \tag{4}
\end{equation*}



\begin{equation*}
\int_{a}^{b}\left\{\sum_{n=1}^{\infty} u_{n}(x)\right\} d x=\sum_{n=1}^{\infty} \int_{a}^{b} u_{n}(x) d x \tag{5}
\end{equation*}


Briefly, a uniformly convergent series of continuous functions can be integrated term by term.

Theorem 8 If $\left\{u_{n}(x)\right\}, n=1,2,3, \ldots$ are continuous and have continuous derivatives in $[a, b]$ and if $\Sigma u_{n}(x)$ converges to $S(x)$ while $\Sigma u_{n}^{\prime}(x)$ is uniformly convergent in $[a, b]$, then in $[a, b]$


\begin{equation*}
S^{\prime}(x)=\sum_{n=1}^{\infty} u_{n}^{\prime}(x) \tag{6}
\end{equation*}


or


\begin{equation*}
\frac{d}{d x}\left\{\sum_{n=1}^{\infty} u_{n}(x)\right\}=\sum_{n=1}^{\infty} \frac{d}{d x} u_{n}(x) \tag{7}
\end{equation*}


This shows conditions under which a series can be differentiated term by term.

Theorems similar to these can be formulated for sequences. For example, if $\left\{u_{n}(x)\right\}, n=1,2,3, \ldots$ is uniformly convergent in $[a, b]$, then


\begin{equation*}
\lim _{n \rightarrow \infty} \int_{a}^{b} u_{n}(x) d x=\int_{a}^{b} \lim _{n \rightarrow \infty} u_{n}(x) d x \tag{8}
\end{equation*}


which is the analog of Theorem 7.

\section*{Power Series}
A series having the form


\begin{equation*}
a_{0}+a_{1} x+a_{2} x^{2}+\cdots=\sum_{n=0}^{\infty} a_{n} x^{n} \tag{9}
\end{equation*}


where $a_{0}, a_{1}, a_{2}, \ldots$ are constants, is called a power series in $x$. It is often convenient to abbreviate the series (9) as $\Sigma a_{n} x^{n}$.

In general, a power series converges for $|x|<R$ and diverges for $|x|>R$, where the constant $R$ is called the radius of convergence of the series. For $|x|=R$, the series may or may not converge.

The interval $|x|<R$ or $-R<x<R$, with possible inclusion of endpoints, is called the interval of convergence of the series. Although the ratio test is often successful in obtaining this interval, it may fail, and in such cases, other tests may be used (see Problem 11.22).

The two special cases $R=0$ and $R=\infty$ can arise. In the first case the series converges only for $x=0$; in the second case it converges for all $x$, sometimes written $-\infty<x<\infty$ (see Problem 11.25). When we speak of a convergent power series, we shall assume, unless otherwise indicated, that $R>0$.

Similar remarks hold for a power series of the form (9), where $x$ is replaced by $(x-a)$.

\section*{Theorems on Power Series}
Theorem 9 A power series converges uniformly and absolutely in any interval which lies entirely within its interval of convergence.

Theorem 10 A power series can be differentiated or integrated term by term over any interval lying entirely within the interval of convergence. Also, the sum of a convergent power series is continuous in any interval lying entirely within its interval of convergence.

This follows at once from Theorem 9 and the theorem on uniformly convergent series on Pages 284 and 285. The results can be extended to include endpoints of the interval of convergence by the following theorems.

Theorem 11 Abel's theorem. When a power series converges up to and including an endpoint of its interval of convergence, the interval of uniform convergence also extends so far as to include this endpoint. See Problem 11.42 .

Theorem 12 Abel's limit theorem. If $\sum_{n=0}^{\infty} a_{n} x^{n}$ converges at $x=x_{0}$, which may be an interior point or an endpoint of the interval of convergence, then


\begin{equation*}
\lim _{x \rightarrow x_{0}}\left\{\sum_{n=0}^{\infty} a_{n} x^{n}\right\}=\sum_{n=0}^{\infty}\left\{\lim _{x \rightarrow x_{0}} a_{n} x^{n}\right\}=\sum_{n=0}^{\infty} a_{n} x_{0}^{n} \tag{10}
\end{equation*}


If $x_{0}$ is an endpoint, we must use $x \rightarrow x_{0}+$ or $x \rightarrow x_{0}$ - in Equation (10) according as $x_{0}$ is a left- or a righthand endpoint.

This follows at once from Theorem 11 and Theorem 6 on the continuity of sums of uniformly convergent series.

\section*{Operations With Power Series}
In the following theorems we assume that all power series are convergent in some interval.

Theorem 13 Two power series can be added or subtracted term by term for each value of $x$ common to their intervals of convergence.

Theorem 14 Two power series, for example, $\sum_{n=0}^{\infty} a_{n} x^{n}$ and $\sum_{n=0}^{\infty} b_{n} x^{n}$, can be multiplied to obtain $\sum_{n=0}^{\infty} c_{n} x^{n}$ where


\begin{equation*}
c_{n}=a_{0} b_{n}+a_{1} b_{n-1}+a_{2} b_{n-2}+\cdots+a_{n} b_{0} \tag{11}
\end{equation*}


the result being valid for each $x$ within the common interval of convergence.

Theorem 15 If the power series $\sum_{n=0}^{\infty} a_{n} x^{n}$ is divided by the power series $\Sigma b_{n} x^{n}$ where $b_{0} \neq 0$, the quotient can be written as a power series which converges for sufficiently small values of $x$.

Theorem 16 If $y=\sum_{n=0}^{\infty} a_{n} x^{n}$, then by substituting $x=\sum_{n=0}^{\infty} b_{n} y^{n}$, we can obtain the coefficients $b_{n}$ in terms of $a_{n}$. This process is often called reversion of series.

\section*{Expansion of Functions in Power Series}
This section gets at the heart of the use of infinite series in analysis. Functions are represented through them. Certain forms bear the names of mathematicians of the eighteenth and early nineteenth centuries who did so much to develop these ideas.

A simple way (and one often used to gain information in mathematics) to explore series representation of functions is to assume such a representation exists and then discover the details. Of course, whatever is found must be confirmed in a rigorous manner. Therefore, assume

$$
f(x)=A_{0}+A_{1}(x-c)+A_{2}(x-c)^{2}+\cdots+A_{n}(x-c)^{n}+\cdots
$$

Notice that the coefficients $A_{n}$ can be identified with derivatives of $f$. In particular,

$$
A_{0}=f(c), A_{1}=f^{\prime}(c), A_{2}=\frac{1}{2 !} f^{\prime \prime}(c), \cdots, A_{n}=\frac{1}{n !} f^{(n)}(c), \cdots
$$

This suggests that a series representation of $f$ is

$$
f(x)=f(c)+f^{\prime}(c)(x-c)+\frac{1}{2 !} f^{\prime \prime}(x-c)^{2}+\cdots+\frac{1}{n !} f^{(n)}(c)(x-c)^{n} \cdots
$$

A first step in formalizing series representation of a function $f$, for which the first $n$ derivatives exist, is accomplished by introducing Taylor polynomials of the function.


\begin{align*}
& P_{0}(x)=f(c) \quad P_{1}(x)=f(c)+f^{\prime}(c)(x-c) \\
& P_{2}(x)=f(c)+f^{\prime}(c)(x-c)+\frac{1}{2 !} f^{\prime \prime}(c)(x-c)^{2} \\
& P_{n}(x)=f(c)+f^{\prime}(c)(x-c)+\cdots \frac{1}{n !} f^{(n)}(c)(x-c)^{n} \tag{12}
\end{align*}


\section*{Taylor's Theorem}
Let $f$ and its derivatives $f^{\prime}, f^{\prime \prime}, \ldots, f^{(n)}$ exist and be continuous in a closed interval $a \leq x \leq b$ and suppose that $f^{(n+1)}$ exists in the open interval $a<x<b$. Then for $c$ in $[a, b]$,

$$
f(x)=P_{n}(x)+R_{n}(x)
$$

where the remainder $R_{n}(x)$ may be represented in any of the three following ways.

For each $n$ there exists $\xi$ such that


\begin{equation*}
R_{n}(x)=\frac{1}{(n+1) !} f^{(n+1)}(\xi)(x-c)^{n+1} \quad \text { (Lagrange form) } \tag{13}
\end{equation*}


( $\xi$ is between $c$ and $x$.)

(The theorem with this remainder is a mean value theorem. Also, it is called Taylor's formula.)

For each $n$ there exists $\xi$ such that

$$
\begin{aligned}
& R_{n}(x)=\frac{1}{n !} f^{(n+1)}(\xi)(x-\xi)^{n}(x-c) \quad \text { (Cauchy form) } \\
& R_{n}(x)=\frac{1}{n !} \int_{c}^{x}(x-t)^{n} f^{(n+1)}(t) d t \quad \text { (Integral form) }
\end{aligned}
$$

If all the derivatives of $f$ exist, then the following form, without remainder, may be explored:


\begin{equation*}
f(x)=\sum_{n=0}^{\infty} \frac{1}{n !} f^{(n)}(c)(x-c)^{n} \tag{16}
\end{equation*}


This infinite series is called a Taylor series, although when $c=0$, it can also be referred to as a MacLaurin series or expansion.

We might be tempted to believe that if all derivatives of $f(x)$ exist at $x=c$, the expansion shown here would be valid. This, however, is not necessarily the case, for although one can then formally obtain the series on the right of the expansion, the resulting series may not converge to $f(x)$. For an example of this see Problem 11.108.

Precise conditions under which the series converges to $f(x)$ are best obtained by means of the theory of functions of a complex variable. (See Chapter 16.)

The determination of values of functions at desired arguments is conveniently approached through Taylor polynomials.

EXAMPLE. The value of $\sin x$ may be determined geometrically for $0, \frac{\pi}{6}$, and an infinite number of other arguments. To obtain values for other real number arguments, a Taylor series may be expanded about any of these points. For example, let $c=0$ and evaluate several derivatives there; i.e., $f(0)=\sin 0=0, f^{\prime}(0)=\cos 0=1$, $f^{\prime \prime}(0)=-\sin 0=0, f^{\prime \prime \prime}(0)=-\cos 0=-1, f^{l v}(0)=\sin 0=0, f^{v}(0)=\cos 0=1$.

Thus, the MacLaurin expansion to five terms is

$$
\sin x=0+x-0-\frac{1}{3 !} x^{3}+0-\frac{1}{51} x^{5}+\cdots
$$

Since the fourth term is 0 , the Taylor polynomials $P_{3}$ and $P_{4}$ are equal, i.e.,

$$
P_{3}(x)=p_{4}(x)=x-\frac{x^{3}}{3 !}
$$

and the Lagrange remainder is

$$
R_{4}(x)=\frac{1}{5 !} \cos \xi x^{5}
$$

Suppose an approximation of the value of $\sin .3$ is required. Then

$$
P_{4}(.3)=.3-\frac{1}{6}(.3)^{3} \approx .2945
$$

The accuracy of this approximation can be determined from examination of the remainder. In particular (remember $|\cos \xi| \leq 1$ ),

$$
\left|R_{4}\right|=\left|\frac{1}{5 !} \cos \xi(.3)^{5}\right| \leq \frac{1}{120} \frac{243}{10^{5}}<.000021
$$

Thus, the approximation $P_{4}(.3)$ for $\sin .3$ is correct to four decimal places.

Additional insight into the process of approximation of functional values results by constructing a graph of $P_{4}(x)$ and comparing it to $y=\sin x$. (See Figure 11.2.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-299}
\end{center}

Figure 11.2

The roots of the equation are $0, \pm \sqrt{6}$. Examination of the first and second derivatives reveals a relative maximum at $x=\sqrt{2}$ and a relative minimum at $x=-\sqrt{2}$. The graph is a local approximation of the sin curve. The reader can show that $P_{6}(x)$ produces an even better approximation.

(For an example of series approximation of an integral, see the example that follows.)

\section*{Some Important Power Series}
The following series, convergent to the given function in the indicated intervals, are frequently employed in practice:

\begin{enumerate}
  \item $\sin x=x-\frac{x^{3}}{3 !}+\frac{x^{5}}{5 !}-\frac{x^{7}}{7 !}+\cdots(-1)^{n-1} \frac{x^{2 n-1}}{(2 n-1) !}+\cdots-\infty<x<\infty$

  \item $\cos x=1-\frac{x^{2}}{2 !}+\frac{x^{4}}{4 !}-\frac{x^{6}}{6 !}+\cdots(-1)^{n-1} \frac{x^{2 n-2}}{(2 n-2) !}+\cdots-\infty<x<\infty$

  \item $e^{x}=1+x+\frac{x^{2}}{2 !}+\frac{x^{3}}{3 !}+\cdots+\frac{x^{n-1}}{(n-1) !}+\cdots-\infty<x<\infty$

  \item $\quad \ln |1+x| \quad=x-\frac{x^{2}}{2}+\frac{x^{3}}{3}-\frac{x^{4}}{4}+\cdots(-1)^{n-1} \frac{x^{n}}{n}+\cdots-1<x \leqq 1$

  \item $\quad \frac{1}{2} \ln \frac{|1+x|}{1-x}=x+\frac{x^{3}}{3}+\frac{x^{5}}{5}+\frac{x^{7}}{7}+\cdots+\frac{x^{2 n-1}}{2 n-1}+\cdots-1<x<1$

  \item $\tan ^{-1} x=x-\frac{x^{3}}{3}+\frac{x^{5}}{5}-\frac{x^{7}}{7}+\cdots(-1)^{n-1} \frac{x^{2 n-1}}{2 n-1}+\cdots-1 \leqq x \leqq 1$

  \item $(1+x)^{p} \quad=1+p x+\frac{P(p-1)}{2 !} x^{2}+\cdots+\frac{P(p-1) \ldots(p-n+1)}{n !} x^{n}+\cdots$

\end{enumerate}

This is the binomial series.

(a) If $p$ is a positive integer or zero, the series terminates.

(b) If $p>0$ but is not an integer, the series converges (absolutely) for $-1 \leqq x \leqq 1$.

(c) If $-1<p<0$, the series converges for $-1<x \leqq 1$.

(d) If $p \leqq-1$, the series converges for $-1<x<1$.

For all $p$, the series certainly converges if $-1<x<1$.

EXAMPLE. Taylor's theorem applied to the series for $e^{x}$ enables us to estimate the value of the integral $\int_{0}^{1} e^{x^{2}} d x$. Substituting $x^{2}$ for $x$, we obtain

where

$$
\int_{0}^{1} e^{x^{2}} d x=\int_{0}^{1}\left(1+x+\frac{x^{4}}{2 !}+\frac{x^{6}}{3 !}+\frac{x^{8}}{4 !}+\frac{e^{\xi}}{5 !} x^{10}\right) d x
$$

and

$$
p_{4}(x)=1+x+\frac{1}{2 !} x^{4}+\frac{1}{3 !} x^{6}+\frac{1}{4 !} x^{8}
$$

Then

$$
R_{4}(x)=\frac{e^{\xi}}{5 !} x^{10}, \quad 0<\xi<x
$$

$$
\begin{gathered}
\int_{0}^{1} P_{4}(x) d x=1+\frac{1}{3}+\frac{1}{5(2 !)}+\frac{1}{7(3 !)}+\frac{1}{9(4 !)} \approx 1.4618 \\
\left|\int_{0}^{1} R_{4}(x) d x\right| \leq \int_{0}^{1}\left|\frac{e^{\xi}}{5 !} x^{10}\right| d x \leq e \int_{0}^{1} \frac{x^{10}}{5 !} d x=\frac{e}{11.5}<.0021
\end{gathered}
$$

Thus, the maximum error is less than .0021 and the value of the integral is accurate to two decimal places.

\section*{Special Topics}
\begin{enumerate}
  \item Functions defined by series are often useful in applications and frequently arise as solutions of differential equations. For example, the function defined by
\end{enumerate}


\begin{align*}
J_{p}(x) & =\frac{x^{p}}{2^{p} p !}\left\{1-\frac{2}{2(2 p+2)}+\frac{x^{4}}{2 \cdot 4(2 p+2)(2 p+4)}-\cdots\right\} \\
& =\sum_{n=0}^{\infty} \frac{(-1)^{n}(x / 2)^{p+2 n}}{n !(n+p) !} \tag{16}
\end{align*}


is a solution of Bessel's differential equation $x^{2} y^{\prime \prime}+x y^{\prime}+\left(x^{2}-p^{2}\right) y=0$ and is thus called a Bessel function of order p. See Problems 11.46 and 11.110 through 11.113.

Similarly, the hypergeometric function


\begin{equation*}
F(a, b ; c ; x)=1+\frac{a \cdot b}{1 \cdot c} x+\frac{a(a+1) b(b+1)}{1 \cdot 2 \cdot c(c+1)} x^{2}+\cdots \tag{17}
\end{equation*}


is a solution of Gauss's differential equation $x(1-x) y^{\prime \prime}+\{c-(a+b+1) x\} y^{\prime}-a b y=0$.

These functions have many important properties.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Infinite series of complex terms, in particular, power series of the form $\sum_{n=0}^{\infty} a_{n} z^{n}$, where $z=x+i y$ and $a_{n}$ may be complex and can be handled in a manner similar to real series.
\end{enumerate}

Such power series converge for $|z|<R$; i.e., interior to a circle of convergence $x^{2}+y^{2}=R^{2}$, where $R$ is the radius of convergence (if the series converges only for $z=0$, we say that the radius of convergence $R$ is zero; if it converges for all $z$, we say that the radius of convergence is infinite). On the boundary of this circle, i.e., $|z|=R$, the series may or may not converge, depending on the particular $z$.

Note that for $y=0$ the circle of convergence reduces to the interval of convergence for real power series. Greater insight into the behavior of power series is obtained by use of the theory of functions of a complex variable (see Chapter 16).

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Infinite series of functions of two (or more) variables, such as $\sum_{n=0}^{\infty} u_{n}(x, y)$, can be treated in a manner analogous to series in one variable. In particular, we can discuss power series in $x$ and $y$ having the form
\end{enumerate}

$$
a_{00}+\left(a_{10} x+a_{01} y\right)+\left(a_{20} x^{2}+a_{11} x y+a_{02} y^{2}\right)+\cdots
$$

using double subscripts for the constants. As for one variable, we can expand suitable functions of $x$ and $y$ in such power series. In particular, the Taylor theroem may be extended as follows.

\section*{Taylor's Theorem (For Two Variables)}
Let $f$ be a function of two variables $x$ and $y$. If all partial derivatives of order $n$ are continuous in a closed region and if all the $(n+1)$ partial derivatives exist in the open region, then


\begin{align*}
f\left(x_{0}+h, y_{0}+k\right) & =f\left(x_{0}, y_{0}\right)+\left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right) f\left(x_{0}, y_{0}\right)+\frac{1}{2 !}\left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right)^{2} f\left(x_{0}, y_{0}\right)+\cdots  \tag{18}\\
& +\frac{1}{n !}\left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right)^{n} f\left(x_{0}, y_{0}\right)+R_{n}
\end{align*}


where

$$
R_{n}=\frac{1}{(n+1) !}\left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right)^{n+1} f\left(x_{0}+\theta h, y_{0}+\theta k\right), \quad 0<\theta<1
$$

and where the meaning of the operator notation is as follows:

$$
\begin{aligned}
& \left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right) f=h f_{x}+k f_{y} \\
& \left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right)^{2}=h^{2} f_{x x}+2 h k f_{x y}+k^{2} f_{y y}
\end{aligned}
$$

and we formally expand $\left(h \frac{\partial}{\partial x}+k \frac{\partial}{\partial y}\right)^{n}$ by the binomial theorem.

Note: In alternate notation $h=\Delta x=x-x_{0}, k=\Delta y=y-y_{0}$.

If $R_{n} \rightarrow 0$ as $n \rightarrow \infty$ then an unending continuation of terms produces the Taylor series for $f(x, y)$.

Multivariable Taylor series have a similar pattern.

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Double series. Consider the array of numbers (or functions)
\end{enumerate}

$$
\left(\begin{array}{cccc}
u_{11} & u_{12} & u_{13} & \cdots \\
u_{21} & u_{22} & u_{23} & \cdots \\
u_{31} & u_{32} & u_{33} & \cdots \\
\vdots & \vdots & \vdots &
\end{array}\right)
$$

Let $S_{m n}=\sum_{p=1}^{m} \sum_{q=1}^{n} u_{p q}$ be the sum of the numbers in the first $m$ rows and first $n$ columns of this array.

If there exists a number $S$ such that $\lim _{\substack{m \rightarrow \infty \\ n \rightarrow \infty}} S_{m n}=S$, we say that the double series $\sum_{p=1}^{\infty} \sum_{q=1}^{\infty} u_{p q}$ converges to the sum $S$; otherwise, it diverges.

Definitions and theorems for double series are very similar to those for series already considered.

\begin{enumerate}
  \setcounter{enumi}{4}
  \item Infinite products. Let $P_{n}=\left(1+u_{1}\right)\left(1+u_{1}\right)\left(1+u_{2}\right)\left(1+u_{3}\right) \ldots\left(1+u_{n}\right)$ denoted by $\prod_{k=1}^{n}\left(1+u_{k}\right)$, where we suppose that $u_{k} \neq-1, k=1,2,3, \ldots$ If there exists a number $P \neq 0$ such that $\lim _{n \rightarrow \infty} P_{n}=P$, we say that the infinite product $\left(\left(1+u_{1}\right)\left(1+u_{2}\right)\left(1+u_{3}\right) \ldots=\prod_{k=1}^{\infty}\left(1+u_{k}\right)\right.$, or, briefly, $\Pi\left(1+u_{k}\right)$, converges to $P$; otherwise, it diverges.
\end{enumerate}

If $\Pi\left(1+\left|u_{k}\right|\right)$ converges, we call the infinite product $\Pi\left(1+u_{k}\right)$ absolutely convergent. It can be shown that an absolutely convergent infinite product converges and that factors can in such cases be rearranged without affecting the result.

Theorems about infinite products can (by taking logarithms) often be made to depend on theorems for infinite series. Thus, for example, we have the following theorem.

Theorem A necessary and sufficient condition that $\Pi\left(1+u_{k}\right)$ converge absolutely is that $\Sigma u_{k}$ converge absolutely.

\begin{enumerate}
  \setcounter{enumi}{5}
  \item Summability. Let $S_{1}, S_{2}, S_{3}, \ldots$ be the partial sums of a divergent series $\Sigma u_{n}$. If the sequence $S_{1}, \frac{S_{1}+S_{2}}{2}, \frac{S_{1}+S_{2}+S_{3}}{3} \cdots$ (formed by taking arithmetic means of the first $n$ terms of $S_{1}, S_{2}, S_{3}, \ldots$ ) converges to $S$, we say that the series $\Sigma u_{n}$ is summable in the Césaro sense, or $C-1$ summable to $S$ (see Problem 11.51).
\end{enumerate}

If $\Sigma u_{n}$ converges to $S$, the Césaro method also yields the result $S$. For this reason, the Césaro method is said to be a regular method of summability.

In case the Césaro limit does not exist, we can apply the same technique to the sequence $S_{1}, \frac{S_{1}+S_{2}}{3}, \frac{S_{1}+S_{2}+S_{3}}{3}, \cdots$ If the $C-1$ limit for this sequence exists and equals $S$, we say that $\Sigma u_{k}$ converges to $S$ in the $C$-2 sense. The process can be continued indefinitely.

\section*{SOLVED PROBLEMS}
\section*{Convergence And Divergence Of Series Of Constants}
11.1. (a) Prove that $\frac{1}{1 \cdot 3}+\frac{1}{3 \cdot 5}+\frac{1}{5 \cdot 7}+\cdots=\sum_{n=1}^{\infty} \frac{1}{(2 n-1)(2 n+1)}$ converges and (b) find its sum.

$$
u_{n}=\frac{1}{(2 n-1)(2 n+1)}=\frac{1}{2}\left(\frac{1}{2 n-1}-\frac{1}{2 n+1}\right)
$$

Then

$$
\begin{aligned}
S_{n}=u_{1}+u_{2}+\cdots+u_{n} & =\frac{1}{2}\left(\frac{1}{1}-\frac{1}{3}\right)+\frac{1}{2}\left(\frac{1}{3}-\frac{1}{5}\right)+\cdots+\frac{1}{2}\left(\frac{1}{2 n-1}-\frac{1}{2 n+1}\right) \\
& =\frac{1}{2}\left(\frac{1}{1}-\frac{1}{3}+\frac{1}{3}-\frac{1}{5}+\frac{1}{5}-\cdots+\frac{1}{2 n-1}-\frac{1}{2 n+1}\right)=\frac{1}{2}\left(1-\frac{1}{2 n+1}\right)
\end{aligned}
$$

Since $\lim _{n \rightarrow \infty} S_{n}=\lim _{n \rightarrow \infty} \frac{1}{2}\left(1-\frac{1}{2 n+1}\right)=\frac{1}{2}$, the series converges and its sum is $1 / 2$.

The series is sometimes called a telescoping series, since the terms of $S_{n}$, other than the first and last, cancel out in pairs.

11.2. (a) Prove that $\frac{2}{3}+\left(\frac{2}{3}\right)^{2}+\left(\frac{2}{3}\right)^{3}+\cdots=\sum_{n=1}^{\infty}\left(\frac{2}{3}\right)^{n}$ converges and (b) find its sum.

This is a geometric series; therefore, the partial sums are of the form $S_{n}=\frac{a\left(1-r^{n}\right)}{1-r}$. Since $|r|<1$ $S=\lim _{n \rightarrow \infty} S_{n}=\frac{a}{1-r}$ and in particular with $r=\frac{2}{3}$ and $a=\frac{2}{3}$, we obtain $S=2$.

11.3. Prove that the series $\frac{1}{2}+\frac{2}{3}+\frac{3}{4}+\frac{4}{5}+\cdots=\sum_{n=1}^{\infty} \frac{n}{n+1}$ diverges.

$\lim _{n \rightarrow \infty} u_{n}=\lim _{n \rightarrow \infty} \frac{n}{n+1}=1$. Hence, by Problem 2.26, the series is divergent.

11.4. Show that the series whose $n$th term is $u_{n}=\sqrt{n+1}-\sqrt{n}$ diverges although $\lim _{x \rightarrow \infty} u_{n}=0$.

It is a fact that $\lim _{x \rightarrow \infty} u_{n}=0$ follows from Problem 2.14(c).

Now $S_{n}=u_{1}+\stackrel{\substack{x \rightarrow \infty \\ u_{2}}}{+\cdots+u_{n}}=(\sqrt{2}-\sqrt{1})+(\sqrt{3}-\sqrt{2})+\cdots+((\sqrt{n+1}-\sqrt{n})=\sqrt{n+1}-\sqrt{1}$.

The $S_{n}$ increases without bound and the series diverges.

This problem shows that $\lim _{x \rightarrow \infty}=0$ is a necessary but not sufficient condition for the convergence of $\Sigma u_{n}$

See also Problem 11.6.

\section*{Comparison test and quotient test}
11.5. If $0 \leq u_{n} \leq v_{n}, n=1,2,3, \ldots$ and if $\Sigma v_{n}$ converges, prove that $\Sigma u_{n}$ also converges (i.e., establish the comparison test for convergence).

Let $S_{n}=u_{1}+u_{2}+\cdots+u_{n}, T_{n}=v_{1}+v_{2}+\cdots+v_{n}$.

Since $\Sigma v_{n}$ converges, $\lim _{n \rightarrow \infty} T_{n}$ exists and equals $T$, say. Also, since $v_{n} \geqq 0, T_{n} \leqq T$.

Then $S_{n}=u_{1}+u_{2}+\cdots+u_{n} \leqq v_{1}+v_{2}+\cdots+v_{n} \leqq T$ or $0 \leqq S_{n} \leqq T$

Thus $S_{n}$ is a bounded monotonic increasing sequence and must have a limit (see Chapter 2); i.e., $\Sigma u_{n}$ converges.

11.6. Using the comparison test, prove that $1+\frac{1}{2}+\frac{1}{3}+\cdots=\sum_{n=1}^{\infty} \frac{1}{n}$ diverges.

We have

$$
\begin{aligned}
1 & \geqq \frac{1}{2} \\
\frac{1}{2}+\frac{1}{3} & \geqq \frac{1}{4}+\frac{1}{4}=\frac{1}{2} \\
\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7} & \geqq \frac{1}{8}+\frac{1}{8}+\frac{1}{8}+\frac{1}{8}=\frac{1}{2} \\
\frac{1}{8}+\frac{1}{9}+\frac{1}{10}+\cdots+\frac{1}{15} & \geqq \frac{1}{16}+\frac{1}{16}+\frac{1}{16}+\cdots+\frac{1}{16}(8 \text { terms })=\frac{1}{2}
\end{aligned}
$$

and soon. Thus, to any desired number of terms.

$$
1+\left(\frac{1}{2}+\frac{1}{3}\right)+\left(\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}\right)+\cdots \geqq \frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\cdots
$$

Since the right-hand side can be made larger than any positive number by choosing enough terms, the given

series diverges.\\
By methods analogous to that used here, we can show that $\sum_{n=1}^{\infty} \frac{1}{n^{p}}$, where $p$ is a constant, diverges if $p$ $\leqq 1$ and converges if $p>1$. This can also be shown in other ways [see Problem 11.13(a)].

11.7. Test for convergence or divergence $\sum_{n=1}^{\infty} \frac{\ln n}{2 n^{3}-1}$.

Since In $n<n$ and $\frac{1}{2 \mathrm{n}^{3}-1} \leqq \frac{1}{n^{3}}$, we have $\frac{\text { In } n}{2 n^{3}-1} \leqq \frac{n}{n^{3}}=\frac{1}{n^{2}}$.

Then the given series converges, since $\sum_{n=1}^{\infty} \frac{1}{n^{2}}$ converges. 11.8. Let $u_{n}$ and $v_{n}$ be positive. If $\lim _{n \rightarrow \infty} \frac{u_{n}}{v_{n}}=$ constant $A \neq 0$, prove that $\Sigma u_{n}$ converges or diverges according as\\
$\Sigma v_{n}$ converges or diverges. By hypothesis, given $\epsilon>0$, we can choose an integer $N$ such that $\left|\frac{u_{n}}{v_{n}}-A\right|<\varepsilon$ for all $n>N$. Then for\\
$n=N+1, N+2, \ldots$


\begin{equation*}
-\varepsilon<f \frac{u_{n}}{v_{n}}-A<\varepsilon \quad \text { or } \quad(A-\varepsilon) v_{n}<u_{n}(A+\varepsilon) v_{n} \tag{1}
\end{equation*}


Summing from $N+1$ to $\infty$ (more precisely, from $N+1$ to $M$ and then letting $M \rightarrow \infty$ ),


\begin{equation*}
(A-\varepsilon) \sum_{N+1}^{\infty} v_{n} \leqq \sum_{N+1}^{\infty} u_{n} \leqq(A+\varepsilon) \sum_{N+1}^{\infty} v_{n} \tag{2}
\end{equation*}


There is no loss in generality in assuming $A-\epsilon>0$. Then from the right-hand inequality of Equation (2), $\Sigma u_{n}$ converges when $\Sigma v_{n}$ does. From the left-hand inequality of Equation (2), $\Sigma u_{n}$ diverges when $\Sigma v_{n}$ does. For the cases $A=0$ or $A=\infty$, see Problem 11.66.

11.9. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{4 n^{2}-n+3}{n^{3}+2 n}$, (b) $\sum_{n=1}^{\infty} \frac{n+\sqrt{n}}{2 n^{3}-1}$, and (c) $\sum_{n=1}^{\infty} \frac{\ln n}{n^{2}+3}$.

(a) For large $n, \frac{4 n^{2}-n+3}{n^{3}+2 n}$ is approximately $\frac{4 n^{2}}{n^{3}}=\frac{4}{n}$. Taking $u_{n}=\frac{4 n^{2}-n+3}{n^{3}+2 n}$ and $v_{n}=\frac{4}{n}$, we have $\lim _{n \rightarrow \infty}=\frac{u_{n}}{v_{n}}=1$.

Since $\Sigma v_{n}=4 \Sigma 1 / n$ diverges, $\Sigma u_{n}$ also diverges, by Problem 11.8 .

Note that the purpose of considering the behavior of $u_{n}$ for large $n$ is to obtain an appropriate comparison series $v_{n}$. In this case we could just as well have taken $v_{n}=1 / n$.

Another method: $\lim _{n \rightarrow \infty} n\left(\frac{4 n^{2}-n+3}{n^{3}+2 n}\right)=4$. Then by Theorem 1, Page 281, the series converges.

(b) For large $n, u_{n}=\frac{n+\sqrt{n}}{2 n^{3}-1}$ is approximately $v_{n}=\frac{n}{2 n^{3}}=\frac{1}{2 n^{2}}$.

Since $\lim _{n \rightarrow \infty} \frac{u_{n}}{v_{n}}=1$ and $\sum v_{n}=\frac{1}{2} \sum \frac{1}{n^{2}}$ converges ( $p$ series with $p=2$ ), the given series converges.

Another method: $\lim _{n \rightarrow \infty} n^{2}\left(\frac{n+\sqrt{n}}{2 n^{3}-1}\right)=\frac{1}{2}$. Then by Theorem 1, Page 281, the series converges.

(c) $\lim _{n \rightarrow \infty} n^{3 / 2}\left(\frac{\ln n}{n^{2}+3}\right) \leqq \lim _{n \rightarrow \infty} n^{3 / 2}\left(\frac{\ln n}{n^{2}}\right)=\lim _{n \rightarrow \infty} \frac{\ln n}{\sqrt{n}}=0$ (by L'Hospital's rule or otherwise). Then by Theorem 1 with $\mathrm{p}=3 / 2$. the series converges.

Note that the method of Problem 11.6(a) yields $\frac{\ln n}{n^{2}+3}<\frac{n}{n^{2}}=\frac{1}{n}$, but nothing can be deduced, since $\Sigma 1 / n$ diverges.

11.10. Examine for convergence: (a) $\sum_{n=1}^{\infty} e^{-n^{2}}$ and (b) $\sum_{n=1}^{\infty} \sin ^{3}\left(\frac{1}{n}\right)$.

(a) $\lim _{n \rightarrow \infty} n^{2} e^{-n^{2}}=0$ (by L'Hospital's rule or otherwise). Then by Theorem 1 with $\mathrm{p}=2$, the series converges.

(b) For large $\mathrm{n}, \sin (1 / \mathrm{n})$ is approximately $1 / \mathrm{n}$. This leads to consideration of

$$
\lim _{n \rightarrow \infty} n^{3} \sin ^{3}\left(\frac{1}{n}\right)=\lim _{n \rightarrow \infty}\left\{\frac{\sin (1 / n}{1 / n}\right\}^{3}=1
$$

from which we deduce, by Theorem 1 with $p=3$, that the given series converges.

\section*{Integral test}
11.11. Establish the integral test (see Page 281).

We perform the proof taking $N=1$. Modifications are easily made if $N>1$.

From the monotonicity of $f(x)$, we have

$$
u_{n+1}=f(n+1) \leqq f(x) \leqq f(n)=u_{n} \quad n=1,2,3, \ldots
$$

Integrating from $x=n$ to $x=n+1$, using Property 7, Page 98,

$$
u_{n+1} \leqq \int_{n}^{n+1} f(x) d x \leqq u_{n} \quad n=1,2,3 \ldots
$$

Summing from $n=1$ to $M-1$,


\begin{equation*}
u_{2}+u_{3}+\cdots+u_{M} \leqq \int_{1}^{M} f(x) d x \leqq u_{1}+u_{2}+\cdots u_{M-1} \tag{1}
\end{equation*}


If $f(x)$ is strictly decreasing, the equality signs in Equation (1) can be omitted.

If $\lim _{M \rightarrow \infty} \int_{1}^{M} f(x) d x$ exists and is equal to $S$, we see from the left-hand inequality in Equation (1) that $u_{2}+u_{3}+\cdots+u_{M}$ is monotonic increasing and bounded above by $S$, so that $\Sigma u_{n}$ converges.

If $\lim _{M \rightarrow \infty} \int_{1}^{M} f(x) d x$ is unbounded, we see from right-hand inequality in Equation (1) that $\sigma u_{n}$ diverges.

Thus, the proof is complete.

11.12. Illustrate geometrically the proof in Problem 11.11.

Geometrically, $u_{2}+u_{3}+\cdots+u_{M}$ is the total area of the rectangles shown shaded in Figure 11.3, while $u_{1}+u_{2}+\cdots+u_{M-1}$ is the total area of the rectangles which are shaded and nonshaded.

The area under the curve $y=f(x)$ from $x=1$ to $x=M$ is intermediate in value between the two areas given above, thus illustrating the result (1) of Problem 11.11.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-306}
\end{center}

Figure 11.3

11.13. Test for convergence:

(a) $\sum_{1}^{\infty} \frac{1}{n^{p}}, p=\mathrm{constant}$

(b) $\sum_{1}^{\infty} \frac{n}{n^{2}+1}$

(c) $\sum_{2}^{\infty} \frac{1}{n \ln n}$

(d) $\sum_{1}^{\infty} n e^{-n^{2}}$

(a) Consider $\int_{1}^{M} \frac{d x}{x^{p}}=\int_{1}^{M} x^{-p} d x=\left.\frac{x^{1-p}}{1-p}\right|_{1} ^{M}=\frac{M^{1-p}}{1-p}$, where $p \neq 1$.

If $p<1, \lim _{M \rightarrow \infty} \frac{M^{1-p}-1}{1-p}=\infty$, so that integral, and thus the series, diverges.

If $p>1, \lim _{M \rightarrow \infty} \frac{M^{1-p}-1}{1-p}=\frac{1}{p-1}$, so that the integral, and thus the series, converges.

If $p=1, \quad \int_{1}^{M} \frac{d x}{x^{p}}=\int_{1}^{M} \frac{d x}{x}=\ln M$ and $\lim _{M \rightarrow \infty} \ln M=\infty$, so that the integral, and thus the series, diverges. Thus, the series converges if $p>1$ and diverges if $p \leqq 1$.\\
(b) $\lim _{M \rightarrow \infty} \int_{1}^{M} \frac{x d x}{x^{2}+1}=\left.\lim _{M \rightarrow \infty} \frac{1}{2} \ln \left(x^{2}+1\right)\right|_{1} ^{M}=\lim _{M \rightarrow \infty}\left\{\frac{1}{2} \ln \left(M^{2}+1\right)-\frac{1}{2}\right\}=\infty$ and the series diverges.

(c) $\lim _{M \rightarrow \infty} \int_{1}^{M} \frac{d x}{x \ln x}=\left.\lim _{M \rightarrow \infty} \ln (\ln x)\right|_{2} ^{M}=\lim _{M \rightarrow \infty}\{\ln (\ln M)-\ln (\ln 2)\}=\infty$ and the series diverges.

(d) $\lim _{M \rightarrow \infty} \int_{1}^{M} x e^{-x^{2}} d x=\lim _{M \rightarrow \infty}-\left.\frac{1}{2} e^{-x 2}\right|_{1} ^{M}=\lim _{M \rightarrow \infty}\left\{\frac{1}{2} e^{-1}-\frac{1}{2} e^{-M^{2}}\right\}=\frac{1}{2} e^{-1}$ and the series converges.

Note that when the series converges, the value of the corresponding integral is not (in general) the same as the sum of the series. However, the approximate sum of a series can often be obtained quite accurately by using integrals. See Problem 11.74.

11.14. Prove that Prove that $\frac{\pi}{4}<\sum_{n=1}^{\infty} \frac{1}{n^{2}+1}<\frac{1}{2}+\frac{\pi}{4}$.

From Problem 11.11 it follows that

$$
\lim _{M \rightarrow \infty} \sum_{n=2}^{M} \frac{1}{n^{2}+1}<\lim _{M \rightarrow \infty} \int_{1}^{M} \frac{d x}{x^{2}+1}<\lim _{M \rightarrow \infty} \sum_{n=1}^{M-1} \frac{1}{n^{2}+1}
$$

i.e., $\sum_{n=2}^{\infty} \frac{1}{n^{2}+1}<\frac{\pi}{4}<\sum_{n=1}^{\infty} \frac{1}{n^{2}+1}$, from which $\frac{\pi}{4}<\sum_{n=1}^{\infty} \frac{1}{n^{2}+1}$ as required.

Since $\sum_{n=2}^{\infty} \frac{1}{n^{2}+1}<\frac{\pi}{4}$, we obtain, on adding $\frac{1}{2}$ to each side, $\sum_{n=1}^{\infty} \frac{1}{n^{2}+1}<\frac{1}{2}+\frac{\pi}{4}$.

The required result is therefore proved.

\section*{Alternating series}
11.15. Given the alternating series $a_{1}-a_{2}+a_{3}-a_{4}+\cdots$, where $0 \leq a_{n+1} \leq a_{n}$ and where $\lim _{n \rightarrow \infty} a_{n}=0$, prove that (a) the series converges and (b) the error made in stopping at any term is not greater than the absolute value of the next term.

(a) The sum of the series to $2 \mathrm{M}$ terms is

$$
\begin{aligned}
& S_{2 M}=\left(a_{1}-a_{2}\right)+\left(a_{3}-\left(a_{4}\right)+\cdots+\left(a_{2 M-1}-a_{2 M}\right)\right. \\
= & a_{1}-\left(a_{2}-a_{3}\right)-\left(a_{4}-a_{5}\right)-\cdots-\left(a_{2 M-2}-a_{2 M-1}\right)-a_{2 M}
\end{aligned}
$$

Since the quantities in parentheses are nonnegative, we have

$$
S_{2 M} \geqq 0, \quad S_{2} \leqq S_{4} \leqq S_{6} \leqq S_{8} \leqq \cdots \leqq S_{2 M} \leqq a_{1}
$$

Therefore, $\left\{S_{2 M}\right\}$ is a bounded monotonic increasing sequence and thus has limit $S$.

Also, $S_{2 M+1}=S_{2 M}+a_{2 M+1}$. Since $\lim _{M \rightarrow \infty} S_{2 M}=S$ and $\lim _{M \rightarrow \infty} a_{2 M+1}=0$ (for, by hypothesis, $\lim _{M \rightarrow \infty}$ $a_{n}=0$ ), it follows that $\lim _{M \rightarrow \infty} S_{2 M+1}=\lim _{M \rightarrow \infty} S_{2 M}+\lim _{M \rightarrow \infty} a_{2 M+1}=S+0=S$.

Thus, the partial sums of the series approach the limit $S$ and the series converges.

(b) The error made in stopping after $2 \mathrm{M}$ terms is

$$
\left(a_{2 M+1}-a_{2 M+2}\right)+\left(a_{2 M+3}-a_{2 M+4}\right)+\cdots=a_{2 M+1}-\left(a_{2 M+2}-a_{2 M+3}\right)-\cdots
$$

and is thus nonnegative and less than or equal to $a_{2 M+1}$, the first term which is omitted.

Similarly, the error made in stopping after $2 M+1$ terms is

$$
-a_{2 M+2}+\left(a_{2 M+3}-a_{2 M+4}\right)+\cdots=-\left(a_{2 M+2}-a_{2 M+3}\right)-\left(a_{2 M+4}-a_{2 M+5}\right)-\cdots
$$

which is nonpositive and greater than $-a_{2 M+2}$.

11.16. (a) Prove that the series $\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{2 n-1}$. converges. (b) Find the maximum error made in approximating the sum by the first eight terms and the first nine terms of the series. (c)How many terms of the series are needed in order to obtain an error which does not exceed .001 in absolute value?

(a) The series is $1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\cdots$. If $u_{n}=\frac{(-1)^{n+1}}{2 n-1}$, then $a_{n}=\left|u_{n}\right|=\frac{1}{2 n-1}, a_{n+1}=\left|u_{n+1}\right|=$ $\frac{1}{2 n+1}$. Since $\frac{1}{2 n+1} \leqq \frac{1}{2 n-1}$ and since $\lim _{n \rightarrow \infty} \frac{1}{2 n-1} 0$, it follows by Problem 11.5(a) that the series converges.

(b) Use the results of Problem 11.15(b). Then the first eight terms give $1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\frac{1}{11}+\frac{1}{13}-\frac{1}{15}$, and the error is positive and does not exceed $\frac{1}{17}$.

Similarly, the first nine terms are $1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\frac{1}{11}+\frac{1}{13}-\frac{1}{15}+\frac{1}{17}$ and the error is negative and greater than or equal to $-\frac{1}{19}$; i.e., the error does not exceed $\frac{1}{19}$ in absolute value.

(c) The absolute value of the error made in stopping after $\mathrm{M}$ terms is less than $1 /(2 \mathrm{M}+1)$. To obtain the desired accuracy, we must have $1 /(2 M+1) \leqq .001$, form which $M \geqq 499.5$. Thus, at least 500 terms are needed.

\section*{Absolute and conditional convergence}
11.17. Prove that an absolutely convergent series is convergent.

Given that $\Sigma\left|u_{n}\right|$ converges, we must show that $\Sigma u_{n}$ converges.

Let $S_{M}=u_{1}+u_{2}+\cdots+u_{M}$ and $T_{M}=\left|u_{1}\right|+\left|u_{2}\right|+\cdots+\left|u_{M}\right|$. Then

$$
\begin{aligned}
S_{M}+T_{M}= & \left(u_{1}+\left|u_{1}\right|\right)+\left(u_{2}+\left|u_{2}\right|\right)+\cdots+\left(u_{M}+\left|u_{M}\right|\right) \\
& \leqq 2\left|u_{1}\right|+2\left|u_{2}\right|+\cdots+2\left|u_{M}\right|
\end{aligned}
$$

Since $\Sigma\left|u_{n}\right|$ converges and since $u_{n}+\left|u_{n}\right| \geqq 0$, for $n=1,2,3, \ldots$, it follows that $S_{M}+T_{M}$ is a bounded monotonic increasing sequence, and so $\lim _{M \rightarrow \infty}\left(S_{M}+T_{M}\right)$ exists.

Also, since $\lim _{M \rightarrow \infty} T_{M}$ exists (since the series is absolutely convergent by hypothesis),

$$
\lim _{M \rightarrow \infty} S_{M}=\lim _{M \rightarrow \infty}\left(S_{M}+T_{M}-T_{M}\right)=\lim _{M \rightarrow \infty}\left(S_{M}+T_{M}\right)-\lim _{M \rightarrow \infty} T_{M}
$$

must also exist, and the result is proved.

11.18. Investigate the convergence of the series $\frac{\sin \sqrt{1}}{1^{3 / 2}}-\frac{\sin \sqrt{2}}{2^{3 / 2}}+\frac{\sin \sqrt{3}}{3^{3 / 2}}-\cdots$.

Since each term is, in absolute value, less than or equal to the corresponding term of the series $\frac{1}{1^{3 / 2}}+\frac{1}{2^{3 / 2}}+\frac{1}{3^{3 / 2}}+\cdots$. which converges, it follows that the given series is absolutely convergent and, hence, convergent by Problem 11.17.

11.19. Examine for convergence and absolute convergence:

(a) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{2}+1}$\\
(b) $\sum_{n=2}^{\infty} \frac{(-1)^{n-1}}{n \operatorname{In}^{2} n}$

(c) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} 2^{n}}{n^{2}}$

(a) The series of absolute values is $\sum_{n=1}^{\infty} \frac{n^{2}}{n^{2}+1}$, which is divergent by Problem 11.13(b). Hence, the given series is not absolutely convergent

However, if $a_{n}=\left|u_{n}\right|=\frac{n}{n^{2}+1}$ and $a_{n+1}=\left|u_{n+1}\right|=\frac{n+1}{(n+1)^{2}+1}$, then $a_{n+1} \leqq a_{n}$ for all $n \geqq 1$, and

also $\lim _{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} \frac{n}{n^{2}+1}=0$. Hence, by Problem 11.15 the series converges.

Since the series converges but is not absolutely convergent, it is conditionally convergent.

(b) The series of absolute values is $\sum_{n=2}^{\infty} \frac{1}{n \ln ^{2} n}$. exist.

By the integral test, this series converges or diverges according as $\lim _{M \rightarrow \infty} \int_{2}^{M} \frac{d x}{x \ln ^{2} x}$ exists or does not If $u=\ln x, \int \frac{d x}{x \ln ^{2} x}=\int \frac{d u}{u^{2}}=-\frac{1}{u}+c=-\frac{1}{\ln x}+c$. verges.

Hence, $\lim _{M \rightarrow \infty} \int_{2}^{M} \frac{d x}{x \ln ^{2} x}=\lim _{M \rightarrow \infty}\left(\frac{1}{\ln 2}-\frac{1}{\ln M}\right)=\frac{1}{\ln 2}$, and the integral exists. Thus, the series conThen $\sum_{n=2}^{\infty} \frac{(-1)^{n-1}}{n \ln ^{2} n}$ converges absolutely and thus converges.

Another method: Since $\frac{1}{(n+1) \ln ^{2}(n+1)} \leqq \frac{1}{n \ln ^{2} n}$ and $\lim _{n \rightarrow \infty} \frac{1}{n \ln ^{2} n}=0$, it follows by Problem 11.15(a), that the given alternating series converges. To examine its absolute convergence, we must proceed as

before.\\
(c) Since $\lim _{n \rightarrow \infty} u_{n} \neq 0$ where $u_{n}=\frac{(-1)^{n-1} 2^{n}}{n^{2}}$, the given series cannot be convergent. To show that $\lim _{n \rightarrow \infty} u_{n} \neq 0$, it suffices to show that $\lim _{n \rightarrow \infty}\left|u_{n}\right|=\lim _{n \rightarrow \infty} \frac{2 n}{n^{2}} \neq 0$. This can be accomplished by L'Hospital's rule or other methods [see Problem 11.21(b)].

\section*{Ratio test}
11.20. Establish the ratio test for convergence.

Consider first the series $u_{1}+u_{2}+u_{3}+\cdots$ where each term is nonnegative. We must prove that if $\lim _{n \rightarrow \infty} \frac{u_{n+1}}{u_{n}} L<1$, then necessarily $\Sigma u_{n}$ converges.

By hypothesis, we can choose an integer $N$ so large that for all $n \geqq N,\left(u_{n+1} / u_{n}\right)<r$ where $L<r<1$. Then

$$
\begin{aligned}
& u_{N+1}<r u_{N} \\
& u_{N+2}<r u_{N+1}<r^{2} u_{N} \\
& u_{N+3}<r u_{N+2}<r^{3} u_{N}
\end{aligned}
$$

and so on. By addition,

$$
u_{N+1}+u_{N+2}+\cdots<u_{N}\left(r+r^{2}+r^{3}+\cdots\right)
$$

and so the given series converges by the comparison test, since $0<r<1$.

In case the series has terms with mixed signs, we consider $\left|u_{1}\right|+\left|u_{2}\right|\left|u_{3}\right|+\cdots$. By the preceding proof and Problem 11.17, it follows that if $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=L<1$, then $\Sigma u_{n}$ converges (absolutely).

Similarly, we can prove that if $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=L>1$ the series $\Sigma u_{n}$ diverges, while if $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=L=1$ the ratio test fails [see Problem 11.21(c)].

11.21. Investigate the convergence of

(a) $\sum_{n=1}^{\infty} n^{4} e^{-n^{2}}$

(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} 2^{n}}{n^{2}}$

(c) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} n}{n^{2}+1}$

(a) Here $u_{n}=e^{-n^{2}}$. Then

$$
\begin{aligned}
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right| & =\lim _{n \rightarrow \infty}\left|\frac{(n+1)^{4} e^{-(n+1)^{2}}}{n^{4} e^{-n^{2}}}\right|=\lim _{n \rightarrow \infty} \frac{(n+1)^{4} e-^{\left(n^{2}+2 n+1\right)}}{n^{4} e^{-n^{2}}} \\
& =\lim _{n \rightarrow \infty}\left(\frac{n+1}{n}\right)^{4} e^{-2 n-1}=\lim _{n \rightarrow \infty}\left(\frac{n+1}{n}\right)^{4} \lim _{n \rightarrow \infty} e^{-2 n-1}=1 \cdot 0=0
\end{aligned}
$$

Since $0<1$, the series converges.

(b) Here $u_{n}=\frac{(-1)^{n-1} 2^{n}}{n^{2}}$. Then

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(-1)^{n} 2^{n+1}}{(n+1)^{2}} \cdot \frac{n^{2}}{(-1)^{n-1} 2^{n}}\right|=\lim _{n \rightarrow \infty} \frac{2 n^{2}}{(n+1)^{2}}=2
$$

Since $s>1$, the series diverges. Compare Problem 11.19(c).

(c) Here $u_{n}=\frac{(-1)^{n-1} n}{n^{2}+1}$. Then

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(-1)^{n}(n+1)}{(n+1)^{2}+1} \cdot \frac{n^{2}+1}{(-1)^{n-1} n}\right|=\lim _{n \rightarrow \infty} \frac{(n+1)\left(n^{2}+1\right)}{\left(n^{2}+2 n+2\right)^{n}}=1
$$

and the ratio test fails. By using other tests [see Problem 11.19(a)], the series is seen to be convergent.

\section*{Miscellaneous tests}
11.22. Test for convergence $1+2 r+r^{2}+2 r^{3}+r^{4}+2 r^{5}+\cdots$ where (a) $r=2 / 3$, (b) $r=-2 / 3$, (c) $r=4 / 3$.

Here the ratio test is inapplicable, since $\left|\frac{u_{n+1}}{u_{n}}\right|=2|r|$ or $\frac{1}{2}|r|$, depending on whether $n$ is odd or even.

However, using the $n$th root test, we have

$$
\sqrt[n]{\left|u_{n}\right|}= \begin{cases}\sqrt[n]{2\left|r^{n}\right|}=\sqrt[n]{2|r|} & \text { if } n \text { is odd } \\ \sqrt[n]{\left|r^{n}\right|}=|r| & \text { if } n \text { is even }\end{cases}
$$

Then $\lim _{n \rightarrow \infty} \sqrt[n]{\left|u_{n}\right|}=|r|\left(\right.$ since $\left.\lim _{n \rightarrow \infty} 2^{1 / n}=1\right)$.

Thus, if $|r|<1$ the series converges, and if $|r|>1$ the series diverges.

Hence, the series converges for cases (a) and (b), and diverges in case (c).

11.23. Test for convergence $\left(\frac{1}{3}\right)^{2}+\left(\frac{1 \cdot 4}{3 \cdot 6}\right)^{2}+\left(\frac{1 \cdot 4 \cdot 7}{3 \cdot 6 \cdot 9}\right)^{2}+\cdots+\left(\frac{1 \cdot 4 \cdot 7 \ldots(3 n-2)}{3 \cdot 6 \cdot 9 \ldots(3 n)}\right)^{2}+\cdots$.

The ratio test fails, since $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left(\frac{3 n+1}{3 n+3}\right)^{2}=1$. However, by Raabe's test,

$$
\lim _{n \rightarrow \infty} n\left(1-\left|\frac{u_{n+1}}{u_{n}}\right|\right)=\lim _{n \rightarrow \infty} n\left\{1-\left(\frac{3 n+1}{3 n+3}\right)^{2}\right\}=\frac{4}{3}>1
$$

and so the series converges.

11.24. Test for convergence $\left(\frac{1}{2}\right)^{2}+\left(\frac{1 \cdot 3}{2 \cdot 4}\right)^{2}+\left(\frac{1 \cdot 3 \cdot 5}{24 t}\right)^{2}+\cdots+\left(\frac{1 \cdot 3 \cdot 5 \ldots(2 n-1)^{2}}{2 \cdot 4 \cdot 6 \ldots(2 n)}\right)+\cdots$.

The ratio test fails, since $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left(\frac{2 n+1}{2 n+2}\right)^{2}=1$. Also, Raabe's test fails since

$$
\lim _{n \rightarrow \infty} n\left(1-\left|\frac{u_{n+1}}{u_{n}}\right|\right)=\lim _{n \rightarrow \infty} n\left\{1-\left(\frac{2 n+1}{2 n+2}\right)^{2}\right\}=1
$$

However, using long division,

$$
\left|\frac{u_{n+1}}{u_{n}}\right|=\left(\frac{2 n+1}{2 n+2}\right)^{2}=1-\frac{1}{n}+\frac{5-4 / n}{4 n^{2}+8 n+4}=1-\frac{1}{n}+\frac{c_{n}}{n^{2}} \quad \text { where }\left|c_{n}\right|<P
$$

so that the series diverges by Gauss's test.

\section*{Series of functions}
11.25. For what values of $x$ do the following series converge?

(a) $\sum_{n=1}^{\infty} \frac{x^{n-1}}{n \cdot 3^{n}}$

(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} x^{2 n-1}}{(2 n-1) !}$

(c) $\sum_{n=1}^{\infty} n !(x-a)^{n}$

(d) $\sum_{n=1}^{\infty} \frac{n(x-1)^{n}}{2^{n}(3 n-1)}$

(a) $u_{n}=\frac{x^{n-1}}{n \cdot 3^{n}}$. Assuming $\mathrm{x} \neq 0$ (if $\mathrm{x}=0$ the series converges), we have

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{x^{n}}{(n+1) \cdot 3^{n+1}} \cdot \frac{n \cdot 3^{n}}{x^{n-1}}\right|=\lim _{n \rightarrow \infty} \frac{n}{3(n+1)}|x|=\frac{|x|}{3}
$$

Then the series converges if $\frac{|x|}{3}<1$, and diverges if $\frac{|x|}{3}>1$. If $\frac{|x|}{3}=1$. i.e., $x= \pm 3$, the test fails.

If $x=3$ the series becomes $\sum_{n=1}^{\infty} \frac{1}{3 n}=\frac{1}{3} \sum_{n=1}^{\infty} \frac{1}{n}$, which diverges.

If $x=-3$ the series becomes $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{3 n}=\frac{1}{3} \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}$, which converges.

Then the interval of convergence is $-3 \leqq x<3$. The series diverges outside this interval.

Note that the series converges absolutely for $-3<x<3$. At $x=-3$ the series converges conditionally.

(b) Proceed as in (a) with $u_{n}=\frac{(-1)^{n-1} x^{2 n-1}}{(2 n-1) !}$. Then

$$
\begin{aligned}
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right| & =\lim _{n \rightarrow \infty}\left|\frac{(-1)^{n} x^{2 n+1}}{(2 n+1) !} \cdot \frac{(2 n-1) !}{(-1)^{n-1} x^{2 n-1}}\right|=\lim _{n \rightarrow \infty} \frac{(2 n-1) !}{(2 n+1) !} x^{2} \\
& =\lim _{n \rightarrow \infty} \frac{(2 n-1) !}{(2 n+1)(2 n)(2 n-1) !} x^{2}=\lim _{n \rightarrow \infty} \frac{x^{2}}{(2 n+1)(2 n)}=0
\end{aligned}
$$

Then the series converges (absolutely) for all $x$, i.e., the interval of (absolute) convergence is $-\infty<x<\infty$.

(c) $u_{n}=n !(x-a)^{n}, \lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(n+1) !(x-a)^{n+1}}{n !(x-a)^{n}}\right|=\lim _{n \rightarrow \infty}(n+1)|x-a|$.

This limit is infinite if $x \neq a$. Then the series converges only for $x=a$.

(d) $u_{n}=\frac{n(x-1)^{n}}{2^{n}(3 n-1)}, u_{n+1}=\frac{(n+1)(x-1)^{n+1}}{2^{n+1}(3 n+2)}$. Then

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(n+1)(3 n-1)(x-1)}{2 n(3 n+2)}\right|=\left|\frac{x-1}{2}\right|=\frac{|x-1|}{2}
$$

Thus, the series converges for $|x-1|<2$ and diverges for $|x-1|>2$.

The test fails for $|x-1|=2$; i.e., $x-1= \pm 2$ or $x=3$ and $x=-1$.

For $x=3$ the series becomes $\sum_{n=1}^{\infty} \frac{n}{3 n-1}$, which diverges, since the $n$th term does not approach zero. zero.

For $x=-1$ the series becomes $\sum_{n=1}^{\infty} \frac{(-1)^{n} n}{3 n-1}$, which also diverges, since the $n$ the term does not approach

Then the series converges only for $|x-1|<2$; i.e., $-2<x-1<2$ or $-1<x<3$.

11.26. For what values of $x$ does (a) $\sum_{n=1}^{\infty} \frac{1}{2 n-1}\left(\frac{x+2}{x-1}\right)^{n}$ and (b) $\sum_{n=1}^{\infty} \frac{1}{(x+n)(x+n-1)}$ converge?

(a) $u_{n}=\frac{1}{2 n-1}\left(\frac{x+2}{x-1}\right)^{n}$. Then $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty} \frac{2 n-1}{2 n+1}\left|\frac{x+2}{x-1}\right|=\left|\frac{x+2}{x-1}\right| \quad$ if $\quad x \neq 1,-2$.

\includegraphics[max width=\textwidth, center]{2024_04_03_ffb6ac533fe0a53b3ceeg-312}\\
$x=-\frac{1}{2}$.

If $x=1$, the series diverges.

If $x=-2$, the series converges.

If $x-\frac{1}{2}$, the series is $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{2 n-1}$, which converges.

Thus, the series converges for $\left|\frac{x+2}{x-1}\right|<1, x=-\frac{1}{2}$, and $x=-2$, i.e., for $x \leqq-\frac{1}{2}$.\\
(b) The ratio test fails, since $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=1$, where $u_{n}=\frac{1}{(x+n)(x+n-1)}$. However, noting that

$$
\frac{1}{(x+n)(x+n-1)}=\frac{1}{x+n-1}-\frac{1}{x+n}
$$

we see that if $x \neq 0,-1,-2, \ldots,-n$,

$$
\begin{aligned}
S_{n}=u_{1}+u_{2}+\cdots+u_{n}= & \left(\frac{1}{x}-\frac{1}{x+1}\right)+\left(\frac{1}{x+1}-\frac{1}{x+2}\right)+\cdots+\left(\frac{1}{x+n-1}-\frac{1}{x+n}\right) \\
= & \frac{1}{x}-\frac{1}{x+n}
\end{aligned}
$$

and $\lim _{n \rightarrow \infty} S_{n}=1 / x$, provided $x \neq 0,-1,-2,-3, \ldots$.

Then the series converges for all $x$ except $x=0,-1,-2,-3, \ldots$, and its sum is $1 / x$.

\section*{Uniform convergence}
11.27. Find the domain of convergence of $(1-x)+x(1-x)+x^{2}(1-x)+\cdots$.

\section*{Method 1:}
Sum of first $n$ terms $S_{n}(x)=(1-x)+x(1-x)+x^{2}(1-x)+\cdots+x^{n-1}(1-x)$

$$
\begin{aligned}
& =1-x+x-x^{2}+x^{2}+\cdots+x^{n-1}-x^{n} \\
& =1-x^{n}
\end{aligned}
$$

If $|x|<1, \lim _{n \rightarrow \infty} S_{n}(x)=\lim _{n \rightarrow \infty}\left(1-x^{n}\right)=1$.

If $|x|>1 . \lim _{n \rightarrow \infty} S_{n}(x)$ does not exist.

If $x=1, S_{n}(x)=0$ and $\lim _{n \rightarrow \infty} S_{n}(x)=0$.

If $x=-1, S_{n}(x)=1-(-1)^{n}$ and $\lim _{n \rightarrow \infty} S_{n}(x)$ does not exist.

Thus, the series converges for $|x|<1$ and $x=1$, i.e., for $-1<x \leqq 1$.

Method 2, using the ratio test: The series converges if $x=1$. If $x \neq 1$ and $u_{n}=x^{n-1}(1-x)$, then $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}|x|$.

Thus, the series converges if $|x|<1$ and diverges if $|x|>1$. The test fails if $|x|=1$. If $x=1$, the series converges; if $x=-1$, the series diverges. Then the series converges for $-1<x \leqq 1$.

11.28. Investigate the uniform convergence of the series of Problem 11.27 in the interval (a) $-\frac{1}{2}<x<\frac{1}{2}$,

(b) $-\frac{1}{2} \leqq x \leqq \frac{1}{2}$, (c) $-.99 \leqq x \leqq .99$, (d) $-1<x<1$, and (e) $0 \leqq x<2$. (a) By Problem 11.27, $S_{n}(x)=1-x^{n}, S(x)=\lim _{n \rightarrow \infty} S_{n}(x)=1$ if $-\frac{1}{2}<x<\frac{1}{2}$; thus, the series converges in\\
this interval. We have

$$
\text { Remainder after } n \text { terms }=R_{n}(x)=S(x)=S_{n}(x)=1-\left(1-x^{n}\right)=x^{n}
$$

The series is uniformly convergent in the interval if given any $\epsilon>0$ we can find $N$ dependent on $\epsilon$, but not on $x$, such that $\left|R_{n}(x)\right|<\epsilon$ for all $n>N$. Now

$$
\left|R_{n}(x)\right|=\left|x^{n}\right|=|x|^{n}<\varepsilon \quad \text { when } \quad n \ln |x|<\ln \varepsilon \quad \text { or } \quad n>\frac{\ln \varepsilon}{\ln |x|}
$$

since division by $\ln |x|$ (which is negative, since $|x|<\frac{1}{2}$ ) reverses the sense of the inequality.

But if $|x|<\frac{1}{2}, \ln |x|<\ln \left(\frac{1}{2}\right)$, and $n>\frac{\ln \varepsilon}{\ln |x|}>\frac{\ln \varepsilon}{\ln \left(\frac{1}{2}\right)}=N$. Thus, since $N$ is independent of $x$, the series is uniformly convergent in the interval. (b) In this case $|x| \leqq \frac{1}{2} \cdot \ln |x| \leqq \ln \left(\frac{1}{2}\right)$, and $n>\frac{\ln \varepsilon}{\ln |x|} \geqq \frac{\ln \varepsilon}{\ln \left(\frac{1}{2}\right)}=N$, so that the series is also uniformly\\
convergent in $-\frac{1}{2} \leqq x \leqq \frac{1}{2}$.

(c) Reasoning similarly, with $\frac{1}{2}$ replaced by .99 , shows that the series is uniformly convergent in $-.99 \leqq$ $x \leqq 99$.

(d) The arguments used here break down in this case, since $\frac{\ln \varepsilon}{\ln |x|}$ can be made larger than any positive number by choosing $|x|$ sufficiently close to 1 . Thus, no $\mathrm{N}$ exists and it follows that the series is not uniformly convergent in $-1<x<1$.

(e) Since the series does not even converge at all points in this interval, it cannot converge uniformly in the interval.

11.29. Discuss the continuity of the sum function $S(x)=\lim _{n \rightarrow \infty} S_{n}(x)$ of Problem 11.27 for the interval $0 \leqq x \leqq 1$.

$$
\text { If } 0 \leqq x<1, S(x)=\lim _{n \rightarrow \infty} S_{n}(x)=\lim _{n \rightarrow \infty}\left(1-x^{n}\right)=1
$$

If $x=1, S_{n}(x)=0$ and $\mathrm{S}(x)=0$.

\includegraphics[max width=\textwidth, center]{2024_04_03_ffb6ac533fe0a53b3ceeg-314}\\
$0 \leqq x<1$.

In Problem 11.34 it is shown that if a series is uniformly convergent in an interval, the sum function $S(x)$ must be continuous in the interval. It follows that if the sum function is not continuous in an interval, the series cannot be uniformly convergent. This fact is often used to demonstrate the nonuniform convergence of a series (or sequence).

11.30. Investigate the uniform convergence of $x^{2}+\frac{x^{2}}{1+x^{2}}+\frac{x^{2}}{\left(1+x^{2}\right)^{2}}+\cdots+\frac{x^{2}}{\left(1+x^{2}\right)^{n}}+\cdots$.

Suppose $x \neq 0$. Then the series is a geometric series with ratio $1 /\left(1+x^{2}\right)$ whose sum is (see Problem 2.25).

$$
S(x)=\frac{x^{2}}{1-1 /\left(1+x^{2}\right)}=1+x^{2}
$$

If $x=0$, the sum of the first $n$ terms is $S_{n}(0)=0$; hence, $S(0)=\lim _{n \rightarrow \infty} S_{n}(0)=0$.

Since $\lim _{x \rightarrow 0} S(x)=1 \neq S(0), S(x)$ is discontinuous at $x=0$. Then, by Problem 11.34, the series cannot be uniformly convergent in any interval which includes $x=0$, although it is (absolutely) convergent in any interval. However, it is uniformly convergent in any interval which excludes $x=0$.

This can also be shown directly (see Problem 11.93).

\section*{Weierstrass $M$ test}
11.31. Prove the Weierstrass $M$ test; i.e., if $\left|u_{n}(\mathrm{x})\right| \leq M_{n}, n=1,2,3, \ldots$, where $M_{n}$ are positive constants such that $\Sigma M_{n}$ converges, then $\Sigma u_{n}(\mathrm{x})$ is uniformly (and absolutely) convergent.

The remainder of the series $\Sigma u_{n}(x)$ after $n$ terms is $\left.R_{n}(x)=u_{n+1}(x)+u_{n+2} x\right)+\cdots$. Now

$$
\left|R_{n}(x)\right|=\left|u_{n+1}(x)+u_{n+2}(x)+\cdots\right| \leqq\left|u_{n+1}(x)\right|+\left|u_{n+2}(x)\right|+\cdots \leqq M_{n+1}+M_{n+2}+\cdots
$$

But $M_{n+1}+M_{n+2}+\cdots$ can be made less than $\epsilon$ by choosing $n>N$, since $\Sigma M_{n}$ converges. Since $N$ is clearly independent of $x$, we have $\left|R_{n}(x)\right|<\epsilon$ for $n>N$, and the series is uniformly convergent. The absolute convergence follows at once from the comparison test.

11.32. Test for uniform convergence: (a) $\sum_{n=1}^{\infty} \frac{\cos n x}{n^{4}}$, (b) $\sum_{n=1}^{\infty} \frac{x^{n}}{n^{3 / 2}}$, (c) $\sum_{n=1}^{\infty} \frac{\sin x^{n}}{n}$, and (d) $\sum_{n=1}^{\infty} \frac{1}{n^{2}+x^{2}}$,

(a) $\left|\frac{\cos n x}{n^{4}}\right| \leqq \frac{1}{n^{4}}=M_{n}$. Then, since $\Sigma M$ nn converges (p series with $\mathrm{p}=4>1$ ), the series is uniformly (and absolutely) convergent for all $x$ by the $\mathrm{M}$ test.

(b) By the ratio test, the series converges in the interval $-1 \leqq x \leqq 1$; i.e., $|x| \leqq 1$.

For all $x$ in this interval, $\left|\frac{x^{n}}{n^{3 / 2}}\right|=\frac{|x|^{n}}{n^{3 / 2}} \leqq \frac{1}{n^{3 / 2}}$. Choosing $M_{n}=\frac{1}{n^{3 / 2}}$, we see that $\Sigma M_{n}$ converges. Thus, the given series converges uniformly for $-1 \leqq x \leqq 1$ by the $M$ test.

(c) $\left|\frac{\sin n x}{n^{4}}\right| \leqq \frac{1}{n}$. However, $\Sigma M_{n}$ where $M_{n}=\frac{1}{n}$ does not converge. The $M$ test cannot be used in this case and we cannot conclude anything about the uniform convergence by this test (see, however, Problem

(d) $\left|\frac{1}{n^{2}+x^{2}}\right| \leqq \frac{1}{n^{2}}$, and $\sum \frac{1}{n^{2}}$ converges. Then, by the $\mathrm{M}$ test, the given series converges uniformly for all $x$.

11.33. If a power series $\Sigma a_{n} x^{n}$ converges for $x=x_{0}$, Prove that it converges. (a) absolutely in the interval $|x| \leqq$ $\left|x_{0}\right|$ and (b) uniformly in the interval $|x| \leqq\left|x_{1}\right|$, where $\left|x_{1}\right|<\left|x_{0}\right|$.

(a) Since $\Sigma a_{n} x^{n}{ }_{0}$ converges, $\lim _{n \rightarrow \infty} a_{n} x_{0}^{n}=0$, and so we can make $\left|a_{n} x_{0}^{n}\right|<1$ by choosing $n$ large enough; i.e.,


\begin{align*}
& \left|a_{n}\right|<\left|a_{n}\right|<\frac{1}{\left|x_{0}\right|^{n}} \text { for } n>N \text {. Then } \\
& \qquad \sum_{N+1}^{\infty}\left|a_{n} x^{n}\right|=\sum_{N+1}^{\infty}\left|a_{n}\right||x|^{n}<\sum_{N+1}^{\infty} \frac{|x|^{n}}{\left|x_{0}\right|^{n}} \tag{1}
\end{align*}


Since the last series in Equation (1) converges for $|x|<\left|x_{0}\right|$, it follows by the comparison test that the first series converges; i.e., the given series is absolutely convergent.

(b) Let $M_{n}=\frac{\left|x_{1}\right|^{n}}{\left|x_{0}\right|^{n}}$. Then $\Sigma M_{n}$ converges, since $\left|x_{1}\right|<\left|x_{0}\right|$. As in (a), $\left|a_{n} x^{n}\right|<M_{n}$ for $|x| \leqq\left|x_{1}\right|$, so that by the Weierstrass $M$ test, $\Sigma a_{n} x^{n}$ is uniformly convergent.

It follows that a power series is uniformly convergent in any interval within its interval of convergence.

\section*{Theorems on uniform convergence}
\subsection*{11.34. Prove Theorem 6, Page 284.}
We must show that $S(x)$ is continuous in $[a, b]$.

Now $S(x)=S_{n}(x)+R_{n}(x)$, so that $S(x+h)=S_{n}(x+h)+R_{n}(x+h)$ and thus,


\begin{equation*}
\left.S(x+h)-S(x)=S_{n}(x)+h\right)-S_{n}(x)+R_{n}(x+h)-R_{n}(\mathrm{x}) \tag{1}
\end{equation*}


where we choose $h$ so that both $x$ and $x+h$ lie in $[a, b]$ (if $x=b$, for example, this will require $h<0$ ).

Since $S_{n}(x)$ is a sum of a finite number of continuous functions, it must also be continuous. Then, given $\epsilon>0$, we can find $\delta$ so that


\begin{equation*}
\left|S_{n}(x+h)-S_{n}(x)\right|<\epsilon / 3 \text { whenever }|h|<\delta \tag{2}
\end{equation*}


Since the series, by hypothesis, is uniformly convergent, we can choose $N$ so that


\begin{equation*}
\left|R_{n}(x)\right|<\varepsilon / 3 \text { and }|| R_{n}(x+h) \mid<\varepsilon / 3 \text { for } n>N \tag{3}
\end{equation*}


Then from Equations (1), (2), and (3),

$$
\left.|S(x+h)-S(x)| \leqq \mid S_{n}(x)+h\right)-S_{n}(x)|+| R_{n}(x+h)|+| R_{n}(x) \mid<\epsilon
$$

for $|h|<\delta$, and so the continuity is established.

\subsection*{11.35. Prove Theorem 7, Page 285.}
If a function is continuous in $[a, b]$, its integral exists. Then, since $S(x), S_{n}(x)$ and $R_{n}(x)$ are continuous,

$$
\int_{a}^{a} S(x) \int_{a}^{b} S_{n}(x) d x+\int_{a}^{b} R_{n}(x) d x
$$

To prove the theorem we must show that

$$
\left|\int_{a}^{b} S(x) d x-\int_{a}^{b} S_{n}(x) d x\right|=\left|\int_{a}^{b} R_{n}(x) d x\right|
$$

can be made arbitrarily small by choosing $n$ large enough. This, however, follows at once, since by the uniform convergence of the series we can make $\left|R_{n}(x)\right|<\epsilon /(b-a)$ for $n>N$ independent of $x$ in $[a, b]$, and so

$$
\left|\int_{a}^{b} R_{n}(x) d x\right| \leqq \int_{a}^{b}\left|R_{n}(x)\right| d x<\int_{a}^{b} \frac{\varepsilon}{b-a} d x=\varepsilon
$$

This is equivalent to the statements

$$
\int_{a}^{b} S(x) d x=\lim _{n \rightarrow \infty} \int_{a}^{b} S_{n}(x) d x \quad \text { or } \quad \lim _{n \rightarrow \infty} \int_{a}^{b} S_{n}(x) d x=\int_{a}^{b}\left\{\lim _{n \rightarrow \infty} S_{n}(x)\right\} d x
$$

11.36. Prove Theorem 8, Page 285.

Let $g(x)=\sum_{n=1}^{\infty} u_{n}^{\prime}(x)$. Since, by hypothesis, this series converges uniformly in $[a, b]$, we can integrate term by term (by Problem 11.35) to obtain

$$
\begin{aligned}
\int_{a}^{x} g(x) d x=\sum_{n=1}^{\infty} \int_{a}^{x} u_{n}^{\prime}(x) d x & =\sum_{n=1}^{\infty}\left\{u_{n}(x)-u_{n}(a)\right\} \\
& =\sum_{n=1}^{\infty} u_{n}(x)-\sum_{n=1}^{\infty} u_{n}(a)=S(x)-S(a)
\end{aligned}
$$

because, by hypothesis, $\sum_{n=1}^{\infty} u_{n}(x)$ converges to $S(x)$ in $[a, b]$. theorem.

Differentiating both sides of $\int_{0}^{x} g(x) d x=S(x)-S(a)$ then shows that $g(x)=S^{\prime}(x)$, which proves the

11.37. Let $S_{n}(\mathrm{x})=n x e^{-n x 2}, n=1,2,3, \ldots, 0 \leqq x \leqq 1$. (a) Determine whether $\lim _{n \rightarrow \infty} \int_{0}^{1} S_{n}(x) d x=\int_{0}^{1} \lim _{n \rightarrow \infty} S_{n}(x) d x$.\\
(b) Explain the result in (a).

(a) $\int_{0}^{1} S_{n}(x) d x=\int_{0}^{1} n x e^{-n x^{2}} d x=-\left.\frac{1}{2} e^{-n x^{2}}\right|_{0} ^{1}=\frac{1}{2}\left(1-e^{-n}\right)$. Then

$$
\lim _{n \rightarrow \infty} \int_{0}^{1} S_{n}(x) d x=\lim _{n \rightarrow \infty} \frac{1}{2}\left(1-e^{-n}\right)=\frac{1}{2}
$$

$S(x)=\lim _{n \rightarrow \infty} S_{n}(x)=\lim _{n \rightarrow \infty} n x e^{-n x^{2}}=0$. whether $x=0$ or $<x \leqq 1 . \quad$ Then.

$$
\int_{0}^{1} S(x) d x=0
$$

It follows that $\lim _{n \rightarrow \infty} \int_{0}^{1} S_{n}(x) d x \neq \int_{0}^{1} \lim _{n \rightarrow \infty} S_{n}(x) d x$; i.e., the limit cannot be taken under the integral sign.

(b) The reason for the result in (a) is that although the sequence $\operatorname{Sn}(\mathrm{x})$ converges to 0 . it does not converge uniformly to 0 . To show this, observe that the function $n x e^{-n x^{2}}$ has a maximum at $x=1 / \sqrt{2 n}$ (by the usual rules of elementary calculus), the value of this maximum being $\sqrt{\frac{1}{2} n} e^{-1 / 2}$. Hence, as $\mathrm{n} \rightarrow \infty$, $\operatorname{Sn}(\mathrm{x})$ cannot be made arbitrarily small for all $\mathrm{x}$ and so cannot converge uniformly to 0 .

11.38. Let $f(x)=\sum_{n=1}^{\infty} \frac{\sin n x}{n^{3}}$. Prove that $\int_{0}^{\pi} f(x) d x=2 \sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{4}}$.

We have $\left|\frac{\sin n x}{n^{3}}\right| \leqq \frac{1}{n^{3}}$. Then, by the Weierstrass $M$ test, the series is uniformly convergent for all $x$, in particular $0 \leqq x \leqq \pi$, and can be integrated term by term. Thus,

$$
\begin{aligned}
\int_{0}^{\pi} f(x) d x & =\int_{0}^{\pi}\left(\sum_{n=1}^{\infty} \frac{\sin n x}{n^{3}}\right) d x \sum_{n=1}^{\infty} \int_{0}^{\pi} \frac{\sin n x}{n^{3}} d x \\
& =\sum_{n=1}^{\infty} \frac{1-\cos n \pi}{n^{4}}=2\left(\frac{1}{1^{4}}+\frac{1}{3^{4}}+\frac{1}{5^{4}}+\cdots\right)=2 \sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{4}}
\end{aligned}
$$

\section*{Power series}
11.39. Prove that both the power series $\sum_{n=0}^{\infty} a_{n} x^{n}$ and the corresponding series of derivatives $\sum_{n=0}^{\infty} n a_{n} x^{n-1}$ have\\
the same radius of convergence.

Let $R>0$ be the radius of convergence of $\Sigma a_{n} x^{n}$. Let $0<\left|x_{0}\right|<R$. Then, as in Problem 11.33, we can choose $N$ as that ||$a_{n} \left\lvert\,<\frac{1}{\left|x_{0}\right|^{n}}\right.$ for $n>N$.

Thus, the terms of the series $\Sigma\left|n a_{n} x^{n-1}\right|=\Sigma n\left|a_{n}\right||x|^{n-1}$ can for $n>N$ be made less than corresponding terms of the series $\sum n \frac{|x|^{n-1}}{\left|x_{0}\right|^{n}}$, which converges, by the ratio test, for $|x|<\left|x_{0}\right|<R$.

Hence, $\Sigma n a_{n} x^{n-1}$ converges absolutely for all points $x_{0}$ (no matter how close $\left|x_{0}\right|$ is to $R$ ).

If, however, $|x|>R, \lim _{n \rightarrow \infty} a_{n} x^{n} \neq 0$ and thus $\lim _{n \rightarrow \infty} n a_{n} x^{n-1} \neq 0$, so that $\Sigma n a_{n} x^{n-1}$ does not converge.

Thus, $R$ is the radius of convergence of $\sum n a_{n} x^{n-1}$.

Note that the series of derivatives may or may not converge for values of $x$ such that $|x|=R$.

11.40. Illustrate Problem 11.39 by using the series $\sum_{n=1}^{\infty} \frac{x^{n}}{n^{2} \cdot 3^{n}}$.

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{x^{n+1}}{(n+1)^{2} \cdot 3^{n+1}} \cdot \frac{n^{2} \cdot 3 n}{x^{n}}\right|=\lim _{n \rightarrow \infty} \frac{n^{2}}{3(n+1)^{2}}|x|=\frac{|x|}{3}
$$

so that the series converges for $|x|<3$. At $x= \pm 3$ the series also converges, so that the interval of convergence is $-3 \leqq x \leqq 3$.

The series of derivatives is

$$
\sum_{n=1}^{\infty} \frac{n x^{n-1}}{n^{2} \cdot 3 n}=\sum_{n=1}^{\infty} \frac{x^{n-1}}{n \cdot 3 n}
$$

By Problem 11.25(a), this has the interval of convergence $-3 \leqq x<3$.

The two series have the same radius of convergence, i.e., $R=3$, although they do not have the same interval of convergence.

Note that the result of Problem 11.39 can also be proved by the ratio test if this test is applicable. The proof given there, however, applies even when the test is not applicable, as in the series in Problem 11.22.

11.41. Prove that in any interval within its interval of convergence, a power series (a) represents a continuous function, say, $\mathrm{f}(\mathrm{x})$; (b) can be integrated term by term to yield the integral of $\mathrm{f}(\mathrm{x})$; and (c) can be differentiated term by term to yield the derivative of $f(x)$.

We consider the power series $\Sigma a_{n} x^{n}$, although analogous results hold for $\Sigma a_{n}(x-a)^{n}$.

(a) This follows from Problem 11.33 and 11.34, and the fact that each term anxn of the series is continuous.

(b) This follows from Problems 11.33 and 11.35, and the fact that each term anxn of the series is continuous and thus integrable.

(c) From Problem 11.39, the series of derivatives of a power series always converges within the interval of convergence of the original power series and therefore is uniformly convergent within this interval. Thus, the required result follows from Problems 11.33 and 11.36.

If a power series converges at one (or both) endpoints of the interval of convergence, it is possible to establish (a) and (b) to include the endpoint (or endpoints). See Problem 11.42.

11.42. Prove Abel's theorem that if a power series converges at an endpoint of its interval of convergence, then the interval of uniform convergence includes this endpoint.

For simplicity in the proof, we assume the power series to be $\sum_{k=0}^{\infty} a_{k} x^{k}$ with the endpoint of its interval of convergence at $x=1$, so that the series surely converges for $0 \leqq x \leqq 1$. Then we must show that the series converges uniformly in this interval.

Let

$$
R_{n}(x)=a_{n} x^{n}+a_{n+1} x^{n+1}+a_{n+2} x^{n+2}+\cdots, \quad R_{n}=a_{n}+a a_{n+2}+\cdots
$$

To prove the required result we must show that given any $\epsilon>0$, we can find $N$ such that $\mid R_{n \text { all } n>N}$, where $N$ is independent of the particular $x$ in $0 \leqq x \leqq 1$.

Now

$$
\begin{aligned}
R_{n}(x) & =\left(R_{n}-R_{n+1}\right) x^{n}+\left(R_{n+1}-R_{n+2}\right) x^{n+1}+\left(R_{n+2}-R_{n+3}\right) x^{n+2}+\cdots \\
& =R_{n} x^{n}+R_{n+1}\left(x^{n+1}-x^{n}\right)+R_{n+2}\left(x^{n+2}-x^{n+1}\right)+\cdots \\
& =x^{n}\left\{R_{n}-(1-x)\left(R_{n+1}+R_{n+2} x+R_{n+3} x^{2}+\cdots\right)\right\}
\end{aligned}
$$

Hence, for $0 \leqq x<1$,


\begin{equation*}
\left|R_{n}(x)\right| \leqq\left|R_{n}\right|+(1-x)\left(\left|R_{n+1}\right|+\left|R_{n+2}\right| x+\left|R_{n}\right| R n+3 \mid x^{2}+\cdots\right) \tag{1}
\end{equation*}


Since $\Sigma a_{k}$ converges by hypothesis, it follows that given $\epsilon>0$, we can choose $N$ such that $\left|R_{k}\right|<\epsilon / 2$ for all $k \geqq n$. Then for $n>N$ we have, from Equation (1),


\begin{equation*}
\left|R_{n}(x)\right| \leqq \frac{\epsilon}{2}+(1-x)\left(\frac{\epsilon}{2}+\frac{\epsilon}{2} x+\frac{\epsilon}{2} x^{2}+\cdots\right)=\frac{\epsilon}{2}+\frac{\epsilon}{2}=\in \tag{2}
\end{equation*}


since $(1-x)\left(1+x+x^{2}+x^{3}+\ldots\right)=1$ (if $0 \leq x<1$ )

Also, for $x=1,\left|R_{n}(x)\right|=\left|R_{n}\right|<\epsilon$ for $n>N$.

Thus, $\left|R_{n}(x)\right|<\epsilon$ for all $n>N$, where $N$ is independent of the value of $x$ in $0 \leqq x \leqq 1$, and the required result follows.

Extensions to other power series are easily made.

11.43. Prove Abel's limit theorem (see Page 286).

As in Problem 11.42, assume the power series to be $\sum_{k=1}^{\infty} a_{k} x^{k}$, convergent for $0 \leqq x \leqq 1$.

Then we must show that $\lim _{x \rightarrow 1-} \sum_{k=0}^{\infty} a_{k} x^{k}=\sum_{k=0}^{\infty} a_{k}$.

This follows at once from Problem 11.42, which shows that $\Sigma a_{k} x^{k}$ is uniformly convergent for $0 \leqq x \leqq 1$, and from Problem 11.34, which shows that $\sum a_{k} x^{k}$ is continuous at $x=1$.

Extensions to other power series are easily made.

11.44. (a) Prove that $\tan ^{-1} x=x-\frac{x^{3}}{3}+\frac{x^{5}}{5}-\frac{x^{7}}{7}+\cdots$ where the series is uniformly convergent in $-1 \leqq x \leqq 1$.

(b) Prove that $\frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots$

(a) By Problem 2.25, with $\mathrm{r}=-\mathrm{x} 2$ and $\mathrm{a}=1$, we have


\begin{equation*}
\frac{1}{1+x^{2}}=1-x^{2}+x^{4}-x^{6}+\cdots-1<x<1 \tag{1}
\end{equation*}


Integrating from 0 to $x$, where $-1<x<1$, yields


\begin{equation*}
\int_{0}^{x} \frac{d x}{1+x^{2}}=\tan ^{-1} x=x-\frac{x^{3}}{3}+\frac{x^{5}}{5}-\frac{x^{7}}{7}+\cdots \tag{2}
\end{equation*}


using Problems 11.33 and 11.35 .

Since the series on the right of Equation (2) converges for $x= \pm 1$, it follows by Problem 11.42 that the series is uniformly convergent in $-1 \leqq x \leqq 1$ and represents $\tan ^{-1} x$ in this interval

(b) By Problem 11.43 and (a), we have

$$
\lim _{x \rightarrow 1-} \tan ^{-1} x=\lim _{x \rightarrow 1-}\left(x-\frac{x^{3}}{3}+\frac{x^{5}}{5}-\frac{x^{7}}{7}+\cdots\right) \text { or } \frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots
$$

11.45. Evaluate $\int_{0}^{1} \frac{1-e^{-x 2}}{x^{2}} d x$ to three-decimal-place accuracy.

We have $e^{u} 1+u+\frac{u^{2}}{2 !}+\frac{u^{3}}{3 !}+\frac{u^{4}}{4 !}+\frac{u^{5}}{5 !}+\cdots, \quad-\infty<u<\infty$.

Then, if $u=-x^{2}, e^{-x^{2}}=1-x^{2}+\frac{x^{4}}{2 !}-\frac{x^{6}}{3 !}+\frac{x^{8}}{3 !}=\frac{x^{10}}{5 !}+\cdots, \quad-\infty<x<\infty$.

Thus, $\frac{1-e^{-x^{2}}}{x^{2}}=1-\frac{x^{2}}{2 !}+\frac{x^{4}}{3 !}-\frac{x^{6}}{4 !}+\frac{x^{8}}{5 !}-\cdots$.

Since the series converges for all $x$ and so, in particular, converges uniformly for $0 \leqq x \leqq 1$, we can integrate term by term to obtain

$$
\begin{aligned}
\int_{0}^{1} \frac{1-e^{-x^{2}}}{x^{2}} d x & =x-\frac{x^{3}}{3 \cdot 2 !}+\frac{x^{5}}{5 \cdot 3 !}-\frac{x^{7}}{7 \cdot 4 !}+\frac{x^{9}}{9 \cdot 5 !}-\left.\cdots\right|_{0} ^{1} \\
& =1-\frac{1}{3 \cdot 2 !}+\frac{1}{5 \cdot 3 !}-\frac{1}{7 \cdot 4 !}+\frac{1}{9 \cdot 5 !}-\cdots \\
& =1-0.16666+0.03333-0.00595+0.00092-\cdots=0.862
\end{aligned}
$$

Note that the error made in adding the first four terms of the alternating series is less than the fifth term, i.e., less than 0.001 (see Problem 11.15).

\section*{Miscellaneous problems}
11.46. Prove that $y=J_{p}$ (x) defined by Equation (16), Page 287, satisfies Bessel's differential equation:

$$
x^{2} y \cdot+x y^{\prime}+\left(x^{2}-p^{2}\right) y=0
$$

The series for $J_{p}(x)$ converges for all $x$ [see Problem 11.10 (a)]. Since a power series can be differentiated term by term within its interval of convergence, we have for all $x$,

$$
\begin{aligned}
& y=\sum_{n=0}^{\infty} \frac{(-1)^{n} x^{p+2 n}}{2^{p+2 n} n !(n+p) !} \\
& y^{\prime}=\sum_{n=0}^{\infty} \frac{(-1)^{n}(p+2 n) x^{p+2 n-1}}{2^{p+2 n} n !(n+p) !} \\
& y^{\prime \prime}=\sum_{n=0}^{\infty} \frac{(-1)^{n}(p+2 n)(p+2 n-1) x^{p+2 n-2}}{2^{p+2 n} n !(n+p) !}
\end{aligned}
$$

Then,

$$
\begin{aligned}
\left(x^{2}-p^{2}\right) y & =\sum_{n=0}^{\infty} \frac{(-1)^{n} x^{p+2 n+2}}{2^{p+2 n} n !(n+p) !}-\sum_{n=0}^{\infty} \frac{(-1)^{n} p^{2} x^{p+2 n}}{2^{p+2 n} n !(n+p) !} \\
x y^{\prime} & =\sum_{n=0}^{\infty} \frac{(-1)^{n}(p+2 n) x^{p+2 n}}{2^{p+2 n} n !(n+p) !} \\
y^{\prime \prime} & =\sum_{n=0}^{\infty} \frac{(-1)^{n}(p+2 n)(p+2 n-1) x^{p+2 n}}{2^{p+2 n} n !(n+p) !}
\end{aligned}
$$

Adding,

$$
\begin{aligned}
x^{2} y^{\prime \prime}+x y^{\prime}+\left(x^{2}-p^{2}\right) y & =\sum_{n=0}^{\infty} \frac{(-1)^{n} x^{p+2 n+2}}{2^{p+2 n} n !(n+p) !} \\
& +\sum_{n=0}^{\infty} \frac{(-1)^{n}\left[-p^{2}+(p+2 n)+(p+2 n)(p+2 n-1)\right] x^{p+2 n}}{2^{p+2 n} n !(n+p) !} \\
& =\sum_{n=0}^{\infty} \frac{(-1)^{n} x^{p+2 n+2}}{2^{p+2 n} n !(n+p) !}+\sum_{n=0}^{\infty} \frac{(-1)^{n}[4 n(n+p)] x^{p+2 n}}{2^{p+2 n} n !(n+p) !} \\
& =\sum_{n=1}^{\infty} \frac{(-1)^{n} 4 x^{p+2 n}}{2^{p+2 n-2}(n-1) !(n-1+p) !}+\sum_{n=1}^{\infty} \frac{(-1)^{n} 4 x^{p+2 n}}{2^{p+2 n}(n-1) !(n+p-1) !} \\
& =-\sum_{n=1}^{\infty} \frac{(-1)^{n} 4 x^{p+2 n}}{2^{p+2 n}(n-1) !(n+1-p) !}+\sum_{n=1}^{\infty} \frac{(-1)^{n} 4 x^{p+2 n}}{2^{p+2 n}(n-1) !(n+p-1) !} \\
& =0
\end{aligned}
$$

11.47. Test for convergence the complex power series $\sum_{n=1}^{\infty} \frac{z^{n-1}}{n^{3} \cdot 3^{n-1}}$.

Since $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{z_{n}}{(n+1)^{3} \cdot 3^{n}} \cdot \frac{n^{3} \cdot 3^{n-1}}{z^{n-1}}\right|=\lim _{n \rightarrow \infty} \frac{n^{3}}{3(n+1)^{3}}|z|=\frac{|z|}{3}$, the series converges for $\frac{|z|}{3}<1$, i.e., $|z|<3$, and diverges for $|z|>3$.\\
For $|z|=3$, the series of absolute values is $\sum_{n=1}^{\infty} \frac{|z|^{n-1}}{n^{3} \cdot 3^{n-1}}=\sum_{n=1}^{\infty} \frac{1}{n^{3}}$, so that the series is absolutely con-\\
vergent and thus convergent for $|z|=3$.

Thus, the series converges within and on the circle $|z|=3$.

11.48. Assuming the power series for $e^{x}$ holds for complex numbers, show that $e^{i x}=\cos x+i \sin x$.

Letting $z=i x$ in $e^{2}=1+z+\frac{z^{2}}{2 !}+\frac{z^{3}}{3 !}+\cdots$, we have

$$
\begin{aligned}
e^{i x} & =1+i x+\frac{i^{2} x^{2}}{2 !}+\frac{i^{3} x^{3}}{3 !}+\cdots=\left(1-\frac{x^{2}}{2 !}+\frac{x^{4}}{4 !}-\cdots\right)+i\left(x-\frac{x^{3}}{3 !}+\frac{x^{5}}{5 !}-\cdots\right) x \\
& =\cos x+i \sin x
\end{aligned}
$$

Similarly, $e^{-i x}=\cos x-i \sin x$. The results are called Euler's identities.

11.49. Prove that $\lim _{n \rightarrow \infty}\left(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{n}-\operatorname{In} n\right)$ exists.

Letting $f(x)=1 / x$ in Equation (1), Problem 11.11, we find

$$
\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{M} \leqq \ln M \leqq 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{M-1}
$$

from which, on replacing $M$ by $n$, we have

$$
\frac{1}{n} \leqq 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{n}-\operatorname{In} n \leqq 1
$$

Thus, the sequence $S_{n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{n}-\ln n$ is bounded by 0 and 1 .

Consider $S_{n+1}-S_{n}=\frac{1}{n+1}-\ln \left(\frac{n+1}{n}\right)$. By integrating the inequality $\frac{1}{n+1} \leqq \frac{1}{x} \leqq \frac{1}{n}$ with respect to $x$ from $n$ to $n+1$, we have

$$
\frac{1}{n+1} \leqq \ln \left(\frac{n+1}{n}\right) \leqq \frac{1}{n} \quad \text { or } \quad \frac{1}{n+1}-\frac{1}{n} \leqq \frac{1}{n+1}-\ln \left(\frac{n+1}{n}\right) \leqq 0
$$

i.e., $S_{n+1}-S_{n} \leqq 0$, so that $S_{n}$ is monotonic decreasing.

Since $S_{n} \overline{\bar{i}}$ bounded and monotonic decreasing, it has a limit. This limit, denoted by $\gamma$, is equal to $0.577215 \ldots$. and is called Euler's constant. It is not yet known whether $\gamma$ is rational or not.

11.50. Prove that the infinite product $\prod_{k=1}^{\infty}\left(1+u_{k}\right)$, converges $\sum_{k=1}^{\infty} u_{k}$ converges.

According to the Taylor series for $e^{x}$ (Page 289), $1+x \leqq e^{\mathrm{x}}$ for $x>0$, so that

$$
P_{n}=\prod_{k=1}^{n}\left(1+u_{k}\right)=\left(1+u_{1}\right)\left(1+u_{2}\right) \cdots\left(1+u_{n}\right) \leqq e^{u_{1}} \cdot e^{u_{2}} \cdots e^{u_{n}}=e^{u_{1}+u_{2}+\cdots u_{n}}
$$

Since $u_{1}+u_{2}+\cdots$ converges, it follows that $P_{n}$ is a bounded monotonic increasing sequence and so has a limit, thus proving the required result.

11.51. Prove that the series $1-1+1-1+1-1+\cdots$ is $C-1$ summable to $1 / 2$.

The sequence of partial sums is $1,0,1,0,1,0, \ldots$.

Then $S_{1}=1, \frac{S_{1}+S_{2}}{2}=\frac{1+0}{2}=\frac{1}{2}, \frac{S_{1}+S_{2}+S_{3}}{3}=\frac{1+0+1}{3}=\frac{2}{3}, \cdots$.

Continuing in this manner, we obtain the sequence $1 . \frac{1}{2}, \frac{2}{3}, \frac{1}{2}, \frac{3}{5}, \frac{1}{2}, \ldots$, the $n$th term being $T_{n}=\left\{\begin{array}{ll}1 / 2 & \text { if } n \text { is even } \\ n /(2 n-1) & \text { if } n \text { is odd }\end{array}\right.$. Thus, $\lim _{n \rightarrow \infty} T_{n}=\frac{1}{2}$ and the required result follows.

11.52. (a) If $f^{(n+1)}(\mathrm{x})$ is continuous in $[a, b]$ prove that for $c$ in $[a, b], f(x)=f(c)+f^{\prime}(c)(x-c)+$ $\frac{1}{2 !} f^{\prime \prime}(c)(x-c)^{2}+\cdots+\frac{1}{n !} f^{(n)}(c)(x-c)^{n}+\frac{1}{n !} \int_{c}^{x}(x-1)^{n} f^{(n+1)}(t) d t$. (b) Obtain the Lagrange and Cauchy forms of the remainder in Taylor's formula. (See Page 290.)

The proof of (a) is made using mathematical induction. (See Chapter 1.) The result holds for $n=0$, since

$$
f(x)=f(c)+\int_{c}^{x} f^{\prime}(t) d t=f(c)+f(x)-f(c)
$$

We make the induction assumption that it holds for $n=k$ and then use integration by parts with

$$
d v=\frac{(x-t)^{k}}{k !} d t \text { and } u=f^{k+1}(t)
$$

Then

$$
v=-\frac{(x-t)^{k+1}}{(k+1) !} \text { and } d u=f^{k+2}(t) d t
$$

Thus,

$$
\begin{aligned}
\frac{1}{k !} \int_{C}^{x}(x-t)^{k} f^{(k+1)}(t) d t & =-\left.\frac{f^{k+1}(x-t)^{k+1}}{(k+1) !}\right|_{C} ^{x}+\frac{1}{(k+1) !} \int_{C}^{x}(x-t)^{k+1} f^{(k+2)}(t) d t \\
& =-\frac{f^{k+1}(x-t)^{k+1}}{(k+1) !}+\frac{1}{(k+1) !} \int_{C}^{x}(x-t)^{k+1} f^{(k+2)}(t) d t
\end{aligned}
$$

Having demonstrated that the result holds for $k+1$, we conclude that it holds for all positive integers.

To obtain the Lagrange form of the remainder $R_{n}$, consider the form

$$
f(x)=f(c)+f^{\prime}(c)(x-c)+\frac{1}{2 !} f^{n}(c)(x-c)^{2}+\cdots+\frac{K}{n !}(x-c)^{n}
$$

This is the Taylor polynomial $P_{n-1}(x)$ plus $\frac{K}{n !}(x-c)^{n}$. Also, it could be looked upon as $P_{n}$ except that in the last term, $f^{(n)}$ (c) is replaced by a number $K$ such that for fixed $c$ and $x$ the representation of $f(x)$ is exact. Now define a new function

$$
\Phi(t)=f(t)-f(x)+\sum_{j=1}^{n-1} f^{(j)}(t) \frac{(x-t)^{j}}{j !}+\frac{K(x-t)^{n}}{n !}
$$

The function $\Phi$ satisfies the hypothesis of Rolle's Theorem in that $\Phi(c)=\Phi(x)=0$, the function is continuous on the interval bound by $c$ and $x$, and $\Phi^{\prime}$ exists at each point of the interval. Therefore, there exists $\xi$ in the interval such that $\Phi^{\prime}(\xi)=0$. We proceed to compute $\Phi^{\prime}$ and set it equal to zero.

$$
\Phi^{\prime}(t)=f^{\prime}(t)+\sum_{j=1}^{n-1} f^{(j+1)}(t) \frac{(x-t)^{j}}{j !}-\sum_{j=1}^{n-1} f^{(j)}(t) \frac{(x-t)^{j-1}}{(j-1) !}-\frac{K(x-t)^{n-1}}{(n-1) !}
$$

This reduces to

$$
\Phi^{\prime}(t)=\frac{f^{(n)}(t)}{(n-1) !}(x-t)^{n-1}-\frac{K}{(n-1) !}(x-t)^{n-1}
$$

According to hypothesis, for each $n$ there is $\xi_{n}$ such that

$$
\Phi\left(\xi_{n}\right)=0
$$

Thus,

$$
K=f^{(n)}\left(\xi_{n}\right)
$$

and the Lagrange remainder is

$$
R_{n-1}=\frac{f^{(n)}\left(\xi_{n}\right)}{n !}(x-c)^{n}
$$

or, equivalently,

$$
R_{n}=\frac{1}{(n+1) !} f^{(n+1)}\left(\xi_{n+1}\right)(x-c)^{n+1}
$$

The Cauchy form of the remainder follows immediately by applying the mean value theorem for integrals. (See Page 287.)

11.53. Extend Taylor's theorem to functions of two variables $x$ and $y$.

Define $F(t)=f\left(x_{0}+h t, y_{0}+k t\right)$; then, applying Taylor's theorem for one variable (about $t=0$ ),

$$
F(t)=F(0)+F^{\prime}(0)+\frac{1}{2 !} F^{\prime \prime}(0) t^{2}+\cdots+\frac{1}{n !} F^{(n)}(0) t^{n}+\frac{1}{(n+1) !} F^{(n+1)}(\theta) t^{n+1}, 0<\theta<t
$$

Now let $t=1$

$F(1)=f\left(x_{0}+h, y_{0}+k\right)=F(0)+F^{\prime}(0)+\frac{1}{2 !} F^{\prime \prime}(0)+\cdots+\frac{1}{n !} F^{(n)}(0)+\frac{1}{(n+1) !} F^{(n+1)}(\theta)$

When the derivatives $F^{\prime}(t), \ldots, F^{(n)}(t), F^{(n+1)}(\theta)$ are computed and substituted into the previous expression, the two-variable version of Taylor's formula results. (See Page 290, where this form and notational details can be found.)

11.54. Expand $x^{2}+3 y-2$ in powers of $x-1$ and $y+2$. Use Taylor's formula. with $h=x-x_{0}, k=y-y_{0}$, where $x_{0}=$ 1 and $y_{0}=-2$.

$$
x^{2}+3 y-2=-10-4(x-1)+4(y+2)-2(x-1)^{2}+2(x-1)(y+2)+(x-1)^{2}(y+2) \text { (Check this algebraically.) }
$$

11.55. Prove that $\ln \frac{x+y}{2}=\frac{x+y-2}{2+\theta(x+y-2)}, 0<\theta 1, x>0, y>0$. (Hint: Use Taylor's formula with the linear term as the remainder.)

11.56. Expand $f(x, y)=\sin x y$ in powers of $x-1$ and $y-\frac{\pi}{2}$ to second-degree terms.

$$
1-\frac{1}{8} \pi^{2}(x-1)^{2}-\frac{\pi}{2}(x-1)\left(y-\frac{\pi}{2}\right)-\left(y-\frac{\pi}{2}\right)^{2}
$$

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Convergence And Divergence Of Series Of Constants}
11.57. (a) Prove that the series $\frac{1}{3 \cdot 7}+\frac{1}{7 \cdot 11}+\frac{1}{11 \cdot 15}+\cdots=\sum_{n=1}^{\infty} \frac{1}{(4 n-1)(4 n+3)}$ converges and (b) find its\\
sum. Ans. (b) $1 / 12$

11.58. Prove that the convergence or divergence of a series is not affected by (a) multiplying each term by the same nonzero constant and (b) removing (or adding) a finite number of terms.

11.59. If $\Sigma u_{n}$ and $\Sigma v_{n}$ converge to $A$ and $B$, respectively, prove that $\Sigma\left(u_{n}+v_{n}\right)$ converges to $A+B$.

11.60. Prove that the series $\frac{3}{2}+\left(\frac{3}{2}\right)^{2}+\left(\frac{3}{2}\right)^{3}+\cdots=\sum\left(\frac{3}{2}\right)^{n}$ diverges.

11.61. Find the fallacy: Let $S=1-1+1-1+1-1+\cdots$. Then $S=11-(1-1)-(1-1)-\cdots=1$ and $S=(1-1)$ $+(1-1)+(1-1)+\cdots=0$. Hence, $1=0$.

\section*{Comparison test and quotient test}
11.62. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{1}{n^{2}+1}$, (b) $\sum_{n=1}^{\infty} \frac{n}{4 n^{2}-3}$, (c) $\sum_{n=1}^{\infty} \frac{n+2}{(n+1) \sqrt{n+3}}$, (d) $\sum_{n=1}^{\infty} \frac{3^{n}}{n \cdot 5^{n}}$,

(e) $\sum_{n=1}^{\infty} \frac{1}{5 n-3}$, and (f) $\sum_{n=1}^{\infty} \frac{2 n-1}{(3 n+2) n^{4 / 3}}$.

Ans. (a) convergence (b) divergence (c) divergence (d) convergence (e) divergence (f) convergence

11.63. Investigate the convergence of (a) $\sum_{n=1}^{\infty} \frac{4 n^{2}+5 n-2}{n\left(n^{2}+1\right)^{3 / 2}}$ and (b) $\sum_{n=1}^{\infty} \sqrt{\frac{n-\ln n}{n^{2}+10 n^{3}}}$.

Ans. (a) convergence (b) divergence

11.64. Establish the comparison test for divergence (see Page 280).

11.65. Use the comparison test to prove that (a) $\sum_{n=1}^{\infty} \leqq \frac{1}{n^{p}}$ converges if $p>1$ and diverges if $p \leqq 1$,

(b) $\sum_{n=1}^{\infty} \frac{\tan ^{-1} n}{n}$ diverges, and (c) $\sum_{n=1}^{\infty} \frac{n^{2}}{2 n}$ converges.

11.66. Establish the results (b) and (c) of the quotient test, Page 280.

11.67. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{(\ln n)^{2}}{n^{2}}$, (b) $\sum_{n=1}^{\infty} \sqrt{n \tan ^{-1}\left(1 / n^{3}\right)}$, (c) $\sum_{n=1}^{\infty} \frac{3+\sin n}{n\left(1+e^{-n}\right)}$, and

(d) $\sum_{n=1}^{\infty} n \sin ^{2}(1 / n)$

Ans. (a) convergence (b) divergence

(c) divergence

(d) divergence

11.68. If $\Sigma u_{n}$ converges, where $u_{n} \geqq 0$ for $n>N$, and if $\lim _{n \rightarrow \infty} n u_{n}$ exists, prove that $\lim _{n \rightarrow \infty} n u_{n}=0$.

11.69. (a) Test for convergence $\sum_{n=1}^{\infty} \frac{1}{n^{1+1 / n}}$. (b) Does your answer to (a) contradict the statement about the $p$ series made on Page 266 that $\Sigma 1 / n^{p}$ converges for $p>1$ ?

Ans. (a) divergence

\section*{Integral test}
11.70. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{n^{2}}{2 n^{3}-1}$, (b) $\sum_{n=2}^{\infty} \frac{1}{n(\ln n)^{3}}$, (c) $\sum_{n=1}^{\infty} \frac{n}{2 n}$, (d) $\sum_{n=1}^{\infty} \frac{e^{-\sqrt{n}}}{\sqrt{n}}$, (e) $\sum_{n=2}^{\infty} \frac{\ln n}{n}$, and (f) $\sum_{n=10}^{\infty} \frac{2^{\ln (\ln n)}}{n \ln n}$.

Ans. (a) divergence (b) convergence (c) convergence (d) convergence (e) divergence (f) divergence

11.71. Prove that $\sum_{n=2}^{\infty} \frac{1}{n(\ln n)^{p}}$, where $p$ is a constant, (a) converges if $p>1$ and (b) diverges if $p \leqq 1$.

11.72. Prove that $\frac{9}{8}<\sum_{n=1}^{\infty} \frac{1}{n^{3}}<\frac{5}{4}$.

11.73. Investigate the convergence of $\sum_{n=1}^{\infty} \frac{e^{\tan ^{-1} n}}{n^{2}+1}$. Ans. convergence

11.74. (a) Prove that $\frac{2}{3} n^{3 / 2}+\frac{1}{3} \leqq \sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n} \leqq \frac{2}{3} n^{3 / 2}+n^{1 / 2}-\frac{2}{3}$. (b) Use (a) to estimate the value of $\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{100}$, giving the maximum error. (c) Show how the accuracy in (b) can be improved by estimating, for example, $\sqrt{10}+\sqrt{11}+\cdots+\sqrt{100}$ and adding on the value of $\sqrt{1}+\sqrt{2}+\cdots+\sqrt{9}$ computed to some desired degree of accuracy.

Ans. (b) $671.5 \pm 4.5$

\section*{Alternating series}
11.75. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{2 n}$, (b) $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{n^{2}+2 n+2}$, (c) $\sum_{n=1}^{\infty} \frac{(-1)^{n+1} n}{3 n-1}$, (d) $\sum_{n=1}^{\infty}(-1)^{n} \sin ^{-1} \frac{1}{n}$,\\
and (e) $\sum_{n=2}^{\infty} \frac{(-1)^{n} \sqrt{n}}{\ln n}$. Ans. (a) convergence, (b) convergence, (c) divergence, (d) convergence, (e) divergence

11.76. (a) What is the largest absolute error made in approximating the sum of the series $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{2^{n}(n+1)}$ by the sum of the first five terms? (b) What is the least number of terms which must be taken in order that threedecimal-place accuracy will result?

Ans. (a) 1/192 (b) eight terms

11.77. (a) Prove that $S \frac{1}{1^{3}}+\frac{1}{2^{3}}+\frac{1}{3^{3}}+\cdots=\frac{4}{3}\left(\frac{1}{1^{3}}+\frac{1}{2^{3}}+\frac{1}{3^{3}}-\cdots\right)$. (b) How many terms of the series on the right are needed in order to calculate $\mathrm{S}$ to six-decimal-place accuracy?

Ans. (b) at least 100 terms

\section*{Absolute and conditional convergence}
11.78. Test for absolute or conditional convergence:\\
(a) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{2}+1}$\\
(c) $\sum_{n=2}^{\infty} \frac{(-1)^{n}}{n \ln n}$\\
(e) $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{2 n-1} \sin \frac{1}{\sqrt{n}}$\\
(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} n}{n^{2}+1}$\\
(d) $\sum_{n=1}^{\infty} \frac{(-1)^{n} n^{3}}{\left(n^{2}+1\right)^{4 / 3}}$\\
(f) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} n^{3}}{2 n-1}$

Ans. (a) absolute convergence, (b) conditional convergence, (c) conditional convergence, (d) divergence, (e) absolute convergence, (f) absolute convergence

11.79. Prove that $\sum_{n=1}^{\infty} \frac{\cos n \pi a}{x^{2}+n^{2}}$ converges absolutely for all real $x$ and $a$.

11.80. If $1-\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots$ converges to $S$, prove that the rearranged series $1+\frac{1}{3}-\frac{1}{2}+\frac{1}{5}+\frac{1}{7}-\frac{1}{4}+\frac{1}{9}+\frac{1}{11}$ $-\frac{1}{6}+\cdots=\frac{3}{2} S$. Explain. (Hint: Take $\frac{1}{2}$ of the first series and write it as $1+\frac{1}{2}+0-\frac{1}{4}+0+\frac{1}{6}+\cdots$; then add term by term to the first series. Note that $S=\ln 2$, as shown in Problem 11.100.)

11.81. Prove that the terms of an absolutely convergent series can always be rearranged without altering the sum.

\section*{Ratio test}
11.82. Test for convergence: (a) $\sum_{n=1}^{\infty} \frac{(-1)^{n} n}{(n+1) e^{n}}$ (b) $\sum_{n=1}^{\infty} \frac{10^{n} n}{(2 n+1) !}$, (c) $\sum_{n=1}^{\infty} \frac{3^{n}}{n^{3}}$, (d) $\sum_{n=1}^{\infty} \frac{(-1)^{n} 2^{3 n}}{3^{2 n}}$, and (e) $\sum_{n=1}^{\infty} \frac{(\sqrt{5}-1)^{n}}{n^{2}+1}$. Ans. (a) convergence (absolute) (b) convergence (c) divergence (d) convergence (absolute) (e) divergence

11.83. Show that the ratio test cannot be used to establish the conditional convergence of a series.

11.84. Prove that (a) $\sum_{n=1}^{\infty} \frac{n !}{n^{n}}$ converges and (b) $\lim _{n \rightarrow \infty} \frac{n !}{n^{n}}=0$.

\section*{Miscellaneous tests}
11.85. Establish the validity of the $n$th root test on Page 282 .

11.86. Apply the $n$th root test to work Problems 11.82(a), (c), (d), and (e).

11.87. Prove that $\frac{1}{3}+\left(\frac{2}{3}\right)^{2}+\left(\frac{1}{3}\right)^{3}+\left(\frac{2}{3}\right)^{4}+\left(\frac{1}{3}\right)^{5}+\left(\frac{2}{3}\right)^{6}+\cdots$ converges.

11.88. Test for convergence: (a) $\frac{1}{3}+\frac{1 \cdot 4}{3 \cdot 6}+\frac{1 \cdot 4 \cdot 7}{3 \cdot 6 \cdot 9}+\cdots$ and (b) $\frac{2}{9}+\frac{2 \cdot 5}{9 \cdot 12}+\frac{2 \cdot 5 \cdot 8}{9 \cdot 12 \cdot 15}+\cdots$. Ans. (a) divergence (b) convergence

11.89. If $a, b$, and $d$ are positive numbers and $b>a$, prove that $\frac{a}{b}+\frac{a(a+d)}{b(b+d)}+\frac{a(a+d)(a+2 d)}{b(b+d)(b+2 d)}+\cdots$ converges if $b-a>d$, and diverges if $b-a \leqq d$.

\section*{Series of functions}
11.90. Find the domain of convergence of the series

(d) $\sum_{n=1}^{\infty} n^{2}\left(\frac{1-x}{1+x}\right)^{n}$, and (e) $\sum_{n=1}^{\infty} \frac{e^{n x}}{n^{2}-n+1}$\\
$\sum_{n=1}^{\infty} \frac{x^{n}}{n^{3}}$\\
(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n}(x-1)^{n}}{2^{n}(3 n-1)}$\\
(c) $\sum_{n=1}^{\infty} \frac{1}{n\left(1+x^{2}\right)^{n}}$

Ans. (a) $-1 \leqq x \leqq 1$

(b) $-1<x \leqq 3$

(c) all $x \neq 0$

(d) $x>0$

(e) $x \leqq 0$

11.91. Prove that $\sum_{n=1}^{\infty} \frac{1 \cdot 3 \cdot 5 \cdots(2 n-1)}{2 \cdot 4 \cdot 6 \cdots(2 n)} x^{n}$ converges for $-1 \leqq x<1$.

\section*{Uniform convergence}
11.92. By use of the definition, investigate the uniform convergence of the series

$$
\sum_{n=1}^{\infty} \frac{x}{[1+(n-1) x][1+n x]}
$$

[Hint: Resolve the $n$th term into partial fractions and show that the $n$th partial sum is $S_{n}(x)=1-\frac{1}{1+n x}$.]

Ans. Not uniformly convergent in any interval which includes $x=0$; uniformly convergent in any other interval.

11.93. Work Problem 11.30 directly by first obtaining $S_{n}(\mathrm{x})$.

11.94. Investigate by any method the convergence and uniform convergence of the series (a) $\sum_{n=1}^{\infty}\left(\frac{x}{3}\right)^{n}$,

(b) $\sum_{n=1}^{\infty} \frac{\sin ^{2} n x}{2 n-1}$, and (c) $\sum_{n=1}^{\infty} \frac{x}{(1+x)^{n}}, x \geqq 0$.

Ans. (a) convergence for $|x|<3$; uniform convergence for $|x| \leqq r<3$ (b) uniform convergence for all $x$ (c) convergence for $x \geqq 0$; not uniform convergence for $x \geqq 0$, but uniform convergence for $x \geqq r>0$

11.95. If $F(x)=\sum_{n=1}^{\infty} \frac{\sin n x}{n^{3}}$, prove that (a) $F(x)$ is continuous for all $x$, (b) $\lim _{x \rightarrow 0} F(x)=0$, and

(c) $F^{\prime}(x)=\sum_{n=1}^{\infty} \frac{\cos n x}{n^{2}}$, is continuous everywhere.

11.96. Prove that $\int_{0}^{\pi}\left(\frac{\cos 2 x}{1 \cdot 3}+\frac{\cos 4 x}{3 \cdot 5} \frac{\cos 6 x}{5 \cdot 7}+\cdots\right) d x=0$.

11.97. Prove that $F(x)=\sum_{n=1}^{\infty} \frac{\sin n x}{\sinh n \pi}$ has derivatives of all orders for any real $x$.

11.98. Examine the sequence $u_{n}(x)=\frac{1}{1+x^{2 n}}, n=1,2,3, \ldots$, for uniform convergence.

11.99. Prove that $\lim _{n \rightarrow \infty} \int_{0}^{1} \frac{d x}{(1+x / n)^{n}}=1-e^{-1}$.

\section*{Power series}
11.100. (a) Prove that In $(1+x)=x-\frac{x^{2}}{x}+\frac{x^{3}}{3}-\frac{x^{4}}{4}+\cdots$ (b) Prove that In $2=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$. (Hint: Use the fact that $\frac{1}{1+x}=1-x+x^{2}-x^{3}+\cdots$ and integrate.)

11.101. Prove that $\sin ^{-1} x=x+\frac{1}{2} \frac{x^{3}}{3}+\frac{1 \cdot 3}{2 \cdot 4} \frac{x^{5}}{5}+\frac{1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6} \frac{x^{7}}{7}+\cdots,-1 \leqq x \leqq 1$.

11.102. Evaluate (a) $\int_{0}^{1 / 2} e^{-x^{2}} d x$ and (b) $\int_{0}^{1} \frac{1-\cos x}{x} d x$ to three decimal places, justifying all steps. Ans. (a) 0.461 (b) 0.486

11.103. Evaluate (a) $\sin 40^{\circ}$, (b) $\cos 65^{\circ}$, and (c) $\tan 12^{\circ}$ correctly to three decimal places.

Ans. (a) 0.643 (b) 0.423 (c) 0.213

11.104. Verify the expansions 4, 5, and 6 on Page 289.

11.105. By multiplying the series for $\sin x$ and $\cos x$, verify that $2 \sin x \cos x=\sin 2 x$.

11.106. Show that $e^{\cos x}=e\left(1-\frac{x^{2}}{2 !}+\frac{4 x^{4}}{4 !}-\frac{31 x^{6}}{6 !}+\cdots\right), \quad-\infty<x<\infty$.

11.107. Obtain the expansions\\
(a) $\tanh ^{-1} x \quad=x+\frac{x^{3}}{3}+\frac{x^{5}}{5}+\frac{x^{7}}{7}+\cdots$\\
$-1<x<1$\\
(b) $\ln \left(x+\sqrt{x^{2}+1}\right)=x-\frac{1}{2} \frac{x^{3}}{3}+\frac{1 \cdot 3}{2 \cdot 4} \frac{x^{5}}{5}-\frac{1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6}+\frac{x^{7}}{7}+\cdots \quad-1<x \leqq 1$

11.108. Let $f(x)=\left\{\begin{array}{ll}e^{-1 / x^{2}} & x \neq 0 \\ 0 & x=0\end{array}\right.$. Prove that the formal Taylor series about $x=0$ corresponding to $f(\mathrm{x})$ exists but that it does not converge to the given function for any $x \neq 0$.

11.109. Prove that

(a) $\frac{\ln (1+x)}{1+x}=x-\left(1+\frac{1}{2}\right) x^{2}+\left(1+\frac{1}{2}+\frac{1}{3}\right) x^{3}-\cdots \quad$ for $-1<x<1$

(b) $\left\{\ln (1+x)^{2}\right\}=x^{2}-\left(1+\frac{1}{2}\right) \frac{2 x^{3}}{3}+\left(1+\frac{1}{2}+\frac{1}{3}\right) \frac{2 x^{4}}{4}-\cdots \quad$ for $-1<x \leqq 1$

\section*{Miscellaneous problems}
11.110. Prove that the series for $J_{p}(\mathrm{x})$ converges (a) for all $x$ and (b) absolutely and uniformly in any finite interval.

11.111. Prove that (a) $\frac{d}{d x}\left\{J_{0}(x)\right\}=-J_{1}(x)$, (b) $\frac{d}{d x}\left\{x^{p} J_{p}(x)\right\}=x^{p} J_{p-1}(x)$, and

(c) $J_{p+1}(x)=\frac{2 p}{x} J_{p}(x)-J_{p-1}(x)$.

11.112. Assuming that the result of Problem 11.111(c) holds for $p=0,-1,-2, \ldots$, prove that (a) $J_{-1}(x)=-J_{1}(x)$, (b) $J_{-2}(x)=J_{2}(x)$, and (c) $J_{-n}(x)=(-1)^{n} J_{n}(x), n=1,2,3, \ldots$

11.113. Prove that $e^{1 / 2 x(t-1 / t)}=\sum_{p=-\infty}^{\infty} J_{p}(x) t^{p}$ (Hint: Write the left side as $e^{x t / 2} e^{-x / 2 t}$, expand, and use Problem\\
11.112.)

11.114. Prove that $\sum_{n=1}^{\infty} \frac{(n+1) z^{n}}{n(n+2)^{2}}$ is absolutely and uniformly convergent at all points within and on the circle $|z|=1$.

11.115. (a) If $\sum_{n=1}^{\infty} a_{n} x^{n}=\sum_{n=1}^{\infty} b_{n} x^{n}$ for all $x$ in the common interval of convergence $|x|<R$ where $R>0$, prove that $a_{n}=b_{n}$ for $n=0,1,2, \ldots$ (b) Use (a) to show that the Taylor expansion of a function exists and the expansion is unique.

11.116. Suppose that $\varlimsup \sqrt[n]{\left|u_{n}\right|}=L$. Prove that $\Sigma u_{n}$ converges or diverges according as $L<1$ or $L>1$. If $L=1$, the test fails.

11.117. Prove that the radius of convergence of the series $\Sigma a_{n} x^{\mathrm{n}}$ can be determined by the following limits, when they exist, and give examples: (a) $\lim _{n \rightarrow \infty}\left|\frac{a_{n}}{a_{n+1}}\right|$, (b) $\lim _{n \rightarrow \infty} \frac{1}{\sqrt[n]{\left|a_{n}\right|}}$, and (c) $\varlimsup_{n \rightarrow \infty} \frac{1}{\sqrt[n]{\left|a_{n}\right|}}$.

11.118. Use Problem 11.117 to find the radius of convergence of the series in Problem 11.22.

11.119. (a) Prove that a necessary and sufficient condition that the series $\Sigma u_{n}$ converge is that, given any $\epsilon>0$, we can find $N>0$ depending on $\epsilon$ such that $\left|S_{p}-S_{q}\right|<\epsilon$ whenever $p>N$ and $q>N$, where $S_{k}=u_{1}+u_{2}+\cdots+$ $u_{k}$. (b) Use (a) to prove that the series $\sum_{n=1}^{\infty} \frac{n}{(n+1) 3^{n}}$ converges. (c) How could you use (a) to prove that the series $\sum_{n=1}^{\infty} \frac{1}{n}$ diverges? (Hint: Use the Cauchy convergence criterion, Page 27.)

11.120. Prove that the hypergeometric series (Page 276) (a) is absolutely convergent for $|x|<1$, (b) is divergent for $|x|>1$, (c) is absolutely divergent for $|x|=1$ if $a+b-c<0$, and (d) satisfies the differential equation $x(1-x) y^{\prime \prime}+\{c-(a+b+1) x\} y^{\prime}-a b y=0$.

11.121. If $F(a, b ; c ; x)$ is the hypergeometric function defined by the series on Page 290, prove that (a) $F(-p, 1 ; 1 ;-x)=$ $(1+x)^{p}$, (b) $x F(1,1 ; 2 ;-x)=\operatorname{In}(1+x)$, and (c) $F\left(\frac{1}{2}, \frac{1}{2} ; \frac{3}{2} ; x^{2}\right)=\left(\sin ^{-1} x\right) / x$.

11.122. Find the sum of the series $S(x)=x+\frac{x^{3}}{1 \cdot 3}+\frac{x^{5}}{1 \cdot 3 \cdot 5}+\cdots$. (Hint: Show that $S^{\prime}(x)-1+x S(x)$ and solve.) Ans. $e^{x^{2} / 2} \int_{0}^{x} e^{-x^{2} / 2} d x$

11.123. Prove that $1+\frac{1}{1 \cdot 3}+\frac{1}{1 \cdot 3 \cdot 5}+\frac{1}{1 \cdot 3 \cdot 5 \cdot 7}+\cdots=\sqrt{e}\left(1-\frac{1}{2 \cdot 3}+\frac{1}{2^{2} \cdot 2 ! \cdot 5}-\frac{1}{2^{3} \cdot 3 ! \cdot 7}+\frac{1}{2^{4} \cdot 4 ! \cdot 9}-\cdots\right)$.

11.124. Establish Dirichlet's test on Page 284.

11.125. Prove that $\sum_{n=1}^{\infty} \frac{\sin n x}{n}$ is uniformly convergent in any interval which does not include $0, \pm \pi, \pm 2 \pi, \ldots$ (Hint: use the Dirichlet test, Page 284, and Problem 1.94.)

11.126. Establish the results on Page 289 concerning the binomial series. (Hint: Examine the Lagrange and Cauchy forms of the remainder in Taylor's theorem.)

11.127. Prove that $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n+x^{2}}$ converges uniformly for all $x$, but not absolutely.

11.128. Prove that $1-\frac{1}{4}+\frac{1}{7}-\frac{1}{10}+\cdots=\frac{\pi}{3 \sqrt{3}}+\frac{1}{3} \ln 2$.

11.129. If $x=y e^{y}$, prove that $y=\sum_{n=1}^{\infty} \frac{(-1)^{n-1} n^{n-1}}{n !} x^{n} \quad$ for $-1 / \mathrm{e}<x \leqq 1 / e$.

11.130. Prove that the equation $e^{-\lambda}=\lambda-1$ has only one real root and show that it is given by

$$
\lambda=1+\sum_{n=1}^{\infty} \frac{(-1)^{n-1} n^{n-1} e^{-n}}{n !}
$$

11.131. Let $\frac{x}{e^{x}-1}=1+B_{1} x+\frac{B_{2} x^{2}}{2 !}+\frac{B_{3} x^{3}}{3 !}+\cdots$. (a) Show that the numbers $B_{n}$, called the Bernoulli numbers, satisfy the recursion formula $(B+1)^{n}-B^{n}=0$ where $B^{k}$ is formally replaced by $B_{k}$ after expanding.

(b) Using (a) or otherwise, determine $B_{1} \ldots, B_{6}$.

$$
\text { Ans. (b) } B_{1}=-\frac{1}{2}, B_{2}=\frac{1}{6}, B_{3}=0, B_{4}=-\frac{1}{30}, B_{5}=0, B_{6}=\frac{1}{42}
$$

11.132. (a) Prove that $\frac{x}{e^{x}-1}=\frac{x}{2}\left(\operatorname{coth} \frac{x}{2}-1\right)$. (b) Use Problem 11.127 and (a) to show that $B_{2 k+1}=0$ if $k$

11.133. Derive the series expansions:

(a) $\operatorname{coth} x=\frac{1}{x}+\frac{x}{3}-\frac{x^{3}}{45}+\cdots+\frac{B_{2 n}(2 x)^{2 n}}{(2 n) ! x}+\cdots$

(b) $\quad \operatorname{coth} x=\frac{1}{x}+\frac{x}{3}-\frac{x^{3}}{45}+\cdots(-1)^{n} \frac{B_{2 n}(2 x)^{2 n}}{(2 n) ! x}+\cdots$

(c) $\tan x=x+\frac{x^{3}}{3}+\frac{2 x^{5}}{15}+\cdots(-1)^{n-1} \frac{2\left(2^{2 n-1}-1\right) B_{2 n}(2 x)^{2 n-1}}{(2 n) !}+\cdots$

(d) $\csc x=\frac{1}{x}+\frac{x}{6}+\frac{7}{360} x^{3}+\cdots(-1)^{n-1} \frac{2\left(2^{2 n-1}-1\right) B_{2 n} x^{2 n-1}}{(2 n) !}+\cdots$

[Hint: For (a) use Problem 11.132; for (b) replace $x$ by $i x$ in (a); for (c) use $\tan x=\cot x-2 \cot 2 x$; for (d) use $\csc x=\cot x+\tan x / 2$.]

11.134. Prove that $\prod_{n=1}^{\infty}\left(1+\frac{1}{n^{3}}\right)$ converges.

11.135. Use the definition to prove that $\prod_{n=1}^{\infty}\left(1+\frac{1}{n}\right)$ diverges.

11.136. (a) Prove that $\prod_{n=1}^{\infty}\left(1-u_{n}\right)$, where $0<u_{n}<1$, converges if and only if $\Sigma u_{n}$ converges.

11.137. (a)Prove that $\prod_{n=2}^{\infty}\left(1-\frac{1}{n^{2}}\right)$ converges to $\frac{1}{2}$. (b) Evaluate the infinite product in (a) to two decimal places and compare with the true value.

11.138. Prove that the series $1+0-1+1+0-1+1+0-1+\cdots$ is the $C-1$ summable to zero.

11.139. Prove that the Cesaro method of summability is regular. (Hint: See Page 291.)

11.140. Prove that the series $1+2 x+3 x^{2}+4 x^{3}+\cdots+n x^{n-1}+\cdots$ converges to $1 /(1-x)^{2}$ for $|x|<1$.

11.141. A series $\sum_{n=0}^{\infty} a$ is called Abel summable to $S$ if $S=\lim _{x \rightarrow 1-} \sum_{n=0}^{\infty} a_{n} X^{n}$ exists. Prove that

(a) $\sum_{n=0}^{\infty}(-1)^{n}(n+1)(n+1)$ is Abel summable to $\frac{1}{4}$

(b) $\sum_{n=0}^{\infty} \frac{(-1)^{n}(n+1)(n+2)}{2}$ is Abel summable to $\frac{1}{8}$

11.142. Prove that the double series $\sum_{m=0}^{\infty} \sum_{n=0}^{\infty} \frac{1}{\left(m^{2}+n^{2}\right)^{p}}$, where $p$ is a constant, converges or diverges according as $p>1$ or $p \leqq 1$, respectively.

11.143. (a) Prove that $\int_{x}^{\infty} \frac{e^{x-u}}{u} d u=\frac{1}{x}-\frac{1}{x^{2}}+\frac{2 !}{x^{3}}-\frac{3 !}{x^{4}}+\cdots \frac{(-1)^{n-1}(n-1) !}{x^{n}}+(-1)^{n} n ! \int_{x}^{\infty} \frac{e^{x-u}}{u^{n+1}} d u$.

(b) Use (a) to prove that $\int_{x}^{\infty} \frac{e^{x-u}}{u} d u \sim \frac{1}{x}-\frac{1}{x^{2}}+\frac{2 !}{x^{3}}-\frac{3 !}{x^{4}}+\cdots$.

\section*{CHAPTER 12}
\section*{Improper Integrals}
\section*{Definition of an Improper Integral}
The functions that generate the Riemann integrals of Chapter 5 are continuous on closed intervals. Thus, the functions are bounded and the intervals are finite. Integrals of functions with these characteristics are called proper integrals. When one or more of these restrictions are relaxed, the integrals are said to be improper. Categories of improper integrals are established as follows.

The integral $\int_{a}^{b} f(x) d x$ is called an improper integral if

\begin{enumerate}
  \item $a=-\infty$ or $b=\infty$ or both; i.e., one or both integration limits is infinite.
  \item $f(x)$ is unbounded at one or more points of $a \leqq x \leqq b$. Such points are called singularities of $f(x)$.
\end{enumerate}

Integrals corresponding to (1) and (2) are called improper integrals of the first and second kinds, respectively. Integrals with both conditions (1) and (2) are called improper integrals of the third kind

EXAMPLE 1. $\int_{0}^{\infty} \sin x^{2} d x$ is an improper integral of the first kind.

EXAMPLE 2. $\int_{0}^{4} \frac{d x}{x-3}$ is an improper integral of the second kind.

EXAMPLE 3. $\int_{0}^{\infty} \frac{e^{-x}}{\sqrt{x}} d x$ is an improper integral of the third kind.

EXAMPLE 4. $\int_{0}^{1} \frac{\sin x}{x} d x$ is a proper integral, since $\lim _{x \rightarrow 0+} \frac{\sin x}{x}=1$.

\section*{Improper Integrals of the First Kind (Unbounded Intervals)}
If $f$ is integrable on the appropriate domains, then the indefinite integrals $\int_{a}^{x} f(t) d t$ and $\int_{x}^{a} f(t) d t$ (with variable upper and lower limits, respectively) are functions. Through them we define three forms of the improper integral of the first kind.

\section*{Definition}
(a) If $f$ is integrable on $a \leqq x<\infty$, then $\int_{a}^{\infty} f(x) d x=\lim _{x \rightarrow \infty} \int_{a}^{x} f(t) d t$.

(b) If $f$ is integrable on $-\infty<x \leqq a$, then $\int_{-\infty}^{a} f(x) d x=\lim _{x \rightarrow-\infty} \int_{x}^{a} f(t) d t$.

(c) If $f$ is integrable on $-\infty<x<\infty$, then

$$
\begin{aligned}
\int_{-\infty}^{\infty} f(x) d x & =\int_{-\infty}^{\alpha} f(x) d x+\int_{\alpha}^{\infty} f(x) d x \\
& =\lim _{x \rightarrow-\infty} \int_{x}^{\alpha} f(t) d t+\lim _{x \rightarrow \infty} \int_{\alpha}^{x} f(t) d t
\end{aligned}
$$

In (c) it is important to observe that

$$
\lim _{x \rightarrow-\infty} \int_{x}^{\alpha} f(t) d t+\lim _{x \rightarrow \infty} \int_{\alpha}^{x} f(t) d t
$$

and

$$
\lim _{x \rightarrow \infty}\left[\int_{-x}^{\alpha} f(t) d t+\int_{\alpha}^{x} f(t) d t\right]
$$

are not necessarily equal.

This can be illustrated with $f(x)=x e^{x^{2}}$. The first expression is not defined, since neither of the improper integrals (i.e., limits) is defined, while the second form yields the value 0 .

EXAMPLE. The function $F(x)=\frac{1}{\sqrt{2 \pi}} e^{-\left(x^{2} / 2\right)}$ is called the normal density function and has numerous applications in probability and statistics. In particular (see the bell-shaped curve in Figure 12.1),

$$
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e-\frac{x^{2}}{2}: d x=1
$$

(See Problem 12.31 for the trick of making this evaluation.)

Perhaps at some point in your academic career you were "graded on the curve." The infinite region under the curve with the limiting area of 1 corresponds to the assurance of getting a grade. C's are assigned to those whose grades fall in a designated central section, and so on. (Of course, this grading procedure is not valid for a small number of students, but as the number increases it takes on statistical meaning.)

In this chapter we formulate tests for convergence or divergence of improper integrals. It will be found that such tests and proofs of theorems bear close analogy to convergence and divergence tests and corresponding theorems for infinite series (see Chapter 11).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-333}
\end{center}

Figure 12.1

\section*{Convergence or Divergence of Improper Integrals of the First Kind}
Let $f(x)$ be bounded and integrable in every finite interval $a \leqq x \leqq b$. Then we define


\begin{equation*}
\int_{a}^{\infty} f(x) d x=\lim _{b \rightarrow \infty} \int_{a}^{b} f(x) d x \tag{1}
\end{equation*}


where $b$ is a variable on the positive real numbers.

The integral on the left is called convergent or divergent according as the limit on the right does or does not exist. Note that $\int_{a}^{\infty} f(x) d x$ bears close analogy to the infinite series $\sum_{n=1}^{\infty} u_{n}$, where $u_{n}=f(n)$, while $\int_{a}^{b} f(x) d x$ corresponds to the partial sums of such infinite series. We often write $M$ in place of $b$ in Equation (1).

Similarly, we define


\begin{equation*}
\int_{-\infty}^{b} f(x) d x=\lim _{a \rightarrow-\infty} \int_{a}^{b} f(x) d x \tag{2}
\end{equation*}


where $a$ is a variable on the negative real numbers. And we call the integral on the left convergent or divergent according as the limit on the right does or does not exist.

EXAMPLE 1. $\int_{1}^{\infty} \frac{d x}{x^{2}}=\lim _{b \rightarrow \infty} \int_{1}^{b} \frac{d x}{x^{2}}=\lim _{b \rightarrow \infty}\left(1-\frac{1}{b}\right)=1$ so that $\int_{t}^{\infty} \frac{d x}{x^{2}}$ converges to 1. EXAMPLE 2. $\int_{-\infty}^{a} \cos x d x=\lim _{a \rightarrow \infty} \int_{a}^{u} \cos x d x=\lim _{a \rightarrow \infty}(\sin u-\sin a)$. Since this limit does not exit, $\int_{-\infty}^{a} \cos x d x$\\
is divergent.

In like manner, we define


\begin{equation*}
\int_{-\infty}^{\infty} f(x) d x=\int_{-\infty}^{x_{0}} f(x) d x+\int_{x_{0}}^{\infty} f(x) d x \tag{3}
\end{equation*}


where $x_{0}$ is a real number, and we call the integral convergent or divergent according as the integrals on the right converge or not, as in definitions (1) and (2). [See the previous remarks in part (c) of the definition of improper integrals of the first kind.]

\section*{Special Improper Integrals of the First Kind}
\begin{enumerate}
  \item Geometric or exponential integral $\int_{a}^{\infty} e^{t-1 x} d x$, where $t$ is a constant, converges if $t>0$ and diverges if $t \leqq 0$. Note the analogy with the geometric series if $r=e^{-t}$ so that $e^{-t x}=r^{x}$.

  \item The $p$ integral of the first kind $\int_{a}^{\infty} \frac{d x}{x^{p}}$, where $p$ is a constant and $a>0$, converges if $p>1$ and diverges if $p \leqq 1$. Compare with the $p$ series.

\end{enumerate}

\section*{Convergence Tests for Improper Integrals of the First Kind}
The following tests are given for cases where an integration limit is $\infty$. Similar tests exist where an integration limit is $-\infty$ (a change of variable $x=-y$ then makes the integration limit $\infty$ ). Unless otherwise specified, we assume that $f(x)$ is continuous and thus integrable in every finite interval $a \leqq x \leqq b$.

\begin{enumerate}
  \item Comparison test for integrals with nonnegative integrands.
\end{enumerate}

(a) Convergence. Let $g(x) \geqq 0$ for all $x \geqq a$, and suppose that $\int_{a}^{\infty} g(x) d x$ converges. Then if $0 \leqq f(x) \leqq$ $g(x)$ for all $x \geqq a, \int_{a}^{\infty} f(x) d x$ also converges.

EXAMPLE. Since $\frac{1}{e^{x}+1} \leqq \frac{1}{e^{x}}=e^{-x}$ and $\int_{a}^{\infty} e^{-x} d x$ converges, $\int_{0}^{\infty} \frac{d x}{e^{x}+1}$ also converges.

(b) Divergence. Let $g(x) \geqq 0$ for all $x \geqq a$, and suppose that $\int_{a}^{\infty} g(x) d x$ diverges. Then if $f(x) \leqq g(x)$ for all $x \geqq a, \int_{a}^{\infty} f(x) d x$ also diverges.

EXAMPLE. Since $\frac{1}{\ln x}>\frac{1}{x}$ for $x \geqq 2$ and $\int_{2}^{\infty} \frac{d x}{x}$ diverges ( $p$ integral with $p=1$ ), $\int_{2}^{\infty} \frac{\mathrm{dx}}{\ln \mathrm{x}}$ also diverges.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Quotient test for integrals with nonnegative integrands.
\end{enumerate}

(a) If $f(x) \geqq$ and $g(x) \geqq 0$, and if $\lim _{x \rightarrow \infty} \frac{f(x)}{g(x)}=A \neq 0$ or $\infty$, then $\int_{a}^{\infty} f(x) d x$ and $\int_{a}^{\infty} g(x) d x$ either both\\
converge or both diverge.

(b) If $A=0$ in (a) and $\int_{a}^{\infty} g(x) d x$ converges, then $\int_{a}^{\infty} f(x) d x$ converges.

(c) If $A=\infty$ in (a) and $\int_{a}^{\infty} g(x) d x$ diverges, then $\int_{a}^{\infty} f(x) d x$ diverges.

This test is related to the comparison test and is often a very useful alternative to it. In particular, taking $g(x)=1 / x^{p}$, we have, from known facts about the $p$ integral, the following theorem.

Theorem 1 Let $\lim _{x \rightarrow \infty} x^{p} f(x)=A$. Then

(i) $\int_{a}^{\infty} f(x) d x$ converges if $p>1$ and $A$ is finite.

(ii) $\int_{a}^{\infty} f(x) d x$ diverges if $p \leqq 1$ and $A \neq 0$ ( $A$ may be infinite).

EXAMPLE 1. $\int_{0}^{\infty} \frac{x^{2} d x}{4 x^{4}+25}$ converges since $\lim _{x \rightarrow \infty} x^{2} \cdot \frac{x^{2}}{4 x^{4}+25}=\frac{1}{4}$.

EXAMPLE 2. $\int_{0}^{\infty} \frac{x d x}{\sqrt{x^{4}+x^{2}+1}}$ diverges since $\lim _{x \rightarrow \infty} x \cdot \frac{x}{\sqrt{x^{4}+x^{2}+1}}=1$.

A similar test can be devised using $g(x)=e^{-t x}$.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Series test for integrals with nonnegative integrands. $\int_{a}^{\infty} f(x) d x$ converges or diverges according as $\Sigma u_{n}$ where $u_{n}=f(n)$, converges or diverges.

  \item Absolute and conditional convergence. $\int_{a}^{\infty} f(x) d x$ is called absolutely convergent if $\int_{a}^{\infty}|f(x)| d x$ converges. If $\int_{a}^{\infty} f(x) d x$ converges but $\int_{a}^{\infty}|f(x)| d x$ diverges, then $\int_{a}^{\infty} f(x) d x$ is called conditionally convergent.

\end{enumerate}

Theorem 2 If $\int_{a}^{\infty} \mid f(x) d x$ converges, then $\int_{\mathrm{a}}^{\infty} f(x) d x$ converges. In words, an absolutely convergent integral converges.

EXAMPLE 1. $\int_{a}^{\infty} \frac{\cos x}{x^{2}+1} d x$ is absolutely convergent and thus convergent, since

$\int_{0}^{\infty}\left|\frac{\cos x}{x^{2}+1}\right| d x \leqq \int_{0}^{\infty} \frac{d x}{x^{2}+1}$ and $\int_{0}^{\infty} \frac{d x}{x^{2}+1}$ converges.

EXAMPLE 2. $\int_{0}^{\infty} \frac{\sin x}{x} d x$ converges (see Problem 12.11), but $\int_{0}^{\infty}\left|\frac{\sin x}{x}\right| d x$ does not converge (see Problem 12.12). Thus, $\int_{0}^{\infty} \frac{\sin x}{x} d x d x$ is conditionally convergent.

Any of the tests used for integrals with nonnegative integrands can be used to test for absolute convergence.

\section*{Improper Integrals of the Second Kind}
If $f(x)$ becomes unbounded only at the endpoint $x=a$ of the interval $a \leqq x \leqq b$, then we define


\begin{equation*}
\int_{a}^{b} f(x) d x=\lim _{\epsilon \rightarrow 0+} \int_{a+\epsilon}^{b} f(x) d x \tag{4}
\end{equation*}


and define it to be an improper integral of the second kind. If the limit on the right of Equation (4) exists, we call the integral on the left convergent; otherwise, it is divergent.

Similarly, if $f(x)$ becomes unbounded only at the endpoint $x=b$ of the interval $a \leqq x \leqq b$, then we extend the category of improper integrals of the second kind.


\begin{equation*}
\int_{a}^{b} f(x) d x=\lim _{\epsilon \rightarrow 0+} \int_{a}^{b-e} f(x) d x \tag{5}
\end{equation*}


Note: Be alert to the word unbounded. This is distinct from undefined. For example, $\int_{a}^{1} \frac{\sin x}{x} d x=\lim _{\epsilon \rightarrow 0} \int_{\epsilon}^{1} \frac{\sin x}{x} d x$ is a proper integral, since $\lim _{x \rightarrow 0} \frac{\sin x}{x}=1$ and, hence, is bounded as $x \rightarrow 0$ even though the function is undefined at $x=0$. In such case the integral on the left of Equation (5) is called convergent or divergent according as the limit on the right exists or does not exist.

Finally, the category of improper integrals of the second kind also includes the case where $f(x)$ becomes unbounded only at an interior point $x=x_{0}$ of the interval $a \leqq x \leqq b$; then we define


\begin{equation*}
\int_{a}^{b} f(x) d x=\lim _{\epsilon_{1} \rightarrow 0+} \int_{a}^{x_{0}-\epsilon_{1}} f(x) d x+\lim _{\epsilon_{2} \rightarrow 0+} \int_{x_{0}+\epsilon_{2}}^{b} f(x) d x \tag{6}
\end{equation*}


The integral on the left of Equation (6) converges or diverges according as the limits on the right exist or do not exist.

Extensions of these definitions can be made in case $f(x)$ becomes unbounded at two or more points of the interval $a \leqq x \leqq b$.

\section*{Cauchy Principal Value}
It may happen that the limits on the right of Equation (6) do not exist when $\epsilon_{1}$ and $\epsilon_{2}$ aproach zero independently. In such case it is possible that by choosing $\epsilon_{1}=\epsilon_{2}=\epsilon$ in (6), i.e., writing


\begin{equation*}
\int_{a}^{b} f(x) d x=\lim _{\epsilon \rightarrow 0+}\left\{\int_{a}^{x_{0}-\epsilon} f(x) d x+\int_{x_{0}+\epsilon}^{b} f(x) d x\right\} \tag{7}
\end{equation*}


the limit does exist. If the limit on the right of Equation (7) does exist, we call this limiting value the Cauchy principal value of the integral on the left. See Problem 12.14.

EXAMPLE. The natural logarithm (i.e., base $e$ ) may be defined as follows:

$$
\ln x=\int_{t}^{x} \frac{d t}{t}, \quad 0<x<\infty
$$

Since $f(x)=\frac{1}{x}$ is unbounded as $x \rightarrow 0$, this is an improper integral of the second kind (see Figure 12.2).

Also, $\int_{0}^{\infty} \frac{d t}{t}$ is an integral of the third kind, since the interval to the right is unbounded.

Now $\lim _{\epsilon \rightarrow 0} \int_{\epsilon}^{1} \frac{d t}{t}=\lim _{\epsilon \rightarrow 0}[\ln 1-\ln \in] \rightarrow-\infty$ as $\in \rightarrow 0$; therefore, this improper integral of the second kind is divergent. Also, $\int_{t}^{\infty} \frac{d t}{t}=\lim _{x \rightarrow \infty} \int_{1}^{x} \frac{d t}{t}=\lim _{x \rightarrow \infty}[\ln x-\ln 1] \rightarrow \infty$; this integral (which is of the first kind) also diverges.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-336}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-336(1)}
\end{center}

(b)

Figure 12.2

\section*{Special Improper Integrals of the Second Kind}
\begin{enumerate}
  \item $\quad \int_{a}^{b} \frac{d x}{(x-a)^{p}}$ converges if $p<1$ and diverges if $p \geqq 1$.

  \item $\int_{a}^{b} \frac{d x}{(b-x)^{p}}$ converes if $p<1$ and diverges if $p \geqq 1$.

\end{enumerate}

These can be called $p$ integrals of the second kind. Note that when $p \leqq 0$ the integrals are proper.

\section*{Convergence Tests for Improper Integrals of the Second Kind}
The following tests are given for the case where $f(x)$ is unbounded only at $x=a$ in the interval $a \leqq x \leqq b$. Similar tests are available if $f(x)$ is unbounded at $x=b$ or at $x=x_{0}$ where $a<x_{0}<b$.

\begin{enumerate}
  \item Comparison test for integrals with nonnegative integrands.
\end{enumerate}

(a) Convergence. Let $g(x) \varepsilon 0$ for $a<x \leqq b$, and suppose that $\int_{a}^{b} g(x) d x$ converges. Then if $0 \leqq f(x) \leqq g(x)$ for $a<x \leqq b, \int_{a}^{b} f(x) d x$ also converges.

EXAMPLE. $\frac{1}{\sqrt{x^{4}-1}}<\frac{1}{\sqrt{x-1}}$ for $x>1$. Then since $\int_{1}^{5} \frac{d x}{\sqrt{x-1}}$ converges ( $p$ integral with $a=1, p=\frac{1}{2}$ ), $\int_{1}^{5} \frac{d x}{\sqrt{x^{4}-1}}$ also converges.

(b) Divergence. Let $g(x) \geqq 0$ for $a<x \leqq b$, and suppose that $\int_{a}^{b} g(x) d x$ diverges. Then if $f(x) \geqq g(x)$ for $a<x \geqq b, \int_{a}^{b} f(x) d x$ also diverges.

EXAMPLE. $\frac{\ln x}{(x-3)^{4}}>\frac{1}{(x-3)^{4}}$ for $x>3$. Then since $\int_{3}^{b} \frac{d x}{(x-3)^{4}}$ diverges ( $p$ integral with $a=3, p=4$ ),

$\int_{3}^{b} \frac{\ln x}{(x-3)^{4}} d x$ also diverges.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Quotient test for integrals with nonnegative integrands.
\end{enumerate}

(a) If $f(x) \geqq 0$ and $g(x) \geqq 0$ for $a<x \leqq b$, and if $\lim _{x \rightarrow a} \frac{f(x)}{g(x)}=A \neq 0$ or $\infty$, then $\int_{a}^{b} f(x) d x$ and $\int_{a}^{b} g(x) d x$ either both converge or both diverge.

(b) If $A=0$ in (a), and $\int_{\mathrm{a}}^{\mathrm{b}} g(x) d x$ converges, then $\int_{a}^{b} f(x) d x$ converges.

(c) If $A=\infty$ in (a), and $\int_{\mathrm{a}}^{\mathrm{b}} g(x) d x$ diverges, then $\int_{a}^{b} f(x) d x$ diverges.

This test is related to the comparison test and is a very useful alternative to it. In particular, taking $g(x)=$ $1 /(x-a)^{p}$ we have, from known facts about the $p$ integral, the following theorems.

Theorem 3. Let $\lim _{x \rightarrow a+}(x-a)^{p} f(x)=A$. Then

(i) $\int_{a}^{b} f(x) d x$ converges if $p<1$ and $A$ is finite.

(ii) $\int_{a}^{b} f(x) d x$ diverges if $p \geq 1$ and $A \neq 0$ (A may be infinite).

If $f(x)$ becomes unbounded only at the upper limit, these conditions are replaced by those in Theorem 4.

Theorem 4. Let $\lim _{x \rightarrow b-}(b-x)^{p} f(x)=B$. Then

(i) $\int_{a}^{b} f(x) d x$ converges of $p<1$ and $B$ is finite.

(ii) $\int_{a}^{b} f(x) d x$ diverges if $p \geqq 1$ and $B \neq 0$ ( $B$ may be infinite).

EXAMPLE 1. $\int_{1}^{5} \frac{d x}{\sqrt{x^{4}-1}}$ converges, since $\lim _{x \rightarrow 1+}(x-1)^{1 / 2} \cdot \frac{1}{\left(x^{4}-1\right)^{1 / 2}}=\lim _{x \rightarrow 1+} \sqrt{\frac{x-1}{x^{4}-1}}=\frac{1}{2}$.

EXAMPLE 2. $\int_{0}^{3} \frac{d x}{(3-x) \sqrt{x^{2}+1}}$ diverges, since $\lim _{x \rightarrow 3-}(3-x) \cdot \frac{1}{(3-x) \sqrt{x^{2}+1}}=\frac{1}{\sqrt{10}}$.

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Absolute and conditional convergence. $\int_{a}^{b} f(x) d x$ is called absolute convergent if $\int_{\mathrm{a}}^{\mathrm{b}}|f(x)| d x$ converges. If $\int_{a}^{b} f(x) d x$ converges but $\int_{a}^{b}|f(x)| d x$ diverges, then $\int_{a}^{b} f(x) d x$ is called conditionally convergent.
\end{enumerate}

Theorem 5. If $\int_{a}^{b}|f(x)| d x$ converges, then $\int_{a}^{b} f(x) d x$ converges. In words, an absolutely convergent integral converges.

EXAMPLE. $\left|\frac{\sin x}{\sqrt[3]{x-\pi}}\right| \leqq \frac{1}{\sqrt[3]{x-\pi}}$ and $\int_{\pi}^{4 \pi} \frac{d x}{\sqrt[3]{x-\pi}}$ converges $\left(p\right.$ integral with $\left.a=\pi, p=\frac{1}{3}\right)$, it follows that $\int_{\pi}^{4 \pi}\left|\frac{\sin x}{\sqrt[3]{x-\pi}}\right| d x$ converges and thus $\int_{\pi}^{4 \pi} \frac{\sin x}{\sqrt[3]{x-\pi}} d x$ converges (absolutely).

Any of the tests used for integrals with nonnegative integrands can be used to test for absolute convergence.

\section*{Improper Integrals of the Third Kind}
Improper integrals of the third kind can be expressed in terms of improper integrals of the first and second kinds, and, hence, the question of their convergence or divergence is answered by using results already established.

\section*{Improper Integrals Containing a Parameter, Uniform Convergence}
Let


\begin{equation*}
\phi(\alpha)=\int_{a}^{\infty} f(x, \alpha) d x \tag{8}
\end{equation*}


This integral is analogous to an infinite series of functions. In seeking conditions under which we may differentiate or integrate $\phi(\alpha)$ with respect to $\alpha$, it is convenient to introduce the concept of convergence for integrals by analogy with infinite series.

We shall suppose that the integral (8) converges for $\alpha_{1} \leq \alpha \leq \alpha_{2}$, or, briefly, $\left[\alpha_{1}, \alpha_{2}\right]$.

Definition. The integral (8) is said to be uniformly convergent in $\left[\alpha_{1}, \alpha_{2}\right]$ if for each $\epsilon>0$ we can find a number $N$ depending on $\epsilon$ but not on $\alpha$, such that

$$
\left|\phi(\alpha)-\int_{a}^{u} f(x, \alpha) d x\right|<\in \quad \text { for all } u>N \text { and all } \alpha \text { in }\left[\alpha_{1}, \alpha_{2}\right]
$$

This can be restated by noting that $\left|\phi(\boldsymbol{\alpha})-\int_{a}^{u} f(x, \alpha) d x\right|=\left|\int_{u}^{\infty} f(x, \alpha) d x\right|$, which is analogous in an infinite series to the absolute value of the remainder after $N$ terms.

This definition and the properties of uniform convergence to be developed are formulated in terms of improper integrals of the first kind. However, analogous results can be given for improper integrals of the second and third kinds.

\section*{Special Tests for Uniform Convergence of Integrals}
\begin{enumerate}
  \item Weierstrass $M$ test. If we can find a function $M(x) \varepsilon 0$ such that
\end{enumerate}

(a) $|f(x, \alpha)| \leqq M(x) \alpha_{1} \leqq \alpha \leqq \alpha_{2}, x>a$

(b) $\int_{a}^{\infty} M(x) d x$ converges

then $\int_{a}^{\infty} f(x, \alpha) d x$ is uniformly and absolutely convergent in $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$.

EXAMPLE. Since $\left|\frac{\cos \alpha x}{x^{2}+1}\right| \leqq \frac{1}{x^{2}+1}$ and $\int_{0}^{\infty} \frac{\mathrm{dx}}{\mathrm{x}^{2}+1}$ converges, it follows that $\int_{0}^{\infty} \frac{\cos \alpha \mathrm{x}}{\mathrm{x}^{2}+1} d x$ is uniformly and absolutely convergent for all real values of $\alpha$.

As in the case of infinite series, it is possible for integrals to be uniformly convergent without being absolutely convergent, and conversely.

2.

Dirichlet's test. Suppose that

(a) $\psi(x)$ is a positive monotonic decreasing function which approches zero as $x \rightarrow \infty$.

(b) $\left|\int_{a}^{u} f(x, \alpha) d x\right|<P$ for all $u>a$ and $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$.

Then the integral $\int_{a}^{\infty} f(x, \alpha) \psi(x) d x$ is uniformly convergent for $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$.

\section*{Theorems on Uniformly Convergent Integrals}
Theorem 6. If $f(x, \alpha)$ is continuous for $x \varepsilon a$ and $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, and if $\int_{a}^{\infty} f(x, \alpha) d x$ is uniformly convergent for $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, then $\phi(\alpha)=\int_{a}^{\infty} f(x, \alpha) d x$ is continuous in $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$. In particular, if $\alpha_{0}$ is any point of $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, we can write


\begin{equation*}
\lim _{\alpha \rightarrow \alpha_{a}} \phi(\alpha) d \alpha=\lim _{\alpha \rightarrow \alpha_{u}} \int_{a}^{\infty} f(x, \alpha)=\int_{a}^{\infty} \lim _{\alpha \rightarrow \alpha_{v}} f(x, \alpha) d x \tag{9}
\end{equation*}


If $\alpha_{0}$ is one of the endpoints, we use right- or left-hand limits.

Theorem 7. Under the conditions of Theorem 6, we can integrate $\phi(\alpha)$ with respect to $\alpha_{1}$ to $\alpha_{2}$ to obtain


\begin{equation*}
\int_{\alpha_{1}}^{\alpha_{2}} \phi(\alpha) d \alpha=\int_{\alpha_{1}}^{\alpha_{2}}\left\{\int_{a}^{\infty} f(x, \alpha) d x\right\} d \alpha \int_{a}^{\infty}\left\{\int_{\alpha_{1}}^{\alpha_{2}} f(x, \alpha) d \alpha\right\} d x \tag{10}
\end{equation*}


which corresponds to a change of the order of integration.

Theorem 8. If $f(x, \alpha)$ is continuous and has a continuous partial derivative with respect to $\alpha$ for $x \varepsilon a$ and $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, and if $\int_{a}^{\infty} \frac{\partial f}{\partial \alpha} d x$ converges uniformly in $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, then if $a$ does not depend on $\alpha$,


\begin{equation*}
\frac{d \phi}{d \alpha}=\int_{a}^{\infty} \frac{\partial f}{\partial \alpha} d x \tag{11}
\end{equation*}


If $a$ depends on $\alpha$, this result is easily modified (see Leibniz's rule, Page 198).

\section*{Evaluation of Definite Integrals}
Evaluation of definite integrals which are improper can be achieved by a variety of techniques. One useful device consists of introducing an appropriately placed parameter in the integral and then differentiating or integrating with respect to the parameter, employing the aforementioned properties of uniform convergence.

\section*{Laplace Transforms}
Operators that transform one set of objects into another are common in mathematics. Both the derivative and the indefinite integral are examples. Logarithms provide an immediate arithmetic advantage by replacing multiplication, division, and powers, respectively, by the relatively simpler processes of addition, subtraction, and multiplication. After obtaining a result with logarithms, an antilogarithm procedure is necessary to find its image in the original framework. The Laplace transform has a role similar to that of logarithms but in the more sophisticated world of differential equations. (See Problems 12.34 and 12.36.)

The Laplace transform of a function $F(x)$ is defined as


\begin{equation*}
f(s)=L\{F(x)\}=\int_{0}^{\infty} e^{-5 x} F(x) d x \tag{12}
\end{equation*}


and is analogous to power series as seen by replacing $e^{-s}$ by $t$ so that $e^{-s x}=t^{x}$. Many properties of power series also apply to Laplace transforms. Table 12.1, showing Laplace transforms, is useful. In each case, $a$ is a real constant.

\section*{TABLE 12.1}
\begin{center}
\begin{tabular}{ccc}
\hline
$F(x)$ & \multicolumn{2}{c}{$F(x)$} \\
\hline
$a$ & $\frac{a}{s}$ & $s>0$ \\
$e^{a x}$ & $\frac{1}{s-a}$ & $s>a$ \\
$\sin a x$ & $\frac{a}{s^{2}+a^{2}}$ & $s>0$ \\
$\cos a x$ & $\frac{8}{s^{2}+a^{2}}$ & $s>0$ \\
$x^{n} n=1,2,3, \ldots$ & $\frac{n !}{s^{n+1}}$ & $s>0$ \\
$Y^{\prime}(x)$ & $s \mathscr{L}\{Y(x)\}-Y(0)$ &  \\
$Y^{\prime \prime}(x)$ & $s^{2} \mathscr{L}\{Y(x)\}-s Y(0)-Y^{\prime}(0)$ &  \\
\hline
\end{tabular}
\end{center}

\section*{Linearity}
The Laplace transform is a linear operator; i.e., $\zeta\{F(x)+G(x)\}=\zeta\{F(x)\}+\zeta\{G(x)\}$.

This property is essential for returning to the solution after having calculated in the setting of the transforms. (See the example that follows and the previously cited problems.)

\section*{Convergence}
The exponential $e^{-s t}$ contributes to the convergence of the improper integral. What is required is that $F(x)$ does not approach infinity too rapidly as $x \rightarrow \infty$. This is formally stated as follows: If there is some constant $a$ such that $|F(x)| \leqq e^{a x}$ for all sufficiently large values of $x$, then $f(s)=\int_{0}^{\infty} e^{-5 s} F(x) d x$ converges when $s>a$ and $f$ has derivatives of all orders. (The differentiations of $f$ can occur under the integral sign.)

\section*{Application}
The feature of the Laplace transform that (when combined with linearity) establishes it as a tool for solving differential equations is revealed by applying integration by parts to $f(s)=\int_{0}^{x} e^{-s t} F(t) d t$. By letting $u=F(t)$ and $d v=e^{-s t} d t$, we obtain, after letting $x \rightarrow \infty$,

$$
\int_{0}^{x} e^{-s t} F(t) d t=\frac{1}{s} F(0)+\frac{1}{s} \int_{0}^{\infty} e^{-s t} F^{\prime}(t) d t
$$

Conditions must be satisfied that guarantee the convergence of the integrals (for example, $e^{-s t} F(t) \rightarrow 0$ as $t \rightarrow \infty$ ).

This result of integration by parts may be put in the form

(a) $\zeta\left\{F^{\prime}(t)\right\}=s \zeta\{F(t)\}-F(0)$.

Repetition of the procedure combined with a little algebra yields

(b) $\zeta\left\{F^{\prime \prime}(t)\right\}=s^{2} \zeta\{F(t)\}-s F(0)-F^{\prime}(0)$.

The Laplace representation of derivatives of the order needed can be obtained by repeating the process.

To illustrate application, consider the differential equation

$$
\frac{d^{2} y}{d t^{2}}+4 y=3 \sin t
$$

where $y=F(t)$ and $F(0)=1, F^{\prime}(0)=0$. We use

$$
\zeta(\sin a t\}=\frac{\alpha}{s^{2}+a^{2}}, \zeta\{\cos a t\}=\frac{5}{s^{2}+a^{2}}
$$

and recall that

$$
f(s)=\zeta\{F(t)\} \zeta\left\{F^{\prime \prime}(t)\right\}+4 \zeta\{F(t)\}=3 \zeta\{\sin t\}
$$

Using (b), we obtain

Solving for $f(s)$ yields

$$
s^{2} f(s)-s+4 f(s)=\frac{3}{s^{2}+1}
$$

$$
f(s)=\frac{3}{\left(s^{2}+4\right)\left(s^{2}+1\right)}+\frac{s}{s^{2}+4}=\frac{j}{s^{2}+1}-\frac{1}{s^{2}+4}+\frac{s}{s^{2}+4}
$$

(Partial fractions were employed.)

Referring to the table of Laplace transforms, we see that this last expression may be written

$$
f(s)=\zeta\{\sin t\}-\frac{1}{2} \zeta\{\sin 2 t)+\zeta\{\cos 2 t\}
$$

then, using the linearity of the Laplace transform,

$$
f(s)=\zeta\left\{\sin t-\frac{1}{2} \sin 2 t+\cos 2 t\right\}
$$

We find that

$$
F(t)=\sin t-\frac{1}{2} \sin 2 t+\cos 2 t
$$

satisfies the differential equation.

\section*{Improper Multiple Integrals}
The definitions and results for improper single integrals can be extended to improper multiple integrals.

\section*{SOLVED PROBLEMS}
\section*{Improper integrals}
12.1. Classify according to the type of improper integral:\\
(a) $\int_{-1}^{1} \frac{d x}{\sqrt[3]{x}(x+1)}$\\
(c) $\int_{3}^{10} \frac{x d x}{(x-2)^{2}}$\\
(e) $\int_{0}^{\pi} \frac{1-\cos x}{x^{2}} d x$\\
(b) $\int_{0}^{\infty} \frac{d x}{1+\tan x}$\\
(d) $\int_{-\infty}^{\infty} \frac{x^{2} d x}{x^{4}+x^{2}+1}$

(a) Second kind (integrand is unbounded at $x=0$ and $x=-1$ ).

(b) Third kind (integration limit is infinite and integrand is unbounded where $\tan x=-1$ ).

(c) This is a proper integral (integrand becomes unbounded at $x=2$, but this is outside the range of integration $3 \leqq x \leqq 10$ ).

(d) First kind (integration limits are infinite but integrand is bounded).

(e) This is a proper integral (since $\lim _{x \rightarrow 0+} \frac{1-\cos x}{x^{2}}=\frac{1}{2}$ by applying L' Hospital's rule).

12.2. Show how to transform the improper integral of the second kind, $\int_{t}^{2} \frac{d x}{\sqrt{x(2-x)}}$, into (a) an improper\\
integral of the first kind, (b) a proper integral.

(a) Consider $\int_{t}^{2-\epsilon} \frac{d x}{\sqrt{x(2-x)}}$, where $0<\epsilon<1$, say. Let $2-x=\frac{1}{y}$. Then the integral becomes $\int_{1}^{1 / \epsilon} \frac{d y}{y \sqrt{2 y-1}}$. As $\epsilon \rightarrow 0+$, we see that consideration of the given integral is equivalent to consideration $\int_{1}^{\infty} \frac{d y}{y \sqrt{2 y-1}}$, which is an improper integral of the first kind.

(b) Letting $2-x=v 2$ in the integral of (a), it becomes $2 \int_{\sqrt{\epsilon}}^{1} \frac{d v}{\sqrt{v^{2}+2}}$. We are thus led to consideration of $2 \int_{0}^{1} \frac{d v}{\sqrt{v^{2}+1}}$, which is a proper integral.

From this, we see that an improper integral of the first kind may be transformed into an improper integral of the second kind, and conversely (actually this can always be done).

We also see that an improper integral may be transformed into a proper integral (this can only sometimes be done).

\section*{Improper integrals of the first kind}
12.3. Prove the comparison test (Page 326) for convergence of improper integrals of the first kind.

Since $0 \leqq f(x) \leqq g(x)$ for $x \varepsilon a$, we have, using Property 7, Page 98,

$$
0 \leqq \int_{a}^{b} f(x) d x \leqq \int_{a}^{b} g(x) d x \leqq \int_{\alpha}^{\infty} g(x) d x
$$

But, by hypothesis, the last integral exists. Thus, $\lim _{b \rightarrow \infty} \int_{a}^{b} f(x) d x$ exists, and, hence, $\int_{\alpha}^{\infty} f(x) d x$ converges.

12.4. Prove the quotient test (a) on Page 326 .

By hypothesis, $\lim _{x \rightarrow \infty} \frac{f(x)}{g(x)}=A>0$. Then, given any $\epsilon>0$, we can find $N$ such that $\left|\frac{f(x)}{g(x)}-A\right|<\epsilon$ when $x \varepsilon N$. Thus, for $x \varepsilon N$, we have

$$
A-\in \leq \frac{f(x)}{g(x)} \leqq A+\in \quad \text { or } \quad(A-\in) g(x) \leqq f(x) \leqq(A+\in) g(x)
$$

Then


\begin{equation*}
(A-\in) \int_{N}^{b} g(x) d x \leqq \int_{N}^{b} f(x) d x \leqq(A+\in) \int_{N}^{b} g(x) d x \tag{1}
\end{equation*}


There is no loss of generality in choosing $A-\epsilon>0$.

If $\int_{a}^{\infty} g(x) d x$ converges, then by the inequality on the right of Equation (1),

$\lim _{b \rightarrow \infty} \int_{N}^{b} f(x) d x$ exists, and so $\int_{a}^{\infty} f(x) d x$ converges

If $\int_{a}^{\infty} g(x) d x$ diverges, then by the inequality on the left of Equation (1), $\lim _{b \rightarrow \infty} \int_{N}^{b} f(x) d x=\infty$, and so $\int_{a}^{\infty} f(x) d x$ diverges.

For the cases where $A=0$ and $A=\infty$, see Problem 12.41.

As seen in this and Problem 12.3, there is, in general, a marked similarity between proofs for infinite series and improper integrals.

12.5. Test for convergence: (a) $\int_{1}^{\infty} \frac{x d x}{3 x^{4}+5 x^{2}+1}$ and (b) $\int_{2}^{\infty} \frac{x^{2}-1}{\sqrt{x^{6}+16}} d x$.

(a) Method 1: For large $x$, the integrand is approximately $x / 3 \times 4=1 / 3 \times 3$.

Since $\frac{x}{3 x^{4}+5 x^{2}+1} \leqq \frac{1}{3 x^{3}}$ and $\frac{1}{3} \int_{t}^{\infty} \frac{d x}{x^{3}}$ converges ( $p$ integral with $p=3$ ), it follows by the comparison test that $\int_{1}^{\infty} \frac{x d x}{3 x^{4}+5 x^{2}+1}$ also converges.

Note that the purpose of examining the integrand for large $x$ is to obtain a suitable comparison integral.

Method 2: Let $f(x)=\frac{x}{3 x^{4}+5 x^{2}+1}, g(x)=\frac{1}{x^{3}}$. Since $\lim _{x \rightarrow \infty} \frac{f(x)}{3}$, and $\int_{1}^{\infty} g(x) d x$ converges,

$\int_{1}^{\infty} f(x) d x$ also converges by the quotient test. Note that in the comparison function $g(x)$, we have discarded the factor $\frac{1}{3}$. However, it could just as well\\
have been included.

$\begin{aligned} & \text { Method 3: } \\ & \text { converges. }\end{aligned} \lim _{x \rightarrow \infty} x^{3}\left(\frac{x}{3 x^{4}+5 x^{2}+1}\right)=\frac{1}{3}$. Hence, by Theorem 1, Page 324, the required integral

(b) Method 1: For large $\mathrm{x}$, the integrand is approximately $x^{2} / x^{6}=1 / x$

For $x \geq 2, \frac{x^{2}-1}{\sqrt{x^{6}+1}} \geq \frac{1}{2} . \frac{1}{x}$. Since $\frac{1}{2} \int_{2}^{\infty} \frac{d x}{x}$ diverges, $\int_{2}^{\infty} \frac{x^{2}-1}{\sqrt{x^{6}+16}} d x$ also diverges.

Method 2: Let $f(x)=\frac{x^{2}-1}{\sqrt{\mathrm{x}^{6}-16}}, g(x)=\frac{1}{x}$. Then, since $\lim _{x \rightarrow \infty} \frac{f(x)}{g(x)}=1$ and $\int_{2}^{\infty} g(x) d x$ diverges,

$\int_{2}^{\infty} f(x) d x$ also diverges.

Method 3: Since $\lim _{x \rightarrow \infty} x\left(\frac{x^{2}-1}{\sqrt{x^{6}+16}}\right)=1$, the required integral diverges by Theorem 1, Page 324.

Note that Method 1 may (and often does) require us to obtain a suitable inequality factor (in this case, $\frac{1}{2}$ or any positive constant less than $\frac{1}{2}$ ) before the comparison test can be applied. Methods 2 and 3, however, do not require this.

12.6. Prove that $\int_{0}^{\infty} e^{-x^{2}} d x$ converges.

$\lim _{x \rightarrow \infty} x^{2} e^{-x^{2}}=0$ (by L'Hospital's rule or otherwise). Then, by Theorem 1, with $A=0, p=2$, the given integral converges. Compare Problem 11.10(a).

12.7. Examine for convergence: (a) $\int_{1}^{\infty} \frac{\ln x}{x+a} d x$, where a is a positive constant and (b) $\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x$.

(a) $\lim _{x \rightarrow \infty} x \cdot \frac{\ln x}{x+a}=\infty$. Hence, by Theorem 1, Page 324, with $\mathrm{A}=\infty, \mathrm{p}=1$, the given integral diverges.

(b) $\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x=\int_{0}^{\pi} \frac{1-\cos x}{x^{2}} d x+\int_{\pi}^{\infty} \frac{1-\cos x}{x^{2}} d x$.

The first integral on the right converges [see Problem 12.1(e)].

Since $\lim _{x \rightarrow \infty} x^{3 / 2}\left(\frac{1-\cos x}{x^{2}}\right)=0$, the second integral on the right converges by Theorem 1, Page 324, with $A=0$ and $p=3 / 2$.

Thus, the given integral converges.

12.8. Test for convergence: (a) $\int_{-\infty}^{-1} \frac{e^{x}}{x} d x$ and (b) $\int_{-\infty}^{\infty} \frac{x^{3}+x^{2}}{x^{6}+1} d x$.

(a) Let $\mathrm{x}=-\mathrm{y}$. Then the integral becomes $-\int_{1}^{\infty} \frac{e^{-y}}{y} d y$.

Method 1: $\quad \frac{e^{-y}}{y} \leqq e^{-y}$ for $\leqq 1$. Then, since $\int_{1}^{\infty} \mathrm{e}^{-y} d y$ converges, $\int_{1}^{\infty} \frac{e^{-y}}{y} d y$ converges; hence, the given integral converges.

Method 2: $\lim _{x \rightarrow \infty} y^{2}\left(\frac{e^{-y}}{y}\right)=\lim _{y \rightarrow \infty} y e^{-y}=0$. Then the given integral converges by Theorem 1, Page 324, with $A=0$ and $p=2$.

(b) Write the given integral as $\int_{-\infty}^{0} \frac{x^{3}+x^{2}}{x^{6}+1} d x+\int_{0}^{\infty} \frac{x^{3}+x^{2}}{x^{6}+1} d x$. Letting $x=-y$ in the first integral, it becomes $-\int_{0}^{\infty} \frac{y^{3}-y^{2}}{y^{6}+1} d y$. Since $\lim _{y \rightarrow \infty} y^{3}\left(\frac{y^{3}-y^{2}}{y^{6}+1}\right)=1$, this integral converges.

Since $\lim _{x \rightarrow \infty} x^{3}\left(\frac{x^{3}+x^{2}}{x^{6}+1}\right)=1$, the second integral converges.

Thus, the given integral converges.

\section*{Absolute and conditional convergence for improper integrals of the first kind}
12.9. Prove that $\int_{\alpha}^{\infty} f(x) d x$ converges if $\int_{0}^{\infty}|f(x)| d x$ converges; i.e., an absolutely convergent integral is\\
convergent.

We have $-|f(x)| \leqq f(x) \leqq|f(x)|$; i.e., $0 \leqq f(x)+|f(x)| 2|f(x)|$. Then

$$
0 \leqq \int_{a}^{b}[f(x) \leqq+\mid f(x)] d x \leqq 2 \int_{a}^{b}|f(x)| d x
$$

If $\int_{\alpha}^{\infty}|f(x)| d x$ converges, it follows that $\left.\int_{\alpha}^{\infty}|f(x)+| f(x) \mid\right] d x$ converges. Hence, by subtracting $\int_{\alpha}^{\infty}|f(x)| d x$, which converges, we see that $\int_{\alpha}^{\infty} f(x) d x$ converges.

12.10. Prove that $\int_{t}^{\infty} \frac{\cos x}{x^{2}} d x$ converges.

Method 1: $\left|\frac{\cos x}{x^{2}}\right| \leqq \frac{1}{x^{2}}$ for $x \geqq 1$. Then by the comparison test, since $\int_{t}^{\infty} \frac{d x}{x^{2}}$ converges, it follows that $\int_{t}^{\infty}\left|\frac{\cos x}{x^{2}}\right| d x$ converges; $\int_{t}^{\infty}\left|\frac{\cos x}{x^{2}}\right| d x$ converges, i.e., $\int_{t}^{\infty} \frac{\cos x}{x^{2}}$ converges absolutely, and so converges by Problem 12.9.

Method 2: Since $\lim _{x \rightarrow \infty} x^{3 / 2}\left|\frac{\cos x}{e^{2}}\right|=\lim _{x \rightarrow \infty}\left|\frac{\cos x}{x^{1 / 2}}\right|=0$ it follows from Theorem 1, Page 324, with $A=0$ and $p=3 / 2$. that $\int_{t}^{\infty}\left|\frac{\cos x}{x^{2}}\right| d x$ converges, and, hence, $\int_{t}^{\infty} \frac{\cos x}{x^{2}} d x$ converges (absolutely).

12.11. Prove that $\int_{t}^{\infty} \frac{\sin x}{x} d x$ converges.

Since $\int_{0}^{1} \frac{\sin x}{x} d x$ converges $\left(\right.$ because $\frac{\sin x}{x}$ is continuous in $0<x \leqq 1$ and $\lim _{x \rightarrow 0+} \frac{\sin x}{x}=1$ ), we need only show that $\int_{1}^{\infty} \frac{\sin x}{x} d x$ converges.

Method 1: Integration by parts yields


\begin{equation*}
\int_{t}^{M} \frac{\sin x}{x} d x=-\left.\frac{\cos x}{x}\right|_{1} ^{M}+\int_{t}^{M} \frac{\cos x}{x^{2}} d x=\cos 1-\frac{\cos M}{M}+\int_{t}^{M} \frac{\cos x}{x^{2}} d x \tag{1}
\end{equation*}


or, on taking the limit on both sides of Equation (1) as $M \rightarrow \infty$ and using the fact that $\lim _{M \rightarrow \infty} \frac{\cos M}{M}=0$,


\begin{equation*}
\int_{1}^{\infty} \frac{\sin x}{x} d x=\cos 1+\int_{1}^{\infty} \frac{\cos x}{x^{2}} d x \tag{2}
\end{equation*}


Since the integral on the right of Equation (2) converges by Problem 12.10, the required result follows. The technique of integration by parts to establish convergence is often useful in practice.

Method 2:

$$
\begin{aligned}
\int_{0}^{\infty} \frac{\sin x}{x} d x & =\int_{0}^{\pi} \frac{\sin x}{x} d x+\int_{\pi}^{2 \pi} \frac{\sin x}{x} d x+\ldots+\int_{n \pi}^{(n+1) \pi} \frac{\sin x}{x} d x+\ldots \\
& =\sum_{n=0}^{\infty} \int_{n \pi}^{(n+1) \pi} \frac{\sin x}{x} d x
\end{aligned}
$$

Letting $x=v+n \pi$, the summation becomes

$$
\sum_{n=0}^{\infty}(-1)^{n} \int_{0}^{\pi} \frac{\sin v}{n+n \pi} d v \int_{0}^{\pi} \frac{\sin v}{v} d v+\int_{0}^{\pi} \frac{\sin v}{v+\pi} d v+\int_{0}^{\pi} \frac{\sin v}{v+2 \pi} d v-\ldots
$$

This is an alternating series. Since $\frac{1}{v+n \pi} \leqq \frac{1}{v+(n+1) \pi}$ and $\sin v \geqq 0$ in $[0, \pi]$, it follows that

$$
\int_{0}^{\pi} \frac{\sin v}{v+n \pi} d v \leqq \int_{0}^{\pi} \frac{\sin v}{v+(n+1) \pi} d v
$$

Also,

$$
\lim _{n \rightarrow \infty} \int_{0}^{\pi} \frac{\sin v}{v+n \pi} d v \leq \lim _{n \rightarrow \infty} \int_{0}^{\pi} \frac{d v}{n \pi}=0
$$

Thus, each term of the alternating series is, in absolute value, less than or equal to the preceding term, and the $n$th term approaches zero as $n \rightarrow \infty$. Hence, by the alternating series test (Page 281), the series and, thus, the integral converge.

12.12. Prove that $\int_{0}^{\infty} \frac{\sin x}{x} d x$ converges conditionally.

Since, by Problem 12.11, the given integral converges, we must show that it is not absolutely convergent; i.e., $\int_{0}^{\infty}\left|\frac{\sin x}{x}\right| d x$ diverges. diverges.

As in Problem 12.11, Method 2, we have


\begin{equation*}
\int_{0}^{\infty}\left|\frac{\sin x}{x}\right| d x=\sum_{n=0}^{\infty} \int_{n \pi}^{(n+1) \pi}\left|\frac{\sin x}{x}\right| d x=\sum_{n=0}^{\infty} \int_{0}^{\pi} \frac{\sin v}{v+n \pi} d v \tag{1}
\end{equation*}


Now $\frac{1}{v+n \pi} \geq \frac{1}{(n+1) \pi}$ for $0 \leqq v \leqq \pi$. Hence,


\begin{equation*}
\int_{0}^{\pi} \frac{\sin v}{v+n \pi} d v \geq \frac{1}{(n+1) \pi} \int_{9}^{\pi} \sin v d v=\frac{2}{(n+1) \pi} \tag{2}
\end{equation*}


Since $\sum_{n=0}^{\infty} \frac{2}{(n+1) \pi}$ diverges, the series on the right of Equation (1) diverges by the comparison test.

Hence, $\int_{0}^{\infty}\left|\frac{\sin x}{x}\right| d x$ diverges and the required result follows.

\section*{Improper integrals of the second kind, cauchy principal value}
12.13. (a) Prove that $\int_{-1}^{7} \frac{d x}{\sqrt[3]{x+1}}$ converges and (b) find its value.

The integrand is unbounded at $x=-1$. Then we define the integral as

$$
\lim _{\in \rightarrow 0+} \int_{-1+\in}^{7} \frac{d x}{\sqrt[3]{x+1}}=\left.\lim _{\epsilon \rightarrow 0+} \frac{(x+1)^{2 / 3}}{2 / 3}\right|_{-1+\epsilon} ^{7}=\lim _{\epsilon \rightarrow 0+}\left(6-\frac{3}{2} \epsilon^{2 / 3}\right)=6
$$

This shows that the integral converges to 6 .

12.14. Determine whether $\int_{-1}^{5} \frac{d x}{(x-1)^{3}}$ converges (a) in the usual sense and (b) in the Cauchy principal value sense.

(a) By definition,

$$
\begin{aligned}
\int_{-1}^{5} \frac{d x}{(x-1)^{3}} & =\lim _{\epsilon_{1} \rightarrow 0+} \int_{-1}^{1-\epsilon_{1}} \frac{d x}{(x-1)^{3}}+\lim _{\epsilon_{2} \rightarrow 0+} \int_{1+\epsilon_{2}}^{5} \frac{d x}{(x-1)^{3}} \\
& =\lim _{\epsilon_{1} \rightarrow 0+}\left(\frac{1}{8}-\frac{1}{2 \epsilon_{1}^{2}}\right)+\lim _{\epsilon_{2} \rightarrow 0+}\left(\frac{1}{2 \epsilon_{2}^{2}}-\frac{1}{32}\right)
\end{aligned}
$$

and, since the limits do not exist, the integral does not converge in the usual sense.

(b) Since

$$
\lim _{\epsilon \rightarrow 0+}\left\{\int_{-1}^{1-\epsilon} \frac{d x}{(x-1)^{3}}+\int_{1+\epsilon}^{5} \frac{d x}{(x-1)^{3}}\right\}=\lim _{\epsilon \rightarrow 0+}\left\{\frac{1}{8}-\frac{1}{2 \epsilon^{2}}+\frac{1}{2 \epsilon^{2}}-\frac{1}{32}\right\}=\frac{3}{32}
$$

the integral exists in the Cauchy principal value sense. The principal value is $3 / 32$.

12.15. Investigate the convergence of:\\
(a) $\int_{2}^{3} \frac{d x}{x^{2}\left(x^{3}-8\right)^{2 / 3}}$\\
(c) $\int_{1}^{5} \frac{d x}{\sqrt{(5-x)(x-1)}}$\\
(e) $\int_{0}^{\pi / 2} \frac{d x}{(\cos x)^{1 / n}}, n>1$\\
(b) $\int_{0}^{\pi} \frac{\sin x}{x^{3}} d x$\\
(d) $\int_{-1}^{1} \frac{2^{\sin ^{-1} x}}{1-x} d x$

(a) $\lim _{x \rightarrow 2+}(x-2)^{2 / 3} \cdot \frac{1}{x^{2}\left(x^{3}-8\right)^{2 / 3}}=\lim _{x \rightarrow 2+} \frac{1}{x^{2}}\left(\frac{1}{x^{2}+2 x+4}\right)^{2 / 3}=\frac{1}{8 \sqrt[3]{18}}$. Hence, the integral converges by Theorem 3(i), Page 326.

(b) $\lim _{x \rightarrow 0+} x^{2} \cdot \frac{\sin x}{x^{3}}=1$. Hence, the integral diverges by Theorem 3(ii) on Page 326.

(c) Write the integral as $\int_{1}^{3} \frac{d x}{\sqrt{(5-x)(x-1)}}+\int_{3}^{5} \frac{d x}{\sqrt{(5-x)(x-1)}}$.

Since $\lim _{x \rightarrow 1+}(x-1)^{1 / 2} \cdot \frac{1}{\sqrt{(5-x)(x-1)}}=\frac{1}{2}$, the first integral converges.

Since $\lim _{x \rightarrow 5-}(5-x)^{1 / 2} \cdot \frac{1}{\sqrt{(5-x)(x-1)}}=\frac{1}{2}$, the second integral converges.

Thus, the given integral converges.

(d) $\lim _{x \rightarrow 1-}(1-x) \cdot \frac{2^{\sin ^{-1} x}}{1-x}=2^{\pi / 2}$. Hence, the integral diverges.

Another method: $\quad \frac{2^{\sin ^{-1} x}}{1-x} \geqq \frac{2^{-\pi / 2}}{1-x}$, and $\int_{-1}^{1} \frac{d x}{1-x}$ diverges. Hence, the given integral diverges.

(e) $\lim _{x \rightarrow 1 / 2 \pi-}(\pi / 2-x)^{1 / n} \cdot \frac{1}{(\cos x)^{1 / n}}=\lim _{x \rightarrow 1 / 2 \pi-}\left(\frac{\pi / 2-x}{\cos x}\right)^{1 / n}=1$. Hence, the integral converges.

12.16. If $m$ and $n$ are real numbers, prove that $\int_{0}^{1} x^{m-1}(1-x)^{n-1} d x$ (a) converges if $m>0$ and $n>0$ simultaneously and (b) diverges otherwise.

(a) For $m \geqq 1$ and $n \geqq 1$ simultaneously, the integral converges, since the integrand is continuous in $0 \leqq x$ $\leq 1$. Write the integral as


\begin{equation*}
\int_{0}^{1 / 2} x^{m-1}(1-x)^{n-1} d x+\int_{1 / 2}^{1} x^{m-1}(1-x)^{n-1} d x \tag{1}
\end{equation*}


If $0<m<1$ and $0<n<1$, the first integral converges, since $\lim _{x \rightarrow 0}{ }^{+} x^{1-m} \cdot x^{m-1}(1-x)^{n-1}=1$, using Theorem 3(i), Page 326, with $p=1-m$ and $a=0$.

Similarly, the second integral converges, since $\left.\lim _{x \rightarrow 1-}(1-x)^{1-n} \cdot x\right)^{n-1}(1-x)^{n-1}=1$, using 4(i), Page 326, with $p=1-n$ and $b=1$.

Thus, the given integral converges if $m>0$ and $n>0$ simultaneously.

(b) If $m \leqq 0, \lim _{x \rightarrow 0+} x \cdot x^{m-1}(1-x)^{n-1}=\infty$. Hence, the first integral in Equation (1) diverges, regardless of the value of $n$, by Theorem 3(ii), Page 326, with $p=1$ and $a=0$. lows.

Similarly, the second integral diverges if $n \leqq 0$, regardless of the value of $m$, and the required result fol-

Some interesting properties of the given integral, called the beta integral or beta function, are considered in Chapter 15.

12.17. Prove that $\int_{0}^{\pi} \frac{1}{x} \sin \frac{1}{x} d x$ converges conditionally.

Letting $x=1 / y$, the integral becomes $\int_{1 / \pi}^{\infty} \frac{\sin y}{y} d y$ and the required result follows from Problem 12.12.

\section*{Improper integrals of the third kind}
12.18. If $n$ is a real number, prove that $\int_{0}^{\infty} x^{n-1} e^{-x} d x$ (a) converges if $n>0$ and (b) diverges if $n \leqq 0$.

Write the integral as


\begin{equation*}
\int_{0}^{1} x^{n-1} e^{-x} d x+\int_{1}^{\infty} x^{n-1} e^{-x} d x \tag{1}
\end{equation*}


(a) If $n \geqq 1$, the first integral in Equation (1) converges, since the integrand is continuous in $0 \leqq x \leqq 1$.

If $0<n<1$, the first integral in Equation (1) is an improper integral of the second kind at $x=0$. Since $\lim _{x \rightarrow 0^{+}} x^{1-n} \cdot x^{n-1} e^{-x}=1$, the integral converges by Theorem 3(i), Page 326, with $p=1-n$ and $a=0$.

Thus, the first integral converges for $n>0$.

If $n>0$, the second integral in Equation (1) is an improper integral of the first kind. Since $\lim _{x \rightarrow \infty} x^{2} \cdot x^{n-1}$ $e^{-x}=0$ (by L'Hospital's rule or otherwise), this integral converges by Theorem 1(i), Page 324, with $p=2$.

Thus, the second integral also converges for $n>0$, and so the given integral converges for $n>0$.

(b) If $\mathrm{n} \leqq 0$, the first integral of Equation (1) diverges, since $\lim _{x \rightarrow 0+} x \cdot x^{n-1} e^{-x}=\infty$ [Theorem 3(ii), Page 326].

If $n \leqq 0$, the second integral of Equation (1) converges, since $\lim _{x \rightarrow \infty} \cdot x^{n-1} e^{-x}=0$ [Theorem 1(i), Page 324].

Since the first integral in Equation (1) diverges while the second integral converges, their sum also diverges; i.e., the given integral diverges if $n \leqq 0$.

Some interesting properties of the given integral, called the gamma function, are considered in Chapter 15 .

\section*{Uniform convergence of improper integrals}
12.19. (a) Evaluate $\phi(\alpha)=\int_{0}^{\infty} \alpha e^{-\alpha x} d x$ for $\alpha>0$. (b) Prove that the integral in (a) converges uniformly to 1 for $\alpha$ $\epsilon \alpha 1>0$. (c) Explain why the integral does not converge uniformly to 1 for $\alpha>0$.

(a) $\phi(\alpha)=\lim _{b \rightarrow \infty} \int_{a}^{b} \alpha e^{-a e} d x=\lim _{b \rightarrow \infty}-\left.e^{-\alpha x}\right|_{x=0} ^{b}=\lim _{b \rightarrow \infty} 1-e^{-a b}=1$ if $\alpha>0$

Thus, the integral converges to 1 for all $\alpha>0$.

(b) Method 1, using definition: The integral converges uniformly to 1 in $\alpha \varepsilon \alpha_{1}>0$ if for each $\epsilon>0$ we can find $N$, depending on $\epsilon$ but not on $\alpha$, such that $\left|1-\int_{0}^{u} \alpha e^{-\alpha x} d x\right|<\in$ for all $u>N$.

Since $\left|1-\int_{0}^{u} \alpha e^{-\alpha x} d x\right|=\left|1-\left(1-e^{-\alpha u}\right)\right|=e^{-a u}<e^{-\alpha_{1} u}<\in$ for $u>\frac{1}{\alpha_{1}} \ln \frac{1}{\in}=N$, the result follows.

Method 2, using the Weierstrass $M$ test: Since $\lim _{x \rightarrow \infty} x^{2} \cdot \alpha e^{-\alpha x}=0$ for $\alpha \alpha_{1}>0$, we can choose $\left|\alpha e^{-\alpha x}\right|<\frac{1}{x^{2}}$ for sufficiently large $x$-say, $x \geqq x_{0}$. Taking $M(x)=\frac{1}{x^{2}}$ and noting that $\int_{x_{0}}^{\infty} \frac{d x}{x^{2}}$ converges, it follows that the given integral is uniformly convergent to 1 for $\alpha \varepsilon \alpha_{1}>0$.

(c) As $\alpha 1 \rightarrow 0$, the number $\mathrm{N}$ in the first method of (b) increases without limit, so that the integral cannot be uniformly convergent for $\alpha>0$.

12.20. If $\phi(\alpha)=\int_{0}^{\infty} f(x, \alpha) d x$ is uniformly convergent for $\alpha_{1} \leqq \alpha \leqq \alpha_{2}$, prove that $\phi(\alpha)$ is continuous this interval.

Let $\phi(\boldsymbol{\alpha})=\int_{u}^{u} f(x, \alpha) d x+R(u, \boldsymbol{\alpha})$, where $\mathrm{R}(u, \boldsymbol{\alpha})=\int_{u}^{\infty} f(x, \boldsymbol{\alpha}) d x$.

Then $\phi(\alpha+h)=\int_{u}^{u} f(x, \alpha+h) d x+R(u, \alpha+h)$ and so

$$
\phi(\alpha+h)-\phi(\alpha)=\int_{u}^{u}\{f(x, \alpha+h)-f(x, \alpha)\} d x+R(u, \alpha+h)-R(u, \alpha)
$$

Thus,


\begin{equation*}
|\phi(\alpha+h)-\phi(\alpha)| \leqq \int_{a}^{u}|f(x, \alpha+h)-f(x, \alpha)| d x+\mid R(u, \alpha+h|+| R(u, \alpha) \mid \tag{1}
\end{equation*}


Since the integral is uniformly convergent in $(\alpha)_{1} \leq(\alpha) \leq \alpha_{2}$, we can, for find $N$ independent of $(\alpha)$ such that for $(u)>N$,


\begin{equation*}
|R(u, \alpha+h)|<\epsilon / 3,|R(u, \alpha)|<\epsilon / 3 \tag{2}
\end{equation*}


Since $f(x, \alpha)$ is continuous, we can find $\delta>0$ corresponding to each $\epsilon>0$ such that


\begin{equation*}
\int_{a}^{v}|f(x, \alpha+h)-f(x, \alpha)| d x<\in / 3 \quad \text { for }|h|<\delta \tag{3}
\end{equation*}


Using Equations (2) and (3) in (1), we see that $|\phi(\alpha+h)-\phi(\alpha)|<\epsilon$ for $|(h)|<\delta$, so that $\phi(\alpha)$ is continuous.

Note that in this proof we assume that both $\alpha$ and $\alpha+h$ are in the interval $\alpha_{1} \leq \alpha \leq \alpha_{2}$. Thus, if $\alpha=\alpha_{1}$, for example, $h>0$ and right-hand continuity is assumed.

Also note the analogy of this proof with that for infinite series.

Other properties of uniformly convergent integrals can be proved similarly.

12.21. (a) Show that $\lim _{\alpha \rightarrow 0+} \int_{0}^{\infty} \alpha e^{-\alpha x} d x \neq \int_{0}^{\infty}\left(\lim _{\alpha \rightarrow 0+} \alpha e^{-\alpha x}\right) d x$ and (b) explain the result in (a).

(a) $\lim _{\alpha \rightarrow 0+} \int_{0}^{\infty} \alpha e^{-\alpha x} d x=\lim _{\alpha \rightarrow 0+}=1$ by Problem 12.19(a).

$\int_{0}^{\infty}\left(\lim _{\alpha \rightarrow 0+} \alpha e^{-\alpha x}\right) d x=\int_{0}^{\infty} 0 d x=0$. Thus, the required result follows.

(b) Since $\phi(\alpha)=\int_{0}^{\infty} \alpha e^{-\alpha x} d x$ is not uniformly convergent for $\alpha \geqq 0$ (see Problem 12.19), there is no guarantee that $\phi(\alpha)$ will be continuous for $\alpha \geqq 0$. Thus, $\lim _{\alpha \rightarrow 0+} \phi(\alpha)$ may not be equal to $\phi(0)$.

12.22. (a) Prove that $\int_{0}^{\infty} e^{-\alpha x} \cos r x d x=\frac{\alpha}{\alpha^{2}+r^{2}}$ for $\alpha>0$ and any real value of $r$. (b) Prove that the integral in

(a) converges uniformly and absolutely for $a \leqq \alpha \leqq b$, where $0<a<b$ and any $\mathrm{r}$.

(a) From integration formula 34, Page 103, we have

$$
\lim _{M \rightarrow \infty} \int_{0}^{M} e^{-\alpha x} \cos r x d x=\left.\lim _{M \rightarrow \infty} \frac{e^{-\alpha x}(r \sin r x-\alpha \cos r x)}{\alpha^{2}+r^{2}}\right|_{0} ^{M}=\frac{\alpha}{\alpha^{2}+r^{2}}
$$

(b) This follows at once from the Weierstrass $M$ test for integrals, by noting that $\left|e^{-\alpha x} \cos r x\right| \leqq e^{-\alpha x}$ and $\int_{0}^{\infty} e^{-\alpha x} d x$ converges.

\section*{Evaluation of definite integrals}
12.23. Prove that $\int_{0}^{\pi / 2} \ln \sin x d x=\frac{\pi}{2} \ln 2$.

The given integral converges [Problem 12.42(f)]. Letting $x=\pi / 2-y$,

$$
I=\int_{0}^{\pi / 2} \ln \sin x d x=\int_{0}^{\pi / 2} \ln \cos y d y=\int_{0}^{\pi / 2} \ln \cos x d x
$$

Then


\begin{align*}
2 I & =\int_{0}^{\pi / 2}(\ln \sin x+\ln \cos x) d x=\int_{0}^{\pi / 2} \ln \left(\frac{\sin 2 x}{2}\right) d x  \tag{1}\\
& =\int_{0}^{\pi / 2} \ln \sin 2 x d x-\int_{0}^{\pi / 2} \ln 2 d x=\int_{0}^{\pi / 2} \ln \sin 2 x d x-\frac{\pi}{2} \ln 2
\end{align*}


Letting $2 x=v$,

$$
\begin{aligned}
\int_{02}^{\pi / 2} \ln \sin 2 x d x & =\frac{1}{2} \int_{0}^{\pi} \ln \sin v d v=\frac{1}{2}\left\{\int_{0}^{\pi / 2} \ln \sin v d v\right\} \\
& \left.=\frac{1}{2}(I+I)=I \quad \text { (letting } v=\pi-\text { in the last integral }\right)
\end{aligned}
$$

Hence, Equation $(l)$ becomes $2 I=I \frac{\pi}{2} \ln$ on $I=-\frac{\pi}{2} \ln 2$.

12.24. Prove that $\int_{0}^{\pi} x \ln \sin x d x=-\frac{\pi^{2}}{2} \ln 2$.

Let $x=\pi-y$. Then, using the results in the preceding problem,

$$
\begin{aligned}
J=\int_{0}^{\pi} x \ln \sin x d x & =\int_{0}^{\pi}(\pi-u) \ln \sin u d u=\int_{0}^{\pi}(\pi-x) \ln \sin x d x \\
& =\pi \int_{0}^{\pi} \ln \sin x d x-\int_{0}^{\pi} x \ln \sin x d x \\
& =-\pi^{2} \ln 2-J
\end{aligned}
$$

or $\quad J=-\frac{\pi^{2}}{2} \ln 2$.

12.25. (a) Prove that $\phi(\alpha)=\int_{0}^{\infty} \frac{d x}{x^{2+\alpha}}$ is uniformly convergent for $\alpha \geqq 1$. (b) Show that $\phi(\alpha)=\frac{\pi}{2 \sqrt{\alpha}}$.

(c) Evaluate $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{2}}$. (d) Prove that $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{n+1}}=\int_{0}^{\pi / 2} \cos ^{2 n} \theta d \theta=\frac{1.3 .5 \cdots(2 n-1)}{2.4 .6 \cdots(2 n)} \frac{\pi}{2}$.

(a) The result follows from the Weierstrass test, since $\frac{1}{x^{2}+\alpha} \leqq \frac{1}{x^{2}+1}$ for $a \geqq 1$ and $\int_{0}^{\infty} \frac{d x}{x^{2}+1}$ converges.

(b) $\phi(\alpha)=\lim _{b \rightarrow \infty} \int_{0}^{b} \frac{d x}{x^{2}+\alpha}=\left.\lim _{b \rightarrow \infty} \frac{1}{\sqrt{\alpha}} \tan ^{-1} \frac{x}{\sqrt{\alpha}}\right|_{0} ^{b}=\lim _{b \rightarrow \infty} \frac{1}{\sqrt{\alpha}} \tan ^{-1} \frac{b}{\sqrt{\alpha}}=\frac{\pi}{2 \sqrt{\alpha}}$\\
(c) From (b), $\int_{0}^{\infty} \frac{d x}{x^{2}+\alpha}=\frac{\pi}{2 \sqrt{\alpha}}$. Differentiating both sides with respect to $\alpha$, we have

$$
\int_{0}^{\infty} \frac{\partial}{\partial \alpha}\left(\frac{1}{x^{2}+\alpha}\right) d x=-\int_{0}^{\infty} \frac{d x}{\left(x^{2}+\alpha\right)^{2}}=\frac{\pi}{4} \alpha^{-3 / 2}
$$

the result being justified by Theorem 8 , Page 328, since $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+\alpha\right)^{2}}$ is uniformly convergent for $\alpha \geqq 1$ (because $\frac{1}{\left(x^{2}+\alpha\right)^{2}} \leq \frac{1}{\left(x^{2}+1\right)^{2}}$ and $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{2}}$ converges ).

Taking the limit as $\alpha \rightarrow 1+$, using Theorem 6, Page 328, we find $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{2}}=\frac{\pi}{4}$.

(d) Differentiating both sides of $\int_{0}^{\infty} \frac{d x}{x^{2}+\alpha}=\frac{\pi}{2} \alpha^{-1 / 2} n$ times, we find

$$
(-1)(-2) \cdots(-n) \int_{0}^{\infty} \frac{d x}{\left(x^{2}+\alpha\right)^{n+1}}=\left(-\frac{1}{2}\right)\left(-\frac{3}{2}\right)\left(-\frac{5}{2}\right) \cdots\left(-\frac{2 n-1}{2}\right) \frac{\pi}{2} \alpha^{-(2 n-1,2)}
$$

where justification proceeds as in (c). Letting $\alpha \rightarrow 1+$, we find

$$
\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{n+1}}=\frac{1 \cdot 3 \cdot 5 \cdots(2 n-1)}{2^{n} n !} \frac{\pi}{2}=\frac{1 \cdot 3 \cdot 5 \cdots(2 n-1)}{2 \cdot 4 \cdot 6 \cdots(2 n)} \frac{\pi}{2}
$$

Substituting $x=\tan \theta$, the integral becomes $\int_{0}^{\pi / 2} \cos ^{2 n} \theta d \theta$ and the required result is obtained.

12.26. Prove that $\int_{0}^{\infty} \frac{e^{-a x}-e^{b x}}{x \sec r x} d x=\frac{1}{2} \ln \frac{b^{2}+r^{2}}{a^{2}+e^{2}}$ where $a, b>0$.

From Problem 12.22 and Theorem 7, Page 328, we have

$$
\int_{x=0}^{\infty}\left\{\int_{\alpha=a}^{b} e^{-\alpha x} \cos r x d \alpha\right\} d x=\int_{\alpha=a}^{b}\left\{\int_{x=0}^{\infty} e^{-\alpha x} \cos r x d x\right\} d \alpha
$$

or

$$
\left.\int_{x=0}^{\infty} \frac{e^{\alpha x} \cos r x}{-x}\right|_{\alpha=a} ^{b} d x=\int_{\alpha=a}^{b} \frac{\alpha}{\alpha^{2}+r^{2}} d \alpha
$$

i.e.,

$$
\int_{0}^{\infty} \frac{e^{-a x}-e^{-b x}}{x \sec r x} d x=\frac{1}{2} \ln \frac{b^{2}+r^{2}}{a^{2}+r^{2}}
$$

12.27. Prove that $\int_{0}^{\infty} e^{\alpha x} \frac{1-\cos x}{x^{2}} d x=\tan ^{-1} \frac{1}{\alpha}-\frac{\alpha}{2} \ln \left(\alpha^{2}+1\right), \alpha>0$.

By Problem 12.22 and Theorem 7, Page 328, we have

or

$$
\int_{0}^{r}\left\{\int_{0}^{\infty} e^{-\alpha x} \cos r x d x\right\} d r=\int_{0}^{\infty}\left\{\int_{0}^{r} e^{-a x} \cos r x d r\right\} d x
$$

$$
\int_{0}^{\infty} e^{-a x} \frac{\sin r x}{x} d x=\int_{0}^{r} \frac{\alpha}{\alpha^{2}+r^{2}}=\tan ^{-1} \frac{r}{\alpha}
$$

Integrating again with respect to $r$ from 0 to $r$ yields

$$
\int_{0}^{\infty} e^{-a x} \frac{1-\cos r x}{x^{2}} d x=\int_{0}^{r} \tan ^{-1} \frac{r}{\alpha} d r=r \tan ^{-1} \frac{r}{\alpha}-\frac{\alpha}{2} \ln \left(\alpha^{2}+r^{2}\right)
$$

using integration by parts. The required result follows on letting $r=1$.

12.28. Prove that $\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x=\frac{\pi}{2}$.

Since $e^{-a x} \frac{1-\cos x}{x^{2}} \leqq \frac{1-\cos x}{x^{2}}$ for $\alpha \geqq 0, x \geqq 0$ and $\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x$ converges [see Problem 12.7(b)], it follows by the Weierstrass test that $\int_{0}^{\infty} e^{\alpha x} \frac{1-\cos x}{x^{2}} d x$ is uniformly convergent and represents a continuous function of $\alpha$ for $\alpha \geqq 0$ (Theorem 6, Page 328). Then, letting $\alpha \rightarrow 0+$, using Problem 12.27, we have

$$
\lim _{\alpha \rightarrow 0} \int_{0}^{\infty} e^{-\alpha x} \frac{1-\cos x}{x^{2}} d x=\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x=\lim _{\alpha \rightarrow 0}\left\{\tan ^{-1} \frac{1}{\alpha}-\frac{\alpha}{2} \ln \left(\alpha^{2}+1\right)\right\}=\frac{\pi}{2}
$$

12.29. Prove that $\int_{0}^{\infty} \frac{\sin x}{x}=\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=\frac{\pi}{2}$.

Integrating by parts, we have

$$
\int_{\epsilon}^{M} \frac{1-\cos x}{x^{2}} d x=\left.\left(-\frac{1}{x}\right)(1-\cos x)\right|_{\epsilon} ^{M}+\int_{\epsilon}^{M} \frac{\sin x}{x} d x=\frac{1-\cos \epsilon}{\epsilon}-\frac{1-\cos M}{M}+\int_{\epsilon}^{M} \frac{\sin x}{x} d x
$$

Taking the limit as $\epsilon \rightarrow 0+$ and $M \rightarrow \infty$ shows that

$$
\int_{0}^{\infty} \frac{\sin x}{x} d x=\int_{0}^{\infty} \frac{1-\cos x}{x} d x=\frac{\pi}{2}
$$

Since $\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x=2 \int_{0}^{\infty} \frac{\sin ^{2}(x / 2)}{x^{2}} d x=\int_{0}^{\infty} \frac{\sin ^{2} u}{u^{2}} d u$ on letting $\mathrm{u}=x / 2$, we also have $\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x=\frac{\pi}{2}$.

12.30. Prove that $\int_{0}^{\infty} \frac{\sin ^{3} x}{x} d x=\frac{\pi}{4}$.

$$
\begin{aligned}
\sin ^{3} x & =\left(\frac{e^{i x}-e^{-i x}}{2 i}\right)^{2}=\frac{\left(e^{i x}\right)^{3}-3\left(e^{i x}\right)^{2}\left(e^{-i x}\right)+3\left(e^{i x}\right)\left(e^{i x}\right)^{2}-\left(e^{-i x}\right)^{3}}{(2 i)^{3}} \\
& =-\frac{1}{4}\left(\frac{e^{-3 i x}-e^{-3 i x}}{2 i}\right)+\frac{3}{4}\left(\frac{e^{i x}-e^{-i x}}{2 i}\right)=-\frac{1}{4} \sin 3 x+\frac{3}{4} \sin x
\end{aligned}
$$

Then

$$
\begin{aligned}
\int_{0}^{\infty} \frac{\sin ^{3} x}{x} d x & =\frac{3}{4} \int_{0}^{\infty} \frac{\sin x}{x} d x-\frac{1}{4} \int_{0}^{\infty} \frac{\sin 3 x}{x} d x=\frac{3}{4} \int_{0}^{\infty} \frac{\sin x}{x} d x-\frac{1}{4} \int_{0}^{\infty} \frac{\sin u}{u} d u \\
& =\frac{3}{4}\left(\frac{\pi}{2}\right)-\frac{1}{4}\left(\frac{\pi}{2}\right)=\frac{\pi}{4}
\end{aligned}
$$

\section*{Miscellaneous problems}
12.31. Prove that $\int_{0}^{\infty} e^{-x^{2}} d x=\sqrt{\pi} / 2$.

By Problem 12.6, the integral converges. Let $I_{M}=\int_{0}^{M} e^{-x^{2}} d x=\int_{0}^{M} e^{-y^{2}} d y$ and let $\lim _{M \rightarrow \infty} I_{M}=I$, the required value of the integral. Then

$$
\begin{aligned}
I_{M}^{2} & =\left(\int_{0}^{M} e^{-x^{2}} d x\right)\left(\int_{0}^{M} e^{-x^{2}} d y\right) \\
& =\int_{0}^{M} \int_{0}^{M} e^{-\left(x^{2}+y^{2}\right)} d x d y \\
& =\iint_{M}^{-\left(x^{2}+y^{2}\right)} d x d y
\end{aligned}
$$

where $\Re_{M}$ is the square $O A C E$ of side $M$ (see Figure 12.3). Since the integrand is positive, we have


\begin{equation*}
\iint_{M} e^{-\left(x^{2}+y^{2}\right)} d x d y \leqq I_{M}^{2} \leqq \iint_{M} e^{-\left(x^{2}+y^{2}\right)} d x d y \tag{1}
\end{equation*}


where $\Re_{1}$ and $\Re_{2}$ are the regions in the first quadrant bounded by the circles having radii $M$ and $M \sqrt{2}$, respectively.

Using polar coordinates, we have, from Equation (1),


\begin{equation*}
\int_{\phi=0}^{\pi / 2} \int_{p=0}^{M} e^{-\rho^{2}} \rho d \rho d \phi \leqq I_{M}^{2} \leqq \int_{\phi=0}^{\pi / 2} \int_{\pi=0}^{M \sqrt{2}} e^{-\rho^{2}} \rho d \rho d \phi \tag{2}
\end{equation*}


or


\begin{equation*}
\frac{\pi}{4}\left(1-e^{M^{2}}\right) \leqq I_{M}^{2} \leqq \frac{\pi}{4}\left(1-e^{-2 M^{2}}\right) \tag{3}
\end{equation*}


Then, taking the limit as $M \rightarrow \infty$ in Equation (3), we find $\lim _{M \rightarrow \infty} I^{2}{ }_{M}=I^{2}=\pi / 4$ and $I=\sqrt{\pi} / 2$

12.32. Evaluate $\int_{0}^{\infty} e^{-x^{2}} \cos \alpha x d x$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-352}
\end{center}

Figure 12.3

Let $I(\alpha)=\int_{0}^{\infty} e^{-x^{2}} \cos \alpha x d x$. Then, using integration by parts and appropriate limiting procedures.

$$
\frac{d t}{d \alpha}=\int_{0}^{\infty}-x e^{-x^{2}} \sin \alpha x d x=\left.\frac{1}{2} e^{-x^{2}} \sin \alpha x\right|_{0} ^{\infty}-\frac{1}{2} \alpha \int_{0}^{\infty} e^{-x^{2}} \cos \alpha x d x=-\frac{\alpha}{2} I
$$

The differentiation under the integral sign is justified by Theorem 8, Page 328, and the fact that $\int_{0}^{\infty} x e^{-x^{2}} \sin$ $\alpha x d x$ is uniformly convergent for all $\alpha$ (since by the Weierstrass test, $\left|x e^{-x_{2}} \sin \alpha x\right| \leqq x e^{-x_{2}}$ and $\int_{0}^{\infty} x e^{-x^{2}} d x$ converges).

From Problem 12.31 and the uniform convergence, and thus continuity, of the given integral (since $l e^{-x 2}$ $\cos \alpha x \mid \leqq e^{-x 2}$ and $\int_{0}^{\infty} e^{-x 2} d x$ converges, so that that Weierstrass test applies), we have $I(0)=$ $\lim _{\alpha \rightarrow 0} I(\alpha)=\frac{1}{2} \sqrt{\pi}$.

Solving $\frac{d I}{d \alpha}=-\frac{\alpha}{2} I$ subject to $I(0)=\frac{\sqrt{\pi}}{2}$, we find $I(\alpha)=\frac{\sqrt{\pi}}{2} e^{-\alpha^{2} / 4}$

12.33. (a) Prove that $I(\alpha)=\int_{0}^{\infty} e^{-(x-\alpha / x)^{2}} d x=\frac{\sqrt{\pi}}{2}$. (b) Evaluate $\int_{0}^{\infty} \mathrm{e}^{-\left(x^{2}+x^{-2}\right)} d x$.

(a) We have $I^{\prime}(\alpha)=2 \int_{0}^{\infty} e^{-(x-\alpha / x)^{2}}\left(1-\alpha / x^{2}\right) d x$

The differentiation is proved valid by observing that the integrand remains bounded as $x \rightarrow 0+$ and that for sufficiently large $x$,

$$
e^{-(x-\alpha / x) 2}\left(1-\alpha / x^{2}\right)=e^{-x 2}+2 \alpha-\alpha^{2 / x 2}\left(1-\alpha / x^{2}\right) \leqq e^{2 \alpha} e^{-x^{2}}
$$

so that $I^{\prime}(\alpha)$ converges uniformly for $\alpha \varepsilon 0$ by the Weierstrass test, since $\int_{0}^{\infty} e^{-x^{2}} d x$ converges. Now

$$
I^{\prime}(\alpha)=2 \int_{0}^{\infty} e^{-(x-\alpha / x)^{2}} d x-2 \alpha \int_{0}^{\infty} \frac{e^{-(x-\alpha / x)^{2}}}{x^{2}} d x=0
$$

as seen by letting $\alpha / x=y$ in the second integral. Thus, $I(\alpha)=c$, a constant. To determine $c$, let $\alpha \rightarrow 0+$ in the required integral and use Problem 12.31 to obtain $c=\sqrt{\pi} / 2$.

(b) From (a), $\int_{0}^{\infty} e^{(x-\alpha / x)^{2}} d x=\int_{0}^{\infty} e^{-\left(x^{2}+\alpha^{2} x^{-2}\right)} d x=e^{2 \alpha} \int_{0}^{\infty} e^{-\left(x^{2}+\alpha^{2} x^{-2}\right)} d x=\frac{\sqrt{\pi}}{2}$.

Then $\int_{0}^{\infty} e^{-\left(x^{2}+\alpha^{2} x^{-2}\right)} d x=\frac{\sqrt{\pi}}{2} e^{-2 \alpha}$. Putting $\alpha=1, \int_{0}^{\infty} e^{-\left(x^{2}+x^{-2}\right)} d x=\frac{\sqrt{\pi}}{2} e^{-2}$.

12.34. Verify the results: (a) $\mathscr{L}\left\{e^{a x}\right\}=\frac{1}{s-\alpha}, s>a$ and (b) $\mathscr{L}\{\cos a z\}=\frac{s}{s^{2}+a^{2}}, s>0$.

(a)

$$
\begin{aligned}
\mathscr{L}\left\{e^{a x}\right\} & =\int_{0}^{\infty} e^{-s x} e^{a x} d x=\lim _{M \rightarrow \infty} \int_{0}^{M} e^{-(s-a) x} d x \\
& =\lim _{M \rightarrow \infty} \frac{1-e^{-(s-a) M}}{s-a}=\frac{1}{s-a} \quad \text { if } s>a
\end{aligned}
$$

(b) $\mathscr{L}\{\cos a x)=\int_{0}^{\infty} e^{-s x} \cos a x d x=\frac{s}{s^{2}+a^{2}}$ by Problem 12.22 with $\alpha=s, r=a$.

Another method, using complex numbers. From (a), $\mathscr{L}\left\{e^{a x}\right\}=\frac{1}{s-a}$. Replace $a$ by ai. Then

$$
\begin{aligned}
\mathscr{L}\left\{e^{a i x}\right\} & =\mathscr{L}\{\cos a x+i \sin a x\}=\mathscr{L}\{\cos a x\}+i \mathscr{L}\{\sin a x\} \\
& =\frac{1}{s-a i}=\frac{s+a i}{s^{2}+a^{2}}=\frac{s}{s^{2}+a^{2}}+i \frac{a}{s^{2}+a^{2}}
\end{aligned}
$$

Equating real and imaginary parts:

$$
\mathscr{L}\{\cos a x\}=\frac{s}{s^{2}+a^{2}}, \mathscr{L}\{\sin a x\}=\frac{a}{s^{2}+a^{2}}
$$

This formal method can be justified using the methods in Chapter 16.

12.35. Prove that (a) $\mathscr{L}\left\{Y^{\prime}(x)\right\}=$ (s) $\mathscr{L}\{Y(x)\}-Y(0)$ and (b) $\mathscr{L}\left\{Y^{\prime \prime}(x)\right\}=s^{2} \mathscr{L}\{Y(x)\}-s Y(0)-Y^{\prime}(0)$ under suitable conditions on $Y(x)$.

(a) By definition (and with the aid of integration by parts),

$$
\begin{aligned}
\mathscr{L}\left\{Y^{\prime}(x)\right\} & =\int_{0}^{\infty} e^{-s x} Y^{\prime}(x) d x=\lim _{M \rightarrow 0} \int_{0}^{M} e^{-s x} Y^{\prime}(x) d x \\
& =\lim _{M \rightarrow \infty}\left\{\left.e^{-s x} Y(x)\right|_{0} ^{M}+s \int_{0}^{M} e^{-s x} Y(x) d x\right\} \\
& =s \int_{0}^{\infty} e^{-s x} Y(x) d x-Y(0)=s \mathscr{L}\{(x)\}-Y(0)
\end{aligned}
$$

assuming that $s$ is such that $\lim _{M \rightarrow \infty} e^{-s M} Y(M)=0$.

(b) Let $U(x)=Y^{\prime}(x)$. Then by (a), $\mathscr{L}\left\{U^{\prime}(x)\right\}-U(0)$. Thus,

$$
\begin{gathered}
\mathscr{L}\left\{Y^{\prime}(x)\right\}=s \mathscr{L}\left\{Y^{\prime}(x)\right\}-Y^{\prime}(0)=s[s \mathscr{L}\{Y(x)\}-Y(0)]-Y^{\prime}(0) \\
=s^{2} \mathscr{L}\{Y(x)\}-s Y(0)-Y^{\prime}(0)
\end{gathered}
$$

12.36. Solve the differential equation $Y^{\prime \prime}(x)+Y(x)=x, Y(0)=0, Y^{\prime}(0)=2$.

Take the Laplace transform of both sides of the given differential equation. Then by Problem 12.35.

$$
\mathscr{L}\left\{Y^{\prime \prime}(x)+Y(x)\right\}=\mathscr{L}\{x\}, \mathscr{L}\left\{Y^{\prime \prime}(x)\right\}+\mathscr{L}\{Y(x)\}=1 / s^{2}
$$

and so

$$
s^{2} \mathscr{L}\{Y(x)\}-s \mathscr{L}(0)-Y^{\prime}(0)+\mathscr{L}\{Y(x)\}=1 / s^{2}
$$

Solving for $\mathscr{L}\{Y(x)\}$ using the given conditions, we find


\begin{equation*}
\mathscr{L}\{Y(x)\}=\frac{2 s^{2}}{s^{2}\left(s^{2}+1\right)}=\frac{1}{s^{2}}+\frac{1}{s^{2}+1} \tag{1}
\end{equation*}


by methods of partial fractions.

Since $\frac{1}{s^{2}}=\mathscr{L}\{x\}$ and $\frac{1}{s^{2}+1}=\mathscr{L}\{\sin x\}$, it follows that $\frac{1}{s^{2}}+\frac{1}{s^{2}+1}=\mathscr{L}\{x+\sin x\}$

Hence, from (1), $\mathscr{L}\{Y(x)\}=\mathscr{L}\{x+\sin x\}$, from which we can conclude that $Y(x)=x+\sin x$, which is, in fact, found to be a solution.

Another method: If $\mathscr{L}\{F(x)\}=f(s)$, we call $f(s)$ the inverse Laplace transform of $F(x)$ and write $f(s)=\mathscr{L}^{-1}$ $\{F(x)\}$.

By Problem 12.78. $\mathscr{L}^{-1}\{f(s)+g(s)\}=\mathscr{L}^{-1}=\mathscr{L}^{-1}\left\{f(s)+\mathscr{L}^{-1}\{g(s)\}\right.$. Then, from Equation (1),

$$
Y(x)=\mathscr{L}^{-1}\left\{\frac{1}{s^{2}}+\frac{1}{s^{2}+1}\right\}=\mathscr{L}^{-1}\left\{\frac{1}{s^{2}}\right\}+\mathscr{L}^{-1}\left\{\frac{1}{s^{2}+1}\right\}=x+\sin
$$

Inverse Laplace transforms can be read from Table 12.1.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Improper integrals of the first kind}
12.37. Test for convergence:\\
(a) $\int_{0}^{\infty} \frac{x^{2}+1}{x^{4}+1} d x$\\
(d) $\int_{0}^{\infty} \frac{d x}{x^{4}+4}$\\
(g) $\int_{0}^{\infty} \frac{x^{2} d x}{\left(x^{2}+x+1\right)^{5 / 2}}$\\
(b) $\int_{2}^{\infty} \frac{x d x}{\sqrt{x^{3}-1}}$\\
(e) $\int_{-\infty}^{\infty} \frac{2+\sin x}{x^{2}+1} d x$\\
(h) $\int_{t}^{\infty} \frac{\ln x d x}{x+e^{-x}}$\\
(c) $\int_{t}^{\infty} \frac{d x}{x \sqrt{3 x+2}}$\\
(f) $\int_{2}^{\infty} \frac{x d x}{(\ln x)^{3}}$\\
(i) $\int_{0}^{\infty} \frac{\sin ^{2} x}{x^{2}} d x$

Ans. (a) convergence (b) divergence (c) convergence (d) convergence (e) convergence ( $f$ ) divergence (g) convergence $(h)$ divergence $(i)$ convergence

12.38. Prove that $\int_{-\infty}^{\infty} \frac{d x}{x^{2}+2 a x+b^{2}}=\frac{\pi}{\sqrt{b^{2}-a^{2}}}$ if $b>|a|$.

12.39. Test for convergence: (a) $\int_{t}^{\infty} e^{-x} \ln x d x$, (b) $\int_{0}^{\infty} e^{-x} \ln \left(1+e^{x}\right) d x$, and (c) $\int_{0}^{\infty} e^{-x} \cosh x^{2} d x$. Ans. (a) convergence (b) convergence (c) divergence

12.40. Test for convergence, indicating absolute or conditional convergence where possible: $(a) \int_{0}^{\infty} \frac{\sin 2 x}{x^{3}+1} d x$;

(b) $\int_{-\infty}^{\infty} e^{-a x^{2}} \cos b x d x$, where $a, b$ are positive constants; (c) $\int_{0}^{\infty} \frac{\cos x}{\sqrt{x^{2}+1}} d x$; (d) $\int_{0}^{\infty} \frac{x \sin x}{\sqrt{x^{2}+a^{2}}} d x$; and

(e) $\int_{0}^{\infty} \frac{\cos x}{\cosh x} d x$.

Ans. (a) absolute convergence (b) absolute convergence (c) conditional convergence (d) divergence (e) absolute convergence

12.41. Prove the quotient tests (b) and (c) on Page 323.

\section*{Improper integrals of the second kind}
12.42. Test for convergence:\\
(a) $\int_{0}^{1} \frac{d x}{(x+1) \sqrt{1-x^{2}}}$\\
(d) $\int_{1}^{2} \frac{\ln x}{\sqrt[3]{8-x^{3}}} d x$\\
(g) $\int_{0}^{3} \frac{x^{2}}{(3-x)^{2}} d x$\\
(j) $\int_{0}^{3} \frac{d x}{x^{x}}$\\
(b) $\int_{0}^{1} \frac{\cos x}{x^{2}} d x$\\
(e) $\int_{0}^{1} \frac{d x}{\sqrt{\ln (1 / x)}}$\\
(h) $\int_{0}^{\pi / 2} \frac{e^{-x} \cos x}{x} d x$\\
(c) $\int_{-1}^{1} \frac{e^{\tan ^{-1} x}}{x} d x$\\
(f) $\int_{0}^{\pi / 2} \ln \sin x d x$\\
(i) $\int_{0}^{1} \sqrt{\frac{1-k^{2} x^{2}}{1-x^{2}}} d x,|k|<1$

Ans. (a) convergence (b) divergence (c) divergence (d) convergence (e) convergence (f) convergence (g) divergence (h) divergence (i) convergence (j) convergence

12.43. (a) Prove that $\int_{0}^{5} \frac{d x}{4-x}$ diverges in the usual sense but converges in the Cauchy principal value senses. (b)

Find the Cauchy principal value of the integral in (a) and give a geometric interpretation.

Ans. (b) In 4

12.44. Test for convergence, indicating absolute or conditional convergence where possible: (a) $\int_{0}^{1} \cos \left(\frac{1}{x}\right) d x$,

(b) $\int_{0}^{1} \frac{1}{x} \cos \left(\frac{1}{x}\right) d x$, and (c) $\int_{0}^{1} \frac{1}{x^{2}} \cos \left(\frac{1}{x}\right) d x$.

Ans. (a) absolute convergence (b) conditional convergence (c) divergence

12.45. Prove that $\int_{0}^{4 \pi}\left(3 x^{2} \sin \frac{1}{x}-x \cos \frac{1}{x}\right) d x=\frac{32 \sqrt{2}}{\pi^{3}}$.

\section*{Improper integrals of the third kind}
12.46. Test for convergence: (a) $\int_{0}^{\infty} e^{-x} \ln x d x$, (b) $\int_{0}^{\infty} \frac{e^{-x} d x}{\sqrt{x \ln (x+1)}}$, and (c) $\int_{0}^{\infty} \frac{e^{-x} d x}{\sqrt[3]{x}(3+2 \sin x)}$. Ans. (a) convergence (b) divergence (c) convergence

12.47. Test for convergence: (a) $\int_{0}^{\infty} \frac{d x}{\sqrt[3]{x^{4}+x^{2}}}$ and (b) $\int_{0}^{\infty} \frac{e^{x} d x}{\sqrt{\sinh (a x)}}, a>0$.

Ans. (a) convergence (b) convergence if a $>2$, divergence if $0<a \leqq 2$.

12.48. Prove that $\int_{0}^{\infty} \frac{\sinh (a x)}{\sinh (\pi x)} d x$ converges if $0 \leqq|a|<\pi$ and diverges if $|a| \leqq \pi$.

12.49. Test for convergence, indicating absolute or conditional convergence where possible: (a) $\int_{0}^{\infty} \frac{\sin x}{\sqrt{2}} d x$ and

(b) $\int_{0}^{\infty} \frac{\sin \sqrt{x}}{\sinh \sqrt{x}} d x$

Ans. (a) conditional convergence (b) absolute convergence

\section*{Uniform convergence of improper integrals}
12.50. (a) Prove that $\phi(\alpha)=\int_{0}^{\infty} \frac{\cos \alpha x}{1+x^{2}} d x$ is uniformly convergent for all $\alpha$. (b) Prove that $\phi(\alpha)$ is continuous for all $\alpha$. (c) Find $\lim _{\alpha \rightarrow 0} \phi(\alpha)$.

Ans. (c) $\pi / 2$

12.51. Let $\left.\phi(\alpha)=\int_{0}^{\infty} F(x), \alpha\right) d x$, where $F(x, \alpha)=\alpha^{2} x e^{-\alpha x^{2}}$. (a) Show that $\phi(\alpha)$ is not continuous at $\alpha=0$; i.e., $\lim _{\alpha \rightarrow 0} \int_{0}^{\infty} F(x, \alpha) d x \neq \int_{0}^{\infty} \lim _{\alpha \rightarrow 0} F(x, \alpha) d x$. (b) Explain the result in (a).

12.52. Work Problem 12.51 if $F(x, \alpha)=\alpha^{2} x e^{-\alpha x}$.

12.53. If $F(x)$ is bounded and continuous for $-\infty<x<\infty$ and

$$
V(x, y)=\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{y F(\lambda) d \lambda}{y^{2}+(\lambda-x)^{2}}
$$

Prove that $\lim _{y \rightarrow 0} V(x, y)=F(x)$

12.54. Prove (a) Theorem 7 and (b) Theorem 8 on Page 328 .

12.55. Prove the Weierstrass $M$ test for uniform convergence of integrals.

12.56. Prove that if $\int_{0}^{\infty} F(x) d x$ converges, then $\int_{0}^{\infty} e^{-\alpha x} F(x) d x$ converges uniformly for $\alpha \geqq 0$.

12.57. Prove that (a) (a) $\phi(a)=\int_{0}^{\infty} e^{-a x} \frac{\sin x}{x} d x$ converges uniformly for $a \geqq 0,(b) \quad \phi(a)=\frac{\pi}{2}-\tan ^{-1} a$, and (c) $\int_{0}^{\infty} \frac{\sin x}{x} d x=\frac{\pi}{2}$ (compare Problems 12.27 through 12.29).

12.58. State the definition of uniform convergence for improper integrals of the second kind.

12.59. State and prove a theorem corresponding to Theorem 8 , Page 328 , if $\alpha$ is a differentiable function of $\alpha$.

\section*{Evaluation of definite integrals}
Establish each of the following results. Justify all steps in each case.

12.60. $\quad \int_{0}^{\infty} \frac{e^{-a x}-e^{-b x}}{x} d x=\ln (b / a), \quad a, b>0$

12.61. $\int_{0}^{\infty} \frac{e^{-a x}-e^{-b x}}{x \csc r x} d x=\tan ^{-1}(b / r)-\tan ^{-1}(a / r), \quad a, b, r>0$

12.62. $\int_{0}^{\infty} \frac{\sin r x}{x\left(1+x^{2}\right)} d x=\frac{\pi}{2}\left(1-e^{-r}\right), \quad r \geq 0$

12.63. $\int_{0}^{\infty} \frac{1-\cos r x}{x^{2}} d x=\frac{\pi}{2}|r|$

12.64. $\int_{0}^{\infty} \frac{x \sin r x}{a^{2}+x^{2}} d x=\frac{\pi}{2} e^{-a r} \quad a, r \geqq 0$

12.65. (a) Prove that $\int_{0}^{\infty} e^{-\alpha x}\left(\frac{\cos a x-\cos b x}{x}\right) d x=\frac{1}{2} \ln \left(\frac{\alpha^{2}+b^{2}}{\alpha^{2}+a^{2}}\right), \quad \alpha \geq 0$. (b) Use (a) to prove that $\int_{0}^{\infty} \frac{\cos a x-\cos b x}{x} d x=\ln \left(\frac{b}{a}\right)$. [The results of (b) and Problem 12.60 are special cases of Frullani's integral, $\int_{0}^{\infty} \frac{F(a x)-F(b x)}{x} d x=F(0) \ln \left(\frac{b}{a}\right)$, where $F(t)$ is continuous for $t<0, t>0, F^{\prime}(0)$ exists and $\int_{1}^{\infty} \frac{F(t)}{t} d t$ converges.]

12.66. Given $\int_{0}^{\infty} e^{-\alpha x^{2}} d x=\frac{1}{2} \sqrt{\pi / \alpha}, \alpha>0$, prove that for $p=1,2,3, \ldots$,

$$
\int_{0}^{\infty} x^{2 p} e^{-\alpha x^{2}} d x=\frac{1}{2} \cdot \frac{3}{2} \cdot \frac{5}{2} \cdots \frac{(2 p-1)}{2} \frac{\sqrt{\pi}}{2 \alpha^{(2 p+1) / 2}}
$$

12.67. If $a>0, b>0$, prove that $\int_{0}^{\infty}\left(e^{-u / x^{2}}-e^{-b / x^{2}}\right) d x=\sqrt{\pi b}-\sqrt{\pi a}$.

12.68. Prove that $\int_{0}^{\infty} \frac{\tan ^{-1}(x / a)-\tan ^{-1}(x / b)}{x} d x=\frac{\pi}{2} \ln \left(\frac{b}{a}\right)$, where $a>0, b>0$.

12.69. Prove that $\int_{-\infty}^{\infty} \frac{d x}{\left(x^{2}+x+1\right)^{3}}=\frac{4 \pi}{3 \sqrt{3}}$. (Hint: Use Problem 12.38.)

\section*{Miscellaneous problems}
12.70. Prove that $\int_{0}^{\infty}\left\{\frac{\ln (1+x)}{x}\right\}^{2} d x$ converges.

12.71. Prove that $\int_{0}^{\infty} \frac{d x}{1+x^{3} \sin ^{2} x}$ converges. [Hint: Consider $\sum_{n=0}^{\infty} \int_{n \pi}^{(n+1) \pi} \frac{d x}{1+x^{3} \sin ^{2} x}$ and use the fact that $\left.\int_{n \pi}^{(\mathrm{n}+1) \pi} \frac{d x}{1+x^{3} \sin ^{2} x}<\int_{n \pi}^{(n+1) \pi} \frac{d x}{1+(n \pi)^{3} \sin ^{2} x}\right]$.

12.72. Prove that $\int_{0}^{\infty} \frac{x d x}{1+x^{3} \sin ^{2} x}$ diverges.

12.73. (a) Prove that $\int_{0}^{\infty} \frac{\ln \left(1+\alpha^{2} x^{2}\right)}{1+x^{2}} d x=\pi \ln (1+\alpha), \quad \alpha \geqq 0$. (b) Use (a) to show that $\int_{0}^{\pi / 2} \ln \sin \theta \mathrm{d} \theta=-\frac{\pi}{2} \ln 2$.

12.74. Prove that $\int_{0}^{\infty} \frac{\sin ^{4} x}{x^{4}} d x=\frac{\pi}{3}$.

12.75. Evaluate (a) $\mathscr{L}(1 / \sqrt{x})$, (b) $\mathscr{L}\{\cosh a x\}$, and (c) $\mathscr{L}\{(\sin x) / x\}$.

$$
\text { Ans. (a) } \sqrt{\pi / s}, s>0 \text { (b) } \frac{s}{s^{2}-a^{2}}, s>|a| \text { (c) } \tan ^{-1}\left(\frac{1}{s}\right), s>0
$$

12.76. (a) If $\mathscr{L}\left\{e^{a x} F(\mathrm{x})\right\}$ prove that $\mathscr{L}\left\{e^{a x} F(\mathrm{x})\right\}=f(s-a)$ and (b) evaluate $\mathscr{L}\left\{e^{a x} \sin b x\right\}$.

$$
\text { Ans. (b) } \frac{b}{(s-a)^{2}+b^{2}}, \quad s>a
$$

12.77. (a) If $\mathscr{L}\{F(x)\}=f(s)$, prove that $\mathscr{L}\left\{x^{n} F(\mathrm{x})\right\}=(-1)^{n} f^{(n)}(s)$, giving suitable restrictions on $F(x)$. (b) Evaluate $\mathscr{L}\{x \cos x]$.

$$
\text { Ans. (b) } \frac{s^{2}-1}{\left(s^{2}+1\right)^{2}}, \quad s>0
$$

12.78. Prove that $\mathscr{L}^{-1}\{f(s)+g(s)\}=\mathscr{L}^{-1}\{f(s)\}+\mathscr{L}^{-1}[g(s)]$, stating any restrictions.

12.79. Solve using Laplace transforms, the following differential equations subject to the given conditions:

(a) $Y^{\prime \prime}(x)+3 Y^{\prime}(x)+2 Y(x)=0 ; Y(0)=3, Y^{\prime}(0)=0$

(b) $Y^{\prime \prime}(x)-Y^{\prime}(x)=x ; Y(0)=2, Y^{\prime}(0)=-3$

(c) $Y^{\prime \prime}(x)+2 Y^{\prime}(x)+2 Y(x)=4 ; Y(0)=0 Y^{\prime}(0)=0$

Ans. (a) $Y(x)=6 e^{-x}-3 e^{-2 x}$ (b) $Y(x)=4-2 e^{x}-\frac{1}{2} x^{2}-x$ (c) $Y(x)=1-e^{-x}(\sin x+\cos x)$

12.80. Prove that $\mathscr{L}\{F(x)\}$ exists if $F(x)$ is piecewise continuous in every finite interval $[0, b]$ where $b>0$ and if $F(x)$ is of exponential order as $x \rightarrow \infty$; i.e., there exists a constant $\alpha$ such that $\left|e^{-\alpha x} F(x)\right|<P$ (a constant) for all $x>b$.

12.81. If $f(s)=\mathscr{L}\{F(x)\}$ and $g(s)=\mathscr{L}\{G(x)\}$, prove that $f(s) g(s)=\mathscr{L}\{H(x)\}$ where

$$
H(x)=\int_{0}^{x} F(u) G(x-u) d u
$$

is called the convolution of $F$ and $G$, written $F^{*} G$. Hint: Write

$$
\begin{aligned}
f(s) g(s) & =\lim _{M \rightarrow \infty}\left\{\int_{0}^{M} e^{-s u} F(u) d u\right\}\left\{\int_{0}^{M} e^{-s e} G(v) d v\right\} \\
& =\lim _{M \rightarrow \infty} \int_{0}^{M} \int_{0}^{M} e^{s(u+v)} F(u) G(v) d u d v \text { and then let } \mathrm{u}+v=t
\end{aligned}
$$

12.82. (a) Find $\mathscr{L}^{-1}\left\{\frac{1}{\left(s^{2}+1\right)^{2}}\right.$ (b) Solve $Y^{\prime \prime}(\mathrm{x})+Y(\mathrm{x})=R(\mathrm{x}), Y(0)=Y^{\prime}(0)=0$. (c) Solve the integral equation $Y(x)=x+\int_{0}^{x} Y(u) \sin (x-u) d u$. (Hint: Use Problem 12.81.)

Ans. (a) $\frac{1}{2}(\sin x-x \cos x)$ (b) $Y(x)=\int_{0}^{x} R(u) \sin (x-u) d u$ (c) $Y(x)=x+x^{3} / 6$

12.83. Let $f(x), g(x)$, and $g^{\prime}(x)$ be continuous in every finite interval $a \leqq x \leqq b$ and suppose that $g^{\prime}(x) \leqq 0$.

Suppose also that $h(x)=\int_{0}^{x} f(x) d x$ is bounded for all $x \varepsilon a$ and $\lim _{x \rightarrow 0} g(x)=0$.

(a) Prove that $\int_{0}^{\infty} f(x) g(x) d x=-\int_{a}^{\infty} g^{\prime}(x) h(x) d x$.

(b) Prove that the integral on the right, and, hence, the integral on the left, are convergent. The result is that under the given conditions on $f(x)$ and $g(x), \int_{a}^{\infty} f(x) g(x) d x$ converges and is sometimes called Abel's integral test. [Hint: For (a), consider $\lim _{b \rightarrow \infty} \int_{a}^{b} f(x) g(x) d x$ after replacing $f(x)$ by $h^{\prime}(x)$ and integrating by parts. For (b), first prove that if $|h(x)|<H$ (a constant), then $\left|\int_{a}^{b} g^{\prime}(x) h(x) d x\right|$ and then let $b \rightarrow \infty$.]

12.84. Use Problem 12.83 to prove that (a) $\int_{0}^{\infty} \frac{\sin x}{x} d x$ and (b) $\int_{0}^{\infty} \sin x^{p} d x, p>1$ converge.

12.85. (a) Given that $\int_{0}^{\infty} \sin x^{2} d x=\int_{0}^{\infty} \cos x^{2} d x=\frac{1}{2} \sqrt{\frac{\pi}{2}}$ [see Problems 15.27 and 15.68(a)], evaluate

$\int_{0}^{\infty} \int_{0}^{\infty} \sin \left(x^{2}+y^{2}\right) d x d y$ and (b) Explain why the method of Problem 12.31 cannot be used to evaluate the multiple integral in (a).

Ans. $\pi / 4$

This page intentionally left blank

\section*{CHAPTER 13}
\section*{Fourier Series}
Mathematicians of the eighteenth century, including Daniel Bernoulli and Leonhard Euler, expressed the problem of the vibratory motion of a stretched string through partial differential equations that had no solutions in terms of "elementary functions." Their resolution of this difficulty was to introduce infinite series of sine and cosine functions that satisfied the equations. In the early nineteenth century, Joseph Fourier, while studying the problem of heat flow, developed a cohesive theory of such series. Consequently, they were named after him. Fourier series and Fourier integrals are investigated in this chapter and Chapter 14. As you explore the ideas, notice the similarities and differences with the chapters on infinite series and improper integrals.

\section*{Periodic Functions}
A function $f(x)$ is said to have a period $T$ or to be periodic with period $T$ if for all $x, f(x+T)=f(x)$, where $T$ is a positive constant. The least value of $T>0$ is called the least period or simply the period of $f(x)$.

EXAMPLE 1. The function $\sin x$ has periods $2 \pi, 4 \pi, 6 \pi, \ldots, \operatorname{since} \sin (x+2 \pi), \sin (+4 \pi), \sin (x+$ $6 \pi), \ldots$ all equal $\sin x$. However, $2 \pi$ is the least period or the period of $\sin x$.

EXAMPLE 2. The period of $\sin n x$ or $\cos n x$, where $n$ is a positive integer, is $2 \pi / n$.

EXAMPLE 3. The period of $\tan x$ is $\pi$.

EXAMPLE 4. A constant has any positive number as period.

Other examples of periodic functions are shown in the graphs of Figure 13.1(a), (b), and (c).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-360(2)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-360(1)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-360}
\end{center}

(c)

Figure 13.1

\section*{Fourier Series}
Let $f(x)$ be defined in the interval $(-L, L)$ and outside of this interval by $f(x+2 L)=f(x)$; i.e., $f(x)$ is $2 L$ periodic. It is through this avenue that a new function on an infinite set of real numbers is created from the image on $(-L, L)$. The Fourier series or Fourier expansion corresponding to $f(x)$ is given by


\begin{equation*}
\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right) \tag{1}
\end{equation*}


where the Fourier coefficients $a_{n}$ and $b_{n}$ are

\[
\left\{\begin{array}{l}
a_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \cos \frac{n \pi x}{L} d x  \tag{2}\\
b_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x
\end{array} \quad n=0,1,2, \ldots\right.
\]

To correlate the coefficients with the expansion, see the following Examples 1 and 2.

\section*{Orthogonality Conditions for the Sine and Cosine Functions}
Notice that the Fourier coefficients are integrals. These are obtained by starting with the series (1) and employing the following properties called orthogonality conditions:

(a) $\int_{-L}^{L} \cos \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=0$ if $m \neq n$ and $L$ if $m=n$

(b) $\int_{-L}^{L} \sin \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x=0$ if $m \neq n$ and $L$ if $m=n$

(c) $\int_{-L}^{L} \sin \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=0$. Where $m$ and $n$ assume any positive integer values.

An explanation for calling these orthogonality conditions is given on Page 355. Their application in determining the Fourier coefficients is illustrated in the following pair of examples and then demonstrated in detail in Problem 13.4.

EXAMPLE 1. To determine the Fourier coefficient $a_{0}$, integrate both sides of the Fourier series (1) and employ the orthogonality conditions (2).

Now

$$
\int_{-L}^{L} f(x) d x=\int_{-L}^{L} \frac{a_{0}}{2} d x+\int_{-L}^{L} \sum_{n=1}^{\infty}\left\{a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right\} d x
$$

therefore

$$
\int_{-L}^{L} \frac{a_{0}}{2} d x=a_{0} L, \int_{-L}^{L} \sin \frac{n \pi x}{L} d x=0, \int_{-L}^{L} \cos \frac{n \pi x}{L} d x=0
$$

$$
a_{0}=\frac{1}{L} \int_{-L}^{L} f(x) d x
$$

EXAMPLE 2. To determine $a_{1}$, multiply both sides of series (1) by $\cos \frac{\pi x}{L}$ and then integrate. Using the orthogonality conditions (3) $)_{a}$ and (3), we obtain $a_{1}=\frac{1}{L} \int_{-L}^{L} f(x) \cos \frac{\pi x}{L} d x$. Now see Problem 13.4.

If $L=\pi$, the series (1) and the coefficients (2) or (3) are particularly simple. The function in this case has the period $2 \pi$.

\section*{Dirichlet Conditions}
Suppose that

\begin{enumerate}
  \item $f(x)$ is defined except possibly at a finite number of points in $(-L, L)$.

  \item $f(x)$ is periodic outside $(-L, L)$ with period $2 L$.

  \item $f(x)$ and $f^{\prime}(x)$ are piecewise continuous in $(-L, L)$.

\end{enumerate}

Then the series (1) with Fourier coefficients converges to

(a) $f(x)$ if $x$ is a point of continuity

(b) $\frac{f(x+0)+f(x-0)}{2}$ if $x$ is a point of discontinuity

Here $f(x+0)$ and $f(x-0)$ are the right- and left-hand limits of $f(x)$ at $x$ and represent $\lim _{\varepsilon \rightarrow 0+} f(x+\epsilon)$ and $\lim _{\varepsilon \rightarrow 0+}$ $f(x-\epsilon)$, respectively. For a proof, see Problems 13.18 through 13.23.

The conditions 1,2, and 3 imposed on $f(x)$ are sufficient but not necessary, and are generally satisfied in practice. There are at present no known necessary and sufficient conditions for convergence of Fourier series. It is of interest that continuity of $f(x)$ does not alone ensure convergence of a Fourier series.

\section*{Odd and Even Functions}
A function $f(x)$ is called odd if $f(-x)=-f(x)$. Thus, $x^{3}, x^{5}-3 x^{3}+2 x$, $\sin x$, and $\tan 3 x$ are odd functions.

A function $f(x)$ is called even if $f(-x)=f(x)$. Thus, $x^{4}, 2 x^{6}-4 x^{2}+5, \cos x$, and $e^{x}+e^{-x}$ are even functions.

The functions portrayed graphically in Figure 13.1(a) and (b) are odd and even, respectively, but that of Figure 13.1(c) is neither odd nor even.

In the Fourier series corresponding to an odd function, only sine terms can be present. In the Fourier series corresponding to an even function, only cosine terms (and possibly a constant, which we shall consider a cosine term) can be present.

\section*{Half Range Fourier Sine or Cosine Series}
A half range Fourier sine or cosine series is a series in which only sine terms or only cosine terms are present, respectively. When a half range series corresponding to a given function is desired, the function is generally defined in the interval $(0, L)$ [which is half of the interval $(-L, L)$, thus accounting for the name half range] and then the function is specified as odd or even, so that it is clearly defined in the other half of the interval, namely, $(-L, 0)$. In such case, we have

\[
\left\{\begin{array}{lll}
a_{n}=0, & b_{n}=\frac{2}{L} \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x & \text { for half range sine series }  \tag{4}\\
b_{n}=0, & a_{n}=\frac{2}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x & \text { for half range consine series }
\end{array}\right.
\]

\section*{Parseval's Identity}
If $a_{n}$ and $b_{n}$ are the Fourier coefficients corresponding to $f(x)$ and if $f(x)$ satisfies the Dirichlet conditions, then

(See Problem 13.13.)


\begin{equation*}
\frac{1}{L} \int_{-L}^{L}\{f(x)\}^{2} d x=\frac{a_{0}^{2}}{2}+\sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right) \tag{5}
\end{equation*}


\section*{Differentiation and Integration of Fourier Series}
Differentiation and integration of Fourier series can be justified by using the theorems on Page 285, which hold for series in general. It must be emphasized, however, that those theorems provide sufficient conditions and are not necessary. The following theorem for integration is respecially useful.

Theorem The Fourier series corresponding to $f(x)$ may be integrated term by term from $a$ to and the resulting series will converge uniformly to $\int_{a}^{x} f(x) d x$ provided that $F(x)$ is piecewise continueus in $-L \leqq x \leqq L$ and both $a$ and $x$ are in this interval.

\section*{Complex Notation for Fourier Series}
Using Euler's identities,


\begin{equation*}
e^{i \theta}=\cos \theta+i \sin \theta, e^{-i \theta}=\cos \theta-i \sin \theta \tag{6}
\end{equation*}


where $i=\sqrt{-1}$ (see Problem 11.48), the Fourier series for $f(x)$ can be written as


\begin{equation*}
f(x)=\sum_{n=-\infty}^{\infty} c_{n} e^{i n \pi x / L} \tag{7}
\end{equation*}


where


\begin{equation*}
c_{n}=\frac{1}{2 L} \int_{-L}^{L} f(x) e^{-i n \pi x / L} d x \tag{8}
\end{equation*}


In writing the equality (7), we are supposing that the Dirichlet conditions are satisfied and, further, that $f(x)$ is continuous at $x$. If $f(x)$ is discontinuous at $x$, the left side of (7) should be replaced by $\frac{(f(x+0)+f(x-0))}{2}$.

\section*{Boundary-Value Problems}
Boundary value problems seek to determine solutions of partial differential equations satisfying certain prescribed conditions called boundary conditions. Some of these problems can be solved by use of Fourier series (see Problem 13.24).

EXAMPLE. The classical problem of a vibrating string may be idealized in the following way. See Figure 13.2.

Suppose a string is tautly stretched between points $(0,0)$ and $(L, 0)$. Suppose the tension $\mathbf{F}$ is the same at every point of the string. The string is made to vibrate in the $x y$ plane by pulling it to the parabolic position $g(x)=m\left(L x-x^{2}\right)$ and releasing it ( $m$ is a numerically small positive constant). Its equation will be of the form $y=f(x, t)$. The problem of establishing this equation is idealized by (a) assuming that the constant tension $F$ is so large as compared to the weight $w L$ of the string that the gravitational force can be neglected, (b) the displacement at any point of the string is so small that the length of the string may be taken as $L$ for any of its positions, and (c) the vibrations are purely transverse.

The force acting on a segment $P Q$ is

$$
\frac{w}{g} \Delta x \frac{\partial^{2} y}{\partial t^{2}}, x<x_{1}<x+\Delta x, g \approx 32 \mathrm{ft} \text { per } \sec ^{2}
$$

If $\alpha$ and $\beta$ are the angles that $\mathbf{F}$ makes with the horizontal, then the vertical difference in tensions is $F(\sin \alpha-\sin \beta)$. This is the force producing the acceleration that accounts for the vibratory motion.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-364}
\end{center}

Figure 13.2

Now

$$
F\{\sin \alpha-\sin \beta\}=F\left\{\frac{\tan \alpha}{\sqrt{1+\tan ^{2} \alpha}}-\frac{\tan \beta}{\sqrt{1+\tan ^{2} \beta}}\right\} \approx F\{\tan \alpha-\tan \beta\}=F\left\{\frac{\partial y}{\partial x}(x+\Delta x, t)-\frac{\partial y}{\partial x}(x, t)\right\}
$$

where the squared terms in the denominator are neglected because the vibrations are small.

Next, equate the two forms of the force, i.e.,

$$
F\left\{\frac{\partial y}{\partial x}(x+\Delta x, t)-\frac{\partial y}{\partial x}(x, t)\right\}=\frac{w}{g} \Delta x \frac{\partial^{2} y}{\partial t^{2}}
$$

divide by $\Delta x$, and then let $\Delta x \rightarrow 0$. After letting $\alpha=\frac{\sqrt{F g}}{w}$, the resulting equation is

$$
\frac{\partial^{2} y}{\partial t^{2}}=\alpha^{2} \frac{\partial^{2} y}{\partial x^{2}}
$$

This homogeneous second partial derivative equation is the classical equation for the vibrating string. Associated boundary conditions are

$$
y(0, t)=0, y(L, t)=0, t>0
$$

The initial conditions are

$$
y(x, 0)=m\left(L x-x^{2}\right), \frac{\partial y}{\partial t}(x, 0)=0,0<x<L
$$

The method of solution is to separate variables, i.e., assume

$$
y(x, t)=G(x) H(t)
$$

Then, upon substituting,

$$
G(x) H^{\prime \prime}(t)=\alpha^{2} G^{\prime \prime}(x) H(t)
$$

Separating variables yields

$$
\frac{G^{\prime \prime}}{G}=k, \frac{H^{\prime \prime}}{H}=\alpha^{2} k, \text { where } k \text { is an arbitrary constant }
$$

Since the solution must be periodic, trial solutions are

Therefore,

$$
\begin{aligned}
& G(x)=c_{1} \sin \sqrt{-k} x+c_{2} \cos \sqrt{-k} x,<0 \\
& H(t)=c_{3} \sin \alpha \sqrt{-k} t+c_{4} \cos \alpha \sqrt{-k} t
\end{aligned}
$$

$$
y=G H=\left[c_{1} \sin \sqrt{-k} x+c_{2} \cos \sqrt{-k} x\right]\left[c_{3} \sin \alpha \sqrt{-k} t+c_{4} \cos \alpha \sqrt{-k} t\right]
$$

The initial condition $\mathrm{y}=0$ at $x=0$ for all $t$ leads to the evaluation $c_{2}=0$.

Thus,

$$
y=\left[c_{1} \sin \sqrt{-k} x\right]\left[c_{3} \sin \alpha \sqrt{-k} t+c_{4} \cos \alpha \sqrt{-k} t\right]
$$

Now impose the boundary condition $y=0$ at $x=L$; thus, $0=\left[c_{1} \sin \sqrt{-k} L\right]\left[c_{3} \sin \alpha \sqrt{-k} t+c_{4} \cos \alpha\right.$ $\sqrt{-k} t]$

$c_{1} \neq 0$, as that would imply $y=0$ and a trivial solution. The next-simplest solution results from the

choice $\sqrt{-k}=\frac{n \pi}{L}$, since $y=\left[c_{1} \sin \frac{n \pi}{L} x\right]\left[c_{3} \sin \alpha \frac{n \pi}{l} t+c_{4} \cos \alpha \frac{n \pi}{L} t\right]$ and the first factor is zero when $x=L$.

With this equation in place, the boundary condition $\frac{\partial y}{\partial t}=(x, 0)=0,0<x<L$ can be considered.

At $t=0$,

$$
\frac{\partial y}{\partial t}=\left[c_{1} \sin \frac{n \pi}{L} x\right]\left[c_{3} \alpha \frac{n \pi}{L} \cos \alpha \frac{n \pi}{L} t-c_{4} \alpha \frac{n \pi}{L} \sin \alpha \frac{n \pi}{L} t\right]
$$

$$
0=\left[c_{1} \sin \frac{n \pi}{L} x\right] c_{3} \alpha \frac{n \pi}{L}
$$

Since $c_{1} \neq 0$ and $\sin \frac{n \pi}{L} x$ is not identically zero, it follows that $c_{3}=0$ and that

The remaining initial condition is

$$
y=\left[c_{1} \sin \frac{n \pi}{L} x\right]\left[c_{4} \alpha \frac{n \pi}{L} \cos \alpha \frac{n \pi}{L} t\right]
$$

$$
y(x, 0)=m\left(L x-x^{2}\right), 0<x<L
$$

When it is imposed,

$$
m\left(L x-x^{2}\right)=c_{1} c_{4} \alpha \frac{n \pi}{L} \sin \frac{n \pi}{L} x
$$

However, this relation cannot be satisfied for all $x$ on the interval $(0, L)$. Thus, the preceding extensive analysis of the problem of the vibrating string has led us to an inadequate form:

$$
y=c_{1} c_{4} \alpha \frac{n \pi}{L} \sin \frac{n \pi}{L} x \cos \alpha \frac{n \pi}{L} t
$$

and an initial condition that is not satisfied. At this point the power of Fourier series is employed. In particular, a theorem of differential equations states that any finite sum of a particular solution also is a solution. Generalize this to infinite sum and consider

$$
y=\sum_{n=1}^{\infty} b_{n} \sin \frac{n \pi}{L} x \cos \alpha \frac{n \pi}{L} t
$$

with the initial condition expressed through a half range sine series, i.e.,

$$
\sum_{n=1}^{\infty} b_{n} \sin \frac{n \pi}{L} x=m\left(L x-x^{2}\right), \quad t=0
$$

According to the formula on Page 351 for the coefficient of a half range sine series,

$$
\frac{L}{2 m} b_{n}=\int_{0}^{L}\left(L x-x^{2}\right) \sin \frac{n \pi x}{L} d x
$$

That is,

$$
\frac{L}{2 m} b_{n}=\int_{0}^{L} L x \sin \frac{n \pi x}{L} d x-\int_{0}^{L} x^{2} \sin \frac{n \pi x}{L} d x
$$

Application of integration by parts to the second integral yields

$$
\frac{L}{2 m} b_{n}=L \int_{0}^{L} x \sin \frac{n \pi x}{L} d x+\frac{L^{3}}{n \pi} \cos n \pi+\int_{0}^{L} \frac{L}{n \pi} \cos \frac{n \pi x}{L} 2 x d x
$$

When integration by parts is applied to the two integrals of this expression and a little algebra is employed, the result is

$$
b_{n}=\frac{4 L^{2}}{(n \pi)^{3}}(1-\cos n \pi)
$$

Therefore,

$$
y=\sum_{n=1}^{\infty} b_{n} \sin \frac{n \pi}{L} x \cos \alpha \frac{n \pi}{L} t
$$

with the coefficients $b_{n}$ defined previously.

\section*{Orthogonal Functions}
Two vectors $\mathbf{A}$ and $\mathbf{B}$ are called orthogonal (perpendicular) if $\mathbf{A} \cdot \mathbf{B}=0$ or or $A_{1} B_{1}+A_{2} B_{2}+A_{3} B_{3}=0$, where $\mathbf{A}=A_{1} \mathbf{i}+A_{2} \mathbf{j}+A_{3} \mathbf{k}$ and $\mathbf{B}=B_{1} \mathbf{i}+B_{2} \mathbf{j}+B_{3} \mathbf{k}$. Although not geometrically or physically evident, these ideas can be generalized to include vectors with more than three components. In particular, we can think of a function — say, $A(x)$ - as being a vector with an infinity of components (i.e., an infinite dimensional vector), the value of each component being specified by substituting a particular value of $x$ in some interval $(a, b)$. It is natural in such case to define two functions, $A(x)$ and $B(x)$, as orthogonal in $(a, b)$ if


\begin{equation*}
\int_{a}^{b} A(x) B(x) d x=0 \tag{9}
\end{equation*}


A vector $\mathbf{A}$ is called a unit vector or normalized vector if its magnitude is unity, i.e., if $\mathbf{A} \cdot \mathbf{A}=A^{2}=1$. Extending the concept, we say that the function $A(x)$ is normal or normalized in $(a, b)$ if


\begin{equation*}
\int_{a}^{b}\{A(x)\}^{2} d x=1 \tag{10}
\end{equation*}


From this, it is clear that we can consider a set of functions $\left\{\phi_{k}(x)\right\}, k=1,2,3, \ldots$, having the properties


\begin{gather*}
\int_{a}^{b} \phi_{m}(x) \phi_{n}(x) d x=0 \quad m \neq n  \tag{11}\\
\int_{a}^{b}\left\{\phi_{m}(x)\right\}^{2} d x=1 \quad m=1,2,3, \ldots \tag{12}
\end{gather*}


In such case, each member of the set is orthogonal to every other member of the set and is also normalized. We call such a set of functions an orthonormal set.

Equations (11) and (12) can be summarized by writing


\begin{equation*}
\int_{a}^{b} \phi_{m}(x) \phi_{n}(x) d x=\delta_{m n} \tag{13}
\end{equation*}


where $\delta_{m n}$, called Kronecker's symbol, is defined as 0 if $m \neq n$ and 1 if $m=n$.

Just as any vector $\mathbf{r}$ in three dimensions can be expanded in a set of mutually orthogonal unit vectors $\mathbf{i}, \mathbf{j}$, $\mathbf{k}$ in the form $\mathbf{r}=c_{1} \mathbf{i}+c_{2} \mathbf{j}+c_{3} \mathbf{k}$, so we consider the possibility of expanding a function $f(x)$ in a set of orthonormal functions, i.e.,


\begin{equation*}
f(x)=\sum_{n=1}^{\infty} c_{n} \phi_{n}(x) \quad a \leqq x \leqq b \tag{14}
\end{equation*}


As we have seen, Fourier series are constructed from orthogonal functions. Generalizations of Fourier series are of great interest and utility from both theoretical and applied viewpoints.

\section*{SOLVED PROBLEMS}
\section*{Fourier Series}
13.1. Graph each of the following functions.

(a) $f(x)=\left\{\begin{array}{cc}3 & 0<x<5 \\ -3 & -5<x<0\end{array} \quad\right.$ Period $=10$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-367(1)}
\end{center}

Figure 13.3

Since the period is 10, that portion of the graph in $-5<x<5$ (indicated by heavy lines in Figure 13.3) is extended periodically outside this range (indicated by dashed lines). Note that $f(x)$ is not defined at $x=0,5$, $-5,10,-10,15,-15$, and so on. These values are the discontinuities of $f(x)$.

(b) $f(x)=\left\{\begin{array}{cc}\sin x & 0 \leqq x \leqq \pi \\ 0 & \pi<x<2 \pi\end{array}\right.$ Period $=2 \pi$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-367}
\end{center}

Figure 13.4

Refer to Figure 13.4. Note that $f(x)$ is defined for all $x$ and is continuous everywhere.

(c) $f(x)=\left\{\begin{array}{ll}0 & 0 \leqq x<2 \\ 1 & 2 \leqq x<4 \\ 0 & 4 \leqq x<6\end{array} \quad\right.$ Period $=6$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-367(2)}
\end{center}

Fig.13.5

Refer to Figure 13.5. Note that $f(x)$ is defined for all $x$ and is discontinuous at $x= \pm 2, \pm 4, \pm 8, \pm 10$, $\pm 14, \ldots$.

13.2. prove $\int_{-L}^{L} \sin \frac{\mathrm{k} \pi \mathrm{x}}{\mathrm{L}} d x=\int_{-L}^{L} \cos \frac{\mathrm{k} \pi \mathrm{x}}{\mathrm{L}} d x=0$ if $k=1,2,3, \ldots$

$$
\begin{aligned}
& \int_{-L}^{L} \sin \frac{\mathrm{k} \pi \mathrm{x}}{\mathrm{L}} d x=-\left.\frac{L}{k \pi} \cos \frac{\mathrm{k \pi x} \mathrm{x}}{\mathrm{L}}\right|_{-L} ^{L}=\frac{L}{k \pi} \cos k \pi+\frac{L}{k \pi} \cos (-k \pi)=0 \\
& \int_{-L}^{L} \cos \frac{\mathrm{k} \pi \mathrm{x}}{\mathrm{L}} d x=\left.\frac{L}{k \pi} \sin \frac{k \pi x}{L}\right|_{-L} ^{L}=\frac{L}{k \pi} \sin k \pi-\frac{L}{k \pi} \sin (-k \pi)=0
\end{aligned}
$$

13.3. Prove (a) $\int_{-L}^{L} \cos \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=\int_{-L}^{L} \sin \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x= \begin{cases}0 & m \neq n \\ L & m=n\end{cases}$

(b) $\int_{-L}^{L} \sin \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=0$

where $m$ and $n$ can assume any of the values $1,2,3, \ldots$.

(a) From trigonometry: $\cos \mathrm{A} \cos \mathrm{B}=\frac{1}{2}\{\cos (\mathrm{A}-\mathrm{B})+\cos (\mathrm{A}+\mathrm{B})\}, \sin \mathrm{A} \sin \mathrm{B}=\frac{1}{2}\{\cos (\mathrm{A}-\mathrm{B})-$ $\cos (\mathrm{A}+\mathrm{B})\}$.

Then, if $m \neq n$, by Problem 13.2,

$$
\int_{-L}^{L} \cos \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L}\left\{\cos \frac{(m-x) \pi x}{L}+\cos \frac{(m+n) \pi x}{L}\right\} d x=0
$$

Similarly, if $m \neq n$,

$$
\int_{-L}^{L} \sin \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L}\left\{\cos \frac{(m-n) \pi x}{L}-\cos \frac{(m+n) \pi x}{L}\right\} d x=0
$$

If $m=n$, we have

$$
\begin{aligned}
& \int_{-L}^{L} \cos \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L}\left(1+\cos \frac{2 n \pi x}{L}\right) d x=L \\
& \int_{-L}^{L} \sin \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L}\left(1-\cos \frac{2 n \pi x}{L}\right) d x=L
\end{aligned}
$$

Note that if $m=n$ these integrals are equal to $2 L$ and 0 , respectively.

(b) We have $\sin A \cos B=1 / 2\{\sin (A-B)+\sin (A+B)\}$. Then by Problem 13.2, if $m \neq n$,

$$
\int_{-L}^{L} \sin \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L}\left\{\sin \frac{(m-n) \pi x}{L}+\sin \frac{(m+n) \pi x}{L}\right\} d x=0
$$

If $m=n$,

$$
\int_{-L}^{L} \sin \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x=\frac{1}{2} \int_{-L}^{L} \sin \frac{2 n \pi x}{L} d x=0
$$

The results of (a) and (b) remain valid even when the limits of integration $-L, L$ are replaced by $c, c+2 L$, respectively.

13.4. If the series $A+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right)$ converges uniformly to $f(\mathrm{x})$ in $(-L, L)$, show that for $n=$

$1,2,3, \ldots$, (a) $\quad a_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \cos \frac{n \pi x}{L} d x$ and (b) $b_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x$, (c) $A=\frac{a_{0}}{2}$.

(a) Multiplying


\begin{equation*}
f(x)=A+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right) \tag{1}
\end{equation*}


by $\cos \frac{m \pi x}{L}$ and integrating from $-L$ to $L$, using Problem 13.3, we have

$$
\begin{aligned}
\int_{-L}^{L} f(x) \cos \frac{m \pi x}{L} d x & =A \int_{-L}^{L} \cos \frac{m \pi x}{L} d x \\
& +\sum_{n=1}^{\infty}\left\{a_{n} \int_{-L}^{L} \cos \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x+b_{n} \int_{-L}^{L} \cos \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x\right\} \\
& =a_{m} L \quad \text { if } m \neq 0
\end{aligned}
$$

Thus,

$$
a_{m}=\frac{1}{L} \int_{-L}^{L} f(x) \cos \frac{m \pi x}{L} d x \quad \text { if } m=1,2,3, \ldots
$$

(b) Multiplying Equation (1) by $\sin \frac{m \pi x}{L}$ and integrating from $-\mathrm{L}$ to $\mathrm{L}$, using Problem 13.3, we have

Thus,

$$
\begin{aligned}
\int_{-L}^{L} f(x) \sin \frac{m \pi x}{L} d x= & A \int_{-L}^{L} \sin \frac{m \pi x}{L} d x \\
& +\sum_{n=1}^{\infty}\left\{a_{n} \int_{-L}^{L} \sin \frac{m \pi x}{L} \cos \frac{n \pi x}{L} d x+b_{n} \int_{-L}^{L} \cos \frac{m \pi x}{L} \sin \frac{n \pi x}{L} d x\right\}
\end{aligned}
$$

$$
\begin{aligned}
& =b_{m} L \\
b_{m} & =\frac{1}{L} \int_{-L}^{L} f(x) \sin \frac{m \pi x}{L} d x \text { if } m=1,2,3, \ldots
\end{aligned}
$$

(c) Integrating Equation (1) from $-L$ to $L$, using Problem 13.2, gives

$$
\int_{-L}^{L} f(x) d x=2 A L \quad \text { or } \quad A=\frac{1}{2 L} \int_{-L}^{L} f(x) d x
$$

Putting $m=0$ in the result of (a), we find $a_{0}=\frac{1}{L} \int_{-L}^{L} f(x) d x$ and so $A=\frac{a_{0}}{2}$.

The above results also hold when the integration limits $-L, L$ are replaced by $c, c+2 L$.

Note that in (a), (b), and (c), interchange of summation and integration is valid because the series is assumed to converge uniformly to $f(x)$ in $(-L, L)$. Even when this assumption is not warranted, the coefficients $a_{m}$ and $b_{m}$ as obtained are called Fourier coefficients corresponding to $f(x)$, and the corresponding series with these values of $a_{m}$ and $b_{m}$ is called the Fourier series corresponding to $f(x)$. An important problem in this case is to investigate conditions under which this series actually converges to $f(x)$. Sufficient conditions for this convergence are the Dirichlet conditions established in Problems 13.18 through 13.23.

13.5. (a) Find the Fourier coefficients corresponding to the function

$$
f(x)=\left\{\begin{array}{cc}
0 & -5<x<0 \\
3 & 0<x<5
\end{array} \quad \text { Period }=10\right.
$$

(b) Write the corresponding Fourier series.

(c) How should $f(x)$ be defined at $x=-5, x=0$, and $x=5$ in order that the Fourier series will converge to $f(x)$ for $-5 \leqq x \leqq 5$ ?

The graph of $f(x)$ is shown in Figure 13.6.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-370}
\end{center}

Figure 13.6

(a) Period $=2 L=10$ and $L=5$. Choose the interval $c$ to $c+2 L$ as -5 to 5 , so that $c=-5$. Then

$$
\begin{aligned}
a_{n} & =\frac{1}{L} \int_{c}^{c+2 L} f(x) \cos \frac{n \pi x}{L} d x=\frac{1}{5} \int_{-5}^{5} f(x) \cos \frac{n \pi x}{5} d x \\
& =\frac{1}{5}\left\{\int_{-5}^{0}(0) \cos \frac{n \pi x}{5} d x+\int_{0}^{5}(3) \cos \frac{n \pi x}{5} d x\right\}=\frac{3}{5} \int_{0}^{5} \cos \frac{n \pi x}{5} d x \\
& =\left.\frac{3}{5}\left(\frac{5}{n \pi} \sin \frac{n \pi x}{5}\right)\right|_{0} ^{5}=0 \quad \text { if } n \neq 0 \\
\text { If } n & =0, a_{n}=a_{0}=\frac{3}{5} \int_{0}^{5} \cos \frac{0 \pi x}{5} d x=\frac{3}{5} \int_{0}^{5} d x=3 . \\
b_{n} & =\frac{1}{L} \int_{c}^{c+2 L} f(x) \sin \frac{n \pi x}{L} d x=\frac{1}{5} \int_{-5}^{5} f(x) \sin \frac{n \pi x}{5} d x \\
& =\frac{1}{5}\left\{\int_{-5}^{0}(0) \sin \frac{n \pi x}{5} d x+\int_{0}^{5}(3) \sin \frac{n \pi x}{5} d x\right\}=\frac{3}{5} \int_{0}^{5} \sin \frac{n \pi x}{5} d x \\
& =\frac{3}{5}\left(-\frac{5}{n \pi} \cos \frac{n \pi x}{5}\right)_{0}^{5}=\frac{3(1-\cos n \pi)}{n \pi}
\end{aligned}
$$

(b) The corresponding Fourier series is

$$
\begin{aligned}
\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right) & =\frac{3}{2}+\sum_{n=1}^{\infty} \frac{3(1-\cos n \pi)}{n \pi} \sin \frac{n \pi x}{5} \\
& =\frac{3}{2}+\frac{6}{\pi}\left(\sin \frac{\pi x}{5}+\frac{1}{3} \sin \frac{3 \pi x}{5}+\frac{1}{5} \sin \frac{5 \pi x}{5}+\cdots\right)
\end{aligned}
$$

(c) Since $f(x)$ satisfies the Dirichlet conditions, we can say that the series converges to $f(x)$ at all points of continuity and to $\frac{f(x+0)+f(x-0)}{2}$ at points of discontinuity. At $x=-5,0$, and 5 , which are points of discontinuity, the series converges to $(3+0) / 2=3 / 2$, as seen from the graph. If we redefine $f(x)$ as follows,

$$
f(x)=\left\{\begin{array}{lc}
3 / 2 & x=-5 \\
0 & -5<x<0 \\
3 / 2 & x=0 \\
3 & 0<x<5 \\
3 / 2 & x=5
\end{array} \quad \text { Period }=10\right.
$$

then the series will converge to $f(x)$ for $-5 \leqq x \leqq 5$.

13.6. Expand $f(x)=x^{2}, 0<x<2 \pi$ in a Fourier series if (a) the period is $2 \pi$ and (b) the period is not specified.

(a) The graph of $f(x)$ with period $2 \pi$ is shown in Figure 13.7.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-371}
\end{center}

Fig. 13-7

Period $=2 L=2 \pi$ and $L=\pi$. Choosing $c=0$, we have

$$
\begin{aligned}
a_{n} & =\frac{1}{L} \int_{c}^{c+2 L} f(x) \cos \frac{n \pi x}{L} d x=\frac{1}{\pi} \int_{0}^{2 \pi} x^{2} \cos n x d x \\
& =\left.\frac{1}{\pi}\left\{\left(x^{2}\right)\left(\frac{\sin n x}{n}\right)-(2 x)\left(\frac{-\cos n x}{n^{2}}\right)+2\left(\frac{-\sin n x}{n^{3}}\right)\right\}\right|_{0} ^{2 \pi}=\frac{4}{n^{2}}, \quad n \neq 0
\end{aligned}
$$

If $n=0, a_{0}=\frac{1}{\pi} \int_{0}^{2 \pi} x^{2} d x=\frac{8 \pi^{2}}{3}$.

$$
\begin{aligned}
b_{n} & =\frac{1}{L} \int_{c}^{c+2 L} f(x) \sin \frac{n \pi x}{L} d x=\frac{1}{\pi} \int_{0}^{2 \pi} x^{2} \sin n x d x \\
& ==\left.\frac{1}{\pi}\left\{\left(x^{2}\right)\left(\frac{\sin n x}{n}\right)-(2 x)\left(\frac{-\cos n x}{n^{2}}\right)+2\left(\frac{-\sin n x}{n^{3}}\right)\right\}\right|_{0} ^{2 \pi}
\end{aligned}
$$

Then $f(x)=x^{2}=\frac{4 \pi^{2}}{3}+\sum_{n=1}^{\infty}\left(\frac{4}{n^{2}} \cos n x-\frac{4 \pi}{n} \sin n x\right)$.

This is valid for $0<x<2 \pi$. At $x=0$ and $x=2 \pi$ the series converges to $2 \pi^{2}$.

(b) If the period is not specified, the Fourier series cannot be determined uniquely in general.

13.7. Using the results of Problem 13.6, prove that $\frac{1}{1^{2}}+\frac{1}{2^{2}}+\frac{1}{3^{2}}+\cdots=\frac{\pi^{2}}{6}$.

At $x=0$, the Fourier series of Problem 13.6 reduces to $\frac{4 \pi^{2}}{3}+\sum_{n=1}^{\infty} \frac{4}{n^{2}}$.

By the Dirichlet conditions, the series converges at $x=0$ to $\frac{1}{2}\left(0+4 \pi^{2}\right)=2 \pi^{2}$.

Then $\frac{4 \pi^{2}}{3}+\sum_{n=1}^{\infty} \frac{4}{n^{2}}=2 \pi^{2}$, and so $\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}$.

\section*{Odd and even functions, half range Fourier series}
13.8. Classify each of the following functions according to whether they are even, odd, or neither even nor odd.

(a) $f(x)=\left\{\begin{array}{cc}2 & 0<x<3 \\ -2 & -3<x<0\end{array} \quad\right.$ Period $=6$

From Figure 13.8, it is seen that $f(-x)=-f(x)$, so that the function is odd.

(b) $f(x)=\left\{\begin{array}{cc}\cos x & 0<x<\pi \\ 0 & \pi<x<2 \pi\end{array}\right.$ Period $=2 \pi$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-372}
\end{center}

Figure 13.8

From Figure 13.9, it is seen that the function is neither even nor odd.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-372(1)}
\end{center}

Figure 13.9

(c) $f(x)=x(10-x), 0<x<10 \quad$ Period $=10$

From Figure 13.10 below the function is seen to be even.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-372(2)}
\end{center}

Figure 13.10

13.9. Show that an even function can have no sine terms in its Fourier expansion.

Method 1: No sine terms appear if $b_{n}=0, n=1,2,3, \ldots$ To show this, let us this, let us write


\begin{equation*}
b_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x=\frac{1}{L} \int_{-L}^{0} f(x) \sin \frac{n \pi x}{L} d x+\frac{1}{L} \int_{0}^{L} f(x) \sin \frac{n \pi x}{L} d x \tag{1}
\end{equation*}


If we make the transformation $x=-u$ in the first integral on the right of Equation (1), we obtain


\begin{align*}
\frac{1}{L} \int_{-L}^{0} f(x) \sin \frac{n \pi x}{L} d x & =\frac{1}{L} \int_{0}^{L} f(-u) \sin \left(-\frac{n \pi u}{L}\right) d u=-\frac{1}{L} \int_{0}^{L} f(-u) \sin \frac{n \pi u}{L} d u  \tag{2}\\
& =-\frac{1}{L} \int_{0}^{L} f(u) \sin \frac{n \pi u}{L} d u=-\frac{1}{L} \int_{0}^{L} f(x) \sin \frac{n \pi u}{L} d x
\end{align*}


where we have used the fact that for an even function $f(-u)=f(u)$ and in the last last step that the dummy variable of integration $u$ can be replaced by any other symbol, in particular, $x$. Thus, from Equation (1), using Equation (2), we have

$$
b_{n}=-\frac{1}{L} \int_{0}^{L} f(x) \sin \frac{n \pi x}{L} d x+\frac{1}{L} \int_{0}^{L} f(x) \sin \frac{n \pi u}{L} d x=0
$$

Method 2: Assume

$$
f(x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right)
$$

Then

$$
f(-x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}-b_{N} \sin \frac{n \pi x}{L}\right)
$$

If $f(x)$ is even, $f(-x)=f(x)$. Hence,

$$
\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}-b_{n} \sin \frac{n \pi x}{L}\right)
$$

and so

$$
\sum_{n=1}^{\infty} b_{n} \sin \frac{n \pi x}{L}=0, \text { i.e., } f(x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty} a_{n} \cos \frac{n \pi x}{L}
$$

and no sine terms appear.

In a similar manner, we can show that an odd function has no cosine terms (or constant term) in its Fourier expansion.

13.10. If $f(\mathrm{x})$ is even, show that (a) $a_{n}=\frac{2}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x$, and (b) $b_{n}=0$.

(a) $\quad a_{n}=\frac{1}{L} \int_{-L}^{L} f(x) \cos \frac{n \pi x}{L} d x=\frac{1}{L} \int_{-L}^{0} f(x) \cos \frac{n \pi x}{L} d x+\frac{1}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x$

Letting $x=-u$,

$$
\frac{1}{L} \int_{-L}^{0} f(x) \cos \frac{n \pi x}{L} d x=\frac{1}{L} \int_{0}^{L} f(-u) \cos \left(\frac{-n \pi x}{L}\right) d u=\frac{1}{L} \int_{0}^{L} f(u) \cos \frac{n \pi x}{L} d u
$$

since, by definition of an even function, $f(-u)=f(u)$. Then

$$
a_{n}=\frac{1}{L} \int_{0}^{L} f(u) \cos \frac{n \pi u}{L} d u+\frac{1}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x=\frac{2}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x
$$

(b) This follows by Method 1 of Problem 13.9.

13.11. Expand $f(\mathrm{x})=\sin x, 0<x<\pi$, in a Fourier cosine series.

A Fourier series consisting of cosine terms alone is obtained only for an even function. Hence, we extend the definition of $f(x)$ so that it becomes even (dashed part of Figure 13.11). With this extension, $f(x)$ is then defined in an interval of length $2 \pi$. Taking the period as $2 \pi$, we have $2 L=2 \pi$ so that $L=\pi$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-373}
\end{center}

Figure 13.11

By Problem 13.10, $b_{n}=0$ and

$$
a_{n}=\frac{2}{L} \int_{0}^{L} f(x) \cos \frac{n \pi u}{L} d x=\frac{2}{\pi} \int_{0}^{\pi} \sin x \cos n x d x
$$

$$
\begin{aligned}
& =\frac{1}{\pi} \int_{0}^{\pi}\{\sin (x+n x)+\sin (x-n x)\}=\left.\frac{1}{\pi}\left\{-\frac{\cos (n+1) x}{n+1}+\frac{\cos (n-1) x}{n-1}\right\}\right|_{0} ^{\pi} \\
& =\frac{1}{\pi}\left\{\frac{1-\cos (n+1) \pi}{n+1}+\frac{\cos (n-1) \pi-1}{n-1}\right\}=\frac{1}{\pi}\left\{\frac{1+\cos n \pi}{n+1}+\frac{1+\cos n \pi}{n-1}\right\} \\
& =\frac{-2(1+\cos n \pi)}{\pi\left(n^{2}-1\right)} \text { if } n \neq 1 .
\end{aligned}
$$

For $n=1$,

$$
n=1, \quad a_{1}=\frac{2}{\pi} \int_{0}^{\pi} \sin x \cos x d x=\left.\frac{2}{\pi} \frac{\sin ^{2} x}{2}\right|_{0} ^{\pi}=0
$$

For $n=0$,

$$
n=0, \quad a_{0}=\frac{2}{\pi} \int_{0}^{\pi} \sin x d x=\left.\frac{2}{\pi}(-\cos x)\right|_{0} ^{\pi}=\frac{4}{\pi}
$$

Then

$$
\begin{aligned}
f(x) & =\frac{2}{\pi}-\frac{2}{\pi} \sum_{n=2}^{\infty} \frac{(1+\cos n \pi)}{n^{2}-1} \cos n x \\
& =\frac{2}{\pi}-\frac{4}{\pi}\left(\frac{\cos 2 x}{2^{2}-1}+\frac{\cos 4 x}{4^{2}-1}+\frac{\cos 6 x}{6^{2}-1}+\cdots\right)
\end{aligned}
$$

13.12. Expand $f(\mathrm{x})=x, 0<x<2$, in a half range (a) sine series and (b) cosine series.

(a) Extend the definition of the given function to that of the odd function of period 4 shown in Figure 13.12. This is sometimes called the odd extension of $f(x)$. Then $2 L=4, L=2$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-374}
\end{center}

Figure 13.12

Thus, $a_{n}=0$ and

$$
\begin{aligned}
b_{n} & =\frac{2}{L} \int_{0}^{L} f(x) \sin \frac{n \pi x}{L} d x=\frac{2}{2} \int_{0}^{2} x \sin \frac{n \pi x}{2} d x \\
& =\left.\left\{(x)\left(\frac{-2}{n \pi} \cos \frac{n \pi x}{2}\right)-(1)\left(\frac{-4}{n^{2} \pi^{2}} \sin \frac{n \pi x}{2}\right)\right\}\right|_{0} ^{2}=\frac{-4}{n \pi} \cos n \pi \cdot
\end{aligned}
$$

Then

$$
\begin{aligned}
f(x) & =\sum_{n=1}^{\infty} \frac{-4}{n \pi} \cos n \pi \sin \frac{n \pi x}{2} \\
& =\frac{4}{\pi}\left(\sin \frac{\pi x}{2}-\frac{1}{2} \sin \frac{2 \pi x}{2}+\frac{1}{3} \sin \frac{3 \pi x}{2}-\cdots\right)
\end{aligned}
$$

(b) Extend the definition of $f(x)$ to that of the even function of period 4 shown in Figure 13.13. This is the even extension of $f(x)$. Then $2 L=4, L=2$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-375}
\end{center}

Figure 13.13

Thus, $b_{n}=0$,

$$
\begin{aligned}
a_{n} & =\frac{2}{L} \int_{0}^{L} f(x) \cos \frac{n \pi x}{L} d x=\frac{2}{2} \int_{0}^{2} x \cos \frac{n \pi x}{2} d x \\
& =\left.\left\{(x)\left(\frac{2}{\pi} \sin \frac{n \pi x}{2}\right)-(1)\left(\frac{-4}{n^{2} \pi^{2}} \cos \frac{n \pi x}{2}\right)\right)\right|_{0} ^{2} \\
& =\frac{4}{n^{2} \pi^{2}}(\cos n \pi-1) \quad \text { If } \mathrm{n} \neq 0
\end{aligned}
$$

$$
\text { If } n=0, a_{0}=\int_{0}^{2} x d x=2
$$

Then

$$
\begin{aligned}
f(x) & =1+\sum_{n=1}^{\infty} \frac{4}{n^{2} \pi^{2}}(\cos n \pi-1) \cos \frac{n \pi x}{2} \\
& =1-\frac{8}{\pi^{2}}\left(\cos \frac{n x}{2}+\frac{1}{3^{2}} \cos \frac{3 \pi x}{2}=\frac{1}{5^{2}} \cos \frac{5 \pi x}{2}+\cdots\right)
\end{aligned}
$$

It should be noted that the given function $f(x)=x, 0<x<2$ is represented equally well by the two different series in (a) and (b).

\section*{Parseval's identity}
13.13. Assuming that the Fourier series corresponding to $f(x)$ converges uniformly to $f(x)$ in $(-L, L)$, prove Parseval's identity

$$
\frac{1}{L} \int_{-L}^{L}\{f(x)\}^{2} d x=\frac{a_{0}^{2}}{2}+\sum\left(a_{n}^{2}+b_{n}^{2}\right)
$$

where the integral is assumed to exist.

If $f(x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right)$, then multiplying by $f(x)$ and integrating term by term from $-L$ to $L$ (which is justified since the series is uniformly convergent), we obtain


\begin{align*}
\int_{-L}^{L}\{f(x)\}^{2} d x & =\frac{a_{0}}{2} \int_{-L}^{L} f(x) d x+\sum_{n=1}^{\infty}\left\{a_{n} \int_{-L}^{L} f(x) \cos \frac{n \pi x}{L} d x+b_{n} \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x\right\} \\
& =\frac{a_{0}^{2}}{2} L+L \sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right) \tag{1}
\end{align*}


where we have used the results


\begin{equation*}
\int_{-L}^{L} f(x) \cos \frac{n \pi x}{L} d x=L a_{n}, \quad \int_{-L}^{L} f(x) \sin \frac{n \pi x}{L} d x=L b_{n}, \quad \int_{-L}^{L} f(x) d x=L a_{0} \tag{2}
\end{equation*}


obtained from the Fourier coefficients.

The required result follows on dividing both sides of Equation (1) by $L$. Parseval's identity is valid under less restrictive conditions than that imposed here.

13.14. (a) Write Parseval's identity corresponding to the Fourier series of Problem 13.12(b). (b) Determine from (a) the sum $\mathrm{S}$ of the series $\frac{1}{1^{4}}+\frac{1}{2^{4}}+\frac{1}{3^{4}}+\cdots+\frac{1}{n^{4}}+\cdots$

(a) Here $L=2, a_{0}=2, a_{n}=\frac{4}{n^{2} \pi^{2}}(\cos n \pi-1), n \neq 0, b_{n}=0$.

Then Parseval's identity becomes

$$
\frac{1}{2} \int_{-2}^{2}\{f(x)\}^{2} d x=\frac{1}{2} \int_{-2}^{2} x^{2} d x=\frac{(2)^{2}}{2}+\sum_{n=1}^{\infty} \frac{16}{n^{4} \pi^{4}}(\cos n \pi-1)^{2}
$$

or

$$
\frac{8}{3}=2+\frac{64}{\pi^{4}}\left(\frac{1}{1^{4}}+\frac{1}{3^{4}}+\frac{1}{5^{4}}+\cdots\right) \text {. ie., } \frac{1}{1^{4}}+\frac{1}{3^{4}}+\frac{1}{5^{4}}+\cdots=\frac{\pi^{4}}{96}
$$

(b) $S=, \frac{1}{1^{4}}+\frac{1}{2^{4}}+\frac{1}{3^{4}}+\cdots=\left(\frac{1}{1^{4}}+\frac{1}{3^{4}}+\frac{1}{5^{4}}+\cdots\right)+\left(\frac{1}{2^{4}}+\frac{1}{4^{4}}+\frac{1}{6^{4}}+\cdots\right)$

$$
\begin{aligned}
& =\left(\frac{1}{1^{4}}+\frac{1}{3^{4}}+\frac{1}{5^{4}}+\cdots\right)+\frac{1}{2^{4}}\left(\frac{1}{1^{4}}+\frac{1}{2^{4}}+\frac{1}{3^{4}}+\cdots\right) \\
& =\frac{\pi^{4}}{96}+\frac{S}{16}, \quad \text { from which } S=\frac{\pi^{4}}{90}
\end{aligned}
$$

13.15. Prove that for all positive integers $M$,

$$
\frac{a_{0}^{2}}{2}+\sum_{n=1}^{M}\left(a_{n}^{2}+b_{n}^{2}\right) \leqq \frac{1}{L} \int_{-L}^{L}\{f(x)\}^{2} d x
$$

where $a_{n}$ and $b_{n}$ are the Fourier coefficients corresponding to $f(x)$, and $f(x)$ is assumed piecewise continuous in $(-L, L)$.

Let


\begin{equation*}
S_{M}(x)=\frac{a_{0}}{2}+\sum_{n=1}^{M}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right) \tag{1}
\end{equation*}


For $M=1,2,3, \ldots$, this is the sequence of partial sums of the Fourier series corresponding to $f(x)$.

We have


\begin{equation*}
\int_{-L}^{L}\left\{f(x)-S_{M}(x)\right\}^{2} d x \geq 0 \tag{2}
\end{equation*}


since the integrand is nonnegative. Expanding the integrand, we obtain


\begin{equation*}
2 \int_{-L}^{L} f(x) S_{M}(x) d x-\int_{-L}^{L} S_{M}^{2}(x) d x \leqq \int_{-L}^{L}\{f(x)\}^{2} d x \tag{3}
\end{equation*}


Multiplying both sides of Equation (1) by $2 f(x)$ and integrating from $-L$ to $L$, using Equations (2) of Problem 13.13, gives


\begin{equation*}
2 \int_{-L}^{L} f(x) S_{M}(x) d x=2 L\left\{\frac{a_{0}^{2}}{2}+\sum_{n=1}^{M}\left(a_{n}^{2}+b_{n}^{2}\right)\right\} \tag{4}
\end{equation*}


Also, squaring Equation (1) and integrating from $-L$ to $L$, using Problem 13.3, we find


\begin{equation*}
\int_{-L}^{L} S_{M}^{2}(x) d x=2 L\left\{\frac{a^{2} o}{2}+\sum_{n=1}^{M}\left(a_{n}^{2}+b_{n}^{2}\right)\right\} \tag{5}
\end{equation*}


Substitution of Equations (4) and (5) into Equation (3) and dividing by $L$ yields the required result.

Taking the limit as $M \rightarrow \infty$, we obtain Bessel's inequality


\begin{equation*}
\frac{a_{0}^{2}}{2}+\sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right) \leqq \frac{1}{L} \int_{-L}^{L}\{f(x)\}^{2} d x \tag{6}
\end{equation*}


If the equality holds, we have Parseval's identity (Problem 13.13).

We can think of $S_{M}(x)$ as representing an approximation to $f(x)$, while the left-hand side of Equation (2), divided by $2 L$, represents the mean square error of the approximation. Parseval's identity indicates that as $M \rightarrow \infty$, the mean square error approaches zero, while Bessels' inequality indicates the possibility that this mean square error does not approach zero.

The results are connected with the idea of completeness of an orthonormal set. If, for example, we were to leave out one or more terms in a Fourier series ( $\cos 4 \pi x / L$, for example), we could never get the mean square error to approach zero no matter how many terms we took. For an analogy with three-dimensional vectors, see Problem 13.60.

\section*{Differentiation and integration of Fourier series}
13.16. (a) Find a Fourier series for $f(x)=x^{2}, 0<x<2$, by integrating the series of Problem 13.12(a). (b) Use (a) to evaluate the series $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{2}}$.

(a) From Problem 13.12(a).


\begin{equation*}
x=\frac{4}{\pi}\left(\sin \frac{\pi x}{2}-\frac{1}{2} \sin \frac{2 \pi x}{2}+\frac{1}{3} \sin \frac{3 \pi x}{2}-\cdots\right) \tag{1}
\end{equation*}


Integrating both sides from 0 to $x$ (applying the theorem of Page 352) and multiplying by 2, we find


\begin{equation*}
x^{2}=C=\frac{16}{\pi^{2}}\left(\cos \frac{\pi x}{2}-\frac{1}{2^{2}} \cos \frac{2 \pi x}{2}+\frac{1}{3^{2}} \cos \frac{3 \pi x}{2}-\cdots\right) \tag{2}
\end{equation*}


where $x^{2}=C=\frac{16}{\pi^{2}}\left(1-\frac{1}{2^{2}}+\frac{1}{3^{2}}-\frac{1}{4^{2}}+\cdots\right)$

(b) To determine $\mathrm{C}$ in another way, note that Equation (2) represents the Fourier cosine series for $x^{2}$ in $0<x$ $<2$. Then, since $L=2$ in this case,

$$
C=\frac{a_{0}}{2}=\frac{1}{L} \int_{0}^{L} f(x)=\frac{1}{2} \int_{0}^{2} x^{2} d x=\frac{4}{3}
$$

Then, from the value of $C$ in (a), we have

$$
\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{2}}=1-\frac{1}{2^{2}}+\frac{1}{3^{2}}-\frac{1}{4^{2}}+\cdots=\frac{\pi^{2}}{16} \cdot \frac{4}{3}=\frac{\pi^{2}}{12}
$$

13.17. Show that term-by-term differentiation of the series in Problem 13.12(a) is not valid.

Term-by-term differentiation yields $2\left(\cos \frac{\pi x}{2}-\cos \frac{2 \pi x}{2}+\cos \frac{3 \pi x}{2}-\cdots\right)$. Since the $n$th term of this series does not approach 0 , the series does not converge for any value of $x$.

\section*{Convergence of Fourier series}
13.18. Prove that

(a) $\frac{1}{2}+\cos t+\cos 2 t+\cdots+\cos M t=\frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t}$

(b) $\frac{1}{\pi} \int_{0}^{\pi} \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t=\frac{1}{2}, \quad \frac{1}{\pi} \int_{-\pi}^{0} \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t=\frac{1}{2}$\\
(a) We have $\cos n t \sin \frac{1}{2} t=\frac{1}{2}\left\{\sin \left(n+\frac{1}{2}\right) t-\sin \left(n-\frac{1}{2}\right) t\right\}$.

Then, summing from $n=1$ to $M$,

$$
\begin{aligned}
\sin \frac{1}{2} t\{\cos t+\cos 2 t+\cdots+\cos M t\} .= & \left(\sin \frac{3}{2} t-\sin \frac{1}{2} t\right)+\left(\sin \frac{5}{2} t-\sin \frac{3}{2} t\right) \\
& +\cdots+\left(\sin \left(M+\frac{1}{2}\right) t-\sin \left(M-\sin \frac{1}{2} t\right)\right. \\
= & \frac{1}{2}\left\{\sin \left(M+\frac{1}{2}\right) t-\sin \frac{1}{2} t\right\}
\end{aligned}
$$

On dividing by $\sin \frac{1}{2} t$ and adding $\frac{1}{2}$, the required result follows.

(b) Integrating the result in (a) from $-\pi$ to 0 and 0 to $\pi$, respectively, gives the required results, since the integrals of all the cosine terms are zero.

13.19. Prove that $\lim _{n \rightarrow \infty} \int_{-\pi}^{\pi} f(x) \sin n x d x=\lim _{n \rightarrow \infty} \int_{-\pi}^{\pi} f(x) \cos n x d x=0$ if $f(x)$ is piecewise continuous.

This follows at once from Problem 13.15, since if the series $\frac{a_{0}^{2}}{2}+\sum_{n=1}^{\infty}\left(a_{n}^{2}+b_{n}^{2}\right)$ is convergent,\\
$a=\lim b=0$. $\lim _{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} b_{n}=0$.

The result is sometimes called Riemann's theorem.

13.20. Prove that $\lim _{m \rightarrow \infty} \int_{-\pi}^{\pi} f(x) \sin \left(M+\frac{1}{2}\right) x d x=0$ if $f(x)$ is piecewise continuous.

We have

$$
\int_{-\pi}^{\pi} f(x) \sin \left(M+\frac{1}{2}\right) x d x=\int_{-\pi}^{\pi}\left\{f(x) \sin \frac{1}{2} x\right\} \cos M x d x+\int_{-\pi}^{\pi}\left\{f(x) \cos \frac{1}{2} x\right\} \sin M x d x
$$

Then the required result follows at once by using the result of Problem 13.19, with $f(x)$ replaced by $f(x) \sin \frac{1}{2}$ $x$ and $f(x) \cos \frac{1}{2} x$, respectively, which are piecewise continuous if $f(x)$ is.

The result can also be proved when the integration limits are $a$ and $b$ instead of $-\pi$ and $\pi$.

13.21. Assuming that $L=\pi$, i.e., that the Fourier series corresponding to $f(\mathrm{x})$ has period $2 L=2 \pi$, show that

$$
S_{M}(x)=\frac{a_{0}}{2}+\sum_{n=1}^{M}\left(a_{n} \cos n x+b_{n} \sin n x\right)=\frac{1}{\pi} \int_{-\pi}^{\pi} f(t+x) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t
$$

Using the formulas for the Fourier coefficients with $L=\pi$, we have

$$
\begin{aligned}
a_{n} \cos n x+b_{n} \sin n x & =\left(\frac{1}{\pi} \int_{-\pi}^{\pi} f(u) \cos n u d u\right) \cos n x+\left(\frac{1}{\pi} \int_{-\pi}^{\pi} f(u) \sin n u d u\right) \sin n x \\
& =\frac{1}{\pi} \int_{-\pi}^{\pi} f(u)(\cos n u \cos n x+\sin n u \sin n x) d u \\
& =\frac{1}{\pi} \int_{-\pi}^{\pi} f(u) \cos n(u-x) d u
\end{aligned}
$$

Also,

$$
\frac{a_{o}}{2}=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(u) d u
$$

Then

$$
\begin{aligned}
S_{M}(x) & =\frac{a_{0}}{2}+\sum_{n=1}^{M}\left(a_{n} \cos n x+b_{n} \sin n x\right) \\
& =\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(u) d u+\frac{1}{\pi} \sum_{n=1}^{M} \int_{-\pi}^{\pi} f(u) \cos n(u-x) d u \\
& =\frac{1}{\pi} \int_{-\pi}^{\pi} f(u)\left\{\frac{1}{2}+\sum_{n=1}^{M} \cos n(u-x)\right\} d u \\
& =\frac{1}{\pi} \int_{-\pi}^{\pi} f(u) \frac{\sin \left(M+\frac{1}{2}\right)(u-x)}{2 \sin \frac{1}{2}(u-x)} d u
\end{aligned}
$$

using Problem 13.18. Letting $u-x=t$, we have

$$
S_{M}(x)=\frac{1}{\pi} \int_{-\pi-x}^{\pi-x} f(t+x) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t
$$

Since the integrand has period $2 \pi$, we can replace the interval $-\pi-x, \pi-x$ by any other interval of length $2 \pi$, in particular, $-\pi, \pi$. Thus, we obtain the required result.

13.22.

Prove that

$$
\begin{aligned}
S_{M}(x)-\left(\frac{(f(x+0)+f(x-0))}{2}\right)= & \frac{1}{\pi} \int_{-\pi}^{0}\left(\frac{(f(t+x)-f(x-0))}{2 \sin \frac{1}{2} t}\right) \sin \left(M+\frac{1}{2}\right) t d t \\
& +\frac{1}{\pi} \int_{0}^{\pi}\left(\frac{(f(t+x)-f(x+0))}{2 \sin \frac{1}{2} t}\right) \sin \left(M+\frac{1}{2}\right) t d t
\end{aligned}
$$

From Problem 13.21,


\begin{equation*}
S_{M}(x)=\frac{1}{\pi} \int_{-\pi}^{0} f(t+x) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t+\frac{1}{\pi} \int_{0}^{\pi} f(t+x) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t \tag{1}
\end{equation*}


Multiplying the integrals of Problem 13.18(b) by $f(x-0)$ and $f(x+0)$, respectively,


\begin{equation*}
\frac{f(x+0)+f(x-0)}{2}=\frac{1}{\pi} \int_{-\pi}^{0} f(x-0) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t+\frac{1}{\pi} \int_{-\pi}^{0} f(x+0) \frac{\sin \left(M+\frac{1}{2}\right) t}{2 \sin \frac{1}{2} t} d t \tag{2}
\end{equation*}


Subtracting Equation (2) from Equation (1) yields the required result.

13.23. If $f(\mathrm{x})$ and $f^{\prime}(x)$ are piecewise continuous in $(-\pi, \pi)$, prove that

$$
\lim _{M \rightarrow \infty} S_{M}(x)=\frac{f(x+0)+f(x-0)}{2}
$$

tinuous.

The function $\frac{f(t+x)-f(x+0)}{2 \sin \frac{1}{2} t}$ is piecewise continuous in $0<t \leqq \pi$ because $f(x)$ is piecewise con-\\
ous.

Also, $\lim _{t \rightarrow 0+} \frac{f(t+x)-f(x+0)}{2 \sin \frac{1}{2} t}=\lim _{t \rightarrow 0+} \frac{f(t+x)-f(x+0)}{t} \cdot \frac{t}{2 \sin \frac{1}{2} t}=\lim _{t \rightarrow 0+} \frac{f(t+x)-f(x+0)}{t}$ exists, since, by hypothesis, $f^{\prime}(x)$ is piecewise continuous so that the right-hand derivative of $f(x)$ at each $x$ exists.

Thus, $\frac{f(t+x)-f(x-0)}{2 \sin \frac{1}{2} t}$ is piecewise continuous in $0 \leqq t \leqq \pi$.

Similarly, $\frac{f(t+x)-f(x-0)}{2 \sin \frac{1}{2} t}$ is piecewise continuous in $-\pi \leqq t \leqq 0$.

Then, from Problems 13.20 and 13.22, we have

$$
\lim _{M \rightarrow \infty} S_{M}(x)-\left\{\frac{f(x+0)-f(x-0)}{2}\right\}=0 \quad \text { or } \quad \lim _{M \rightarrow \infty} S_{M}(x)=\left\{\frac{f(x+0)+f(x-0)}{2}\right\}
$$

\section*{Boundary value problems}
13.24. Find a solution $U(x, t)$ of the boundary value problem

$$
\begin{array}{ll}
\frac{\partial U}{\partial t}=3 \frac{\partial^{2} U}{\partial x^{2}} & t>0,0<x<2 \\
U(0, t)=0, U(2, t)=0 & t>0 \\
U(x, 0)=x & 0<x<2
\end{array}
$$

A method commonly employed in practice is to assume the existence of a solution of the partial differential equation having the particular form $U(x, t)=X(x) T(t)$, where $X(x)$ and $T(t)$ are functions of $x$ and $t$, respectively, which we shall try to determine. For this reason, the method is often called the method of separation of variables.

Substitution in the differential equation yields


\begin{equation*}
\frac{\partial U}{\partial t}(X T)=3 \frac{\partial^{2}}{\partial x^{2}}(X T) \tag{1}
\end{equation*}


or


\begin{equation*}
X \frac{d T}{d t}=3 T \frac{d^{2} X}{d x^{2}} \tag{2}
\end{equation*}


where we have written $X$ and $T$ in place of $X(x)$ and $T(t)$.

Equation (2) can be written as


\begin{equation*}
\frac{1}{3 T} \frac{d T}{d t}=\frac{1}{X} \frac{d^{2} X}{d x^{2}} \tag{3}
\end{equation*}


Since one side depends only on $t$ and the other only on $x$, and since $x$ and $t$ are independent variables, it is clear that each side must be a constant $c$.

In Problem 13.47 we see that if $c \geq 0$, a solution satisfying the given boundary conditions cannot exist.

Let us thus assume that $c$ is a negative constant, which we write as $-\lambda^{2}$. Then, from Equation (3), we obtain two ordinary differentiation equations


\begin{equation*}
\frac{d T}{d t}+3 \lambda^{2} T=0, \quad \frac{d^{2} X}{d x^{2}}+\lambda^{2} X=0 \tag{4}
\end{equation*}


whose solutions are, respectively,


\begin{equation*}
T=C_{1} e^{-3 \lambda^{2} t}, \quad X=A_{1} \cos \lambda x+B_{1} \sin \lambda x \tag{5}
\end{equation*}


A solution is given by the product of $X$ and $T$, which can be written


\begin{equation*}
U(x, t)=e^{-3 \lambda^{2} t}(A \cos \lambda x+B \sin \lambda x) \tag{6}
\end{equation*}


where $A$ and $B$ are constants.

We now seek to determine $A$ and $B$ so that Equation (6) satisfies the given boundary conditions. To satisfy the condition $U(0, t)=0$, we must have


\begin{equation*}
e^{-s \lambda^{2} t}(A)=0 \quad \text { or } \quad A=0 \tag{7}
\end{equation*}


so that Equation (6) becomes


\begin{equation*}
U(x, t)=B e^{-3 \lambda^{2} t} \sin \lambda x \tag{8}
\end{equation*}


To satisfy the condition $U(2, t)=0$, we must then have


\begin{equation*}
B e^{-s \lambda^{2} t} \sin 2 \lambda=0 \tag{9}
\end{equation*}


Since $B=0$ makes the solution (8) identically zero, we avoid this choice and instead take


\begin{equation*}
\sin 2 \lambda=0, \quad \text { i.e., } \quad 2 \lambda=m \pi \quad \text { or } \quad \lambda=\frac{m \pi}{2} \tag{10}
\end{equation*}


where $m=0, \pm 1, \pm 2, \ldots$.

Substitution in Equation (8) now shows that a solution satisfying the first two boundary conditions is


\begin{equation*}
U(x, t)=B_{m} e^{-3 m^{2} \pi^{2} t / 4} \sin \frac{m \pi x}{2} \tag{11}
\end{equation*}


where we have replaced $B$ by $B_{m}$, indicating that different constants can be used for different values of $m$.

If we now attempt to satisfy the last boundary condition $U(x, 0)=x, 0<x<2$, we find it to be impossible using Equation (11). However, upon recognizing the fact that sums of solutions having the form (11) are also solutions (called the principle of superposition), we are led to the possible solution


\begin{equation*}
U(x, t)=\sum_{m=1}^{\infty} B_{m} e^{-3 m^{2} \pi^{2} t / 4} \sin \frac{m \pi x}{2} \tag{12}
\end{equation*}


From the condition $U(x, 0)=x, 0<x<2$, we see, on placing $t=0$, that Equation (12) becomes


\begin{equation*}
x=\sum_{m=1}^{\infty} B_{m} \sin \frac{m \pi x}{2} \quad 0<x<2 \tag{13}
\end{equation*}


This, however, is equivalent to the problem of expanding the function $f(x)=x$ for $0<x<2$ into a sine series. The solution to this is given in Problem 13.12(a), from which we see that $B_{m}=\frac{-4}{m \pi} \cos m \pi$ so that\\
Equation (12) becomes


\begin{equation*}
U(x, t)=\sum_{m=1}^{\infty}\left(-\frac{4}{m \pi} \cos m \pi\right) e^{-3 m^{2} \pi^{2} t / 4} \sin \frac{m \pi x}{2} \tag{14}
\end{equation*}


which is a formal solution. To check that Equation (14) is actually a solution, we must show that it satisfies the partial differential equation and the boundary conditions. The proof consists in justification of term-by-term differentiation and use of limiting procedures for infinite series and may be accomplished by methods of Chapter 11.

The boundary value problem considered here has an interpretation in the theory of heat conduction. The equation $\frac{\partial U}{\partial t}=k \frac{\partial^{2} U}{\partial x^{2}}$ is the equation for heat conduction in a thin rod or wire located on the $x$ axis between $x=0$ and $x=L$ if the surface of the wire is insulated so that heat cannot enter or escape. $U(x, t)$ is the temperature at any place $x$ in the rod at time $t$. The constant $k=K / s p$ (where $K$ is the thermal conductivity, $s$ is the specific heat, and $\rho$ is the density of the conducting material) is called the diffusivity. The boundary conditions $U(0, t)=0$ and $U(L, t)=0$ indicate that the end temperatures of the rod are kept at zero units for all time $t>0$, while $U(x, 0)$ indicates the initial temperature at any point $x$ of the rod. In this problem the length of the rod is $L=2$ units, while the diffusivity is $k=3$ units.

\section*{Orthogonal functions}
13.25. (a) Show that the set of functions

$$
1, \sin \frac{\pi x}{L}, \cos \frac{\pi x}{L}, \sin \frac{2 \pi x}{L}, \cos \frac{2 \pi x}{L}, \sin \frac{3 \pi x}{L}, \cos \frac{3 \pi x}{L}, \ldots
$$

forms an orthogonal set in the interval $(-L, L)$.

(b) Determine the corresponding normalizing constants for the set in (a) so that the set is orthonormal in $(-L, L)$.

(a) This follows at once from the results of Problems 13.2 and 13.3.

(b) By Problem 13.3,

$$
\int_{-L}^{L} \sin ^{2} \frac{m \pi x}{L} d x=L, \quad \int_{-L}^{L} \cos ^{2} \frac{m \pi x}{L} d x=L
$$

Then

$$
\int_{-L}^{L}\left(\sqrt{\frac{1}{L}} \sin \frac{m \pi x}{L}\right)^{2} d x=1 \quad \int_{-L}^{L}\left(\sqrt{\frac{1}{L}} \cos \frac{m \pi x}{L}\right)^{2} d x=1
$$

Also,

$$
\int_{-L}^{L}(1)^{2} d x=2 L \quad \text { or } \quad \int_{-L}^{L}\left(\frac{1}{\sqrt{2 L}}\right)^{2} d x=1
$$

Thus, the required orthonormal set is given by

$$
\frac{1}{\sqrt{2 L}}, \frac{1}{\sqrt{L}} \sin \frac{\pi x}{L}, \frac{1}{\sqrt{L}} \cos \frac{\pi x}{L}, \frac{1}{\sqrt{L}} \sin \frac{2 \pi x}{L}, \frac{1}{\sqrt{L}} \cos \frac{2 \pi x}{L}, \ldots
$$

\section*{Miscellaneous problems}
13.26. Find a Fourier series for $f(\mathrm{x})=\cos \alpha x,-\pi \leqq x \leqq \pi$, where $\alpha \neq 0, \pm 1, \pm 2, \pm 3, \ldots$

We shall take the period as $2 \pi$ so that $2 L=2 \pi, L=\pi$. Since the function is even, $b_{n}=0$ and

$$
\begin{aligned}
a_{n} & =\frac{2}{L} \int_{0}^{L} f(x) \cos n x d x=\frac{2}{\pi} \int_{0}^{\pi} \cos \alpha x \cos n x d x \\
& =\frac{1}{\pi} \int_{0}^{\pi}\{\cos (\alpha-n) x+\cos (\alpha+n) x\} d x \\
& =\frac{1}{\pi}\left\{\frac{\sin (\alpha-n) \pi}{\alpha-n}+\frac{\sin (\alpha+n) \pi}{\alpha+n}+\cdots\right\}=\frac{2 \alpha \sin \alpha \pi \cos n \pi}{\pi\left(\alpha^{2}-n^{2}\right)} \\
\alpha_{0} & =\frac{2 \sin \alpha \pi}{\alpha \pi}
\end{aligned}
$$

Then

$$
\begin{aligned}
\cos \alpha x & =\frac{\sin \alpha \pi}{\alpha \pi}+\frac{2 \alpha \sin \alpha \pi}{\pi} \sum_{n=1}^{\infty} \frac{\cos n \pi}{\alpha^{2}-2^{2}} \cos n x \\
& =\frac{\sin \alpha \pi}{\pi}\left(\frac{1}{\alpha}-\frac{2 \alpha}{\alpha^{2}-1^{2}} \cos x+\frac{2 \alpha}{\alpha^{2}-2^{2}} \cos 2 x-\frac{2 \alpha}{\alpha^{2}-3^{2}} \cos 3 x+\cdots\right)
\end{aligned}
$$

13.27. Prove that $\sin x=x\left(1-\frac{x^{2}}{\pi^{2}}\right)\left(1-\frac{x^{2}}{(2 \pi)^{2}}\right)\left(1-\frac{x^{2}}{(3 \pi)^{2}}\right) \ldots$

Let $x=\pi$ in the Fourier series obtained in Problem 13.26. Then

$$
\cos \alpha=\frac{\sin \alpha \pi}{\pi}\left(\frac{1}{\alpha}+\frac{2 \alpha}{\alpha^{2}-1^{2}}+\frac{2 \alpha}{\alpha^{2}-2^{2}}+\frac{2 \alpha}{\alpha^{2}-3^{2}}+\cdots\right)
$$

or


\begin{equation*}
\pi \cot \alpha \pi-\frac{1}{\alpha}=\frac{2 \alpha}{\alpha^{2}-1^{2}}+\frac{2 \alpha}{\alpha^{2}-2^{2}}+\frac{2 \alpha}{\alpha^{2}-3^{2}}+\cdots \tag{1}
\end{equation*}


This result is of interest since it represents an expansion of the contangent into partial fractions.

By the Weierstrass $M$ test, the series on the right of Equation (1) converges uniformly for $0 \leqq|\alpha| \leqq|x|<1$ and the left-hand side of (1) approaches zero as $\alpha \rightarrow 0$, as is seen by using L'Hospital's rule. Thus, we can integrate both sides of (1) from 0 to $x$ to obtain

$$
\int_{0}^{x}\left(\pi \cot \alpha \pi-\frac{1}{\alpha}\right) d \alpha=\int_{0}^{x} \frac{2 \alpha}{\alpha^{2}-1} d \alpha+\int_{0}^{x} \frac{2 \alpha}{\alpha^{2}-2^{2}} d \alpha+\cdots
$$

or

$$
\left.\ln \left(\frac{\sin \alpha \pi}{\alpha \pi}\right)\right|_{0} ^{x}=\ln \left(1-\frac{x^{2}}{1^{2}}\right)+\ln \left(1-\frac{x^{2}}{2^{2}}\right)+\cdots
$$

i.e.,

$$
\begin{aligned}
\ln \left(\frac{\sin \pi}{\pi x}\right) & \left.=\lim _{n \rightarrow \infty} \ln \left(1-\frac{x^{2}}{1^{2}}\right)+\ln \left(1-\frac{x^{2}}{2^{2}}\right) \right\rvert\,+\cdots+\ln \left(1-\frac{x^{2}}{n^{2}}\right) \\
= & \lim _{n \rightarrow \infty} \ln \left\{\left(1-\frac{x^{2}}{1^{2}}\right)\left(1-\frac{x^{2}}{2^{2}}\right) \cdots\left(1-\frac{x^{2}}{n^{2}}\right)\right\} \\
= & \ln \left\{\lim _{n \rightarrow \infty}\left(1-\frac{x^{2}}{1^{2}}\right)\left(1-\frac{x^{2}}{2^{2}}\right) \cdots\left(1-\frac{x^{2}}{n^{2}}\right)\right\}
\end{aligned}
$$

so that


\begin{equation*}
\frac{\sin \pi}{\pi x}=\lim _{n \rightarrow \infty}\left(1-\frac{x^{2}}{1^{2}}\right)\left(1-\frac{x^{2}}{2^{2}}\right) \cdots\left(1-\frac{x^{2}}{n^{2}}\right)=\left(1-\frac{x^{2}}{1^{2}}\right)\left(1-\frac{x^{2}}{2^{2}}\right) \ldots \tag{2}
\end{equation*}


Replacing $x$ by $x / \pi$, we obtain


\begin{equation*}
\sin x=x\left(1-\frac{x^{2}}{\pi^{2}}\right)\left(1-\frac{x^{2}}{(2 \pi)^{2}}\right) \cdots \tag{3}
\end{equation*}


called the infinite product for $\sin x$, which can be shown valid for all $x$. The result is of interest since it corresponds to a factorization of $\sin x$ in a manner analogous to factorization of a polynomial.

13.28. Prove that $\frac{\pi}{2}=\frac{2 \cdot 2 \cdot 4 \cdot 4 \cdot 6 \cdot 6 \cdot 8 \cdot 8 \cdots}{1 \cdot 3 \cdot 3 \cdot 5 \cdot 5 \cdot 7 \cdot 7 \cdot 9 \ldots}$.

Let $x=1 / 2$ in Equation (2) of Problem 13.27. Then,

$$
\frac{2}{\pi}=\left(1-\frac{1}{2^{2}}\right)\left(1-\frac{1}{4^{2}}\right)\left(1-\frac{1}{6^{2}}\right) \cdots=\left(\frac{1}{2} \cdot \frac{3}{2}\right)\left(\frac{3}{4} \cdot \frac{5}{4}\right)\left(\frac{5}{6} \cdot \frac{7}{6}\right) \cdots
$$

Taking reciprocals of both sides, we obtain the required result, which is often called Wallis's product.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Fourier Series}
13.29. Graph each of the following functions and find their corresponding Fourier series using properties of even and odd functions wherever applicable.\\
(a) $f(x)=\left\{\begin{array}{cc}8 & 0<x<2 \\ -8 & 2<x<4\end{array}\right.$ Period $4 \quad$ (c) $f(x)=4 x, 0<x<10$, Period 10

(b) $f(x)=\left\{\begin{array}{cc}-x & -4 \leqq x \leqq 0 \\ x & 0 \leqq x \leqq 4\end{array}\right.$ Period $8 \quad$ (d) $f(x)=\left\{\begin{array}{cc}2 x & 0<x<3 \\ 0 & -3<x<0\end{array}\right\}$ Period 6

Ans. (a) $\frac{16}{\pi} \sum_{n=1}^{\infty} \frac{(1-\cos n \pi)}{n} \sin \frac{n \pi x}{2} \quad$ (c) $20-\frac{40}{\pi} \sum_{n=1}^{\infty} \frac{1}{n} \sin \frac{n \pi x}{5}$\\
(b) $2-\frac{8}{\pi^{2}} \sum_{n=1}^{\infty} \frac{(1-\cos n \pi)}{n^{2}} \cos \frac{n \pi x}{4}$\\
(d) $\frac{3}{2} \sum_{n=1}^{\infty}+\left\{\frac{6(\cos n \pi-1)}{n^{2} \pi^{2}} \cos \frac{n \pi x}{3}-\frac{6 \cos n \pi}{n \pi} \sin \frac{n \pi x}{3}\right\}$

13.30. In each part of Problem 13.29, tell where the discontinuities of $f(x)$ are located and to what value the series converges at the discontunities.\\
Ans. (a) $x=0, \pm 2, \pm 4, \ldots ; 0$\\
(c) $x=0, \pm 10, \pm 20, \ldots ; 20$\\
(b) no discontinuities\\
(d) $x= \pm 3, \pm 9, \pm 15, \ldots ; 3$

13.31. Expand $f(x)=\left\{\begin{array}{ll}2-x & 0<x<4 \\ x-6 & 4<x<8\end{array}\right.$ in a Fourier series of period 8 .

$$
\text { Ans. } \frac{16}{\pi^{2}}\left\{\cos \frac{\pi x}{4}+\frac{1}{3^{2}} \cos \frac{3 \pi x}{4}+\frac{1}{5^{2}} \cos \frac{5 \pi x}{4}+\cdots\right\}
$$

13.32. (a) Expand $f(x)=\cos x, 0<x<\pi$, in a Fourier sine series. (b) How should $\mathrm{f}(\mathrm{x})$ be defined at $\mathrm{x}=0$ and $\mathrm{x}=\pi$ so that the series will converge to $\mathrm{f}(\mathrm{x})$ for $0 \leqq \mathrm{x} \leqq \pi$ ?

$$
\text { Ans. (a) } \frac{8}{\pi} \sum_{n=1}^{\infty} \frac{n \sin 2 n x}{4 n^{2}-1} \quad(b) f(0)=f(\pi)=0
$$

13.33. (a) Expand in a Fourier series $f(x)=\cos x, 0<x<\pi$ if the period is $\pi$, and (b) compare with the result of Problem 13.32, explaining the similarities and differences, if any.

Ans. Answer is the same as in Problem 13.32.

13.34. Expand $f(x)=\left\{\begin{array}{cc}x & 0<x<4 \\ 8-x & 4<x<8\end{array}\right.$ in a series of (a) sines and (b) cosines.

Ans. (a) $\frac{32}{\pi^{2}} \sum_{n=1}^{\infty} \frac{1}{n^{2}} \sin \frac{n \pi}{2} \sin \frac{n \pi x}{8}$ (b) $\frac{16}{\pi^{2}} \sum_{n=1}^{\infty}\left(\frac{2 \cos n \pi / 2-\cos n \pi-1}{n^{2}}\right) \cos \frac{n \pi x}{8}$

13.35. Prove that for $0 \leqq x \leqq \pi$,

(a) $x(\pi-x)=\frac{\pi^{2}}{6}-\left(\frac{\cos 2 x}{1^{2}}+\frac{\cos 4 x}{2^{2}} \frac{\cos 6 x}{3^{2}}+\cdots\right)$

(b) $x(\pi-x)=\frac{8}{\pi}\left(\frac{\sin x}{1^{3}}+\frac{\sin 3 x}{3^{3}}+\frac{\sin 5 x}{5^{3}}+\cdots\right)$

13.36. Use the preceding problem to show that\\
(a) $\sum_{n=1}^{\infty} \frac{1}{n^{2}}=\frac{\pi^{2}}{6}$\\
(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{2}}=\frac{\pi^{2}}{12}$\\
(c) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(2 n-1)^{3}}=\frac{\pi^{3}}{32}$

13.37. Show that $\frac{1}{1^{3}}+\frac{1}{3^{3}}-\frac{1}{5^{3}}-\frac{1}{7^{3}}+\frac{1}{9^{3}}+\frac{1}{11^{3}}-\cdots=\frac{3 \pi^{2} \sqrt{2}}{16}$.

\section*{Differentiation and integration of Fourier series}
13.38. (a) Show that for $-\pi<x<\pi$,

$$
x=2\left(\frac{\sin x}{1}-\frac{\sin 2 x}{2}+\frac{\sin 3 x}{3}-\ldots\right)
$$

(b) By integrating the result of (a), show that for $-\pi \leqq x \leqq \pi$,

$$
x^{2}=\frac{\pi^{2}}{3}-4\left(\frac{\cos x}{1^{2}}-\frac{\cos 2 x}{2^{2}}+\frac{\sin 3 x}{3^{2}}-\cdots\right)
$$

(c) By integrating the result of (b), show that for $-\pi \leqq \mathrm{x} \leqq \pi$,

$$
x(\pi-x)(\pi+x)=12\left(\frac{\sin x}{1^{3}}-\frac{\sin 2 x}{2^{3}}+\frac{\sin 3 x}{3^{3}}-\cdots\right)
$$

13.39. (a) Show that for $-\pi<x<\pi$,

$$
x \cos x=-\frac{1}{2} \sin x+2\left(\frac{2}{1 \cdot 3} \sin 2 x-\frac{3}{2 \cdot 4} \sin 3 x+\frac{4}{3 \cdot 5} \sin 4 x-\cdots\right)
$$

(b) Use (a) to show that for $-\pi \leqq \mathrm{x} \leqq \pi$,

$$
x \sin x=1-\frac{1}{2} \cos x-2\left(\frac{\cos 2 x}{1.3}-\frac{\cos 3 x}{2.4}+\frac{\cos 4 x}{3.5}-\cdots\right)
$$

13.40. By differentiating the result of Problem 13.35 (b), prove that for $0 \leqq x \leqq \pi$,

$$
x=\frac{\pi}{2}-\frac{4}{\pi}\left(\frac{\cos x}{1^{2}}+\frac{\cos 3 x}{3^{2}}+\frac{\cos 5 x}{5^{2}}+\cdots\right)
$$

\section*{Parseval's identity}
13.41. By using Problem 13.35 and Parseval's identity, show that

$$
\begin{array}{ll}
\text { (a) } \sum_{n=1}^{\infty} \frac{1}{n^{4}}=\frac{\pi^{4}}{90} & \text { (b) } \sum_{n=1}^{\infty} \frac{1}{n^{6}}=\frac{\pi^{6}}{945}
\end{array}
$$

13.42. Show that $\frac{1}{1^{2} \cdot 3^{2}}+\frac{1}{3^{2} \cdot 5^{2}}+\frac{1}{5^{2} \cdot 7^{2}}+\cdots=\frac{\pi^{2}-8}{16}$. (Hint: Use Problem 13.11.)

13.43. Show that

(a) $\sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{4}}=\frac{\pi^{4}}{96}$

(b) $\sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{6}}=\frac{\pi^{6}}{960}$

13.44. Show that $\frac{1}{1^{2} \cdot 2^{2} \cdot 3^{2}}+\frac{1}{2^{2} \cdot 3^{2} \cdot 4^{2}}+\frac{1}{3^{2} \cdot 4^{2} \cdot 5^{2}}+\cdots=\frac{4 \pi^{2}-39}{16}$

\section*{Boundary value problems}
13.45. (a) Solve $\frac{\partial U}{\partial t}=2 \frac{\partial^{2} U}{\partial x^{2}}$, subject to the conditions $U(0, t)=0, U(4, t)=0, U(x, 0)=3 \sin \pi x-2 \sin 5 \pi x$, where $0<x<4, t>0$. (b) Give a possible physical interpretation of the problem and solution.

Ans. (a) $U(x, t)=3 e^{-2 \pi 2} t \sin \pi x-2 e^{-50 \pi 2} t \sin 5 \pi x$

13.46. Solve $\frac{\partial U}{\partial t}=\frac{\partial^{2} U}{\partial x^{2}}$, subject to the conditions $U(0, t)=0, U(6, t)=0, U(x, 0)=\left\{\begin{array}{ll}1 & 0<x<3 \\ 0 & 3<x<6\end{array}\right.$, and interpret and interpret physically.

Ans. $U(x, t)=\sum_{m=1}^{\infty} 2\left[\frac{1-\cos (m \pi / 3)}{m \pi}\right] e^{-m^{2} \pi^{2} t / 36} \sin \frac{m \pi x}{6}$

13.47. Show that if each side of Equation (3), Page 369, is a constant $c$, where $c \geqq 0$, then there is no solution satisfying the boundary value problem.

13.48. A flexible string of length $\pi$ is tightly stretched between points $x=0$ and $x==\pi$ on the $x$ axis, its ends are fixed at these points. When set into small transverse vibration, the displacement $Y(x, t)$ from the the $x$ axis of any point $x$ at time $t$ is given by $\frac{\partial^{2} Y}{\partial t^{2}}=a^{2} \frac{\partial^{2} Y}{\partial x^{2}}$, where $a^{2}=T / \rho, T=$ tension, $\rho=$ mass per unit length.

(a) Find a solution of this equation (sometimes called the wave equation) with $a^{2}=4$ which satisfies the conditions $Y(0, t)=0, Y(\pi, t)=0, Y(x, 0)=0.1 \sin x+0.01 \sin 4 x, Y t(x, 0)=0$ for $0<x<\pi, t>0$.

(b) Interpret physically the boundary conditions in (a) and the solution.

Ans. (a) $Y(x, t)=0.1 \sin x \cos 2 t+0.01 \sin 4 x \cos 8 t$

13.49. (a) Solve the boundary value problem $\frac{\partial^{2} Y}{\partial t^{2}}=9 \frac{\partial^{2} Y}{\partial x^{2}}$, subject to the conditions $Y(0, t)=0, Y(2, t)=0, Y(x, 0)$ $=0.05 x(2-x), Y_{t}(x, 0)=0$, where $0<x<2, t>0$. (b) Interpret physically.

Ans. (a) $Y(x, t)=\frac{1.6}{\pi^{3}} \sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{3}} \sin \frac{(2 n-1) \pi x}{2} \cos \frac{3(2 n-1) \pi t}{2}$

13.50. Solve the boundary value problem $\frac{\partial U}{\partial t}=\frac{\partial^{2} U}{\partial x^{2}}, U(0, t)=1, U(\pi, t)=3, U(x, 0)=2$. [Hint: Let $U(x, t)=$ $V(x, t)+F(x)$ and choose $F(x)$ so as to simplify the differential equation and boundary conditions for $V(x, t)$.]

Ans. $U(x, t)=1+\frac{2 x}{\pi}+\sum_{m=1}^{\infty} \frac{4 \cos m \pi}{m \pi} e^{-m^{2} t} \sin m x$

13.51. Give a physical interpretation to Problem 13.50 .

13.52. Solve Problem 13.49 with the boundary conditions for $Y(x, 0)$ and $Y_{t}(x, 0)$ interchanged; i.e., $Y(x)=$,0 , $Y_{t}(x, 0)=0.05 x(2-x)$, and give a physical interpretation.

Ans. $Y(x, t)=\frac{3.2}{3 \pi^{4}} \sum_{n=1}^{\infty} \frac{1}{(2 n-1)^{4}} \sin \frac{(2 n-1) \pi x}{2} \sin \frac{3(2 n-1) \pi t}{2}$

13.53. Verify that the boundary value problem of Problem 13.24 actually has the solution (14), Page 370 .

\section*{Miscellaneous Problems}
13.54. If $-\pi<x<\pi$ and $\alpha \neq 0, \pm 1 . \pm 2, \ldots$, prove that

$$
\frac{\pi \sin \alpha x}{2 \sin \alpha \pi}=\frac{\sin x}{1^{2}-\alpha^{2}}-\frac{2 \sin 2 x}{2^{2}-\alpha^{2}}+\frac{3 \sin 3 x}{3^{2}-\alpha^{2}}-\cdots
$$

13.55. If $-\pi<x<\pi$, prove that

(a) $\frac{\pi}{2} \frac{\sinh \alpha x}{\sinh \alpha \pi}=\frac{\sin x}{\alpha^{2}+1^{2}}-\frac{2 \sin 2 x}{\alpha^{2}+2^{2}}+\frac{3 \sin 3 x}{\alpha^{2}+3^{2}}-\cdots$

(b) $\frac{\pi}{2} \frac{\cosh \alpha x}{\sinh \alpha \pi}=\frac{1}{2 \alpha}-\frac{\alpha \cos x}{\alpha^{2}+1^{2}}+\frac{\alpha \cos 2 x}{\alpha^{2}+2^{2}}-\cdots$

13.56. Prove that $\sinh x=x\left(1+\frac{x^{2}}{\pi^{2}}\right)\left(1+\frac{x^{2}}{(2 \pi)^{2}}\right)\left(1+\frac{x^{2}}{(3 \pi)^{2}}\right) \ldots$

13.57. Prove that $\cos x=\left(1-\frac{4 x^{2}}{\pi^{2}}\right)\left(1-\frac{4 x^{2}}{(3 \pi)^{2}}\right)\left(1-\frac{4 x^{2}}{(5 \pi)^{2}}\right) \ldots[$ Hint: $\cos x=(\sin 2 x) /(2 \sin x)$.]

13.58. Show that

(a) $\frac{\sqrt{2}}{\sqrt{2}}=\frac{1 \cdot 3 \cdot 5 \cdot 7 \cdot 9 \cdot 22 \cdot 13 \cdot 15 \ldots}{2 \cdot 2 \cdot 6 \cdot 6 \cdot 10 \cdot 10 \cdot 14 \cdot 14 \ldots}$

(b) $\pi \sqrt{2}=4\left(\frac{4 \cdot 4 \cdot 8 \cdot 8 \cdot 12 \cdot 12 \cdot 16 \cdot 16 \ldots}{3 \cdot 5 \cdot 7 \cdot 9 \cdot 11 \cdot 13 \cdot 15 \cdot 17 \ldots}\right)$

13.59. Let $\mathbf{r}$ be any three-dimensional vector. Show that (a) $(\mathbf{r} \cdot \mathbf{i})^{2}+(\mathbf{r} \cdot \mathbf{j})^{2} \leqq(\mathbf{r})^{2}$ and (b) $(\mathbf{r} \cdot \mathbf{i})^{2}+(\mathbf{r} \cdot \mathbf{j})^{2}+(\mathbf{r} \cdot \mathbf{k})^{2}$ $=\mathbf{r}^{2}$ and discuss these with reference to Parseval's identity.

13.60. If $\left\{\phi_{n}(\mathrm{x})\right\}, n=1,2,3, \ldots$ is orthonormal in (a, b, prove that $\int_{a}^{b}\left\{f(x)-\sum_{n=1}^{\infty} c_{n} \phi_{n}(x)\right\}^{2} d x$ is a minimum when $c_{n}=\int_{a}^{b} f(x) \phi_{n}(x) d x$. Discuss the relevance of this result to Fourier series.

\section*{CHAPTER 14}
\section*{Fourier Integrals}
Fourierintegrals are generalizations of Fourierseries. The series representation $\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left\{a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right\}$ of a function is a periodic form on $-\infty<x<\infty$ obtained by generating the coefficients from the function's definition on the least period $[-L, L]$. If a function defined on the set of all real numbers has no period, then an analogy to Fourier integrals can be envisioned as letting $L \rightarrow \infty$ and replacing the integer valued index $n$ by a real valued function $\alpha$. The coefficients $a_{n}$ and $b_{n}$ then take the form $A(\alpha)$ and $B(\alpha)$. This mode of thought leads to the following definition. (See Problem 14.8.)

\section*{The Fourier Integral}
Let us assume the following conditions on $f(x)$ :

\begin{enumerate}
  \item $f(x)$ satisfies the Dirichlet conditions (Page 350) in every finite interval $(-L, L)$.

  \item $\quad \int_{-\infty}^{\infty}|f(x)| d x$ converges; i.e., $f(x)$ is absolutely integrable in $(-\infty, \infty)$.

\end{enumerate}

Then Fourier's integral theorem states that the Fourier integral of a function $f$ is


\begin{equation*}
f(x)=\int_{0}^{\infty}\{A(\alpha) \cos \alpha x+B(\alpha) \sin \alpha x\} d \alpha \tag{1}
\end{equation*}


where

\[
\left\{\begin{array}{l}
A(\alpha)=\frac{1}{\pi} \int_{-\infty}^{\infty} f(x) \cos \alpha x d x  \tag{2}\\
B(\alpha)=\frac{1}{\pi} \int_{-\infty}^{\infty} f(x) \sin \alpha x d x
\end{array}\right\}
\]

$A(\alpha)$ and $B(\alpha)$ with $-\infty<\alpha<\infty$ are generalizations of the Fourier coefficients $a_{n}$ and $b_{n}$. The right-hand side of Equation (1) is also called a Fourier integral expansion of $f$. (Since Fourier integrals are improper integrals, a review of Chapter 12 is a prerequisite to the study of this chapter.) The result (1) holds if $x$ is a point of continuity of $f(x)$. If $x$ is a point of discontinuity, we must replace $f(x)$ by $\frac{f(x+0)+f(x-0)}{2}$, as in the case of Fourier series. Note that these conditions are sufficient but not necessary.

In the generalization of Fourier coefficients to Fourier integrals, $a_{0}$ may be neglected, since whenever $\int_{-\infty}^{\infty} f(x) d x$ exists,

$$
\left|a_{0}\right|=\left|\frac{1}{L} \int_{-L}^{L} f(x) d x\right| \rightarrow 0 \quad \text { as } \quad L \rightarrow \infty
$$

\section*{Equivalent Forms of Fourier's Integral Theorem}
Fourier's integral theorem can also be written in the forms


\begin{align*}
f(x) & =\frac{1}{\pi} \int_{\alpha=0}^{\infty} \int_{u=-\infty}^{\infty} f(u) \cos \alpha(x-u) d u d \alpha  \tag{3}\\
f(x) & =\frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{i \alpha x} d \alpha \int_{-\infty}^{\infty} f(u) e^{i a u} d u \\
& =\frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(u) e^{i \alpha(u-x)} d u d \alpha \tag{4}
\end{align*}


where it is understood that if $f(x)$ is not continuous at $x$, the left side must be replaced by $\frac{f(x+0)+f(x-0)}{2}$.

These results can be simplified somewhat if $f(x)$ is either an odd or an even function, and we have

\[
\begin{array}{ll}
f(x)=\frac{2}{\pi} \int_{0}^{\infty} \cos \alpha x d x \int_{0}^{\infty} f(u) \cos \alpha u d u & \text { if } f(x) \text { is even } \\
f(x)=\frac{2}{\pi} \int_{0}^{\infty} \sin \alpha x d x \int_{0}^{\infty} f(u) \sin \alpha u d u & \text { if } f(x) \text { is odd } \tag{6}
\end{array}
\]

An entity of importance in evaluating integrals and solving differential and integral equations is introduced in the next paragraph. It is abstracted from the Fourier integral form of a function, as can be observed by putting Equation (4) in the form

$$
f(x)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-i \alpha x}\left\{\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{i \alpha u} f(u) d u\right\} d \alpha
$$

and identifying the parenthetic expression, as $F(\alpha)$. The following Fourier transforms result.

\section*{Fourier Transforms}
From Equation (4) it follows that

then


\begin{equation*}
F(\alpha)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) e^{i \alpha u} d u \tag{7}
\end{equation*}



\begin{equation*}
F(x)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} F(\alpha) e^{i a x} d \alpha \tag{8}
\end{equation*}


The function $F(\alpha)$ is called the Fourier transform of $f(x)$ and is sometimes written $F(\alpha)=\mathscr{F}\{f(x)\}$. The function $f(x)$ is the inverse Fourier transform of $F(\alpha)$ and is written $f(x)=\mathscr{F}^{-1}\{F(\alpha)\}$.

Note: The constants preceding the integral signs in Equations (7) and (8) were here taken as equal to $1 / \sqrt{2 \pi}$. However, they can be any constants different from zero so long as their product is $1 / 2 \pi$. This is called the symmetric form. The literature is not uniform as to whether the negative exponent appears in Equation (7) or in (8).

EXAMPLE. Determine the Fourier transform of $f$ if $f(x)=e^{-x}$ for $x>0$ and $e^{2 x}$ when $x<0$.

$$
\begin{aligned}
F(\alpha) & =\frac{1}{2 \sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{i \alpha x} f(x) d x=\frac{1}{2 \pi}\left\{\int_{-\infty}^{0} e^{i \alpha x} e^{2 x} d x+\int_{0}^{\infty} e^{i \alpha x} e^{-x} d x\right\} \\
& =\frac{1}{\sqrt{2 \pi}}\left\{\left.\frac{e^{i \alpha+2}}{i \alpha+2}\right|_{x \rightarrow-\infty} ^{x \rightarrow 0-}+\left.\frac{e^{i \alpha-1}}{i \alpha-1}\right|_{x \rightarrow 0+} ^{x \rightarrow \infty}\right\}=\frac{1}{\sqrt{2 \pi}}\left\{\frac{1}{2+\alpha i}+\frac{1}{1-\alpha i}\right\}
\end{aligned}
$$

If $f(x)$ is an even function, Equation (5) yields

\[
\left\{\begin{array}{l}
F_{c}(\alpha)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} f(u) \cos \alpha u d u  \tag{9}\\
f(x)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} F_{c}(\alpha) \cos \alpha x d x
\end{array}\right.
\]

and we call $F_{c}(\alpha)$ and $f(x)$ Fourier cosine transforms of each other.

If $f(x)$ is an odd function, Equation (6) yields

\[
\left\{\begin{array}{l}
F_{s}(\alpha)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} f(u) \sin \alpha u d u  \tag{10}\\
f(x)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} F_{s}(\alpha) \sin \alpha x d \alpha
\end{array}\right.
\]

and we call $F_{s}(\alpha)$ and $f(x)$ Fourier sine transforms of each other.

Note: The Fourier transforms $F_{c}$ and $F_{s}$ are (up to a constant) of the same form as $A(\alpha)$ and $B(\alpha)$. Since $f$ is even for $F_{c}$ and odd for $F_{s}$, the domains can be shown to be $0<\alpha<\infty$.

When the product of Fourier transforms is considered, a new concept called convolution comes into being, and in conjunction with it, a new pair (function and its Fourier transform) arises. In particular, if $F(\alpha)$ and $G(\alpha)$ are the Fourier transforms of $f$ and $g$, respectively, and the convolution of $f$ and $g$ is defined to be

then


\begin{equation*}
f * g=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} f(u) g(x-u) d u \tag{11}
\end{equation*}



\begin{align*}
& F(\alpha) G(\alpha)=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} e^{i \alpha u} f * g d u  \tag{12}\\
& f * g=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} e^{i \alpha x} F(\alpha) G(\alpha) d \alpha \tag{13}
\end{align*}


where in both Equations (11) and (13) the convolution $f * g$ is a function of $x$.

It may be said that multiplication is exchanged with convolution. Also "the Fourier transform of the convolution of two functions, $f$ and $g$ is the product of their Fourier transforms," i.e.,

$$
T(f * g)=G(f) T(g)
$$

$[F(\alpha) G(\alpha)$ and $f * g)$ are demonstrated to be a Fourier transform pair in Problem 14.29.]

Now equate the representations of $f * g$ expressed in Equations (11) and (13), i.e.,


\begin{equation*}
\frac{1}{\pi} \int_{-\infty}^{\infty} f(u) g(x-u) d u=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} e^{i a x} F(\alpha) G(\alpha) d \alpha \tag{14}
\end{equation*}


and let the parameter $x$ be zero; then


\begin{equation*}
\int_{-\infty}^{\infty} f(u) g(-u) d u=\int_{-\infty}^{\infty} F(\alpha) G(\alpha) d \alpha \tag{15}
\end{equation*}


Now suppose that $g=\bar{f}$ and, thus, $G=\bar{F}$, where the bar symbolizes the complex conjugate function. Then Equation (15) takes the form


\begin{equation*}
\int_{-\infty}^{\infty}|f(u)|^{2} d u=\int_{-\infty}^{\infty}|F(\alpha)|^{2} d \alpha \tag{16}
\end{equation*}


This is Parseval's theorem for Fourier integrals.

Furthermore, if $f$ and $g$ are even functions, it can be shown that Equation (15) reduces to the following Parseval identities:


\begin{equation*}
\int_{0}^{\infty} f(u) g(u) d u=\int_{0}^{\infty} F_{c}(\alpha) G_{c}(\alpha) d \alpha \tag{17}
\end{equation*}


where $F_{c}$ and $G_{c}$ are the Fourier cosine transforms of $f$ and $g$. If $f$ and $g$ are odd functions, then Equation (15) takes the form


\begin{equation*}
\int_{0}^{\infty} f(u) g(u) d u=\int_{0}^{\infty} F_{s}(\alpha) G_{s}(\alpha) d \alpha \tag{18}
\end{equation*}


where $F_{s}$ and $G_{s}$ are the Fourier sine transforms of $f$ and $g$. (See Problem 14.3.)

\section*{SOLVED PROBLEMS}
\section*{The Fourier integral and Fourier transforms}
14.1. (a) Find the Fourier transform of $f(x)=\left\{\begin{array}{ll}1 & |x|<a \\ 0 & |x|>a\end{array}\right.$. (b) Graph $f(\mathrm{x})$ and its Fourier transform for $a=3$.

(a) The Fourier transform of $f(\mathrm{x})$ is

$$
\begin{aligned}
F(\alpha) & =\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) e^{i \alpha u} d u=\frac{1}{\sqrt{2 \pi}} \int_{-a}^{a}(1) e^{i \alpha u} d u=\left.\frac{1}{2 \sqrt{\pi}} \frac{e^{i \alpha u}}{i \alpha}\right|_{-a} ^{a} \\
& =\frac{1}{\sqrt{2 \pi}}\left(\frac{e^{i \alpha a}-e^{-i \alpha a}}{i \alpha}\right)=\sqrt{\frac{2}{\pi}} \frac{\sin \alpha a}{\alpha}, \quad \alpha \neq 0
\end{aligned}
$$

For $\alpha=0$, we obtain $F(\alpha)=\sqrt{2 / \pi} a$.

(b) The graphs of $f(\mathrm{x})$ and $F(\alpha)$ for $a=3$ are shown in Figures 14.1, and 14.2, respectively.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-391(1)}
\end{center}

Figure 14.1

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-391}
\end{center}

Figure 14.2

14.2. (a) Use the result of Problem 14.1 to evaluate $\int_{-\infty}^{\infty} \frac{\sin \alpha a \cos \alpha x}{\alpha} d \alpha$. (b) Deduce the value of $\int_{0}^{\infty} \frac{\sin u}{u} d u$. (a) From Fourier's integral theorem, if

$$
F(\alpha)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) e^{i \alpha u} d u \quad \text { then } \quad f(x)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} F(\alpha) e^{-i \alpha x} d \alpha
$$

Then, from Problem 14.1,

\[
\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \sqrt{\frac{2}{\pi}} \frac{\sin \alpha a}{\alpha} e^{-i a x} d x= \begin{cases}1 & |x|<a  \tag{1}\\ 1 / 2 & |x|=a \\ 0 & |x|>a\end{cases}
\]

The left side of Equation (1) is equal to


\begin{equation*}
\frac{1}{\pi} \int_{-\infty}^{\infty} \frac{\sin \alpha a \cos \alpha x}{\alpha} d \alpha-\frac{i}{\pi} \int_{-\infty}^{\infty} \frac{\sin \alpha a \sin \alpha x}{\alpha} d \alpha \tag{2}
\end{equation*}


The integrand in the second integral of Equation (2) is odd, and so the integral is zero. Then from Equations (1) and (2), we have

\[
\int_{-\infty}^{\infty} \frac{\sin \alpha a \cos \alpha x}{\alpha} d \alpha= \begin{cases}\pi & |x|<a  \tag{3}\\ \pi / 2 & |x|=a \\ 0 & |x|>a\end{cases}
\]

Alternative solution: Since the function $f$ in Problem 14.1 is an even function, the result follows immediately from the Fourier cosine transform (9).\\
(b) If $x=0$ and $a=1$ in the result of (a), we have

$$
\int_{-\infty}^{\infty} \frac{\sin \alpha}{\alpha} d \alpha=\pi \quad \text { or } \quad \int_{0}^{\infty} \frac{\sin \alpha}{\alpha} d \alpha=\frac{\pi}{2}
$$

since the integrand is even.

14.3. If $f(x)$ is an even function, show that (a) $F(\alpha)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} f(u) \cos \alpha u d u$ and

(b) $f(x)=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} F(\alpha) \cos \alpha x d \alpha$

We have

$F(\alpha)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) e^{i a u} d u=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) \cos \alpha u d u+\frac{i}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) \sin \alpha u d u$

(a) If $f(u)$ is even, $f(u) \cos \lambda u$ is even and $f(u) \sin \lambda u$ is odd. Then the second integral on the right of Equation (1) is zero, and the result can be written

$$
F(\alpha)=\frac{2}{\sqrt{2 \pi}} \int_{0}^{\infty} f(u) \cos \alpha u d u=\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} f(u) \cos \alpha u d u
$$

(b) From (a), $F(-\alpha)=F(\alpha)$ so that $F(\alpha)$ is an even function. Then, by using a proof exactly analogous to that in (a), the required result follows.

A similar result holds for odd functions and can be obtained by replacing the cosine by the sine.

14.4. Solve the integral equation $\int_{0}^{\infty} f(x) \cos \alpha x d x=\left\{\begin{array}{cr}1-\alpha & 0 \leqq \alpha \leqq 1 \\ 0 & \alpha>1\end{array}\right.$.

Let $\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} f(x) \cos \alpha x d x=F(\alpha)$ and choose $F(\alpha)=\left\{\begin{array}{cc}\sqrt{2 / \pi}(1-\alpha) & 0<\alpha<1 \\ 0 & \alpha>1\end{array}\right.$ Then, by Problem 14.3,

$$
\begin{aligned}
f(x) & =\sqrt{\frac{2}{\pi}} \int_{0}^{\infty} F(\alpha) \cos \alpha x d \alpha=\sqrt{\frac{2}{\pi}} \int_{0}^{1} \sqrt{\frac{2}{\pi}}(1-\alpha) \cos \alpha x d \alpha \\
& =\frac{2}{\pi} \int_{0}^{1}(1-\alpha) \cos \alpha x d \alpha=\frac{2(1-\cos x)}{\pi x^{2}}
\end{aligned}
$$

14.5. Use Problem 14.4 to show that $\int_{0}^{\infty} \frac{\sin ^{2} u}{u^{2}} d u=\frac{\pi}{2}$.

As obtained in Problem 14.4,

$$
\frac{2}{\pi} \int_{0}^{\infty} \frac{1-\cos x}{x^{2}} \cos \alpha x d x=\left\{\begin{array}{rr}
1-\alpha & 0 \leqq \alpha \leqq 1 \\
0 & \alpha>1
\end{array}\right.
$$

Taking the limit as $\alpha \rightarrow 0+$, we find

$$
\int_{0}^{\infty} \frac{1-\cos x}{x^{2}} d x=\frac{\pi}{2}
$$

But this integral can be written as $\int_{0}^{\infty} \frac{2 \sin ^{2}(x / 2)}{x^{2}} d x$, which becomes $\int_{0}^{\infty} \frac{\sin ^{2} u}{u^{2}} d u$ on letting $x=2 u$, so\\
that the required result follows.

14.6. Show that $\int_{0}^{\infty} \frac{\cos \alpha x}{a^{2}+1} d \alpha=\frac{\pi}{2} e^{-x}, x \geqq 0$.

Let $f(x)=e^{-x}$ in the Fourier integral theorem

$$
f(x)=\frac{2}{\pi} \int_{0}^{\infty} \cos \alpha x d \alpha \int_{0}^{\infty} f(u) \cos \lambda u d u
$$

Then

$$
\frac{2}{\pi} \int_{0}^{\infty} \cos \alpha x d \alpha \int_{0}^{\infty} e^{-u} \cos \alpha u d u=e^{-x}
$$

But by Problem 12.22, we have $\int_{0}^{\infty} e^{-u} \cos \alpha u d u=\frac{1}{\alpha^{2}+1}$. Then

$$
\frac{2}{\pi} \int_{0}^{\infty} \frac{\cos \alpha x}{\alpha^{2}+1} d \alpha=e^{-x} \quad \text { or } \quad \int_{0}^{\infty} \frac{\cos \alpha x}{a^{2}+1} d \alpha=\frac{\pi}{2} e^{-x}
$$

\section*{Parseval's identity}
14.7. Verify Parseval's identity for Fourier integrals for the Fourier transforms of Problem 14.1.

We must show that

$$
\int_{-\infty}^{\infty}\{f(x)\}^{2} d x=\int_{-\infty}^{\infty}\{F(\alpha)\}^{2} d \alpha
$$

where

$$
f(x)=\left\{\begin{array}{ll}
1 & |x|<a \\
0 & |x|<a
\end{array} \text { and } F(\alpha)=\sqrt{\frac{2}{\pi}} \frac{\sin \alpha a}{\alpha}\right.
$$

This is equivalent to

$$
\int_{-a}^{a}(1)^{2} d x=\int_{-\infty}^{\infty} \frac{2}{\pi} \frac{\sin ^{2} \alpha a}{\alpha^{2}} d \alpha
$$

or

$$
\int_{-\infty}^{\infty} \frac{\sin ^{2} \alpha a}{\alpha^{2}} d \alpha=2 \int_{0}^{\infty} \frac{\sin ^{2} \alpha a}{a^{2}} d \alpha=\pi a
$$

i.e.,

$$
\int_{0}^{\infty} \frac{\sin ^{2} \alpha a}{\alpha^{2}} d \alpha=\frac{\pi a}{2}
$$

By letting $\alpha a=u$ and using Problem 14.5, it is seen that this is correct. The method can also be used to find $\int_{0}^{\infty} \frac{\sin ^{2} u}{u^{2}} d u$ directly.

\section*{Proof of the Fourier integral theorem}
14.8. Present a heuristic demonstration of Fourier's integral theorem by use of a limiting form of Fourier series.

Let


\begin{equation*}
f(x)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left(a_{n} \cos \frac{n \pi x}{L}+b_{n} \sin \frac{n \pi x}{L}\right) \tag{1}
\end{equation*}


where

$$
a_{n}=\frac{1}{L} \int_{-L}^{L} f(u) \cos \frac{n \pi u}{L} d u \text { and } b_{n}=\frac{1}{L} \int_{-L}^{L} f(u) \sin \frac{n \pi u}{L} d u
$$

Then, by substitution (see Problem 13.21),


\begin{equation*}
f(x)=\frac{1}{2 L} \int_{-L}^{L} f(u) d u+\frac{1}{L} \sum_{n=1}^{\infty} f(u) \cos \frac{n \pi}{L}(u-x) d u \tag{2}
\end{equation*}


If we assume that $\int_{-\infty}^{\infty}|f(u)| d u$ converges, the first term on the right of Equation (2) approaches zero as $L \rightarrow \infty$, while the remaining part appears to approach


\begin{equation*}
\lim _{L \rightarrow \infty} \frac{1}{L} \sum_{n=1}^{\infty} \int_{-\infty}^{\infty} f(u) \cos \frac{n \pi}{L}(u-x) d u \tag{3}
\end{equation*}


This last step is not rigorous and makes the demonstration heuristic.

Calling $\Delta \alpha=\pi / L$, Equation (3) can be written


\begin{equation*}
f(x)=\lim _{\Delta \alpha \rightarrow 0} \sum_{n=1}^{\infty} \Delta \alpha F(n \Delta \alpha) \tag{4}
\end{equation*}


where we have written


\begin{equation*}
F(\alpha)=\frac{1}{\pi} \int_{0}^{\infty} f(u) \cos \alpha(u-x) d u \tag{5}
\end{equation*}


But the limit (4) is equal to

$$
f(x)=\int_{0}^{\infty} F(\alpha) d \alpha=\frac{1}{\pi} \int_{0}^{\infty} d \alpha \int_{-\infty}^{\infty} f(u) \cos \alpha(u-x) d u
$$

which is Fourier's integral formula.

This demonstration serves only to provide a possible result. To be rigorous, we start with the integral

$$
1 / \pi \int_{0}^{\infty} d \alpha \int_{-\infty}^{\infty} f(u) \cos \alpha(u-x) d x
$$

and examine the convergence. This method is considered in Problems 14.9 through 14.12.

14.9. Prove that

(a) $\lim _{\alpha \rightarrow \infty} \int_{0}^{L} \frac{\sin \alpha v}{v} d v=\frac{\pi}{2}$,

(b) $\lim _{\alpha \rightarrow \infty} \int_{-L}^{0} \frac{\sin \alpha v}{v} d v=\frac{\pi}{2}$.

(a) Let $\alpha v=y$. Then $\lim _{\alpha \rightarrow \infty} \int_{0}^{L} \frac{\sin \alpha v}{v} d v=\lim _{\alpha \rightarrow \infty} \int_{0}^{\alpha L} \frac{\sin y}{y} d y=\int_{0}^{\infty} \frac{\sin y}{y} d y=\frac{\pi}{2}$. by Problem 12.29.

(b) Let $\alpha v=-y$. Then $\lim _{\alpha \rightarrow \infty} \int_{-L}^{0} \frac{\sin \alpha v}{v} d v=\lim _{x \rightarrow \infty} \int_{0}^{\alpha L} \frac{\sin y}{y} d y=\frac{\pi}{2}$.

14.10. Riemann's theorem states that if $F(\mathrm{x})$ is piecewise continuous in $(a, b)$, then

$$
\lim _{\alpha \rightarrow \infty} \int_{a}^{b} F(x) \sin \alpha x d x=0
$$

with a similar result for the cosine (see Problem 14.32). Use this to prove that

(a) $\lim _{\alpha \rightarrow \infty} \int_{0}^{L} f(x+v) \frac{\sin \alpha v}{v} d v=\frac{\pi}{2} f(x+0)$

(b) $\lim _{\alpha \rightarrow \infty} \int_{-L}^{0} f(x+v) \frac{\sin \alpha v}{v} d v=\frac{\pi}{2} f(x-0)$

where $f(x)$ and $f^{\prime}(x)$ are assumed piecewise continuous in $(0, L)$ and $(-L, 0)$, respectively.

(a) Using Problem 14.9(a), it is seen that a proof of the given result amounts to proving that

$$
\lim _{\alpha \rightarrow \infty} \int_{0}^{L}\{f(x+v)-f(x+0)\} \frac{\sin \alpha v}{v} d v=0
$$

This follows at once from Riemann's theorem, because $F(v)=\frac{f(x+v)-f(x+0)}{v}$ is piecewise continuous in $(0, L)$, since $\lim _{n \rightarrow 0+} F(v)$ exists and $f(x)$ is piecewise continuous.

(b) A proof of this is analogous to that in (a) if we make use of Problem 14.9(b).

14.11. If $f(\mathrm{x})$ satisfies the additional condition that $\int_{-\infty}^{\infty}|f(x)| d x$ converges, prove that

(a) $\lim _{\alpha \rightarrow \infty} \int_{0}^{\infty} f(x+v) \frac{\sin \alpha v}{v} d v=\frac{\pi}{2} f(x+0)\left(\right.$ b) $\lim _{\alpha \rightarrow \infty} \int_{-\infty}^{0} f(x+v) \frac{\sin \alpha v}{v} d v=\frac{\pi}{2} f(x-0)$

We have


\begin{align*}
& \int_{0}^{\infty} f(x+v) \frac{\sin \alpha v}{v} d v=\int_{0}^{L} f(x+v) \frac{\sin \alpha v}{v} d v+\int_{L}^{\infty} f(x+v) \frac{\sin \alpha v}{v} d v  \tag{1}\\
& \int_{0}^{\infty} f(x+0) \frac{\sin \alpha v}{v} d v=\int_{0}^{L} f(x+0) \frac{\sin \alpha v}{v} d v+\int_{L}^{\infty} f(x+0) \frac{\sin \alpha v}{v} d v \tag{2}
\end{align*}


Subtracting,


\begin{align*}
\int_{0}^{\infty}\{f(x+v)-f(x+0)\} \frac{\sin \alpha v}{v} d v= & \int_{0}^{L}\{f(x+v)-f(x+0)\} \frac{\sin \alpha v}{v} d v \\
& +\int_{L}^{\infty} f(x+v) \frac{\sin \alpha v}{v} d v-\int_{L}^{\infty} f(x+0) \frac{\sin \alpha v}{v} d v \tag{3}
\end{align*}


Denoting the integrals in Equation (3) by $I, I_{1}, I_{2}$, and $I_{3}$, respectively, we have $I=I_{1}+I_{2}+I_{3}$ so that


\begin{equation*}
|I| \leqq\left|I_{1}\right|+\left|I_{2}\right|+\left|I_{3}\right| \tag{4}
\end{equation*}


Now

$$
\left|I_{2}\right| \leqq \int_{L}^{\infty}\left|f(x+v) \frac{\sin \alpha v}{v}\right| d v \leqq \frac{1}{L} \int_{L}^{\infty}|f(x+v)| d v
$$

Also,

$$
\left|I_{3}\right| \leqq|f(x+0)|\left|\int_{L}^{\infty} \frac{\sin \alpha v}{v} d v\right|
$$

Since both $\left.\int_{0}^{\infty} \mid f(x)\right\} d x$ and $\int_{0}^{\infty} \frac{\sin \alpha v}{v} d v$ converge, we can choose $L$ so large that $\left|I_{2}\right| \leqq \epsilon / 3,\left|I_{3}\right|$ $\leqq \epsilon / 3$. Also, we can choose $\alpha$ so large that $\left|I_{1}\right| \leqq \epsilon / 3$. Then from Equation (4) we have $|I|<\epsilon$ for $\alpha$ and $\bar{L}$ sufficiently large so that the required result follows.

This result follows by reasoning exactly analogous to that in (a).

14.12. Prove Fourier's integral formula where $f(\mathrm{x})$ satisfies the conditions stated on Page 377.

We must prove that $\lim _{L \rightarrow \infty} \frac{1}{\pi} \int_{\alpha=0}^{L} \int_{u=-\infty}^{\infty} f(u) \cos \alpha(x-u) d u d \alpha=\frac{f(x+0)+f(x-0)}{2}$.

Since $\left|\int_{-\infty}^{\infty} f(u) \cos \alpha(x-u) d u\right| \leqq \int_{-\infty}^{\infty}|f(u)| d u$, which converges, it follows by the Weierstrass test that $\int_{-\infty}^{\infty} f(u) \cos \alpha(x-u) d u$ converges absolutely and uniformly for all $\alpha$. Thus, we can reverse the order of integration to obtain

$$
\begin{aligned}
\frac{1}{\pi} \int_{\alpha=0}^{L} d \alpha \int_{u=-\infty}^{\infty} f(u) \cos \alpha(x-u) d u & =\frac{1}{\pi} \int_{u=-\infty}^{\infty} f(u) d u \int_{\alpha=0}^{L} \cos \alpha(x-u) d \alpha \\
& =\frac{1}{\pi} \int_{u=-\infty}^{\infty} f(u) \frac{\sin L(u-x)}{u-x} d u \\
& =\frac{1}{\pi} \int_{u=-\infty}^{\infty} f(x+v) \frac{\sin L v}{v} d v \\
& =\frac{1}{\pi} \int_{-\infty}^{0} f(x+v) \frac{\sin L v}{v} d v+\frac{1}{\pi} \int_{0}^{\infty} f(x+v) \frac{\sin L v}{v} d v
\end{aligned}
$$

where we have let $u=x+v$.\\
Letting $L \rightarrow \infty$, we see by Problem 14.11 that the given integral converges to $\frac{f(x)+0)+f(x-0)}{2}$ as\\
required. required.

\section*{Miscellaneous problems}
14.13. Solve $\frac{\partial U}{\partial t}=\frac{\partial^{2} U}{\partial x^{2}}$, subject to the conditions $U(0, t)=0, U(x, 0)=\left\{\begin{array}{rr}1 & 0<x<1 \\ 0 & x \geqq 1\end{array}, U(x, 1)\right.$ is bounded where $x>0, t>0$.

We proceed as in Problem 13.24. A solution satisfying the partial differential equation and the first boundary condition is given by $B e^{-\lambda 2} t \sin \lambda x$. Unlike Problem 13.24, the boundary conditions do not prescribe the specific values for $\lambda$, so we must assume that all values of $\lambda$ are possible. By analogy with that problem, we sum over all possible values of $\lambda$, which corresponds to an integration in this case, and are led to the possible solution


\begin{equation*}
U(x, 1)=\int_{0}^{\infty} B(\lambda) e^{-\lambda^{2} t} \sin \lambda x d \lambda \tag{1}
\end{equation*}


where $B(\lambda)$ is undetermined. By the second condition, we have

\[
\int_{0}^{\infty} B(\lambda) \sin \lambda x d \lambda=\left\{\begin{array}{rr}
1 & 0<x<1  \tag{2}\\
0 & x \geqq 1
\end{array}=f(x)\right.
\]

from which we have, by Fourier's integral formula,


\begin{equation*}
B(\lambda)=\frac{2}{\pi} \int_{0}^{\infty} f(x) \sin \lambda x d x=\frac{2}{\pi} \int_{0}^{1} \sin \lambda x d x=\frac{2(1-\cos \lambda)}{\pi \lambda} \tag{3}
\end{equation*}


so that, at least formally, the solution is given by


\begin{equation*}
U(x, 1)=\frac{2}{\pi} \int_{0}^{\infty}\left(\frac{1-\cos \lambda}{\lambda}\right) e^{-\lambda^{2} t} \sin \lambda x d x \tag{4}
\end{equation*}


See Problem 14.26.

14.14. Show that $e^{-x 2} / 2$ is its own Fourier transform.

Since $e^{-x 2} / 2$ is even, its Fourier transform is given by $\sqrt{2 / \pi}=\int_{0}^{\infty} e^{-x^{2} / 2} \cos x \alpha d x$.

Letting $x=\sqrt{2 u}$ and using Problem 12.32, the integral becomes

$$
\frac{2}{\sqrt{\pi}} \int_{0}^{\infty} e^{-u^{2}} \cos (\alpha \sqrt{2} u) d u=\frac{2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} e^{-\alpha^{2} / 2}
$$

which proves the required result.

14.15. Solve the integral equation

$$
y(x)=g(x)+\int_{-\infty}^{\infty} y(u) r(x-u) d u
$$

where $g(x)$ and $r(x)$ are given.

Suppose that the Fourier transforms of $y(x), g(x)$, and $r(x)$ exist, and denote them by $Y(\alpha), G(\alpha)$, and $R(\alpha)$, respectively. Then, taking the Fourier transform of both sides of the given integral equation, we have, by the convolution theorem,

$$
Y(\alpha)=G(\alpha)+\sqrt{2 \pi} Y(\alpha) R(\alpha) \quad \text { or } \quad Y(\alpha)=\frac{G(\alpha)}{1-\sqrt{2 \pi} R(\alpha)}
$$

Then

$$
y(x)=\mathscr{F}^{-1}\left\{\frac{G(\alpha)}{1-\sqrt{2 \pi} R(\alpha)}\right\}=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \frac{G(\alpha)}{1-\sqrt{2 \pi} R(\alpha)} e^{-i \alpha x} d \alpha
$$

assuming this integral exists.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{The Fourier integral and Fourier transforms}
14.16. (a) Find the Fourier transform of $f(x)=\left\{\begin{array}{ll}1 / 2 \in & |x| \leqq \epsilon \\ 0 & |x|>\epsilon\end{array}\right.$. (b) Determine the limit of this transform as\\
Ans. (a) $\frac{1}{\sqrt{2 \pi}} \frac{\sin \alpha \in}{\alpha \in}$\\
(b) $\frac{1}{\sqrt{2 \pi}}$

14.17. (a) Find the Fourier transform of $f(x)=\left\{\begin{array}{cc}1-x^{2} & |x|<1 \\ 0 & |x|>1\end{array}\right.$. (b) Evaluate $\int_{0}^{\infty}\left(\frac{x \cos x-\sin x}{x^{3}}\right) \cos \frac{x}{2} d x$.

$$
\text { Ans. (a) } 2 \sqrt{\frac{2}{\pi}}\left(\frac{\alpha \cos \alpha-\sin \alpha}{\alpha^{3}}\right) \text { (b) } \frac{3 \pi}{16}
$$

14.18. If $f(x)=\left\{\begin{array}{rr}1 & 0<x \leqq 1 \\ 0 & x \geqq 1\end{array}\right.$, find (a) the Fourier since transform and (b) the Fourier cosine transform of $f(x)$. In each case, obtain the graph of $f(\mathrm{x})$ and its transform.

$$
\text { Ans. (a) } \sqrt{\frac{2}{\pi}}\left(\frac{1-\cos \alpha}{\alpha}\right) \text { (b) } \sqrt{\frac{2}{\pi}} \frac{\sin \alpha}{\alpha}
$$

14.19. (a) Find the Fourier sine transform of $e^{-x}, x \geq 0$. (b) Show that $\int_{0}^{\infty} \frac{x \sin m x}{x^{2}+1} d x=\frac{\pi}{2} e^{-m}, m>0$ by using the result in (a). (c) Explain from the viewpoint of Fourier's integral theorem why the result in (b) does not hold for $\mathrm{m}=0$.

$$
\text { Ans. (a) } \sqrt{2 / \pi}\left[\alpha /\left(1+\alpha^{2}\right)\right]
$$

14.20. Solve for $Y(\mathrm{x})$ the integral equation

$$
\int_{0}^{\infty} Y(x) \sin x t d x= \begin{cases}1 & 0 \leqq t<1 \\ 2 & 1 \leqq t<2 \\ 0 & t \geqq 0\end{cases}
$$

and verify the solution by direction substitution.

Ans. $Y(x)=(2+2 \cos x-4 \cos 2 x) / \pi x$

\section*{Parseval's identity}
14.21. Evaluate (a) $\int_{0}^{\infty} \frac{d x}{\left(x^{2}+1\right)^{2}}$ and (b) $\int_{0}^{\infty} \frac{x^{2} d x}{\left(x^{2}+1\right)^{2}}$ by use of Parseval's identity. (Hint: Use the Fourier sine and cosine transforms of $e^{-x}, x>0$.)

Ans. (a) $\pi / 4$ (b) $\pi / 4$

14.22. Use Problem 14.18 to show that (a) $\int_{0}^{\infty}\left(\frac{1-\cos x}{x}\right)^{2} d x=\frac{\pi}{2}$ and (b) $\int_{0}^{\infty} \frac{\sin ^{4} x}{x^{2}} d x=\frac{\pi}{2}$.

14.23. Show that $\int_{0}^{\infty} \frac{(x \cos x-\sin x)^{2}}{x^{6}} d x=\frac{\pi}{15}$.

\section*{Miscellaneous problems}
14.24. (a) Solve $\frac{\partial U}{\partial t}=2 \frac{\partial^{2} U}{\partial x^{2}}, U(0, t)=0, U(x, 0)=e^{-x}, x>0, U(x, t)$ is bounded where $x>0, t>0$.

(b) Give a physical interpretation.

Ans. $U(x, t)=\frac{2}{\pi} \int_{0}^{\infty} \frac{\lambda e^{-2 \lambda^{2} t} \sin \lambda x}{\lambda^{2}+1} d \lambda$

14.25. Solve $\frac{\partial U}{\partial t}=\frac{\partial^{2} U}{\partial x^{2}}, U_{x}(0, t)=0, U(x, 0)=\left\{\begin{array}{ll}x & 0 \leqq x \leqq 1 \\ 0 & x>1\end{array}, U(x, t)\right.$ is bounded where $x>0, t>0$.

$$
\text { Ans. } U(x, t)=\frac{2}{\pi} \int_{0}^{\infty}\left(\frac{\sin \lambda}{\lambda}+\frac{\cos \lambda-1}{\lambda^{2}}\right) e^{-\lambda^{2} t} \cos \lambda x d \lambda
$$

14.26. (a) Show that the solution to Problem 14.13 can be written

$$
U(x, t)=\frac{2}{\sqrt{\pi}} \int_{0}^{x / 2 \sqrt{t}} e^{-v^{2}} d v-\frac{1}{\sqrt{\pi}} \int_{(1-x) / 2 \sqrt{t}}^{(1+x) / 2 \sqrt{2}} e^{-v^{2}} d v
$$

(b) Prove directly that the function in (a) satisfies $\frac{\partial U}{\partial t}=\frac{\partial^{2} U}{\partial x^{2}}$ and the conditions of Problem 14.13.

14.27. Verify the convolution theorem for the functions $f(x)=g(x)=\left\{\begin{array}{ll}1 & |x|<1 \\ 0 & |x|>1\end{array}\right.$.

14.28. Establish Equation (4), Page 378, form Equation (3), Page 378.

14.29. Prove the result (12), Page 379. [Hint: If $F(\alpha)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(u) e^{i \alpha u} d u$ and $G(\alpha)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} g(v) e^{i \alpha v} d v$, then $F(\alpha) G(\alpha)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{i \alpha(u+v)} f(u) g(v) d u d v$. Now make the transformation $u+v=x$.]

$$
F(\alpha) G(\alpha)=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{i \alpha x} f(u) g(x-u) d u d x
$$

Define

$$
f * g=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} f(u) g(x-u) d u \quad(f * g \quad \text { is a function of } x)
$$

then

$$
F(\alpha) G(\alpha)=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{i \alpha x} f * g d x
$$

Thus, $F(\alpha) G(\alpha)$ is the Fourier transform of the convolution $f * g$ and, conversely, as indicated in Equation (13), $f * g$ is the Fourier transform of $F(\alpha) G(\alpha)$.

14.30. If $F(\alpha)$ and $G(\alpha)$ are the Fourier transforms of $f(\mathrm{x})$ and $g(\mathrm{x})$, respectively, prove (by repeating the pattern of Problem 14.29) that

$$
\int_{-\infty}^{\infty} F(\alpha) \overline{G(\alpha)} d \alpha=\int_{-\infty}^{\infty} f(x) \overline{g(x)} d x
$$

where the bar signifies the complex conjugate. Observe that if $G$ is expressed as in Problem 14.29, then

$$
\bar{G}(\alpha)=\frac{1}{\pi} \int_{-\infty}^{\infty} e^{-i \alpha x} f(u) \bar{g}(v) d v
$$

14.31. Show that the Fourier transform of $g(u-x)$ is $e^{i \alpha x}$; i.e., $e^{i \alpha x} G(\alpha)=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} e^{i \alpha u} f(u) g(u-x) d u$. (Hint:\\
See Problem 14.29. Let $v=u-x$.)

14.32. Prove Riemann's theorem (see Problem 14.10).

\section*{CHAPTER 15}
\section*{Gamma and Beta Functions}
\section*{The Gamma Function}
The gamma function may be regarded as a generalization of $n$ ! ( $n$-factorial), where $n$ is any positive integer to $x$ !, where $x$ is any real number. (With limited exceptions, the discussion that follows will be restricted to positive real numbers.) Such an extension does not seem reasonable, yet, in certain ways, the gamma function defined by the improper integral


\begin{equation*}
\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} d t \tag{1}
\end{equation*}


meets the challenge. This integral has proved valuable in applications. However, because it cannot be represented through elementary functions, establishment of its properties takes some effort. Some of the important properties are outlined as follows.

The gamma function is convergent for $x>0$. (See Problem 12.18.)

The fundamental property


\begin{equation*}
\Gamma(x+1)=x \Gamma(\mathrm{x}) \tag{2}
\end{equation*}


may be obtained by employing the technique of integration by parts to Equation (1). The process is carried out in Problem 15.1. From the form of Equation (2), the function $\Gamma(x)$ can be evaluated for all $x>0$ when its values in the interval $1 \leqq x<2$ are known. (Any other interval of unit length will suffice.) Table 15.1 and the graph in Figure 15.1 illustrate this idea.

\section*{Tables of Values and Graph of the Gamma Function}
TABLE 15.1

\begin{center}
\begin{tabular}{ll}
\hline
$N$ & $\Gamma(N)$ \\
\hline
1.00 & 1.0000 \\
1.10 & 0.9514 \\
1.20 & 0.9182 \\
1.30 & 0.8975 \\
1.40 & 0.8873 \\
1.50 & 0.8862 \\
1.60 & 0.8935 \\
1.70 & 0.9086 \\
1.80 & 0.9314 \\
1.90 & 0.9618 \\
2.00 & 1.0000 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-400}
\end{center}

Figure 15.1

Equation (2) is a recurrence relationship that leads to the factorial concept. First observe that if $x=1$, then Equation (1) can be evaluated and in particular,

$$
\Gamma(1)=1
$$

From Equation (2)

$$
\Gamma(x+1)=x \Gamma(x)=x(x-1) \Gamma(x-1)=\ldots x(x-1)(x-2) \ldots(x-k) \Gamma(x-k)
$$

If $x=n$, where $n$ is a positive integer, then


\begin{equation*}
\Gamma(n+1)=n(n-1)(n-2) \ldots 1=n ! \tag{3}
\end{equation*}


If $x$ is a real number, then $x !=\Gamma(x+1)$ is defined by $\Gamma(x+1)$. The value of this identification is in intuitive guidance.

If the recurrence relation (2) is characterized as a differential equation, then the definition of $\Gamma(x)$ can be extended to negative real numbers by a process called analytic continuation. The key idea is that even though $\Gamma(x)$ as defined in Equation (1) is not convergent for $x<0$, the relation $\Gamma(x)=\frac{1}{x} \Gamma(x+1)$ allows the meaning to be extended to the interval $-1<x<0$, and from there to $-2<x<-1$, and so on. A general development of this concept is beyond the scope of this presentation; however, some information is presented in Problem 15.7 .

The factorial notion guides us to information about $\Gamma(x+1)$ in more than one way. In the eighteenth century, James Stirling introduced the formula (for positive integer values $n$ )


\begin{equation*}
\lim _{n \rightarrow \infty} \frac{\sqrt{2 \pi} n^{n+1} e^{-n}}{n !}=1 \tag{4}
\end{equation*}


This is called Stirling's formula, and it indicates that $n$ ! asymptotically approaches $\sqrt{2 \pi} n^{n+1} e^{-n}$ for large values of $n$. This information has proved useful, since $n$ ! is difficult to calculate for large values of $n$.

There is another consequence of Stirling's formula. It suggests the possibility that for sufficiently large values of $x$,


\begin{equation*}
x !=\Gamma(x+1) \approx \sqrt{2 \pi} x^{x+1} e^{-x} \tag{5a}
\end{equation*}


(An argument supporting this is made in Problem 15.20.)

It is known that $\Gamma(x+1)$ satisfies the inequality


\begin{equation*}
\sqrt{2 \pi} x^{x+1} e^{-x}<\Gamma(x+1)<\sqrt{2 \pi} x^{x+1} e^{-x} \frac{1}{e^{12(x+1)}} \tag{5b}
\end{equation*}


Since the factor $\frac{1}{e^{12(x+1)}} \rightarrow 0$ for large values of $x$, the suggested value (5a) of $\Gamma(x+1)$ is consistent with (5b).

An exact representation of $\Gamma(x+1)$ is suggested by the following manipulation of $n$ !. [It depends on $(n+k)$ ! $=(k+n) !$.

$$
n !=\lim _{k \rightarrow \infty} \frac{12 \ldots n(n+1)+(n+2) \ldots(n+k)}{(n+1)(n+2) \ldots(n+k)}=\lim _{k \rightarrow \infty} \frac{k ! k^{n}}{(n+1) \ldots(n+k)} \lim _{k \rightarrow \infty} \frac{(k+1)(k+2) \ldots(k+n)}{k^{n}}
$$

Since $n$ is fixed, the second limit is one; therefore, $n !=\lim _{k \rightarrow \infty} \frac{k ! k^{n}}{(n+1) \ldots(n+k)}$. (This must be read as an in-\\
nite product.)

This factorial representation for positive integers suggests the possibility that


\begin{equation*}
\Gamma(x+1)=x !=\lim _{k \rightarrow \infty} \frac{k ! k^{x}}{(x+1) \ldots(x+k)} \quad x \neq-1,-2,-k \tag{6}
\end{equation*}


Carl Friedrich Gauss verified this identification back in the nineteenth century. This infinite product is symbolized by $\Pi(x, k)$; i.e., $\Pi(x, k)=\frac{k ! k^{x}}{(x+1) \cdots(x+k)}$. It is called Gauss's function,\\
and through this symbolism,


\begin{equation*}
\Gamma(x+1)=\lim _{k \rightarrow \infty} \Pi(x, k) \tag{7}
\end{equation*}


The expression for $\frac{1}{\Gamma(x)}$ [which has some advantage in developing the derivative of $\Gamma(x)$ ] results as follows. Put Equation (6) in the form

$$
\lim _{k \rightarrow \infty} \frac{k^{x}}{(1+x)(1+x / 2) \ldots(1+x / k)} \quad x \neq-\frac{1}{2}, \frac{1}{3}, \ldots, \frac{1}{k}
$$

Next, introduce

$$
\gamma_{k}=1+\frac{1}{2}+\frac{1}{3}+\cdots \frac{1}{k}-\ln k
$$

Then

$$
\gamma=\lim _{k \rightarrow \infty} \gamma_{k}
$$

is Euler's constant. This constant has been calculated to many places, a few of which are $\gamma \approx 0.57721566 \ldots$

By letting $k^{x}=e^{x \ln k}=e^{x\left[-\gamma_{k}+1+1 / 2+\cdots+1 / k\right]}$, the representation (6) can be further modified so that


\begin{align*}
\Gamma(x+1) & =e^{-\gamma x} \lim _{k \rightarrow \infty} \frac{e^{x}}{1+x} \frac{e^{x / 2}}{1+x / 2} \cdots \frac{e^{x / k}}{1+x / k}=e^{-\gamma x} \prod_{k=1}^{\infty} e^{\gamma x} e^{x \ln k} /\left(1+\frac{x}{k}\right)  \tag{8}\\
& =\prod_{k=1}^{\infty} k^{x} k !(k+x)=\lim _{k \rightarrow \infty} \frac{1 \cdot 2 \cdot 3 \cdots k}{(x+1)(x+2) \cdots(x+k)} x^{x}=\lim _{k \rightarrow \infty} \prod(x, k)
\end{align*}


Since $\Gamma(x+1)=x \Gamma(x)$,


\begin{equation*}
\frac{1}{\Gamma(x)}=x e^{\gamma x} \lim _{k \rightarrow \infty} \frac{1+x}{e^{x}} \frac{1+x / 2}{e^{x / 2}} \cdots \frac{1+x / k}{e^{x / k}}=x e^{\gamma x} \prod_{k=1}^{\infty}(1+x / k) e^{-x / k} \tag{9}
\end{equation*}


Another result of special interest emanates from a comparison of $\Gamma(x) \Gamma(1-x)$ with the well-known formula


\begin{equation*}
\frac{\pi x}{\sin \pi x}=\lim _{k \rightarrow \infty}\left\{\frac{1}{1-x^{2}} \frac{1}{1-(x / 2)^{2}} \cdots \frac{1}{\left(1-x / k^{2}\right.}\right\}=\prod_{k=1}^{\infty}\left\{1-\left(x / k^{2}\right\}\right. \tag{10}
\end{equation*}


[See Differential and Integral Calculus, by R. Courant (translated by E. J. McShane), Blackie \& Son Limited.]

$\Gamma(1-x)$ is obtained from $\Gamma(y)=\frac{1}{y} \Gamma(y+1)$ by letting $y=-x$; i.e.,

$$
\Gamma(-x)=-\frac{1}{x} \Gamma(1-x) \quad \text { or } \quad \Gamma(1-x)=-x \Gamma(-x)
$$

Now use Equation (8) to produce

$$
\Gamma(x) \Gamma(1-x)=\left(\left\{x^{-1} e^{-\gamma x} \lim _{k \rightarrow 1} \prod_{\kappa=\infty}^{\infty}(1+/ k)^{-1} e^{x / k}\right\}\right)\left(e^{\gamma x} \lim _{\kappa=1}(1-x / k)^{-1} e^{-x / k}\right)=\frac{1}{x} \lim _{k \rightarrow \infty} \prod_{\kappa=1}^{\infty}\left(1(x / k)^{2}\right)
$$

Thus,


\begin{equation*}
\Gamma(x) \Gamma(1-x)=\frac{\pi}{\sin \pi}, \quad 0<x<1 \tag{11a}
\end{equation*}


Observe that Equation (11a) yields the result


\begin{equation*}
\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi} \tag{11b}
\end{equation*}


Another exact representation of $\Gamma(x+1)$ is


\begin{equation*}
\Gamma(x+1)=\sqrt{2 \pi} x^{x+1} e^{-x}\left\{1+\frac{1}{12 x}+\frac{1}{288 x^{2}}+\frac{139}{51840 x^{3}}+\cdots\right\} \tag{12}
\end{equation*}


The method of obtaining this result is closely related to Stirling's asymptotic series for the gamma function. (See Problems 15.20 and 15.74.)

The duplication formula


\begin{equation*}
2^{2 x-1} \Gamma(x) \Gamma\left(x+\frac{1}{2}\right)=\sqrt{\pi} \Gamma(2 x) \tag{13a}
\end{equation*}


is also part of the literature. Its proof is given in Problem 15.24.

The duplication formula is a special case $(m=2)$ of the following product formula:


\begin{equation*}
\Gamma(x) \Gamma\left(x+\frac{1}{m}\right) \cdots \Gamma\left(x+\frac{2}{m}\right) \cdots \Gamma\left(X+\frac{m-1}{m}\right)=m^{\frac{1}{2}-m x}(2 \pi)^{\frac{m-1}{2}} \Gamma(m x) \tag{13b}
\end{equation*}


It can be shown that the gamma function has continuous derivatives of all orders. They are obtained by differentiating (with respect to the parameter) under the integral sign.

It helps to recall that $\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-y t} d t$ and that if $\mathrm{y}=\mathrm{t}^{\mathrm{x}-1}$, then $\ln y=\ln t^{\mathrm{x}-1}=(x-1) \ln t$.

Therefore; $\frac{1}{y} y^{\prime}=\ln t$.

It follows that


\begin{equation*}
\Gamma^{\prime}(x)=\int_{0}^{\infty} t^{x-1} e^{-t} \ln t d t \tag{14a}
\end{equation*}


This result can be obtained (after making assumptions about the interchange of differentiation with limits) by taking the logarithm of both sides of Equation (9) and then differentiating.

In particular,


\begin{equation*}
\Gamma^{\prime}(1)=-\gamma(\gamma \text { is Euler's constant. }) \tag{14b}
\end{equation*}


It also may be shown that


\begin{equation*}
\frac{\Gamma^{\prime}(x)}{\Gamma(x)}=-\gamma+\left(\frac{1}{1}-\frac{1}{x}\right)+\left(\frac{1}{2}-\frac{1}{x+1}\right)+\cdots\left(\frac{1}{n}-\frac{1}{x+n-1}\right) \tag{15}
\end{equation*}


(See Problem 15.73 for further information.)

\section*{The Beta Function}
The beta function is a two-parameter composition of gamma functions that has been useful enough in application to gain its own name. Its definition is


\begin{equation*}
B(x, y)=\int_{0}^{1} t^{x-1}(1-t)^{y-1} d t \tag{16}
\end{equation*}


If $x \geq 1$ and $y \geq 1$, this is a proper integral. If $x>0, y>0$, and either or both $x<1$ or $y<1$, the integral is improper but convergent.

It is shown in Problem 15.11 that the beta function can be expressed through gamma functions in the following way


\begin{equation*}
B(x, y)=\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)} \tag{17}
\end{equation*}


Many integrals can be expressed through beta and gamma functions. Two of special interest are


\begin{gather*}
\int_{0}^{\pi / 2} \sin ^{2 x-1} \theta \cos ^{2 y-1} \theta d \theta=\frac{1}{2} B(x, y)=\frac{1}{2} \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}  \tag{18}\\
\int_{0}^{\infty} \frac{x^{p-1}}{1+x} d x=\Gamma(p) \Gamma(p-1)=\frac{\pi}{\sin \pi P} \quad 0<p<1 \tag{19}
\end{gather*}


See Problem 15.17. Also see Page 391, where a classical reference is given. Finally, see Problem 16.38, where an elegant complex variable resolution of the integral is presented.

\section*{Dirichlet Integrals}
 If $V$ denotes the closed region in the first octant bounded by the surface $\left(\frac{x}{a}\right)^{p}+\left(\frac{y}{a}\right)^{q}+\left(\frac{z}{c}\right)^{r}=1$ and thecoordinate planes, then if all the constants are positive,


\begin{equation*}
\iiint_{V} x^{\alpha-1} y^{\beta-1} z^{\gamma-1} d x d y d z=\frac{a^{\alpha} b^{\beta} c^{\gamma}}{p q r} \frac{\Gamma\left(\frac{\alpha}{p}\right) \Gamma\left(\frac{\beta}{b}\right) \Gamma\left(\frac{\gamma}{r}\right)}{\Gamma\left(1+\frac{\alpha}{p}+\frac{\beta}{q}+\frac{\gamma}{r}\right)} \tag{20}
\end{equation*}


Integrals of this type are called Dirichlet integrals and are often useful in evaluating multiple integrals (see Problem 15.21).

\section*{SOLVED PROBLEMS}
\section*{The gamma function}
15.1. $\quad$ Prove (a) $\Gamma(x+1)=x \Gamma(\mathrm{x}), x>0$ and (b) $\Gamma(n+1)=n !, n=1,2,3, \ldots$

$$
\begin{aligned}
\Gamma(v+1) & =\int_{0}^{\infty} x^{v} e^{-x} d x=\lim _{M \rightarrow \infty} \int_{0}^{M} x^{v} e^{-x} d x \\
& =\lim _{M \rightarrow \infty}\left\{\left.\left(x^{v}\right)\left(-e^{-x}\right)\right|_{0} ^{M}-\int_{0}^{M}\left(-e^{-x}\right)\left(v x^{v-1}\right) d x\right\} \\
& =\lim _{M \rightarrow \infty}\left\{-M^{v} e^{-M}+v \int_{0}^{M} x^{v-1} e^{-x} d x\right\}=v \Gamma(v) \quad \text { if } v>0
\end{aligned}
$$

(a)

(b) $\Gamma(1)=\int_{0}^{\infty} e^{-x} d x=\lim _{M \rightarrow \infty} \int_{0}^{M} e^{-x} d x=\lim _{M \rightarrow \infty}\left(1-e^{-M}\right)=1$.

Put $n=1,2,3, \ldots$ in $\Gamma(n+1)=n \Gamma(n)$. Then

$$
\Gamma(2)=1 \Gamma(1)=1, \Gamma(3)=2 \Gamma(2)=2 \cdot 1=2 ! \Gamma(4)=3 \Gamma(3)=3 \cdot 2 !=3 !
$$

In general, $\Gamma(n+1)=n$ ! if $n$ is a positive integer.

15.2. Evaluate each of the following:

(a) $\frac{\Gamma(6)}{2 \Gamma(3)}=\frac{5 !}{2 \cdot 2 !}=\frac{5 \cdot 4 \cdot 3 \cdot 2}{2 \cdot 2}=30$

(b) $\frac{\Gamma\left(\frac{5}{2}\right)}{\Gamma\left(\frac{1}{2}\right)}=\frac{\frac{3}{2} \Gamma\left(\frac{3}{2}\right)}{\Gamma\left(\frac{1}{2}\right)}=\frac{\frac{3}{2} \cdot \frac{1}{2} \Gamma\left(\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)}=\frac{3}{4}$

(c) $\frac{\Gamma(3) \Gamma(2.5)}{\Gamma(5.5)}=\frac{2 !(1.5)(0.5) \Gamma(0.5)}{(4.5)(3.5)(2.5)(1.5)(0.5) \Gamma(0.5)}=\frac{16}{315}$

(d) $\frac{6 \Gamma\left(\frac{8}{3}\right)}{5 \Gamma\left(\frac{2}{3}\right)}=\frac{6\left(\frac{5}{3}\right)\left(\frac{2}{3}\right) \Gamma\left(\frac{2}{3}\right)}{5 \Gamma\left(\frac{2}{3}\right)}=\frac{4}{3}$

15.3. Evaluate each integral.

(a) $\int_{0}^{\infty} x^{3} e^{-x} d x=\Gamma(4)=3 !=6$

(b) $\int_{0}^{\infty} x^{6} e^{-2 x} d x$

Let $2 x=7$. Then the integral becomes

$$
\int_{0}^{\infty}\left(\frac{y}{2}\right)^{6} e^{-y} \frac{d y}{2}=\frac{1}{2^{7}} \int_{0}^{\infty} y^{6} e^{-y} d y=\frac{\Gamma(7)}{2^{7}}=\frac{6 !}{2^{7}}=\frac{45}{8}
$$

15.4. Prove that $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$.

$$
\Gamma\left(\frac{1}{2}\right)=\int_{0}^{\infty} x^{-1 / 2} e^{-x} d x
$$

Letting $x=u^{2}$ this integral becomes

$$
2 \int_{0}^{\infty} e^{-u^{2}} d u=2\left(\frac{\sqrt{\pi}}{2}\right)=\sqrt{\pi}
$$

using Problem 12.31. This result also is described in Equation (11a, b) on Page 391.

15.5. Evaluate each integral.

(a) $\int_{0}^{\infty} \sqrt{y} e^{-y^{2}} d y$.

Letting $y^{3}=x$, the intergral becomes

$$
\int_{0}^{\infty} \sqrt{x^{1 / 3}} e^{-x} \cdot \frac{1}{3} x^{-2 / 3} d x=\frac{1}{3} \int_{0}^{\infty} x^{-1 / 2} e^{-x} d x=\frac{1}{3} \Gamma\left(\frac{1}{2}\right)=\frac{\sqrt{\pi}}{3}
$$

(b)

$\int_{0}^{\infty} 3^{-4 x^{2}} d x=\int_{0}^{\infty}\left(e^{\operatorname{In} 3}\right)^{\left(-4 x^{2}\right)} d z=\int_{0}^{\infty}\left(e^{-(4 \ln 3) z^{2}} d z\right.$

Letting $(4 \ln 3) z^{2}=x$, the integral becomes

$$
\int_{0}^{\infty} \mathrm{e}^{-x} d\left(\frac{x^{1 / 2}}{\sqrt{4 \ln 3}}\right)=\frac{1}{2 \sqrt{4 \ln 3}} \int_{0}^{\infty} x^{-1 / 2} e^{-x} d x=\frac{\Gamma(1 / 2)}{2 \sqrt{4 \ln 3}}=\frac{\sqrt{\pi}}{4 \sqrt{\ln 3}}
$$

(c) $\int_{0}^{1} \frac{d x}{\sqrt{-\ln x}}$.

Let $-\ln \mathrm{x}=\mathrm{u}$. Then $\mathrm{x}=\mathrm{e}-\mathrm{u}$. When $\mathrm{x}=1, \mathrm{u}=0$; when $\mathrm{x}=0, \mathrm{u}=\infty$. The integral becomes

$$
\int_{0}^{\infty} \frac{e^{-u}}{\sqrt{u}} d u=\int_{0}^{\infty} u^{-1 / 2} e^{-u} d u=\Gamma(1 / 2)=\sqrt{\pi}
$$

15.6. Evaluate $\int_{0}^{\infty} x^{m} e^{-a x^{n}} d x$, where $m, n$, and $a$ are positive constants.

Letting $a x^{n}=y$, the integral becomes

$$
\int_{0}^{\infty}\left\{\left(\frac{y}{a}\right)^{1 / n}\right\}^{m} e^{-y} d\left\{\left(\frac{y}{a}\right)^{1 / n}\right\}=\frac{1}{n a^{(m+1) / n}} \int_{0}^{\infty} y^{(m+1) / n-1} e^{-y} d y=\frac{1}{n a^{(m+1) / n}} \Gamma\left(\frac{m+1}{n}\right)
$$

15.7. Evaluate (a) (a) $\Gamma(-1 / 2)$ (b) $(-5 / 2)$

We use the generalization to negative values defined by $\Gamma(x)=\frac{\Gamma(x+1)}{x}$.

(a) Letting $x=-\frac{1}{2}, \quad \Gamma(-1 / 2)=\frac{\Gamma(1 / 2)}{-1 / 2}=-2 \sqrt{\pi}$.

(b) Letting $x=-3 / 2, \quad \Gamma(-3 / 2)=\frac{\Gamma(-1 / 2)}{-3 / 2}=\frac{-2 \sqrt{\pi}}{-3 / 2}=\frac{4 \sqrt{\pi}}{3}$, using (a)

Then $\Gamma(-5 / 2)=\frac{\Gamma(-3 / 2)}{-5 / 2}=-\frac{8}{15} \sqrt{\pi}$.

15.8. Prove that $\int_{0}^{1} x^{m}(\ln x)^{n} d x=\frac{(-1)^{n} n !}{(m+1)^{n+1}}$, where $n$ is a positive integer and $m>-1$.

Letting $x=e^{-y}$, the integral becomes $(-1)^{n} \int_{0}^{\infty} y^{n} e^{-(m+1) y} d y$. If $(m+1) y=u$, this last integral becomes

$$
(-1)^{n} \int_{0}^{\infty} \frac{u^{n}}{(m+1)^{n}} e^{-u} \frac{d u}{m+1}=\frac{(-1)^{n}}{(m+1)^{n+1}} \int_{0}^{\infty} u^{n} e^{-u} d u=\frac{(-1)^{n}}{(m+1)^{n+1}} \Gamma(n+1)=\frac{(-1)^{n} n !}{(m+1)^{n+1}}
$$

Compare with Problem 8.50.

15.9. A particle is attracted toward a fixed point $O$ with a force inversely proportional to its instantaneous distance from $O$. If the particle is released from rest, find the time for it to reach $O$

At time $t=0$, let the particle be located on the $x$ axis at $x=a>0$ and let $O$ be the origin. Then, by Newton's law,


\begin{equation*}
m \frac{d^{2} x}{d t^{2}}=-\frac{k}{x} \tag{1}
\end{equation*}


where $m$ is the mass of the particle and $k>0$ is a constant of proportionality. comes

Let $\frac{d x}{d t}=v$, the velocity of the particle. Then $\frac{d^{2} x}{d t^{2}}=\frac{d v}{d t}=\frac{d v}{d x} \cdot \frac{d x}{d t}=v \cdot \frac{d v}{d x}$ and Equation (1) be-


\begin{equation*}
m v \frac{d v}{d x}=-\frac{k}{x} \quad \text { or } \quad \frac{m v^{2}}{2}=-k \ln x+c \tag{2}
\end{equation*}


upon integrating. Since $v=0$ at $x=a$, we find $c=k \ln a$. Then


\begin{equation*}
\frac{m v^{2}}{2}=k \ln \frac{a}{x} \quad \text { or } \quad v=\frac{d x}{d t}=-\sqrt{\frac{2 k}{m}} \sqrt{\ln \frac{a}{x}} \tag{3}
\end{equation*}


where the negative sign is chosen, since $x$ is decreasing as $t$ increases. We thus find that the time $T$ taken for the particle to go from $x=a$ to $x=0$ is given by


\begin{equation*}
T=\sqrt{\frac{m}{2 k}} \int_{0}^{a} \frac{d x}{\sqrt{\ln a / x}} \tag{4}
\end{equation*}


Letting $\ln a / x=u$ or $x=a e^{-u}$, this becomes

$$
T=a \sqrt{\frac{m}{2 k}} \int_{0}^{\infty} u^{-1 / 2} e^{-u} d u=a \sqrt{\frac{m}{2 k}} \Gamma\left(\frac{1}{2}\right)=a \sqrt{\frac{\pi m}{2 k}}
$$

\section*{The Beta Function}
15.10. Prove that (a) $B(u, v)=B(v, u)$ and (b) $B(u, v)=2 \int_{0}^{\pi / 2} \sin ^{2 u-1} \theta \cos ^{2 v-1} \theta d \theta$.

(a) Using the transformation $\mathrm{x}=1-\mathrm{y}$, we have

$$
B(u, v)=\int_{0}^{1} x^{u-1}(1-x)^{v-1} d x=\int_{0}^{1}(1-y)^{u-1} y^{v-1} d y=\int_{0}^{1} y^{v-1}(1-y)^{u-1} d y=B(v, u)
$$

(b) Using the transformation $\mathrm{x}=\sin ^{2} \theta$, we have

$$
\begin{aligned}
B(u, v) & =\int_{0}^{1} x^{u-1}(1-x)^{v-1} d x=\int_{0}^{\pi / 2}\left(\sin ^{2} \theta\right)^{u-1}\left(\cos ^{2} \theta\right)^{v-1} 2 \sin \theta \cos \theta d \theta \\
& =2 \int_{0}^{\pi / 2} \sin ^{2 u-1} \theta \cos ^{2 x-1} \theta d \theta
\end{aligned}
$$

15.11. Prove that $B(u, v)=\frac{\Gamma(u) \Gamma(v)}{\Gamma(u+v)} \quad u, v>0$.

Letting $z^{2}=x^{2}$, we have $\Gamma(u)=\int_{0}^{\infty} z^{u-1} e^{-z} d x=2 \int_{0}^{\infty} x^{2 u-1} e^{-x^{2}} d x$.

Similarly, $\Gamma(v)=2 \int_{0}^{\infty} y^{2 v-1} e^{-y^{2}} d y$. Then

$$
\begin{aligned}
\Gamma(u) \Gamma(v) & =4\left(\int_{0}^{\infty} x^{2 u-1} e^{-x^{2}} d x\right)\left(\int_{0}^{\infty} y^{2 v-1} e^{-y^{2}} d y\right) \\
& =4 \int_{0}^{\infty} \int_{0}^{\infty} x^{2 u-1} y^{2 v-1} e^{-\left(x^{2}+y^{2}\right)} d x d y
\end{aligned}
$$

Transforming to polar coordinates, $x=\rho \cos \phi, y=\rho \sin \phi$,

$$
\begin{aligned}
\Gamma(u) \Gamma(v) & =4 \int_{\phi=0}^{\pi / 2} \int_{\rho=0}^{\infty} \rho^{2}(u+v)-1 e^{-\rho^{2}} \cos ^{2 u-1} \phi \sin ^{2 v-1} \phi d \rho d \phi \\
& =4\left(\int_{p=0}^{\infty} p^{2(u+v)-1} e^{-p^{2}} d p\right)\left(\int_{\phi=0}^{\pi / 2} \cos ^{2 u-1} \phi \sin ^{2 v-1} \phi d \phi\right) \\
& =2 \Gamma(u+v) \int_{0}^{\pi / 2} \cos ^{2 u-1} \phi \sin ^{2 v-1} \phi d \phi=\Gamma(u+v) B(v, u) \\
& =\Gamma(u+v) B(u, v)
\end{aligned}
$$

using the results of Problem 15.10. Hence, the required result follows.

This argument can be made rigorous by using a limiting procedure as in Problem 12.31.

15.12. Evaluate each of the following integrals.

(a) $\int_{0}^{1} x^{4}(1-x)^{3} d x=B(5,4)=\frac{\Gamma(5) \Gamma(4)}{\Gamma(9)}=\frac{4 ! 3 !}{8 !}=\frac{1}{280}$

(b) $\int_{0}^{2} \frac{x^{2} d x}{\sqrt{2-x}}$

Letting $\mathrm{x}=2 \mathrm{v}$, the integral becomes

$4 \sqrt{2} \int_{0}^{1} \frac{v^{2}}{\sqrt{1-v}} d v=4 \sqrt{2} \int_{0}^{1} v^{2}(1-v)^{-1 / 2} d v=4 \sqrt{2} B\left(3, \frac{1}{2}\right)=\frac{4 \sqrt{2 \Gamma(3) \Gamma(1 / 2)}}{\Gamma(7 / 2)}=\frac{64 \sqrt{2}}{15}$

(c) $\int_{0}^{a} y^{4} \sqrt{a^{2}-y^{2}} d y$

Letting $y^{2}=a^{2} x$ or $y=\sqrt{x}$, the integral becomes

$$
a^{6} \int_{0}^{1} x^{3 / 2}(1-x)^{1 / 2} d x=a^{6} B(5 / 2,3 / 2)=\frac{a^{6} \Gamma(5 / 2) \Gamma(3 / 2)}{\Gamma(4)}=\frac{\pi a^{6}}{16}
$$

15.13. Show that $\int_{0}^{\pi / 2} \sin ^{2 u-1} \theta \cos ^{2 v-1} \theta d \theta=\frac{\Gamma(u) \Gamma(v)}{2 \Gamma(u+v)} \quad u, v>0$.

This follows at once from Problems 15.10 and 15.11 .

15.14. Evaluate (a) $\int_{0}^{\pi / 2} \sin ^{6} \theta d \theta$, (b) $\int_{0}^{\pi / 2} \sin ^{4} \theta \cos ^{5} \theta d \theta$, and (c) $\int_{0}^{\pi / 2} \cos ^{4} \theta d \theta$.

(a) Let $2 u-1=6,2 v-1=0$, i.e., $u=7 / 2, v=1 / 2$, in Problem 15.13. Then the required integral has the value $\frac{\Gamma(7 / 2) \Gamma(1 / 2)}{2 \Gamma(4)}=\frac{5 \pi}{32}$.

(b) Letting $2 \mathrm{u}-1=4,2 v-1=5$, the required integral has the value $\frac{\Gamma(5 / 2) \Gamma(3)}{2 \Gamma(11 / 2)}=\frac{8}{315}$.

(c) The given integral $=2 \int_{0}^{\pi / 2} \cos ^{4} \theta d \theta$. Thus, letting $2 u-1=0,2 v-1=4$ in Problem 15.13, the value is $\frac{2 \Gamma(1 / 2) \Gamma(5 / 2)}{2 \Gamma(3)}=\frac{3 \pi}{8}$.

15.15. Prove $\int_{0}^{\pi / 2} \sin ^{p} \theta d \theta=\int_{0}^{\pi / 2} \cos ^{p} \theta d \theta=(a) \frac{1 \cdot 3 \cdot 5 \cdots(p-1)}{2 \cdot 4 \cdot 6 \cdots p} \frac{\pi}{2}$ if $p$ is an even positive integer and

(b) $\frac{2 \cdot 4 \cdot 6 \cdots(p-1)}{1 \cdot 3 \cdot 5 \cdots p}$ if $\mathrm{p}$ is an odd positive integer.

From Problem 15.13, with $2 u-1=p, 2 v-1=0$, we have

$$
\int_{0}^{\pi / 2} \sin ^{p} \theta d \theta=\frac{\Gamma\left[\frac{1}{2}(p+1) \Gamma\left(\frac{1}{2}\right)\right.}{2 \Gamma\left[\frac{1}{2}(p+2)\right]}
$$

(a) If $p=2 r$, the integral equals

$\frac{\Gamma(r+1) \Gamma\left(\frac{1}{2}\right)}{2 \Gamma(r+1)}=\frac{\left(r-\frac{1}{2}\right)\left(r-\frac{3}{2}\right) \cdots \frac{1}{2} \Gamma\left(\frac{1}{2}\right) \cdot \Gamma\left(\frac{1}{2}\right)}{2 r(r-1)}=\frac{(2 r-1)(2 r-3) \cdots 1}{2 r(2 r-2) \cdots 2} \frac{\pi}{2}=\frac{1 \cdot 3 \cdot 5 \cdots(2 r-1)}{2 \cdot 4 \cdot 6 \cdots 2 r} \frac{\pi}{2}$

(b) If $\mathrm{p}=2 \mathrm{r}+1$, the integral equals

$$
\frac{\Gamma(r+1) \Gamma\left(\frac{1}{2}\right)}{2 \Gamma\left(r+\frac{3}{2}\right)}=\frac{r(r-1) \cdots 1 \cdot \sqrt{\pi}}{2\left(r+\frac{1}{2}\right)\left(r-\frac{1}{2}\right) \cdots \frac{1}{2} \sqrt{\pi}}=\frac{2 \cdot 4 \cdot 6 \cdots 2 r}{1 \cdot 3 \cdot 5 \cdots(2 r+1)}
$$

In both cases, $\int_{0}^{\pi / 2} \sin ^{p} \theta d \theta=\int_{0}^{\pi / 2} \cos ^{p} \theta d \theta$, as seen by letting $\theta=\pi / 2-\phi$.

15.16. Evaluate (a) $\int_{0}^{\pi / 2} \cos ^{6} \theta d \theta$, (b) $\int_{0}^{\pi / 2} \sin ^{3} \theta \cos ^{2} \theta d \theta$, and (c) $\int_{0}^{2 \pi} \sin ^{8} \theta d \theta$.

(a) From Problem 15.15, the integral equals $\frac{1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6}=\frac{5 \pi}{32}$ [compare Problem 15.14(a)].

(b) The integral equals

$$
\int_{0}^{\pi / 2} \sin ^{3} \theta\left(1-\sin ^{2} \theta\right) d \theta=\int_{0}^{\pi / 2} \sin ^{3} \theta d \theta-\int_{0}^{\pi / 2} \sin ^{5} \theta d \theta=\frac{2}{1 \cdot 3}-\frac{2 \cdot 4}{1 \cdot 3 \cdot 5}=\frac{2}{15}
$$

The method of Problem 15.14(b) can also be used.

(c) The given integral equals $4 \int_{0}^{\pi / 2} \sin ^{8} \theta d \theta=4\left(\frac{1 \cdot 3 \cdot 5 \cdot 7}{2 \cdot 4 \cdot 6 \cdot 8} \frac{\pi}{2}\right)=\frac{35 \pi}{64}$.

15.17. Given $\int_{0}^{\infty} \frac{x^{p-1}}{1+x} d x=\frac{\pi}{\sin p \pi}$, show that $\Gamma(p) \Gamma(1-p)=\frac{\pi}{\sin p \pi}$, where $0<p<1$.

Letting $\frac{x}{1+x}=y$ or $x=\frac{y}{1-y}$, the given integral becomes

$$
\int_{0}^{1} y^{p-1}(1-y)^{-p} d y=B(p, 1-p)=\Gamma(p) \Gamma(1-p)
$$

and the result follows.

15.18. Evaluate $\int_{0}^{\infty} \frac{d y}{1+y^{4}}$.

Let $y^{4}=x$. Then the integral becomes $\frac{1}{4} \int_{0}^{\infty} \frac{x^{-3 / 4}}{1+x} d x=\frac{\pi}{4 \sin (\pi / 4)}=\frac{\pi \sqrt{2}}{4}$ by Problem 15.17, with $p=\frac{1}{4}$. The result can also be obtained by letting $y^{2}=\tan \theta$.

15.19. Show that $\int_{0}^{2} x \sqrt[3]{8-x^{3}} d x=\frac{16 \pi}{9 \sqrt{3}}$.

Letting $x^{3}-8 y$ or $x=2 y^{1 / 3}$, the integral becomes

$$
\begin{aligned}
\int_{0}^{1} 2 y^{1 / 3} \sqrt{8(1-y)} \cdot \frac{2}{3} y^{-2 / 3} d y & =\frac{8}{3} \int_{0}^{1} y^{-1 / 3}(1-y)^{1 / 3} d y=\frac{8}{3} B\left(\frac{2}{3}, \frac{4}{3}\right) \\
& =\frac{8}{3} \frac{\Gamma\left(\frac{2}{3} \Gamma \frac{4}{3}\right)}{\Gamma(2)}=\frac{8}{9} \Gamma\left(\frac{1}{3}\right) \Gamma\left(\frac{2}{3}\right)=\frac{8}{9} \cdot \frac{\pi}{\sin \pi / 3}=\frac{16 \pi}{9 \sqrt{3}}
\end{aligned}
$$

\section*{Stirling's formula}
15.20. Show that for large positive integers $n, n !=\sqrt{2 \pi n} n^{n} e^{-n}$ approximately.

By definition, $\Gamma(z)=\int_{0}^{\infty} t^{z-1} e^{-t} d t$. Let $l f z=x+1$, then


\begin{equation*}
\Gamma(x+1)=\int_{0}^{\infty} t^{x} e^{-t} d t=\int_{0}^{\infty} e^{-t+\operatorname{In} t^{x}} d t=\int_{0}^{\infty} e^{-t+x \operatorname{In} t} d t \tag{1}
\end{equation*}


For a fixed value of $x$ the function $x, \ln t-t$ has a relative maximum for $t=x$ (as is demonstrated by elementary ideas of calculus). The substitution $t=x+y$ yields


\begin{equation*}
\Gamma(x+1)=e^{-x} \int_{-x}^{\infty} e^{x \operatorname{In}(\mathrm{x}+\mathrm{y})-\mathrm{y}} d y=x^{x} e^{-x} \int_{-x}^{\infty} e^{x \operatorname{In}\left(1+\frac{\mathrm{y}}{\mathrm{x}}\right)-y} d y \tag{2}
\end{equation*}


To this point the analysis has been rigorous. The following formal steps can be made rigorous by incorporating appropriate limiting procedures; however, because of the difficulty of the proofs, they have been omitted.

In Equation (2) introduce the logarithmic expansion


\begin{equation*}
\ln \left(1+\frac{y}{x}\right)=\frac{y}{x}-\frac{y^{2}}{2 x^{2}}+\frac{y^{3}}{3 x^{3}}-+\cdots \tag{3}
\end{equation*}


and also let

$$
y=\sqrt{x} v, \quad d y=\sqrt{x} d v
$$

Then


\begin{equation*}
\Gamma(x+1)=x^{x} e^{-x} \sqrt{x} \int_{-x}^{\infty} e^{-v^{2} / 2+\left(v^{3} / 3\right) \sqrt{x-\cdots}} d v \tag{4}
\end{equation*}


For large values of $x$

$$
\Gamma(x+1) \approx x^{x} e^{-x} \sqrt{x} \int_{-x}^{\infty} e^{-v^{2} / 2} d v=x^{x} e^{-x} \sqrt{2 \pi x}
$$

When $x$ is replaced by integer values $n$, then the Stirling relation


\begin{equation*}
n !=\Gamma(x+1) \approx \sqrt{2 \pi x} x^{x} e^{-x} \tag{5}
\end{equation*}


is obtained.

It is of interest that from Equation (4) we can also obtain the result (12) on Page 391. See Problem 15.72.

\section*{Dirichlet integrals}
15.21. Evaluate $I=\iiint_{V} x^{\alpha-1} y^{\beta-1} z^{y-1} d x d y d z$, where $V$ is the region in the first octant bounded by the sphere $x^{2}+y^{2}$ $+z^{2}=1$ and the coordinate planes.

Let $x^{2}=u, y^{2}=v, z^{2}=w$. Then


\begin{align*}
I & =\iiint_{\Re} u^{(\alpha-1) / 2} v^{(\beta-1) / 2} w^{(y-1) / 2} \frac{d u}{2 \sqrt{u}} \frac{d v}{2 \sqrt{v}} \frac{d w}{2 \sqrt{w}}  \tag{1}\\
& =\frac{1}{8} \iiint_{\Re} u^{(\alpha / 2)-1} v^{(\beta / 2)-1} w^{(\gamma / 2)-1} d u d v d w
\end{align*}


where $\Re$ is the region in the $u v w$ space bounded by the plane $u+v+w=1$ and the $u v, v w$, and $u w$ planes, as in Figure 15.2. Thus,


\begin{align*}
I & =\frac{1}{8} \int_{u=0}^{1} \int_{v=0}^{1-u} \int_{w=0}^{1-u-v} u^{(\alpha / 2)-1} v^{(\beta / 2)-1} w^{(y-/ 2)-1} d u d v d w \\
& =\frac{1}{4 \gamma} \int_{u=0}^{1} \int_{v=0}^{1-u} u^{(\alpha / 2)-1} v^{(\beta / 2)-1}(1-u-v)^{\gamma / 2} d u d v  \tag{2}\\
& =\frac{1}{4 \gamma} \int_{u=0}^{1} u^{(\alpha / 2)-1}\left\{\int_{v=0}^{1-u} v^{(\beta / 2)-1}(1-u-v)^{\gamma / 2} d v\right\} d u
\end{align*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-410}
\end{center}

Figure 15.2

Letting $v=(1-u) t$, we have

$$
\begin{aligned}
\int_{v=0}^{1-u} v^{(\beta / 2)-1}(1-u-v)^{\gamma / 2} d v & =(1-u)^{(\beta+\gamma) / 2} \int_{t=0}^{1} t^{(\beta / 2)-1}(1-t)^{\gamma / 2} d t \\
& =(1-u)^{(\beta+\gamma) / 2} \frac{\Gamma(\beta / 2) \Gamma(\gamma / 2+1)}{\Gamma[(\beta+\gamma) / 2+1]}
\end{aligned}
$$

so that Equation (2) becomes


\begin{align*}
I & =\frac{1}{4 \gamma} \frac{\Gamma(\beta / 2) \Gamma(\gamma / 2+1)}{\Gamma[(\beta+\gamma) / 2+1]} \int_{u=0}^{1} u^{(\alpha / 2)^{-1}}(1-u)^{(\beta+\gamma) / 2} d u \\
& =\frac{1}{4 \gamma} \frac{\Gamma(\beta / 2) \Gamma(\gamma / 2+1)}{\Gamma[(\beta+\gamma) / 2+1]} \cdot \frac{\Gamma(\alpha / 2) \Gamma(\beta+\gamma) / 2+1)}{\Gamma[(\alpha+\beta+\gamma) / 2+1]}=\frac{\Gamma(\alpha / 2) \Gamma(\beta / 2) \Gamma(\gamma / 2)}{8 \Gamma[(\alpha+\beta+) / 2+1]} \tag{3}
\end{align*}


where we have used $(\gamma / 2) \Gamma(\gamma / 2)=\Gamma(\gamma / 2+1)$.

The integral evaluated here is a special case of the Dirichlet integral Equation (20), Page 393. The general case can be evaluated similarly.

15.22. Find the mass of the region bounded by $x^{2}+y^{2}+z^{2}=a^{2}$ if the density is $\sigma=x^{2} y^{2} z^{2}$.

The required mass $=8 \iiint_{V} x^{2} y^{2} z^{2} d x d y d z$, where $V$ is the region in the first octant bounded by the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ and the coordinate planes.

In the Dirichlet integral, Equation (20), Page 393, let $b=c=a, p=q=r=2$, and $\alpha=\beta=\gamma=3$. Then the required result is

$$
8 . \frac{a^{3} \cdot a^{3} \cdot a^{3}}{2 \cdot 2 \cdot 2} \frac{\Gamma(3 / 2) \Gamma(3 / 2) \Gamma(3 / 2)}{\Gamma(1+3 / 2+3 / 2+3 / 2)}=\frac{4 \pi s^{9}}{945}
$$

\section*{Miscellaneous problems}
15.23. Show that $\int_{0}^{1} \sqrt{1-x^{4}} d x=\frac{\{\Gamma(1 / 4)\}^{2}}{6 \sqrt{2 \pi}}$.

Let $x^{4}=y$. Then the integral becomes

$$
\frac{1}{4} \int_{0}^{1} y^{-3 / 4}(1-y)^{1 / 2} d y=\frac{1}{4} \frac{\Gamma(1 / 4) \Gamma(3 / 2)}{\Gamma(7 / 4)}=\frac{\sqrt{\pi}}{4} \frac{\{\Gamma(1 / 4)\}^{2}}{\Gamma(1.4) \Gamma(3 / 4)} .
$$

From Problem 15.17, with $p=1 / 4, \Gamma(1 / 4) \Gamma(3 / 4)=\pi \sqrt{2}$, so that the required result follows.

15.24. Prove the duplication formula $2^{2 p-1} \Gamma(\mathrm{p}) \Gamma\left(p+\frac{1}{2}\right)=\sqrt{\pi} \Gamma(2 p)$.

Let $I=\int_{0}^{\pi / 2} \sin ^{2 p} x d x, J=\int_{0}^{\pi / 2} \sin ^{2 p} 2 x d x$.

Then $I=\frac{1}{2} B\left(p+\frac{1}{2}, \frac{1}{2}\right)=\frac{\Gamma\left(p+\frac{1}{2}\right) \sqrt{\pi}}{2 \Gamma(p+1)}$.

Letting $2 x=u$, we find

$$
J=\frac{1}{2} \int_{0}^{\pi} \sin ^{2 p} u d u=\int_{0}^{\pi / 2} \sin ^{2 p} u d u=I
$$

But

$$
\begin{aligned}
J & =\int_{0}^{\pi / 2}(2 \sin x \cos x)^{2 p} d x=2^{2 p} \int_{0}^{\pi / 2} \sin ^{2 p} x \cos ^{2 p} x d x \\
& =2^{2 p-1} B\left(p+\frac{1}{2}, P+\frac{1}{2}\right)=\frac{2^{2 p-1}\left\{\Gamma\left(p+\frac{1}{2}\right)\right\}^{2}}{\Gamma(2 p+1)}
\end{aligned}
$$

Then, since $I=J$,

$$
\frac{\Gamma\left(p+\frac{1}{2}\right) \sqrt{\pi}}{2 p \Gamma(p)}=\frac{2^{2 p-1}\left\{\Gamma\left(p+\frac{1}{2}\right)\right\}^{2}}{2 p \Gamma(2 p)}
$$

and the required result follows. (See Problem 15.74, where the duplication formula is developed for the simpler case of integers.)

15.25. Show that $\int_{0}^{\pi / 2} \frac{d \phi}{\sqrt{1-\frac{1}{2} \sin ^{2} \phi}}=\frac{\{\Gamma(1 / 4)\}^{2}}{4 \sqrt{\pi}}$.

Consider

$$
I=\int_{0}^{\pi / 2} \frac{d \theta}{\sqrt{\cos \theta}}=\int_{0}^{\pi / 2} \cos ^{-1 / 2} \theta d \theta=\frac{1}{2} B\left(\frac{1}{4}, \frac{1}{2}\right)=\frac{\Gamma\left(\frac{1}{4}\right) \sqrt{\pi}}{2 \Gamma\left(\frac{3}{4}\right)}=\frac{\left\{\Gamma\left(\frac{1}{4}\right)\right\}^{2}}{2 \sqrt{2 \pi}}
$$

as in Problem 15.23.

But $I=\int_{0}^{\pi / 2} \frac{d \theta}{\sqrt{\cos \theta}}=\int_{0}^{\pi / 2} \frac{d \theta}{\sqrt{\cos ^{2} \theta / 2-\sin ^{2} \theta / 2}}=\int_{0}^{\pi / 2} \frac{d \theta}{\sqrt{1-2 \sin ^{2} \theta / 2}}$. follows.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-411}
\end{center}

15.26. Prove that $\int_{0}^{\infty} \frac{\cos x}{x^{p}} d x=\frac{\pi}{2 \Gamma(p) \cos (p \pi / 2)}, 0<p<1$.

We have $\frac{1}{x^{p}}=\frac{1}{\Gamma(p)} \int_{0}^{\infty} u^{p-1} e^{-x u} d u$. Then


\begin{align*}
\int_{0}^{\infty} \frac{\cos x}{x^{p}} d x & =\frac{1}{\Gamma(p)} \int_{0}^{\infty} \int_{0}^{\infty} u^{p-1} e^{-x u} \cos x d u d x  \tag{1}\\
& =\frac{1}{\Gamma(p)} \int_{0}^{\infty} \frac{u^{p}}{1+u^{2}} d u
\end{align*}


where we have reversed the order of integration and used Problem 12.22.

Letting $u^{2}=v$ in the last integral, we have, by Problem 15.17,


\begin{equation*}
\int_{0}^{\infty} \frac{u^{p}}{1+u^{2}} d u=\frac{1}{2} \int_{0}^{\infty} \frac{v^{(p-1) / 2}}{1+v} d v=\frac{\pi}{2 \sin (p+1) \pi / 2}=\frac{\pi}{2 \cos p \pi / 2} \tag{2}
\end{equation*}


Substitution of Equation (2) in Equation (1) yields the required result.

15.27. Evaluate $\int_{0}^{\infty} \cos x^{2} d x$.

Letting $x^{2}=y$, the integral becomes $\frac{1}{2} \int_{0}^{\infty} \frac{\cos y}{\sqrt{y}} d y=\frac{1}{2}\left(\frac{\pi}{2 \Gamma\left(\frac{1}{2}\right) \cos \pi / 4}\right)=\frac{1}{2} \sqrt{\pi / 2}$ by Problem 15.26.

This integral and the corresponding one for the sine [see Problem 15.68(a)] are called Fresnel integrals.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{The gamma function}
15.28. Evaluate (a) $\frac{\Gamma(7)}{2 \Gamma(4) \Gamma(3)}$, (b) $\frac{\Gamma(3) \Gamma(3 / 2)}{\Gamma(9 / 2)}$, and (c) $\Gamma(1 / 2) \Gamma(3 / 2) \Gamma(5 / 2)$.

Ans. (a) 30 (b) $16 / 105$ (c) $\frac{3}{8} \pi^{3 / 2}$

15.29. Evaluate (a) $\int_{0}^{\infty} x^{4} e^{-x} d x$, (b) $\int_{0}^{\infty} x^{6} e^{-3 x} d x$, and (c) $\int_{0}^{\infty} x^{2} e^{-2 x^{2}} d x$.

$$
\text { Ans. (a) } 24 \text { (b) } \frac{80}{243} \text { (c) } \frac{\sqrt{2 \pi}}{16}
$$

15.30. Find (a) $\int_{0}^{\infty} e^{-x^{2}} d x$, (b) $\int_{0}^{\infty} \sqrt[4]{x} e^{-\sqrt{x}} d x$, and (c) $\int_{0}^{\infty} y^{3} e^{-2 y^{5}} d y$.

$$
\text { Ans. (a) } \frac{1}{3} \Gamma\left(\frac{1}{3}\right) \text { (b) } \frac{3 \sqrt{\pi}}{2} \text { (c) } \frac{\Gamma(4 / 5)}{5 \sqrt[5]{16}}
$$

15.31. Show that $\int_{0}^{\infty} \frac{e^{-s t}}{\sqrt{t}} d t=\sqrt{\frac{\pi}{8}}, \quad s>0$.

15.32. Prove that $\Gamma(v)=\int_{0}^{1}\left(\ln \frac{1}{x}\right)^{v-1} d x, \quad v>0$.

15.33. Evaluate (a) $\int_{0}^{1}(\ln x)^{4} d x$, (b) $\int_{0}^{1}(x \ln x)^{3} d x$, and (c) $\int_{0}^{1} \sqrt[3]{\ln (1 / x)} d x$.

$$
\text { Ans. (a) } 24 \text { (b) }-3 / 128 \text { (c) } \frac{1}{3} \Gamma\left(\frac{1}{3}\right)
$$

15.34. Evaluate (a) $\Gamma(-7 / 2)$ and (b) $\Gamma(-1 / 3)$.

$$
\text { Ans. (a) }(16 \sqrt{\pi}) / 105 \text { (b) }-3 \Gamma(2 / 3)
$$

15.35. Prove that $\lim _{x \rightarrow-m} \Gamma(\mathrm{x})=\infty$, where $m=0,1,2,3, \ldots$

15.36. Prove that if $m$ is a positive integer, $\Gamma\left(-m+\frac{1}{2}\right)=\frac{(-1)^{m} 2^{m} \sqrt{\pi}}{1 \cdot 3 \cdot 5 \cdots(2 m-1)}$.

15.37. Prove that $\Gamma^{\prime}(1)=\int_{0}^{\infty} e^{-x} \ln x d x$ is a negative number (it is equal to $-\gamma$, where $\gamma=0.577215 \ldots$ is called Euler's constant, as in Problem 11.49).

\section*{The beta function}
15.38. Evaluate (a) $B(3,5)$, (b) $B(3 / 2,2)$, and (c) $B(1 / 3,2 / 3)$.\\
Ans. (a) $1 / 105$ (b) $4 / 15$ (c) $2 \pi / \sqrt{3}$

15.39. Find (a) $\int_{0}^{1} x^{2}(1-x)^{3} d x$, (b) $\int_{0}^{1} \sqrt{(1-x) / x} d x$, and (c) $\int_{0}^{1}\left(4-x^{2}\right)^{3 / 2} d x$.

Ans. (a) $1 / 60$ (b) $\pi / 2$ (c) $3 \pi$

15.40. Evaluate (a) $\int_{0}^{4} u^{3 / 2}(4-u)^{5 / 2} d u$ and (b) $\int_{0}^{3} \frac{d x}{\sqrt{3 x-x^{2}}}$.

Ans. (a) $12 \pi$ (b) $\pi$

15.41. Prove that $\int_{0}^{a} \frac{d y}{\sqrt{a^{4}-y^{4}}}=\frac{\{\Gamma(1 / 4)\}^{2}}{4 a \sqrt{2 \pi}}$.

15.42. Evaluate (a) $\int_{0}^{\pi / 2} \sin ^{4} \theta \cos ^{4} \theta d \theta$ and (b) $\int_{0}^{2 \pi} \cos ^{6} \theta d \theta$.

Ans. (a) $3 \pi / 256$ (b) $5 \pi / 8$

15.43. Evaluate (a) $\int_{0}^{\pi} \sin ^{5} \theta d \theta$ and (b) $\int_{0}^{\pi / 2} \cos ^{5} \theta \sin ^{2} \theta d \theta$.

Ans. (a) $16 / 15$ (b) $8 / 105$

15.44. Prove that $\int_{0}^{\pi / 2} \sqrt{\tan \theta} d \theta=\pi / \sqrt{2}$.

15.45. Prove that (a) $\int_{0}^{\infty} \frac{x d x}{1+x^{6}}=\frac{\pi}{3 \sqrt{3}}$ and (b) $\int_{0}^{\infty} \frac{y^{2} d y}{1+y^{4}}=\frac{\pi}{2 \sqrt{2}}$.

15.46. Prove that $\int_{-\infty}^{\infty} \frac{e^{2 x}}{a e^{3 x}+b} d x=\frac{2 \pi}{3 \sqrt{3} a^{2 / 3} b^{1 / 3}}$. where $a, b>0$.

15.47. Prove that $\int_{-\infty}^{\infty} \frac{e^{2 x}}{\left(e^{3 x}+1\right)} d x=\frac{2 \pi}{9 \sqrt{3}}$. [Hint: Differentiate with respect to Problem 15.46(b).]

15.48. Use the method of Problem 12.31 to justify the procedure used in Problem 15.11.

\section*{Dirichlet integrals}
15.49. Find the mass of the region in the $x y$ plane bounded by $x+y=1, x=0, y=0$ if the density is $\sigma=\sqrt{x y}$.\\
15.50. Find the mass of the region bounded by the ellipsoid $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}=1$ if the density varies as the square of\\
the distance from its center.

$$
\text { Ans. } \frac{\pi a b c k}{30}\left(a^{2}+b^{2}+c^{2}\right), k=\text { constant of proportionality }
$$

15.51. Find the volume of the region bounded by $x^{2 / 3}+y^{2 / 3}+z^{2 / 3}=1$.

Ans. $4 \pi / 35$

15.52. Find the centroid of the region in the first octant bounded by $x^{2 / 3}+y^{2 / 3}+z^{2 / 3}=1$.

Ans. $\bar{x}=\bar{y}=\bar{z}=21 / 128$

15.53. Show that the volume of the region bounded by $x^{m}+y^{m}+z^{m}=a^{m}$, where $m>0$, is given by $\frac{8\{\Gamma(1 / m)\}^{3}}{3 m^{2} \Gamma(3 / m)} a^{3}$.

15.54. Show that the centroid of the region in the first octant bounded by $x^{m}+y^{m}+z^{m}=a^{m}$, where $m>0$, is given by $\bar{x}=\bar{y}=\bar{z}=\frac{3 \Gamma(2 / m) \Gamma(3 / m)}{4 \Gamma(1 / m) \Gamma(4 / m)} a$.

\section*{Miscellaneous problems}
15.55. Prove that $\int_{a}^{b}(x-a)^{p}(b-x)^{q} d x=(b-a)^{p+q+1} \mathrm{~B}(p+1, q+1)$, where $p>-1, q>-1$, and $b>a$. [Hint:

Let $x-a=(b-a) y$.]

15.56. Evaluate (a) $\int_{1}^{3} \frac{d x}{\sqrt{(x-1)(3-x)}}$ and (b) $\int_{3}^{7} \sqrt[4]{(7-x)(x-3)} d x$.

$$
\text { Ans. (a) } \pi \text { (b) } \frac{2\{\Gamma(1 / 4)\}^{2}}{3 \sqrt{\pi}}
$$

15.57. Show that $\frac{\{\Gamma(1 / 3)\}^{2}}{\Gamma(1 / 6)}=\frac{\sqrt{\pi} \sqrt[3]{2}}{3}$.

15.58. Prove that $\mathbf{B}(u, v)=\frac{1}{2} \int_{0}^{1} \frac{x^{u-1}+x^{v-1}}{(1+x)^{u+v}} d x$, where $u, v>0$. [Hint: Let $y=x /(1+x)$.]

15.59. If $0<p<1$, prove that $\int_{0}^{\pi / 2} \tan ^{p} \theta d \theta=\frac{\pi}{2} \sec \frac{p \pi}{2}$.

15.60. Prove that $\int_{0}^{1} \frac{x^{u-1}(1-x)^{v-1}}{(x+r)^{u+v}}=\frac{\mathrm{B}(u, v)}{r^{u}(1+r)^{u+v}}$, where $u, v$ and $r$ are positive constants. [Hint: Let $x=(r+1) y /$

15.61. Prove that $\int_{0}^{\pi / 2} \frac{\sin ^{2 u-1} \theta \cos ^{2 v-1} \theta d \theta}{\left(a \sin ^{2} \theta+b \cos ^{2} \theta\right)^{u+v}}=\frac{\mathrm{B}(u, v)}{2 a^{v} b^{u}}$ where $u, v>0$. (Hint: Let $x=\sin ^{2} \theta$ in Problem 15.60 and choose $r$ appropriately.)

15.62. Prove that $\int_{0}^{1} \frac{d x}{x^{x}}=\frac{1}{1^{1}}+\frac{1}{2^{2}}+\frac{1}{3^{3}}+\cdots$

15.63. Prove that for $m=2,3,4, \ldots, \sin \frac{\pi}{m} \sin \frac{2 \pi}{m} \sin \frac{3 \pi}{m} \cdots \sin \frac{(m-1) \pi}{m}=\frac{m}{2^{m-1}}$. [Hint: Use the factored form $x^{m}-1=(x-1)\left(x-\alpha_{1}\right)\left(x-\alpha_{2}\right) \ldots\left(x-\alpha_{n-1}\right)$, divide both sides by $x-1$, and consider the limit as $x \rightarrow 1$.]

15.64. Prove that $\int_{0}^{\pi / 2} \ln \sin x d x=-\pi / 2 \ln 2$, using Problem 15.63. (Hint: Take logarithms of the result in Problem 15.63 and write the limit as $m \rightarrow \infty$ as a definite integral.)

15.65. Prove that $\Gamma\left(\frac{1}{m}\right) \Gamma\left(\frac{2}{m}\right) \Gamma\left(\frac{3}{m}\right) \cdots \Gamma \frac{(m-1)}{m}=\frac{(2 \pi)^{(m-1) / 2}}{\sqrt{m}}$. [Hint: Square the left-hand side and use Problem 15.63 and Equation (11a), Page 391.]

15.66. Prove that $\int_{0}^{1} \ln \Gamma(x) d x=\frac{1}{2} \operatorname{In}(2 \pi)$. (Hint: Take logarithms of the result in Problem 15.65 and let $m \rightarrow \infty$.)

15.67. (a) Prove that $\int_{0}^{\infty} \frac{\sin x}{x^{p}} d x=\frac{\pi}{2 \Gamma(p) \sin (p \pi / 2)}, \quad 0<p<1$. (b) Discuss the cases $\mathrm{p}=0$ and $\mathrm{p}=1$.

15.68. Evaluate (a) $\int_{0}^{\infty} \sin x^{2} d x$ and (b) $\int_{0}^{\infty} x \cos x^{3} d x$

$$
\text { Ans. (a) } \frac{1}{2} \sqrt{\pi / 2} \text { (b) } \frac{\pi}{3 \sqrt{3} \Gamma(1 / 3)}
$$

15.69. Prove that $\int_{0}^{\infty} \frac{x^{p-1} \ln x}{1+x} d x=-\pi^{2} \csc p \pi \cot p \pi, 0<p<1$.

15.70. Show that $\int_{0}^{\infty} \frac{\ln x}{x^{4}+1} d x=\frac{-\pi^{2} \sqrt{2}}{16}$

15.71. If $a>0, b>0$, and $4 a c>b^{2}$, prove that $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-\left(a x^{2}+b x y+c y^{2}\right)} d x d y=\frac{2 \pi}{\sqrt{4 a c-b^{2}}}$.

15.72. Obtain Equation (12) on Page 391 from the result (4) of Problem 15.20. [Hint: Expand $e^{v^{3} /(3 \sqrt{n})}+\cdots$ in a power series and replace the lower limit of the integral by $-\infty$.]

15.73. Obtain the result (15) on Page 392. [Hint: Observe that $\Gamma(x)=\frac{1}{x} \Gamma(x+!)$; thus, $\ln \Gamma(x)=\ln \Gamma(x+1)-\ln x$, and $\frac{\Gamma^{\prime}(x)}{\Gamma(x)}=\frac{\Gamma^{\prime}(x+1)}{\Gamma(x+1)}-\frac{1}{x}$. Furthermore, according to Equation (6), page $390, \Gamma(x+!)=\lim _{k \rightarrow \infty} \frac{k ! k^{x}}{(x+1) \cdots(x+k)}$. Now take the logarithm of this expression and then differentiate. Also, recall the definition of the Euler constant $\gamma$.]

15.74. The duplication formula (13a), Page 392, is proved in Problem 15.24. For further insight, develop it for positive integers; i.e., show that $2^{2 n-1} \Gamma\left(n+\frac{1}{2}\right) \Gamma(n)=\Gamma(2 n) \sqrt{\pi}$. [Hint: Recall that $\Gamma\left(\frac{1}{2}\right)=\pi$, then show that $\Gamma\left(n+\frac{1}{2}\right) \Gamma\left(\frac{2 n+1}{2}\right)=\frac{(2 n-1) \cdots 5 \cdot 3 \cdot 1}{2^{n}} \sqrt{\pi}$. Observe that $\frac{\Gamma(2 n+1)}{2^{n} \Gamma(n+1)}=\frac{(2 n !)}{2^{n} n !}=(2 n-1) \cdots 5 \cdot 3 \cdot 1$. Now substitute and refine.

\section*{Functions of a Complex Variable}
Ultimately, it was realized that to accept numbers that provided solutions to equations such as $x^{2}+1=0$ was no less meaningful than had been the extension of the real number system to admit a solution for $x+1=0$ or roots for $x^{2}-2=0$. The complex number system was in place around 1700, and by the early nineteenth century, mathematicians were comfortable with it. Physical theories took on a completeness not possible without this foundation of complex numbers and the analysis emanating from it. The theorems of the differential and integral calculus of complex functions introduce mathematical surprises as well as analytic refinement. This chapter is a summary of the basic ideas.

\section*{Functions}
If to each of a set of complex numbers which a variable $z$ may assume there corresponds one or more values of a variable $w$, then $w$ is called a function of the complex variable $z$, written $w=f(z)$. The fundamental operations with complex numbers have already been considered in Chapter 1.

A function is single-valued if for each value of $z$ there corresponds only one value of $w$; otherwise, it is multiplevalued or many-valued. In general, we can write $w=f(z)=u(x, y)+i v(x, y)$, where $u$ and $v$ are real functions of $x$ and $y$.

EXAMPLE. $w=z^{2}=(x+i y)^{2}=x^{2}-y^{2}+2 i x y=u+i v$ so that $u(x, y)=x^{2}-y^{2}, v(x, y)=2 x y$. These are called the real and imaginary parts of $w=z^{2}$, respectively.

In complex variables, multiple-valued functions often are replaced by a specially constructed singlevalued function with branches. This idea is discussed in a later paragraph.

EXAMPLE. Since $e^{2 \pi k i}=1$, the general polar form of $z$ is $z=\rho e^{i(\theta+2 \pi k)}$. This form and the fact that the logarithm and exponential functions are inverse leads to the following definition of $\ln z$ :

$$
\ln z=\ln \rho+(\theta+2 \pi k) i k=0,1,2, \ldots, n \ldots
$$

Each value of $k$ determines a single-valued function from this collection of multiple-valued functions. These are the branches from which (in the realm of complex variables) a single-valued function can be constructed.

\section*{Limits and Continuity}
Definitions of limits and continuity for functions of a complex variable are analogous to those for a real variable. Thus, $f(z)$ is said to have the limit $l$ as $z$ approaches $z_{0}$ if, given any $\epsilon>0$, there exists a $\delta>0$ such that $|f(z)-l|$ $<\epsilon$ whenever $0<\left|z-z_{0}\right|<\delta$.

Similarly, $f(z)$ is said to be continuous at $z_{0}$ if, given any $\epsilon>0$, there exists a $\delta>0$ such that $\left|f(z)-f\left(z_{0}\right)\right|<$ $\epsilon$ whenever $\left|z-z_{0}\right|<\delta$. Alternatively, $f(z)$ is continuous at $z_{0}$ if $\lim _{z \rightarrow z_{0}} f(z)=f\left(z_{0}\right)$.

Note: While these definitions have the same appearance as in thereal variable setting, remember that $\left|z-z_{0}\right|<\delta$ means

$$
\mid\left(x-x_{0}\left|+i\left(y-y_{0}\right)\right|=\sqrt{\left(x-x_{0}\right)^{2}\left(y-y_{0}\right)^{2}}<\delta\right.
$$

Thus, there are two degrees of freedom as $(x, y) \rightarrow\left(x_{0}, y_{0}\right)$.

\section*{Derivatives}
If $f(z)$ is single-valued in some region of the $z$ plane, the derivative of $f(z)$, denoted by $f^{\prime}(z)$, is defined as


\begin{equation*}
\lim _{\Delta z \rightarrow 0} \frac{(f(z+\Delta z)-f(z)}{\Delta z} \tag{1}
\end{equation*}


provided the limit exists independent of the manner in which $\Delta z \rightarrow 0$. If the limit (1) exists for $z=z_{0}$, then $f(z)$ is called analytic at $z_{0}$. If the limit exists for all $z$ in a region $\Re$, then $f(z)$ is called analytic in $\Re$. In order to be analytic, $f(z)$ must be single-valued and continuous. The converse, however, is not necessarily true.

We define elementary functions of a complex variable by a natural extension of the corresponding functions of a real variable. Where series expansions for real functions $f(x)$ exist, we can use as definition the series with $x$ replaced by $z$. The convergence of such complex series has already been considered in Chapter 11 .

EXAMPLE 1. We define $e^{x}=1+z+\frac{z^{2}}{2 !}+\frac{z^{3}}{3 !}+\cdots, \sin z=z-\frac{z^{3}}{3 !}+\frac{z^{5}}{5 !}-\frac{z^{7}}{7 !}+\cdots$, and $\cos z=1-\frac{z^{2}}{2 !}+\frac{z^{4}}{4 !}-\frac{z^{6}}{6 !}+\cdots$. From these we can show that $e^{x}=e^{x+i y}=e^{x}(\cos y+\sin y)$, as well as numerous other relations.

Rules for differentiating functions of a complex variable are much the same as for those of real variables.

Thus, $\frac{d}{d z}\left(z^{n}\right)=n z^{n-1}, \frac{d}{d z}(\sin z)=\cos z$ and so on.

\section*{Cauchy-Riemann Equations}
A necessary condition that $w=f(z)=u(x, y)+i v(x, y)$ be analytic in a region $\Re$ is that $u$ and $v$ satisfy the Cauchy-Riemann equations


\begin{equation*}
\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}, \quad \frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x} \tag{2}
\end{equation*}


(see Problem 16.7). If the partial derivatives in Equations (2) are continuous in $\Re$, the equations are sufficient conditions that $f(z)$ be analytic in $\Re$.

If the second derivatives of $u$ and $v$ with respect to $x$ and $y$ exist and are continuous, we find by differentiating Equations (2) that


\begin{equation*}
\frac{\partial^{2} u}{\partial x^{2}}=\frac{\partial^{2} u}{\partial y^{2}}=0, \quad \frac{\partial^{2} v}{\partial x^{2}}+\frac{\partial^{2} v}{\partial y^{2}}=0 \tag{3}
\end{equation*}


Thus, the real and imaginary parts satisfy Laplace's equation in two dimensions. Functions satisfying Laplace's equation are called harmonic functions.

\section*{Integrals}
Let $f(z)$ be defined, single-valued, and continuous in a region $\Re$. We define the integral of of $f(z)$ along some path $C$ in $\Re$ from point $z_{1}$ to point $z_{2}$, where $z_{1}=x_{1}+i y_{1}, z_{2}=x_{2}+i y_{2}$, as

$$
\int_{c} f(z) d z=\int_{\left(x_{1}, y_{1}\right)}^{\left(x_{2}, y_{2}\right)}(u+i v)(d x+i d y)=\int_{\left(x_{1}, y_{1}\right)}^{\left(x_{2}, y_{2}\right)} u d x-v d y+i \int_{\left(x_{1}, y_{1}\right)}^{\left(x_{2}, y_{2}\right)} v d x+u d y
$$

With this definition, the integral of a function of a complex variable can be made to depend on line integrals for real functions already considered in Chapter 10. An alternative definition based on the limit of a sum, as for functions of a real variable, can also be formulated and turns out to be equivalent to the one aforementioned.

The rules for complex integration are similar to those for real integrals. An important result is


\begin{equation*}
\left|\int_{c} f(z) d z\right| \leqq \int_{c}|f(z)||d z| \leqq M \int_{c} d s=M L \tag{4}
\end{equation*}


where $M$ is an upper bound of $|f(z)|$ on $C$; i.e., $|f(z)| \leq M$, and $L$ is the length of the path $C$.

Complex function integral theory is one of the most esthetically pleasing constructions in all of mathematics. Major results are outlined as follows.

\section*{Cauchy's Theorem}
Let $C$ be a simple closed curve. If $f(z)$ is analytic within the region bounded by $C$ as well as on $C$, then we have Cauchy's theorem that


\begin{equation*}
\int_{c} f(z) d z \equiv \oint_{c} f(z) d z=0 \tag{5}
\end{equation*}


where the second integral emphasizes the fact that $C$ is a simple closed curve.

Expressed in another way, Equation (5) is equivalent to the statement that $\int_{z_{1}}^{z_{2}} f(z) d z$ has a value independent of the path joining $z_{1}$ and $z_{2}$. Such integrals can be evaluated as $F\left(z_{2}\right)-F\left(z_{1}\right)$, where $F^{\prime}(z)=f(z)$. These results are similar to corresponding results for line integrals developed in Chapter 10.

EXAMPLE. Since $f(z)=2 z$ is analytic everywhere, we have for any simple closed curve $C$

Also,

$$
\oint_{c} 2 z d z=0
$$

$$
\int_{2 i}^{1+i} 2 z d z=\left.z^{2}\right|_{2 i} ^{1+i}=(1+i)^{2}(2 i)^{2}=2 i+4
$$

\section*{Cauchy's Integral Formulas}
If $f(z)$ is analytic within and on a simple closed curve $C$ and $a$ is any point interior to $C$, then


\begin{equation*}
f(a)=\frac{1}{2 \pi i} \oint_{C z-a} \frac{f(z)}{d z} \tag{6}
\end{equation*}


where $C$ is traversed in the positive (counterclockwise) sense.

Also, the $n$th derivative of $f(z)$ at $z=a$ is given by


\begin{equation*}
f^{(n)}(a)=\frac{n !}{2 \pi i} \oint_{C} \frac{f(z)}{(z-a)^{n+1}} d z \tag{7}
\end{equation*}


These are called Cauchy's integral formulas. They are quite remarkable because they show that if the function $f(z)$ is known on the closed curve $C$ then it is also known within $C$, and the various derivatives at points within $C$ can be calculated. Thus, if a function of a complex variable has a first derivative, it has all higher derivatives as well. This, of course, is not necessarily true for functions of real variables.

\section*{Taylor's Series}
Let $f(z)$ be analytic inside and on a circle having its center at $z=a$. Then for all points $z$ in the circle we have the Taylor series representation of $f(z)$ given by


\begin{equation*}
f(z)=f(a)+f^{\prime}(a)(z-a)+\frac{f^{\prime \prime}(a)}{2 !}(z-a)^{2}+\frac{f^{\prime \prime \prime}(a)}{3 !}(z-a)^{3}+\cdots \tag{8}
\end{equation*}


See Problem 16.21.

\section*{Singular Points}
A singular point of a function $f(z)$ is a value of $z$ at which $f(z)$ fails to be analytic. If $f(z)$ is analytic everywhere in some region except at an interior point $z=a$, we call $z=a$ an isolated singularity of $f(z)$.

EXAMPLE. $f(z)=\frac{1}{(z-3)^{2}}$, then $z=3$ is an isolated singularity of $f(z)$.

EXAMPLE. The function $f(z)=\frac{\sin z}{z}$ has a singularity at $z=0$. Because $\lim _{z \rightarrow 0}$ is finite, this singularity is called a removable singularity.

\section*{Poles}
If $f(z)=\frac{\phi(z)}{(z-a)^{n}}, \phi(a) \neq 0$, where $\phi(z)$ is analytic everywhere in a region including $z=a$, and if $n$ is a positive integer, then $f(z)$ has an isolated singularity at $z=a$, which is called a pole of order $n$. If $n=1$, the pole is often called a simple pole; if $n=2$, it is called a double pole, and so on.

\section*{Laurent's Series}
If $f(z)$ has a pole of order $n$ at $z=a$ but is analytic at every other point inside and on a circle $C$ with center at $a$, then $(z-a)^{n} f(z)$ is analytic at all points inside and on $C$ and has a Taylor series about $z=a$ so that


\begin{equation*}
f(z)=\frac{a_{-n}}{(z-a)^{n}}+\frac{a_{-n+1}}{(z-a)^{n-1}}+\cdots+\frac{a_{-1}}{z-a}+a_{0}+a_{1}(z-a)+a_{2}(z-a)^{2}+\cdots \tag{9}
\end{equation*}


This is called a Laurent series for $f(z)$. The part $a_{0}+a_{1}(z-a)+a_{2}(z-a)^{2}+\ldots$ is called the analytic part. while the remainder consisting of inverse powers of $z-a$ is called the principal part. More generally, we refer to the series $\sum_{k=-\infty}^{\infty} a_{k}(z-a)^{k}$ as a Laurent series, where the terms with $k<0$ constitute the principal part. A function which is analytic in a region bounded by two concentric circles having center at $z=a$ can always be expanded into such a Laurent series (see Problem 16.92).

It is possible to define various types of singularities of a function $f(z)$ from its Laurent series. For example, when the principal part of a Laurent series has a finite number of terms and $a_{-n} \neq 0$ while $a_{-n-1}, a_{-n-2}, \ldots$ are all zero, then $z=a$ is a pole of order $n$. If the principal part has infinitely many terms, $z=a$ is called an $e s-$ sential singularity or sometimes a pole of infinite order.

EXAMPLE. The function $e^{1 / z}=1+\frac{1}{z}+\frac{1}{2 ! z^{2}}+\cdots$ has an essential singularity at $z=0$.

\section*{Branches and Branch Points}
Another type of singularity is a branch point. These points play a vital role in the construction of singlevalued functions from ones that are multiple-valued, and they have an important place in the computation of integrals.

In the study of functions of a real variable, domains were chosen so that functions were single-valued. This guaranteed inverses and removed any ambiguities from differentiation and integration. The applications of complex variables are best served by the approach illustrated here. It is in the realm of real variables and yet illustrates a pattern appropriate to complex variables.

Let $y^{2}=x, x<0$, then $y= \pm \sqrt{x}$. In real variables, two functions $f_{1}$ and $f_{2}$ are described by $y=+\sqrt{x}$ on $x>0$ and $y=-\sqrt{x}$ on $x>0$, respectively. Each of them is single-valued.

An approach that can be extended to complex variables results by defining the positive $x$ axis (not including zero) as a cut in the plane. This creates two branches $f_{1}$ and $f_{2}$ of a new function on a domain called the Riemann axis. The only passage joining the spaces in which the branches $f_{1}$ and $f_{2}$, respectively, are defined is through 0 . This connecting point, zero, is given the special name branch point. Observe that two points $x^{*}$ in the space of $f_{1}$ and $x^{* *}$ in that of $f_{2}$ can appear to be near each other in the ordinary view but, from the Riemannian perspective, are not. (See Figure 16.1.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-420}
\end{center}

Figure 16.1

The preceding real variables construction suggests one for complex variables illustrated by $w=z^{1 / 2}$.

In polar coordinates, $e^{2 \pi i}=1$; therefore, the general representation of $w=z^{1 / 2}$ in that system is $w=\rho^{1 / 2} e^{i(\theta+2 \pi k) / 2}$, $k=0,1$.

Thus, this function is double-valued.

If $k=0$, then $w_{1}=\rho^{1 / 2} \cdot e^{i \theta / 2}, 0<\theta \leq 2 \pi, \rho>0$

If $k=1$, then $w_{2}=\rho^{1 / 2} \cdot e^{i(\theta+2 \pi) / 2}=\rho^{1 / 2} \cdot e^{i \theta / 2} e^{i \pi}=-\rho^{1 / 2}, 2 \pi<\theta \leq 4 \pi, \rho>0$.

Thus, the two branches of $w$ are $w_{1}$ and $w_{2}$, where $w_{1}=-w_{2}$. (The double-valued characteristic of $w$ is illustrated by noticing that as $z$ traverses a circle, $C:|z|=\rho$ through the values $\epsilon$ to $2 \pi$; the functional values run from $\rho^{1 / 2} e^{i \epsilon / 2}$ to $\rho^{1 / 2} e^{\pi i}$. In other words, as $z$ navigates the entire circle, the range variable only moves halfway around the corresponding range circle. In order for that variable to complete the circuit, $z$ would have to make a second revolution. Thus, we would have coincident positions of $z$ giving rise to distinct values of $w$. For example, $z_{1}=e^{(\pi / 2) i}$ are and $z_{2}=e^{(\pi / 2+2 \pi) i}$ are coincident points on on the unit circle. The distinct functional values are $z_{1}^{1 / 2}=\frac{\sqrt{2}}{2}(1+i)$ and $z_{2}^{1 / 2}=-\frac{\sqrt{2}}{2}(1+i)$.

The following abstract construction replaces the multiple-valued function with a new single-valued one.

Make a cut in the complex plane that includes all of the positive $x$ axis except the origin. Think of two planes $P_{1}$ and $P_{2}$, the first one of infinitesimal distance above the complex plane and the other infinitesimally below it. The point 0 which connects these spaces is called a branch point. The planes and the connecting point constitute a Riemann surface, and $w_{1}$ and $w_{2}$ are the branches of the function each defined in one of the planes. (Since the space of complex variables is the complex plane, this Riemann surface may be thought of as a flight of fancy that supports a rigorous analytic construction.)

To visualize this Riemann surface and perceive the single-valued character of the new function in it, first think of duplicates $C_{1}$ and $C_{2}$ of the domain circle $C$ : $|z|=\rho$ in the planes $P_{1}$ and $P_{2}$, respectively. Start at $\theta=\epsilon$ on $C_{1}$, and proceed counterclockwise to the edge $U_{2}$ of the cut of $P_{1}$. (This edge corresponds to $\theta=2 \pi$.) Paste $U_{2}$ to $L_{1}$, the initial edge of the cut on $P_{2}$. Transfer to $P_{2}$ through this join and continue on $C_{2}$. Now, after a complete counterclockwise circuit of $C_{2}$, we reach the edge $L_{2}$ of the cut. Pasting $L_{2}$ to $U_{1}$ provides passage back to $P_{1}$ and makes it possible to close the curve in the Riemann plane. See Figure 16.2.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-421}
\end{center}

Figure 16.2

Note that the function is not continuous on the positive $x$ axis. Also, the cut is somewhat arbitrary. Other rays and even curves extending from the origin to infinity can be employed. In many integration applications the cut $\theta=\pi i$ proves valuable. On the other hand, the branch point ( 0 in this example) is special. If another point, $z_{0} \neq 0$, were chosen as the center of a small circle with radius less than $\left|z_{0}\right|$, then the origin would lie outside it. As a point $z$ traversed its circumference, its argument would return to the original value, as would the value of $w$. However, for any circle that has the branch point as an interior point, a similar traversal of the circumference will change the value of the argument by $2 \pi$, and the values of $w_{1}$ and $w_{2}$ will be interchanged. (See Problem 16.37.)

\section*{Residues}
The coefficients in Equation (9) can be obtained in the customary manner by writing the coefficients for the Taylor series corresponding to $(z-a)^{n} f(z)$. In further developments, the coefficient $a_{-1}$, called the formula residue of $f(z)$ at the pole $z=a$, is of considerable importance. It can be found from the formula


\begin{equation*}
a_{-1}=\lim _{z \rightarrow a} \frac{1}{(n-1) !} \frac{d^{n-1}}{d z^{n-1}}\left\{(z-a)^{n} f(z)\right\} \tag{10}
\end{equation*}


where $n$ is the order of the pole. For simple poles, the calculation of the residue is of particular simplicity since it reduces to


\begin{equation*}
a_{-1}=\lim _{z \rightarrow a}(z-a) f(z) \tag{11}
\end{equation*}


\section*{Residue Theorem}
If $f(z)$ is analytic in a region $\Re$ except for a pole of order $n$ at $z=a$ and if $C$ is any simple closed curve in $\Re$ containing $z=a$, then $f(z)$ has the form of Equation (9). Integrating Equation (9), using the fact that

(see Problem 16.13), it follows that

\[
\oint_{C} \frac{d z}{(z-a)^{n}}= \begin{cases}0 & \text { if } n \neq 1  \tag{12}\\ 2 \pi i & \text { if } n \neq 1\end{cases}
\]


\begin{equation*}
\oint_{C} f(z) d z=2 \pi i a_{-1} \tag{13}
\end{equation*}


i.e., the integral of $f(z)$ around a closed path enclosing a single pole of $f(z)$ is $2 \pi i$ times the residue at the pole.

More generally, we have the following important theorem.

Theorem. If $f(z)$ is analytic within and on the boundary $C$ of a region $\Re$ except at a finite number of poles $a, b, c, \ldots$ within $\Re$, having residues $a_{-1}, b_{-1}, c_{-1}, \ldots$, respectively, then


\begin{equation*}
\oint_{C} f(z) d z=2 \pi i\left(a_{-1}+b_{-1}+c_{-1}+\cdots\right) \tag{14}
\end{equation*}


i.e., the integral of $f(z)$ is $2 \pi i$ times the sum of the residues of $f(z)$ at the poles enclosed by $C$. Cauchy's theorem and integral formulas are special cases of this result, which we call the residue theorem.

\section*{Evaluation of Definite Integrals}
The evaluation of various definite integrals can often be achieved by using the residue theorem together with a suitable function $f(z)$ and a suitable path or contour $C$, the choice of which may require great ingenuity. The following types are most common in practice.

\begin{enumerate}
  \item $\int_{0}^{\infty} F(x) d x, F(x)$ is an even function.
\end{enumerate}

Consider $\oint_{C} F(z) d z$ along a contour $C$ consisting of the line along the $x$ axis from $-R$ to $+R$ and the semicircle above the $x$ axis having this line as diameter. Then let $R \rightarrow \infty$. See Problems 16.29 and 16.30 .\\
2. $\int_{0}^{2 \pi} G(\sin \theta, \cos \theta) d \theta, G$ is a rational function of $\sin \theta$ and $\cos \theta$.

Let $z=e^{i \theta}$. Then $\sin \theta=\frac{z-z^{-1}}{2 i}, \cos \theta=\frac{z+z^{-1}}{2}$ and $d z=i e^{\theta} d \theta$ or $d \theta=d z / i z$. The given integral is equivalent to $\oint_{C} F(z) d z$, where $C$ is the unit circle with center at the origin. See Problems 16.31 and 16.32 .\\
3. $\int_{-\infty}^{\infty} F(x)\left\{\begin{array}{c}\cos m x \\ \sin m x\end{array}\right\} d x, F(x)$ is a rational function.

Here we consider $\oint_{C} F(z) e^{i m z} d z$, where $C$ is the same contour as that in Type 1. See Problem

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Miscellaneous integrals involving particular contours. See Problems 16.35 and 16.38. In particular, Problem 16.38 illustrates a choice of path for an integration about a branch point.
\end{enumerate}

\section*{SOLVED PROBLEMS}
\section*{Functions, limits, continuity}
16.1. Determine the locus represented by (a) $|z-2|=3$, (b) $|z-2|=|z+4|$, and (c) $|z-3|+|z+3|=10$.

(a) Method 1: $|z-2|=|x+i y-2|=|x-2+i y|=\sqrt{(x-2)^{2}+y^{2}}=3$ or $(x-2)^{2}+y^{2}=9$, a circle with center at $(2,0)$ and radius 3 .

Method 2: $\quad|z-2|$ is the distance between the complex numbers $z=x+i y$ and $2+0 i$. If this distance is always 3 , the locus is a circle of radius 3 with center at $2+0 i$ or $(2,0)$.\\
(b) Method 1: $|x+i y-2|=|x+i y+4|=$ or $\sqrt{(x-2)^{2}+y^{2}}=\sqrt{(x+4)^{2}+y^{2}}$, Squaring, we find $\mathrm{x}=-1$, a straight line.

Method 2: The locus is such that the distances from any point on it to $(2,0)$ and $(-4,0)$ are equal. Thus, the locus is the perpendicular bisector of the line joining $(2,0)$ and $(-4,0)$, or $x=-1$.

(c) Method 2: The locus is given by $\sqrt{(x-3)^{2}+y^{2}}+\sqrt{(x+3)^{2}+y^{2}}=10$ or $\sqrt{(x-3)^{2}+y^{2}}$ $=10-\sqrt{(x+3)^{2}+y^{2}}$. Squaring and simplifying, $25+3 x=5 \sqrt{(x+3)^{2}+y^{2}}$. Squaring and simplifying again yields $\frac{x^{2}}{25}+\frac{y^{2}}{16}=1$, an ellipse with semimajor and semiminor axes of lengths 5 and 4 , respectively.

Method 2: The locus is such that the sum of the distances from any point on it to $(3,0)$ and $(-3,0)$ is 10. Thus, the locus is an ellipse whose foci are at $(-3,0)$ and $(3,0)$ and whose major axis has length 10.

16.2. Determine the region in the $z$ plane represented by each of the following.

(a) $|\mathrm{z}|<1$.

Interior of a circle of radius 1. See Figure 16.3(a).

(b) $1<|z+2 i| \leqq 2$.

$|z+2 i|$ is the distance from $z$ to $-2 i$, so that $|z+2 i|=1$ is a circle of radius 1 with center at $-2 i$; Then $1<|z+2 i| \leqq 2$ represents the region exterior to $|z+2 i|=1$ but interior to or on $|z+2 i|=2$. See Figure 16.3(b).

(c) $\pi / 3 \leqq \arg \mathrm{z} \leqq \pi / 2$.

Note that $\arg z=\phi$, where $z=\rho e^{i \phi}$. The required region is the infinite region bounded by the lines $\phi=\pi / 3$ and $\phi=\pi / 2$, including these lines. See Figure 16.3(c).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-423(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-423(2)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-423}
\end{center}

(c)

Figure 16.3

16.3. Express each function in the form $u(x, y)+i v(x, y)$, where $u$ and $v$ are real: (a) $z^{3},(\mathrm{~b}) 1 /(1-z)$, (c) $e^{3 z}$, and (d) $\ln z$.

(a) $w=z^{3}=(x+i y)^{3}=x^{3}+3 x^{2}(i y)+3 x(i y)^{2}+\left(i y^{3}=x^{3}+3 i x^{2} y-3 x y^{2}-i y^{2}\right.$

$$
=x^{3}-3 x y^{2}+i\left(3 x^{2} y-y^{3}\right)
$$

Then $u(x, y)=x^{3}-3 x y^{2}, v(x, y)=3 x 3 x^{2} y-y^{3}$.

(b) $w=\frac{1}{1-z}=\frac{1}{1-(x+i y)}=\frac{1}{1-x-i y} \cdot \frac{1-x+i y}{1-x+i y}=\frac{1-x+i y}{(1-x)^{2}+y^{2}}$

Then $u(x, y)=\frac{1-x}{(1-x)^{2}+y^{2}}, v(x, y)=\frac{y}{(1-x)^{2}+y^{2}}$.\\
(c) $e^{3} z=e^{3}(x+i y)=e^{3} x=e^{3} x e^{3} i y=e^{3} x(\cos 3 y+i \sin 3 y)$ and $u=e^{3} x \cos 3 y, v=e^{3} x \sin 3 y$

(d) $\ln z=\ln \left(\rho \mathrm{e}^{i \phi}\right)=\ln \rho+i \phi=\ln \sqrt{x^{2}+y^{2}}+i \tan ^{-1} y / x$ and

$$
u=\frac{1}{2} \ln \left(x^{2}+y^{2}\right), \quad v=\tan ^{-1} y / x
$$

Note that In $z$ is a multiple-valued function (in this case it is infinitely many-valued), since $\phi$ can be increased by any multiple of $2 \pi$. The principal value of the logarithm is defined as that value for which $0 \leqq \phi$ $<2 \pi$ and is called the principal branch of In $z$.

16.4. Prove (a) $\sin (x+i y)=\sin x \cosh y+i \cos x \sinh y$ and (b) $\cos (x+i y)=\cos x \cosh y-i \sin x \sinh y$.

We use the relations $e^{i x}=\cos z+i \sin z, e^{-i x}=\cos z-i \sin z$, from which

$$
\sin z=\frac{e^{i z}-e^{-i z}}{2 i}, \quad \cos z=\frac{e^{i z}+e^{-i z}}{2}
$$

Then

$$
\begin{aligned}
\sin z & =\sin (x+i y)=\frac{e^{i(x+i y)}-e^{-i(x+i y)}}{2 i}=\frac{e^{i x-y}-e^{-i x+y}}{2 i} \\
& =\frac{1}{2 i}\left\{e^{-y}(\cos x+i \sin x)-e^{y}(\cos x-i \sin x)\right\} \\
& =(\sin x)\left(\frac{e^{y}+e^{-y}}{2}\right)+i(\cos x)\left(\frac{e^{y}-e^{-y}}{2}\right) \\
& =\sin x \cosh y+i \cos x \sinh y
\end{aligned}
$$

Similarly,

$$
\begin{aligned}
\cos z & =\cos (x+i y)=\frac{e^{i(x+i y)}+e^{-i(x+i y)}}{2} \\
& =\frac{1}{2}\left\{e^{i x-y}+e^{-i x+y}\right\}=\frac{1}{2}\left\{e^{-y}(\cos x+i \sin x)+e^{y}(\cos x-i \sin x)\right\} \\
& =(\cos x)\left(\frac{e^{y}+e^{-y}}{2}\right)-i(\sin x)\left(\frac{e^{y}-e^{-y}}{2}\right)=\cos x \cosh y-i \sin x \sinh y
\end{aligned}
$$

\section*{Derivatives, cauchy-riemann equations}
16.5. Prove that $\frac{d}{d z} \bar{z}$, where $\bar{z}$ is the conjugate of $z$, does not exist anywhere.

By definition, $\frac{d}{d z} f(z)=\lim _{\Delta z \rightarrow 0} \frac{f(z+\Delta z)-f(z)}{\Delta z}$ if this limit exists independent of the manner in which $\Delta z=\Delta x+i \Delta y$ approaches zero. Then

$$
\begin{aligned}
\frac{d}{d z} \bar{z} & =\lim _{\Delta z \rightarrow 0} \frac{\overline{z+\Delta z}-\bar{z}}{\Delta z}=\lim _{\substack{\Delta x \rightarrow 0 \\
\Delta y \rightarrow 0}} \frac{\overline{x+i y+\Delta x+i \Delta y}-\overline{x+i y}}{\Delta x+i \Delta y} \\
& =\lim _{\substack{\Delta x \rightarrow 0 \\
\Delta y \rightarrow 0}} \frac{x-i y+\Delta x+i \Delta y-(x-i y)}{\Delta x+i \Delta y}=\lim _{\substack{\Delta x \rightarrow 0 \\
\Delta y \rightarrow 0}} \frac{\Delta x-i \Delta y}{\Delta x+i \Delta y}
\end{aligned}
$$

If $\Delta y=0$, the required limit is $\lim _{\Delta x \rightarrow 0} \frac{\Delta x}{\Delta x}=1$.

If $\Delta x=0$, the required limit is $\lim _{\Delta y \rightarrow 0} \frac{-i \Delta y}{i \Delta y}=-1$.

These two possible approaches show that the limit depends on the manner in which $\Delta z \rightarrow 0$, so that the derivative does not exist; i.e., $\bar{z}$ is nonanalytic anywhere.

16.6.

(a) If $w=f(z)=\frac{1+z}{1-z}$, find $\frac{d w}{d z}$. (b) Determine where $w$ is nonanalytic.

(a) Method 1: $\frac{d w}{d z}=\lim _{\Delta z \rightarrow \infty} \frac{\frac{1+(z+\Delta z)}{1-(z+\Delta z)}-\frac{1+z}{1-z}}{\Delta z}=\lim _{\Delta z \rightarrow 0} \frac{2}{(1-z-\Delta z)(1-z)}$

$$
=\frac{2}{(1-z)^{2}}
$$

provided $z \neq 1$, independent of the manner in which $\Delta z \rightarrow 0$.

Method 2: The usual rules of differentiation apply provided $z \neq 1$. Thus, by the quotient rule for differentiation,

$$
\frac{d}{d z}\left(\frac{1+z}{1-z}\right)=\frac{(1-z) \frac{d}{d z}(1+z)-(1+z) \frac{d}{d z}(1-z)}{(1-z)^{2}}=\frac{(1-z)(1)-(1+z)(-1)}{(1-z)^{2}}=\frac{2}{(1-z)^{2}}
$$

(b) The function is analytic everywhere except at $\mathrm{z}=1$, where the derivative does not exist; i.e., the function is nonanalytic at $\mathrm{z}=1$.

16.7. Prove that a necessary condition for $w=f(z)=u(x, y)+i v(x, y)$ to be analytic in a region is that the CauchyRiemann equations $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}, \frac{\partial u}{\partial x}=-\frac{\partial v}{\partial x}$ be satisfied in the region.

Since $f(z)=f(x+i y)=u(x, y)+i v(x, y)$, we have

$$
f(z+\Delta z)=f[x+\Delta x+i(y+\Delta y)]=u(x+\Delta x, y+\Delta y)+i v(x+\Delta x, y+\Delta y)
$$

Then

$$
\begin{aligned}
& \lim _{\Delta z \rightarrow 0} \frac{f(z+\Delta z)-f(z)}{\Delta z} \\
& =\lim _{\substack{\Delta x \rightarrow 0 \\
\Delta y \rightarrow 0}} \frac{u(x+\Delta x, y+\Delta y)-u(x, y)+i\{v(x+\Delta x, y+\Delta y)-v(x, y)\}}{\Delta x+i \Delta y}
\end{aligned}
$$

If $\Delta y=0$, the required limit is

$$
\lim _{\Delta x \rightarrow 0} \frac{u(x+\Delta x, y)-u(x, y)}{\Delta x}+i\left\{\frac{v(x+\Delta x, y)-v(x, y)}{\Delta x}\right\}=\frac{\partial u}{\partial x}+i \frac{\partial v}{\partial x}
$$

If $\Delta x=0$, the required limit is

$$
\lim _{\Delta y \rightarrow 0} \frac{u(x, y+\Delta y)-u(x, y)}{i \Delta y}+\left\{\frac{v(x, y+\Delta y)-v(x, y)}{\Delta y}\right\}=\frac{1}{i} \frac{\partial u}{\partial y}+\frac{\partial v}{\partial y}
$$

If the derivative is to exist, these two special limits must be equal, i.e.,

$$
\frac{\partial u}{\partial x}+i \frac{\partial v}{\partial x}=\frac{1}{i} \frac{\partial u}{\partial y}+\frac{\partial v}{\partial y}=-1 \frac{\partial u}{\partial y}+\frac{\partial v}{\partial y}
$$

so that we must have $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial x}$ and $\frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}$.

Conversely, we can prove that if the first partial derivatives of $u$ and $v$ with respect to $x$ and $y$ are continuous in a region, then the Cauchy-Riemann equations provide sufficient conditions for $f(z)$ to be analytic.

16.8. (a) If $f(z)=u(x, y)+i v(x, y)$ is analytic in a region $\Re$, prove that the one-parameter families of curves $u(x, y)=C_{1}$ and $v(x, y)=C_{2}$ are orthogonal families. (b) Illustrate by $\operatorname{sing} f(z)=z^{2}$.

(a) Consider any two particular members of these families $u(x, y)=u 0, v(x, y)=v 0$ which intersect at the point (x0, y0).

Since $d u=u_{x} d x+u_{y} d y=0$, we have $\frac{d y}{d x}=\frac{u_{x}}{u_{y}}$.

Also, since $d v=v_{x} d x+v_{y} d y=0, \frac{d y}{d x}=\frac{v_{x}}{v_{y}}$.

When evaluated at $\left(x_{0}, y_{0}\right)$, these represent, respectively, the slopes of the two curves at this point of intersection. equal to

By the Cauchy-Riemann equations, $u_{x}=v_{y}, u_{y}=-v_{x}$, we have the product of the slopes at the point $\left(x_{0}, y_{0}\right)$

$$
\left(-\frac{u_{x}}{u_{y}}\right)\left(-\frac{v_{x}}{v_{y}}\right)=-1
$$

so that any two members of the respective families are orthogonal, and thus the two families are orthogonal.

(b) If $f(z)=z 2$, then $\mathrm{u}=\mathrm{x} 2-\mathrm{y} 2, v=2 \mathrm{xy}$. The graphs of several members of $\mathrm{x} 2-\mathrm{y} 2=\mathrm{C} 1,2 \mathrm{xy}=\mathrm{C} 2$ are shown in Figure 16.4.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-426}
\end{center}

Figure 16.4

16.9. In aerodynamics and fluid mechanics, the functions $\phi$ and $\psi \operatorname{in} f(z)=\phi+i \psi$, where $f(z)$ is analytic, are called the velocity potential and stream function, respectively. If $\phi=x^{2}+4 x-y^{2}+2 y$, (a) find $\psi$ and (b) find $f(z)$.

(a) By the Cauchy-Riemann equations, $\frac{\partial \phi}{\partial x}=\frac{\partial \psi}{\partial y}, \frac{\partial \psi}{\partial x}=-\frac{\partial \phi}{\partial y}$. Then


\begin{align*}
& \frac{\partial \psi}{\partial y}=2 x+4  \tag{1}\\
& \frac{\partial \psi}{\partial x}=2 x-2 \tag{2}
\end{align*}


Method 1: Integrating Equation (1), $\psi=2 x y+4 y+F(x)$.

Integrating Equation (2), $\psi=2 x y-2 x+G(y)$.

These are identical if $F(x)=-2 x+c, G(y)=4 y+c$, where $c$ is a real constant. Thus, $\psi=2 x y+4 y-2 x+c$.

Method 2: Integrating Equation (1), $\psi=2 x y+4 y+F(x)$. Then substituting in Equation (2), $2 y+F^{\prime}(x)=$ $2 y-2$ or $F^{\prime}(x)=-2$ and $F(x)=-2 x+c$. Hence, $\psi=2 x y+4 y-2 x+c$.

(b) From (a),

$$
\begin{aligned}
f(z) & =\phi+\mathrm{i} \psi=\mathrm{x} 2+4 \mathrm{x}-\mathrm{y} 2+2 \mathrm{y}+\mathrm{i}(2 \mathrm{xy}+4 \mathrm{y}-2 \mathrm{x}+\mathrm{c}) \\
& =\left(x^{2}-y^{2}+2 i x y\right)+4(x+i y)-2 i(x+i y)+i c=z^{2}+4 z-2 i z+c_{1}
\end{aligned}
$$

where $c_{1}$ is a pure imaginary constant.

This can also be accomplished by noting that $z=x+i y, \bar{z}=x-i y$ so that $x=\frac{z+\bar{z}}{2}, y=\frac{z-\bar{z}}{2 i}$. The result is then obtained by substitution; the terms involving $\bar{z}$ drop out.

\section*{Integrals, Cauchy's theorem, Cauchy's integral formulas}
16.10. Evaluate $\int_{1+i}^{2+4 i} z^{2} d z$

(a) along the parabola $x=t, y=t^{2}$, where $1 \leqq t \leqq 2$

(b) along the straight line joining $1+i$ and $2+4 i$

(c) along straight lines from $1+i$ to $2+i$ and then to $2+4 i$

We have

$$
\begin{aligned}
\int_{1+i}^{2+4 i} z^{2} d z & =\int_{(1.1)}^{(2.4)}(x+i y)^{2}(d x+i d y)=\int_{(1.1)}^{(2.4)}\left(x^{2}-y^{2}+2 i x y\right)(d x+i d y) \\
& =\int_{(1.1)}^{(2.4)}\left(x^{2}-y^{2}\right) d x-2 x y d y+i \int_{(1.1)}^{(2.4)} 2 x y d x+\left(x^{2}-y^{2}\right) d y
\end{aligned}
$$

\section*{Method 1:}
(a) The points $(1,1)$ and $(2,4)$ correspond to $t=1$ and $t=2$, respectively. Then the preceding line integrals become

$$
\int_{t=1}^{2}\left\{\left(t^{2}-t^{4}\right) d t-2(t)(t)^{2} 2 t d t\right\}+i \int_{t=1}^{2}\left\{2(t)\left(t^{2}\right) d t+\left(t^{2}-t^{4}\right)(2 t) d t\right\}=-\frac{86}{3}-6 i
$$

(b) The line joining $(1,1)$ and $(2,4)$ has the equation $y-1=\frac{4-1}{2-1}(x-1)$ or $y=3 x-2$. Then we find

$$
\begin{gathered}
\int_{x=1}^{2}\left\{\left[x^{2}-(3 x-2)^{2}\right] d x-2 x(3 x-2) 3 d x\right\} \\
+i \int_{x=1}^{2}\left\{2 x(3 x-2) d x+\left[x^{2}-(3 x-2)^{2}\right] 3 d x\right\}=-\frac{86}{3}-6 i
\end{gathered}
$$

(c) From $1+i$ to $2+i$ [or $(1,1)$ to $(2,1)], y=1, d y=0$ and we have

$$
\int_{x=1}^{2}\left(x^{2}-1\right) d x+i \int_{x=1}^{2} 2 x d x=\frac{4}{3}+3 i
$$

From $2+i$ to $2+4 i$ [or $(2,1)$ to $(2,4)], x=2, d x=0$ and we have

$$
\int_{y=1}^{4}-4 y d y+i \int_{y=1}^{4}\left(4-y^{2}\right) d y=-30-9 i
$$

Adding, $\left(\frac{4}{3}+3 i\right)+(-30-91)=-\frac{86}{3}-6 i$.

Method 2: By the methods of Chapter 10 it is seen that the line integrals are independent of the path, thus accounting for the same values obtained in (a), (b), and (c). In such case the integral can be evaluated directly, as for real variables, as follows:

$$
\int_{1+i}^{2+4 i} z^{2} d z=\left.\frac{z^{3}}{3}\right|_{1 i} ^{2+4 i}=\frac{(2+4 i)^{3}}{3}-\frac{(1+i)^{3}}{3}=-\frac{86}{3}-6 i
$$

16.11. (a) Prove Cauchy's theorem: If $f(z)$ is analytic inside and on a simple closed curve $C$, then $\oint_{C} f(z) d z=0$.

(b) Under these conditions prove that $\int_{p_{1}}^{p_{2}} f(z) d z$ is independent of the path joining $P_{1}$ and $P_{2}$.

(a) $\oint_{C} f(z) d z=\oint_{C}(u+i v)(d x+i d y)=\oint_{C} u d x-v d y+i \oint_{C} v d x+u d y$

By Green's theorem (Chapter 10),

$\oint_{C} u d x-v d y=\iint_{\Re}\left(-\frac{\partial u}{\partial x}-\frac{\partial v}{\partial y}\right) d x d y, \quad \oint_{C} v d x+u d y=\iint_{\Re}\left(\frac{\partial u}{\partial x}-\frac{\partial v}{\partial y}\right) d x d y$

where $\Re$ is the region (simply-connected) bounded by $C$.

Since $f(z)$ is analytic, $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}, \frac{\partial v}{\partial x}=-\frac{\partial u}{\partial y}$ (Problem 16.7), and so these integrals are zero. Then $\oint_{C} f(z) d z=0$, assuming $f^{\prime}(z)$ (and, thus, the partial derivatives) to be continuous.

(b) Consider any two paths joining points $P_{1}$ and $P_{2}$ (see Figure 16.5).

By Cauchy's theorem,

Then

$$
\int_{P_{1} A P_{2} B P_{1}} f(z) d z=0
$$

$$
\int_{P_{1} A P_{2}} f(z) d z+\int_{P_{2} B P_{1}} f(z) d z=0
$$

or

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-428}
\end{center}

Figure 16.5

$$
\int_{P_{1} A P_{2}} f(z) d z=-\int_{P_{2} B P_{1}} f(z) d z=\int_{P_{1} B P_{2}} f(z) d z
$$

i.e., the integral along $P_{1} A P_{2}$ (path 1) = integral along $P_{1} B P_{2}$ (path 2), and so the integral is independent of the path joining $P_{1}$ and $P_{2}$.

This explains the results of Problem 16.10, since $f(z)=z^{2}$ is analytic.

16.12. If $f(z)$ is analytic within and on the boundary of a region bounded by two closed curves $C_{1}$ and $C_{2}$ (see Figure 16.6), prove that

$$
\oint_{C_{1}} f(z) d z=\oint_{C_{2}} f(z) d z
$$

As in Figure 16.6, construct line $A B$ (called a crosscut) connecting any point on $C_{2}$ and a point on $C_{1}$. By Cauchy's theorem (Problem 16.11),

$$
\int_{A Q P A B R S T B A} f(z) d z=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-428(1)}
\end{center}

Figure 16.6

since $f(z)$ is analytic within the region shaded and also on the boundary. Then


\begin{equation*}
\int_{A Q P A} f(z) d z+\int_{A B} f(z) d z+\int_{B R S T B} f(z) d z+\int_{B A} f(z) d z=0 \tag{1}
\end{equation*}


But $\int_{A B} f(z) d z=-\int_{B A} f(z) d z$. Hence, (1) gives

$$
\int_{A Q P A} f(z) d z=-\int_{B R S T B} f(z) d z=\int_{B T S R B} f(z) d z
$$

i.e.,

$$
\oint_{C_{1}} f(z) d z=\oint_{C_{2}} f(z) d z
$$

Note that $f(z)$ need not be analytic within curve $C_{2}$.

16.13. (a) Prove that $\oint_{C} \frac{d z}{(z-a)^{n}}=\left\{\begin{array}{ll}2 \pi i & \text { if } n=1 \\ 0 & \text { if } n=2,3,4, \ldots\end{array}\right.$, where $C$ is a simple closed curve bounding a region having $z=a$ as interior point. (b) What is the value of the integral if $n=0,-1,-2,-3, \ldots$ ?

(a) Let $C_{1}$ be a circle of radius $\epsilon$ having center at $\mathrm{z}=\mathrm{a}$ (see Figure 16.7). Since $(z-a)^{-n}$ is analytic within and on the boundary of the region bounded by $C$ and $C_{1}$, we have, by Problem 16.12,

$$
\oint_{C} \frac{d z}{(z-a)^{n}}=\oint_{C_{1}} \frac{d z}{(z-a)^{n}}
$$

To evaluate this last integral, note that on $C_{1},|z-a|=\varepsilon$ or $z-a=\varepsilon e^{i \theta}$ and $d z=i \varepsilon e^{i \theta} d \theta$. The integral equals

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-429}
\end{center}

Figure 16.7

$$
\int_{0}^{2 \pi} \frac{i \varepsilon e^{i \theta} d \theta}{\varepsilon^{n} \varepsilon^{i n \theta}}=\frac{i}{\varepsilon^{n-1}} \int_{0}^{2 \pi} e^{(1-n) i \theta} d \theta=\left.\frac{i}{\varepsilon^{n-1}} \frac{e^{(1-n) i \theta}}{(1-n) i}\right|_{0} ^{2 \pi}=0 \quad \text { if } n \neq 1
$$

If $n=1$, the integral equals $i \int_{0}^{2 \pi} d \theta=2 \pi i$.

(b) For $n=0,-1,-2, \ldots$, the integrand is $1,(z-a),(z-a)^{2}, \ldots$ and is analytic everywhere inside $C_{1}$, including $z=a$. Hence, by Cauchy's theorem, the integral is zero.

16.14. Evaluate $\oint_{C} \frac{d z}{z-3}$, where $C$ is (a) the circle $|z|=1$ and (b) the circle $|z+i|=4$.

(a) Since $\mathrm{z}=3$ is not interior to $|z|=1$, the integral equals zero (Problem 16.11).

(b) Since $\mathrm{z}=3$ is interior to $|z+i|=4$, the integral equals $2 \pi i$ (Problem 16.13).

16.15. If $f(z)$ is analytic inside and on a simple closed curve $C$, and $a$ is any point within $C$, prove that

$$
f(a)=\frac{1}{2 \pi i} \oint_{C} \frac{f(z)}{z-a} d z
$$

Referring to Problem 16.12 and Figure 16.7, we have

$$
\oint_{C} \frac{f(z)}{z-a} d z=\oint_{C_{1}} \frac{f(z)}{z-a} d z
$$

Letting $z-a=\epsilon e^{i \theta}$, the last integral becomes $i \int_{0}^{2 \pi} f\left(a+\varepsilon e^{i \theta}\right) d \theta$. But since $f(z)$ is analytic, it is continuous. Hence,

$$
\lim _{\varepsilon \rightarrow 0} \int_{0}^{2 \pi} f\left(a+\varepsilon e^{i \theta}\right) d \theta=i \int_{0}^{2 \pi} \lim _{\varepsilon \rightarrow 0} f\left(a+\varepsilon e^{i \theta}\right) d \theta=i \int_{0}^{2 \pi} f(a) d \theta=2 \pi i f(a)
$$

and the required result follows.

16.16. Evaluate (a) $\oint_{C} \frac{\cos z}{z-\pi} d z$ and (b) $\oint_{C} \frac{e^{x}}{z(z+1)} d z$, where $C$ is the circle $|z-1|=3$.

(a) Since $z=\pi$ lies within $C, \frac{1}{2 \pi i} \oint_{C} \frac{\cos z}{z-\pi} d z=\cos \pi=-1$ by Problem 16.15 with $f(z)=\cos \mathrm{z}, \mathrm{a}=\pi$.

Then $\oint_{C} \frac{\cos z}{z-\pi} d z=-2 \pi i$

(b) $\oint_{C} \frac{e^{z}}{z(z+1)} d z=\oint_{C} e^{z}\left(\frac{1}{z}-\frac{1}{z+1}\right) d z=\oint_{C} \frac{e^{z}}{z} d z-\oint_{C} \frac{e^{z}}{z+1} d z$

$$
=2 \pi i e^{0}-2 \pi i e^{-1}=2 \pi i\left(1-e^{-1}\right)
$$

by Problem 16.15, since $z=0$ and $z=-1$ are both interior to $C$.

16.17. Evaluate $\oint_{C} \frac{5 z^{2}-3 z+2}{(z-1)^{3}} d z$, where $C$ is any simple closed curve enclosing $z=1$.

Method 1: By Cauchy's integral formula, $f^{(n)}(a)=\frac{n !}{2 \pi i} \oint_{C} \frac{f(z)}{(z-a)^{n+1}} d z$.

If $n=2$ and $f(z)=5 z^{2}-3 z+2$, then $f^{\prime \prime}(1)=10$. Hence,

$$
10=\frac{2 !}{2 \pi i} \oint_{C} \frac{5 z^{2}-3 z+2}{(z-1)^{3}} d z \quad \text { or } \quad \oint_{C} \frac{5 z^{2}-3 z+2}{(z-1)^{3}} d z=10 \pi i
$$

Method 2: $\quad 5 z^{2}-3 z+2=5(z-1)^{2}+7(z-1)+4$. Then

$$
\begin{aligned}
\oint_{C} \frac{5 z^{2}-3 z+2}{(z-1)^{3}} d z & =\oint_{C} \frac{5(z-1)^{2}+7(z-1)+4}{(z-1)^{3}} d z \\
& =5 \oint_{C} \frac{d}{z-1}+7 \oint_{C} \frac{d z}{(z-1)^{2}}+4 \oint_{C} \frac{d z}{(z-1)^{3}}=5(2 \pi i)+7(0)+4(0) \\
& =10 \pi i
\end{aligned}
$$

by Problem 16.13.

\section*{Series and singularities}
16.18. For what values of $z$ does each series converge?

(a) $\sum_{n=1}^{\infty} \frac{z^{n}}{n^{2} 2^{n}}$. The $n$th term $=u_{n}=\frac{z^{n}}{n^{2} 2^{n}}$. Then

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{z^{n+1}}{(n+1)^{2} 2^{n+1}} \cdot \frac{n^{2} 2^{n}}{z^{n}}\right|=\frac{|z|}{2}
$$

By the ratio test, the series converges if $|z|<2$ and diverges if $|z|>2$. If $|z|=2$, the ratio test fails. converges.

$\begin{aligned} & \text { However, the series of absolute values } \\ & \text { verges. }\end{aligned} \sum_{n=1}^{\infty}\left|\frac{z^{n}}{n^{2} 2^{n}}\right|=\sum_{n=1}^{\infty} \frac{\left|z^{n}\right|}{n^{2} 2^{n}}$ converges if $|z|=2$, since $\sum_{n=1}^{\infty} \frac{1}{n^{2}}$

Thus, the series converges (absolutely) for $|z| \leqq 2$, i.e., at all points inside and on the circle $|z|=2$

(b) $\sum_{n=1}^{\infty} \frac{(-1)^{n-1} z^{2 n-1}}{(2 n-1) !}=z-\frac{z^{3}}{3 !}+\frac{z^{5}}{5 !}-\cdots$. We have

$$
\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(-1)^{n} z^{2 n+1}}{(2 n+1) !} \cdot \frac{(2 n-1) !}{(-1)^{n-1} z^{2 n-1}}\right|=\lim _{n \rightarrow \infty}\left|\frac{-z^{2}}{2 n(2 n+1)}\right|=0
$$

Then the series, which represents $\sin z$, converges for all values of $z$.

(c) $\sum_{n=1}^{\infty} \frac{(z-i)^{n}}{3^{n}}$. We have $\lim _{n \rightarrow \infty}\left|\frac{u_{n+1}}{u_{n}}\right|=\lim _{n \rightarrow \infty}\left|\frac{(z-i)^{n+1}}{3^{n+1}} \cdot \frac{3 n}{(z-i)^{n}}\right|=\frac{|z-i|}{3}$.

The series converges if $|z-i|<3$, and diverges if $|z-i|>3$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-430}
\end{center}

Thus, the series converges within the circle $|z-i|=3$ but not on the boundary.

16.19. If $\sum_{n=0}^{\infty} a_{n} z^{n}$ is absolutely convergent for $|z| \leqq R$, show that it is uniformly convergent for these values of $z$.

The definitions, theorems, and proofs for series of complex numbers are analogous to those for real series.

In this case we have $\left|a_{n} z^{n}\right| \leqq\left|a_{n}\right| R^{n}=M_{n}$. Since, by hypothesis, $\sum_{n=1}^{\infty} M_{n}$ converges, it follows by the Weierstrass $M$ test that $\sum_{n=0}^{\infty} a_{n} z^{n}$ converges uniformly for $|z| \leqq R$.

16.20. Locate in the finite $z$ plane all the singularities, if any, of each function and name them.

(a) $\frac{z^{2}}{(z+1)^{3}} . \quad z=-1$ is a pole of order 3 .

(b) $\frac{2 z^{3}-z+1}{(z-4)^{2}(z-i)(z-1+2 i)} \cdot z=4$ is a pole of order 2 (double pole); $z=i$ and $z=1-2 i$ are poles of order 1 (simple poles).

(c) $\frac{\sin m z}{z^{2}+2 z+2}, m \neq 0$. Since $z^{2}+2 z+2=0$ when $z=\frac{-2 \pm \sqrt{4-8}}{2}=\frac{-2 \pm 2 i}{2}=1 \pm i$, we can write

$z^{2}+2 z+2=\{z-(-1+i)\}\{z-(-1-i)\}=(z+1-i)(z+1+i)$.

The function has the two simple poles: $z=-1+i$ and $z=-1-i$.

(d) $\frac{1-\cos z}{z} \cdot z=0$ appears to be a singularity. However, since $\lim _{x \rightarrow 0} \frac{1-\cos z}{z}=0$, singularity. Another method: Since $\frac{1-\cos z}{z}=\frac{1}{z}\left\{1-\left(\frac{z^{2}}{2 !}+\frac{z^{4}}{4 !}-\frac{z^{6}}{6 !}+\cdots\right)\right\}=\frac{z}{2 !}-\frac{z^{3}}{4 !}+\cdots$, we see that $z=0$\\
is a removable singularity.

(e) $e^{-1 /(x-1) 2}=1-\frac{1}{(z-1)^{2}}+\frac{1}{2 !(z-1)^{4}}-\cdots$. This is a Laurent series where the principal part has an infinite number of nonzero terms. Then $z=1$ is an essential singularity.

(f) $e^{z}$. This function has no finite singularity. However, letting $z=1 / u$, we obtain $e^{1 / u}$, which has an essential singularity at $u=0$. We conclude that $z=\infty$ is an essential singularity of $e^{z}$.

In general, to determine the nature of a possible singularity of $f(z)$ at $z=\infty$, we let $z=\infty$, we let $z=1 / u$ and then examine the behavior of the new function at $u=0$.

16.21. If $f(z)$ is analytic at all points inside and on a circle of radius $R$ with center at $a$, and if $a+h$ is any point inside $C$, prove Taylor's theorem that

$$
f(a+h)=f(a)+h f^{\prime}(a)+\frac{h^{2}}{2 !} f^{\prime \prime}(a)+\frac{h^{3}}{3 !} f^{\prime \prime \prime}(\alpha)+\cdots
$$

By Cauchy's integral formula (Problem 16.15), we have


\begin{equation*}
f(a+h)=\frac{1}{2 \pi i} \oint_{C z-a-h} \frac{f(z) d z}{(a)} \tag{1}
\end{equation*}


By division,


\begin{align*}
\frac{1}{z-a-h} & =\frac{1}{(z-a)[1-h /(z-a)]} \\
& =\frac{1}{(z-a)}\left\{1+\frac{h}{(z-a)}+\frac{h^{2}}{(z-a)^{2}}+\cdots+\frac{h^{n}}{(z-a)^{n}}+\frac{h^{n+1}}{(z-a)^{n}(z-a-h)}\right\} \tag{2}
\end{align*}


Substituting Equation (2) in Equation (1) and using Cauchy's integral formulas, we have

$$
\begin{aligned}
f(a+h) & =\frac{1}{2 \pi i} \oint_{C} \frac{f(z) d z}{z-a}+\frac{h}{2 \pi i} \oint_{C} \frac{f(z) d z}{(z-a)^{2}}+\cdots+\frac{h^{n}}{2 \pi i} \oint_{C} \frac{f(z) d z}{(z-a)^{n+1}}+R_{n} \\
& =f(a)+h f^{\prime}(a)+\frac{h^{2}}{2 !} f^{\prime \prime}(a)+\cdots+\frac{h^{n}}{n !} f^{(n)}(a)+R_{n}
\end{aligned}
$$

where

$$
R_{n}=\frac{h^{n+1}}{2 \pi i} \oint_{C} \frac{f(z) d z}{(z-a)^{n+1}(z-a-h)}
$$

Now when $z$ is on $C,\left|\frac{f(z)}{z-a-h}\right| \leqq M$ and $|z-a|=R$, so that by Equation (4), Page 407, we have, since $2 \pi R$ is the length of $C$,

$$
\left|R_{n}\right| \leqq \frac{|h|^{n+1} M}{2 \pi R^{n+1}} \cdot 2 \pi R
$$

As $n \rightarrow \infty,\left|R_{n}\right| \rightarrow 0$. Then $R_{n} \rightarrow 0$, and the required result follows.

If $f(z)$ is analytic in an annular region $r_{1} \leqq|z-a| \leqq r_{2}$, we can generalize the Taylor series to a Laurent series (see Problem 16.92). In some cases, as shown in Problem 16.22, the Laurent series can be obtained by use of known Taylor series.

16.22. Find Laurent series about the indicated singularity for each of the following functions. Name the singularity in each case and give the region of convergence of each series.

(a) $\frac{e^{z}}{(z-1)^{2}} ; z=1 . \quad$ Let $z-1=u$. Then $z=1+u$ and

$$
\begin{aligned}
\frac{e^{z}}{(z-1)^{2}} & =\frac{e^{1+u}}{u^{2}}=e \cdot \frac{e^{u}}{u^{2}}=\frac{e}{u^{2}}\left\{1+u+\frac{u^{2}}{2 !}+\frac{u^{3}}{3 !}+\frac{u^{4}}{4 !}+\cdots\right\} \\
& =\frac{e}{(z-1)^{2}}+\frac{e}{z-1}+\frac{e}{2 !}+\frac{e(z-1)}{3 !}+\frac{e(z-1)^{2}}{4 !}+\cdots
\end{aligned}
$$

$z=1$ is a pole of order 2 , or double pole.

The series converges for all values of $z \neq 1$.

(b) $z \cos \frac{1}{z} ; z=0$.

$$
z \cos \frac{1}{z}=z\left(1-\frac{1}{2 ! z^{2}}+\frac{1}{4 ! z^{4}}-\frac{1}{6 ! z^{6}}+\cdots\right)=z-\frac{1}{2 ! z}+\frac{1}{4 ! z^{3}}-\frac{1}{6 ! z^{5}}+\cdots
$$

$z=0$ is an essential singularity.

The series converges for all values of $z \neq 0$.

(c) $\frac{\sin z}{z-\pi} ; z=\pi . \quad$ Let $z-\pi=u$. Then $z=\pi+u$ and

$$
\begin{aligned}
\frac{\sin z}{z-\pi} & =\frac{\sin (u+\pi)}{u}=-\frac{\sin u}{u}=-\frac{1}{u}\left(u-\frac{u^{3}}{3 !}+\frac{u^{5}}{5 !}-\cdots\right) \\
& =-1+\frac{u^{2}}{3 !}+\frac{u^{4}}{5 !}+\cdots=-1+\frac{(z-\pi)^{2}}{3 !}-\frac{(z-\pi)^{4}}{5 !}+\cdots
\end{aligned}
$$

$z=\pi$ is a removable singularity.

The series converges for all values of $z$.

(d) $\frac{z}{(z+1)(z+2)} ; z=-1 . \quad$ Let $z+1=u$. Then

$$
\begin{aligned}
\frac{z}{(z+1)(z+2)} & =\frac{u-1}{u(u-1)}=\frac{u-1}{u}\left(1-u+u^{2}-u^{3}+u^{4}-\cdots\right) \\
& =-\frac{1}{u}+2-2 u+2 u^{2}-2 u^{3}+\cdots \\
& =-\frac{1}{z+1}+2-2(z+1)+2(z+1)^{2}-\cdots
\end{aligned}
$$

$z=-1$ is a pole of order 1 , or simple pole.

The series converges for values of $z$ such that $0<|z+1|<1$.

(e) $\frac{1}{z(z+2)^{3}} ; z=0,-2$.

Case $1, z=0$. Using the binomial theorem,

$$
\begin{aligned}
\frac{1}{z(z+2)^{3}} & =\frac{1}{8 z(1+z / 2)^{3}}=\frac{1}{8 z}\left\{1+(-3)\left(\frac{z}{2}\right)+\frac{(-3)(-4)}{2 !}\left(\frac{z}{2}\right)^{2}+\frac{(-3)(-4)(-5)}{3 !}\left(\frac{z}{2}\right)^{3}+\cdots\right\} \\
& =\frac{1}{8 z}-\frac{3}{16}+\frac{3}{16} z-\frac{5}{32} z^{2}+\cdots
\end{aligned}
$$

$z=0$ is a pole of order 1 , or simple pole.

The series converges for $0<|z|<2$.

Case 2, $z=-2$. Let $z+2=u$. Then

$$
\begin{aligned}
\frac{1}{z(z+2)^{3}} & =\frac{1}{(u-2) u^{3}}=\frac{1}{-2 u^{3}(1-u / 2)}=-\frac{1}{2 u^{3}}\left\{1+\frac{u}{2}+\left(\frac{u}{2}\right)^{2}+\left(\frac{u}{2}\right)^{3}+\left(\frac{u}{2}\right)^{4}+\cdots\right\} \\
& =-\frac{1}{2 u^{3}}-\frac{1}{4 u^{2}}-\frac{1}{8 u}-\frac{1}{16}-\frac{1}{32} u-\cdots \\
& =-\frac{1}{2(z+2)^{3}}-\frac{1}{4(z+2)^{2}}--\frac{1}{8(z+2)}-\frac{1}{16}-\frac{1}{32}(z+2)-\cdots
\end{aligned}
$$

$z=-2$ is a pole of order 3 .

The series converges for $0<|z+2|<2$.

\section*{Residues and the residue theorem}
16.23. Suppose $f(z)$ is analytic everywhere inside and on a simple closed curve $C$ except at $z=a$, which is a pole of order $n$. Then

$$
f(z)=\frac{a_{-n}}{(z-a)^{n}}+\frac{a_{-n+1}}{(z-a)^{n-1}}+\cdots+a_{0}+a_{1}(z-a)+a_{2}(z-a)^{2}+\cdots
$$

where $a_{-n} \neq 0$. Prove that

(a) $\oint_{C} f(z) d z=2 \pi i a_{-1}$

(b) $a_{-1}=\lim _{z \rightarrow a} \frac{1}{(n-1) !} \frac{d^{n-1}}{d z^{n-1}}\left\{(z-a)^{n} f(z)\right\}$

(a) By integration, we have, on using Problem 16.13,

$$
\begin{aligned}
\oint_{C} f(z) d z & =\oint_{C} \frac{a_{-n}}{(z-a)^{n}} d z+\cdots+\oint_{C} \frac{a_{-1}}{z-a} d z+\oint_{C}\left\{a_{0}+a_{1}(z-a)+a_{2}(z-a)^{2}+\cdots\right) d z \\
& =2 \pi i a_{-1}
\end{aligned}
$$

Since only the term involving $a_{-1}$ remains, we call $a_{-1}$ the residue of $f(z)$ at the pole $z=a$.

(b) Multiplication by $(z-a)^{n}$ gives the Taylor series

$$
(z-a)^{n} f(z)=a_{-n}+a_{-n+1}(z-a)+\cdots+a_{-1}(z-a)^{n-1}+\cdots
$$

Taking the $(n-1)$ st derivative of both sides and letting $z \rightarrow a$, we find

$$
(n-1) ! a_{-1}=\lim _{z \rightarrow a} \frac{d^{n-1}}{d z^{n-1}}\left\{(z-a)^{n} f(z)\right\}
$$

from which the required result follows.

16.24. Determine the residues of each function at the indicated poles.

(a) $\frac{z^{2}}{(z-2)\left(z^{2}+1\right)} ; 2, i,-i$. These are simple poles. Then:

Residue at $z=2$ is $\lim _{z \rightarrow 2}(z-2)\left\{\frac{z^{2}}{(z-2)\left(z^{2}+1\right)}\right\}=\frac{4}{5}$

Residue at $z=i$ is $\quad \lim _{z \rightarrow i}(z-i)\left\{\frac{z^{2}}{(z-2)(z-i)(z+i)}\right\}=\frac{i^{2}}{(i-2)(2 i)}=\frac{1-2 i}{10}$

Residue at $z=-i$ is $\lim _{z \rightarrow-i}(z+i)\left\{\frac{z^{2}}{(z-2)(z-i)(z+i)}\right\}=\frac{i^{2}}{(-i-2)(-2 i)}=\frac{1+2 i}{10}$

(b) $\frac{1}{z(z+2)^{3}} ; z=0,-2 \cdot z=0$ is a simple pole, $z=-2$ is a pole of order 3 . Then:

$$
\begin{array}{ll}
\text { Residue at } z=0 \text { is } & \lim _{z \rightarrow 0} z \cdot \frac{1}{z(z+2)^{3}}=\frac{1}{8} \\
\text { Residue at } z=-2 \text { is } & \lim _{z \rightarrow-2} \frac{1}{2 !} \frac{d^{2}}{d z^{2}}\left\{(z+2)^{3} \cdot \frac{1}{z(z+2)^{3}}\right\} \\
& =\lim _{z \rightarrow-2} \frac{1}{2} \frac{d^{2}}{d z^{2}}\left(\frac{1}{z}\right)=\lim _{z \rightarrow-2} \frac{1}{2}\left(\frac{2}{z^{3}}\right)=-\frac{1}{8}
\end{array}
$$

Note that these residues can also be obtained from the coefficients of $1 / z$ and $1 /(z+2)$ in the respective Laurent series [see Problem 16.22(e)].

(c) $\frac{z e^{z t}}{(z-3)^{2}} ; z=3$, a pole of order 2 or double pole. Then:

(d) $\cot z ; z=5 \pi$, a pole of order 1 . Then:

Residue is $\lim _{z \rightarrow 3} \frac{d}{d z}\left\{(z-3)^{2} \cdot \frac{z e^{z t}}{(z-3)^{2}}\right\}=\lim _{z \rightarrow 3} \frac{d}{d z}\left(z e^{z t}\right)=\lim _{z \rightarrow 3}\left(e^{z t}+z t e^{z t}\right)$

$$
=e^{3 t}+3 t e^{3 t}
$$

Residue is $\lim _{z \rightarrow 5 \pi}(z-5 \pi) \cdot \frac{\cos z}{\sin z}=\left(\lim _{z \rightarrow 5 \pi} \frac{z-5 \pi}{\sin z}\right)\left(\lim _{z \rightarrow 5 \pi} \cos z\right)=\left(\lim _{z \rightarrow 5 \pi} \frac{1}{\cos z}\right)(-1)$

where we have used L'Hospital's rule, which can be shown to be applicable for functions of a complex variable.

16.25. If $f(z)$ is analytic within and on a simple closed curve $C$ except at a number of poles $a, b, c, \ldots$ interior to $C$, prove that

$$
\left.\oint_{C} f(z) d z=2 \pi i \text { \{sum of residues of } f(z) \text { at poles } a, b, c, \text { etc. }\right\}
$$

Refer to Figure 16.8 .

By reasoning similar to that of Problem 16.12 (i.e., by constructing crosscuts from $C$ to $C_{1}, C_{2}, C_{3}$, etc.), we have

$$
\oint_{C} f(z) d z=\oint_{C_{1}} f(z) d z+\oint_{C_{2}} f(z) d z+\cdots
$$

For pole $a$,

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-434}
\end{center}

Figure 16.8

$f(z) \frac{a_{-m}}{(z-a)^{m}}+\cdots+\frac{a_{-1}}{(z-a)}+a_{0}+a_{1}(z-a)+\cdots$\\
hence, as in Problem 16.23,

$$
\oint_{C_{1}} f(z) d z=2 \pi i a_{-1}
$$

Similarly for pole $b$,

$$
f(z)=\frac{b_{-n}}{(z-b)^{n}}+\cdots+\frac{b_{-1}}{(z-b)}+b_{0}+b_{1}(z-b)+\cdots
$$

so that

$$
\oint_{C_{2}} f(z) d z=2 \pi i b_{-1}
$$

Continuing in this manner, we see that

$$
\oint_{C_{2}} f(z) d z=2 \pi i\left(a_{-1}+b_{-1}+\cdots\right)=2 \pi i \text { (sum of residues) }
$$

16.26. Evaluate $\oint_{C} \frac{e^{z} d z}{(z-1)(z+3)^{2}}$, where $C$ is given by (a) $|z|=3 / 2$ and (b) $|z|=10$.

Residue at simple pole $z=1$ is $\lim _{z \rightarrow 1}\left\{(z-1) \frac{e^{2}}{(z-1)(z+3)^{2}}\right\}=\frac{e}{16}$

Residue at double pole $z=-3$ is $\lim _{z \rightarrow-3} \frac{d}{d z}\left\{(z+3)^{2} \frac{e^{z}}{(z-1)(z+3)^{2}}\right\}=\lim _{z \rightarrow-3} \frac{(z-1) e^{z}-e^{z}}{(z-1)^{2}}=\frac{-5 e^{-3}}{16}$

(a) Since $|z|=3 / 2$ encloses only the pole $z=1$,

$$
\text { the required integral }=2 \pi i\left(\frac{e}{16}\right)=\frac{\pi i e}{8}
$$

(b) Since $|z|=10$ encloses both poles $z=1$ and $z=-3$,

$$
\text { the required integral }=2 \pi i\left(\frac{e}{16}-\frac{5 e^{-3}}{16}\right)=\frac{\pi i\left(e-5 e^{-3}\right)}{8}
$$

\section*{Evaluation of definite integrals}
16.27. If $|f(z)| \leqq \frac{M}{R^{k}}$ for $z=\operatorname{Re}^{i \theta}$, where $k>1$ and $M$ are constants, prove that $\lim _{R \rightarrow \infty} \int_{\Gamma} f(z) d z=0$, where $\Gamma$ is the semicircular arc of radius $R$ shown in Figure 16.9.

By the result (4), Page 407, we have

$$
\left|\int_{\Gamma} f(z) d z\right| \leqq \int_{\Gamma}|f(z)||d z| \leqq \frac{M}{R^{k}} \cdot \pi R+\frac{\pi M}{R^{k-1}}
$$

since the length of $\operatorname{arc} L=\pi R$. Then

$$
\lim _{R \rightarrow \infty}\left|\int^{\Gamma} f(z) d z\right|=0
$$

and so

$$
\lim _{R \rightarrow \infty} \int_{\Gamma} f(z) d z=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_ffb6ac533fe0a53b3ceeg-435}
\end{center}

Figure 16.9

16.28. Show that for $\mathrm{z}=\operatorname{Re}^{i \theta},|f(z)| \leqq \frac{M}{R^{k}}, k>1$ if $f(z)=\frac{1}{1+z^{4}}$.

$$
\text { If } z=\operatorname{Re}^{i \theta},|f(z)|=\left|\frac{1}{1+R^{4} e^{4 i \theta}}\right| \leqq \frac{1}{\left|R^{4} e^{4 i \theta}\right|-1}=\frac{1}{R^{4}-1} \leqq \frac{2}{R^{4}} \text { if } R \text { is large enough }(R>2 \text {, for ex- }
$$

ample) so that $M=2, k=4$.


\end{document}