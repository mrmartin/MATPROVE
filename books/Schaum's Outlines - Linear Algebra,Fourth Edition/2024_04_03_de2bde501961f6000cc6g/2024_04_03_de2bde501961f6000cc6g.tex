\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{mathrsfs}

\begin{document}
\section*{Vectors in $\mathbf{R}^{n}$ and $\mathbf{C}^{\boldsymbol{n}}$, Spatial Vectors}
\subsection*{1.1 Introduction}
There are two ways to motivate the notion of a vector: one is by means of lists of numbers and subscripts, and the other is by means of certain objects in physics. We discuss these two ways below.

Here we assume the reader is familiar with the elementary properties of the field of real numbers, denoted by $\mathbf{R}$. On the other hand, we will review properties of the field of complex numbers, denoted by C. In the context of vectors, the elements of our number fields are called scalars.

Although we will restrict ourselves in this chapter to vectors whose elements come from $\mathbf{R}$ and then from $\mathbf{C}$, many of our operations also apply to vectors whose entries come from some arbitrary field $K$.

\section*{Lists of Numbers}
Suppose the weights (in pounds) of eight students are listed as follows:

$156, \quad 125, \quad 145, \quad 134, \quad 178, \quad 145, \quad 162, \quad 193$

One can denote all the values in the list using only one symbol, say $w$, but with different subscripts; that is,

$w_{1}, \quad w_{2}, w_{3}, \quad w_{4}, \quad w_{5}, \quad w_{6}, \quad w_{7}, \quad w_{8}$

Observe that each subscript denotes the position of the value in the list. For example,

$w_{1}=156$, the first number, $w_{2}=125$, the second number, $\ldots$

Such a list of values,

$$
w=\left(w_{1}, w_{2}, w_{3}, \ldots, w_{8}\right)
$$

is called a linear array or vector.

\section*{Vectors in Physics}
Many physical quantities, such as temperature and speed, possess only "magnitude." These quantities can be represented by real numbers and are called scalars. On the other hand, there are also quantities, such as force and velocity, that possess both "magnitude" and "direction." These quantities, which can be represented by arrows having appropriate lengths and directions and emanating from some given reference point $O$, are called vectors.

Now we assume the reader is familiar with the space $\mathbf{R}^{3}$ where all the points in space are represented by ordered triples of real numbers. Suppose the origin of the axes in $\mathbf{R}^{3}$ is chosen as the reference point $O$ for the vectors discussed above. Then every vector is uniquely determined by the coordinates of its endpoint, and vice versa.

There are two important operations, vector addition and scalar multiplication, associated with vectors in physics. The definition of these operations and the relationship between these operations and the endpoints of the vectors are as follows.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-009(1)}
\end{center}

(a) Vector Addition

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-009}
\end{center}

(b) Scalar Multiplication

Figure 1-1

(i) Vector Addition: The resultant $\mathbf{u}+\mathbf{v}$ of two vectors $\mathbf{u}$ and $\mathbf{v}$ is obtained by the parallelogram law; that is, $\mathbf{u}+\mathbf{v}$ is the diagonal of the parallelogram formed by $\mathbf{u}$ and $\mathbf{v}$. Furthermore, if $(a, b, c)$ and $\left(a^{\prime}, b^{\prime}, c^{\prime}\right)$ are the endpoints of the vectors $\mathbf{u}$ and $\mathbf{v}$, then $\left(a+a^{\prime}, b+b^{\prime}, c+c^{\prime}\right)$ is the endpoint of the vector $\mathbf{u}+\mathbf{v}$. These properties are pictured in Fig. 1-1(a).

(ii) Scalar Multiplication: The product $k \mathbf{u}$ of a vector $\mathbf{u}$ by a real number $k$ is obtained by multiplying the magnitude of $\mathbf{u}$ by $k$ and retaining the same direction if $k>0$ or the opposite direction if $k<0$. Also, if $(a, b, c)$ is the endpoint of the vector $\mathbf{u}$, then $(k a, k b, k c)$ is the endpoint of the vector $k \mathbf{u}$. These properties are pictured in Fig. 1-1(b).

Mathematically, we identify the vector $\mathbf{u}$ with its $(a, b, c)$ and write $\mathbf{u}=(a, b, c)$. Moreover, we call the ordered triple $(a, b, c)$ of real numbers a point or vector depending upon its interpretation. We generalize this notion and call an $n$-tuple $\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ of real numbers a vector. However, special notation may be used for the vectors in $\mathbf{R}^{3}$ called spatial vectors (Section 1.6).

\subsection*{1.2 Vectors in $\mathbf{R}^{n}$}
The set of all $n$-tuples of real numbers, denoted by $\mathbf{R}^{n}$, is called $n$-space. A particular $n$-tuple in $\mathbf{R}^{n}$, say

$$
u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)
$$

is called a point or vector. The numbers $a_{i}$ are called the coordinates, components, entries, or elements of $u$. Moreover, when discussing the space $\mathbf{R}^{n}$, we use the term scalar for the elements of $\mathbf{R}$.

Two vectors, $u$ and $v$, are equal, written $u=v$, if they have the same number of components and if the corresponding components are equal. Although the vectors $(1,2,3)$ and $(2,3,1)$ contain the same three numbers, these vectors are not equal because corresponding entries are not equal.

The vector $(0,0, \ldots, 0)$ whose entries are all 0 is called the zero vector and is usually denoted by 0 .

\section*{EXAMPLE 1.1}
(a) The following are vectors:

$$
(2,-5), \quad(7,9), \quad(0,0,0), \quad(3,4,5)
$$

The first two vectors belong to $\mathbf{R}^{2}$, whereas the last two belong to $\mathbf{R}^{3}$. The third is the zero vector in $\mathbf{R}^{3}$.

(b) Find $x, y, z$ such that $(x-y, x+y, z-1)=(4,2,3)$.

By definition of equality of vectors, corresponding entries must be equal. Thus,

$$
x-y=4, \quad x+y=2, \quad z-1=3
$$

Solving the above system of equations yields $x=3, y=-1, z=4$.

\section*{Column Vectors}
Sometimes a vector in $n$-space $\mathbf{R}^{n}$ is written vertically rather than horizontally. Such a vector is called a column vector, and, in this context, the horizontally written vectors in Example 1.1 are called row vectors. For example, the following are column vectors with $2,2,3$, and 3 components, respectively:

$$
\left[\begin{array}{l}
1 \\
2
\end{array}\right], \quad\left[\begin{array}{r}
3 \\
-4
\end{array}\right], \quad\left[\begin{array}{r}
1 \\
5 \\
-6
\end{array}\right], \quad\left[\begin{array}{r}
1.5 \\
\frac{2}{3} \\
-15
\end{array}\right]
$$

We also note that any operation defined for row vectors is defined analogously for column vectors.

\subsection*{1.3 Vector Addition and Scalar Multiplication}
Consider two vectors $u$ and $v$ in $\mathbf{R}^{n}$, say

$$
u=\left(a_{1}, a_{2}, \ldots, a_{n}\right) \quad \text { and } \quad v=\left(b_{1}, b_{2}, \ldots, b_{n}\right)
$$

Their sum, written $u+v$, is the vector obtained by adding corresponding components from $u$ and $v$. That is,

$$
u+v=\left(a_{1}+b_{1}, a_{2}+b_{2}, \ldots, a_{n}+b_{n}\right)
$$

The scalar product or, simply, product, of the vector $u$ by a real number $k$, written $k u$, is the vector obtained by multiplying each component of $u$ by $k$. That is,

$$
k u=k\left(a_{1}, a_{2}, \ldots, a_{n}\right)=\left(k a_{1}, k a_{2}, \ldots, k a_{n}\right)
$$

Observe that $u+v$ and $k u$ are also vectors in $\mathbf{R}^{n}$. The sum of vectors with different numbers of components is not defined.

Negatives and subtraction are defined in $\mathbf{R}^{n}$ as follows:

$$
-u=(-1) u \quad \text { and } \quad u-v=u+(-v)
$$

The vector $-u$ is called the negative of $u$, and $u-v$ is called the difference of $u$ and $v$.

Now suppose we are given vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $\mathbf{R}^{n}$ and scalars $k_{1}, k_{2}, \ldots, k_{m}$ in $\mathbf{R}$. We can multiply the vectors by the corresponding scalars and then add the resultant scalar products to form the vector

$$
v=k_{1} u_{1}+k_{2} u_{2}+k_{3} u_{3}+\cdots+k_{m} u_{m}
$$

Such a vector $v$ is called a linear combination of the vectors $u_{1}, u_{2}, \ldots, u_{m}$.

\section*{EXAMPLE 1.2}
(a) Let $u=(2,4,-5)$ and $v=(1,-6,9)$. Then

$$
\begin{aligned}
u+v & =(2+1,4+(-5),-5+9)=(3,-1,4) \\
7 u & =(7(2), 7(4), 7(-5))=(14,28,-35) \\
-v & =(-1)(1,-6,9)=(-1,6,-9) \\
3 u-5 v & =(6,12,-15)+(-5,30,-45)=(1,42,-60)
\end{aligned}
$$

(b) The zero vector $0=(0,0, \ldots, 0)$ in $\mathbf{R}^{n}$ is similar to the scalar 0 in that, for any vector $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$.

$$
u+0=\left(a_{1}+0, a_{2}+0, \ldots, a_{n}+0\right)=\left(a_{1}, a_{2}, \ldots, a_{n}\right)=u
$$

(c) Let $u=\left[\begin{array}{r}2 \\ 3 \\ -4\end{array}\right]$ and $v=\left[\begin{array}{r}3 \\ -1 \\ -2\end{array}\right]$. Then $2 u-3 v=\left[\begin{array}{r}4 \\ 6 \\ -8\end{array}\right]+\left[\begin{array}{r}-9 \\ 3 \\ 6\end{array}\right]=\left[\begin{array}{r}-5 \\ 9 \\ -2\end{array}\right]$.

Basic properties of vectors under the operations of vector addition and scalar multiplication are described in the following theorem.

THEOREM 1.1: For any vectors $u, v, w$ in $\mathbf{R}^{n}$ and any scalars $k, k^{\prime}$ in $\mathbf{R}$,

$$
\begin{aligned}
& \text { (i) }(u+v)+w=u+(v+w) \\
& \text { (v) } \quad k(u+v)=k u+k v \text {, } \\
& \text { (ii) } u+0=u \text {, } \\
& \text { (vi) }\left(k+k^{\prime}\right) u=k u+k^{\prime} u \text {, } \\
& \text { (iii) } u+(-u)=0 \text {, } \\
& \text { (vii) } \quad\left(\mathrm{kk}^{\prime}\right) \mathrm{u}=\mathrm{k}\left(\mathrm{k}^{\prime} \mathrm{u}\right) \\
& \text { (iv) } u+v=v+u \text {, } \\
& \text { (viii) } 1 u=u \text {. }
\end{aligned}
$$

We postpone the proof of Theorem 1.1 until Chapter 2, where it appears in the context of matrices (Problem 2.3).

Suppose $u$ and $v$ are vectors in $\mathbf{R}^{n}$ for which $u=k v$ for some nonzero scalar $k$ in $\mathbf{R}$. Then $u$ is called a multiple of $v$. Also, $u$ is said to be in the same or opposite direction as $v$ according to whether $k>0$ or $k<0$.

\subsection*{1.4 Dot (Inner) Product}
Consider arbitrary vectors $u$ and $v$ in $\mathbf{R}^{n}$; say,

$$
u=\left(a_{1}, a_{2}, \ldots, a_{n}\right) \quad \text { and } \quad v=\left(b_{1}, b_{2}, \ldots, b_{n}\right)
$$

The dot product or inner product or scalar product of $u$ and $v$ is denoted and defined by

$$
u \cdot v=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}
$$

That is, $u \cdot v$ is obtained by multiplying corresponding components and adding the resulting products. The vectors $u$ and $v$ are said to be orthogonal (or perpendicular) if their dot product is zero - that is, if $u \cdot v=0$.

\section*{EXAMPLE 1.3}
(a) Let $u=(1,-2,3), v=(4,5,-1)$, $w=(2,7,4)$. Then,

$$
\begin{aligned}
& u \cdot v=1(4)-2(5)+3(-1)=4-10-3=-9 \\
& u \cdot w=2-14+12=0, \quad v \cdot w=8+35-4=39
\end{aligned}
$$

Thus, $u$ and $w$ are orthogonal.

(b) Let $u=\left[\begin{array}{r}2 \\ 3 \\ -4\end{array}\right]$ and $v=\left[\begin{array}{r}3 \\ -1 \\ -2\end{array}\right]$. Then $u \cdot v=6-3+8=11$.

(c) Suppose $u=(1,2,3,4)$ and $v=(6, k,-8,2)$. Find $k$ so that $u$ and $v$ are orthogonal.

First obtain $u \cdot v=6+2 k-24+8=-10+2 k$. Then set $u \cdot v=0$ and solve for $k$ :

$$
-10+2 k=0 \quad \text { or } \quad 2 k=10 \quad \text { or } \quad k=5
$$

Basic properties of the dot product in $\mathbf{R}^{n}$ (proved in Problem 1.13) follow.

THEOREM 1.2: For any vectors $u, v, w$ in $\mathbf{R}^{n}$ and any scalar $k$ in $\mathbf{R}$ :

$$
\begin{aligned}
& \text { (i) } \quad(u+v) \cdot w=u \cdot w+v \cdot w, \quad \text { (iii) } \quad u \cdot v=v \cdot u \text {, } \\
& \text { (ii) } \quad(k u) \cdot v=k(u \cdot v), \quad \text { (iv) } u \cdot u \geq 0 \text {, and } u \cdot u=0 \text { iff } u=0 \text {. }
\end{aligned}
$$

Note that (ii) says that we can "take $k$ out" from the first position in an inner product. By (iii) and (ii),

$$
u \cdot(k v)=(k v) \cdot u=k(v \cdot u)=k(u \cdot v)
$$

That is, we can also "take $k$ out" from the second position in an inner product.

The space $\mathbf{R}^{n}$ with the above operations of vector addition, scalar multiplication, and dot product is usually called Euclidean n-space.

\section*{Norm (Length) of a Vector}
The norm or length of a vector $u$ in $\mathbf{R}^{n}$, denoted by $\|u\|$, is defined to be the nonnegative square root of $u \cdot u$. In particular, if $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$, then

$$
\|u\|=\sqrt{u \cdot u}=\sqrt{a_{1}^{2}+a_{2}^{2}+\cdots+a_{n}^{2}}
$$

That is, $\|u\|$ is the square root of the sum of the squares of the components of $u$. Thus, $\|u\| \geq 0$, and $\|u\|=0$ if and only if $u=0$.

A vector $u$ is called a unit vector if $\|u\|=1$ or, equivalently, if $u \cdot u=1$. For any nonzero vector $v$ in $\mathbf{R}^{n}$, the vector

$$
\hat{v}=\frac{1}{\|v\|} v=\frac{v}{\|v\|}
$$

is the unique unit vector in the same direction as $v$. The process of finding $\hat{v}$ from $v$ is called normalizing $v$.

\section*{EXAMPLE 1.4}
(a) Suppose $u=(1,-2,-4,5,3)$. To find $\|u\|$, we can first find $\|u\|^{2}=u \cdot u$ by squaring each component of $u$ and adding, as follows:

$$
\|u\|^{2}=1^{2}+(-2)^{2}+(-4)^{2}+5^{2}+3^{2}=1+4+16+25+9=55
$$

Then $\|u\|=\sqrt{55}$.

(b) Let $v=(1,-3,4,2)$ and $w=\left(\frac{1}{2},-\frac{1}{6}, \frac{5}{6}, \frac{1}{6}\right)$. Then

$$
\|v\|=\sqrt{1+9+16+4}=\sqrt{30} \quad \text { and } \quad\|w\|=\sqrt{\frac{9}{36}+\frac{1}{36}+\frac{25}{36}+\frac{1}{36}}=\sqrt{\frac{36}{36}}=\sqrt{1}=1
$$

Thus $w$ is a unit vector, but $v$ is not a unit vector. However, we can normalize $v$ as follows:

$$
\hat{v}=\frac{v}{\|v\|}=\left(\frac{1}{\sqrt{30}}, \frac{-3}{\sqrt{30}}, \frac{4}{\sqrt{30}}, \frac{2}{\sqrt{30}}\right)
$$

This is the unique unit vector in the same direction as $v$.

The following formula (proved in Problem 1.14) is known as the Schwarz inequality or CauchySchwarz inequality. It is used in many branches of mathematics.

THEOREM 1.3 (Schwarz): For any vectors $u, v$ in $\mathbf{R}^{n},|u \cdot v| \leq\|u\|\|v\|$.

Using the above inequality, we also prove (Problem 1.15) the following result known as the "triangle inequality" or Minkowski's inequality.

THEOREM 1.4 (Minkowski): For any vectors $u, v$ in $\mathbf{R}^{n},\|u+v\| \leq\|u\|+\|v\|$.

\section*{Distance, Angles, Projections}
The distance between vectors $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ and $v=\left(b_{1}, b_{2}, \ldots, b_{n}\right)$ in $\mathbf{R}^{n}$ is denoted and defined by

$$
d(u, v)=\|u-v\|=\sqrt{\left(a_{1}-b_{1}\right)^{2}+\left(a_{2}-b_{2}\right)^{2}+\cdots+\left(a_{n}-b_{n}\right)^{2}}
$$

One can show that this definition agrees with the usual notion of distance in the Euclidean plane $\mathbf{R}^{2}$ or space $\mathbf{R}^{3}$.

The angle $\theta$ between nonzero vectors $u, v$ in $\mathbf{R}^{n}$ is defined by

$$
\cos \theta=\frac{u \cdot v}{\|u\|\|v\|}
$$

This definition is well defined, because, by the Schwarz inequality (Theorem 1.3),

$$
-1 \leq \frac{u \cdot v}{\|u\|\|v\|} \leq 1
$$

Note that if $u \cdot v=0$, then $\theta=90^{\circ}$ (or $\theta=\pi / 2$ ). This then agrees with our previous definition of orthogonality.

The projection of a vector $u$ onto a nonzero vector $v$ is the vector denoted and defined by

$$
\operatorname{proj}(u, v)=\frac{u \cdot v}{\|v\|^{2}} v=\frac{u \cdot v}{v \cdot v} v
$$

We show below that this agrees with the usual notion of vector projection in physics.

\section*{EXAMPLE 1.5}
(a) Suppose $u=(1,-2,3)$ and $v=(2,4,5)$. Then

$$
d(u, v)=\sqrt{(1-2)^{2}+(-2-4)^{2}+(3-5)^{2}}=\sqrt{1+36+4}=\sqrt{41}
$$

To find $\cos \theta$, where $\theta$ is the angle between $u$ and $v$, we first find

$$
u \cdot v=2-8+15=9, \quad\|u\|^{2}=1+4+9=14, \quad\|v\|^{2}=4+16+25=45
$$

Then

$$
\cos \theta=\frac{u \cdot v}{\|u\|\|v\|}=\frac{9}{\sqrt{14} \sqrt{45}}
$$

Also,

$$
\operatorname{proj}(u, v)=\frac{u \cdot v}{\|v\|^{2}} v=\frac{9}{45}(2,4,5)=\frac{1}{5}(2,4,5)=\left(\frac{2}{5}, \frac{4}{5}, 1\right)
$$

(b) Consider the vectors $u$ and $v$ in Fig. 1-2(a) (with respective endpoints $A$ and $B$ ). The (perpendicular) projection of $u$ onto $v$ is the vector $u^{*}$ with magnitude

$$
\left\|u^{*}\right\|=\|u\| \cos \theta=\|u\| \frac{u \cdot v}{\|u\| v \|}=\frac{u \cdot v}{\|v\|}
$$

To obtain $u^{*}$, we multiply its magnitude by the unit vector in the direction of $v$, obtaining

$$
u^{*}=\left\|u^{*}\right\| \frac{v}{\|v\|}=\frac{u \cdot v}{\|v\|} \frac{v}{\|v\|}=\frac{u \cdot v}{\|v\|^{2}} v
$$

This is the same as the above definition of $\operatorname{proj}(u, v)$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-013}
\end{center}

Projection $u^{*}$ of $u$ onto $v$

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-013(1)}
\end{center}

(b)

\subsection*{1.5 Located Vectors, Hyperplanes, Lines, Curves in $\mathbf{R}^{\boldsymbol{n}}$}
This section distinguishes between an $n$-tuple $P\left(a_{i}\right) \equiv P\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ viewed as a point in $\mathbf{R}^{n}$ and an $n$-tuple $u=\left[c_{1}, c_{2}, \ldots, c_{n}\right]$ viewed as a vector (arrow) from the origin $O$ to the point $C\left(c_{1}, c_{2}, \ldots, c_{n}\right)$.

\section*{Located Vectors}
Any pair of points $A\left(a_{i}\right)$ and $B\left(b_{i}\right)$ in $\mathbf{R}^{n}$ defines the located vector or directed line segment from $A$ to $B$, written $\overrightarrow{A B}$. We identify $\overrightarrow{A B}$ with the vector

$$
u=B-A=\left[b_{1}-a_{1}, b_{2}-a_{2}, \ldots, b_{n}-a_{n}\right]
$$

because $\overrightarrow{A B}$ and $u$ have the same magnitude and direction. This is pictured in Fig. 1-2(b) for the points $A\left(a_{1}, a_{2}, a_{3}\right)$ and $B\left(b_{1}, b_{2}, b_{3}\right)$ in $\mathbf{R}^{3}$ and the vector $u=B-A$ which has the endpoint $P\left(b_{1}-a_{1}, b_{2}-a_{2}, b_{3}-a_{3}\right)$.

\section*{Hyperplanes}
A hyperplane $H$ in $\mathbf{R}^{n}$ is the set of points $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ that satisfy a linear equation

$$
a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b
$$

where the vector $u=\left[a_{1}, a_{2}, \ldots, a_{n}\right]$ of coefficients is not zero. Thus a hyperplane $H$ in $\mathbf{R}^{2}$ is a line, and a hyperplane $H$ in $\mathbf{R}^{3}$ is a plane. We show below, as pictured in Fig. 1-3(a) for $\mathbf{R}^{3}$, that $u$ is orthogonal to any directed line segment $\overrightarrow{P Q}$, where $P\left(p_{i}\right)$ and $Q\left(q_{i}\right)$ are points in $H$. [For this reason, we say that $u$ is normal to $H$ and that $H$ is normal to $u$.]

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-014(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-014}
\end{center}

(b)

Figure 1-3

Because $P\left(p_{i}\right)$ and $Q\left(q_{i}\right)$ belong to $H$, they satisfy the above hyperplane equation - that is,

$$
\begin{aligned}
& \quad a_{1} p_{1}+a_{2} p_{2}+\cdots+a_{n} p_{n}=b \text { and } a_{1} q_{1}+a_{2} q_{2}+\cdots+a_{n} q_{n}=b \\
& \text { Let } \quad v=\overrightarrow{P Q}=Q-P=\left[q_{1}-p_{1}, q_{2}-p_{2}, \ldots, q_{n}-p_{n}\right]
\end{aligned}
$$

Then

$$
\begin{aligned}
u \cdot v & =a_{1}\left(q_{1}-p_{1}\right)+a_{2}\left(q_{2}-p_{2}\right)+\cdots+a_{n}\left(q_{n}-p_{n}\right) \\
& =\left(a_{1} q_{1}+a_{2} q_{2}+\cdots+a_{n} q_{n}\right)-\left(a_{1} p_{1}+a_{2} p_{2}+\cdots+a_{n} p_{n}\right)=b-b=0
\end{aligned}
$$

Thus $v=\overrightarrow{P Q}$ is orthogonal to $u$, as claimed.

\section*{Lines in $\mathbf{R}^{\boldsymbol{n}}$}
The line $L$ in $\mathbf{R}^{n}$ passing through the point $P\left(b_{1}, b_{2}, \ldots, b_{n}\right)$ and in the direction of a nonzero vector $u=\left[a_{1}, a_{2}, \ldots, a_{n}\right]$ consists of the points $X\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ that satisfy

$$
X=P+t u \quad \text { or } \quad\left\{\begin{array}{l}
x_{1}=a_{1} t+b_{1} \\
x_{2}=a_{2} t+b_{2} \\
\cdots \ldots \ldots \ldots \ldots \ldots \\
x_{n}=a_{n} t+b_{n}
\end{array} \quad \text { or } L(t)=\left(a_{i} t+b_{i}\right)\right.
$$

where the parameter $t$ takes on all real values. Such a line $L$ in $\mathbf{R}^{3}$ is pictured in Fig. 1-3(b).

\section*{EXAMPLE 1.6}
(a) Let $H$ be the plane in $\mathbf{R}^{3}$ corresponding to the linear equation $2 x-5 y+7 z=4$. Observe that $P(1,1,1)$ and $Q(5,4,2)$ are solutions of the equation. Thus $P$ and $Q$ and the directed line segment

$$
v=\overrightarrow{P Q}=Q-P=[5-1,4-1,2-1]=[4,3,1]
$$

lie on the plane $H$. The vector $u=[2,-5,7]$ is normal to $H$, and, as expected,

$$
u \cdot v=[2,-5,7] \cdot[4,3,1]=8-15+7=0
$$

That is, $u$ is orthogonal to $v$.

(b) Find an equation of the hyperplane $H$ in $\mathbf{R}^{4}$ that passes through the point $P(1,3,-4,2)$ and is normal to the vector $u=[4,-2,5,6]$.

The coefficients of the unknowns of an equation of $H$ are the components of the normal vector $u$; hence, the equation of $H$ must be of the form

$$
4 x_{1}-2 x_{2}+5 x_{3}+6 x_{4}=k
$$

Substituting $P$ into this equation, we obtain

$$
4(1)-2(3)+5(-4)+6(2)=k \quad \text { or } \quad 4-6-20+12=k \quad \text { or } \quad k=-10
$$

Thus, $4 x_{1}-2 x_{2}+5 x_{3}+6 x_{4}=-10$ is the equation of $H$.

(c) Find the parametric representation of the line $L$ in $\mathbf{R}^{4}$ passing through the point $P(1,2,3,-4)$ and in the direction of $u=[5,6,-7,8]$. Also, find the point $Q$ on $L$ when $t=1$.

Substitution in the above equation for $L$ yields the following parametric representation:

$$
x_{1}=5 t+1, \quad x_{2}=6 t+2, \quad x_{3}=-7 t+3, \quad x_{4}=8 t-4
$$

or, equivalently,

$$
L(t)=(5 t+1,6 t+2,-7 t+3,8 t-4)
$$

Note that $t=0$ yields the point $P$ on $L$. Substitution of $t=1$ yields the point $Q(6,8,-4,4)$ on $L$.

\section*{Curves in $\mathbf{R}^{\boldsymbol{n}}$}
Let $D$ be an interval (finite or infinite) on the real line $\mathbf{R}$. A continuous function $F: D \rightarrow \mathbf{R}^{n}$ is a curve in $\mathbf{R}^{n}$. Thus, to each point $t \in D$ there is assigned the following point in $\mathbf{R}^{n}$ :

$$
F(t)=\left[F_{1}(t), F_{2}(t), \ldots, F_{n}(t)\right]
$$

Moreover, the derivative (if it exists) of $F(t)$ yields the vector

$$
V(t)=\frac{d F(t)}{d t}=\left[\frac{d F_{1}(t)}{d t}, \frac{d F_{2}(t)}{d t}, \ldots, \frac{d F_{n}(t)}{d t}\right]
$$

which is tangent to the curve. Normalizing $V(t)$ yields

$$
\mathbf{T}(t)=\frac{V(t)}{\|V(t)\|}
$$

Thus, $\mathbf{T}(t)$ is the unit tangent vector to the curve. (Unit vectors with geometrical significance are often presented in bold type.)

EXAMPLE 1.7 Consider the curve $F(t)=[\sin t, \cos t, t]$ in $\mathbf{R}^{3}$. Taking the derivative of $F(t)$ [or each component of $F(t)]$ yields

$$
V(t)=[\cos t,-\sin t, 1]
$$

which is a vector tangent to the curve. We normalize $V(t)$. First we obtain

$$
\|V(t)\|^{2}=\cos ^{2} t+\sin ^{2} t+1=1+1=2
$$

Then the unit tangent vection $\mathbf{T}(t)$ to the curve follows:

$$
\mathbf{T}(t)=\frac{V(t)}{\|V(t)\|}=\left[\frac{\cos t}{\sqrt{2}}, \frac{-\sin t}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right]
$$

\subsection*{1.6 Vectors in $R^{3}$ (Spatial Vectors), ijk Notation}
Vectors in $\mathbf{R}^{3}$, called spatial vectors, appear in many applications, especially in physics. In fact, a special notation is frequently used for such vectors as follows:

$\mathbf{i}=[1,0,0]$ denotes the unit vector in the $x$ direction.

$\mathbf{j}=[0,1,0]$ denotes the unit vector in the $y$ direction.

$\mathbf{k}=[0,0,1]$ denotes the unit vector in the $z$ direction.

Then any vector $u=[a, b, c]$ in $\mathbf{R}^{3}$ can be expressed uniquely in the form

$$
u=[a, b, c]=a \mathbf{i}+b \mathbf{j}+c \mathbf{j}
$$

Because the vectors $\mathbf{i}, \mathbf{j}, \mathbf{k}$ are unit vectors and are mutually orthogonal, we obtain the following dot products:

$$
\mathbf{i} \cdot \mathbf{i}=1, \quad \mathbf{j} \cdot \mathbf{j}=1, \quad \mathbf{k} \cdot \mathbf{k}=1 \quad \text { and } \quad \mathbf{i} \cdot \mathbf{j}=0, \quad \mathbf{i} \cdot \mathbf{k}=0, \quad \mathbf{j} \cdot \mathbf{k}=0
$$

Furthermore, the vector operations discussed above may be expressed in the ijk notation as follows. Suppose

$$
u=a_{1} \mathbf{i}+a_{2} \mathbf{j}+a_{3} \mathbf{k} \quad \text { and } \quad v=b_{1} \mathbf{i}+b_{2} \mathbf{j}+b_{3} \mathbf{k}
$$

Then

$$
u+v=\left(a_{1}+b_{1}\right) \mathbf{i}+\left(a_{2}+b_{2}\right) \mathbf{j}+\left(a_{3}+b_{3}\right) \mathbf{k} \quad \text { and } \quad c u=c a_{1} \mathbf{i}+c a_{2} \mathbf{j}+c a_{3} \mathbf{k}
$$

where $c$ is a scalar. Also,

$$
u \cdot v=a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3} \quad \text { and } \quad\|u\|=\sqrt{u \cdot u}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}
$$

EXAMPLE 1.8 Suppose $u=3 \mathbf{i}+5 \mathbf{j}-2 \mathbf{k}$ and $v=4 \mathbf{i}-8 \mathbf{j}+7 \mathbf{k}$.

(a) To find $u+v$, add corresponding components, obtaining $u+v=7 \mathbf{i}-3 \mathbf{j}+5 \mathbf{k}$

(b) To find $3 u-2 v$, first multiply by the scalars and then add:

$$
3 u-2 v=(9 \mathbf{i}+13 \mathbf{j}-6 \mathbf{k})+(-8 \mathbf{i}+16 \mathbf{j}-14 \mathbf{k})=\mathbf{i}+29 \mathbf{j}-20 \mathbf{k}
$$

(c) To find $u \cdot v$, multiply corresponding components and then add:

$$
u \cdot v=12-40-14=-42
$$

(d) To find $\|u\|$, take the square root of the sum of the squares of the components:

$$
\|u\|=\sqrt{9+25+4}=\sqrt{38}
$$

\section*{Cross Product}
There is a special operation for vectors $u$ and $v$ in $\mathbf{R}^{3}$ that is not defined in $\mathbf{R}^{n}$ for $n \neq 3$. This operation is called the cross product and is denoted by $u \times v$. One way to easily remember the formula for $u \times v$ is to use the determinant (of order two) and its negative, which are denoted and defined as follows:

$$
\left|\begin{array}{ll}
a & b \\
c & d
\end{array}\right|=a d-b c \quad \text { and } \quad-\left|\begin{array}{ll}
a & b \\
c & d
\end{array}\right|=b c-a d
$$

Here $a$ and $d$ are called the diagonal elements and $b$ and $c$ are the nondiagonal elements. Thus, the determinant is the product $a d$ of the diagonal elements minus the product $b c$ of the nondiagonal elements, but vice versa for the negative of the determinant.

Now suppose $u=a_{1} \mathbf{i}+a_{2} \mathbf{j}+a_{3} \mathbf{k}$ and $v=b_{1} \mathbf{i}+b_{2} \mathbf{j}+b_{3} \mathbf{k}$. Then

$$
\begin{aligned}
u \times v & =\left(a_{2} b_{3}-a_{3} b_{2}\right) \mathbf{i}+\left(a_{3} b_{1}-a_{1} b_{3}\right) \mathbf{j}+\left(a_{1} b_{2}-a_{2} b_{1}\right) \mathbf{k} \\
& =\left|\begin{array}{lll}
a_{1} & a_{2} & a_{3} \\
b_{1} & b_{2} & b_{3}
\end{array}\right| \mathbf{i}-\left|\begin{array}{lll}
a_{1} & a_{2} & a_{3} \\
b_{1} & b_{2} & b_{3}
\end{array}\right| \mathbf{j}+\left|\begin{array}{lll}
a_{1} & a_{2} & a_{3} \\
b_{1} & b_{2} & b_{3}
\end{array}\right| \mathbf{i}
\end{aligned}
$$

That is, the three components of $u \times v$ are obtained from the array

$$
\left[\begin{array}{lll}
a_{1} & a_{2} & a_{3} \\
b_{1} & b_{2} & b_{3}
\end{array}\right]
$$

(which contain the components of $u$ above the component of $v$ ) as follows:

(1) Cover the first column and take the determinant.

(2) Cover the second column and take the negative of the determinant.

(3) Cover the third column and take the determinant.

Note that $u \times v$ is a vector; hence, $u \times v$ is also called the vector product or outer product of $u$ and $v$.

EXAMPLE 1.9 Find $u \times v$ where: (a) $u=4 \mathbf{i}+3 \mathbf{j}+6 \mathbf{k}, v=2 \mathbf{i}+5 \mathbf{j}-3 \mathbf{k}$, (b) $u=[2,-1,5], v=[3,7,6]$.

(a) Use $\left[\begin{array}{rrr}4 & 3 & 6 \\ 2 & 5 & -3\end{array}\right]$ to get $u \times v=(-9-30) \mathbf{i}+(12+12) \mathbf{j}+(20-6) \mathbf{k}=-39 \mathbf{i}+24 \mathbf{j}+14 \mathbf{k}$

(b) Use $\left[\begin{array}{rrr}2 & -1 & 5 \\ 3 & 7 & 6\end{array}\right]$ to get $u \times v=[-6-35,15-12,14+3]=[-41,3,17]$

Remark: The cross products of the vectors $\mathbf{i}, \mathbf{j}, \mathbf{k}$ are as follows:

$$
\begin{array}{lll}
\mathbf{i} \times \mathbf{j}=\mathbf{k}, & \mathbf{j} \times \mathbf{k}=\mathbf{i}, & \mathbf{k} \times \mathbf{i}=\mathbf{j} \\
\mathbf{j} \times \mathbf{i}=-\mathbf{k}, & \mathbf{k} \times \mathbf{j}=-\mathbf{i}, & \mathbf{i} \times \mathbf{k}=-\mathbf{j}
\end{array}
$$

Thus, if we view the triple $(\mathbf{i}, \mathbf{j}, \mathbf{k})$ as a cyclic permutation, where $\mathbf{i}$ follows $\mathbf{k}$ and hence $\mathbf{k}$ precedes $\mathbf{i}$, then the product of two of them in the given direction is the third one, but the product of two of them in the opposite direction is the negative of the third one.

Two important properties of the cross product are contained in the following theorem.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-018(1)}
\end{center}

Volume $=u \cdot v \times w$

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-018}
\end{center}

Complex plane

(b)

Figure 1-4

THEOREM 1.5: Let $u, v, w$ be vectors in $\mathbf{R}^{3}$.

(a) The vector $u \times v$ is orthogonal to both $u$ and $v$.

(b) The absolute value of the "triple product"

$$
u \cdot v \times w
$$

represents the volume of the parallelopiped formed by the vectors $u, v, w$. [See Fig. 1-4(a).]

We note that the vectors $u, v, u \times v$ form a right-handed system, and that the following formula gives the magnitude of $u \times v$ :

$$
\|u \times v\|=\|u\|\|v\| \sin \theta
$$

where $\theta$ is the angle between $u$ and $v$.

\subsection*{1.7 Complex Numbers}
The set of complex numbers is denoted by $\mathbf{C}$. Formally, a complex number is an ordered pair $(a, b)$ of real numbers where equality, addition, and multiplication are defined as follows:

$$
\begin{aligned}
(a, b) & =(c, d) \quad \text { if and only if } a=c \text { and } b=d \\
(a, b)+(c, d) & =(a+c, b+d) \\
(a, b) \cdot(c, d) & =(a c-b d, a d+b c)
\end{aligned}
$$

We identify the real number $a$ with the complex number $(a, 0)$; that is,

$$
a \leftrightarrow(a, 0)
$$

This is possible because the operations of addition and multiplication of real numbers are preserved under the correspondence; that is,

$$
(a, 0)+(b, 0)=(a+b, 0) \quad \text { and } \quad(a, 0) \cdot(b, 0)=(a b, 0)
$$

Thus we view $\mathbf{R}$ as a subset of $\mathbf{C}$, and replace $(a, 0)$ by $a$ whenever convenient and possible.

We note that the set $\mathbf{C}$ of complex numbers with the above operations of addition and multiplication is a field of numbers, like the set $\mathbf{R}$ of real numbers and the set $\mathbf{Q}$ of rational numbers.

The complex number $(0,1)$ is denoted by $i$. It has the important property that

$$
i^{2}=i i=(0,1)(0,1)=(-1,0)=-1 \quad \text { or } \quad i=\sqrt{-1}
$$

Accordingly, any complex number $z=(a, b)$ can be written in the form

$$
z=(a, b)=(a, 0)+(0, b)=(a, 0)+(b, 0) \cdot(0,1)=a+b i
$$

The above notation $z=a+b i$, where $a \equiv \operatorname{Re} z$ and $b \equiv \operatorname{Im} z$ are called, respectively, the real and imaginary parts of $z$, is more convenient than $(a, b)$. In fact, the sum and product of complex numbers $z=a+b i$ and $w=c+d i$ can be derived by simply using the commutative and distributive laws and $i^{2}=-1$ :

$$
\begin{aligned}
z+w & =(a+b i)+(c+d i)=a+c+b i+d i=(a+b)+(c+d) i \\
z w & =(a+b i)(c+d i)=a c+b c i+a d i+b d i^{2}=(a c-b d)+(b c+a d) i
\end{aligned}
$$

We also define the negative of $z$ and subtraction in $\mathbf{C}$ by

$$
-z=-1 z \quad \text { and } \quad w-z=w+(-z)
$$

Warning: The letter $i$ representing $\sqrt{-1}$ has no relationship whatsoever to the vector $\mathbf{i}=[1,0,0]$ in Section 1.6.

\section*{Complex Conjugate, Absolute Value}
Consider a complex number $z=a+b i$. The conjugate of $z$ is denoted and defined by

$$
\bar{z}=\overline{a+b i}=a-b i
$$

Then $z \bar{z}=(a+b i)(a-b i)=a^{2}-b^{2} i^{2}=a^{2}+b^{2}$. Note that $z$ is real if and only if $\bar{z}=z$.

The absolute value of $z$, denoted by $|z|$, is defined to be the nonnegative square root of $z \bar{z}$. Namely,

$$
|z|=\sqrt{z \bar{z}}=\sqrt{a^{2}+b^{2}}
$$

Note that $|z|$ is equal to the norm of the vector $(a, b)$ in $\mathbf{R}^{2}$.

Suppose $z \neq 0$. Then the inverse $z^{-1}$ of $z$ and division in $\mathbf{C}$ of $w$ by $z$ are given, respectively, by

$$
z^{-1}=\frac{\bar{z}}{z \bar{z}}=\frac{a}{a^{2}+b^{2}}-\frac{b}{a^{2}+b^{2}} i \quad \text { and } \quad \frac{w}{z}-\frac{w \bar{z}}{z \bar{z}}=w z^{-1}
$$

EXAMPLE 1.10 Suppose $z=2+3 i$ and $w=5-2 i$. Then

$$
\begin{aligned}
z+w & =(2+3 i)+(5-2 i)=2+5+3 i-2 i=7+i \\
z w & =(2+3 i)(5-2 i)=10+15 i-4 i-6 i^{2}=16+11 i \\
\bar{z} & =\overline{2+3 i}=2-3 i \quad \text { and } \quad \bar{w}=\overline{5-2 i}=5+2 i \\
\frac{w}{z} & =\frac{5-2 i}{2+3 i}=\frac{(5-2 i)(2-3 i)}{(2+3 i)(2-3 i)}=\frac{4-19 i}{13}=\frac{4}{13}-\frac{19}{13} i \\
|z| & =\sqrt{4+9}=\sqrt{13} \quad \text { and } \quad|w|=\sqrt{25+4}=\sqrt{29}
\end{aligned}
$$

\section*{Complex Plane}
Recall that the real numbers $\mathbf{R}$ can be represented by points on a line. Analogously, the complex numbers C can be represented by points in the plane. Specifically, we let the point $(a, b)$ in the plane represent the complex number $a+b i$ as shown in Fig. 1-4(b). In such a case, $|z|$ is the distance from the origin $O$ to the point $z$. The plane with this representation is called the complex plane, just like the line representing $\mathbf{R}$ is called the real line.

\subsection*{1.8 Vectors in $\mathbf{C}^{n}$}
The set of all $n$-tuples of complex numbers, denoted by $\mathbf{C}^{n}$, is called complex $\mathrm{n}$-space. Just as in the real case, the elements of $\mathbf{C}^{n}$ are called points or vectors, the elements of $\mathbf{C}$ are called scalars, and vector addition in $\mathbf{C}^{n}$ and scalar multiplication on $\mathbf{C}^{n}$ are given by

$$
\begin{aligned}
{\left[z_{1}, z_{2}, \ldots, z_{n}\right]+\left[w_{1}, w_{2}, \ldots, w_{n}\right] } & =\left[z_{1}+w_{1}, z_{2}+w_{2}, \ldots, z_{n}+w_{n}\right] \\
z\left[z_{1}, z_{2}, \ldots, z_{n}\right] & =\left[z z_{1}, z z_{2}, \ldots, z z_{n}\right]
\end{aligned}
$$

where the $z_{i}, w_{i}$, and $z$ belong to $\mathbf{C}$.

EXAMPLE 1.11 Consider vectors $u=[2+3 i, 4-i, 3]$ and $v=[3-2 i, 5 i, 4-6 i]$ in $\mathbf{C}^{3}$. Then

$$
\begin{aligned}
u+v & =[2+3 i, 4-i, 3]+[3-2 i, 5 i, 4-6 i]=[5+i, 4+4 i, 7-6 i] \\
(5-2 i) u & =[(5-2 i)(2+3 i),(5-2 i)(4-i),(5-2 i)(3)]=[16+11 i, 18-13 i, 15-6 i]
\end{aligned}
$$

\section*{Dot (Inner) Product in $\mathbf{C}^{\boldsymbol{n}}$}
Consider vectors $u=\left[z_{1}, z_{2}, \ldots, z_{n}\right]$ and $v=\left[w_{1}, w_{2}, \ldots, w_{n}\right]$ in $\mathbf{C}^{n}$. The dot or inner product of $u$ and $v$ is denoted and defined by

$$
u \cdot v=z_{1} \bar{w}_{1}+z_{2} \bar{w}_{2}+\cdots+z_{n} \bar{w}_{n}
$$

This definition reduces to the real case because $\bar{w}_{i}=w_{i}$ when $w_{i}$ is real. The norm of $u$ is defined by

$$
\|u\|=\sqrt{u \cdot u}=\sqrt{z_{1} \bar{z}_{1}+z_{2} \bar{z}_{2}+\cdots+z_{n} \bar{z}_{n}}=\sqrt{\left|z_{1}\right|^{2}+\left|z_{2}\right|^{2}+\cdots+\left|v_{n}\right|^{2}}
$$

We emphasize that $u \cdot u$ and so $\|u\|$ are real and positive when $u \neq 0$ and 0 when $u=0$.

EXAMPLE 1.12 Consider vectors $u=[2+3 i, 4-i, 3+5 i]$ and $v=[3-4 i, 5 i, 4-2 i]$ in $\mathbf{C}_{3}$. Then

$$
\begin{aligned}
u \cdot v & =(2+3 i)(\overline{3-4 i})+(4-i)(\overline{5 i})+(3+5 i)(\overline{4-2 i}) \\
& =(2+3 i)(3+4 i)+(4-i)(-5 i)+(3+5 i)(4+2 i) \\
& =(-6+13 i)+(-5-20 i)+(2+26 i)=-9+19 i \\
u \cdot u & =|2+3 i|^{2}+|4-i|^{2}+|3+5 i|^{2}=4+9+16+1+9+25=64 \\
\|u\| & =\sqrt{64}=8
\end{aligned}
$$

The space $\mathbf{C}^{n}$ with the above operations of vector addition, scalar multiplication, and dot product, is called complex Euclidean n-space. Theorem 1.2 for $\mathbf{R}^{n}$ also holds for $\mathbf{C}^{n}$ if we replace $u \cdot v=v \cdot u$ by

$$
u \cdot v=\overline{u \cdot v}
$$

On the other hand, the Schwarz inequality (Theorem 1.3) and Minkowski's inequality (Theorem 1.4) are true for $\mathbf{C}^{n}$ with no changes.

\section*{SOLVED PROBLEMS}
\section*{Vectors in $\mathbf{R}^{\boldsymbol{n}}$}
1.1. Determine which of the following vectors are equal:

$$
u_{1}=(1,2,3), \quad u_{2}=(2,3,1), \quad u_{3}=(1,3,2), \quad u_{4}=(2,3,1)
$$

Vectors are equal only when corresponding entries are equal; hence, only $u_{2}=u_{4}$.

1.2. Let $u=(2,-7,1), v=(-3,0,4), w=(0,5,-8)$. Find:

(a) $3 u-4 v$,

(b) $2 u+3 v-5 w$.

First perform the scalar multiplication and then the vector addition.

(a) $3 u-4 v=3(2,-7,1)-4(-3,0,4)=(6,-21,3)+(12,0,-16)=(18,-21,-13)$

(b) $2 u+3 v-5 w=(4,-14,2)+(-9,0,12)+(0,-25,40)=(-5,-39,54)$

1.3. Let $u=\left[\begin{array}{r}5 \\ 3 \\ -4\end{array}\right], v=\left[\begin{array}{r}-1 \\ 5 \\ 2\end{array}\right], w=\left[\begin{array}{r}3 \\ -1 \\ -2\end{array}\right]$. Find:

(a) $5 u-2 v$,

(b) $-2 u+4 v-3 w$.

First perform the scalar multiplication and then the vector addition:

(a) $5 u-2 v=5\left[\begin{array}{r}5 \\ 3 \\ -4\end{array}\right]-2\left[\begin{array}{r}-1 \\ 5 \\ 2\end{array}\right]=\left[\begin{array}{r}25 \\ 15 \\ -20\end{array}\right]+\left[\begin{array}{r}2 \\ -10 \\ -4\end{array}\right]=\left[\begin{array}{r}27 \\ 5 \\ -24\end{array}\right]$

(b) $-2 u+4 v-3 w=\left[\begin{array}{r}-10 \\ -6 \\ 8\end{array}\right]+\left[\begin{array}{r}-4 \\ 20 \\ 8\end{array}\right]+\left[\begin{array}{r}-9 \\ 3 \\ 6\end{array}\right]=\left[\begin{array}{r}-23 \\ 17 \\ 22\end{array}\right]$

1.4. Find $x$ and $y$, where: (a) $(x, 3)=(2, x+y)$, (b) $(4, y)=x(2,3)$.

(a) Because the vectors are equal, set the corresponding entries equal to each other, yielding

$$
x=2, \quad 3=x+y
$$

Solve the linear equations, obtaining $x=2, y=1$.

(b) First multiply by the scalar $x$ to obtain $(4, y)=(2 x, 3 x)$. Then set corresponding entries equal to each other to obtain

$$
4=2 x, \quad y=3 x
$$

Solve the equations to yield $x=2, y=6$.

1.5. Write the vector $v=(1,-2,5)$ as a linear combination of the vectors $u_{1}=(1,1,1), u_{2}=(1,2,3)$, $u_{3}=(2,-1,1)$.

We want to express $v$ in the form $v=x u_{1}+y u_{2}+z u_{3}$ with $x, y, z$ as yet unknown. First we have

$$
\left[\begin{array}{r}
1 \\
-2 \\
5
\end{array}\right]=x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+z\left[\begin{array}{r}
2 \\
-1 \\
1
\end{array}\right]=\left[\begin{array}{l}
x+y+2 z \\
x+2 y-z \\
x+3 y+z
\end{array}\right]
$$

(It is more convenient to write vectors as columns than as rows when forming linear combinations.) Set corresponding entries equal to each other to obtain

$$
\begin{aligned}
& x+y+2 z=1 \quad x+y+2 z=1 \quad x+y+2 z=1 \\
& x+2 y-z=-2 \quad \text { or } \quad y-3 z=-3 \quad \text { or } \quad y-3 z=-3 \\
& x+3 y+z=5 \quad 2 y-z=4 \quad 5 z=10
\end{aligned}
$$

This unique solution of the triangular system is $x=-6, y=3, z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.

1.6. Write $v=(2,-5,3)$ as a linear combination of

$$
u_{1}=(1,-3,2), u_{2}=(2,-4,-1), u_{3}=(1,-5,7) \text {. }
$$

Find the equivalent system of linear equations and then solve. First,

$$
\left[\begin{array}{r}
2 \\
-5 \\
3
\end{array}\right]=x\left[\begin{array}{r}
1 \\
-3 \\
2
\end{array}\right]+y\left[\begin{array}{r}
2 \\
-4 \\
-1
\end{array}\right]+z\left[\begin{array}{r}
1 \\
-5 \\
7
\end{array}\right]=\left[\begin{array}{r}
x+2 y+z \\
-3 x-4 y-5 z \\
2 x-y+7 z
\end{array}\right]
$$

Set the corresponding entries equal to each other to obtain

$$
\begin{aligned}
& x+2 y+z=2 \quad x+2 y+z=2 \quad x+2 y+z=2 \\
& -3 x-4 y-5 z=-5 \quad \text { or } \quad 2 y-2 z=1 \quad \text { or } \quad 2 y-2 z=1 \\
& 2 x-y+7 z=3 \quad-5 y+5 z=-1 \quad 0=3
\end{aligned}
$$

The third equation, $0 x+0 y+0 z=3$, indicates that the system has no solution. Thus, $v$ cannot be written as a linear combination of the vectors $u_{1}, u_{2}, u_{3}$.

\section*{Dot (Inner) Product, Orthogonality, Norm in $\mathbf{R}^{n}$}
1.7. Find $u \cdot v$ where:

(a) $u=(2,-5,6)$ and $v=(8,2,-3)$,

(b) $u=(4,2,-3,5,-1)$ and $v=(2,6,-1,-4,8)$.

Multiply the corresponding components and add:

(a) $u \cdot v=2(8)-5(2)+6(-3)=16-10-18=-12$

(b) $u \cdot v=8+12+3-20-8=-5$

1.8. Let $u=(5,4,1), v=(3,-4,1), w=(1,-2,3)$. Which pair of vectors, if any, are perpendicular (orthogonal)?

Find the dot product of each pair of vectors:

$$
u \cdot v=15-16+1=0, \quad v \cdot w=3+8+3=14, \quad u \cdot w=5-8+3=0
$$

Thus, $u$ and $v$ are orthogonal, $u$ and $w$ are orthogonal, but $v$ and $w$ are not.

1.9. Find $k$ so that $u$ and $v$ are orthogonal, where:

(a) $u=(1, k,-3)$ and $v=(2,-5,4)$,

(b) $u=(2,3 k,-4,1,5)$ and $v=(6,-1,3,7,2 k)$.

Compute $u \cdot v$, set $u \cdot v$ equal to 0 , and then solve for $k$ :

(a) $u \cdot v=1(2)+k(-5)-3(4)=-5 k-10$. Then $-5 k-10=0$, or $k=-2$.

(b) $u \cdot v=12-3 k-12+7+10 k=7 k+7$. Then $7 k+7=0$, or $k=-1$.

1.10. Find $\|u\|$, where: (a) $u=(3,-12,-4)$,

(b) $u=(2,-3,8,-7)$.

First find $\|u\|^{2}=u \cdot u$ by squaring the entries and adding. Then $\|u\|=\sqrt{\|u\|^{2}}$.

(a) $\|u\|^{2}=(3)^{2}+(-12)^{2}+(-4)^{2}=9+144+16=169$. Then $\|u\|=\sqrt{169}=13$.

(b) $\|u\|^{2}=4+9+64+49=126$. Then $\|u\|=\sqrt{126}$.

1.11. Recall that normalizing a nonzero vector $v$ means finding the unique unit vector $\hat{v}$ in the same direction as $v$, where

$\hat{v}=\frac{1}{\|v\|} v$

Normalize: (a) $u=(3,-4), \quad$ (b) $v=(4,-2,-3,8), \quad$ (c) $\quad w=\left(\frac{1}{2}, \frac{2}{3},-\frac{1}{4}\right)$.

(a) First find $\|u\|=\sqrt{9+16}=\sqrt{25}=5$. Then divide each entry of $u$ by 5 , obtaining $\hat{u}=\left(\frac{3}{5},-\frac{4}{5}\right)$.

(b) Here $\|v\|=\sqrt{16+4+9+64}=\sqrt{93}$. Then

$$
\hat{v}=\left(\frac{4}{\sqrt{93}}, \frac{-2}{\sqrt{93}}, \frac{-3}{\sqrt{93}}, \frac{8}{\sqrt{93}}\right)
$$

(c) Note that $w$ and any positive multiple of $w$ will have the same normalized form. Hence, first multiply $w$ by 12 to "clear fractions"-that is, first find $w^{\prime}=12 w=(6,8,-3)$. Then

$$
\left\|w^{\prime}\right\|=\sqrt{36+64+9}=\sqrt{109} \text { and } \hat{w}=\widehat{w^{\prime}}=\left(\frac{6}{\sqrt{109}}, \frac{8}{\sqrt{109}}, \frac{-3}{\sqrt{109}}\right)
$$

1.12. Let $u=(1,-3,4)$ and $v=(3,4,7)$. Find:

(a) $\cos \theta$, where $\theta$ is the angle between $u$ and $v$;

(b) $\operatorname{proj}(u, v)$, the projection of $u$ onto $v$;

(c) $d(u, v)$, the distance between $u$ and $v$.

First find $u \cdot v=3-12+28=19,\|u\|^{2}=1+9+16=26,\|v\|^{2}=9+16+49=74$. Then

(a) $\cos \theta=\frac{u \cdot v}{\|u\|\|v\|}=\frac{19}{\sqrt{26} \sqrt{74}}$,

(b) $\operatorname{proj}(u, v)=\frac{u \cdot v}{\|v\|^{2}} v=\frac{19}{74}(3,4,7)=\left(\frac{57}{74}, \frac{76}{74}, \frac{133}{74}\right)=\left(\frac{57}{74}, \frac{38}{37}, \frac{133}{74}\right)$,

(c) $d(u, v)=\|u-v\|=\|(-2,-7-3)\|=\sqrt{4+49+9}=\sqrt{62}$.

1.13. Prove Theorem 1.2: For any $u, v, w$ in $\mathbf{R}^{n}$ and $k$ in $\mathbf{R}$ :\\
(i) $(u+v) \cdot w=u \cdot w+v \cdot w$,\\
(ii) $(k u) \cdot v=k(u \cdot v)$,\\
(iii) $u \cdot v=v \cdot u$,

(iv) $u \cdot u \geq 0$, and $u \cdot u=0$ iff $u=0$.

Let $u=\left(u_{1}, u_{2}, \ldots, u_{n}\right), v=\left(v_{1}, v_{2}, \ldots, v_{n}\right), w=\left(w_{1}, w_{2}, \ldots, w_{n}\right)$.

(i) Because $u+v=\left(u_{1}+v_{1}, u_{2}+v_{2}, \ldots, u_{n}+v_{n}\right)$,

$$
\begin{aligned}
(u+v) \cdot w & =\left(u_{1}+v_{1}\right) w_{1}+\left(u_{2}+v_{2}\right) w_{2}+\cdots+\left(u_{n}+v_{n}\right) w_{n} \\
& =u_{1} w_{1}+v_{1} w_{1}+u_{2} w_{2}+\cdots+u_{n} w_{n}+v_{n} w_{n} \\
& =\left(u_{1} w_{1}+u_{2} w_{2}+\cdots+u_{n} w_{n}\right)+\left(v_{1} w_{1}+v_{2} w_{2}+\cdots+v_{n} w_{n}\right) \\
& =u \cdot w+v \cdot w
\end{aligned}
$$

(ii) Because $k u=\left(k u_{1}, k u_{2}, \ldots, k u_{n}\right)$,

$$
(k u) \cdot v=k u_{1} v_{1}+k u_{2} v_{2}+\cdots+k u_{n} v_{n}=k\left(u_{1} v_{1}+u_{2} v_{2}+\cdots+u_{n} v_{n}\right)=k(u \cdot v)
$$

(iii) $u \cdot v=u_{1} v_{1}+u_{2} v_{2}+\cdots+u_{n} v_{n}=v_{1} u_{1}+v_{2} u_{2}+\cdots+v_{n} u_{n}=v \cdot u$

(iv) Because $u_{i}^{2}$ is nonnegative for each $i$, and because the sum of nonnegative real numbers is nonnegative,

$$
u \cdot u=u_{1}^{2}+u_{2}^{2}+\cdots+u_{n}^{2} \geq 0
$$

Furthermore, $u \cdot u=0$ iff $u_{i}=0$ for each $i$, that is, iff $u=0$.

1.14. Prove Theorem 1.3 (Schwarz): $|u \cdot v| \leq\|u\|\|v\|$.

For any real number $t$, and using Theorem 1.2, we have

$$
0 \leq(t u+v) \cdot(t u+v)=t^{2}(u \cdot u)+2 t(u \cdot v)+(v \cdot v)=\|u\|^{2} t^{2}+2(u \cdot v) t+\|v\|^{2}
$$

Let $a=\|u\|^{2}, b=2(u \cdot v), c=\|v\|^{2}$. Then, for every value of $t, a t^{2}+b t+c \geq 0$. This means that the quadratic polynomial cannot have two real roots. This implies that the discriminant $D=b^{2}-4 a c \leq 0$ or, equivalently, $b^{2} \leq 4 a c$. Thus,

$$
4(u \cdot v)^{2} \leq 4\|u\|^{2}\|v\|^{2}
$$

Dividing by 4 gives us our result.

1.15. Prove Theorem 1.4 (Minkowski): $\|u+v\| \leq\|u\|+\|v\|$.

By the Schwarz inequality and other properties of the dot product,

$$
\|u+v\|^{2}=(u+v) \cdot(u+v)=(u \cdot u)+2(u \cdot v)+(v \cdot v) \leq\|u\|^{2}+2\|u\|\|v\|+\|v\|^{2}=(\|u\|+\|v\|)^{2}
$$

Taking the square root of both sides yields the desired inequality.

\section*{Points, Lines, Hyperplanes in $\mathbf{R}^{\mathbf{n}}$}
Here we distinguish between an $n$-tuple $P\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ viewed as a point in $\mathbf{R}^{n}$ and an $n$-tuple $u=\left[c_{1}, c_{2}, \ldots, c_{n}\right]$ viewed as a vector (arrow) from the origin $O$ to the point $C\left(c_{1}, c_{2}, \ldots, c_{n}\right)$.

1.16. Find the vector $u$ identified with the directed line segment $\overrightarrow{P Q}$ for the points:\\
(a) $P(1,-2,4)$ and $Q(6,1,-5)$ in $\mathbf{R}^{3}$,\\
(b) $P(2,3,-6,5)$ and $Q(7,1,4,-8)$ in $\mathbf{R}^{4}$.\\
(a) $u=\overrightarrow{P Q}=Q-P=[6-1,1-(-2),-5-4]=[5,3,-9]$\\
(b) $u=\overrightarrow{P Q}=Q-P=[7-2,1-3,4+6,-8-5]=[5,-2,10,-13]$

1.17. Find an equation of the hyperplane $H$ in $\mathbf{R}^{4}$ that passes through $P(3,-4,1,-2)$ and is normal to $u=[2,5,-6,-3]$.

The coefficients of the unknowns of an equation of $H$ are the components of the normal vector $u$. Thus, an equation of $H$ is of the form $2 x_{1}+5 x_{2}-6 x_{3}-3 x_{4}=k$. Substitute $P$ into this equation to obtain $k=-26$. Thus, an equation of $H$ is $2 x_{1}+5 x_{2}-6 x_{3}-3 x_{4}=-26$.

1.18. Find an equation of the plane $H$ in $\mathbf{R}^{3}$ that contains $P(1,-3,-4)$ and is parallel to the plane $H^{\prime}$ determined by the equation $3 x-6 y+5 z=2$.

The planes $H$ and $H^{\prime}$ are parallel if and only if their normal directions are parallel or antiparallel (opposite direction). Hence, an equation of $H$ is of the form $3 x-6 y+5 z=k$. Substitute $P$ into this equation to obtain $k=1$. Then an equation of $H$ is $3 x-6 y+5 z=1$.

1.19. Find a parametric representation of the line $L$ in $\mathbf{R}^{4}$ passing through $P(4,-2,3,1)$ in the direction of $u=[2,5,-7,8]$.

Here $L$ consists of the points $X\left(x_{i}\right)$ that satisfy

$$
X=P+t u \quad \text { or } \quad x_{i}=a_{i} t+b_{i} \quad \text { or } \quad L(t)=\left(a_{i} t+b_{i}\right)
$$

where the parameter $t$ takes on all real values. Thus we obtain

$$
x_{1}=4+2 t, x_{2}=-2+2 t, x_{3}=3-7 t, x_{4}=1+8 t \quad \text { or } \quad L(t)=(4+2 t,-2+2 t, 3-7 t, 1+8 t)
$$

1.20. Let $C$ be the curve $F(t)=\left(t^{2}, 3 t-2, t^{3}, t^{2}+5\right)$ in $\mathbf{R}^{4}$, where $0 \leq t \leq 4$.

(a) Find the point $P$ on $C$ corresponding to $t=2$.

(b) Find the initial point $Q$ and terminal point $Q^{\prime}$ of $C$.

(c) Find the unit tangent vector $\mathbf{T}$ to the curve $C$ when $t=2$.

(a) Substitute $t=2$ into $F(t)$ to get $P=f(2)=(4,4,8,9)$.

(b) The parameter $t$ ranges from $t=0$ to $t=4$. Hence, $Q=f(0)=(0,-2,0,5)$ and $Q^{\prime}=F(4)=(16,10,64,21)$.

(c) Take the derivative of $F(t)$-that is, of each component of $F(t)$-to obtain a vector $V$ that is tangent to the curve:

$$
V(t)=\frac{d F(t)}{d t}=\left[2 t, 3,3 t^{2}, 2 t\right]
$$

Now find $V$ when $t=2$; that is, substitute $t=2$ in the equation for $V(t)$ to obtain $V=V(2)=[4,3,12,4]$. Then normalize $V$ to obtain the desired unit tangent vector $\mathbf{T}$. We have

$$
\|V\|=\sqrt{16+9+144+16}=\sqrt{185} \quad \text { and } \quad \mathbf{T}=\left[\frac{4}{\sqrt{185}}, \frac{3}{\sqrt{185}}, \frac{12}{\sqrt{185}}, \frac{4}{\sqrt{185}}\right]
$$

\section*{Spatial Vectors (Vectors in $\mathbf{R}^{\mathbf{3}}$ ), ijk Notation, Cross Product}
1.21. Let $u=2 \mathbf{i}-3 \mathbf{j}+4 \mathbf{k}, v=3 \mathbf{i}+\mathbf{j}-2 \mathbf{k}, w=\mathbf{i}+5 \mathbf{j}+3 \mathbf{k}$. Find:\\
(a) $u+v$\\
(b) $2 u-3 v+4 w$\\
(c) $u \cdot v$ and $u \cdot w$,\\
(d) $\|u\|$ and $\|v\|$.

Treat the coefficients of $\mathbf{i}, \mathbf{j}, \mathbf{k}$ just like the components of a vector in $\mathbf{R}^{3}$.

(a) Add corresponding coefficients to get $u+v=5 \mathbf{i}-2 \mathbf{j}-2 \mathbf{k}$.

(b) First perform the scalar multiplication and then the vector addition:

$$
\begin{aligned}
2 u-3 v+4 w & =(4 \mathbf{i}-6 \mathbf{j}+8 \mathbf{k})+(-9 \mathbf{i}+3 \mathbf{j}+6 \mathbf{k})+(4 \mathbf{i}+20 \mathbf{j}+12 \mathbf{k}) \\
& =-\mathbf{i}+17 \mathbf{j}+26 \mathbf{k}
\end{aligned}
$$

(c) Multiply corresponding coefficients and then add:

$$
u \cdot v=6-3-8=-5 \quad \text { and } \quad u \cdot w=2-15+12=-1
$$

(d) The norm is the square root of the sum of the squares of the coefficients:

$$
\|u\|=\sqrt{4+9+16}=\sqrt{29} \quad \text { and } \quad\|v\|=\sqrt{9+1+4}=\sqrt{14}
$$

1.22. Find the (parametric) equation of the line $L$ :

(a) through the points $P(1,3,2)$ and $Q(2,5,-6)$;

(b) containing the point $P(1,-2,4)$ and perpendicular to the plane $H$ given by the equation $3 x+5 y+7 z=15$.

(a) First find $v=\overrightarrow{P Q}=Q-P=[1,2,-8]=\mathbf{i}+2 \mathbf{j}-8 \mathbf{k}$. Then

$$
L(t)=(t+1,2 t+3,-8 t+2)=(t+1) \mathbf{i}+(2 t+3) \mathbf{j}+(-8 t+2) \mathbf{k}
$$

(b) Because $L$ is perpendicular to $H$, the line $L$ is in the same direction as the normal vector $\mathbf{N}=3 \mathbf{i}+5 \mathbf{j}+7 \mathbf{k}$ to $H$. Thus,

$$
L(t)=(3 t+1,5 t-2,7 t+4)=(3 t+1) \mathbf{i}+(5 t-2) \mathbf{j}+(7 t+4) \mathbf{k}
$$

1.23. Let $S$ be the surface $x y^{2}+2 y z=16$ in $\mathbf{R}^{3}$.

(a) Find the normal vector $\mathbf{N}(x, y, z)$ to the surface $S$.

(b) Find the tangent plane $H$ to $S$ at the point $P(1,2,3)$.\\
(a) The formula for the normal vector to a surface $F(x, y, z)=0$ is

$$
\mathbf{N}(x, y, z)=F_{x} \mathbf{i}+F_{y} \mathbf{j}+F_{z} \mathbf{k}
$$

where $F_{x}, F_{y}, F_{z}$ are the partial derivatives. Using $F(x, y, z)=x y^{2}+2 y z-16$, we obtain

$$
F_{x}=y^{2}, \quad F_{y}=2 x y+2 z, \quad F_{z}=2 y
$$

Thus, $\mathbf{N}(x, y, z)=y^{2} \mathbf{i}+(2 x y+2 z) \mathbf{j}+2 y \mathbf{k}$.

(b) The normal to the surface $S$ at the point $P$ is

$$
\mathbf{N}(P)=\mathbf{N}(1,2,3)=4 \mathbf{i}+10 \mathbf{j}+4 \mathbf{k}
$$

Hence, $\mathbf{N}=2 \mathbf{i}+5 \mathbf{j}+2 \mathbf{k}$ is also normal to $S$ at $P$. Thus an equation of $H$ has the form $2 x+5 y+2 z=c$. Substitute $P$ in this equation to obtain $c=18$. Thus the tangent plane $H$ to $S$ at $P$ is $2 x+5 y+2 z=18$.

1.24. Evaluate the following determinants and negative of determinants of order two:

(a) (i) $\left|\begin{array}{ll}3 & 4 \\ 5 & 9\end{array}\right|$, (ii) $\left|\begin{array}{rr}2 & -1 \\ 4 & 3\end{array}\right|$, (iii) $\left|\begin{array}{ll}4 & -5 \\ 3 & -2\end{array}\right|$

(b) (i) $-\left|\begin{array}{ll}3 & 6 \\ 4 & 2\end{array}\right|$, (ii) $-\left|\begin{array}{rr}7 & -5 \\ 3 & 2\end{array}\right|$, (iii) $-\left|\begin{array}{ll}4 & -1 \\ 8 & -3\end{array}\right|$

Use $\left|\begin{array}{ll}a & b \\ c & d\end{array}\right|=a d-b c$ and $-\left|\begin{array}{ll}a & b \\ c & d\end{array}\right|=b c-a d$. Thus,\\
(a) (i) $27-20=7$,\\
(ii) $6+4=10$, (iii) $-8+15=7$.\\
(b) (i) $24-6=18$,\\
(ii) $-15-14=-29$, (iii) $-8+12=4$.

1.25. Let $u=2 \mathbf{i}-3 \mathbf{j}+4 \mathbf{k}, v=3 \mathbf{i}+\mathbf{j}-2 \mathbf{k}, w=\mathbf{i}+5 \mathbf{j}+3 \mathbf{k}$.

Find: (a) $u \times v$, (b) $u \times w$

(a) Use $\left[\begin{array}{rrr}2 & -3 & 4 \\ 3 & 1 & -2\end{array}\right]$ to get $u \times v=(6-4) \mathbf{i}+(12+4) \mathbf{j}+(2+9) \mathbf{k}=2 \mathbf{i}+16 \mathbf{j}+11 \mathbf{k}$.

(b) Use $\left[\begin{array}{rrr}2 & -3 & 4 \\ 1 & 5 & 3\end{array}\right]$ to get $u \times w=(-9-20) \mathbf{i}+(4-6) \mathbf{j}+(10+3) \mathbf{k}=-29 \mathbf{i}-2 \mathbf{j}+13 \mathbf{k}$.

1.26. Find $u \times v$, where: (a) $\quad u=(1,2,3), v=(4,5,6)$; (b) $\quad u=(-4,7,3), v=(6,-5,2)$.

(a) Use $\left[\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6\end{array}\right]$ to get $u \times v=[12-15,12-6,5-8]=[-3,6,-3]$.

(b) Use $\left[\begin{array}{rrr}-4 & 7 & 3 \\ 6 & -5 & 2\end{array}\right]$ to get $u \times v=[14+15,18+8,20-42]=[29,26,-22]$.

1.27. Find a unit vector $u$ orthogonal to $v=[1,3,4]$ and $w=[2,-6,-5]$.

First find $v \times w$, which is orthogonal to $v$ and $w$.

The array $\left[\begin{array}{rrr}1 & 3 & 4 \\ 2 & -6 & -5\end{array}\right]$ gives $v \times w=[-15+24,8+5,-6-61]=[9,13,-12]$.

Normalize $v \times w$ to get $u=[9 / \sqrt{394}, 13 / \sqrt{394},-12 / \sqrt{394}]$.

1.28. Let $u=\left(a_{1}, a_{2}, a_{3}\right)$ and $v=\left(b_{1}, b_{2}, b_{3}\right)$ so $u \times v=\left(a_{2} b_{3}-a_{3} b_{2}, a_{3} b_{1}-a_{1} b_{3}, a_{1} b_{2}-a_{2} b_{1}\right)$. Prove:

(a) $u \times v$ is orthogonal to $u$ and $v$ [Theorem 1.5(a)].

(b) $\|u \times v\|^{2}=(u \cdot u)(v \cdot v)-(u \cdot v)^{2}$ (Lagrange's identity).\\
(a) We have

$$
\begin{aligned}
u \cdot(u \times v) & =a_{1}\left(a_{2} b_{3}-a_{3} b_{2}\right)+a_{2}\left(a_{3} b_{1}-a_{1} b_{3}\right)+a_{3}\left(a_{1} b_{2}-a_{2} b_{1}\right) \\
& =a_{1} a_{2} b_{3}-a_{1} a_{3} b_{2}+a_{2} a_{3} b_{1}-a_{1} a_{2} b_{3}+a_{1} a_{3} b_{2}-a_{2} a_{3} b_{1}=0
\end{aligned}
$$

Thus, $u \times v$ is orthogonal to $u$. Similarly, $u \times v$ is orthogonal to $v$.

(b) We have


\begin{gather*}
\|u \times v\|^{2}=\left(a_{2} b_{3}-a_{3} b_{2}\right)^{2}+\left(a_{3} b_{1}-a_{1} b_{3}\right)^{2}+\left(a_{1} b_{2}-a_{2} b_{1}\right)^{2}  \tag{1}\\
(u \cdot u)(v \cdot v)-(u \cdot v)^{2}=\left(a_{1}^{2}+a_{2}^{2}+a_{3}^{2}\right)\left(b_{1}^{2}+b_{2}^{2}+b_{3}^{2}\right)-\left(a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}\right)^{2} \tag{2}
\end{gather*}


Expansion of the right-hand sides of (1) and (2) establishes the identity.

\section*{Complex Numbers, Vectors in $\mathbf{C}^{n}$}
1.29. Suppose $z=5+3 i$ and $w=2-4 i$. Find: (a) $z+w$, (b) $z-w$, (c) $z w$.

Use the ordinary rules of algebra together with $i^{2}=-1$ to obtain a result in the standard form $a+b i$.

(a) $z+w=(5+3 i)+(2-4 i)=7-i$

(b) $z-w=(5+3 i)-(2-4 i)=5+3 i-2+4 i=3+7 i$

(c) $z w=(5+3 i)(2-4 i)=10-14 i-12 i^{2}=10-14 i+12=22-14 i$

1.30. Simplify: (a) $(5+3 i)(2-7 i)$, (b) $(4-3 i)^{2}$, (c) $(1+2 i)^{3}$.

(a) $(5+3 i)(2-7 i)=10+6 i-35 i-21 i^{2}=31-29 i$

(b) $(4-3 i)^{2}=16-24 i+9 i^{2}=7-24 i$

(c) $(1+2 i)^{3}=1+6 i+12 i^{2}+8 i^{3}=1+6 i-12-8 i=-11-2 i$

1.31. Simplify: (a) $i^{0}, i^{3}, i^{4}$, (b) $i^{5}, i^{6}, i^{7}, i^{8}$, (c) $i^{39}, i^{174}, i^{252}, i^{317}$.

(a) $i^{0}=1, \quad i^{3}=i^{2}(i)=(-1)(i)=-i, \quad i^{4}=\left(i^{2}\right)\left(i^{2}\right)=(-1)(-1)=1$

(b) $i^{5}=\left(i^{4}\right)(i)=(1)(i)=i, \quad i^{6}=\left(i^{4}\right)\left(i^{2}\right)=(1)\left(i^{2}\right)=i^{2}=-1, \quad i^{7}=i^{3}=-i, \quad i^{8}=i^{4}=1$

(c) Using $i^{4}=1$ and $i^{n}=i^{4 q+r}=\left(i^{4}\right)^{q} i^{r}=1^{q} i^{r}=i^{r}$, divide the exponent $n$ by 4 to obtain the remainder $r$ :

$$
i^{39}=i^{4(9)+3}=\left(i^{4}\right)^{9} i^{3}=1^{9} i^{3}=i^{3}=-i, \quad i^{174}=i^{2}=-1, \quad i^{252}=i^{0}=1, \quad i^{317}=i^{1}=i
$$

1.32. Find the complex conjugate of each of the following:\\
(a) $6+4 i, 7-5 i, 4+i,-3-i$,\\
(b) $6,-3,4 i,-9 i$.

(a) $\overline{6+4 i}=6-4 i, \overline{7-5 i}=7+5 i, \overline{4+i}=4-i, \overline{-3-i}=-3+i$

(b) $\overline{6}=6, \overline{-3}=-3, \overline{4 i}=-4 i, \overline{-9 i}=9 i$

(Note that the conjugate of a real number is the original number, but the conjugate of a pure imaginary number is the negative of the original number.)

1.33. Find $z \bar{z}$ and $|z|$ when $z=3+4 i$.

For $z=a+b i$, use $z \bar{z}=a^{2}+b^{2}$ and $z=\sqrt{z \bar{z}}=\sqrt{a^{2}+b^{2}}$.

$$
z \bar{z}=9+16=25, \quad|z|=\sqrt{25}=5
$$

1.34. Simpify $\frac{2-7 i}{5+3 i}$.

To simplify a fraction $z / w$ of complex numbers, multiply both numerator and denominator by $\bar{w}$, the conjugate of the denominator:

$$
\frac{2-7 i}{5+3 i}=\frac{(2-7 i)(5-3 i)}{(5+3 i)(5-3 i)}=\frac{-11-41 i}{34}=-\frac{11}{34}-\frac{41}{34} i
$$

1.35. Prove: For any complex numbers $z, w \in \mathbf{C}$, (i) $\overline{z+w}=\bar{z}+\bar{w}$, (ii) $\overline{z w}=\bar{z} \bar{w}$, (iii) $\bar{z}=z$.

Suppose $z=a+b i$ and $w=c+d i$ where $a, b, c, d \in \mathbf{R}$.

(i)

$$
\begin{aligned}
\overline{z+w} & =\overline{(a+b i)+(c+d i)}=\overline{(a+c)+(b+d) i} \\
& =(a+c)-(b+d) i=a+c-b i-d i \\
& =(a-b i)+(c-d i)=\bar{z}+\bar{w}
\end{aligned}
$$

(ii) $\overline{z w}=\overline{(a+b i)(c+d i)}=\overline{(a c-b d)+(a d+b c) i}$

$$
=(a c-b d)-(a d+b c) i=(a-b i)(c-d i)=\bar{z} \bar{w}
$$

(iii) $\bar{z}=a+b i=\overline{\overline{a-b i}}=a-(-b) i=a+b i=z$

1.36. Prove: For any complex numbers $z, w \in \mathbf{C},|z w|=|z||w|$.

By (ii) of Problem 1.35,

$$
|z w|^{2}=(z w)(\overline{z w})=(z w)(\bar{z} \bar{w})=(z \bar{z})(w \bar{w})=|z|^{2}|w|^{2}
$$

The square root of both sides gives us the desired result.

1.37. Prove: For any complex numbers $z, w \in \mathbf{C},|z+w| \leq|z|+|w|$.

Suppose $z=a+b i$ and $w=c+d i$ where $a, b, c, d \in \mathbf{R}$. Consider the vectors $u=(a, b)$ and $v=(c, d)$ in $\mathbf{R}^{2}$. Note that

$$
|z|=\sqrt{a^{2}+b^{2}}=\|u\|, \quad|w|=\sqrt{c^{2}+d^{2}}=\|v\|
$$

and

$$
|z+w|=|(a+c)+(b+d) i|=\sqrt{(a+c)^{2}+(b+d)^{2}}=\|(a+c, b+d)\|=\|u+v\|
$$

By Minkowski's inequality (Problem 1.15), $\|u+v\| \leq\|u\|+\|v\|$, and so

$$
|z+w|=\|u+v\| \leq\|u\|+\|v\|=|z|+|w|
$$

1.38. Find the dot products $u \cdot v$ and $v \cdot u$ where: (a) $u=(1-2 i, 3+i), v=(4+2 i, 5-6 i)$, (b) $u=(3-2 i, 4 i, 1+6 i), v=(5+i, 2-3 i, 7+2 i)$.

Recall that conjugates of the second vector appear in the dot product

$$
\left(z_{1}, \ldots, z_{n}\right) \cdot\left(w_{1}, \ldots, w_{n}\right)=z_{1} \bar{w}_{1}+\cdots+z_{n} \bar{w}_{n}
$$

(a) $u \cdot v=(1-2 i)(\overline{4+2 i})+(3+i)(\overline{5-6 i})$

$$
=(1-2 i)(4-2 i)+(3+i)(5+6 i)=-10 i+9+23 i=9+13 i
$$

$v \cdot u=(4+2 i)(\overline{1-2 i})+(5-6 i)(\overline{3+i})$

$$
=(4+2 i)(1+2 i)+(5-6 i)(3-i)=10 i+9-23 i=9-13 i
$$

(b) $u \cdot v=(3-2 i)(\overline{5+i})+(4 i)(\overline{2-3 i})+(1+6 i)(\overline{7+2 i})$

$$
=(3-2 i)(5-i)+(4 i)(2+3 i)+(1+6 i)(7-2 i)=20+35 i
$$

$$
\begin{aligned}
v \cdot u & =(5+i)(\overline{3-2 i})+(2-3 i)(\overline{4 i})+(7+2 i)(\overline{1+6 i}) \\
& =(5+i)(3+2 i)+(2-3 i)(-4 i)+(7+2 i)(1-6 i)=20-35 i
\end{aligned}
$$

In both cases, $v \cdot u=\overline{u \cdot v}$. This holds true in general, as seen in Problem 1.40.

1.39. Let $u=(7-2 i, 2+5 i)$ and $v=(1+i,-3-6 i)$. Find:\\
(a) $u+v$\\
(b) $2 i u$,\\
(c) $(3-i) v$,\\
(d) $u \cdot v$,\\
(e) $\|u\|$ and $\|v\|$.\\
(a) $u+v=(7-2 i+1+i, 2+5 i-3-6 i)=(8-i,-1-i)$\\
(b) $2 i u=\left(14 i-4 i^{2}, 4 i+10 i^{2}\right)=(4+14 i,-10+4 i)$\\
(c) $(3-i) v=\left(3+3 i-i-i^{2},-9-18 i+3 i+6 i^{2}\right)=(4+2 i,-15-15 i)$\\
(d) $u \cdot v=(7-2 i)(\overline{1+i})+(2+5 i)(\overline{-3-6 i})$

$$
=(7-2 i)(1-i)+(2+5 i)(-3+6 i)=5-9 i-36-3 i=-31-12 i
$$

(e) $\|u\|=\sqrt{7^{2}+(-2)^{2}+2^{2}+5^{2}}=\sqrt{82}$ and $\|v\|=\sqrt{1^{2}+1^{2}+(-3)^{2}+(-6)^{2}}=\sqrt{47}$

1.40. Prove: For any vectors $u, v \in \mathbf{C}^{n}$ and any scalar $z \in \mathbf{C}$, (i) $u \cdot v=\overline{v \cdot u}$, (ii) $(z u) \cdot v=z(u \cdot v)$, (iii) $u \cdot(z v)=\bar{z}(u \cdot v)$.

Suppose $u=\left(z_{1}, z_{2}, \ldots, z_{n}\right)$ and $v=\left(w_{1}, w_{2}, \ldots, w_{n}\right)$.

(i) Using the properties of the conjugate,

$$
\begin{aligned}
\overline{v \cdot u} & =\overline{w_{1} \bar{z}_{1}+w_{2} \bar{z}_{2}+\cdots+w_{n} \bar{z}_{n}}=\overline{w_{1} \bar{z}_{1}}+\overline{w_{2} \bar{z}_{2}}+\cdots+\overline{w_{n} \bar{z}_{n}} \\
& =\bar{w}_{1} z_{1}+\bar{w}_{2} z_{2}+\cdots+\bar{w}_{n} z_{n}=z_{1} \bar{w}_{1}+z_{2} \bar{w}_{2}+\cdots+z_{n} \bar{w}_{n}=u \cdot v
\end{aligned}
$$

(ii) Because $z u=\left(z z_{1}, z z_{2}, \ldots, z z_{n}\right)$,

$$
(z u) \cdot v=z z_{1} \bar{w}_{1}+z z_{2} \bar{w}_{2}+\cdots+z z_{n} \bar{w}_{n}=z\left(z_{1} \bar{w}_{1}+z_{2} \bar{w}_{2}+\cdots+z_{n} \bar{w}_{n}\right)=z(u \cdot v)
$$

(Compare with Theorem 1.2 on vectors in $\mathbf{R}^{n}$.)

(iii) Using (i) and (ii),

$$
u \cdot(z v)=\overline{(z v) \cdot u}=z(\overline{v \cdot u})=\bar{z}(\overline{v \cdot u})=\bar{z}(u \cdot v)
$$

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Vectors in $\mathbf{R}^{n}$}
1.41. Let $u=(1,-2,4), v=(3,5,1), w=(2,1,-3)$. Find:\\
(a) $3 u-2 v$;\\
(b) $5 u+3 v-4 w$\\
(c) $u \cdot v, \quad u \cdot w, \quad v \cdot w$;\\
(d) $\|u\|,\|v\|$;\\
(e) $\cos \theta$, where $\theta$ is the angle between $u$ and $v$;\\
(f) $d(u, v)$\\
(g) $\operatorname{proj}(u, v)$.

1.42. Repeat Problem 1.41 for vectors $u=\left[\begin{array}{r}1 \\ 3 \\ -4\end{array}\right], v=\left[\begin{array}{l}2 \\ 1 \\ 5\end{array}\right], w=\left[\begin{array}{r}3 \\ -2 \\ 6\end{array}\right]$.

1.43. Let $u=(2,-5,4,6,-3)$ and $v=(5,-2,1,-7,-4)$. Find:\\
(a) $4 u-3 v$;\\
(b) $5 u+2 v$\\
(c)\\
$u \cdot v ;(\mathrm{d})$\\
$\|u\|$ and $\|v\|$\\
(e) $\operatorname{proj}(u, v)$;\\
(f)\\
$d(u, v)$.

1.44. Normalize each vector:\\
(a) $u=(5,-7)$\\
(b) $v=(1,2,-2,4)$;\\
(c) $w=\left(\frac{1}{2},-\frac{1}{3}, \frac{3}{4}\right)$.

1.45. Let $u=(1,2,-2), v=(3,-12,4)$, and $k=-3$.

(a) Find $\|u\|,\|v\|,\|u+v\|,\|k u\|$.

(b) Verify that $\|k u\|=|k|\|u\|$ and $\|u+v\| \leq\|u\|+\|v\|$.

1.46. Find $x$ and $y$ where:\\
(a) $(x, y+1)=(y-2,6)$;\\
(b) $x(2, y)=y(1,-2)$.

1.47. Find $x, y, z$ where $(x, y+1, y+z)=(2 x+y, 4,3 z)$.

1.48. Write $v=(2,5)$ as a linear combination of $u_{1}$ and $u_{2}$, where:

(a) $u_{1}=(1,2)$ and $u_{2}=(3,5)$;

(b) $u_{1}=(3,-4)$ and $u_{2}=(2,-3)$.

1.49. Write $v=\left[\begin{array}{r}9 \\ -3 \\ 16\end{array}\right]$ as a linear combination of $u_{1}=\left[\begin{array}{l}1 \\ 3 \\ 3\end{array}\right], u_{2}=\left[\begin{array}{r}2 \\ 5 \\ -1\end{array}\right], u_{3}=\left[\begin{array}{r}4 \\ -2 \\ 3\end{array}\right]$.

1.50. Find $k$ so that $u$ and $v$ are orthogonal, where:

(a) $u=(3, k,-2), v=(6,-4,-3)$;

(b) $u=(5, k,-4,2), v=(1,-3,2,2 k)$;

(c) $u=(1,7, k+2,-2), v=(3, k,-3, k)$.

\section*{Located Vectors, Hyperplanes, Lines in $\mathbf{R}^{\boldsymbol{n}}$}
1.51. Find the vector $v$ identified with the directed line segment $\overrightarrow{P Q}$ for the points:

(a) $P(2,3,-7)$ and $Q(1,-6,-5)$ in $\mathbf{R}^{3}$;

(b) $P(1,-8,-4,6)$ and $Q(3,-5,2,-4)$ in $\mathbf{R}^{4}$.

1.52. Find an equation of the hyperplane $H$ in $\mathbf{R}^{4}$ that:

(a) contains $P(1,2,-3,2)$ and is normal to $u=[2,3,-5,6]$;

(b) contains $P(3,-1,2,5)$ and is parallel to $2 x_{1}-3 x_{2}+5 x_{3}-7 x_{4}=4$.

1.53. Find a parametric representation of the line in $\mathbf{R}^{4}$ that:

(a) passes through the points $P(1,2,1,2)$ and $Q(3,-5,7,-9)$;

(b) passes through $P(1,1,3,3)$ and is perpendicular to the hyperplane $2 x_{1}+4 x_{2}+6 x_{3}-8 x_{4}=5$.

\section*{Spatial Vectors (Vectors in $\mathbf{R}^{\mathbf{3}}$ ), ijk Notation}
1.54. Given $u=3 \mathbf{i}-4 \mathbf{j}+2 \mathbf{k}, v=2 \mathbf{i}+5 \mathbf{j}-3 \mathbf{k}, w=4 \mathbf{i}+7 \mathbf{j}+2 \mathbf{k}$. Find:\\
(a) $2 u-3 v$;\\
(b) $3 u+4 v-2 w$;\\
(c) $u \cdot v, \quad u \cdot w, \quad v \cdot w$\\
(d) $\|u\|,\|v\|,\|w\|$

1.55. Find the equation of the plane $H$ :

(a) with normal $\mathbf{N}=3 \mathbf{i}-4 \mathbf{j}+5 \mathbf{k}$ and containing the point $P(1,2,-3)$;

(b) parallel to $4 x+3 y-2 z=11$ and containing the point $Q(2,-1,3)$.

1.56. Find the (parametric) equation of the line $L$ :

(a) through the point $P(2,5,-3)$ and in the direction of $v=4 \mathbf{i}-5 \mathbf{j}+7 \mathbf{k}$;

(b) perpendicular to the plane $2 x-3 y+7 z=4$ and containing $P(1,-5,7)$.

1.57. Consider the following curve $C$ in $\mathbf{R}^{3}$ where $0 \leq t \leq 5$ :

$$
F(t)=t^{3} \mathbf{i}-t^{2} \mathbf{j}+(2 t-3) \mathbf{k}
$$

(a) Find the point $P$ on $C$ corresponding to $t=2$.

(b) Find the initial point $Q$ and the terminal point $Q^{\prime}$.

(c) Find the unit tangent vector $\mathbf{T}$ to the curve $C$ when $t=2$.

1.58. Consider a moving body $B$ whose position at time $t$ is given by $R(t)=t^{2} \mathbf{i}+t^{3} \mathbf{j}+3 t \mathbf{k}$. [Then $V(t)=d R(t) / d t$ and $A(t)=d V(t) / d t$ denote, respectively, the velocity and acceleration of $B$.] When $t=1$, find for the body $B$ :\\
(a) position;\\
(b) velocity $v$;\\
(c) speed $s$;\\
(d) acceleration $a$.

1.59. Find a normal vector $\mathbf{N}$ and the tangent plane $H$ to each surface at the given point:

(a) surface $x^{2} y+3 y z=20$ and point $P(1,3,2)$;

(b) surface $x^{2}+3 y^{2}-5 z^{2}=160$ and point $P(3,-2,1)$.

\section*{Cross Product}
1.60. Evaluate the following determinants and negative of determinants of order two:

(a) $\left|\begin{array}{ll}2 & 5 \\ 3 & 6\end{array}\right|, \quad\left|\begin{array}{ll}3 & -6 \\ 1 & -4\end{array}\right|, \quad\left|\begin{array}{rr}-4 & -2 \\ 7 & -3\end{array}\right|$

(b) $-\left|\begin{array}{ll}6 & 4 \\ 7 & 5\end{array}\right|, \quad-\left|\begin{array}{rr}1 & -3 \\ 2 & 4\end{array}\right|, \quad-\left|\begin{array}{rr}8 & -3 \\ -6 & -2\end{array}\right|$

1.61. Given $u=3 \mathbf{i}-4 \mathbf{j}+2 \mathbf{k}, \quad v=2 \mathbf{i}+5 \mathbf{j}-3 \mathbf{k}, \quad w=4 \mathbf{i}+7 \mathbf{j}+2 \mathbf{k}$, find:\\
(a) $u \times v$,\\
(b) $u \times w$\\
(c) $v \times w$

1.62. Given $u=[2,1,3], v=[4,-2,2], w=[1,1,5]$, find:\\
(a) $u \times v$\\
(b) $u \times w$,\\
(c) $v \times w$.

1.63. Find the volume $V$ of the parallelopiped formed by the vectors $u, v, w$ appearing in:\\
(a) Problem 1.60\\
(b) Problem 1.61.

1.64. Find a unit vector $u$ orthogonal to:

(a) $v=[1,2,3]$ and $w=[1,-1,2]$;

(b) $v=3 \mathbf{i}-\mathbf{j}+2 \mathbf{k}$ and $w=4 \mathbf{i}-2 \mathbf{j}-\mathbf{k}$.

1.65. Prove the following properties of the cross product:\\
(a) $u \times v=-(v \times u)$\\
(d) $u \times(v+w)=(u \times v)+(u \times w)$\\
(b) $u \times u=0$ for any vector $u$\\
(e) $(v+w) \times u=(v \times u)+(w \times u)$\\
(c) $(k u) \times v=k(u \times v)=u \times(k v)$\\
(f) $(u \times v) \times w=(u \cdot w) v-(v \cdot w) u$

\section*{Complex Numbers}
1.66. Simplify:\\
(a) $(4-7 i)(9+2 i)$;\\
(b) $(3-5 i)^{2}$;\\
(c) $\frac{1}{4-7 i}$;\\
(d) $\frac{9+2 i}{3-5 i}$;\\
(e) $(1-i)^{3}$.\\
1.67. Simplify: (a) $\frac{1}{2 i}$;\\
(b) $\frac{2+3 i}{7-3 i}$;\\
(c) $i^{15}, i^{25}, i^{34}$;\\
(d) $\left(\frac{1}{3-i}\right)^{2}$.

1.68. Let $z=2-5 i$ and $w=7+3 i$. Find:\\
(a) $v+w$;\\
(b) $z w$;\\
(c) $z / w$;\\
(d) $\bar{z}, \bar{w}$;\\
(e) $|z|,|w|$.

1.69. Show that for complex numbers $z$ and $w$ :\\
(a) $\operatorname{Re} z=\frac{1}{2}(z+\bar{z})$,\\
(b) $\operatorname{Im} z=\frac{1}{2}(z-\bar{z})$\\
(c) $z w=0$ implies $z=0$ or $w=0$.

\section*{Vectors in $\mathbf{C}^{\boldsymbol{n}}$}
1.70. Let $u=(1+7 i, 2-6 i)$ and $v=(5-2 i, 3-4 i)$. Find:\\
(a) $u+v$\\
(b) $(3+i) u$\\
(c) $2 i u+(4+7 i) v$\\
(d) $u \cdot v$\\
(e) $\|u\|$ and $\|v\|$.

1.71. Prove: For any vectors $u, v, w$ in $\mathbf{C}^{n}$ :\\
(a) $(u+v) \cdot w=u \cdot w+v \cdot w$,\\
(b) $w \cdot(u+v)=w \cdot u+w \cdot v$.

1.72. Prove that the norm in $\mathbf{C}^{n}$ satisfies the following laws:

$\left[\mathrm{N}_{1}\right]$ For any vector $u,\|u\| \geq 0$; and $\|u\|=0$ if and only if $u=0$.

$\left[\mathrm{N}_{2}\right]$ For any vector $u$ and complex number $z,\|z u\|=|z|\|u\|$.

$\left[\mathrm{N}_{3}\right]$ For any vectors $u$ and $v,\|u+v\| \leq\|u\|+\|v\|$.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
1.41. (a) $(-3,-16,4)$;\\
(b) $(6,1,35)$;\\
(c) $-3,12,8$;\\
(d) $\sqrt{21}, \sqrt{35}, \sqrt{14}$;\\
(e) $-3 / \sqrt{21} \sqrt{35}$;\\
(f) $\sqrt{62}$;\\
(g) $-\frac{3}{35}(3,5,1)=\left(-\frac{9}{35},-\frac{15}{35},-\frac{3}{35}\right)$

1.42. (Column vectors) (a) $(-1,7,-22)$;

(b) $(-1,26,-29)$;

(c) $-15,-27,34$;\\
(d) $\sqrt{26}, \sqrt{30}$;\\
(e) $-15 /(\sqrt{26} \sqrt{30})$;\\
(f) $\sqrt{86}$;\\
(g) $-\frac{15}{30} v=\left(-1,-\frac{1}{2},-\frac{5}{2}\right)$

1.43. (a) $(-13,-14,13,45,0)$;\\
(e) $-\frac{6}{95} v$

(b) $(20,-29,22,16,-23)$;

(c) -6 ;

(d) $\sqrt{90}, \sqrt{95}$;

1.44. (a) $(5 / \sqrt{76}, 9 / \sqrt{76})$;

(b) $\left(\frac{1}{5}, \frac{2}{5},-\frac{2}{5}, \frac{4}{5}\right)$;

(c) $(6 / \sqrt{133},-4 \sqrt{133}, 9 \sqrt{133})$

1.45. (a) $3,13, \sqrt{120}, 9$

1.46. (a) $x=-3, y=5$;

(b) $x=0, y=0$, and $x=1, y=2$

1.47. $x=-3, \quad y=3, \quad z=\frac{3}{2}$

1.48. (a) $v=5 u_{1}-u_{2}$;

(b) $v=16 u_{1}-23 u_{2}$

1.49. $v=3 u_{1}-u_{2}+2 u_{3}$

1.50. (a) 6 ;

(b) 3 ;

(c) $\frac{3}{2}$

1.51. (a) $v=[-1,-9,2]$;

(b) $[2,3,6,-10]$

1.52. (a) $2 x_{1}+3 x_{2}-5 x_{3}+6 x_{4}=35$;

(b) $2 x_{1}-3 x_{2}+5 x_{3}-7 x_{4}=-16$

1.53. (a) $[2 t+1,-7 t+2,6 t+1,-11 t+2]$;

(b) $[2 t+1,4 t+1,6 t+3,-8 t+3]$

1.54. (a) $-23 \mathbf{j}+13 \mathbf{k}$;

(b) $9 \mathbf{i}-6 \mathbf{j}-10 \mathbf{k}$;

(c) $-20,-12,37$;

(d) $\sqrt{29}, \sqrt{38}, \sqrt{69}$

1.55. (a) $3 x-4 y+5 z=-20$;

(b) $4 x+3 y-2 z=-1$

1.56. (a) $[4 t+2,-5 t+5,7 t-3]$;

(b) $[2 t+1,-3 t-5,7 t+7]$

1.57. (a) $P=F(2)=8 \mathbf{i}-4 \mathbf{j}+\mathbf{k}$;

(c) $\mathbf{T}=(6 \mathbf{i}-2 \mathbf{j}+\mathbf{k}) / \sqrt{41}$

(b) $Q=F(0)=-3 \mathbf{k}, Q^{\prime}=F(5)=125 \mathbf{i}-25 \mathbf{j}+7 \mathbf{k}$;

1.58. (a) $\mathbf{i}+\mathbf{j}+2 \mathbf{k}$;

(b) $2 \mathbf{i}+3 \mathbf{j}+2 \mathbf{k}$

(c) $\sqrt{17}$;

(d) $2 \mathbf{i}+6 \mathbf{j}$

1.59. (a) $\mathbf{N}=6 \mathbf{i}+7 \mathbf{j}+9 \mathbf{k}, 6 x+7 y+9 z=45$;

(b) $\mathbf{N}=6 \mathbf{i}-12 \mathbf{j}-10 \mathbf{k}, 3 x-6 y-5 z=16$

1.60. (a) $-3,-6,26$; (b) $-2,-10,34$

1.61. (a) $2 \mathbf{i}+13 \mathbf{j}+23 \mathbf{k}$;

(b) $-22 \mathbf{i}+2 \mathbf{j}+37 \mathbf{k}$;

(c) $31 \mathbf{i}-16 \mathbf{j}-6 \mathbf{k}$

1.62. (a) $[5,8,-6]$;

(b) $[2,-7,1]$

(c) $[-7,-18,5]$

1.63. (a) 143 ;

(b) 17

1.64. (a) $(7,1,-3) / \sqrt{59}$;

(b) $(5 \mathbf{i}+11 \mathbf{j}-2 \mathbf{k}) / \sqrt{150}$

1.66. (a) $50-55 i$;

(b) $-16-30 i$;

(c) $\frac{1}{65}(4+7 i)$;

(d) $\frac{1}{2}(1+3 i)$;

(e) $-2-2 i$

1.67. (a) $-\frac{1}{2} i$;

(b) $\frac{1}{58}(5+27 i)$;

(c) $-1, i,-1$;

(d) $\frac{1}{50}(4+3 i)$

1.68. (a) $9-2 i$;

(b) $29-29 i$;

(c) $\frac{1}{61}(-1-41 i)$;

(d) $2+5 i, 7-3 i$;

(e) $\sqrt{29}, \sqrt{58}$

1.69. (c) Hint: If $z w=0$, then $|z w|=|z||w|=|0|=0$

1.70. (a) $(6+5 i, 5-10 i)$;

(b) $(-4+22 i, 12-16 i)$;

(d) $12+2 i$

(e) $\sqrt{90}, \sqrt{54}$

(c) $(-8-41 i,-4-33 i)$;

\section*{Algebra of Matrices}
\subsection*{2.1 Introduction}
This chapter investigates matrices and algebraic operations defined on them. These matrices may be viewed as rectangular arrays of elements where each entry depends on two subscripts (as compared with vectors, where each entry depended on only one subscript). Systems of linear equations and their solutions (Chapter 3) may be efficiently investigated using the language of matrices. Furthermore, certain abstract objects introduced in later chapters, such as "change of basis," "linear transformations," and "quadratic forms," can be represented by these matrices (rectangular arrays). On the other hand, the abstract treatment of linear algebra presented later on will give us new insight into the structure of these matrices.

The entries in our matrices will come from some arbitrary, but fixed, field $K$. The elements of $K$ are called numbers or scalars. Nothing essential is lost if the reader assumes that $K$ is the real field $\mathbf{R}$.

\subsection*{2.2 Matrices}
A matrix $A$ over a field $K$ or, simply, a matrix $A$ (when $K$ is implicit) is a rectangular array of scalars usually presented in the following form:

$$
A=\left[\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n}
\end{array}\right]
$$

The rows of such a matrix $A$ are the $m$ horizontal lists of scalars:

$$
\left(a_{11}, a_{12}, \ldots, a_{1 n}\right), \quad\left(a_{21}, a_{22}, \ldots, a_{2 n}\right), \quad \ldots, \quad\left(a_{m 1}, a_{m 2}, \ldots, a_{m n}\right)
$$

and the columns of $A$ are the $n$ vertical lists of scalars:

$$
\left[\begin{array}{c}
a_{11} \\
a_{21} \\
\cdots \\
a_{m 1}
\end{array}\right], \quad\left[\begin{array}{c}
a_{12} \\
a_{22} \\
\cdots \\
a_{m 2}
\end{array}\right], \quad \ldots, \quad\left[\begin{array}{c}
a_{1 n} \\
a_{2 n} \\
\cdots \\
a_{m n}
\end{array}\right]
$$

Note that the element $a_{i j}$, called the $i j$-entry or $i j$-element, appears in row $i$ and column $j$. We frequently denote such a matrix by simply writing $A=\left[a_{i j}\right]$.

A matrix with $m$ rows and $n$ columns is called an $m$ by $n$ matrix, written $m \times n$. The pair of numbers $m$ and $n$ is called the size of the matrix. Two matrices $A$ and $B$ are equal, written $A=B$, if they have the same size and if corresponding elements are equal. Thus, the equality of two $m \times n$ matrices is equivalent to a system of $m n$ equalities, one for each corresponding pair of elements.

A matrix with only one row is called a row matrix or row vector, and a matrix with only one column is called a column matrix or column vector. A matrix whose entries are all zero is called a zero matrix and will usually be denoted by 0 .

Matrices whose entries are all real numbers are called real matrices and are said to be matrices over $\mathbf{R}$. Analogously, matrices whose entries are all complex numbers are called complex matrices and are said to be matrices over $\mathbf{C}$. This text will be mainly concerned with such real and complex matrices.

\section*{EXAMPLE 2.1}
(a) The rectangular array $A=\left[\begin{array}{rrr}1 & -4 & 5 \\ 0 & 3 & -2\end{array}\right]$ is a $2 \times 3$ matrix. Its rows are $(1,-4,5)$ and $(0,3,-2)$,\\
and its columns are

$$
\left[\begin{array}{l}
1 \\
0
\end{array}\right], \quad\left[\begin{array}{r}
-4 \\
3
\end{array}\right], \quad\left[\begin{array}{r}
5 \\
-2
\end{array}\right]
$$

(b) The $2 \times 4$ zero matrix is the matrix $0=\left[\begin{array}{llll}0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right]$.

(c) Find $x, y, z, t$ such that

$$
\left[\begin{array}{cc}
x+y & 2 z+t \\
x-y & z-t
\end{array}\right]=\left[\begin{array}{ll}
3 & 7 \\
1 & 5
\end{array}\right]
$$

By definition of equality of matrices, the four corresponding entries must be equal. Thus,

$$
x+y=3, \quad x-y=1, \quad 2 z+t=7, \quad z-t=5
$$

Solving the above system of equations yields $x=2, y=1, z=4, t=-1$.

\subsection*{2.3 Matrix Addition and Scalar Multiplication}
Let $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ be two matrices with the same size, say $m \times n$ matrices. The sum of $A$ and $B$, written $A+B$, is the matrix obtained by adding corresponding elements from $A$ and $B$. That is,

$$
A+B=\left[\begin{array}{cccc}
a_{11}+b_{11} & a_{12}+b_{12} & \ldots & a_{1 n}+b_{1 n} \\
a_{21}+b_{21} & a_{22}+b_{22} & \ldots & a_{2 n}+b_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
a_{m 1}+b_{m 1} & a_{m 2}+b_{m 2} & \ldots & a_{m n}+b_{m n}
\end{array}\right]
$$

The product of the matrix $A$ by a scalar $k$, written $k \cdot A$ or simply $k A$, is the matrix obtained by multiplying each element of $A$ by $k$. That is,

$$
k A=\left[\begin{array}{cccc}
k a_{11} & k a_{12} & \ldots & k a_{1 n} \\
k a_{21} & k a_{22} & \ldots & k a_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
k a_{m 1} & k a_{m 2} & \ldots & k a_{m n}
\end{array}\right]
$$

Observe that $A+B$ and $k A$ are also $m \times n$ matrices. We also define

$$
-A=(-1) A \quad \text { and } \quad A-B=A+(-B)
$$

The matrix $-A$ is called the negative of the matrix $A$, and the matrix $A-B$ is called the difference of $A$ and $B$. The sum of matrices with different sizes is not defined.

EXAMPLE 2.2 Let $A=\left[\begin{array}{rrr}1 & -2 & 3 \\ 0 & 4 & 5\end{array}\right]$ and $B=\left[\begin{array}{rrr}4 & 6 & 8 \\ 1 & -3 & -7\end{array}\right]$. Then

$$
\begin{gathered}
A+B=\left[\begin{array}{ccc}
1+4 & -2+6 & 3+8 \\
0+1 & 4+(-3) & 5+(-7)
\end{array}\right]=\left[\begin{array}{ccc}
5 & 4 & 11 \\
1 & 1 & -2
\end{array}\right] \\
3 A=\left[\begin{array}{rrr}
3(1) & 3(-2) & 3(3) \\
3(0) & 3(4) & 3(5)
\end{array}\right]=\left[\begin{array}{rrr}
3 & -6 & 9 \\
0 & 12 & 15
\end{array}\right] \\
2 A-3 B=\left[\begin{array}{rrr}
2 & -4 & 6 \\
0 & 8 & 10
\end{array}\right]+\left[\begin{array}{rrr}
-12 & -18 & -24 \\
-3 & 9 & 21
\end{array}\right]=\left[\begin{array}{rrr}
-10 & -22 & -18 \\
-3 & 17 & 31
\end{array}\right]
\end{gathered}
$$

The matrix $2 A-3 B$ is called a linear combination of $A$ and $B$.

Basic properties of matrices under the operations of matrix addition and scalar multiplication follow.

THEOREM 2.1: Consider any matrices $A, B, C$ (with the same size) and any scalars $k$ and $k^{\prime}$. Then

$$
\begin{array}{llll}
\text { (i) } & (A+B)+C=A+(B+C), & \text { (v) } & k(A+B)=k A+k B, \\
\text { (ii) } & A+0=0+A=A, & \text { (vi) } & \left(k+k^{\prime}\right) A=k A+k^{\prime} A, \\
\text { (iii) } & A+(-A)=(-A)+A=0, & \text { (vii) } & \left(k k^{\prime}\right) A=k\left(k^{\prime} A\right), \\
\text { (iv) } & A+B=B+A, & \text { (viii) } & 1 \cdot A=A .
\end{array}
$$

Note first that the 0 in (ii) and (iii) refers to the zero matrix. Also, by (i) and (iv), any sum of matrices

$$
A_{1}+A_{2}+\cdots+A_{n}
$$

requires no parentheses, and the sum does not depend on the order of the matrices. Furthermore, using (vi) and (viii), we also have

$$
A+A=2 A, \quad A+A+A=3 A
$$

and so on.

The proof of Theorem 2.1 reduces to showing that the $i j$-entries on both sides of each matrix equation are equal. (See Problem 2.3.)

Observe the similarity between Theorem 2.1 for matrices and Theorem 1.1 for vectors. In fact, the above operations for matrices may be viewed as generalizations of the corresponding operations for vectors.

\subsection*{2.4 Summation Symbol}
Before we define matrix multiplication, it will be instructive to first introduce the summation symbol $\Sigma$ (the Greek capital letter sigma).

Suppose $f(k)$ is an algebraic expression involving the letter $k$. Then the expression

$$
\sum_{k=1}^{n} f(k) \quad \text { or equivalently } \quad \sum_{k=1}^{n} f(k)
$$

has the following meaning. First we set $k=1$ in $f(k)$, obtaining

$$
f(1)
$$

Then we set $k=2$ in $f(k)$, obtaining $f(2)$, and add this to $f(1)$, obtaining

$$
f(1)+f(2)
$$

Then we set $k=3$ in $f(k)$, obtaining $f(3)$, and add this to the previous sum, obtaining

$$
f(1)+f(2)+f(3)
$$

We continue this process until we obtain the sum

$$
f(1)+f(2)+\cdots+f(n)
$$

Observe that at each step we increase the value of $k$ by 1 until we reach $n$. The letter $k$ is called the index, and 1 and $n$ are called, respectively, the lower and upper limits. Other letters frequently used as indices are $i$ and $j$.

We also generalize our definition by allowing the sum to range from any integer $n_{1}$ to any integer $n_{2}$. That is, we define

$$
\sum_{k=n_{1}}^{n_{2}} f(k)=f\left(n_{1}\right)+f\left(n_{1}+1\right)+f\left(n_{1}+2\right)+\cdots+f\left(n_{2}\right)
$$

\section*{EXAMPLE 2.3}
(a) $\sum_{k=1}^{5} x_{k}=x_{1}+x_{2}+x_{3}+x_{4}+x_{5}$ and $\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}$

(b) $\sum_{j=2}^{5} j^{2}=2^{2}+3^{2}+4^{2}+5^{2}=54$ and $\sum_{i=0}^{n} a_{i} x^{i}=a_{0}+a_{1} x+a_{2} x^{2}+\cdots+a_{n} x^{n}$

(c) $\sum_{k=1}^{p} a_{i k} b_{k j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+a_{i 3} b_{3 j}+\cdots+a_{i p} b_{p j}$

\subsection*{2.5 Matrix Multiplication}
The product of matrices $A$ and $B$, written $A B$, is somewhat complicated. For this reason, we first begin with a special case.

The product $A B$ of a row matrix $A=\left[a_{i}\right]$ and a column matrix $B=\left[b_{i}\right]$ with the same number of elements is defined to be the scalar (or $1 \times 1$ matrix) obtained by multiplying corresponding entries and adding; that is,

$$
A B=\left[a_{1}, a_{2}, \ldots, a_{n}\right]\left[\begin{array}{c}
b_{1} \\
b_{2} \\
\cdots \\
b_{n}
\end{array}\right]=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}=\sum_{k=1}^{n} a_{k} b_{k}
$$

We emphasize that $A B$ is a scalar (or a $1 \times 1$ matrix). The product $A B$ is not defined when $A$ and $B$ have different numbers of elements.

\section*{EXAMPLE 2.4}
(a) $[7,-4,5]\left[\begin{array}{r}3 \\ 2 \\ -1\end{array}\right]=7(3)+(-4)(2)+5(-1)=21-8-5=8$

(b) $[6,-1,8,3]\left[\begin{array}{r}4 \\ -9 \\ -2 \\ 5\end{array}\right]=24+9-16+15=32$

We are now ready to define matrix multiplication in general.

DEFINITION: Suppose $A=\left[a_{i k}\right]$ and $B=\left[b_{k j}\right]$ are matrices such that the number of columns of $A$ is equal to the number of rows of $B$; say, $A$ is an $m \times p$ matrix and $B$ is a $p \times n$ matrix. Then the product $A B$ is the $m \times n$ matrix whose $i j$-entry is obtained by multiplying the $i$ th row of $A$ by the $j$ th column of $B$. That is,

$$
\left[\begin{array}{ccc}
a_{11} & \ldots & a_{1 p} \\
\cdot & \ldots & \cdot \\
a_{i 1} & \ldots & a_{i p} \\
\cdot & \ldots & \cdot \\
a_{m 1} & \ldots & a_{m p}
\end{array}\right]\left[\begin{array}{ccccc}
b_{11} & \ldots & b_{1 j} & \ldots & b_{1 n} \\
\cdot & \ldots & \cdot & \ldots & \cdot \\
\cdot & \ldots & \cdot & \ldots & \cdot \\
\cdot & \ldots & \cdot & \ldots & \cdot \\
b_{p 1} & \ldots & b_{p j} & \ldots & b_{p n}
\end{array}\right]=\left[\begin{array}{ccc}
c_{11} & \ldots & c_{1 n} \\
\cdot & \ldots & \cdot \\
\cdot & c_{i j} & \cdot \\
\cdot & \ldots & \cdot \\
c_{m 1} & \ldots & c_{m n}
\end{array}\right]
$$

where

$$
c_{i j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i p} b_{p j}=\sum_{k=1}^{p} a_{i k} b_{k j}
$$

The product $A B$ is not defined if $A$ is an $m \times p$ matrix and $B$ is a $q \times n$ matrix, where $p \neq q$.

\section*{EXAMPLE 2.5}
(a) Find $A B$ where $A=\left[\begin{array}{rr}1 & 3 \\ 2 & -1\end{array}\right]$ and $B=\left[\begin{array}{rrr}2 & 0 & -4 \\ 5 & -2 & 6\end{array}\right]$.

Because $A$ is $2 \times 2$ and $B$ is $2 \times 3$, the product $A B$ is defined and $A B$ is a $2 \times 3$ matrix. To obtain the first row of the product matrix $A B$, multiply the first row $[1,3]$ of $A$ by each column of $B$,

$$
\left[\begin{array}{l}
2 \\
5
\end{array}\right], \quad\left[\begin{array}{r}
0 \\
-2
\end{array}\right], \quad\left[\begin{array}{r}
-4 \\
6
\end{array}\right]
$$

respectively. That is,

$$
A B=\left[\begin{array}{lll}
2+15 & 0-6 & -4+18
\end{array}\right]=\left[\begin{array}{ccc}
17 & -6 & 14
\end{array}\right]
$$

To obtain the second row of $A B$, multiply the second row $[2,-1]$ of $A$ by each column of $B$. Thus,

$$
A B=\left[\begin{array}{ccc}
17 & -6 & 14 \\
4-5 & 0+2 & -8-6
\end{array}\right]=\left[\begin{array}{rrr}
17 & -6 & 14 \\
-1 & 2 & -14
\end{array}\right]
$$

(b) Suppose $A=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right]$ and $B=\left[\begin{array}{rr}5 & 6 \\ 0 & -2\end{array}\right]$. Then

$$
A B=\left[\begin{array}{rr}
5+0 & 6-4 \\
15+0 & 18-8
\end{array}\right]=\left[\begin{array}{rr}
5 & 2 \\
15 & 10
\end{array}\right] \quad \text { and } \quad B A=\left[\begin{array}{rr}
5+18 & 10+24 \\
0-6 & 0-8
\end{array}\right]=\left[\begin{array}{rr}
23 & 34 \\
-6 & -8
\end{array}\right]
$$

The above example shows that matrix multiplication is not commutative - that is, in general, $A B \neq B A$. However, matrix multiplication does satisfy the following properties.

THEOREM 2.2: Let $A, B, C$ be matrices. Then, whenever the products and sums are defined,

(i) $(A B) C=A(B C)$ (associative law),

(ii) $A(B+C)=A B+A C$ (left distributive law),

(iii) $(B+C) A=B A+C A$ (right distributive law),

(iv) $k(A B)=(k A) B=A(k B)$, where $k$ is a scalar.

We note that $0 A=0$ and $B 0=0$, where 0 is the zero matrix.

\subsection*{2.6 Transpose of a Matrix}
The transpose of a matrix $A$, written $A^{T}$, is the matrix obtained by writing the columns of $A$, in order, as rows. For example,

$$
\left[\begin{array}{lll}
1 & 2 & 3 \\
4 & 5 & 6
\end{array}\right]^{T}=\left[\begin{array}{ll}
1 & 4 \\
2 & 5 \\
3 & 6
\end{array}\right] \quad \text { and } \quad[1,-3,-5]^{T}=\left[\begin{array}{r}
1 \\
-3 \\
-5
\end{array}\right]
$$

In other words, if $A=\left[a_{i j}\right]$ is an $m \times n$ matrix, then $A^{T}=\left[b_{i j}\right]$ is the $n \times m$ matrix where $b_{i j}=a_{j i}$.

Observe that the tranpose of a row vector is a column vector. Similarly, the transpose of a column vector is a row vector.

The next theorem lists basic properties of the transpose operation.

THEOREM 2.3: Let $A$ and $B$ be matrices and let $k$ be a scalar. Then, whenever the sum and product are defined,\\
(i) $(A+B)^{T}=A^{T}+B^{T}$,\\
(iii) $(k A)^{T}=k A^{T}$,\\
(ii) $\left(A^{T}\right)^{T}=A$,\\
(iv) $(A B)^{T}=B^{T} A^{T}$.

We emphasize that, by (iv), the transpose of a product is the product of the transposes, but in the reverse order.

\subsection*{2.7 Square Matrices}
A square matrix is a matrix with the same number of rows as columns. An $n \times n$ square matrix is said to be of order $n$ and is sometimes called an $n$-square matrix.

Recall that not every two matrices can be added or multiplied. However, if we only consider square matrices of some given order $n$, then this inconvenience disappears. Specifically, the operations of addition, multiplication, scalar multiplication, and transpose can be performed on any $n \times n$ matrices, and the result is again an $n \times n$ matrix.

EXAMPLE 2.6 The following are square matrices of order 3:

$$
A=\left[\begin{array}{rrr}
1 & 2 & 3 \\
-4 & -4 & -4 \\
5 & 6 & 7
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{rrr}
2 & -5 & 1 \\
0 & 3 & -2 \\
1 & 2 & -4
\end{array}\right]
$$

The following are also matrices of order 3 :

$$
\begin{aligned}
& A+B=\left[\begin{array}{rrr}
3 & -3 & 4 \\
-4 & -1 & -6 \\
6 & 8 & 3
\end{array}\right], \quad 2 A=\left[\begin{array}{rrr}
2 & 4 & 6 \\
-8 & -8 & -8 \\
10 & 12 & 14
\end{array}\right], \quad A^{T}=\left[\begin{array}{lll}
1 & -4 & 5 \\
2 & -4 & 6 \\
3 & -4 & 7
\end{array}\right] \\
& A B=\left[\begin{array}{rrr}
5 & 7 & -15 \\
-12 & 0 & 20 \\
17 & 7 & -35
\end{array}\right], \quad B A=\left[\begin{array}{rrr}
27 & 30 & 33 \\
-22 & -24 & -26 \\
-27 & -30 & -33
\end{array}\right]
\end{aligned}
$$

\section*{Diagonal and Trace}
Let $A=\left[a_{i j}\right]$ be an $n$-square matrix. The diagonal or main diagonal of $A$ consists of the elements with the same subscripts - that is,

$$
a_{11}, \quad a_{22}, \quad a_{33}, \quad \ldots, \quad a_{n n}
$$

The trace of $A$, written $\operatorname{tr}(A)$, is the sum of the diagonal elements. Namely,

$$
\operatorname{tr}(A)=a_{11}+a_{22}+a_{33}+\cdots+a_{n n}
$$

The following theorem applies.

THEOREM 2.4: Suppose $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ are $n$-square matrices and $k$ is a scalar. Then\\
(i) $\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B)$,\\
(iii) $\operatorname{tr}\left(A^{T}\right)=\operatorname{tr}(A)$,\\
(ii) $\operatorname{tr}(k A)=k \operatorname{tr}(A)$,\\
(iv) $\operatorname{tr}(A B)=\operatorname{tr}(B A)$.

EXAMPLE 2.7 Let $A$ and $B$ be the matrices $A$ and $B$ in Example 2.6. Then

$$
\begin{array}{lll}
\text { diagonal of } A=\{1,-4,7\} & \text { and } & \operatorname{tr}(A)=1-4+7=4 \\
\text { diagonal of } B=\{2,3,-4\} & \text { and } & \operatorname{tr}(B)=2+3-4=1
\end{array}
$$

Moreover,

$$
\begin{array}{ll}
\operatorname{tr}(A+B)=3-1+3=5, & \operatorname{tr}(2 A)=2-8+14=8, \quad \operatorname{tr}\left(A^{T}\right)=1-4+7=4 \\
\operatorname{tr}(A B)=5+0-35=-30, & \operatorname{tr}(B A)=27-24-33=-30
\end{array}
$$

As expected from Theorem 2.4,

$$
\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B), \quad \operatorname{tr}\left(A^{T}\right)=\operatorname{tr}(A), \quad \operatorname{tr}(2 A)=2 \operatorname{tr}(A)
$$

Furthermore, although $A B \neq B A$, the traces are equal.

\section*{Identity Matrix, Scalar Matrices}
The $n$-square identity or unit matrix, denoted by $I_{n}$, or simply $I$, is the $n$-square matrix with 1 's on the diagonal and 0 's elsewhere. The identity matrix $I$ is similar to the scalar 1 in that, for any $n$-square matrix A,

$$
A I=I A=A
$$

More generally, if $B$ is an $m \times n$ matrix, then $B I_{n}=I_{m} B=B$.

For any scalar $k$, the matrix $k I$ that contains $k$ 's on the diagonal and 0 's elsewhere is called the scalar matrix corresponding to the scalar $k$. Observe that

$$
(k I) A=k(I A)=k A
$$

That is, multiplying a matrix $A$ by the scalar matrix $k I$ is equivalent to multiplying $A$ by the scalar $k$.

EXAMPLE 2.8 The following are the identity matrices of orders 3 and 4 and the corresponding scalar matrices for $k=5$ :

$$
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right], \quad\left[\begin{array}{llll}
1 & & & \\
& 1 & & \\
& & 1 & \\
& & & 1
\end{array}\right], \quad\left[\begin{array}{lll}
5 & 0 & 0 \\
0 & 5 & 0 \\
0 & 0 & 5
\end{array}\right], \quad\left[\begin{array}{llll}
5 & & & \\
& 5 & & \\
& & 5 & \\
& & & 5
\end{array}\right]
$$

Remark 1: It is common practice to omit blocks or patterns of 0 's when there is no ambiguity, as in the above second and fourth matrices.

Remark 2: The Kronecker delta function $\delta_{i j}$ is defined by

$$
\delta_{i j}= \begin{cases}0 & \text { if } i \neq j \\ 1 & \text { if } i=j\end{cases}
$$

Thus, the identity matrix may be defined by $I=\left[\delta_{i j}\right]$.

\subsection*{2.8 Powers of Matrices, Polynomials in Matrices}
Let $A$ be an $n$-square matrix over a field $K$. Powers of $A$ are defined as follows:

$$
A^{2}=A A, \quad A^{3}=A^{2} A, \quad \ldots, \quad A^{n+1}=A^{n} A, \quad \ldots, \quad \text { and } \quad A^{0}=I
$$

Polynomials in the matrix $A$ are also defined. Specifically, for any polynomial

$$
f(x)=a_{0}+a_{1} x+a_{2} x^{2}+\cdots+a_{n} x^{n}
$$

where the $a_{i}$ are scalars in $K, f(A)$ is defined to be the following matrix:

$$
f(A)=a_{0} I+a_{1} A+a_{2} A^{2}+\cdots+a_{n} A^{n}
$$

[Note that $f(A)$ is obtained from $f(x)$ by substituting the matrix $A$ for the variable $x$ and substituting the scalar matrix $a_{0} I$ for the scalar $a_{0}$.] If $f(A)$ is the zero matrix, then $A$ is called a zero or root of $f(x)$.

EXAMPLE 2.9 Suppose $A=\left[\begin{array}{rr}1 & 2 \\ 3 & -4\end{array}\right]$. Then

$A^{2}=\left[\begin{array}{rr}1 & 2 \\ 3 & -4\end{array}\right]\left[\begin{array}{rr}1 & 2 \\ 3 & -4\end{array}\right]=\left[\begin{array}{rr}7 & -6 \\ -9 & 22\end{array}\right] \quad$ and $A^{3}=A^{2} A=\left[\begin{array}{rr}7 & -6 \\ -9 & 22\end{array}\right]\left[\begin{array}{rr}1 & 2 \\ 3 & -4\end{array}\right]=\left[\begin{array}{rr}-11 & 38 \\ 57 & -106\end{array}\right]$

Suppose $f(x)=2 x^{2}-3 x+5$ and $g(x)=x^{2}+3 x-10$. Then

$$
\begin{aligned}
& f(A)=2\left[\begin{array}{rr}
7 & -6 \\
-9 & 22
\end{array}\right]-3\left[\begin{array}{rr}
1 & 2 \\
3 & -4
\end{array}\right]+5\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]=\left[\begin{array}{rr}
16 & -18 \\
-27 & 61
\end{array}\right] \\
& g(A)=\left[\begin{array}{rr}
7 & -6 \\
-9 & 22
\end{array}\right]+3\left[\begin{array}{rr}
1 & 2 \\
3 & -4
\end{array}\right]-10\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right]
\end{aligned}
$$

Thus, $A$ is a zero of the polynomial $g(x)$.

\subsection*{2.9 Invertible (Nonsingular) Matrices}
A square matrix $A$ is said to be invertible or nonsingular if there exists a matrix $B$ such that

$$
A B=B A=I
$$

where $I$ is the identity matrix. Such a matrix $B$ is unique. That is, if $A B_{1}=B_{1} A=I$ and $A B_{2}=B_{2} A=I$, then

$$
B_{1}=B_{1} I=B_{1}\left(A B_{2}\right)=\left(B_{1} A\right) B_{2}=I B_{2}=B_{2}
$$

We call such a matrix $B$ the inverse of $A$ and denote it by $A^{-1}$. Observe that the above relation is symmetric; that is, if $B$ is the inverse of $A$, then $A$ is the inverse of $B$.

EXAMPLE 2.10 Suppose that $A=\left[\begin{array}{ll}2 & 5 \\ 1 & 3\end{array}\right]$ and $B=\left[\begin{array}{rr}3 & -5 \\ -1 & 2\end{array}\right]$. Then

$$
A B=\left[\begin{array}{rr}
6-5 & -10+10 \\
3-3 & -5+6
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right] \quad \text { and } \quad B A=\left[\begin{array}{rr}
6-5 & 15-15 \\
-2+2 & -5+6
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]
$$

Thus, $A$ and $B$ are inverses.

It is known (Theorem 3.16) that $A B=I$ if and only if $B A=I$. Thus, it is necessary to test only one product to determine whether or not two given matrices are inverses. (See Problem 2.17.)

Now suppose $A$ and $B$ are invertible. Then $A B$ is invertible and $(A B)^{-1}=B^{-1} A^{-1}$. More generally, if $A_{1}, A_{2}, \ldots, A_{k}$ are invertible, then their product is invertible and

$$
\left(A_{1} A_{2} \ldots A_{k}\right)^{-1}=A_{k}^{-1} \ldots A_{2}^{-1} A_{1}^{-1}
$$

the product of the inverses in the reverse order.

\section*{Inverse of a $2 \times 2$ Matrix}
Let $A$ be an arbitrary $2 \times 2$ matrix, say $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$. We want to derive a formula for $A^{-1}$, the inverse of $A$. Specifically, we seek $2^{2}=4$ scalars, say $x_{1}, y_{1}, x_{2}, y_{2}$, such that

$$
\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]\left[\begin{array}{ll}
x_{1} & x_{2} \\
y_{1} & y_{2}
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right] \quad \text { or } \quad\left[\begin{array}{ll}
a x_{1}+b y_{1} & a x_{2}+b y_{2} \\
c x_{1}+d y_{1} & c x_{2}+d y_{2}
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]
$$

Setting the four entries equal to the corresponding entries in the identity matrix yields four equations, which can be partitioned into two $2 \times 2$ systems as follows:

$$
\begin{aligned}
& a x_{1}+b y_{1}=1, \quad a x_{2}+b y_{2}=0 \\
& c x_{1}+d y_{1}=0, \quad c x_{2}+d y_{2}=1
\end{aligned}
$$

Suppose we let $|A|=a b-b c$ (called the determinant of $A$ ). Assuming $|A| \neq 0$, we can solve uniquely for the above unknowns $x_{1}, y_{1}, x_{2}, y_{2}$, obtaining

$$
x_{1}=\frac{d}{|A|}, \quad y_{1}=\frac{-c}{|A|}, \quad x_{2}=\frac{-b}{|A|}, \quad y_{2}=\frac{a}{|A|}
$$

Accordingly,

$$
A^{-1}=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right]^{-1}=\left[\begin{array}{rr}
d /|A| & -b /|A| \\
-c /|A| & a /|A|
\end{array}\right]=\frac{1}{|A|}\left[\begin{array}{rr}
d & -b \\
-c & a
\end{array}\right]
$$

In other words, when $|A| \neq 0$, the inverse of a $2 \times 2$ matrix $A$ may be obtained from $A$ as follows:

(1) Interchange the two elements on the diagonal.

(2) Take the negatives of the other two elements.

(3) Multiply the resulting matrix by $1 /|A|$ or, equivalently, divide each element by $|A|$.

In case $|A|=0$, the matrix $A$ is not invertible.

EXAMPLE 2.11 Find the inverse of $A=\left[\begin{array}{ll}2 & 3 \\ 4 & 5\end{array}\right]$ and $B=\left[\begin{array}{ll}1 & 3 \\ 2 & 6\end{array}\right]$.

First evaluate $|A|=2(5)-3(4)=10-12=-2$. Because $|A| \neq 0$, the matrix $A$ is invertible and

$$
A^{-1}=\frac{1}{-2}\left[\begin{array}{rr}
5 & -3 \\
-4 & 2
\end{array}\right]=\left[\begin{array}{rr}
-\frac{5}{2} & \frac{3}{2} \\
2 & -1
\end{array}\right]
$$

Now evaluate $|B|=1(6)-3(2)=6-6=0$. Because $|B|=0$, the matrix $B$ has no inverse.

Remark: The above property that a matrix is invertible if and only if $A$ has a nonzero determinant is true for square matrices of any order. (See Chapter 8.)

\section*{Inverse of an $\boldsymbol{n} \times \boldsymbol{n}$ Matrix}
Suppose $A$ is an arbitrary $n$-square matrix. Finding its inverse $A^{-1}$ reduces, as above, to finding the solution of a collection of $n \times n$ systems of linear equations. The solution of such systems and an efficient way of solving such a collection of systems is treated in Chapter 3.

\subsection*{2.10 Special Types of Square Matrices}
This section describes a number of special kinds of square matrices.

\section*{Diagonal and Triangular Matrices}
A square matrix $D=\left[d_{i j}\right]$ is diagonal if its nondiagonal entries are all zero. Such a matrix is sometimes denoted by

$$
D=\operatorname{diag}\left(d_{11}, d_{22}, \ldots, d_{n n}\right)
$$

where some or all the $d_{i i}$ may be zero. For example,

$$
\left[\begin{array}{rrr}
3 & 0 & 0 \\
0 & -7 & 0 \\
0 & 0 & 2
\end{array}\right], \quad\left[\begin{array}{rr}
4 & 0 \\
0 & -5
\end{array}\right], \quad\left[\begin{array}{llll}
6 & & & \\
& 0 & & \\
& & -9 & \\
& & & 8
\end{array}\right]
$$

are diagonal matrices, which may be represented, respectively, by

$$
\operatorname{diag}(3,-7,2), \quad \operatorname{diag}(4,-5), \quad \operatorname{diag}(6,0,-9,8)
$$

(Observe that patterns of 0 's in the third matrix have been omitted.)

A square matrix $A=\left[a_{i j}\right]$ is upper triangular or simply triangular if all entries below the (main) diagonal are equal to 0 - that is, if $a_{i j}=0$ for $i>j$. Generic upper triangular matrices of orders $2,3,4$ are as follows:

$$
\left[\begin{array}{cc}
a_{11} & a_{12} \\
0 & a_{22}
\end{array}\right], \quad\left[\begin{array}{ccc}
b_{11} & b_{12} & b_{13} \\
& b_{22} & b_{23} \\
& & b_{33}
\end{array}\right], \quad\left[\begin{array}{cccc}
c_{11} & c_{12} & c_{13} & c_{14} \\
& c_{22} & c_{23} & c_{24} \\
& & c_{33} & c_{34} \\
& & & c_{44}
\end{array}\right]
$$

(As with diagonal matrices, it is common practice to omit patterns of 0 's.)

The following theorem applies.

THEOREM 2.5: Suppose $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ are $n \times n$ (upper) triangular matrices. Then

(i) $\quad A+B, k A, A B$ are triangular with respective diagonals:

$$
\left(a_{11}+b_{11}, \ldots, a_{n n}+b_{n n}\right), \quad\left(k a_{11}, \ldots, k a_{n n}\right), \quad\left(a_{11} b_{11}, \ldots, a_{n n} b_{n n}\right)
$$

(ii) For any polynomial $f(x)$, the matrix $f(A)$ is triangular with diagonal

$$
\left(f\left(a_{11}\right), f\left(a_{22}\right), \ldots, f\left(a_{n n}\right)\right)
$$

(iii) $A$ is invertible if and only if each diagonal element $a_{i i} \neq 0$, and when $A^{-1}$ exists it is also triangular.

A lower triangular matrix is a square matrix whose entries above the diagonal are all zero. We note that Theorem 2.5 is true if we replace "triangular" by either "lower triangular" or "diagonal."

Remark: A nonempty collection $A$ of matrices is called an algebra (of matrices) if $A$ is closed under the operations of matrix addition, scalar multiplication, and matrix multiplication. Clearly, the square matrices with a given order form an algebra of matrices, but so do the scalar, diagonal, triangular, and lower triangular matrices.

\section*{Special Real Square Matrices: Symmetric, Orthogonal, Normal [Optional until Chapter 12]}
Suppose now $A$ is a square matrix with real entries-that is, a real square matrix. The relationship between $A$ and its transpose $A^{T}$ yields important kinds of matrices.

\section*{(a) Symmetric Matrices}
A matrix $A$ is symmetric if $A^{T}=A$. Equivalently, $A=\left[a_{i j}\right]$ is symmetric if symmetric elements (mirror elements with respect to the diagonal) are equal-that is, if each $a_{i j}=a_{j i}$.

A matrix $A$ is skew-symmetric if $A^{T}=-A$ or, equivalently, if each $a_{i j}=-a_{j i}$. Clearly, the diagonal elements of such a matrix must be zero, because $a_{i i}=-a_{i i}$ implies $a_{i i}=0$.

(Note that a matrix $A$ must be square if $A^{T}=A$ or $A^{T}=-A$.)

EXAMPLE 2.12 Let $A=\left[\begin{array}{rrr}2 & -3 & 5 \\ -3 & 6 & 7 \\ 5 & 7 & -8\end{array}\right], B=\left[\begin{array}{rrr}0 & 3 & -4 \\ -3 & 0 & 5 \\ 4 & -5 & 0\end{array}\right], C=\left[\begin{array}{lll}1 & 0 & 0 \\ 0 & 0 & 1\end{array}\right]$.

(a) By inspection, the symmetric elements in $A$ are equal, or $A^{T}=A$. Thus, $A$ is symmetric.

(b) The diagonal elements of $B$ are 0 and symmetric elements are negatives of each other, or $B^{T}=-B$. Thus, $B$ is skew-symmetric.

(c) Because $C$ is not square, $C$ is neither symmetric nor skew-symmetric.

\section*{(b) Orthogonal Matrices}
A real matrix $A$ is orthogonal if $A^{T}=A^{-1}$-that is, if $A A^{T}=A^{T} A=I$. Thus, $A$ must necessarily be square and invertible.

EXAMPLE 2.13 Let $A=\left[\begin{array}{rrr}\frac{1}{9} & \frac{8}{9} & -\frac{4}{9} \\ \frac{4}{9} & -\frac{4}{9} & -\frac{7}{9} \\ \frac{8}{9} & \frac{1}{9} & \frac{4}{9}\end{array}\right]$. Multiplying $A$ by $A^{T}$ yields $I$; that is, $A A^{T}=I$. This means $A^{T} A=I$, as well. Thus, $A^{T}=A^{-1}$; that is, $A$ is orthogonal.

Now suppose $A$ is a real orthogonal $3 \times 3$ matrix with rows

$$
u_{1}=\left(a_{1}, a_{2}, a_{3}\right), \quad u_{2}=\left(b_{1}, b_{2}, b_{3}\right), \quad u_{3}=\left(c_{1}, c_{2}, c_{3}\right)
$$

Because $A$ is orthogonal, we must have $A A^{T}=I$. Namely,

$$
A A^{T}=\left[\begin{array}{lll}
a_{1} & a_{2} & a_{3} \\
b_{1} & b_{2} & b_{3} \\
c_{1} & c_{2} & c_{3}
\end{array}\right]\left[\begin{array}{lll}
a_{1} & b_{1} & c_{1} \\
a_{2} & b_{2} & c_{2} \\
a_{3} & b_{3} & c_{3}
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]=I
$$

Multiplying $A$ by $A^{T}$ and setting each entry equal to the corresponding entry in $I$ yields the following nine equations:

$$
\begin{aligned}
& a_{1}^{2}+a_{2}^{2}+a_{3}^{2}=1, \quad a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}=0, \quad a_{1} c_{1}+a_{2} c_{2}+a_{3} c_{3}=0 \\
& b_{1} a_{1}+b_{2} a_{2}+b_{3} a_{3}=0, \quad b_{1}^{2}+b_{2}^{2}+b_{3}^{2}=1, \quad b_{1} c_{1}+b_{2} c_{2}+b_{3} c_{3}=0 \\
& c_{1} a_{1}+c_{2} a_{2}+c_{3} a_{3}=0, \quad c_{1} b_{1}+c_{2} b_{2}+c_{3} b_{3}=0, \quad c_{1}^{2}+c_{2}^{2}+c_{3}^{2}=1
\end{aligned}
$$

Accordingly, $u_{1} \cdot u_{1}=1, u_{2} \cdot u_{2}=1, u_{3} \cdot u_{3}=1$, and $u_{i} \cdot u_{j}=0$ for $i \neq j$. Thus, the rows $u_{1}, u_{2}, u_{3}$ are unit vectors and are orthogonal to each other.

Generally speaking, vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $\mathbf{R}^{n}$ are said to form an orthonormal set of vectors if the vectors are unit vectors and are orthogonal to each other; that is,

$$
u_{i} \cdot u_{j}= \begin{cases}0 & \text { if } i \neq j \\ 1 & \text { if } i=j\end{cases}
$$

In other words, $u_{i} \cdot u_{j}=\delta_{i j}$ where $\delta_{i j}$ is the Kronecker delta function.

We have shown that the condition $A A^{T}=I$ implies that the rows of $A$ form an orthonormal set of vectors. The condition $A^{T} A=I$ similarly implies that the columns of $A$ also form an orthonormal set of vectors. Furthermore, because each step is reversible, the converse is true.

The above results for $3 \times 3$ matrices are true in general. That is, the following theorem holds.

THEOREM 2.6: Let $A$ be a real matrix. Then the following are equivalent:

(a) $A$ is orthogonal.

(b) The rows of $A$ form an orthonormal set.

(c) The columns of $A$ form an orthonormal set.

For $n=2$, we have the following result (proved in Problem 2.28).

THEOREM 2.7: Let $A$ be a real $2 \times 2$ orthogonal matrix. Then, for some real number $\theta$,

$$
A=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right] \quad \text { or } \quad A=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{array}\right]
$$

\section*{(c) Normal Matrices}
A real matrix $A$ is normal if it commutes with its transpose $A^{T}$ - that is, if $A A^{T}=A^{T} A$. If $A$ is symmetric, orthogonal, or skew-symmetric, then $A$ is normal. There are also other normal matrices.

EXAMPLE 2.14 Let $A=\left[\begin{array}{rr}6 & -3 \\ 3 & 6\end{array}\right]$. Then

$$
A A^{T}=\left[\begin{array}{rr}
6 & -3 \\
3 & 6
\end{array}\right]\left[\begin{array}{rr}
6 & 3 \\
-3 & 6
\end{array}\right]=\left[\begin{array}{rr}
45 & 0 \\
0 & 45
\end{array}\right] \quad \text { and } \quad A^{T} A=\left[\begin{array}{rr}
6 & 3 \\
-3 & 6
\end{array}\right]\left[\begin{array}{rr}
6 & -3 \\
3 & 6
\end{array}\right]=\left[\begin{array}{rr}
45 & 0 \\
0 & 45
\end{array}\right]
$$

Because $A A^{T}=A^{T} A$, the matrix $A$ is normal.

\subsection*{2.11 Complex Matrices}
Let $A$ be a complex matrix-that is, a matrix with complex entries. Recall (Section 1.7) that if $z=a+b i$ is a complex number, then $\bar{z}=a-b i$ is its conjugate. The conjugate of a complex matrix $A$, written $\bar{A}$, is the matrix obtained from $A$ by taking the conjugate of each entry in $A$. That is, if $A=\left[a_{i j}\right]$, then $\bar{A}=\left[b_{i j}\right]$, where $b_{i j}=\bar{a}_{i j}$. (We denote this fact by writing $\bar{A}=\left[\bar{a}_{i j}\right]$.)

The two operations of transpose and conjugation commute for any complex matrix $A$, and the special notation $A^{H}$ is used for the conjugate transpose of $A$. That is,

$$
A^{H}=(\bar{A})^{T}=\left(\overline{A^{T}}\right)
$$

Note that if $A$ is real, then $A^{H}=A^{T}$. [Some texts use $A^{*}$ instead of $A^{H}$.]

EXAMPLE 2.15 Let $A=\left[\begin{array}{ccc}2+8 i & 5-3 i & 4-7 i \\ 6 i & 1-4 i & 3+2 i\end{array}\right]$. Then $A^{H}=\left[\begin{array}{cc}2-8 i & -6 i \\ 5+3 i & 1+4 i \\ 4+7 i & 3-2 i\end{array}\right]$.

\section*{Special Complex Matrices: Hermitian, Unitary, Normal [Optional until Chapter 12]}
Consider a complex matrix $A$. The relationship between $A$ and its conjugate transpose $A^{H}$ yields important kinds of complex matrices (which are analogous to the kinds of real matrices described above).

A complex matrix $A$ is said to be Hermitian or skew-Hermitian according as to whether

$$
A^{H}=A \quad \text { or } \quad A^{H}=-A \text {. }
$$

Clearly, $A=\left[a_{i j}\right]$ is Hermitian if and only if symmetric elements are conjugate-that is, if each $a_{i j}=\bar{a}_{j i}-$ in which case each diagonal element $a_{i i}$ must be real. Similarly, if $A$ is skew-symmetric, then each diagonal element $a_{i i}=0$. (Note that $A$ must be square if $A^{H}=A$ or $A^{H}=-A$.)

A complex matrix $A$ is unitary if $A^{H} A^{-1}=A^{-1} A^{H}=I$-that is, if

$$
A^{H}=A^{-1} \text {. }
$$

Thus, $A$ must necessarily be square and invertible. We note that a complex matrix $A$ is unitary if and only if its rows (columns) form an orthonormal set relative to the dot product of complex vectors.

A complex matrix $A$ is said to be normal if it commutes with $A^{H}$ - that is, if

$$
A A^{H}=A^{H} A
$$

(Thus, $A$ must be a square matrix.) This definition reduces to that for real matrices when $A$ is real.

EXAMPLE 2.16 Consider the following complex matrices:

$A=\left[\begin{array}{ccc}3 & 1-2 i & 4+7 i \\ 1+2 i & -4 & -2 i \\ 4-7 i & 2 i & 5\end{array}\right] \quad B=\frac{1}{2}\left[\begin{array}{ccc}1 & -i & -1+i \\ i & 1 & 1+i \\ 1+i & -1+i & 0\end{array}\right] \quad C=\left[\begin{array}{cc}2+3 i & 1 \\ i & 1+2 i\end{array}\right]$

(a) By inspection, the diagonal elements of $A$ are real, and the symmetric elements $1-2 i$ and $1+2 i$ are conjugate, $4+7 i$ and $4-7 i$ are conjugate, and $-2 i$ and $2 i$ are conjugate. Thus, $A$ is Hermitian.

(b) Multiplying $B$ by $B^{H}$ yields $I$; that is, $B B^{H}=I$. This implies $B^{H} B=I$, as well. Thus, $B^{H}=B^{-1}$, which means $B$ is unitary.

(c) To show $C$ is normal, we evaluate $C C^{H}$ and $C^{H} C$ :

$$
C C^{H}=\left[\begin{array}{cc}
2+3 i & 1 \\
i & 1+2 i
\end{array}\right]\left[\begin{array}{cc}
2-3 i & -i \\
1 & 1-2 i
\end{array}\right]=\left[\begin{array}{cc}
14 & 4-4 i \\
4+4 i & 6
\end{array}\right]
$$

and similarly $C^{H} C=\left[\begin{array}{cc}14 & 4-4 i \\ 4+4 i & 6\end{array}\right]$. Because $C C^{H}=C^{H} C$, the complex matrix $C$ is normal.

We note that when a matrix $A$ is real, Hermitian is the same as symmetric, and unitary is the same as orthogonal.

\subsection*{2.12 Block Matrices}
Using a system of horizontal and vertical (dashed) lines, we can partition a matrix $A$ into submatrices called blocks (or cells) of $A$. Clearly a given matrix may be divided into blocks in different ways. For example,

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-046}
\end{center}

The convenience of the partition of matrices, say $A$ and $B$, into blocks is that the result of operations on $A$ and $B$ can be obtained by carrying out the computation with the blocks, just as if they were the actual elements of the matrices. This is illustrated below, where the notation $A=\left[A_{i j}\right]$ will be used for a block matrix $A$ with blocks $A_{i j}$.

Suppose that $A=\left[A_{i j}\right]$ and $B=\left[B_{i j}\right]$ are block matrices with the same numbers of row and column blocks, and suppose that corresponding blocks have the same size. Then adding the corresponding blocks of $A$ and $B$ also adds the corresponding elements of $A$ and $B$, and multiplying each block of $A$ by a scalar $k$ multiplies each element of $A$ by $k$. Thus,

$$
A+B=\left[\begin{array}{cccc}
A_{11}+B_{11} & A_{12}+B_{12} & \ldots & A_{1 n}+B_{1 n} \\
A_{21}+B_{21} & A_{22}+B_{22} & \ldots & A_{2 n}+B_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
A_{m 1}+B_{m 1} & A_{m 2}+B_{m 2} & \ldots & A_{m n}+B_{m n}
\end{array}\right]
$$

and

$$
k A=\left[\begin{array}{cccc}
k A_{11} & k A_{12} & \ldots & k A_{1 n} \\
k A_{21} & k A_{22} & \ldots & k A_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
k A_{m 1} & k A_{m 2} & \ldots & k A_{m n}
\end{array}\right]
$$

The case of matrix multiplication is less obvious, but still true. That is, suppose that $U=\left[U_{i k}\right]$ and $V=\left[V_{k j}\right]$ are block matrices such that the number of columns of each block $U_{i k}$ is equal to the number of rows of each block $V_{k j}$. (Thus, each product $U_{i k} V_{k j}$ is defined.) Then

$$
U V=\left[\begin{array}{cccc}
W_{11} & W_{12} & \cdots & W_{1 n} \\
W_{21} & W_{22} & \cdots & W_{2 n} \\
\cdots & \cdots & \cdots & \cdots \\
W_{m 1} & W_{m 2} & \cdots & W_{m n}
\end{array}\right], \quad \text { where } \quad W_{i j}=U_{i 1} V_{1 j}+U_{i 2} V_{2 j}+\cdots+U_{i p} V_{p j}
$$

The proof of the above formula for $U V$ is straightforward but detailed and lengthy. It is left as an exercise (Problem 2.85).

\section*{Square Block Matrices}
Let $M$ be a block matrix. Then $M$ is called a square block matrix if

(i) $M$ is a square matrix.

(ii) The blocks form a square matrix.

(iii) The diagonal blocks are also square matrices.

The latter two conditions will occur if and only if there are the same number of horizontal and vertical lines and they are placed symmetrically.

Consider the following two block matrices:

$$
A=\left[\begin{array}{cc:cc:c}
1 & 2 & 3 & 4 & 5 \\
1 & 1 & 1 & -1 & \frac{1}{2} \\
\hdashline 9 & 8 & 7 & 6 & 5 \\
\hdashline 4 & 4 & 4 & 4 & 4 \\
3 & 5 & 3 & 5 & 3
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{cc:cc:c}
1 & 2 & 3 & 4 & 5 \\
\hdashline 1 & 1 & 1 & 1 & 1 \\
\hdashline 9 & 8 & 7 & 6 & 5 \\
4 & 4 & 4 & 4 & 4 \\
\hdashline 3 & 5 & 3 & 5 & 3
\end{array}\right]
$$

The block matrix $A$ is not a square block matrix, because the second and third diagonal blocks are not square. On the other hand, the block matrix $B$ is a square block matrix.

\section*{Block Diagonal Matrices}
Let $M=\left[A_{i j}\right]$ be a square block matrix such that the nondiagonal blocks are all zero matrices; that is, $A_{i j}=0$ when $i \neq j$. Then $M$ is called a block diagonal matrix. We sometimes denote such a block diagonal matrix by writing

$$
M=\operatorname{diag}\left(A_{11}, A_{22}, \ldots, A_{r r}\right) \quad \text { or } \quad M=A_{11} \oplus A_{22} \oplus \cdots \oplus A_{r r}
$$

The importance of block diagonal matrices is that the algebra of the block matrix is frequently reduced to the algebra of the individual blocks. Specifically, suppose $f(x)$ is a polynomial and $M$ is the above block diagonal matrix. Then $f(M)$ is a block diagonal matrix, and

$$
f(M)=\operatorname{diag}\left(f\left(A_{11}\right), f\left(A_{22}\right), \ldots, f\left(A_{r r}\right)\right)
$$

Also, $M$ is invertible if and only if each $A_{i i}$ is invertible, and, in such a case, $M^{-1}$ is a block diagonal matrix, and

$$
M^{-1}=\operatorname{diag}\left(A_{11}^{-1}, A_{22}^{-1}, \ldots, A_{r r}^{-1}\right)
$$

Analogously, a square block matrix is called a block upper triangular matrix if the blocks below the diagonal are zero matrices and a block lower triangular matrix if the blocks above the diagonal are zero matrices.

EXAMPLE 2.17 Determine which of the following square block matrices are upper diagonal, lower diagonal, or diagonal:

$$
A=\left[\begin{array}{cc:c}
1 & 2 & 0 \\
3 & 4 & 5 \\
\hdashline 0 & 0 & 6
\end{array}\right], \quad B=\left[\begin{array}{c:cc:c}
1 & 0 & 0 & 0 \\
\hdashline 2 & 3 & 4 & 0 \\
5 & 0 & 6 & 0 \\
\hdashline 0 & 7 & 8 & 9
\end{array}\right], \quad C=\left[\begin{array}{c:cc}
1 & 0 & 0 \\
\hdashline 0 & 2 & 3 \\
0 & 4 & 5
\end{array}\right], \quad D=\left[\begin{array}{cc:c}
1 & 2 & 0 \\
3 & 4 & 5 \\
\hdashline 0 & 6 & 7
\end{array}\right]
$$

(a) $A$ is upper triangular because the block below the diagonal is a zero block.

(b) $B$ is lower triangular because all blocks above the diagonal are zero blocks.

(c) $C$ is diagonal because the blocks above and below the diagonal are zero blocks.

(d) $D$ is neither upper triangular nor lower triangular. Also, no other partitioning of $D$ will make it into either a block upper triangular matrix or a block lower triangular matrix.

\section*{SOLVED PROBLEMS}
\section*{Matrix Addition and Scalar Multiplication}
2.1 Given $A=\left[\begin{array}{rrr}1 & -2 & 3 \\ 4 & 5 & -6\end{array}\right]$ and $B=\left[\begin{array}{rrr}3 & 0 & 2 \\ -7 & 1 & 8\end{array}\right]$, find:

(a) $A+B$, (b) $2 A-3 B$.

(a) Add the corresponding elements:

$$
A+B=\left[\begin{array}{rrr}
1+3 & -2+0 & 3+2 \\
4-7 & 5+1 & -6+8
\end{array}\right]=\left[\begin{array}{rrr}
4 & -2 & 5 \\
-3 & 6 & 2
\end{array}\right]
$$

(b) First perform the scalar multiplication and then a matrix addition:

$$
2 A-3 B=\left[\begin{array}{rrr}
2 & -4 & 6 \\
8 & 10 & -12
\end{array}\right]+\left[\begin{array}{rrr}
-9 & 0 & -6 \\
21 & -3 & -24
\end{array}\right]=\left[\begin{array}{rrr}
-7 & -4 & 0 \\
29 & 7 & -36
\end{array}\right]
$$

(Note that we multiply $B$ by -3 and then add, rather than multiplying $B$ by 3 and subtracting. This usually prevents errors.)

2.2. Find $x, y, z, t$ where $3\left[\begin{array}{cc}x & y \\ z & t\end{array}\right]=\left[\begin{array}{rr}x & 6 \\ -1 & 2 t\end{array}\right]+\left[\begin{array}{cc}4 & x+y \\ z+t & 3\end{array}\right]$.

Write each side as a single equation:

$$
\left[\begin{array}{cc}
3 x & 3 y \\
3 z & 3 t
\end{array}\right]=\left[\begin{array}{cc}
x+4 & x+y+6 \\
z+t-1 & 2 t+3
\end{array}\right]
$$

Set corresponding entries equal to each other to obtain the following system of four equations:

$$
\begin{array}{cccc} 
& 3 x=x+4, \quad 3 y=x+y+6, & 3 z=z+t-1, & 3 t=2 t+3 \\
\text { or } \quad 2 x=4, \quad 2 y=6+x, \quad 2 z=t-1, & t=3
\end{array}
$$

The solution is $x=2, y=4, z=1, t=3$.

2.3. Prove Theorem 2.1 (i) and (v): (i) $(A+B)+C=A+(B+C)$, (v) $k(A+B)=k A+k B$.

Suppose $A=\left[a_{i j}\right], B=\left[b_{i j}\right], C=\left[c_{i j}\right]$. The proof reduces to showing that corresponding $i j$-entries in each side of each matrix equation are equal. [We prove only (i) and (v), because the other parts of Theorem 2.1 are proved similarly.]\\
(i) The $i j$-entry of $A+B$ is $a_{i j}+b_{i j}$; hence, the $i j$-entry of $(A+B)+C$ is $\left(a_{i j}+b_{i j}\right)+c_{i j}$. On the other hand, the $i j$-entry of $B+C$ is $b_{i j}+c_{i j}$; hence, the $i j$-entry of $A+(B+C)$ is $a_{i j}+\left(b_{i j}+c_{i j}\right)$. However, for scalars in $K$,

$$
\left(a_{i j}+b_{i j}\right)+c_{i j}=a_{i j}+\left(b_{i j}+c_{i j}\right)
$$

Thus, $(A+B)+C$ and $A+(B+C)$ have identical $i j$-entries. Therefore, $(A+B)+C=A+(B+C)$.

(v) The $i j$-entry of $A+B$ is $a_{i j}+b_{i j}$; hence, $k\left(a_{i j}+b_{i j}\right)$ is the $i j$-entry of $k(A+B)$. On the other hand, the $i j$ entries of $k A$ and $k B$ are $k a_{i j}$ and $k b_{i j}$, respectively. Thus, $k a_{i j}+k b_{i j}$ is the $i j$-entry of $k A+k B$. However, for scalars in $K$,

$$
k\left(a_{i j}+b_{i j}\right)=k a_{i j}+k b_{i j}
$$

Thus, $k(A+B)$ and $k A+k B$ have identical $i j$-entries. Therefore, $k(A+B)=k A+k B$.

\section*{Matrix Multiplication}
2.4. Calculate: (a) $[8,-4,5]\left[\begin{array}{r}3 \\ 2 \\ -1\end{array}\right], \quad$ (b) $[6,-1,7,5]\left[\begin{array}{r}4 \\ -9 \\ -3 \\ 2\end{array}\right], \quad$ (c) $[3,8,-2,4]\left[\begin{array}{r}5 \\ -1 \\ 6\end{array}\right]$

(a) Multiply the corresponding entries and add:

$$
[8,-4,5]\left[\begin{array}{r}
3 \\
2 \\
-1
\end{array}\right]=8(3)+(-4)(2)+5(-1)=24-8-5=11
$$

(b) Multiply the corresponding entries and add:

$$
[6,-1,7,5]\left[\begin{array}{r}
4 \\
-9 \\
-3 \\
2
\end{array}\right]=24+9-21+10=22
$$

(c) The product is not defined when the row matrix and the column matrix have different numbers of elements.

2.5. Let $(r \times s)$ denote an $r \times s$ matrix. Find the sizes of those matrix products that are defined:\\
(a) $(2 \times 3)(3 \times 4)$,\\
(c) $(1 \times 2)(3 \times 1)$,\\
(e) $(4 \times 4)(3 \times 3)$\\
(b) $(4 \times 1)(1 \times 2)$,\\
(d) $(5 \times 2)(2 \times 3)$,\\
(f) $(2 \times 2)(2 \times 4)$

In each case, the product is defined if the inner numbers are equal, and then the product will have the size of the outer numbers in the given order.\\
(a) $2 \times 4$,\\
(c) not defined,\\
(e) not defined\\
(b) $4 \times 2$,\\
(d) $5 \times 3$,\\
(f) $2 \times 4$

2.6. Let $A=\left[\begin{array}{rr}1 & 3 \\ 2 & -1\end{array}\right]$ and $B=\left[\begin{array}{rrr}2 & 0 & -4 \\ 3 & -2 & 6\end{array}\right]$. Find: (a) $A B$, (b) $B A$.

(a) Because $A$ is a $2 \times 2$ matrix and $B$ a $2 \times 3$ matrix, the product $A B$ is defined and is a $2 \times 3$ matrix. To obtain the entries in the first row of $A B$, multiply the first row $[1,3]$ of $A$ by the columns $\left[\begin{array}{l}2 \\ 3\end{array}\right],\left[\begin{array}{r}0 \\ -2\end{array}\right],\left[\begin{array}{r}-4 \\ 6\end{array}\right]$ of $B$, respectively, as follows:

$$
A B=\left[\begin{array}{rr}
1 & 3 \\
2 & -1
\end{array}\right]\left[\begin{array}{rrr}
2 & 0 & -4 \\
3 & -2 & 6
\end{array}\right]=\left[\begin{array}{lll}
2+9 & 0-6 & -4+18
\end{array}\right]=\left[\begin{array}{lll}
11 & -6 & 14
\end{array}\right]
$$

To obtain the entries in the second row of $A B$, multiply the second row $[2,-1]$ of $A$ by the columns of $B$ :

$$
A B=\left[\begin{array}{rr}
1 & 3 \\
2 & -1
\end{array}\right]\left[\begin{array}{rrr}
2 & 0 & -4 \\
3 & -2 & 6
\end{array}\right]=\left[\begin{array}{ccc}
11 & -6 & 14 \\
4-3 & 0+2 & -8-6
\end{array}\right]
$$

Thus,

$$
A B=\left[\begin{array}{rrr}
11 & -6 & 14 \\
1 & 2 & -14
\end{array}\right]
$$

(b) The size of $B$ is $2 \times 3$ and that of $A$ is $2 \times 2$. The inner numbers 3 and 2 are not equal; hence, the product $B A$ is not defined.

2.7. Find $A B$, where $A=\left[\begin{array}{rrr}2 & 3 & -1 \\ 4 & -2 & 5\end{array}\right]$ and $B=\left[\begin{array}{rrrr}2 & -1 & 0 & 6 \\ 1 & 3 & -5 & 1 \\ 4 & 1 & -2 & 2\end{array}\right]$.

Because $A$ is a $2 \times 3$ matrix and $B$ a $3 \times 4$ matrix, the product $A B$ is defined and is a $2 \times 4$ matrix. Multiply the rows of $A$ by the columns of $B$ to obtain

$$
A B=\left[\begin{array}{cccc}
4+3-4 & -2+9-1 & 0-15+2 & 12+3-2 \\
8-2+20 & -4-6+5 & 0+10-10 & 24-2+10
\end{array}\right]=\left[\begin{array}{rrrr}
3 & 6 & -13 & 13 \\
26 & -5 & 0 & 32
\end{array}\right]
$$

2.8. Find: (a) $\left[\begin{array}{rr}1 & 6 \\ -3 & 5\end{array}\right]\left[\begin{array}{r}2 \\ -7\end{array}\right], \quad$ (b) $\left[\begin{array}{r}2 \\ -7\end{array}\right]\left[\begin{array}{rr}1 & 6 \\ -3 & 5\end{array}\right], \quad$ (c) $[2,-7]\left[\begin{array}{rr}1 & 6 \\ -3 & 5\end{array}\right]$.

(a) The first factor is $2 \times 2$ and the second is $2 \times 1$, so the product is defined as a $2 \times 1$ matrix:

$$
\left[\begin{array}{rr}
1 & 6 \\
-3 & 5
\end{array}\right]\left[\begin{array}{r}
2 \\
-7
\end{array}\right]=\left[\begin{array}{c}
2-42 \\
-6-35
\end{array}\right]=\left[\begin{array}{l}
-40 \\
-41
\end{array}\right]
$$

(b) The product is not defined, because the first factor is $2 \times 1$ and the second factor is $2 \times 2$.

(c) The first factor is $1 \times 2$ and the second factor is $2 \times 2$, so the product is defined as a $1 \times 2$ (row) matrix:

$$
[2,-7]\left[\begin{array}{rr}
1 & 6 \\
-3 & 5
\end{array}\right]=[2+21,12-35]=[23,-23]
$$

2.9. Clearly, $0 A=0$ and $A 0=0$, where the 0 's are zero matrices (with possibly different sizes). Find matrices $A$ and $B$ with no zero entries such that $A B=0$.

$$
\text { Let } A=\left[\begin{array}{ll}
1 & 2 \\
2 & 4
\end{array}\right] \text { and } B=\left[\begin{array}{rr}
6 & 2 \\
-3 & -1
\end{array}\right] \text {. Then } A B=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right] \text {. }
$$

2.10. Prove Theorem 2.2(i): $(A B) C=A(B C)$.

Let $A=\left[a_{i j}\right], \quad B=\left[b_{j k}\right], \quad C=\left[c_{k l}\right], \quad$ and let $A B=S=\left[s_{i k}\right], \quad B C=T=\left[t_{j}\right]$. Then

$$
s_{i k}=\sum_{j=1}^{m} a_{i j} b_{j k} \quad \text { and } \quad t_{j l}=\sum_{k=1}^{n} b_{j k} c_{k l}
$$

Multiplying $S=A B$ by $C$, the $i l$-entry of $(A B) C$ is

$$
s_{i 1} c_{1 l}+s_{i 2} c_{2 l}+\cdots+s_{i n} c_{n l}=\sum_{k=1}^{n} s_{i k} c_{k l}=\sum_{k=1}^{n} \sum_{j=1}^{m}\left(a_{i j} b_{j k}\right) c_{k l}
$$

On the other hand, multiplying $A$ by $T=B C$, the $i l$-entry of $A(B C)$ is

$$
a_{i 1} t_{1 l}+a_{i 2} t_{2 l}+\cdots+a_{i n} t_{n l}=\sum_{j=1}^{m} a_{i j} t_{j l}=\sum_{j=1}^{m} \sum_{k=1}^{n} a_{i j}\left(b_{j k} c_{k l}\right)
$$

The above sums are equal; that is, corresponding elements in $(A B) C$ and $A(B C)$ are equal. Thus, $(A B) C=A(B C)$.

2.11. Prove Theorem 2.2(ii): $A(B+C)=A B+A C$.

Let $A=\left[a_{i j}\right], B=\left[b_{j k}\right], C=\left[c_{j k}\right]$, and let $D=B+C=\left[d_{j k}\right], E=A B=\left[e_{i k}\right], F=A C=\left[f_{i k}\right]$. Then

$$
d_{j k}=b_{j k}+c_{j k}, \quad e_{i k}=\sum_{j=1}^{m} a_{i j} b_{j k}, \quad f_{i k}=\sum_{j=1}^{m} a_{i j} c_{j k}
$$

Thus, the $i k$-entry of the matrix $A B+A C$ is

$$
e_{i k}+f_{i k}=\sum_{j=1}^{m} a_{i j} b_{j k}+\sum_{j=1}^{m} a_{i j} c_{j k}=\sum_{j=1}^{m} a_{i j}\left(b_{j k}+c_{j k}\right)
$$

On the other hand, the $i k$-entry of the matrix $A D=A(B+C)$ is

$$
a_{i 1} d_{1 k}+a_{i 2} d_{2 k}+\cdots+a_{i m} d_{m k}=\sum_{j=1}^{m} a_{i j} d_{j k}=\sum_{j=1}^{m} a_{i j}\left(b_{j k}+c_{j k}\right)
$$

Thus, $A(B+C)=A B+A C$, because the corresponding elements are equal.

\section*{Transpose}
2.12. Find the transpose of each matrix:\\
$A=\left[\begin{array}{rrr}1 & -2 & 3 \\ 7 & 8 & -9\end{array}\right]$\\
$B=\left[\begin{array}{lll}1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 5 & 6\end{array}\right]$,\\
$C=[1,-3,5,-7]$,\\
$D=\left[\begin{array}{r}2 \\ -4 \\ 6\end{array}\right]$

Rewrite the rows of each matrix as columns to obtain the transpose of the matrix:

$$
A^{T}=\left[\begin{array}{rr}
1 & 7 \\
-2 & 8 \\
3 & -9
\end{array}\right], \quad B^{T}=\left[\begin{array}{lll}
1 & 2 & 3 \\
2 & 4 & 5 \\
3 & 5 & 6
\end{array}\right], \quad C^{T}=\left[\begin{array}{r}
1 \\
-3 \\
5 \\
-7
\end{array}\right], \quad D^{T}=[2,-4,6]
$$

(Note that $B^{T}=B$; such a matrix is said to be symmetric. Note also that the transpose of the row vector $C$ is a column vector, and the transpose of the column vector $D$ is a row vector.)

2.13. Prove Theorem 2.3(iv): $(A B)^{T}=B^{T} A^{T}$.

Let $A=\left[a_{i k}\right]$ and $B=\left[b_{k j}\right]$. Then the $i j$-entry of $A B$ is

$$
a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i m} b_{m j}
$$

This is the $j i$-entry (reverse order) of $(A B)^{T}$. Now column $j$ of $B$ becomes row $j$ of $B^{T}$, and row $i$ of $A$ becomes column $i$ of $A^{T}$. Thus, the $i j$-entry of $B^{T} A^{T}$ is

$$
\left[b_{1 j}, b_{2 j}, \ldots, b_{m j}\right]\left[a_{i 1}, a_{i 2}, \ldots, a_{i m}\right]^{T}=b_{1 j} a_{i 1}+b_{2 j} a_{i 2}+\cdots+b_{m j} a_{i m}
$$

Thus, $(A B)^{T}=B^{T} A^{T}$ on because the corresponding entries are equal.

\section*{Square Matrices}
2.14. Find the diagonal and trace of each matrix:\\
(a) $A=\left[\begin{array}{rrr}1 & 3 & 6 \\ 2 & -5 & 8 \\ 4 & -2 & 9\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}2 & 4 & 8 \\ 3 & -7 & 9 \\ -5 & 0 & 2\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rrr}1 & 2 & -3 \\ 4 & -5 & 6\end{array}\right]$.

(a) The diagonal of $A$ consists of the elements from the upper left corner of $A$ to the lower right corner of $A$ or, in other words, the elements $a_{11}, a_{22}, a_{33}$. Thus, the diagonal of $A$ consists of the numbers $1,-5$, and 9 . The trace of $A$ is the sum of the diagonal elements. Thus,

$$
\operatorname{tr}(A)=1-5+9=5
$$

(b) The diagonal of $B$ consists of the numbers $2,-7$, and 2 . Hence,

$$
\operatorname{tr}(B)=2-7+2=-3
$$

(c) The diagonal and trace are only defined for square matrices.

2.15. Let $A=\left[\begin{array}{rr}1 & 2 \\ 4 & -3\end{array}\right]$, and let $f(x)=2 x^{3}-4 x+5$ and $g(x)=x^{2}+2 x+11$. Find\\
(a) $A^{2}$,\\
(b) $A^{3}$,\\
(c) $f(A)$,\\
(d) $g(A)$.

(a) $A^{2}=A A=\left[\begin{array}{rr}1 & 2 \\ 4 & -3\end{array}\right]\left[\begin{array}{rr}1 & 2 \\ 4 & -3\end{array}\right]=\left[\begin{array}{ll}1+8 & 2-6 \\ 4-12 & 8+9\end{array}\right]=\left[\begin{array}{rr}9 & -4 \\ -8 & 17\end{array}\right]$

(b) $A^{3}=A A^{2}=\left[\begin{array}{rr}1 & 2 \\ 4 & -3\end{array}\right]\left[\begin{array}{rr}9 & -4 \\ -8 & 17\end{array}\right]=\left[\begin{array}{rr}9-16 & -4+34 \\ 36+24 & -16-51\end{array}\right]=\left[\begin{array}{rr}-7 & 30 \\ 60 & -67\end{array}\right]$

(c) First substitute $A$ for $x$ and $5 I$ for the constant in $f(x)$, obtaining

$$
f(A)=2 A^{3}-4 A+5 I=2\left[\begin{array}{rr}
-7 & 30 \\
60 & -67
\end{array}\right]-4\left[\begin{array}{rr}
1 & 2 \\
4 & -3
\end{array}\right]+5\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]
$$

Now perform the scalar multiplication and then the matrix addition:

$$
f(A)=\left[\begin{array}{rr}
-14 & 60 \\
120 & -134
\end{array}\right]+\left[\begin{array}{rr}
-4 & -8 \\
-16 & 12
\end{array}\right]+\left[\begin{array}{ll}
5 & 0 \\
0 & 5
\end{array}\right]=\left[\begin{array}{rr}
-13 & 52 \\
104 & -117
\end{array}\right]
$$

(d) Substitute $A$ for $x$ and 11I for the constant in $g(x)$, and then calculate as follows:

$$
\begin{aligned}
g(A) & =A^{2}+2 A-11 I=\left[\begin{array}{rr}
9 & -4 \\
-8 & 17
\end{array}\right]+2\left[\begin{array}{rr}
1 & 2 \\
4 & -3
\end{array}\right]-11\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right] \\
& =\left[\begin{array}{rr}
9 & -4 \\
-8 & 17
\end{array}\right]+\left[\begin{array}{rr}
2 & 4 \\
8 & -6
\end{array}\right]+\left[\begin{array}{rr}
-11 & 0 \\
0 & -11
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right]
\end{aligned}
$$

Because $g(A)$ is the zero matrix, $A$ is a root of the polynomial $g(x)$.

2.16. Let $A=\left[\begin{array}{rr}1 & 3 \\ 4 & -3\end{array}\right]$. (a) Find a nonzero column vector $u=\left[\begin{array}{l}x \\ y\end{array}\right]$ such that $A u=3 u$.

(b) Describe all such vectors.

(a) First set up the matrix equation $A u=3 u$, and then write each side as a single matrix (column vector) as follows:

$$
\left[\begin{array}{rr}
1 & 3 \\
4 & -3
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=3\left[\begin{array}{l}
x \\
y
\end{array}\right], \quad \text { and then } \quad\left[\begin{array}{c}
x+3 y \\
4 x-3 y
\end{array}\right]=\left[\begin{array}{l}
3 x \\
3 y
\end{array}\right]
$$

Set the corresponding elements equal to each other to obtain a system of equations:

$$
\begin{aligned}
x+3 y & =3 x \\
4 x-3 y & =3 y
\end{aligned} \quad \text { or } \quad \begin{aligned}
2 x-3 y & =0 \\
4 x-6 y & =0
\end{aligned} \quad \text { or } \quad 2 x-3 y=0
$$

The system reduces to one nondegenerate linear equation in two unknowns, and so has an infinite number of solutions. To obtain a nonzero solution, let, say, $y=2$; then $x=3$. Thus, $u=(3,2)^{T}$ is a desired nonzero vector.

(b) To find the general solution, set $y=a$, where $a$ is a parameter. Substitute $y=a$ into $2 x-3 y=0$ to obtain $x=\frac{3}{2} a$. Thus, $u=\left(\frac{3}{2} a, a\right)^{T}$ represents all such solutions.

\section*{Invertible Matrices, Inverses}
2.17. Show that $A=\left[\begin{array}{rrr}1 & 0 & 2 \\ 2 & -1 & 3 \\ 4 & 1 & 8\end{array}\right]$ and $B=\left[\begin{array}{rrr}-11 & 2 & 2 \\ -4 & 0 & 1 \\ 6 & -1 & -1\end{array}\right]$ are inverses.

Compute the product $A B$, obtaining

$$
A B=\left[\begin{array}{lll}
-11+0+12 & 2+0-2 & 2+0-2 \\
-22+4+18 & 4+0-3 & 4-1-3 \\
-44-4+48 & 8+0-8 & 8+1-8
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]=I
$$

Because $A B=I$, we can conclude (Theorem 3.16) that $B A=I$. Accordingly, $A$ and $B$ are inverses.

2.18. Find the inverse, if possible, of each matrix:\\
(a) $A=\left[\begin{array}{ll}5 & 3 \\ 4 & 2\end{array}\right]$\\
(b) $B=\left[\begin{array}{rr}2 & -3 \\ 1 & 3\end{array}\right]$,\\
(c) $\left[\begin{array}{rr}-2 & 6 \\ 3 & -9\end{array}\right]$.

Use the formula for the inverse of a $2 \times 2$ matrix appearing in Section 2.9.

(a) First find $|A|=5(2)-3(4)=10-12=-2$. Next interchange the diagonal elements, take the negatives of the nondiagonal elements, and multiply by $1 /|A|$ :

$$
A^{-1}=-\frac{1}{2}\left[\begin{array}{rr}
2 & -3 \\
-4 & 5
\end{array}\right]=\left[\begin{array}{rr}
-1 & \frac{3}{2} \\
2 & -\frac{5}{2}
\end{array}\right]
$$

(b) First find $|B|=2(3)-(-3)(1)=6+3=9$. Next interchange the diagonal elements, take the negatives of the nondiagonal elements, and multiply by $1 /|B|$ :

$$
B^{-1}=\frac{1}{9}\left[\begin{array}{rr}
3 & 3 \\
-1 & 2
\end{array}\right]=\left[\begin{array}{rr}
\frac{1}{3} & \frac{1}{3} \\
-\frac{1}{9} & \frac{2}{9}
\end{array}\right]
$$

(c) First find $|C|=-2(-9)-6(3)=18-18=0$. Because $|C|=0, C$ has no inverse.

2.19. Let $A=\left[\begin{array}{lll}1 & 1 & 1 \\ 0 & 1 & 2 \\ 1 & 2 & 4\end{array}\right]$. Find $A^{-1}=\left[\begin{array}{lll}x_{1} & x_{2} & x_{3} \\ y_{1} & y_{2} & y_{3} \\ z_{1} & z_{2} & z_{3}\end{array}\right]$.

Multiplying $A$ by $A^{-1}$ and setting the nine entries equal to the nine entries of the identity matrix $I$ yields the following three systems of three equations in three of the unknowns:

$$
\begin{aligned}
& x_{1}+y_{1}+z_{1}=1 \quad x_{2}+y_{2}+z_{2}=0 \quad x_{3}+y_{3}+z_{3}=0 \\
& y_{1}+2 z_{1}=0 \quad y_{2}+2 z_{2}=1 \quad y_{3}+2 z_{3}=0 \\
& x_{1}+2 y_{1}+4 z_{1}=0 \quad x_{2}+2 y_{2}+4 z_{2}=0 \quad x_{3}+2 y_{3}+4 z_{3}=1
\end{aligned}
$$

[Note that $A$ is the coefficient matrix for all three systems.]

Solving the three systems for the nine unknowns yields

$$
x_{1}=0, \quad y_{1}=2, \quad z_{1}=-1 ; \quad x_{2}=-2, \quad y_{2}=3, \quad z_{2}=-1 ; \quad x_{3}=1, \quad y_{3}=-2, \quad z_{3}=1
$$

Thus,

$$
A^{-1}=\left[\begin{array}{rrr}
0 & -2 & 1 \\
2 & 3 & -2 \\
-1 & -1 & 1
\end{array}\right]
$$

(Remark: Chapter 3 gives an efficient way to solve the three systems.)

2.20. Let $A$ and $B$ be invertible matrices (with the same size). Show that $A B$ is also invertible and $(A B)^{-1}=B^{-1} A^{-1}$. [Thus, by induction, $\left(A_{1} A_{2} \ldots A_{m}\right)^{-1}=A_{m}^{-1} \ldots A_{2}^{-1} A_{1}^{-1}$.]

Using the associativity of matrix multiplication, we get

$$
\begin{aligned}
& (A B)\left(B^{-1} A^{-1}\right)=A\left(B B^{-1}\right) A^{-1}=A I A^{-1}=A A^{-1}=I \\
& \left(B^{-1} A^{-1}\right)(A B)=B^{-1}\left(A^{-1} A\right) B=A^{-1} I B=B^{-1} B=I
\end{aligned}
$$

Thus, $(A B)^{-1}=B^{-1} A^{-1}$.

\section*{Diagonal and Triangular Matrices}
2.21. Write out the diagonal matrices $A=\operatorname{diag}(4,-3,7), B=\operatorname{diag}(2,-6), C=\operatorname{diag}(3,-8,0,5)$.

Put the given scalars on the diagonal and 0 's elsewhere:

$$
\mathrm{A}=\left[\begin{array}{rrr}
4 & 0 & 0 \\
0 & -3 & 0 \\
0 & 0 & 7
\end{array}\right], \quad \mathrm{B}=\left[\begin{array}{rr}
2 & 0 \\
0 & -6
\end{array}\right], \quad \mathrm{C}=\left[\begin{array}{llll}
3 & & & \\
& -8 & & \\
& & 0 & \\
& & & 5
\end{array}\right]
$$

2.22. Let $A=\operatorname{diag}(2,3,5)$ and $B=\operatorname{diag}(7,0,-4)$. Find\\
(a) $A B, A^{2}, B^{2}$;\\
(b) $f(A)$, where $f(x)=x^{2}+3 x-2$;\\
(c) $A^{-1}$ and $B^{-1}$.

(a) The product matrix $A B$ is a diagonal matrix obtained by multiplying corresponding diagonal entries; hence,

$$
A B=\operatorname{diag}(2(7), 3(0), 5(-4))=\operatorname{diag}(14,0,-20)
$$

Thus, the squares $A^{2}$ and $B^{2}$ are obtained by squaring each diagonal entry; hence,

$$
A^{2}=\operatorname{diag}\left(2^{2}, 3^{2}, 5^{2}\right)=\operatorname{diag}(4,9,25) \quad \text { and } \quad B^{2}=\operatorname{diag}(49,0,16)
$$

(b) $f(A)$ is a diagonal matrix obtained by evaluating $f(x)$ at each diagonal entry. We have

$$
f(2)=4+6-2=8, \quad f(3)=9+9-2=16, \quad f(5)=25+15-2=38
$$

Thus, $f(A)=\operatorname{diag}(8,16,38)$.

(c) The inverse of a diagonal matrix is a diagonal matrix obtained by taking the inverse (reciprocal) of each diagonal entry. Thus, $A^{-1}=\operatorname{diag}\left(\frac{1}{2}, \frac{1}{3}, \frac{1}{5}\right)$, but $B$ has no inverse because there is a 0 on the diagonal.

2.23. Find a $2 \times 2$ matrix $A$ such that $A^{2}$ is diagonal but not $A$.

Let $A=\left[\begin{array}{rr}1 & 2 \\ 3 & -1\end{array}\right]$. Then $A^{2}=\left[\begin{array}{ll}7 & 0 \\ 0 & 7\end{array}\right]$, which is diagonal.

2.24. Find an upper triangular matrix $A$ such that $A^{3}=\left[\begin{array}{rr}8 & -57 \\ 0 & 27\end{array}\right]$.

Set $A=\left[\begin{array}{ll}x & y \\ 0 & z\end{array}\right]$. Then $x^{3}=8$, so $x=2$; and $z^{3}=27$, so $z=3$. Next calculate $A^{3}$ using $x=2$ and $y=3$ :

$$
A^{2}=\left[\begin{array}{ll}
2 & y \\
0 & 3
\end{array}\right]\left[\begin{array}{ll}
2 & y \\
0 & 3
\end{array}\right]=\left[\begin{array}{rr}
4 & 5 y \\
0 & 9
\end{array}\right] \quad \text { and } \quad A^{3}=\left[\begin{array}{ll}
2 & y \\
0 & 3
\end{array}\right]\left[\begin{array}{cc}
4 & 5 y \\
0 & 9
\end{array}\right]=\left[\begin{array}{rr}
8 & 19 y \\
0 & 27
\end{array}\right]
$$

Thus, $19 y=-57$, or $y=-3$. Accordingly, $A=\left[\begin{array}{rr}2 & -3 \\ 0 & 3\end{array}\right]$.

2.25. Let $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ be upper triangular matrices. Prove that $A B$ is upper triangular with diagonal $a_{11} b_{11}, a_{22} b_{22}, \ldots, a_{n n} b_{n n}$.

Let $A B=\left[c_{i j}\right]$. Then $c_{i j}=\sum_{k=1}^{n} a_{i k} b_{k j}$ and $c_{i i}=\sum_{k=1}^{n} a_{i k} b_{k i}$. Suppose $i>j$. Then, for any $k$, either $i>k$ or $k>j$, so that either $a_{i k}=0$ or $b_{k j}=0$. Thus, $c_{i j}=0$, and $A B$ is upper triangular. Suppose $i=j$. Then, for $k<i$, we have $a_{i k}=0$; and, for $k>i$, we have $b_{k i}=0$. Hence, $c_{i i}=a_{i i} b_{i i}$, as claimed. [This proves one part of Theorem 2.5(i); the statements for $A+B$ and $k A$ are left as exercises.]

\section*{Special Real Matrices: Symmetric and Orthogonal}
2.26. Determine whether or not each of the following matrices is symmetric-that is, $A^{T}=A$-or skew-symmetric-that is, $A^{T}=-A$ :\\
(a) $A=\left[\begin{array}{rrr}5 & -7 & 1 \\ -7 & 8 & 2 \\ 1 & 2 & -4\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}0 & 4 & -3 \\ -4 & 0 & 5 \\ 3 & -5 & 0\end{array}\right]$,\\
(c) $C=\left[\begin{array}{lll}0 & 0 & 0 \\ 0 & 0 & 0\end{array}\right]$

(a) By inspection, the symmetric elements (mirror images in the diagonal) are -7 and $-7,1$ and 1,2 and 2 . Thus, $A$ is symmetric, because symmetric elements are equal.

(b) By inspection, the diagonal elements are all 0 , and the symmetric elements, 4 and $-4,-3$ and 3 , and 5 and -5 , are negatives of each other. Hence, $B$ is skew-symmetric.

(c) Because $C$ is not square, $C$ is neither symmetric nor skew-symmetric.

2.27. Suppose $B=\left[\begin{array}{cc}4 & x+2 \\ 2 x-3 & x+1\end{array}\right]$ is symmetric. Find $x$ and $B$.

Set the symmetric elements $x+2$ and $2 x-3$ equal to each other, obtaining $2 x-3=x+2$ or $x=5$. Hence, $B=\left[\begin{array}{ll}4 & 7 \\ 7 & 6\end{array}\right]$.

2.28. Let $A$ be an arbitrary $2 \times 2$ (real) orthogonal matrix.

(a) Prove: If $(a, b)$ is the first row of $A$, then $a^{2}+b^{2}=1$ and

$$
A=\left[\begin{array}{rr}
a & b \\
-b & a
\end{array}\right] \quad \text { or } \quad A=\left[\begin{array}{rr}
a & b \\
b & -a
\end{array}\right]
$$

(b) Prove Theorem 2.7: For some real number $\theta$,

$$
A=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right] \quad \text { or } \quad A=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{array}\right]
$$

(a) Suppose $(x, y)$ is the second row of $A$. Because the rows of $A$ form an orthonormal set, we get

$$
a^{2}+b^{2}=1, \quad x^{2}+y^{2}=1, \quad a x+b y=0
$$

Similarly, the columns form an orthogonal set, so

$$
a^{2}+x^{2}=1, \quad b^{2}+y^{2}=1, \quad a b+x y=0
$$

Therefore, $x^{2}=1-a^{2}=b^{2}$, whence $x= \pm b$.

Case (i): $x=b$. Then $b(a+y)=0$, so $y=-a$.

Case (ii): $x=-b$. Then $b(y-a)=0$, so $y=a$.

This means, as claimed,

$$
A=\left[\begin{array}{rr}
a & b \\
-b & a
\end{array}\right] \quad \text { or } \quad A=\left[\begin{array}{rr}
a & b \\
b & -a
\end{array}\right]
$$

(b) Because $a^{2}+b^{2}=1$, we have $-1 \leq a \leq 1$. Let $a=\cos \theta$. Then $b^{2}=1-\cos ^{2} \theta$, so $b=\sin \theta$. This proves the theorem.

2.29. Find a $2 \times 2$ orthogonal matrix $A$ whose first row is a (positive) multiple of $(3,4)$.

Normalize $(3,4)$ to get $\left(\frac{3}{5}, \frac{4}{5}\right)$. Then, by Problem 2.28 ,

$$
A=\left[\begin{array}{rr}
\frac{3}{5} & \frac{4}{5} \\
-\frac{4}{5} & \frac{3}{5}
\end{array}\right] \quad \text { or } \quad A=\left[\begin{array}{rr}
\frac{3}{5} & \frac{4}{5} \\
\frac{4}{5} & -\frac{3}{5}
\end{array}\right]
$$

2.30. Find a $3 \times 3$ orthogonal matrix $P$ whose first two rows are multiples of $u_{1}=(1,1,1)$ and $u_{2}=(0,-1,1)$, respectively. (Note that, as required, $u_{1}$ and $u_{2}$ are orthogonal.)

First find a nonzero vector $u_{3}$ orthogonal to $u_{1}$ and $u_{2}$; say (cross product) $u_{3}=u_{1} \times u_{2}=(2,-1,-1)$. Let $A$ be the matrix whose rows are $u_{1}, u_{2}, u_{3}$; and let $P$ be the matrix obtained from $A$ by normalizing the rows of $A$. Thus,

$$
A=\left[\begin{array}{rrr}
1 & 1 & 1 \\
0 & -1 & 1 \\
2 & -1 & -1
\end{array}\right] \quad \text { and } \quad P=\left[\begin{array}{ccc}
1 / \sqrt{3} & 1 / \sqrt{3} & 1 / \sqrt{3} \\
0 & -1 / \sqrt{2} & 1 / \sqrt{2} \\
2 / \sqrt{6} & -1 / \sqrt{6} & -1 / \sqrt{6}
\end{array}\right]
$$

\section*{Complex Matrices: Hermitian and Unitary Matrices}
2.31. Find $A^{H}$ where (a) $A=\left[\begin{array}{ll}3-5 i & 2+4 i \\ 6+7 i & 1+8 i\end{array}\right], \quad$ (b) $\quad A=\left[\begin{array}{cc}2-3 i & 5+8 i \\ -4 & 3-7 i \\ -6-i & 5 i\end{array}\right]$

Recall that $A^{H}=\bar{A}^{T}$, the conjugate tranpose of $A$. Thus,\\
(a) $A^{H}=\left[\begin{array}{ll}3+5 i & 6-7 i \\ 2-4 i & 1-8 i\end{array}\right]$,\\
(b) $A^{H}=\left[\begin{array}{ccc}2+3 i & -4 & -6+i \\ 5-8 i & 3+7 i & -5 i\end{array}\right]$

2.32. Show that $A=\left[\begin{array}{cc}\frac{1}{3}-\frac{2}{3} i & \frac{2}{3} i \\ -\frac{2}{3} i & -\frac{1}{3}-\frac{2}{3} i\end{array}\right]$ is unitary.

The rows of $A$ form an orthonormal set:

$$
\begin{gathered}
\left(\frac{1}{3}-\frac{2}{3} i, \frac{2}{3} i\right) \cdot\left(\frac{1}{3}-\frac{2}{3} i, \frac{2}{3} i\right)=\left(\frac{1}{9}+\frac{4}{9}\right)+\frac{4}{9}=1 \\
\left(\frac{1}{3}-\frac{2}{3} i, \frac{2}{3} i\right) \cdot\left(-\frac{2}{3} i,-\frac{1}{3}-\frac{2}{3} i\right)=\left(\frac{2}{9} i+\frac{4}{9}\right)+\left(-\frac{2}{9} i-\frac{4}{9}\right)=0 \\
\left(-\frac{2}{3} i,-\frac{1}{3}-\frac{2}{3} i\right) \cdot\left(-\frac{2}{3} i,-\frac{1}{3}-\frac{2}{3} i\right)=\frac{4}{9}+\left(\frac{1}{9}+\frac{4}{9}\right)=1
\end{gathered}
$$

Thus, $A$ is unitary.

2.33. Prove the complex analogue of Theorem 2.6: Let $A$ be a complex matrix. Then the following are equivalent: (i) $A$ is unitary. (ii) The rows of $A$ form an orthonormal set. (iii) The columns of $A$ form an orthonormal set.

(The proof is almost identical to the proof on page 37 for the case when $A$ is a $3 \times 3$ real matrix.)

First recall that the vectors $u_{1}, u_{2}, \ldots, u_{n}$ in $\mathbf{C}^{n}$ form an orthonormal set if they are unit vectors and are orthogonal to each other, where the dot product in $\mathbf{C}^{n}$ is defined by

$$
\left(a_{1}, a_{2}, \ldots, a_{n}\right) \cdot\left(b_{1}, b_{2}, \ldots, b_{n}\right)=a_{1} \bar{b}_{1}+a_{2} \bar{b}_{2}+\cdots+a_{n} \bar{b}_{n}
$$

Suppose $A$ is unitary, and $R_{1}, R_{2}, \ldots, R_{n}$ are its rows. Then $\bar{R}_{1}^{T}, \bar{R}_{2}^{T}, \ldots, \bar{R}_{n}^{T}$ are the columns of $A^{H}$. Let $A A^{H}=\left[c_{i j}\right]$. By matrix multiplication, $c_{i j}=R_{i} \bar{R}_{j}^{T}=R_{i} \cdot R_{j}$. Because $A$ is unitary, we have $A A^{H}=I$. Multiplying $A$ by $A^{H}$ and setting each entry $c_{i j}$ equal to the corresponding entry in $I$ yields the following $n^{2}$ equations:

$$
R_{1} \cdot R_{1}=1, \quad R_{2} \cdot R_{2}=1, \quad \ldots, \quad R_{n} \cdot R_{n}=1, \quad \text { and } \quad R_{i} \cdot R_{j}=0, \quad \text { for } i \neq j
$$

Thus, the rows of $A$ are unit vectors and are orthogonal to each other; hence, they form an orthonormal set of vectors. The condition $A^{T} A=I$ similarly shows that the columns of $A$ also form an orthonormal set of vectors. Furthermore, because each step is reversible, the converse is true. This proves the theorem.

\section*{Block Matrices}
2.34. Consider the following block matrices (which are partitions of the same matrix):\\
(a) $\left[\begin{array}{rr:rr:r}1 & -2 & 0 & 1 & 3 \\ 2 & -3 & 5 & 7 & -2 \\ 3 & 1 & 4 & 5 & 9\end{array}\right]$,\\
(b)\\
$\left[\begin{array}{ll:ll}1 & -2 & 0 & \frac{1}{7}--\frac{3}{2} \\ \hdashline 2 & 3 & \frac{5}{5}-\frac{7}{2}-\frac{2}{2} \\ \hdashline 3 & 1 & 4 & 5\end{array}\right]$

Find the size of each block matrix and also the size of each block.

(a) The block matrix has two rows of matrices and three columns of matrices; hence, its size is $2 \times 3$. The block sizes are $2 \times 2,2 \times 2$, and $2 \times 1$ for the first row; and $1 \times 2,1 \times 2$, and $1 \times 1$ for the second row.

(b) The size of the block matrix is $3 \times 2$; and the block sizes are $1 \times 3$ and $1 \times 2$ for each of the three rows.

2.35. Compute $A B$ using block multiplication, where

$$
A=\left[\begin{array}{cc:c}
1 & 2 & 1 \\
3 & 4 & 0 \\
\hdashline 0 & 0 & 2
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{ccc:c}
1 & 2 & 3 & 1 \\
4 & 5 & 6 & 1 \\
\hdashline 0 & 0 & 0 & 1
\end{array}\right]
$$

Here $A=\left[\begin{array}{cc}E & F \\ 0_{1 \times 2} & G\end{array}\right]$ and $B=\left[\begin{array}{cc}R & S \\ 0_{1 \times 3} & T\end{array}\right]$, where $E, F, G, R, S, T$ are the given blocks, and $0_{1 \times 2}$ and $0_{1 \times 3}$ are zero matrices of the indicated sites. Hence,

$$
A B=\left[\begin{array}{cc}
E R & E S+F T \\
0_{1 \times 3} & G T
\end{array}\right]=\left[\begin{array}{cc}
{\left[\begin{array}{rrr}
9 & 12 & 15 \\
19 & 26 & 33
\end{array}\right]} & {\left[\begin{array}{l}
3 \\
7
\end{array}\right]+\left[\begin{array}{l}
1 \\
0
\end{array}\right]}
\end{array}\right]=\left[\begin{array}{rrrr}
9 & 12 & 15 & 4 \\
19 & 26 & 33 & 7 \\
0 & 0 & 0 & 2
\end{array}\right]
$$

2.36. Let $M=\operatorname{diag}(A, B, C)$, where $A=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right], B=[5], C=\left[\begin{array}{ll}1 & 3 \\ 5 & 7\end{array}\right]$. Find $M^{2}$.

Because $M$ is block diagonal, square each block:

so

$$
A^{2}=\left[\begin{array}{rr}
7 & 10 \\
15 & 22
\end{array}\right], \quad B^{2}=[25], \quad C^{2}=\left[\begin{array}{ll}
16 & 24 \\
40 & 64
\end{array}\right]
$$

$$
M^{2}=\left[\begin{array}{rr:rrr}
7 & 10 & & & \\
15 & 22 & & & \\
\hdashline & & 25 & & \\
& & & 16 & 2 \\
& & & 40 & 64
\end{array}\right]
$$

\section*{Miscellaneous Problem}
2.37. Let $f(x)$ and $g(x)$ be polynomials and let $A$ be a square matrix. Prove

(a) $(f+g)(A)=f(A)+g(A)$,

(b) $(f \cdot g)(A)=f(A) g(A)$,

(c) $f(A) g(A)=g(A) f(A)$.

Suppose $f(x)=\sum_{i=1}^{r} a_{i} x^{i}$ and $g(x)=\sum_{j=1}^{s} b_{j} x^{j}$.

(a) We can assume $r=s=n$ by adding powers of $x$ with 0 as their coefficients. Then

Hence,

$$
\begin{gathered}
f(x)+g(x)=\sum_{i=1}^{n}\left(a_{i}+b_{i}\right) x^{i} \\
(f+g)(A)=\sum_{i=1}^{n}\left(a_{i}+b_{i}\right) A^{i}=\sum_{i=1}^{n} a_{i} A^{i}+\sum_{i=1}^{n} b_{i} A^{i}=f(A)+g(A)
\end{gathered}
$$

(b) We have $f(x) g(x)=\sum_{i, j} a_{i} b_{j} x^{i+j}$. Then

$$
f(A) g(A)=\left(\sum_{i} a_{i} A^{i}\right)\left(\sum_{j} b_{j} A^{j}\right)=\sum_{i, j} a_{i} b_{j} A^{i+j}=(f g)(A)
$$

(c) Using $f(x) g(x)=g(x) f(x)$, we have

$$
f(A) g(A)=(f g)(A)=(g f)(A)=g(A) f(A)
$$

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Algebra of Matrices}
Problems 2.38-2.41 refer to the following matrices:

$$
A=\left[\begin{array}{rr}
1 & 2 \\
3 & -4
\end{array}\right], \quad B=\left[\begin{array}{rr}
5 & 0 \\
-6 & 7
\end{array}\right], \quad C=\left[\begin{array}{rrr}
1 & -3 & 4 \\
2 & 6 & -5
\end{array}\right], \quad D=\left[\begin{array}{rrr}
3 & 7 & -1 \\
4 & -8 & 9
\end{array}\right]
$$

2.38. Find (a) $5 A-2 B$, (b) $2 A+3 B$, (c) $2 C-3 D$.

2.39. Find (a) $A B$ and $(A B) C$, (b) $B C$ and $A(B C)$. [Note that $(A B) C=A(B C)$.]

2.40. Find (a) $A^{2}$ and $A^{3}$, (b) $A D$ and $B D$, (c) $C D$.

2.41. Find (a) $A^{T}$, (b) $B^{T}$, (c) $(A B)^{T}$, (d) $A^{T} B^{T}$. [Note that $A^{T} B^{T} \neq(A B)^{T}$.]

Problems 2.42 and 2.43 refer to the following matrices:\\
$A=\left[\begin{array}{rrr}1 & -1 & 2 \\ 0 & 3 & 4\end{array}\right]$\\
$B=\left[\begin{array}{rrr}4 & 0 & -3 \\ -1 & -2 & 3\end{array}\right]$,\\
$C=\left[\begin{array}{rrrr}2 & -3 & 0 & 1 \\ 5 & -1 & -4 & 2 \\ -1 & 0 & 0 & 3\end{array}\right]$,\\
$D=\left[\begin{array}{r}2 \\ -1 \\ 3\end{array}\right]$

2.42. Find

(a) $3 A-4 B$,

(b) $A C$,

(c) $B C$,

(d) $A D$,

(e) $B D, \quad(f) C D$.

2.43. Find\\
(a) $A^{T}$,\\
(b) $A^{T} B$,\\
(c) $A^{T} C$.

2.44. Let $A=\left[\begin{array}{ll}1 & 2 \\ 3 & 6\end{array}\right]$. Find a $2 \times 3$ matrix $B$ with distinct nonzero entries such that $A B=0$.

2.45 Let $e_{1}=[1,0,0], e_{2}=[0,1,0], e_{3}=[0,0,1]$, and $A=\left[\begin{array}{llll}a_{1} & a_{2} & a_{3} & a_{4} \\ b_{1} & b_{2} & b_{3} & b_{4} \\ c_{1} & c_{2} & c_{3} & c_{4}\end{array}\right]$. Find $e_{1} A, e_{2} A, e_{3} A$.

2.46. Let $e_{i}=[0, \ldots, 0,1,0, \ldots, 0]$, where 1 is the $i$ th entry. Show\\
(a) $e_{i} A=A_{i}$, ith row of $A$.\\
(c) If $e_{i} A=e_{i} B$, for each $i$, then $A=B$.\\
(b) $B e_{j}^{T}=B^{j}, j$ th column of $B$.\\
(d) If $A e_{j}^{T}=B e_{j}^{T}$, for each $j$, then $A=B$.

2.47. Prove Theorem 2.2(iii) and (iv): (iii) $(B+C) A=B A+C A$, (iv) $k(A B)=(k A) B=A(k B)$.

2.48. Prove Theorem 2.3: (i) $(A+B)^{T}=A^{T}+B^{T}$, (ii) $\left(A^{T}\right)^{T}=A$, (iii) $(k A)^{T}=k A^{T}$.

2.49. Show (a) If $A$ has a zero row, then $A B$ has a zero row. (b) If $B$ has a zero column, then $A B$ has a zero column.

\section*{Square Matrices, Inverses}
2.50. Find the diagonal and trace of each of the following matrices:\\
(a) $A=\left[\begin{array}{rrr}2 & -5 & 8 \\ 3 & -6 & -7 \\ 4 & 0 & -1\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}1 & 3 & -4 \\ 6 & 1 & 7 \\ 2 & -5 & -1\end{array}\right]$,\\
(c) $C=\left[\begin{array}{ccc}4 & 3 & -6 \\ 2 & -5 & 0\end{array}\right]$\\
Problems 2.51-2.53 refer to $A=\left[\begin{array}{rr}2 & -5 \\ 3 & 1\end{array}\right], B=\left[\begin{array}{ll}4 & -2 \\ 1 & -6\end{array}\right], C=\left[\begin{array}{ll}6 & -4 \\ 3 & -2\end{array}\right]$.

2.51. Find (a) $A^{2}$ and $A^{3}$, (b) $f(A)$ and $g(A)$, where

$$
f(x)=x^{3}-2 x^{2}-5, \quad g(x)=x^{2}-3 x+17
$$

2.52. Find (a) $B^{2}$ and $B^{3}$, (b) $f(B)$ and $g(B)$, where

$$
f(x)=x^{2}+2 x-22, \quad g(x)=x^{2}-3 x-6 .
$$

2.53. Find a nonzero column vector $u$ such that $C u=4 u$.

2.54. Find the inverse of each of the following matrices (if it exists):

$$
A=\left[\begin{array}{ll}
7 & 4 \\
5 & 3
\end{array}\right], \quad B=\left[\begin{array}{ll}
2 & 3 \\
4 & 5
\end{array}\right], \quad C=\left[\begin{array}{rr}
4 & -6 \\
-2 & 3
\end{array}\right], \quad D=\left[\begin{array}{ll}
5 & -2 \\
6 & -3
\end{array}\right]
$$

2.55. Find the inverses of $A=\left[\begin{array}{lll}1 & 1 & 2 \\ 1 & 2 & 5 \\ 1 & 3 & 7\end{array}\right]$ and $B=\left[\begin{array}{rrr}1 & -1 & 1 \\ 0 & 1 & -1 \\ 1 & 3 & -2\end{array}\right]$. [Hint: See Problem 2.19.]

2.56. Suppose $A$ is invertible. Show that if $A B=A C$, then $B=C$. Give an example of a nonzero matrix $A$ such that $A B=A C$ but $B \neq C$.

2.57. Find $2 \times 2$ invertible matrices $A$ and $B$ such that $A+B \neq 0$ and $A+B$ is not invertible.

2.58. Show (a) $A$ is invertible if and only if $A^{T}$ is invertible. (b) The operations of inversion and transpose commute; that is, $\left(A^{T}\right)^{-1}=\left(A^{-1}\right)^{T}$. (c) If $A$ has a zero row or zero column, then $A$ is not invertible.

\section*{Diagonal and triangular matrices}
2.59. Let $A=\operatorname{diag}(1,2,-3)$ and $B=\operatorname{diag}(2,-5,0)$. Find\\
(a) $A B, A^{2}, B^{2}$;\\
(b)\\
$f(A)$, where $f(x)=x^{2}+4 x-3$;\\
(c) $A^{-1}$ and $B^{-1}$.

2.60. Let $A=\left[\begin{array}{ll}1 & 2 \\ 0 & 1\end{array}\right]$ and $B=\left[\begin{array}{lll}1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1\end{array}\right]$. (a) Find $A^{n}$. (b) Find $B^{n}$.

2.61. Find all real triangular matrices $A$ such that $A^{2}=B$, where (a) $B=\left[\begin{array}{ll}4 & 21 \\ 0 & 25\end{array}\right]$, (b) $B=\left[\begin{array}{rr}1 & 4 \\ 0 & -9\end{array}\right]$.

2.62. Let $A=\left[\begin{array}{ll}5 & 2 \\ 0 & k\end{array}\right]$. Find all numbers $k$ for which $A$ is a root of the polynomial:\\
(a) $f(x)=x^{2}-7 x+10$,\\
(b) $g(x)=x^{2}-25$,\\
(c) $h(x)=x^{2}-4$.

2.63. Let $B=\left[\begin{array}{rr}1 & 0 \\ 26 & 27\end{array}\right]$. Find a matrix $A$ such that $A^{3}=B$.

2.64. Let $B=\left[\begin{array}{lll}1 & 8 & 5 \\ 0 & 9 & 5 \\ 0 & 0 & 4\end{array}\right]$. Find a triangular matrix $A$ with positive diagonal entries such that $A^{2}=B$.

2.65. Using only the elements 0 and 1 , find the number of $3 \times 3$ matrices that are (a) diagonal, (b) upper triangular, (c) nonsingular and upper triangular. Generalize to $n \times n$ matrices.

2.66. Let $D_{k}=k I$, the scalar matrix belonging to the scalar $k$. Show\\
(a) $D_{k} A=k A$,\\
(b) $B D_{k}=k B$,\\
(c) $D_{k}+D_{k^{\prime}}=D_{k+k^{\prime}}$,\\
(d) $D_{k} D_{k^{\prime}}=D_{k k^{\prime}}$

2.67. Suppose $A B=C$, where $A$ and $C$ are upper triangular.

(a) Find $2 \times 2$ nonzero matrices $A, B, C$, where $B$ is not upper triangular.

(b) Suppose $A$ is also invertible. Show that $B$ must also be upper triangular.

\section*{Special Types of Real Matrices}
2.68. Find $x, y, z$ such that $A$ is symmetric, where\\
(a) $A=\left[\begin{array}{lll}2 & x & 3 \\ 4 & 5 & y \\ z & 1 & 7\end{array}\right]$,\\
(b) $A=\left[\begin{array}{rrr}7 & -6 & 2 x \\ y & z & -2 \\ x & -2 & 5\end{array}\right]$.

2.69. Suppose $A$ is a square matrix. Show (a) $A+A^{T}$ is symmetric, (b) $A-A^{T}$ is skew-symmetric, (c) $A=B+C$, where $B$ is symmetric and $C$ is skew-symmetric.

2.70. Write $A=\left[\begin{array}{ll}4 & 5 \\ 1 & 3\end{array}\right]$ as the sum of a symmetric matrix $B$ and a skew-symmetric matrix $C$.

2.71. Suppose $A$ and $B$ are symmetric. Show that the following are also symmetric:\\
(a) $A+B$;\\
(b) $k A$, for any scalar $k$;\\
(c) $A^{2}$;\\
(d) $A^{n}$, for $n>0$;\\
(e) $f(A)$, for any polynomial $f(x)$.

2.72. Find a $2 \times 2$ orthogonal matrix $P$ whose first row is a multiple of\\
(a) $(3,-4)$,\\
(b) $(1,2)$.

2.73. Find a $3 \times 3$ orthogonal matrix $P$ whose first two rows are multiples of\\
(a) $(1,2,3)$ and $(0,-2,3)$,\\
(b) $(1,3,1)$ and $(1,0,-1)$.

2.74. Suppose $A$ and $B$ are orthogonal matrices. Show that $A^{T}, A^{-1}, A B$ are also orthogonal.

2.75. Which of the following matrices are normal? $A=\left[\begin{array}{rr}3 & -4 \\ 4 & 3\end{array}\right], B=\left[\begin{array}{rr}1 & -2 \\ 2 & 3\end{array}\right], C=\left[\begin{array}{lll}1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1\end{array}\right]$.

Complex Matrices\\
2.76. Find real numbers $x, y, z$ such that $A$ is Hermitian, where $A=\left[\begin{array}{ccc}3 & x+2 i & y i \\ 3-2 i & 0 & 1+z i \\ y i & 1-x i & -1\end{array}\right]$.

2.77. Suppose $A$ is a complex matrix. Show that $A A^{H}$ and $A^{H} A$ are Hermitian.

2.78. Let $A$ be a square matrix. Show that (a) $A+A^{H}$ is Hermitian, (b) $A-A^{H}$ is skew-Hermitian, (c) $A=B+C$, where $B$ is Hermitian and $C$ is skew-Hermitian.

2.79. Determine which of the following matrices are unitary:

$$
A=\left[\begin{array}{rr}
i / 2 & -\sqrt{3} / 2 \\
\sqrt{3} / 2 & -i / 2
\end{array}\right], \quad B=\frac{1}{2}\left[\begin{array}{cc}
1+i & 1-i \\
1-i & 1+i
\end{array}\right], \quad C=\frac{1}{2}\left[\begin{array}{ccc}
1 & -i & -1+i \\
i & 1 & 1+i \\
1+i & -1+i & 0
\end{array}\right]
$$

2.80. Suppose $A$ and $B$ are unitary. Show that $A^{H}, A^{-1}, A B$ are unitary.

2.81. Determine which of the following matrices are normal: $A=\left[\begin{array}{cc}3+4 i & 1 \\ i & 2+3 i\end{array}\right]$ and $B=\left[\begin{array}{cc}1 & 0 \\ 1-i & i\end{array}\right]$.

\section*{Block Matrices}
\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-061}
\end{center}

(a) Find $U V$ using block multiplication. (b) Are $U$ and $V$ block diagonal matrices?

(c) Is $U V$ block diagonal?

2.83. Partition each of the following matrices so that it becomes a square block matrix with as many diagonal blocks as possible:

$$
A=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 2 \\
0 & 0 & 3
\end{array}\right], \quad B=\left[\begin{array}{lllll}
1 & 2 & 0 & 0 & 0 \\
3 & 0 & 0 & 0 & 0 \\
0 & 0 & 4 & 0 & 0 \\
0 & 0 & 5 & 0 & 0 \\
0 & 0 & 0 & 0 & 6
\end{array}\right], \quad C=\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
2 & 0 & 0
\end{array}\right]
$$

2.84. Find $M^{2}$ and $M^{3}$ for (a) $\quad M=\left[\begin{array}{c:cc:c}2 & 0 & 0 & 0 \\ \hdashline 0 & 1 & 4 & 0 \\ 0 & 2 & 1 & 0 \\ \hdashline 0 & 0 & 0 & 3\end{array}\right]$, (b) $M=\left[\begin{array}{cc:cc}1 & 1 & 0 & 0 \\ 2 & 3 & 0 & 0 \\ \hdashline 0 & 0 & 1 & 2 \\ 0 & 0 & 4 & 5\end{array}\right]$.

2.85. For each matrix $M$ in Problem 2.84, find $f(M)$ where $f(x)=x^{2}+4 x-5$.

2.86. Suppose $U=\left[U_{i k}\right]$ and $V=\left[V_{k j}\right]$ are block matrices for which $U V$ is defined and the number of columns of each block $U_{i k}$ is equal to the number of rows of each block $V_{k j}$. Show that $U V=\left[W_{i j}\right]$, where $W_{i j}=\sum_{k} U_{i k} V_{k j}$.

2.87. Suppose $M$ and $N$ are block diagonal matrices where corresponding blocks have the same size, say $M=\operatorname{diag}\left(A_{i}\right)$ and $N=\operatorname{diag}\left(B_{i}\right)$. Show\\
(i) $M+N=\operatorname{diag}\left(A_{i}+B_{i}\right)$,\\
(iii) $M N=\operatorname{diag}\left(A_{i} B_{i}\right)$,\\
(ii) $k M=\operatorname{diag}\left(k A_{i}\right)$,\\
(iv) $f(M)=\operatorname{diag}\left(f\left(A_{i}\right)\right)$ for any polynomial $f(x)$.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $A=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ denotes a matrix $A$ with rows $R_{1}, R_{2}, \ldots$\\
2.38. (a) $[-5,10 ; 27,-34]$,\\
(b) $[17,4 ;-12,13]$,\\
(c) $[-7,-27,11 ;-8,36,-37]$

2.39. (a) $[-7,14 ; \quad 39,-28], \quad[21,105,-98 ; \quad-17,-285,296]$

(b) $[5,-15,20 ; 8,60,-59], \quad[21,105,-98 ;-17,-285,296]$

2.40. (a) $[7,-6 ;-9,22], \quad[-11,38 ; \quad 57,-106]$;

(b) $[11,-9,17 ;-7,53,-39], \quad[15,35,-5 ; 10,-98,69]$; (c) not defined

2.41. (a) $[1,3 ; 2,-4]$, (b) $[5,-6 ; \quad 0,7], \quad$ (c) $[-7,39 ; \quad 14,-28]$, (d) $[5,15 ; 10,-40]$

2.42. (a) $[-13,-3,18 ; 4,17,0]$,

(b) $[-5,-2,4,5 ; 11,-3,-12,18]$,

(c) $[11,-12,0,-5 ;-15,5,8,4]$, (d) $[9 ; 9], \quad$ (e) $[-1 ; 9]$, (f) not defined

2.43. (a) $[1,0 ;-1,3 ; 2,4]$, (b) $[4,0,-3 ;-7,-6,12 ; 4,-8,6]$, (c) not defined

2.44. $[2,4,6 ; \quad-1,-2,-3]$

2.45. $\left[a_{1}, a_{2}, a_{3}, a_{4}\right], \quad\left[b_{1}, b_{2}, b_{3}, b_{4}\right], \quad\left[c_{1}, c_{2}, c_{3}, c_{4}\right]$

2.50. (a) $2,-6,-1, \operatorname{tr}(A)=-5, \quad$ (b) $1,1,-1, \operatorname{tr}(B)=1, \quad$ (c) not defined

2.51. (a) $[-11,-15 ; 9,-14], \quad[-67,40 ;-24,-59], \quad$ (b) $[-50,70 ;-42,-36], g(A)=0$

2.52. (a) $[14,4 ;-2,34], \quad[60,-52 ; 26,-200], \quad$ (b) $f(B)=0,[-4,10 ;-5,46]$

2.53. $u=[2 a, a]^{T}$

2.54. $[3,-4 ;-5,7], \quad\left[-\frac{5}{2}, \frac{3}{2} ; 2,-1\right]$, not defined, $\left[1,-\frac{2}{3} ; 2,-\frac{5}{3}\right]$

2.55. $[1,1,-1 ; \quad 2,-5,3 ; \quad-1,2,-1], \quad[1,1,0 ; \quad-1,-3,1 ; \quad-1,-4,1]$

2.56. $A=[1,2 ; \quad 1,2], B=[0,0 ; \quad 1,1], \quad C=[2,2 ; \quad 0,0]$

2.57. $A=[1,2 ; \quad 0,3] ; \quad B=[4,3 ; \quad 3,0]$

2.58. (c) Hint: Use Problem 2.48

2.59. (a) $A B=\operatorname{diag}(2,-10,0), \quad A^{2}=\operatorname{diag}(1,4,9), \quad B^{2}=\operatorname{diag}(4,25,0)$;

(b) $f(A)=\operatorname{diag}(2,9,-6)$;

(c) $A^{-1}=\operatorname{diag}\left(1, \frac{1}{2},-\frac{1}{3}\right), C^{-1}$ does not exist

2.60. (a) $[1,2 n ; 0,1], \quad$ (b) $\left[1, n, \frac{1}{2} n(n-1) ; 0,1, n ; \quad 0,0,1\right]$

2.61. (a) $[2,3 ; 0,5], \quad[-2,-3 ; \quad 0,-5], \quad[2,-7 ; 0,-5], \quad[-2,7 ; 0,5], \quad$ (b) none

2.62. (a) $k=2, \quad$ (b) $k=-5, \quad$ (c) none

2.63. $[1,0 ; \quad 2,3]$

2.64. $[1,2,1 ; \quad 0,3,1 ; \quad 0,0,2]$

2.65. All entries below the diagonal must be 0 to be upper triangular, and all diagonal entries must be 1 to be nonsingular.\\
(a) $8\left(2^{n}\right)$,\\
(b) $2^{6}\left(2^{n(n+1) / 2}\right)$,\\
(c) $2^{3}\left(2^{n(n-1) / 2}\right)$

2.67. (a) $A=[1,1 ; \quad 0,0], B=[1,2 ; \quad 3,4], C=[4,6 ; \quad 0,0]$

2.68. (a) $x=4, y=1, z=3 ; \quad$ (b) $x=0, y=-6, z$ any real number

2.69. (c) Hint: Let $B=\frac{1}{2}\left(A+A^{T}\right)$ and $C=\frac{1}{2}\left(A-A^{T}\right)$.

2.70. $B=[4,3 ; \quad 3,3], C=[0,2 ; \quad-2,0]$

2.72. (a) $\left[\frac{3}{5},-\frac{4}{5} ; \frac{4}{5}, \frac{3}{5}\right], \quad$ (b) $[1 / \sqrt{5}, 2 / \sqrt{5} ; 2 / \sqrt{5},-1 / \sqrt{5}]$

2.73. (a) $[1 / \sqrt{14}, 2 / \sqrt{14}, 3 / \sqrt{14} ; \quad 0,-2 / \sqrt{13}, 3 / \sqrt{13} ; 12 / \sqrt{157},-3 / \sqrt{157},-2 / \sqrt{157}]$

(b) $[1 / \sqrt{11}, 3 / \sqrt{11}, 1 / \sqrt{11} ; \quad 1 / \sqrt{2}, 0,-1 / \sqrt{2} ; 3 / \sqrt{22},-2 / \sqrt{22}, 3 / \sqrt{22}]$

2.75. $A, C$

2.76. $x=3, y=0, z=3$

2.78. (c) Hint: Let $B=\frac{1}{2}\left(A+A^{H}\right)$ and $C=\frac{1}{2}\left(A-A^{H}\right)$.

2.79. $A, B, C$

2.81. $A$

2.82. (a) $U V=\operatorname{diag}([7,6 ; 17,10] ;[-1,9 ; \quad 7,-5]) ; \quad$ (b) no; $\quad$ (c) yes

2.83. $A$ : line between first and second rows (columns);

$B$ : line between second and third rows (columns) and between fourth and fifth rows (columns);

$C: C$ itself-no further partitioning of $C$ is possible.

2.84. (a) $M^{2}=\operatorname{diag}([4],[9,8 ; 4,9], \quad[9])$,

$M^{3}=\operatorname{diag}([8],[25,44 ; 22,25],[27])$

(b) $\quad M^{2}=\operatorname{diag}([3,4 ; 8,11], \quad[9,12 ; 24,33])$

$M^{3}=\operatorname{diag}([11,15 ; \quad 30,41], \quad[57,78 ; \quad 156,213])$

2.85. (a) $\operatorname{diag}([7], \quad[8,24 ; 12,8],[16])$, (b) $\quad \operatorname{diag}([2,8 ; 16,181], \quad[8,20 ; \quad 40,48])$

\section*{Systems of Linear Equations}
\subsection*{3.1 Introduction}
Systems of linear equations play an important and motivating role in the subject of linear algebra. In fact, many problems in linear algebra reduce to finding the solution of a system of linear equations. Thus, the techniques introduced in this chapter will be applicable to abstract ideas introduced later. On the other hand, some of the abstract results will give us new insights into the structure and properties of systems of linear equations.

All our systems of linear equations involve scalars as both coefficients and constants, and such scalars may come from any number field $K$. There is almost no loss in generality if the reader assumes that all our scalars are real numbers - that is, that they come from the real field $\mathbf{R}$.

\subsection*{3.2 Basic Definitions, Solutions}
This section gives basic definitions connected with the solutions of systems of linear equations. The actual algorithms for finding such solutions will be treated later.

\section*{Linear Equation and Solutions}
A linear equation in unknowns $x_{1}, x_{2}, \ldots, x_{n}$ is an equation that can be put in the standard form


\begin{equation*}
a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b \tag{3.1}
\end{equation*}


where $a_{1}, a_{2}, \ldots, a_{n}$, and $b$ are constants. The constant $a_{k}$ is called the coefficient of $x_{k}$, and $b$ is called the constant term of the equation.

A solution of the linear equation (3.1) is a list of values for the unknowns or, equivalently, a vector $u$ in $K^{n}$, say

$$
x_{1}=k_{1}, \quad x_{2}=k_{2}, \quad \ldots, \quad x_{n}=k_{n} \quad \text { or } \quad u=\left(k_{1}, k_{2}, \ldots, k_{n}\right)
$$

such that the following statement (obtained by substituting $k_{i}$ for $x_{i}$ in the equation) is true:

$$
a_{1} k_{1}+a_{2} k_{2}+\cdots+a_{n} k_{n}=b
$$

In such a case we say that $u$ satisfies the equation.

Remark: Equation (3.1) implicitly assumes there is an ordering of the unknowns. In order to avoid subscripts, we will usually use $x, y$ for two unknowns; $x, y, z$ for three unknowns; and $x, y, z, t$ for four unknowns; they will be ordered as shown.

EXAMPLE 3.1 Consider the following linear equation in three unknowns $x, y, z$ :

$$
x+2 y-3 z=6
$$

We note that $x=5, y=2, z=1$, or, equivalently, the vector $u=(5,2,1)$ is a solution of the equation. That is,

$$
5+2(2)-3(1)=6 \quad \text { or } \quad 5+4-3=6 \quad \text { or } \quad 6=6
$$

On the other hand, $w=(1,2,3)$ is not a solution, because on substitution, we do not get a true statement:

$$
1+2(2)-3(3)=6 \quad \text { or } \quad 1+4-9=6 \quad \text { or } \quad-4=6
$$

\section*{System of Linear Equations}
A system of linear equations is a list of linear equations with the same unknowns. In particular, a system of $m$ linear equations $L_{1}, L_{2}, \ldots, L_{m}$ in $n$ unknowns $x_{1}, x_{2}, \ldots, x_{n}$ can be put in the standard form


\begin{align*}
& a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1} \\
& a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2}  \tag{3.2}\\
& \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
& a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}=b_{m}
\end{align*}


where the $a_{i j}$ and $b_{i}$ are constants. The number $a_{i j}$ is the coefficient of the unknown $x_{j}$ in the equation $L_{i}$, and the number $b_{i}$ is the constant of the equation $L_{i}$.

The system (3.2) is called an $m \times n$ (read: $m$ by $n$ ) system. It is called a square system if $m=n$-that is, if the number $m$ of equations is equal to the number $n$ of unknowns.

The system (3.2) is said to be homogeneous if all the constant terms are zero-that is, if $b_{1}=0$, $b_{2}=0, \ldots, b_{m}=0$. Otherwise the system is said to be nonhomogeneous.

A solution (or a particular solution) of the system (3.2) is a list of values for the unknowns or, equivalently, a vector $u$ in $K^{n}$, which is a solution of each of the equations in the system. The set of all solutions of the system is called the solution set or the general solution of the system.

EXAMPLE 3.2 Consider the following system of linear equations:

$$
\begin{array}{r}
x_{1}+x_{2}+4 x_{3}+3 x_{4}=5 \\
2 x_{1}+3 x_{2}+x_{3}-2 x_{4}=1 \\
x_{1}+2 x_{2}-5 x_{3}+4 x_{4}=3
\end{array}
$$

It is a $3 \times 4$ system because it has three equations in four unknowns. Determine whether (a) $u=(-8,6,1,1)$ and (b) $v=(-10,5,1,2)$ are solutions of the system.

(a) Substitute the values of $u$ in each equation, obtaining

$$
\begin{aligned}
& -8+6+4(1)+3(1)=5 \quad \text { or } \quad-8+6+4+3=5 \quad \text { or } \quad 5=5 \\
& 2(-8)+3(6)+1-2(1)=1 \quad \text { or } \quad-16+18+1-2=1 \quad \text { or } \quad 1=1 \\
& -8+2(6)-5(1)+4(1)=3 \quad \text { or } \quad-8+12-5+4=3 \quad \text { or } \quad 3=3
\end{aligned}
$$

Yes, $u$ is a solution of the system because it is a solution of each equation.

(b) Substitute the values of $v$ into each successive equation, obtaining

$$
\begin{aligned}
& -10+5+4(1)+3(2)=5 \quad \text { or } \quad-10+5+4+6=5 \quad \text { or } \quad 5=5 \\
& 2(-10)+3(5)+1-2(2)=1 \quad \text { or } \quad-20+15+1-4=1 \quad \text { or } \quad-8=1
\end{aligned}
$$

No, $v$ is not a solution of the system, because it is not a solution of the second equation. (We do not need to substitute $v$ into the third equation.)

The system (3.2) of linear equations is said to be consistent if it has one or more solutions, and it is said to be inconsistent if it has no solution. If the field $K$ of scalars is infinite, such as when $K$ is the real field $\mathbf{R}$ or the complex field $\mathbf{C}$, then we have the following important result.

THEOREM 3.1: Suppose the field $K$ is infinite. Then any system $\mathscr{L}$ of linear equations has (i) a unique solution, (ii) no solution, or (iii) an infinite number of solutions.

This situation is pictured in Fig. 3-1. The three cases have a geometrical description when the system $\mathscr{L}$ consists of two equations in two unknowns (Section 3.4).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-066}
\end{center}

Figure 3-1

\section*{Augmented and Coefficient Matrices of a System}
Consider again the general system (3.2) of $m$ equations in $n$ unknowns. Such a system has associated with it the following two matrices:

$$
M=\left[\begin{array}{ccccc}
a_{11} & a_{12} & \ldots & a_{1 n} & b_{1} \\
a_{21} & a_{22} & \ldots & a_{2 n} & b_{2} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n} & b_{n}
\end{array}\right] \quad \text { and } \quad A=\left[\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n}
\end{array}\right]
$$

The first matrix $M$ is called the augmented matrix of the system, and the second matrix $A$ is called the coefficient matrix.

The coefficient matrix $A$ is simply the matrix of coefficients, which is the augmented matrix $M$ without the last column of constants. Some texts write $M=[A, B]$ to emphasize the two parts of $M$, where $B$ denotes the column vector of constants. The augmented matrix $M$ and the coefficient matrix $A$ of the system in Example 3.2 are as follows:

$$
M=\left[\begin{array}{rrrrr}
1 & 1 & 4 & 3 & 5 \\
2 & 3 & 1 & -2 & 1 \\
1 & 2 & -5 & 4 & 3
\end{array}\right] \quad \text { and } \quad A=\left[\begin{array}{rrrr}
1 & 1 & 4 & 3 \\
2 & 3 & 1 & -2 \\
1 & 2 & -5 & 4
\end{array}\right]
$$

As expected, $A$ consists of all the columns of $M$ except the last, which is the column of constants.

Clearly, a system of linear equations is completely determined by its augmented matrix $M$, and vice versa. Specifically, each row of $M$ corresponds to an equation of the system, and each column of $M$ corresponds to the coefficients of an unknown, except for the last column, which corresponds to the constants of the system.

\section*{Degenerate Linear Equations}
A linear equation is said to be degenerate if all the coefficients are zero-that is, if it has the form


\begin{equation*}
0 x_{1}+0 x_{2}+\cdots+0 x_{n}=b \tag{3.3}
\end{equation*}


The solution of such an equation depends only on the value of the constant $b$. Specifically,

(i) If $b \neq 0$, then the equation has no solution.

(ii) If $b=0$, then every vector $u=\left(k_{1}, k_{2}, \ldots, k_{n}\right)$ in $K^{n}$ is a solution.

The following theorem applies.

THEOREM 3.2: Let $\mathscr{L}$ be a system of linear equations that contains a degenerate equation $L$, say with constant $b$.

(i) If $b \neq 0$, then the system $\mathscr{L}$ has no solution.

(ii) If $b=0$, then $L$ may be deleted from the system without changing the solution set of the system.

Part (i) comes from the fact that the degenerate equation has no solution, so the system has no solution. Part (ii) comes from the fact that every element in $K^{n}$ is a solution of the degenerate equation.

\section*{Leading Unknown in a Nondegenerate Linear Equation}
Now let $L$ be a nondegenerate linear equation. This means one or more of the coefficients of $L$ are not zero. By the leading unknown of $L$, we mean the first unknown in $L$ with a nonzero coefficient. For example, $x_{3}$ and $y$ are the leading unknowns, respectively, in the equations

$$
0 x_{1}+0 x_{2}+5 x_{3}+6 x_{4}+0 x_{5}+8 x_{6}=7 \quad \text { and } \quad 0 x+2 y-4 z=5
$$

We frequently omit terms with zero coefficients, so the above equations would be written as

$$
5 x_{3}+6 x_{4}+8 x_{6}=7 \quad \text { and } \quad 2 y-4 z=5
$$

In such a case, the leading unknown appears first.

\subsection*{3.3 Equivalent Systems, Elementary Operations}
Consider the system (3.2) of $m$ linear equations in $n$ unknowns. Let $L$ be the linear equation obtained by multiplying the $m$ equations by constants $c_{1}, c_{2}, \ldots, c_{m}$, respectively, and then adding the resulting equations. Specifically, let $L$ be the following linear equation:

$$
\left(c_{1} a_{11}+\cdots+c_{m} a_{m 1}\right) x_{1}+\cdots+\left(c_{1} a_{1 n}+\cdots+c_{m} a_{m n}\right) x_{n}=c_{1} b_{1}+\cdots+c_{m} b_{m}
$$

Then $L$ is called a linear combination of the equations in the system. One can easily show (Problem 3.43) that any solution of the system (3.2) is also a solution of the linear combination $L$.

EXAMPLE 3.3 Let $L_{1}, L_{2}, L_{3}$ denote, respectively, the three equations in Example 3.2. Let $L$ be the equation obtained by multiplying $L_{1}, L_{2}, L_{3}$ by $3,-2,4$, respectively, and then adding. Namely,

$$
\begin{array}{rr}
3 L_{1}: & \quad 3 x_{1}+3 x_{2}+12 x_{3}+9 x_{4}=15 \\
-2 L_{2}: & -4 x_{1}-6 x_{2}-2 x_{3}+4 x_{4}=-2 \\
4 L_{1}: & 4 x_{1}+8 x_{2}-20 x_{3}+16 x_{4}=12 \\
\hline \text { Sum) } L: & 3 x_{1}+5 x_{2}-10 x_{3}+29 x_{4}=25
\end{array}
$$

Then $L$ is a linear combination of $L_{1}, L_{2}, L_{3}$. As expected, the solution $u=(-8,6,1,1)$ of the system is also a solution of $L$. That is, substituting $u$ in $L$, we obtain a true statement:

$$
3(-8)+5(6)-10(1)+29(1)=25 \quad \text { or } \quad-24+30-10+29=25 \quad \text { or } \quad 9=9
$$

The following theorem holds.

THEOREM 3.3: Two systems of linear equations have the same solutions if and only if each equation in each system is a linear combination of the equations in the other system.

Two systems of linear equations are said to be equivalent if they have the same solutions. The next subsection shows one way to obtain equivalent systems of linear equations.

\section*{Elementary Operations}
The following operations on a system of linear equations $L_{1}, L_{2}, \ldots, L_{m}$ are called elementary operations.

$\left[\mathrm{E}_{1}\right]$ Interchange two of the equations. We indicate that the equations $L_{i}$ and $L_{j}$ are interchanged by writing:

$$
\text { "Interchange } L_{i} \text { and } L_{j} \text { " } \quad \text { or } \quad " L_{i} \longleftrightarrow L_{j} "
$$

$\left[\mathrm{E}_{2}\right]$ Replace an equation by a nonzero multiple of itself. We indicate that equation $L_{i}$ is replaced by $k L_{i}$ (where $k \neq 0$ ) by writing

$$
\text { "Replace } L_{i} \text { by } k L_{i} " \quad \text { or } \quad " k L_{i} \rightarrow L_{i} "
$$

$\left[E_{3}\right]$ Replace an equation by the sum of a multiple of another equation and itself. We indicate that equation $L_{j}$ is replaced by the sum of $k L_{i}$ and $L_{j}$ by writing

$$
\text { "Replace } L_{j} \text { by } k L_{i}+L_{j} " \quad \text { or } \quad " k L_{i}+L_{j} \rightarrow L_{j} "
$$

The arrow $\rightarrow$ in $\left[\mathrm{E}_{2}\right]$ and $\left[\mathrm{E}_{3}\right]$ may be read as "replaces."

The main property of the above elementary operations is contained in the following theorem (proved in Problem 3.45).

THEOREM 3.4: Suppose a system of $\mathscr{M}$ of linear equations is obtained from a system $\mathscr{L}$ of linear equations by a finite sequence of elementary operations. Then $\mathscr{M}$ and $\mathscr{L}$ have the same solutions.

Remark: Sometimes (say to avoid fractions when all the given scalars are integers) we may apply $\left[E_{2}\right]$ and $\left[E_{3}\right]$ in one step; that is, we may apply the following operation:

[E] Replace equation $L_{j}$ by the sum of $k L_{i}$ and $k^{\prime} L_{j}$ (where $k^{\prime} \neq 0$ ), written

$$
\text { "Replace } L_{j} \text { by } k L_{i}+k^{\prime} L_{j} " \quad \text { or } \quad " k L_{i}+k^{\prime} L_{j} \rightarrow L_{j} \text { " }
$$

We emphasize that in operations $\left[\mathrm{E}_{3}\right]$ and $[\mathrm{E}]$, only equation $L_{j}$ is changed.

Gaussian elimination, our main method for finding the solution of a given system of linear equations, consists of using the above operations to transform a given system into an equivalent system whose solution can be easily obtained.

The details of Gaussian elimination are discussed in subsequent sections.

\subsection*{3.4 Small Square Systems of Linear Equations}
This section considers the special case of one equation in one unknown, and two equations in two unknowns. These simple systems are treated separately because their solution sets can be described geometrically, and their properties motivate the general case.

\section*{Linear Equation in One Unknown}
The following simple basic result is proved in Problem 3.5.

THEOREM 3.5: Consider the linear equation $a x=b$.

(i) If $a \neq 0$, then $x=b / a$ is a unique solution of $a x=b$.

(ii) If $a=0$, but $b \neq 0$, then $a x=b$ has no solution.

(iii) If $a=0$ and $b=0$, then every scalar $k$ is a solution of $a x=b$.

EXAMPLE 3.4 Solve (a) $4 x-1=x+6$, (b) $2 x-5-x=x+3$, (c) $4+x-3=2 x+1-x$.

(a) Rewrite the equation in standard form obtaining $3 x=7$. Then $x=\frac{7}{3}$ is the unique solution [Theorem 3.5(i)].

(b) Rewrite the equation in standard form, obtaining $0 x=8$. The equation has no solution [Theorem 3.5(ii)].

(c) Rewrite the equation in standard form, obtaining $0 x=0$. Then every scalar $k$ is a solution [Theorem 3.5(iii)].

\section*{System of Two Linear Equations in Two Unknowns ( $2 \times 2$ System)}
Consider a system of two nondegenerate linear equations in two unknowns $x$ and $y$, which can be put in the standard form


\begin{align*}
& A_{1} x+B_{1} y=C_{1}  \tag{3.4}\\
& A_{2} x+B_{2} y=C_{2}
\end{align*}


Because the equations are nondegenerate, $A_{1}$ and $B_{1}$ are not both zero, and $A_{2}$ and $B_{2}$ are not both zero.

The general solution of the system (3.4) belongs to one of three types as indicated in Fig. 3-1. If $\mathbf{R}$ is the field of scalars, then the graph of each equation is a line in the plane $\mathbf{R}^{2}$ and the three types may be described geometrically as pictured in Fig. 3-2. Specifically,

(1) The system has exactly one solution.

Here the two lines intersect in one point [Fig. 3-2(a)]. This occurs when the lines have distinct slopes or, equivalently, when the coefficients of $x$ and $y$ are not proportional:

$$
\frac{A_{1}}{A_{2}} \neq \frac{B_{1}}{B_{2}} \quad \text { or, equivalently, } \quad A_{1} B_{2}-A_{2} B_{1} \neq 0
$$

For example, in Fig. 3-2(a), $1 / 3 \neq-1 / 2$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-069(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-069(2)}
\end{center}

$L_{1}: x+3 y=3$

$L_{2}: 2 x+6 y=-8$

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-069}
\end{center}

$L_{1}: x+2 y=4$

$L_{2}: 2 x+4 y=8$

(c)

(2) The system has no solution.

Here the two lines are parallel [Fig. 3-2(b)]. This occurs when the lines have the same slopes but different $y$ intercepts, or when

$\frac{A_{1}}{A_{2}}=\frac{B_{1}}{B_{2}} \neq \frac{C_{1}}{C_{2}}$

For example, in Fig. $3-2(b), 1 / 2=3 / 6 \neq-3 / 8$.

(3) The system has an infinite number of solutions.

Here the two lines coincide [Fig. 3-2(c)]. This occurs when the lines have the same slopes and same $y$ intercepts, or when the coefficients and constants are proportional,

$\frac{A_{1}}{A_{2}}=\frac{B_{1}}{B_{2}}=\frac{C_{1}}{C_{2}}$

For example, in Fig. 3-2(c), $1 / 2=2 / 4=4 / 8$.

Remark: The following expression and its value is called a determinant of order two:

$$
\left|\begin{array}{ll}
A_{1} & B_{1} \\
A_{2} & B_{2}
\end{array}\right|=A_{1} B_{2}-A_{2} B_{1}
$$

Determinants will be studied in Chapter 8. Thus, the system (3.4) has a unique solution if and only if the determinant of its coefficients is not zero. (We show later that this statement is true for any square system of linear equations.)

\section*{Elimination Algorithm}
The solution to system (3.4) can be obtained by the process of elimination, whereby we reduce the system to a single equation in only one unknown. Assuming the system has a unique solution, this elimination algorithm has two parts.

ALGORITHM 3.1: The input consists of two nondegenerate linear equations $L_{1}$ and $L_{2}$ in two unknowns with a unique solution.

Part A. (Forward Elimination) Multiply each equation by a constant so that the resulting coefficients of one unknown are negatives of each other, and then add the two equations to obtain a new equation $L$ that has only one unknown.

Part B. (Back-Substitution) Solve for the unknown in the new equation $L$ (which contains only one unknown), substitute this value of the unknown into one of the original equations, and then solve to obtain the value of the other unknown.

Part A of Algorithm 3.1 can be applied to any system even if the system does not have a unique solution. In such a case, the new equation $L$ will be degenerate and Part B will not apply.

EXAMPLE 3.5 (Unique Case). Solve the system

$$
\begin{aligned}
& L_{1}: 2 x-3 y=-8 \\
& L_{2}: 3 x+4 y=5
\end{aligned}
$$

The unknown $x$ is eliminated from the equations by forming the new equation $L=-3 L_{1}+2 L_{2}$. That is, we multiply $L_{1}$ by -3 and $L_{2}$ by 2 and add the resulting equations as follows:

$-3 L_{1}: \quad-6 x+9 y=24$

$2 L_{2}: \quad 6 x+8 y=10$

Addition: $\quad 17 y=34$

We now solve the new equation for $y$, obtaining $y=2$. We substitute $y=2$ into one of the original equations, say $L_{1}$, and solve for the other unknown $x$, obtaining

$$
2 x-3(2)=-8 \quad \text { or } \quad 2 x-6=8 \quad \text { or } \quad 2 x=-2 \quad \text { or } \quad x=-1
$$

Thus, $x=-1, y=2$, or the pair $u=(-1,2)$ is the unique solution of the system. The unique solution is expected, because $2 / 3 \neq-3 / 4$. [Geometrically, the lines corresponding to the equations intersect at the point $(-1,2)$.]

\section*{EXAMPLE 3.6 (Nonunique Cases)}
(a) Solve the system

$$
\begin{aligned}
& L_{1}: \quad x-3 y=4 \\
& L_{2}:-2 x+6 y=5
\end{aligned}
$$

We eliminated $x$ from the equations by multiplying $L_{1}$ by 2 and adding it to $L_{2}$ - that is, by forming the new equation $L=2 L_{1}+L_{2}$. This yields the degenerate equation

$$
0 x+0 y=13
$$

which has a nonzero constant $b=13$. Thus, this equation and the system have no solution. This is expected, because $1 /(-2)=-3 / 6 \neq 4 / 5$. (Geometrically, the lines corresponding to the equations are parallel.)

(b) Solve the system

$$
\begin{aligned}
L_{1}: & x-3 y & =4 \\
L_{2}: & -2 x+6 y & =-8
\end{aligned}
$$

We eliminated $x$ from the equations by multiplying $L_{1}$ by 2 and adding it to $L_{2}$ - that is, by forming the new equation $L=2 L_{1}+L_{2}$. This yields the degenerate equation

$$
0 x+0 y=0
$$

where the constant term is also zero. Thus, the system has an infinite number of solutions, which correspond to the solutions of either equation. This is expected, because $1 /(-2)=-3 / 6=4 /(-8)$. (Geometrically, the lines corresponding to the equations coincide.)

To find the general solution, let $y=a$, and substitute into $L_{1}$ to obtain

$$
x-3 a=4 \quad \text { or } \quad x=3 a+4
$$

Thus, the general solution of the system is

$$
x=3 a+4, y=a \quad \text { or } \quad u=(3 a+4, a)
$$

where $a$ (called a parameter) is any scalar.

\subsection*{3.5 Systems in Triangular and Echelon Forms}
The main method for solving systems of linear equations, Gaussian elimination, is treated in Section 3.6. Here we consider two simple types of systems of linear equations: systems in triangular form and the more general systems in echelon form.

\section*{Triangular Form}
Consider the following system of linear equations, which is in triangular form:

$$
\begin{aligned}
2 x_{1}-3 x_{2}+5 x_{3}-2 x_{4} & =9 \\
5 x_{2}-x_{3}+3 x_{4} & =1 \\
7 x_{3}-x_{4} & =3 \\
2 x_{4} & =8
\end{aligned}
$$

That is, the first unknown $x_{1}$ is the leading unknown in the first equation, the second unknown $x_{2}$ is the leading unknown in the second equation, and so on. Thus, in particular, the system is square and each leading unknown is directly to the right of the leading unknown in the preceding equation.

Such a triangular system always has a unique solution, which may be obtained by back-substitution. That is,

(1) First solve the last equation for the last unknown to get $x_{4}=4$.

(2) Then substitute this value $x_{4}=4$ in the next-to-last equation, and solve for the next-to-last unknown $x_{3}$ as follows:

$$
7 x_{3}-4=3 \quad \text { or } \quad 7 x_{3}=7 \quad \text { or } \quad x_{3}=1
$$

(3) Now substitute $x_{3}=1$ and $x_{4}=4$ in the second equation, and solve for the second unknown $x_{2}$ as follows:

$$
5 x_{2}-1+12=1 \quad \text { or } \quad 5 x_{2}+11=1 \quad \text { or } \quad 5 x_{2}=-10 \quad \text { or } \quad x_{2}=-2
$$

(4) Finally, substitute $x_{2}=-2, x_{3}=1, x_{4}=4$ in the first equation, and solve for the first unknown $x_{1}$ as follows:

$2 x_{1}+6+5-8=9 \quad$ or $\quad 2 x_{1}+3=9 \quad$ or $\quad 2 x_{1}=6 \quad$ or $\quad x_{1}=3$

Thus, $x_{1}=3, x_{2}=-2, x_{3}=1, x_{4}=4$, or, equivalently, the vector $u=(3,-2,1,4)$ is the unique solution of the system.

Remark: There is an alternative form for back-substitution (which will be used when solving a system using the matrix format). Namely, after first finding the value of the last unknown, we substitute this value for the last unknown in all the preceding equations before solving for the next-to-last unknown. This yields a triangular system with one less equation and one less unknown. For example, in the above triangular system, we substitute $x_{4}=4$ in all the preceding equations to obtain the triangular system

$$
\begin{aligned}
2 x_{1}-3 x_{2}+5 x_{3} & =17 \\
5 x_{2}-x_{3} & =-1 \\
7 x_{3} & =7
\end{aligned}
$$

We then repeat the process using the new last equation. And so on.

\section*{Echelon Form, Pivot and Free Variables}
The following system of linear equations is said to be in echelon form:

$$
\begin{aligned}
2 x_{1}+6 x_{2}-x_{3}+4 x_{4}-2 x_{5} & =15 \\
x_{3}+2 x_{4}+2 x_{5} & =5 \\
3 x_{4}-9 x_{5} & =6
\end{aligned}
$$

That is, no equation is degenerate and the leading unknown in each equation other than the first is to the right of the leading unknown in the preceding equation. The leading unknowns in the system, $x_{1}, x_{3}, x_{4}$, are called pivot variables, and the other unknowns, $x_{2}$ and $x_{5}$, are called free variables.

Generally speaking, an echelon system or a system in echelon form has the following form:

\[
\begin{array}{r}
a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}+a_{14} x_{4}+\cdots+a_{1 n} x_{n}=b_{1} \\
a_{2 j_{2}} x_{j_{2}}+a_{2, j_{2}+1} x_{j_{2}+1}+\cdots+a_{2 n} x_{n}=b_{2}  \tag{3.5}\\
\cdots \cdots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{r j_{r}} x_{j_{r}}+\cdots+a_{r n} x_{n}=b_{r}
\end{array}
\]

where $1<j_{2}<\cdots<j_{r}$ and $a_{11}, a_{2 j_{2}}, \ldots, a_{r j_{r}}$ are not zero. The pivot variables are $x_{1}, x_{j_{2}}, \ldots, x_{j_{r}}$. Note that $r \leq n$.

The solution set of any echelon system is described in the following theorem (proved in Problem 3.10).

THEOREM 3.6: Consider a system of linear equations in echelon form, say with $r$ equations in $n$ unknowns. There are two cases:

(i) $r=n$. That is, there are as many equations as unknowns (triangular form). Then the system has a unique solution.

(ii) $r<n$. That is, there are more unknowns than equations. Then we can arbitrarily assign values to the $n-r$ free variables and solve uniquely for the $r$ pivot variables, obtaining a solution of the system.

Suppose an echelon system contains more unknowns than equations. Assuming the field $K$ is infinite, the system has an infinite number of solutions, because each of the $n-r$ free variables may be assigned any scalar.

The general solution of a system with free variables may be described in either of two equivalent ways, which we illustrate using the above echelon system where there are $r=3$ equations and $n=5$ unknowns. One description is called the "Parametric Form" of the solution, and the other description is called the "Free-Variable Form."

\section*{Parametric Form}
Assign arbitrary values, called parameters, to the free variables $x_{2}$ and $x_{5}$, say $x_{2}=a$ and $x_{5}=b$, and then use back-substitution to obtain values for the pivot variables $x_{1}, x_{3}, x_{5}$ in terms of the parameters $a$ and $b$. Specifically,

(1) Substitute $x_{5}=b$ in the last equation, and solve for $x_{4}$ :

$$
3 x_{4}-9 b=6 \quad \text { or } \quad 3 x_{4}=6+9 b \quad \text { or } \quad x_{4}=2+3 b
$$

(2) Substitute $x_{4}=2+3 b$ and $x_{5}=b$ into the second equation, and solve for $x_{3}$ :

$$
x_{3}+2(2+3 b)+2 b=5 \quad \text { or } \quad x_{3}+4+8 b=5 \quad \text { or } \quad x_{3}=1-8 b
$$

(3) Substitute $x_{2}=a, x_{3}=1-8 b, x_{4}=2+3 b, x_{5}=b$ into the first equation, and solve for $x_{1}$ :

$$
2 x_{1}+6 a-(1-8 b)+4(2+3 b)-2 b=15 \quad \text { or } \quad x_{1}=4-3 a-9 b
$$

Accordingly, the general solution in parametric form is

$$
x_{1}=4-3 a-9 b, \quad x_{2}=a, \quad x_{3}=1-8 b, \quad x_{4}=2+3 b, \quad x_{5}=b
$$

or, equivalently, $v=(4-3 a-9 b, a, 1-8 b, 2+3 b, b)$ where $a$ and $b$ are arbitrary numbers.

\section*{Free-Variable Form}
Use back-substitution to solve for the pivot variables $x_{1}, x_{3}, x_{4}$ directly in terms of the free variables $x_{2}$ and $x_{5}$. That is, the last equation gives $x_{4}=2+3 x_{5}$. Substitution in the second equation yields $x_{3}=1-8 x_{5}$, and then substitution in the first equation yields $x_{1}=4-3 x_{2}-9 x_{5}$. Accordingly,

$$
x_{1}=4-3 x_{2}-9 x_{5}, \quad x_{2}=\text { free variable }, \quad x_{3}=1-8 x_{5}, \quad x_{4}=2+3 x_{5}, \quad x_{5}=\text { free variable }
$$

or, equivalently,

$$
v=\left(4-3 x_{2}-9 x_{5}, x_{2}, 1-8 x_{5}, 2+3 x_{5}, x_{5}\right)
$$

is the free-variable form for the general solution of the system.

We emphasize that there is no difference between the above two forms of the general solution, and the use of one or the other to represent the general solution is simply a matter of taste.

Remark: A particular solution of the above system can be found by assigning any values to the free variables and then solving for the pivot variables by back-substitution. For example, setting $x_{2}=1$ and $x_{5}=1$, we obtain

$$
x_{4}=2+3=5, \quad x_{3}=1-8=-7, \quad x_{1}=4-3-9=-8
$$

Thus, $u=(-8,1,7,5,1)$ is the particular solution corresponding to $x_{2}=1$ and $x_{5}=1$.

\subsection*{3.6 Gaussian Elimination}
The main method for solving the general system (3.2) of linear equations is called Gaussian elimination. It essentially consists of two parts:

Part A. (Forward Elimination) Step-by-step reduction of the system yielding either a degenerate equation with no solution (which indicates the system has no solution) or an equivalent simpler system in triangular or echelon form.

Part B. (Backward Elimination) Step-by-step back-substitution to find the solution of the simpler system.

Part B has already been investigated in Section 3.4. Accordingly, we need only give the algorithm for Part A, which is as follows.

ALGORITHM 3.2 for (Part A): Input: The $m \times n$ system (3.2) of linear equations.

ELIMINATION STEP: Find the first unknown in the system with a nonzero coefficient (which now must be $x_{1}$ ).

(a) Arrange so that $a_{11} \neq 0$. That is, if necessary, interchange equations so that the first unknown $x_{1}$ appears with a nonzero coefficient in the first equation.

(b) Use $a_{11}$ as a pivot to eliminate $x_{1}$ from all equations except the first equation. That is, for $i>1$ :\\
(1) Set $m=-a_{i 1} / a_{11}$;\\
(2) Replace $L_{i}$ by $m L_{1}+L_{i}$

The system now has the following form:

$$
\begin{array}{r}
a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}+\cdots+a_{1 n} x_{n}=b_{1} \\
a_{2 j_{2}} x_{j_{2}}+\cdots+a_{2 n} x_{n}=b_{2} \\
\cdots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{m j_{2}} x_{j_{2}}+\cdots+a_{m n} x_{n}=b_{n}
\end{array}
$$

where $x_{1}$ does not appear in any equation except the first, $a_{11} \neq 0$, and $x_{j_{2}}$ denotes the first unknown with a nonzero coefficient in any equation other than the first.

(c) Examine each new equation $L$.

(1) If $L$ has the form $0 x_{1}+0 x_{2}+\cdots+0 x_{n}=b$ with $b \neq 0$, then

\section*{STOP}
The system is inconsistent and has no solution.

(2) If $L$ has the form $0 x_{1}+0 x_{2}+\cdots+0 x_{n}=0$ or if $L$ is a multiple of another equation, then delete $L$ from the system.

RECURSION STEP: Repeat the Elimination Step with each new "smaller" subsystem formed by all the equations excluding the first equation.

OUTPUT: Finally, the system is reduced to triangular or echelon form, or a degenerate equation with no solution is obtained indicating an inconsistent system.

The next remarks refer to the Elimination Step in Algorithm 3.2.

(1) The following number $m$ in (b) is called the multiplier:

$m=-\frac{a_{i 1}}{a_{11}}=-\frac{\text { coefficient to be deleted }}{\text { pivot }}$

(2) One could alternatively apply the following operation in (b):

Replace $L_{i}$ by $-a_{i 1} L_{1}+a_{11} L_{i}$

This would avoid fractions if all the scalars were originally integers.

\section*{Gaussian Elimination Example}
Here we illustrate in detail Gaussian elimination using the following system of linear equations:

$$
\begin{aligned}
L_{1}: & x-3 y-2 z & =6 \\
L_{2}: & 2 x-4 y-3 z & =8 \\
L_{3}: & -3 x+6 y+8 z & =-5
\end{aligned}
$$

Part $A$. We use the coefficient 1 of $x$ in the first equation $L_{1}$ as the pivot in order to eliminate $x$ from the second equation $L_{2}$ and from the third equation $L_{3}$. This is accomplished as follows:

(1) Multiply $L_{1}$ by the multiplier $m=-2$ and add it to $L_{2}$; that is, "Replace $L_{2}$ by $-2 L_{1}+L_{2}$."

(2) Multiply $L_{1}$ by the multiplier $m=3$ and add it to $L_{3}$; that is, "Replace $L_{3}$ by $3 L_{1}+L_{3}$."

These steps yield

$$
\begin{aligned}
& \begin{aligned}
(-2) L_{1}: & -2 x+6 y+4 z & =-12 \\
L_{2}: & 2 x-4 y-3 z & =8
\end{aligned} \\
& \text { New } L_{2}: \quad 2 y+z=-4 \\
& \begin{array}{rlrl}
3 L_{1}: & & 3 x-9 y-6 z & =18 \\
L_{3}: & -3 x+6 y+8 z & =-5 \\
\text { New } L_{3}: & -3 y+2 z & =13
\end{array}
\end{aligned}
$$

Thus, the original system is replaced by the following system:

$$
\begin{aligned}
L_{1}: & x-3 y-2 z & =6 \\
L_{2}: & 2 y+z & =-4 \\
L_{3}: & -3 y+2 z & =13
\end{aligned}
$$

(Note that the equations $L_{2}$ and $L_{3}$ form a subsystem with one less equation and one less unknown than the original system.)

Next we use the coefficient 2 of $y$ in the (new) second equation $L_{2}$ as the pivot in order to eliminate $y$ from the (new) third equation $L_{3}$. This is accomplished as follows:

(3) Multiply $L_{2}$ by the multiplier $m=\frac{3}{2}$ and add it to $L_{3}$; that is, "Replace $L_{3}$ by $\frac{3}{2} L_{2}+L_{3}$." (Alternately, 'Replace $L_{3}$ by $3 L_{2}+2 L_{3}$," which will avoid fractions.)

This step yields

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-075}
\end{center}

Thus, our system is replaced by the following system:

$$
\begin{aligned}
L_{1}: & x-3 y-2 z & =6 \\
L_{2}: & 2 y+z & =-4 \\
L_{3}: & 7 z & =14 \quad\left(\text { or } \frac{7}{2} z=7\right)
\end{aligned}
$$

The system is now in triangular form, so Part A is completed.

Part B. The values for the unknowns are obtained in reverse order, $z, y, x$, by back-substitution. Specifically,

(1) Solve for $z$ in $L_{3}$ to get $z=2$.

(2) Substitute $z=2$ in $L_{2}$, and solve for $y$ to get $y=-3$.

(3) Substitute $y=-3$ and $z=2$ in $L_{1}$, and solve for $x$ to get $x=1$.

Thus, the solution of the triangular system and hence the original system is as follows:

$$
x=1, \quad y=-3, \quad z=2 \quad \text { or, equivalently, } \quad u=(1,-3,2) \text {. }
$$

\section*{Condensed Format}
The Gaussian elimination algorithm involves rewriting systems of linear equations. Sometimes we can avoid excessive recopying of some of the equations by adopting a "condensed format." This format for the solution of the above system follows:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-076}
\end{center}

Equation


\begin{equation*}
x-3 y-2 z=6 \tag{1}
\end{equation*}



\begin{align*}
2 x-4 y-3 z & =8  \tag{2}\\
-3 x+6 y+8 z & =-5  \tag{3}\\
2 y+z & =-4 \\
-3 y+2 z & =13 \\
7 z & =14
\end{align*}


\section*{Operation}
Replace $L_{2}$ by $-2 L_{1}+L_{2}$

Replace $L_{3}$ by $3 L_{1}+L_{3}$

Replace $L_{3}$ by $3 L_{2}+2 L_{3}$

That is, first we write down the number of each of the original equations. As we apply the Gaussian elimination algorithm to the system, we only write down the new equations, and we label each new equation using the same number as the original corresponding equation, but with an added prime. (After each new equation, we will indicate, for instructional purposes, the elementary operation that yielded the new equation.)

The system in triangular form consists of equations (1), $\left(2^{\prime}\right)$, and $\left(3^{\prime \prime}\right)$, the numbers with the largest number of primes. Applying back-substitution to these equations again yields $x=1, y=-3, z=2$.

Remark: If two equations need to be interchanged, say to obtain a nonzero coefficient as a pivot, then this is easily accomplished in the format by simply renumbering the two equations rather than changing their positions.

EXAMPLE 3.7 Solve the following system: $x+2 y-3 z=1$

$$
\begin{aligned}
& 2 x+5 y-8 z=4 \\
& 3 x+8 y-13 z=7
\end{aligned}
$$

We solve the system by Gaussian elimination.

Part $A$. (Forward Elimination) We use the coefficient 1 of $x$ in the first equation $L_{1}$ as the pivot in order to eliminate $x$ from the second equation $L_{2}$ and from the third equation $L_{3}$. This is accomplished as follows:

(1) Multiply $L_{1}$ by the multiplier $m=-2$ and add it to $L_{2}$; that is, "Replace $L_{2}$ by $-2 L_{1}+L_{2}$."

(2) Multiply $L_{1}$ by the multiplier $m=-3$ and add it to $L_{3}$; that is, "Replace $L_{3}$ by $-3 L_{1}+L_{3}$."

The two steps yield

$$
\begin{aligned}
& x+2 y-3 z=1 \\
& y-2 z=2 \\
& 2 y-4 z=4 \\
& x+2 y-3 z=1 \\
& y-2 z=2
\end{aligned}
$$

(The third equation is deleted, because it is a multiple of the second equation.) The system is now in echelon form with free variable $z$.

Part B. (Backward Elimination) To obtain the general solution, let the free variable $z=a$, and solve for $x$ and $y$ by back-substitution. Substitute $z=a$ in the second equation to obtain $y=2+2 a$. Then substitute $z=a$ and $y=2+2 a$ into the first equation to obtain

$$
x+2(2+2 a)-3 a=1 \quad \text { or } \quad x+4+4 a-3 a=1 \quad \text { or } \quad x=-3-a
$$

Thus, the following is the general solution where $a$ is a parameter:

$$
x=-3-a, \quad y=2+2 a, \quad z=a \quad \text { or } \quad u=(-3-a, 2+2 a, a)
$$

EXAMPLE 3.8 Solve the following system:

$$
\begin{array}{r}
x_{1}+3 x_{2}-2 x_{3}+5 x_{4}=4 \\
2 x_{1}+8 x_{2}-x_{3}+9 x_{4}=9 \\
3 x_{1}+5 x_{2}-12 x_{3}+17 x_{4}=7
\end{array}
$$

We use Gaussian elimination.

Part $A$. (Forward Elimination) We use the coefficient 1 of $x_{1}$ in the first equation $L_{1}$ as the pivot in order to eliminate $x_{1}$ from the second equation $L_{2}$ and from the third equation $L_{3}$. This is accomplished by the following operations:

(1) "Replace $L_{2}$ by $-2 L_{1}+L_{2}$ " and (2) "Replace $L_{3}$ by $-3 L_{1}+L_{3}$ "

These yield:

$$
\begin{aligned}
x_{1}+3 x_{2}-2 x_{3}+5 x_{4} & =4 \\
2 x_{2}+3 x_{3}-x_{4} & =1 \\
-4 x_{2}-6 x_{3}+2 x_{4} & =-5
\end{aligned}
$$

We now use the coefficient 2 of $x_{2}$ in the second equation $L_{2}$ as the pivot and the multiplier $m=2$ in order to eliminate $x_{2}$ from the third equation $L_{3}$. This is accomplished by the operation "Replace $L_{3}$ by $2 L_{2}+L_{3}$," which then yields the degenerate equation

$$
0 x_{1}+0 x_{2}+0 x_{3}+0 x_{4}=-3
$$

This equation and, hence, the original system have no solution:

\section*{DO NOT CONTINUE}
Remark 1: As in the above examples, Part A of Gaussian elimination tells us whether or not the system has a solution - that is, whether or not the system is consistent. Accordingly, Part B need never be applied when a system has no solution.

Remark 2: If a system of linear equations has more than four unknowns and four equations, then it may be more convenient to use the matrix format for solving the system. This matrix format is discussed later.

\subsection*{3.7 Echelon Matrices, Row Canonical Form, Row Equivalence}
One way to solve a system of linear equations is by working with its augmented matrix $M$ rather than the system itself. This section introduces the necessary matrix concepts for such a discussion. These concepts, such as echelon matrices and elementary row operations, are also of independent interest.

\section*{Echelon Matrices}
A matrix $A$ is called an echelon matrix, or is said to be in echelon form, if the following two conditions hold (where a leading nonzero element of a row of $A$ is the first nonzero element in the row):

(1) All zero rows, if any, are at the bottom of the matrix.

(2) Each leading nonzero entry in a row is to the right of the leading nonzero entry in the preceding row.

That is, $A=\left[a_{i j}\right]$ is an echelon matrix if there exist nonzero entries

$$
a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}}, \quad \text { where } \quad j_{1}<j_{2}<\cdots<j_{r}
$$

with the property that

$$
a_{i j}=0 \quad \text { for } \quad\left\{\begin{array}{l}
\text { (i) } i \leq r, \quad j<j_{i} \\
\text { (ii) } i>r
\end{array}\right.
$$

The entries $a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}}$, which are the leading nonzero elements in their respective rows, are called the pivots of the echelon matrix.

EXAMPLE 3.9 The following is an echelon matrix whose pivots have been circled:

$$
A=\left[\begin{array}{cccccccc}
0 & 2 & 3 & 4 & 5 & 9 & 0 & 7 \\
0 & 0 & 0 & 3 & 4 & 1 & 2 & 5 \\
0 & 0 & 0 & 0 & 0 & 5 & 7 & 2 \\
0 & 0 & 0 & 0 & 0 & 0 & 8 & 6 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Observe that the pivots are in columns $C_{2}, C_{4}, C_{6}, C_{7}$, and each is to the right of the one above. Using the above notation, the pivots are

$$
a_{1 j_{1}}=2, \quad a_{2 j_{2}}=3, \quad a_{3 j_{3}}=5, \quad a_{4 j_{4}}=8
$$

where $j_{1}=2, \quad j_{2}=4, \quad j_{3}=6, \quad j_{4}=7 . \quad$ Here $r=4$.

\section*{Row Canonical Form}
A matrix $A$ is said to be in row canonical form (or row-reduced echelon form) if it is an echelon matrixthat is, if it satisfies the above properties (1) and (2), and if it satisfies the following additional two properties:

(3) Each pivot (leading nonzero entry) is equal to 1.

(4) Each pivot is the only nonzero entry in its column.

The major difference between an echelon matrix and a matrix in row canonical form is that in an echelon matrix there must be zeros below the pivots [Properties (1) and (2)], but in a matrix in row canonical form, each pivot must also equal 1 [Property (3)] and there must also be zeros above the pivots [Property (4)].

The zero matrix 0 of any size and the identity matrix $I$ of any size are important special examples of matrices in row canonical form.

\section*{EXAMPLE 3.10}
The following are echelon matrices whose pivots have been circled:

$$
\left[\begin{array}{rrrrrrr}
(2) & 3 & 2 & 0 & 4 & 5 & -6 \\
0 & 0 & 0 & 1 & -3 & 2 & 0 \\
0 & 0 & 0 & 0 & 0 & 6 & 2 \\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{array}\right], \quad\left[\begin{array}{rrr}
1 & 2 & 3 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{array}\right], \quad\left[\begin{array}{rrrrrr}
0 & 1 & 3 & 0 & 0 & 4 \\
0 & 0 & 0 & 1 & 0 & -3 \\
0 & 0 & 0 & 0 & 1 & 2
\end{array}\right]
$$

The third matrix is also an example of a matrix in row canonical form. The second matrix is not in row canonical form, because it does not satisfy property (4); that is, there is a nonzero entry above the second pivot in the third column. The first matrix is not in row canonical form, because it satisfies neither property (3) nor property (4); that is, some pivots are not equal to 1 and there are nonzero entries above the pivots.

\section*{Elementary Row Operations}
Suppose $A$ is a matrix with rows $R_{1}, R_{2}, \ldots, R_{m}$. The following operations on $A$ are called elementary row operations.

$\left[\mathrm{E}_{1}\right]$ (Row Interchange): Interchange rows $R_{i}$ and $R_{j}$. This may be written as

$$
\text { "Interchange } R_{i} \text { and } R_{j} \text { " } \quad \text { or } \quad " R_{i} \longleftrightarrow R_{j} "
$$

$\left[\mathrm{E}_{2}\right]$ (Row Scaling): Replace row $R_{i}$ by a nonzero multiple $k R_{i}$ of itself. This may be written as

$$
\text { "Replace } R_{i} \text { by } k R_{i}(k \neq 0) " \quad \text { or } \quad " k R_{i} \rightarrow R_{i} "
$$

[E $\left.\mathrm{E}_{3}\right]$ (Row Addition): Replace row $R_{j}$ by the sum of a multiple $k R_{i}$ of a row $R_{i}$ and itself. This may be written as

$$
\text { "Replace } R_{j} \text { by } k R_{i}+R_{j} \text { " } \quad \text { or } \quad " k R_{i}+R_{j} \rightarrow R_{j} \text { " }
$$

The arrow $\rightarrow$ in $\mathrm{E}_{2}$ and $\mathrm{E}_{3}$ may be read as "replaces."

Sometimes (say to avoid fractions when all the given scalars are integers) we may apply $\left[\mathrm{E}_{2}\right]$ and $\left[\mathrm{E}_{3}\right]$ in one step; that is, we may apply the following operation:

[E] Replace $R_{j}$ by the sum of a multiple $k R_{i}$ of a row $R_{i}$ and a nonzero multiple $k^{\prime} R_{j}$ of itself. This may be written as

$$
\text { "Replace } R_{j} \text { by } k R_{i}+k^{\prime} R_{j}\left(k^{\prime} \neq 0\right) " \quad \text { or } \quad " k R_{i}+k^{\prime} R_{j} \rightarrow R_{j} \text { " }
$$

We emphasize that in operations $\left[\mathrm{E}_{3}\right]$ and $[\mathrm{E}]$ only row $R_{j}$ is changed.

\section*{Row Equivalence, Rank of a Matrix}
A matrix $A$ is said to be row equivalent to a matrix $B$, written

$$
A \sim B
$$

if $B$ can be obtained from $A$ by a sequence of elementary row operations. In the case that $B$ is also an echelon matrix, $B$ is called an echelon form of $A$.

The following are two basic results on row equivalence.

THEOREM 3.7: Suppose $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ are row equivalent echelon matrices with respective pivot entries

$$
a_{1 j_{1}}, a_{2 j_{2}}, \ldots a_{r j_{r}} \quad \text { and } \quad b_{1 k_{1}}, b_{2 k_{2}}, \ldots b_{s k_{s}}
$$

Then $A$ and $B$ have the same number of nonzero rows - that is, $r=s$ - and the pivot entries are in the same positions-that is, $j_{1}=k_{1}, j_{2}=k_{2}, \ldots, \quad j_{r}=k_{r}$.

THEOREM 3.8: Every matrix $A$ is row equivalent to a unique matrix in row canonical form.

The proofs of the above theorems will be postponed to Chapter 4. The unique matrix in Theorem 3.8 is called the row canonical form of $A$.

Using the above theorems, we can now give our first definition of the rank of a matrix.

DEFINITION: The rank of a matrix $A$, written $\operatorname{rank}(A)$, is equal to the number of pivots in an echelon form of $A$.

The rank is a very important property of a matrix and, depending on the context in which the matrix is used, it will be defined in many different ways. Of course, all the definitions lead to the same number.

The next section gives the matrix format of Gaussian elimination, which finds an echelon form of any matrix $A$ (and hence the rank of $A$ ), and also finds the row canonical form of $A$.

One can show that row equivalence is an equivalence relation. That is,

(1) $A \sim A$ for any matrix $A$.

(2) If $A \sim B$, then $B \sim A$.

(3) If $A \sim B$ and $B \sim C$, then $A \sim C$.

Property (2) comes from the fact that each elementary row operation has an inverse operation of the same type. Namely,

(i) "Interchange $R_{i}$ and $R_{j}$ " is its own inverse.

(ii) "Replace $R_{i}$ by $k R_{i}$ " and 'Replace $R_{i}$ by $(1 / k) R_{i}$ "' are inverses.

(iii) 'Replace $R_{j}$ by $k R_{i}+R_{j}$ " and "Replace $R_{j}$ by $-k R_{i}+R_{j}$ " are inverses.

There is a similar result for operation [E] (Problem 3.73).

\subsection*{3.8 Gaussian Elimination, Matrix Formulation}
This section gives two matrix algorithms that accomplish the following:

(1) Algorithm 3.3 transforms any matrix $A$ into an echelon form.

(2) Algorithm 3.4 transforms the echelon matrix into its row canonical form.

These algorithms, which use the elementary row operations, are simply restatements of Gaussian elimination as applied to matrices rather than to linear equations. (The term "row reduce" or simply "reduce" will mean to transform a matrix by the elementary row operations.)

ALGORITHM 3.3 (Forward Elimination): The input is any matrix $A$. (The algorithm puts 0's below each pivot, working from the 'top-down.') The output is an echelon form of $A$.

Step 1. Find the first column with a nonzero entry. Let $j_{1}$ denote this column.

(a) Arrange so that $a_{1 j_{1}} \neq 0$. That is, if necessary, interchange rows so that a nonzero entry appears in the first row in column $j_{1}$.

(b) Use $a_{1 j_{1}}$ as a pivot to obtain 0 's below $a_{1 j_{1}}$.

Specifically, for $i>1$ :\\
(1) Set $m=-a_{i j_{1}} / a_{1 j_{1}}$;\\
(2) Replace $R_{i}$ by $m R_{1}+R_{i}$

[That is, apply the operation $-\left(a_{i j_{1}} / a_{1 j_{1}}\right) R_{1}+R_{i} \rightarrow R_{i}$.]

Step 2. Repeat Step 1 with the submatrix formed by all the rows excluding the first row. Here we let $j_{2}$ denote the first column in the subsystem with a nonzero entry. Hence, at the end of Step 2, we have $a_{2 j_{2}} \neq 0$.

Steps 3 to $r$. Continue the above process until a submatrix has only zero rows.

We emphasize that at the end of the algorithm, the pivots will be

$$
a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}}
$$

where $r$ denotes the number of nonzero rows in the final echelon matrix.

Remark 1: The following number $m$ in Step 1(b) is called the multiplier:

$$
m=-\frac{a_{i j_{1}}}{a_{1 j_{1}}}=-\frac{\text { entry to be deleted }}{\text { pivot }}
$$

Remark 2: One could replace the operation in Step 1(b) by the following which would avoid fractions if all the scalars were originally integers.

Replace $R_{i}$ by $-a_{i j_{1}} R_{1}+a_{1 j_{1}} R_{i}$.

ALGORITHM 3.4 (Backward Elimination): The input is a matrix $A=\left[a_{i j}\right]$ in echelon form with pivot entries

$$
\begin{array}{llll}
a_{1 j_{1}} & & a_{2 j_{2}}, & \ldots, \quad a_{r j_{r}}
\end{array}
$$

The output is the row canonical form of $A$.

Step 1. (a) (Use row scaling so the last pivot equals 1.) Multiply the last nonzero row $R_{r}$ by $1 / a_{r j_{r}}$.

(b) (Use $a_{r j_{r}}=1$ to obtain 0 's above the pivot.) For $i=r-1, \quad r-2, \ldots, \quad 2, \quad 1$ :

$$
\text { (1) Set } m=-a_{i j} ; \quad \text { (2) Replace } R_{i} \text { by } m R_{r}+R_{i}
$$

(That is, apply the operations $-a_{i j_{r}} R_{r}+R_{i} \rightarrow R_{i}$.)

Steps 2 to $\boldsymbol{r}-1$. Repeat Step 1 for rows $R_{r-1}, R_{r-2}, \ldots, R_{2}$.

Step $\boldsymbol{r}$. (Use row scaling so the first pivot equals 1.) Multiply $R_{1}$ by $1 / a_{1 j_{1}}$.

There is an alternative form of Algorithm 3.4, which we describe here in words. The formal description of this algorithm is left to the reader as a supplementary problem.

ALTERNATIVE ALGORITHM 3.4 Puts 0's above the pivots row by row from the bottom up (rather than column by column from right to left).

The alternative algorithm, when applied to an augmented matrix $M$ of a system of linear equations, is essentially the same as solving for the pivot unknowns one after the other from the bottom up.

Remark: We emphasize that Gaussian elimination is a two-stage process. Specifically,

Stage A (Algorithm 3.3). Puts 0 's below each pivot, working from the top row $R_{1}$ down.

Stage B (Algorithm 3.4). Puts 0 's above each pivot, working from the bottom row $R_{r}$ up.

There is another algorithm, called Gauss-Jordan, that also row reduces a matrix to its row canonical form. The difference is that Gauss-Jordan puts 0 's both below and above each pivot as it works its way from the top row $R_{1}$ down. Although Gauss-Jordan may be easier to state and understand, it is much less efficient than the two-stage Gaussian elimination algorithm.

EXAMPLE 3.11 Consider the matrix $A=\left[\begin{array}{rrrrr}1 & 2 & -3 & 1 & 2 \\ 2 & 4 & -4 & 6 & 10 \\ 3 & 6 & -6 & 9 & 13\end{array}\right]$.

(a) Use Algorithm 3.3 to reduce $A$ to an echelon form.

(b) Use Algorithm 3.4 to further reduce $A$ to its row canonical form.

(a) First use $a_{11}=1$ as a pivot to obtain 0 's below $a_{11}$; that is, apply the operations "Replace $R_{2}$ by $-2 R_{1}+R_{2}$ " and "Replace $R_{3}$ by $-3 R_{1}+R_{3}$." Then use $a_{23}=2$ as a pivot to obtain 0 below $a_{23}$; that is, apply the operation "Replace $R_{3}$ by $-\frac{3}{2} R_{2}+R_{3}$." This yields

$$
A \sim\left[\begin{array}{rrrrr}
1 & 2 & -3 & 1 & 2 \\
0 & 0 & 2 & 4 & 6 \\
0 & 0 & 3 & 6 & 7
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & -3 & 1 & 2 \\
0 & 0 & 2 & 4 & 6 \\
0 & 0 & 0 & 0 & -2
\end{array}\right]
$$

The matrix is now in echelon form.\\
(b) Multiply $R_{3}$ by $-\frac{1}{2}$ so the pivot entry $a_{35}=1$, and then use $a_{35}=1$ as a pivot to obtain 0 's above it by the operations "Replace $R_{2}$ by $-6 R_{3}+R_{2}$ " and then "Replace $R_{1}$ by $-2 R_{3}+R_{1}$." This yields

$$
A \sim\left[\begin{array}{rrrrr}
1 & 2 & -3 & 1 & 2 \\
0 & 0 & 2 & 4 & 6 \\
0 & 0 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & -3 & 1 & 0 \\
0 & 0 & 2 & 4 & 0 \\
0 & 0 & 0 & 0 & 1
\end{array}\right] .
$$

Multiply $R_{2}$ by $\frac{1}{2}$ so the pivot entry $a_{23}=1$, and then use $a_{23}=1$ as a pivot to obtain 0 's above it by the operation "Replace $R_{1}$ by $3 R_{2}+R_{1}$." This yields

$$
A \sim\left[\begin{array}{rrrrr}
1 & 2 & -3 & 1 & 0 \\
0 & 0 & 1 & 2 & 0 \\
0 & 0 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lllll}
1 & 2 & 0 & 7 & 0 \\
0 & 0 & 1 & 2 & 0 \\
0 & 0 & 0 & 0 & 1
\end{array}\right]
$$

The last matrix is the row canonical form of $A$.

\section*{Application to Systems of Linear Equations}
One way to solve a system of linear equations is by working with its augmented matrix $M$ rather than the equations themselves. Specifically, we reduce $M$ to echelon form (which tells us whether the system has a solution), and then further reduce $M$ to its row canonical form (which essentially gives the solution of the original system of linear equations). The justification for this process comes from the following facts:

(1) Any elementary row operation on the augmented matrix $M$ of the system is equivalent to applying the corresponding operation on the system itself.

(2) The system has a solution if and only if the echelon form of the augmented matrix $M$ does not have a row of the form $(0,0, \ldots, 0, b)$ with $b \neq 0$.

(3) In the row canonical form of the augmented matrix $M$ (excluding zero rows), the coefficient of each basic variable is a pivot entry equal to 1 , and it is the only nonzero entry in its respective column; hence, the free-variable form of the solution of the system of linear equations is obtained by simply transferring the free variables to the other side.

This process is illustrated below.

EXAMPLE 3.12 Solve each of the following systems:

$$
\begin{aligned}
& x_{1}+x_{2}-2 x_{3}+4 x_{4}=5 \\
& x_{1}+x_{2}-2 x_{3}+3 x_{4}=4 \\
& x+2 y+z=3 \\
& 2 x_{1}+2 x_{2}-3 x_{3}+x_{4}=3 \\
& 2 x_{1}+3 x_{2}+3 x_{3}-x_{4}=3 \\
& 2 x+5 y-z=-4 \\
& 3 x_{1}+3 x_{2}-4 x_{3}-2 x_{4}=1 \\
& 5 x_{1}+7 x_{2}+4 x_{3}+x_{4}=5 \\
& 3 x-2 y-z=5
\end{aligned}
$$

(c)

(a) Reduce its augmented matrix $M$ to echelon form and then to row canonical form as follows:

$$
M=\left[\begin{array}{rrrrr}
1 & 1 & -2 & 4 & 5 \\
2 & 2 & -3 & 1 & 3 \\
3 & 3 & -4 & -2 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 1 & -2 & 4 & 5 \\
0 & 0 & 1 & -7 & -7 \\
0 & 0 & 2 & -14 & -14
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 1 & 0 & -10 & -9 \\
0 & 0 & 1 & -7 & -7 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Rewrite the row canonical form in terms of a system of linear equations to obtain the free variable form of the solution. That is,

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-082}
\end{center}

(The zero row is omitted in the solution.) Observe that $x_{1}$ and $x_{3}$ are the pivot variables, and $x_{2}$ and $x_{4}$ are the free variables.\\
(b) First reduce its augmented matrix $M$ to echelon form as follows:

$$
M=\left[\begin{array}{rrrrr}
1 & 1 & -2 & 3 & 4 \\
2 & 3 & 3 & -1 & 3 \\
5 & 7 & 4 & 1 & 5
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 1 & -2 & 3 & 4 \\
0 & 1 & 7 & -7 & -5 \\
0 & 2 & 14 & -14 & -15
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 1 & -2 & 3 & 4 \\
0 & 1 & 7 & -7 & -5 \\
0 & 0 & 0 & 0 & -5
\end{array}\right]
$$

There is no need to continue to find the row canonical form of $M$, because the echelon form already tells us that the system has no solution. Specifically, the third row of the echelon matrix corresponds to the degenerate equation

$$
0 x_{1}+0 x_{2}+0 x_{3}+0 x_{4}=-5
$$

which has no solution. Thus, the system has no solution.

(c) Reduce its augmented matrix $M$ to echelon form and then to row canonical form as follows:

$$
\begin{aligned}
& M=\left[\begin{array}{rrrr}
1 & 2 & 1 & 3 \\
2 & 5 & -1 & -4 \\
3 & -2 & -1 & 5
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10 \\
0 & -8 & -4 & -4
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10 \\
0 & 0 & -28 & -84
\end{array}\right] \\
& \sim\left[\begin{array}{rrrr}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10 \\
0 & 0 & 1 & 3
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & 0 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & 3
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 0 & 0 & 2 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & 3
\end{array}\right]
\end{aligned}
$$

Thus, the system has the unique solution $x=2, y=-1, z=3$, or, equivalently, the vector $u=(2,-1,3)$. We note that the echelon form of $M$ already indicated that the solution was unique, because it corresponded to a triangular system.

\section*{Application to Existence and Uniqueness Theorems}
This subsection gives theoretical conditions for the existence and uniqueness of a solution of a system of linear equations using the notion of the rank of a matrix.

THEOREM 3.9: Consider a system of linear equations in $n$ unknowns with augmented matrix $M=[A, B]$. Then,

(a) The system has a solution if and only if $\operatorname{rank}(A)=\operatorname{rank}(M)$.

(b) The solution is unique if and only if $\operatorname{rank}(A)=\operatorname{rank}(M)=n$.

Proof of (a). The system has a solution if and only if an echelon form of $M=[A, B]$ does not have a row of the form

$$
(0,0, \ldots, 0, b), \quad \text { with } \quad b \neq 0
$$

If an echelon form of $M$ does have such a row, then $b$ is a pivot of $M$ but not of $A$, and hence, $\operatorname{rank}(M)>\operatorname{rank}(A)$. Otherwise, the echelon forms of $A$ and $M$ have the same pivots, and hence, $\operatorname{rank}(A)=\operatorname{rank}(M)$. This proves (a).

Proof of (b). The system has a unique solution if and only if an echelon form has no free variable. This means there is a pivot for each unknown. Accordingly, $n=\operatorname{rank}(A)=\operatorname{rank}(M)$. This proves (b).

The above proof uses the fact (Problem 3.74) that an echelon form of the augmented matrix $M=[A, B]$ also automatically yields an echelon form of $A$.

\subsection*{3.9 Matrix Equation of a System of Linear Equations}
The general system (3.2) of $m$ linear equations in $n$ unknowns is equivalent to the matrix equation

$$
\left[\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n}
\end{array}\right]\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{3} \\
\ldots \\
x_{n}
\end{array}\right]=\left[\begin{array}{c}
b_{1} \\
b_{2} \\
\ldots \\
b_{m}
\end{array}\right] \quad \text { or } \quad A X=B
$$

where $A=\left[a_{i j}\right]$ is the coefficient matrix, $X=\left[x_{j}\right]$ is the column vector of unknowns, and $B=\left[b_{i}\right]$ is the column vector of constants. (Some texts write $A x=b$ rather than $A X=B$, in order to emphasize that $x$ and $b$ are simply column vectors.)

The statement that the system of linear equations and the matrix equation are equivalent means that any vector solution of the system is a solution of the matrix equation, and vice versa.

EXAMPLE 3.13 The following system of linear equations and matrix equation are equivalent:

$$
\begin{array}{r}
x_{1}+2 x_{2}-4 x_{3}+7 x_{4}=4 \\
3 x_{1}-5 x_{2}+6 x_{3}-8 x_{4}=8 \\
4 x_{1}-3 x_{2}-2 x_{3}+6 x_{4}=11
\end{array} \quad \text { and } \quad\left[\begin{array}{rrrr}
1 & 2 & -4 & 7 \\
3 & -5 & 6 & -8 \\
4 & -3 & -2 & 6
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{array}\right]=\left[\begin{array}{r}
4 \\
8 \\
11
\end{array}\right]
$$

We note that $x_{1}=3, \quad x_{2}=1, \quad x_{3}=2, \quad x_{4}=1, \quad$ or, in other words, the vector $u=[3,1,2,1]$ is a solution of the system. Thus, the (column) vector $u$ is also a solution of the matrix equation.

The matrix form $A X=B$ of a system of linear equations is notationally very convenient when discussing and proving properties of systems of linear equations. This is illustrated with our first theorem (described in Fig. 3-1), which we restate for easy reference.

THEOREM 3.1: Suppose the field $K$ is infinite. Then the system $A X=B$ has: (a) a unique solution, (b) no solution, or (c) an infinite number of solutions.

Proof. It suffices to show that if $A X=B$ has more than one solution, then it has infinitely many. Suppose $u$ and $v$ are distinct solutions of $A X=B$; that is, $A u=B$ and $A v=B$. Then, for any $k \in K$,

$$
A[u+k(u-v)]=A u+k(A u-A v)=B+k(B-B)=B
$$

Thus, for each $k \in K$, the vector $u+k(u-v)$ is a solution of $A X=B$. Because all such solutions are distinct (Problem 3.47), $A X=B$ has an infinite number of solutions.

Observe that the above theorem is true when $K$ is the real field $\mathbf{R}$ (or the complex field $\mathbf{C}$ ). Section 3.3 shows that the theorem has a geometrical description when the system consists of two equations in two unknowns, where each equation represents a line in $\mathbf{R}^{2}$. The theorem also has a geometrical description when the system consists of three nondegenerate equations in three unknowns, where the three equations correspond to planes $H_{1}, H_{2}, H_{3}$ in $\mathbf{R}^{3}$. That is,

(a) Unique solution: Here the three planes intersect in exactly one point.

(b) No solution: Here the planes may intersect pairwise but with no common point of intersection, or two of the planes may be parallel.

(c) Infinite number of solutions: Here the three planes may intersect in a line (one free variable), or they may coincide (two free variables).

These three cases are pictured in Fig. 3-3.

\section*{Matrix Equation of a Square System of Linear Equations}
A system $A X=B$ of linear equations is square if and only if the matrix $A$ of coefficients is square. In such a case, we have the following important result.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(7)}
\end{center}

(a) Unique solution

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(2)}
\end{center}

(i)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(5)}
\end{center}

(ii)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085}
\end{center}

(iii)

(c) Infinite number of solutions

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(6)}
\end{center}

(i)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(4)}
\end{center}

(ii)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(1)}
\end{center}

(iii)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-085(3)}
\end{center}

(iv)

(b) No solutions

Figure 3-3

THEOREM 3.10: A square system $A X=B$ of linear equations has a unique solution if and only if the matrix $A$ is invertible. In such a case, $A^{-1} B$ is the unique solution of the system.

We only prove here that if $A$ is invertible, then $A^{-1} B$ is a unique solution. If $A$ is invertible, then

$$
A\left(A^{-1} B\right)=\left(A A^{-1}\right) B=I B=B
$$

and hence, $A^{-1} B$ is a solution. Now suppose $v$ is any solution, so $A v=B$. Then

$$
v=I v=\left(A^{-1} A\right) v=A^{-1}(A v)=A^{-1} B
$$

Thus, the solution $A^{-1} B$ is unique.

EXAMPLE 3.14 Consider the following system of linear equations, whose coefficient matrix $A$ and inverse $A^{-1}$ are also given:

$$
\begin{array}{r}
x+2 y+3 z=1 \\
x+3 y+6 z=3, \\
2 x+6 y+13 z=5
\end{array} \quad A=\left[\begin{array}{rrr}
1 & 2 & 3 \\
1 & 3 & 6 \\
2 & 6 & 13
\end{array}\right], \quad A^{-1}=\left[\begin{array}{rrr}
3 & -8 & 3 \\
-1 & 7 & -3 \\
0 & -2 & 1
\end{array}\right]
$$

By Theorem 3.10, the unique solution of the system is

$$
A^{-1} B=\left[\begin{array}{rrr}
3 & -8 & 3 \\
-1 & 7 & -3 \\
0 & -2 & 1
\end{array}\right]\left[\begin{array}{l}
1 \\
3 \\
5
\end{array}\right]=\left[\begin{array}{r}
-6 \\
5 \\
-1
\end{array}\right]
$$

That is, $x=-6, y=5, z=-1$.

Remark: We emphasize that Theorem 3.10 does not usually help us to find the solution of a square system. That is, finding the inverse of a coefficient matrix $A$ is not usually any easier than solving the system directly. Thus, unless we are given the inverse of a coefficient matrix $A$, as in Example 3.14, we usually solve a square system by Gaussian elimination (or some iterative method whose discussion lies beyond the scope of this text).

\subsection*{3.10 Systems of Linear Equations and Linear Combinations of Vectors}
The general system (3.2) of linear equations may be rewritten as the following vector equation:

$$
x_{1}\left[\begin{array}{c}
a_{11} \\
a_{21} \\
\cdots \\
a_{m 1}
\end{array}\right]+x_{2}\left[\begin{array}{c}
a_{12} \\
a_{22} \\
\cdots \\
a_{m 2}
\end{array}\right]+\cdots+x_{n}\left[\begin{array}{c}
a_{1 n} \\
a_{2 n} \\
\cdots \\
a_{m n}
\end{array}\right]=\left[\begin{array}{c}
b_{1} \\
b_{2} \\
\cdots \\
b_{m}
\end{array}\right]
$$

Recall that a vector $v$ in $K^{n}$ is said to be a linear combination of vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $K^{n}$ if there exist scalars $a_{1}, a_{2}, \ldots, a_{m}$ in $K$ such that

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{m} u_{m}
$$

Accordingly, the general system (3.2) of linear equations and the above equivalent vector equation have a solution if and only if the column vector of constants is a linear combination of the columns of the coefficient matrix. We state this observation formally.

THEOREM 3.11: A system $A X=B$ of linear equations has a solution if and only if $B$ is a linear combination of the columns of the coefficient matrix $A$.

Thus, the answer to the problem of expressing a given vector $v$ in $K^{n}$ as a linear combination of vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $K^{n}$ reduces to solving a system of linear equations.

\section*{Linear Combination Example}
Suppose we want to write the vector $v=(1,-2,5)$ as a linear combination of the vectors

$$
u_{1}=(1,1,1), \quad u_{2}=(1,2,3), \quad u_{3}=(2,-1,1)
$$

First we write $v=x u_{1}+y u_{2}+z u_{3}$ with unknowns $x, y, z$, and then we find the equivalent system of linear equations which we solve. Specifically, we first write

\[
\left[\begin{array}{r}
1  \tag{*}\\
-2 \\
5
\end{array}\right]=x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+z\left[\begin{array}{r}
2 \\
-1 \\
1
\end{array}\right]
\]

Then

$$
\left[\begin{array}{r}
1 \\
-2 \\
5
\end{array}\right]=\left[\begin{array}{l}
x \\
x \\
x
\end{array}\right]+\left[\begin{array}{r}
y \\
2 y \\
3 y
\end{array}\right]+\left[\begin{array}{r}
2 z \\
-z \\
z
\end{array}\right]=\left[\begin{array}{l}
x+y+2 z \\
x+2 y-z \\
x+3 y+z
\end{array}\right]
$$

Setting corresponding entries equal to each other yields the following equivalent system:


\begin{align*}
& x+y+2 z=1 \\
& x+2 y-z=-2  \tag{**}\\
& x+3 y+z=5
\end{align*}


For notational convenience, we have written the vectors in $\mathbf{R}^{n}$ as columns, because it is then easier to find the equivalent system of linear equations. In fact, one can easily go from the vector equation $\left({ }^{*}\right)$ directly to the system $(* *)$.

Now we solve the equivalent system of linear equations by reducing the system to echelon form. This yields

$$
\begin{aligned}
& x+y+2 z=1 \quad x+y+2 z=1 \\
& y-3 z=-3 \quad \text { and then } \quad y-3 z=-3 \\
& 2 y-z=4 \quad 5 z=10
\end{aligned}
$$

Back-substitution yields the solution $x=-6, y=3, z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.

\section*{EXAMPLE 3.15}
(a) Write the vector $v=(4,9,19)$ as a linear combination of

$$
u_{1}=(1,-2,3), \quad u_{2}=(3,-7,10), \quad u_{3}=(2,1,9) \text {. }
$$

Find the equivalent system of linear equations by writing $v=x u_{1}+y u_{2}+z u_{3}$, and reduce the system to an echelon form. We have

$$
\begin{array}{rlrlrl}
x+3 y+2 z & =4 & & x+3 y+2 z & =4 & \\
-2 x-7 y+z & =9 \\
3 x+10 y+9 z & =19 & \text { or } & -y+5 z & =17 \\
y+3 z & =7 & \text { or } & -y+2 z & =4 \\
& & y+3 z & =17 \\
& & 8 z & =24
\end{array}
$$

Back-substitution yields the solution $x=4, y=-2, z=3$. Thus, $v$ is a linear combination of $u_{1}, u_{2}, u_{3}$. Specifically, $v=4 u_{1}-2 u_{2}+3 u_{3}$.

(b) Write the vector $v=(2,3,-5)$ as a linear combination of

$$
u_{1}=(1,2,-3), \quad u_{2}=(2,3,-4), \quad u_{3}=(1,3,-5)
$$

Find the equivalent system of linear equations by writing $v=x u_{1}+y u_{2}+z u_{3}$, and reduce the system to an echelon form. We have

$$
\begin{aligned}
& x+2 y+z=2 \quad x+2 y+z=2 \quad x+2 y+z=2 \\
& 2 x+3 y+3 z=3 \quad \text { or } \quad-y+z=-1 \quad \text { or } \quad-5 y+5 z=-1 \\
& -3 x-4 y-5 z=-5 \quad 2 y-2 z=1 \quad 0=3
\end{aligned}
$$

The system has no solution. Thus, it is impossible to write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$.

\section*{Linear Combinations of Orthogonal Vectors, Fourier Coefficients}
Recall first (Section 1.4) that the dot (inner) product $u \cdot v$ of vectors $u=\left(a_{1}, \ldots, a_{n}\right)$ and $v=\left(b_{1}, \ldots, b_{n}\right)$ in $\mathbf{R}^{n}$ is defined by

$$
u \cdot v=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}
$$

Furthermore, vectors $u$ and $v$ are said to be orthogonal if their dot product $u \cdot v=0$.

Suppose that $u_{1}, u_{2}, \ldots, u_{n}$ in $\mathbf{R}^{n}$ are $n$ nonzero pairwise orthogonal vectors. This means\\
(i) $u_{i} \cdot u_{j}=0$ for $i \neq j \quad$ and\\
(ii) $u_{i} \cdot u_{i} \neq 0$ for each $i$

Then, for any vector $v$ in $\mathbf{R}^{n}$, there is an easy way to write $v$ as a linear combination of $u_{1}, u_{2}, \ldots, u_{n}$, which is illustrated in the next example.

EXAMPLE 3.16 Consider the following three vectors in $\mathbf{R}^{3}$ :

$$
u_{1}=(1,1,1), \quad u_{2}=(1,-3,2), \quad u_{3}=(5,-1,-4)
$$

These vectors are pairwise orthogonal; that is,

$$
u_{1} \cdot u_{2}=1-3+2=0, \quad u_{1} \cdot u_{3}=5-1-4=0, \quad u_{2} \cdot u_{3}=5+3-8=0
$$

Suppose we want to write $v=(4,14,-9)$ as a linear combination of $u_{1}, u_{2}, u_{3}$.

Method 1. Find the equivalent system of linear equations as in Example 3.14 and then solve, obtaining $v=3 u_{1}-4 u_{2}+u_{3}$.

Method 2. (This method uses the fact that the vectors $u_{1}, u_{2}, u_{3}$ are mutually orthogonal, and hence, the arithmetic is much simpler.) Set $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknown scalars $x, y, z$ as follows:


\begin{equation*}
(4,14,-9)=x(1,1,1)+y(1,-3,2)+z(5,-1,-4) \tag{*}
\end{equation*}


Take the dot product of $(*)$ with respect to $u_{1}$ to get

$$
(4,14,-9) \cdot(1,1,1)=x(1,1,1) \cdot(1,1,1) \quad \text { or } \quad 9=3 x \quad \text { or } \quad x=3
$$

(The last two terms drop out, because $u_{1}$ is orthogonal to $u_{2}$ and to $u_{3}$.) Next take the dot product of $\left(^{*}\right)$ with respect to $u_{2}$ to obtain

$$
(4,14,-9) \cdot(1,-3,2)=y(1,-3,2) \cdot(1,-3,2) \quad \text { or } \quad-56=14 y \quad \text { or } \quad y=-4
$$

Finally, take the dot product of $(*)$ with respect to $u_{3}$ to get

$$
(4,14,-9) \cdot(5,-1,-4)=z(5,-1,-4) \cdot(5,-1,-4) \quad \text { or } \quad 42=42 z \quad \text { or } \quad z=1
$$

Thus, $v=3 u_{1}-4 u_{2}+u_{3}$.

The procedure in Method 2 in Example 3.16 is valid in general. Namely,

THEOREM 3.12: Suppose $u_{1}, u_{2}, \ldots, u_{n}$ are nonzero mutually orthogonal vectors in $\mathbf{R}^{n}$. Then, for any vector $v$ in $\mathbf{R}^{n}$,

$$
v=\frac{v \cdot u_{1}}{u_{1} \cdot u_{1}} u_{1}+\frac{v \cdot u_{2}}{u_{2} \cdot u_{2}} u_{2}+\cdots+\frac{v \cdot u_{n}}{u_{n} \cdot u_{n}} u_{n}
$$

We emphasize that there must be $n$ such orthogonal vectors $u_{i}$ in $\mathbf{R}^{n}$ for the formula to be used. Note also that each $u_{i} \cdot u_{i} \neq 0$, because each $u_{i}$ is a nonzero vector.

Remark: The following scalar $k_{i}$ (appearing in Theorem 3.12) is called the Fourier coefficient of $v$ with respect to $u_{i}$ :

$$
k_{i}=\frac{v \cdot u_{i}}{u_{i} \cdot u_{i}}=\frac{v \cdot u_{i}}{\left\|u_{i}\right\|^{2}}
$$

It is analogous to a coefficient in the celebrated Fourier series of a function.

\subsection*{3.11 Homogeneous Systems of Linear Equations}
A system of linear equations is said to be homogeneous if all the constant terms are zero. Thus, a homogeneous system has the form $A X=0$. Clearly, such a system always has the zero vector $0=(0,0, \ldots, 0)$ as a solution, called the zero or trivial solution. Accordingly, we are usually interested in whether or not the system has a nonzero solution.

Because a homogeneous system $A X=0$ has at least the zero solution, it can always be put in an echelon form, say

$$
\begin{array}{r}
a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}+a_{14} x_{4}+\cdots+a_{1 n} x_{n}=0 \\
a_{2 j_{2}} x_{j_{2}}+a_{2, j_{2}+1} x_{j_{2}+1}+\cdots+a_{2 n} x_{n}=0 \\
\cdots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{r j_{r}} x_{j_{r}}+\cdots+a_{r n} x_{n}=0
\end{array}
$$

Here $r$ denotes the number of equations in echelon form and $n$ denotes the number of unknowns. Thus, the echelon system has $n-r$ free variables.

The question of nonzero solutions reduces to the following two cases:

(i) $r=n$. The system has only the zero solution.

(ii) $r<n$. The system has a nonzero solution.

Accordingly, if we begin with fewer equations than unknowns, then, in echelon form, $r<n$, and the system has a nonzero solution. This proves the following important result.

THEOREM 3.13: A homogeneous system $A X=0$ with more unknowns than equations has a nonzero solution.

EXAMPLE 3.17 Determine whether or not each of the following homogeneous systems has a nonzero solution:

$$
\begin{aligned}
& x+y-z=0 \quad x+y-z=0 \quad x_{1}+2 x_{2}-3 x_{3}+4 x_{4}=0 \\
& 2 x-3 y+z=0 \quad 2 x+4 y-z=0 \quad 2 x_{1}-3 x_{2}+5 x_{3}-7 x_{4}=0 \\
& x-4 y+2 z=0 \quad 3 x+2 y+2 z=0 \quad 5 x_{1}+6 x_{2}-9 x_{3}+8 x_{4}=0
\end{aligned}
$$

(c)

(a) Reduce the system to echelon form as follows:

$$
\begin{aligned}
& x+y-z=0 \\
& -5 y+3 z=0 \quad \text { and then } \quad x+y-z=0 \\
& -5 y+3 z=0 \quad-5 y+3 z=0
\end{aligned}
$$

The system has a nonzero solution, because there are only two equations in the three unknowns in echelon form. Here $z$ is a free variable. Let us, say, set $z=5$. Then, by back-substitution, $y=3$ and $x=2$. Thus, the vector $u=(2,3,5)$ is a particular nonzero solution.

(b) Reduce the system to echelon form as follows:

$$
\begin{aligned}
& x+y-z=0 \quad x+y-z=0 \\
& 2 y+z=0 \quad \text { and then } \quad 2 y+z=0 \\
& -y+5 z=0 \quad 11 z=0
\end{aligned}
$$

In echelon form, there are three equations in three unknowns. Thus, the system has only the zero solution.

(c) The system must have a nonzero solution (Theorem 3.13), because there are four unknowns but only three equations. (Here we do not need to reduce the system to echelon form.)

\section*{Basis for the General Solution of a Homogeneous System}
Let $W$ denote the general solution of a homogeneous system $A X=0$. A list of nonzero solution vectors $u_{1}, u_{2}, \ldots, u_{s}$ of the system is said to be a basis for $W$ if each solution vector $w \in W$ can be expressed uniquely as a linear combination of the vectors $u_{1}, u_{2}, \ldots, u_{s}$; that is, there exist unique scalars $a_{1}, a_{2}, \ldots, a_{s}$ such that

$$
w=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{s} u_{s}
$$

The number $s$ of such basis vectors is equal to the number of free variables. This number $s$ is called the dimension of $W$, written as $\operatorname{dim} W=s$. When $W=\{0\}$ - that is, the system has only the zero solutionwe define $\operatorname{dim} W=0$.

The following theorem, proved in Chapter 5, page 171, tells us how to find such a basis.

THEOREM 3.14: Let $W$ be the general solution of a homogeneous system $A X=0$, and suppose that the echelon form of the homogeneous system has $s$ free variables. Let $u_{1}, u_{2}, \ldots, u_{s}$ be the solutions obtained by setting one of the free variables equal to 1 (or any nonzero constant) and the remaining free variables equal to 0 . Then $\operatorname{dim} W=s$, and the vectors $u_{1}, u_{2}, \ldots, u_{s}$ form a basis of $W$.

We emphasize that the general solution $W$ may have many bases, and that Theorem 3.12 only gives us one such basis.

EXAMPLE 3.18 Find the dimension and a basis for the general solution $W$ of the homogeneous system

$$
\begin{array}{r}
x_{1}+2 x_{2}-3 x_{3}+2 x_{4}-4 x_{5}=0 \\
2 x_{1}+4 x_{2}-5 x_{3}+x_{4}-6 x_{5}=0 \\
5 x_{1}+10 x_{2}-13 x_{3}+4 x_{4}-16 x_{5}=0
\end{array}
$$

First reduce the system to echelon form. Apply the following operations:

"Replace $L_{2}$ by $-2 L_{1}+L_{2}$ " and "Replace $L_{3}$ by $-5 L_{1}+L_{3}$ " and then "Replace $L_{3}$ by $-2 L_{2}+L_{3}$ "

These operations yield

$$
\begin{aligned}
& x_{1}+2 x_{2}-3 x_{3}+2 x_{4}-4 x_{5}=0
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-090}
\end{center}

$$
\begin{aligned}
& 2 x_{3}-6 x_{4}+4 x_{5}=0
\end{aligned}
$$

The system in echelon form has three free variables, $x_{2}, x_{4}, x_{5}$; hence, $\operatorname{dim} W=3$. Three solution vectors that form a basis for $W$ are obtained as follows:

(1) Set $x_{2}=1, x_{4}=0, x_{5}=0$. Back-substitution yields the solution $u_{1}=(-2,1,0,0,0)$.

(2) Set $x_{2}=0, x_{4}=1, x_{5}=0$. Back-substitution yields the solution $u_{2}=(7,0,3,1,0)$.

(3) Set $x_{2}=0, x_{4}=0, x_{5}=1$. Back-substitution yields the solution $u_{3}=(-2,0,-2,0,1)$.

The vectors $u_{1}=(-2,1,0,0,0), \quad u_{2}=(7,0,3,1,0), \quad u_{3}=(-2,0,-2,0,1)$ form a basis for $W$.

Remark: Any solution of the system in Example 3.18 can be written in the form

$$
\begin{aligned}
a u_{1}+b u_{2}+c u_{3} & =a(-2,1,0,0,0)+b(7,0,3,1,0)+c(-2,0,-2,0,1) \\
& =(-2 a+7 b-2 c, \quad a, \quad 3 b-2 c, \quad b, \quad c)
\end{aligned}
$$

or

$$
x_{1}=-2 a+7 b-2 c, \quad x_{2}=a, \quad x_{3}=3 b-2 c, \quad x_{4}=b, \quad x_{5}=c
$$

where $a, b, c$ are arbitrary constants. Observe that this representation is nothing more than the parametric form of the general solution under the choice of parameters $x_{2}=a, x_{4}=b, x_{5}=c$.

\section*{Nonhomogeneous and Associated Homogeneous Systems}
Let $A X=B$ be a nonhomogeneous system of linear equations. Then $A X=0$ is called the associated homogeneous system. For example,

$$
\begin{aligned}
x+2 y-4 z & =7 \\
3 x-5 y+6 z & =8
\end{aligned} \quad \text { and } \quad \begin{aligned}
x+2 y-4 z & =0 \\
3 x-5 y+6 z & =0
\end{aligned}
$$

show a nonhomogeneous system and its associated homogeneous system.

The relationship between the solution $U$ of a nonhomogeneous system $A X=B$ and the solution $W$ of its associated homogeneous system $A X=0$ is contained in the following theorem.

THEOREM 3.15: Let $v_{0}$ be a particular solution of $A X=B$ and let $W$ be the general solution of $A X=0$. Then the following is the general solution of $A X=B$ :

$$
U=v_{0}+W=\left\{v_{0}+w: w \in W\right\}
$$

That is, $U=v_{0}+W$ is obtained by adding $v_{0}$ to each element in $W$. We note that this theorem has a geometrical interpretation in $\mathbf{R}^{3}$. Specifically, suppose $W$ is a line through the origin $O$. Then, as pictured in Fig. 3-4, $U=v_{0}+W$ is the line parallel to $W$ obtained by adding $v_{0}$ to each element of $W$. Similarly, whenever $W$ is a plane through the origin $O$, then $U=v_{0}+W$ is a plane parallel to $W$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-091}
\end{center}

Figure 3-4

\subsection*{3.12 Elementary Matrices}
Let $e$ denote an elementary row operation and let $e(A)$ denote the results of applying the operation $e$ to a matrix $A$. Now let $E$ be the matrix obtained by applying $e$ to the identity matrix $I$; that is,

$$
E=e(I)
$$

Then $E$ is called the elementary matrix corresponding to the elementary row operation $e$. Note that $E$ is always a square matrix.

EXAMPLE 3.19 Consider the following three elementary row operations:\\
(1) Interchange $R_{2}$ and $R_{3}$.\\
(2) Replace $R_{2}$ by $-6 R_{2}$.\\
(3) Replace $R_{3}$ by $-4 R_{1}+R_{3}$.

The $3 \times 3$ elementary matrices corresponding to the above elementary row operations are as follows:

$$
E_{1}=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0
\end{array}\right], \quad E_{2}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & -6 & 0 \\
0 & 0 & 1
\end{array}\right], \quad E_{3}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
-4 & 0 & 1
\end{array}\right]
$$

The following theorem, proved in Problem 3.34, holds.

THEOREM 3.16: Let $e$ be an elementary row operation and let $E$ be the corresponding $m \times m$ elementary matrix. Then

$$
e(A)=E A
$$

where $A$ is any $m \times n$ matrix.

In other words, the result of applying an elementary row operation $e$ to a matrix $A$ can be obtained by premultiplying $A$ by the corresponding elementary matrix $E$.

Now suppose $e^{\prime}$ is the inverse of an elementary row operation $e$, and let $E^{\prime}$ and $E$ be the corresponding matrices. We note (Problem 3.33) that $E$ is invertible and $E^{\prime}$ is its inverse. This means, in particular, that any product

$$
P=E_{k} \ldots E_{2} E_{1}
$$

of elementary matrices is invertible.

\section*{Applications of Elementary Matrices}
Using Theorem 3.16, we are able to prove (Problem 3.35) the following important properties of matrices.

THEOREM 3.17: Let $A$ be a square matrix. Then the following are equivalent:

(a) $A$ is invertible (nonsingular).

(b) $A$ is row equivalent to the identity matrix $I$.

(c) $A$ is a product of elementary matrices.

Recall that square matrices $A$ and $B$ are inverses if $A B=B A=I$. The next theorem (proved in Problem 3.36) demonstrates that we need only show that one of the products is true, say $A B=I$, to prove that matrices are inverses.

THEOREM 3.18: Suppose $A B=I$. Then $B A=I$, and hence, $B=A^{-1}$.

Row equivalence can also be defined in terms of matrix multiplication. Specifically, we will prove (Problem 3.37) the following.

THEOREM 3.19: $B$ is row equivalent to $A$ if and only if there exists a nonsingular matrix $P$ such that $B=P A$.

\section*{Application to Finding the Inverse of an $\boldsymbol{n} \times \boldsymbol{n}$ Matrix}
The following algorithm finds the inverse of a matrix.

ALGORITHM 3.5: The input is a square matrix $A$. The output is the inverse of $A$ or that the inverse does not exist.

Step 1. Form the $n \times 2 n$ (block) matrix $M=[A, I]$, where $A$ is the left half of $M$ and the identity matrix $I$ is the right half of $M$.

Step 2. Row reduce $M$ to echelon form. If the process generates a zero row in the $A$ half of $M$, then

\section*{STOP}
$A$ has no inverse. (Otherwise $A$ is in triangular form.)

Step 3. Further row reduce $M$ to its row canonical form

$$
M \sim[I, B]
$$

where the identity matrix $I$ has replaced $A$ in the left half of $M$.

Step 4. Set $A^{-1}=B$, the matrix that is now in the right half of $M$.

The justification for the above algorithm is as follows. Suppose $A$ is invertible and, say, the sequence of elementary row operations $e_{1}, e_{2}, \ldots, e_{q}$ applied to $M=[A, I]$ reduces the left half of $M$, which is $A$, to the identity matrix $I$. Let $E_{i}$ be the elementary matrix corresponding to the operation $e_{i}$. Then, by applying Theorem 3.16. we get

$$
E_{q} \ldots E_{2} E_{1} A=I \quad \text { or } \quad\left(E_{q} \ldots E_{2} E_{1} I\right) A=I, \quad \text { so } \quad A^{-1}=E_{q} \ldots E_{2} E_{1} I
$$

That is, $A^{-1}$ can be obtained by applying the elementary row operations $e_{1}, e_{2}, \ldots, e_{q}$ to the identity matrix $I$, which appears in the right half of $M$. Thus, $B=A^{-1}$, as claimed.

\section*{EXAMPLE 3.20}
Find the inverse of the matrix $A=\left[\begin{array}{rrr}1 & 0 & 2 \\ 2 & -1 & 3 \\ 4 & 1 & 8\end{array}\right]$.

First form the (block) matrix $M=[A, I]$ and row reduce $M$ to an echelon form:

$$
M=\left[\begin{array}{rrr:rrr}
1 & 0 & 2 & 1 & 0 & 0 \\
2 & -1 & 3 & 0 & 1 & 0 \\
4 & 1 & 8 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 2 & 1 & 0 & 0 \\
0 & -1 & -1 & -2 & 1 & 0 \\
0 & 1 & 0 & -4 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 2 & 1 & 0 & 0 \\
0 & -1 & -1 & -2 & 1 & 0 \\
0 & 0 & -1 & -6 & 1 & 1
\end{array}\right]
$$

In echelon form, the left half of $M$ is in triangular form; hence, $A$ has an inverse. Next we further row reduce $M$ to its row canonical form:

$$
M \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & -11 & 2 & 2 \\
0 & -1 & 0 & 4 & 0 & -1 \\
0 & 0 & 1 & 6 & -1 & -1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & -11 & 2 & 2 \\
0 & 1 & 0 & -4 & 0 & 1 \\
0 & 0 & 1 & 6 & -1 & -1
\end{array}\right]
$$

The identity matrix is now in the left half of the final matrix; hence, the right half is $A^{-1}$. In other words,

$$
A^{-1}=\left[\begin{array}{rrr}
-11 & 2 & 2 \\
-4 & 0 & 1 \\
6 & -1 & -1
\end{array}\right]
$$

\section*{Elementary Column Operations}
Now let $A$ be a matrix with columns $C_{1}, C_{2}, \ldots, C_{n}$. The following operations on $A$, analogous to the elementary row operations, are called elementary column operations:

$\left[\mathrm{F}_{1}\right]$ (Column Interchange): Interchange columns $C_{i}$ and $C_{j}$.

$\left[\mathrm{F}_{2}\right]$ (Column Scaling): Replace $C_{i}$ by $k C_{i}$ (where $k \neq 0$ ).

$\left[\mathrm{F}_{3}\right]$ (Column Addition): Replace $C_{j}$ by $k C_{i}+C_{j}$.

We may indicate each of the column operations by writing, respectively,\\
(1) $C_{i} \leftrightarrow C_{j}$,\\
(2) $k C_{i} \rightarrow C_{i}$,\\
(3) $\left(k C_{i}+C_{j}\right) \rightarrow C_{j}$

Moreover, each column operation has an inverse operation of the same type, just like the corresponding row operation.

Now let $f$ denote an elementary column operation, and let $F$ be the matrix obtained by applying $f$ to the identity matrix $I$; that is,

$$
F=f(I)
$$

Then $F$ is called the elementary matrix corresponding to the elementary column operation $f$. Note that $F$ is always a square matrix.

\section*{EXAMPLE 3.21}
Consider the following elementary column operations:\\
(1) Interchange $C_{1}$ and $C_{3}$;\\
(2) Replace $C_{3}$ by $-2 C_{3}$;\\
(3) Replace $C_{3}$ by $-3 C_{2}+C_{3}$

The corresponding three $3 \times 3$ elementary matrices are as follows:

$$
F_{1}=\left[\begin{array}{rrr}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{array}\right], \quad F_{2}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -2
\end{array}\right], \quad F_{3}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & -3 \\
0 & 0 & 1
\end{array}\right]
$$

The following theorem is analogous to Theorem 3.16 for the elementary row operations.

THEOREM 3.20: For any matrix $A, f(A)=A F$.

That is, the result of applying an elementary column operation $f$ on a matrix $A$ can be obtained by postmultiplying $A$ by the corresponding elementary matrix $F$.

\section*{Matrix Equivalence}
A matrix $B$ is equivalent to a matrix $A$ if $B$ can be obtained from $A$ by a sequence of row and column operations. Alternatively, $B$ is equivalent to $A$, if there exist nonsingular matrices $P$ and $Q$ such that $B=P A Q$. Just like row equivalence, equivalence of matrices is an equivalence relation.

The main result of this subsection (proved in Problem 3.38) is as follows.

THEOREM 3.21: Every $m \times n$ matrix $A$ is equivalent to a unique block matrix of the form

$$
\left[\begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array}\right]
$$

where $I_{r}$ is the $r$-square identity matrix.

The following definition applies.

DEFINITION: The nonnegative integer $r$ in Theorem 3.18 is called the $\operatorname{rank}$ of $A$, written $\operatorname{rank}(A)$.

Note that this definition agrees with the previous definition of the rank of a matrix.

\subsection*{3.13 LU DECOMPOSITION}
Suppose $A$ is a nonsingular matrix that can be brought into (upper) triangular form $U$ using only rowaddition operations; that is, suppose $A$ can be triangularized by the following algorithm, which we write using computer notation.

ALGORITHM 3.6: The input is a matrix $A$ and the output is a triangular matrix $U$.

Step 1. Repeat for $i=1,2, \ldots, n-1$ :

Step 2. Repeat for $j=i+1, i+2, \ldots, n$

(a) Set $m_{i j}:=-a_{i j} / a_{i i}$.

(b) Set $R_{j}:=m_{i j} R_{i}+R_{j}$

[End of Step 2 inner loop.]

[End of Step 1 outer loop.]

The numbers $m_{i j}$ are called multipliers. Sometimes we keep track of these multipliers by means of the following lower triangular matrix $L$ :

$$
L=\left[\begin{array}{cccccc}
1 & 0 & 0 & \ldots & 0 & 0 \\
-m_{21} & 1 & 0 & \ldots & 0 & 0 \\
-m_{31} & -m_{32} & 1 & \ldots & 0 & 0 \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
-m_{n 1} & -m_{n 2} & -m_{n 3} & \ldots & -m_{n, n-1} & 1
\end{array}\right]
$$

That is, $L$ has 1's on the diagonal, 0 's above the diagonal, and the negative of the multiplier $m_{i j}$ as its $i j$-entry below the diagonal.

The above matrix $L$ and the triangular matrix $U$ obtained in Algorithm 3.6 give us the classical $L U$ factorization of such a matrix $A$. Namely,

THEOREM 3.22: Let $A$ be a nonsingular matrix that can be brought into triangular form $U$ using only row-addition operations. Then $A=L U$, where $L$ is the above lower triangular matrix with 1's on the diagonal, and $U$ is an upper triangular matrix with no 0 's on the diagonal.

EXAMPLE 3.22 Suppose $A=\left[\begin{array}{rrr}1 & 2 & -3 \\ -3 & -4 & 13 \\ 2 & 1 & -5\end{array}\right]$. We note that $A$ may be reduced to triangular form by the operations

"Replace $R_{2}$ by $3 R_{1}+R_{2}$ "; "Replace $R_{3}$ by $-2 R_{1}+R_{3}$ "; $\quad$ and then "Replace $R_{3}$ by $\frac{3}{2} R_{2}+R_{3}$ "

That is,

$$
A \sim\left[\begin{array}{rrr}
1 & 2 & -3 \\
0 & 2 & 4 \\
0 & -3 & 1
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & -3 \\
0 & 2 & 4 \\
0 & 0 & 7
\end{array}\right]
$$

This gives us the classical factorization $A=L U$, where

$$
L=\left[\begin{array}{rrr}
1 & 0 & 0 \\
-3 & 1 & 0 \\
2 & -\frac{3}{2} & 1
\end{array}\right] \quad \text { and } \quad U=\left[\begin{array}{rrr}
1 & 2 & -3 \\
0 & 2 & 4 \\
0 & 0 & 7
\end{array}\right]
$$

We emphasize:

(1) The entries $-3,2,-\frac{3}{2}$ in $L$ are the negatives of the multipliers in the above elementary row operations.

(2) $U$ is the triangular form of $A$.

\section*{Application to Systems of Linear Equations}
Consider a computer algorithm $M$. Let $C(n)$ denote the running time of the algorithm as a function of the size $n$ of the input data. [The function $C(n)$ is sometimes called the time complexity or simply the complexity of the algorithm $M$.] Frequently, $C(n)$ simply counts the number of multiplications and divisions executed by $M$, but does not count the number of additions and subtractions because they take much less time to execute.

Now consider a square system of linear equations $A X=B$, where

$$
A=\left[a_{i j}\right], \quad X=\left[x_{1}, \ldots, x_{n}\right]^{T}, \quad B=\left[b_{1}, \ldots, b_{n}\right]^{T}
$$

and suppose $A$ has an $L U$ factorization. Then the system can be brought into triangular form (in order to apply back-substitution) by applying Algorithm 3.6 to the augmented matrix $M=[A, B]$ of the system. The time complexity of Algorithm 3.6 and back-substitution are, respectively,

$$
C(n) \approx \frac{1}{2} n^{3} \quad \text { and } \quad C(n) \approx \frac{1}{2} n^{2}
$$

where $n$ is the number of equations.

On the other hand, suppose we already have the factorization $A=L U$. Then, to triangularize the system, we need only apply the row operations in the algorithm (retained by the matrix $L$ ) to the column vector $B$. In this case, the time complexity is

$$
C(n) \approx \frac{1}{2} n^{2}
$$

Of course, to obtain the factorization $A=L U$ requires the original algorithm where $C(n) \approx \frac{1}{2} n^{3}$. Thus, nothing may be gained by first finding the $L U$ factorization when a single system is involved. However, there are situations, illustrated below, where the $L U$ factorization is useful.

Suppose, for a given matrix $A$, we need to solve the system

$$
A X=B
$$

repeatedly for a sequence of different constant vectors, say $B_{1}, B_{2}, \ldots, B_{k}$. Also, suppose some of the $B_{i}$ depend upon the solution of the system obtained while using preceding vectors $B_{j}$. In such a case, it is more efficient to first find the $L U$ factorization of $A$, and then to use this factorization to solve the system for each new $B$.

EXAMPLE 3.23 Consider the following system of linear equations:

$$
\begin{aligned}
x+2 y+z & =k_{1} \\
2 x+3 y+3 z & =k_{2} \\
-3 x+10 y+2 z & =k_{3}
\end{aligned} \quad \text { or } \quad A X=B, \quad \text { where } \quad A=\left[\begin{array}{rrr}
1 & 2 & 1 \\
2 & 3 & 3 \\
-3 & 10 & 2
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{l}
k_{1} \\
k_{2} \\
k_{3}
\end{array}\right]
$$

Suppose we want to solve the system three times where $B$ is equal, say, to $B_{1}, B_{2}, B_{3}$. Furthermore, suppose $B_{1}=[1,1,1]^{T}$, and suppose

$$
B_{j+1}=B_{j}+X_{j} \quad(\text { for } j=1,2)
$$

where $X_{j}$ is the solution of $A X=B_{j}$. Here it is more efficient to first obtain the $L U$ factorization of $A$ and then use the $L U$ factorization to solve the system for each of the $B$ 's. (This is done in Problem 3.42.)

\section*{SOLVED PROBLEMS}
\section*{Linear Equations, Solutions, $2 \times 2$ Systems}
3.1. Determine whether each of the following equations is linear:\\
(a) $5 x+7 y-8 y z=16$\\
(b) $x+\pi y+e z=\log 5$,\\
(c) $3 x+k y-8 z=16$

(a) No, because the product $y z$ of two unknowns is of second degree.

(b) Yes, because $\pi, e$, and $\log 5$ are constants.

(c) As it stands, there are four unknowns: $x, y, z, k$. Because of the term $k y$ it is not a linear equation. However, assuming $k$ is a constant, the equation is linear in the unknowns $x, y, z$.

3.2. Determine whether the following vectors are solutions of $x_{1}+2 x_{2}-4 x_{3}+3 x_{4}=15$ :

(a) $u=(3,2,1,4)$ and (b) $v=(1,2,4,5)$.

(a) Substitute to obtain $3+2(2)-4(1)+3(4)=15$, or $15=15$; yes, it is a solution.

(b) Substitute to obtain $1+2(2)-4(4)+3(5)=15$, or $4=15$; no, it is not a solution.\\
3.3. Solve\\
(a) $e x=\pi$\\
(b) $3 x-4-x=2 x+3$,\\
(c) $7+2 x-4=3 x+3-x$

(a) Because $e \neq 0$, multiply by $1 / e$ to obtain $x=\pi / e$.

(b) Rewrite in standard form, obtaining $0 x=7$. The equation has no solution.

(c) Rewrite in standard form, obtaining $0 x=0$. Every scalar $k$ is a solution.

3.4. Prove Theorem 3.4: Consider the equation $a x=b$.

(i) If $a \neq 0$, then $x=b / a$ is a unique solution of $a x=b$.

(ii) If $a=0$ but $b \neq 0$, then $a x=b$ has no solution.

(iii) If $a=0$ and $b=0$, then every scalar $k$ is a solution of $a x=b$.

Suppose $a \neq 0$. Then the scalar $b / a$ exists. Substituting $b / a$ in $a x=b$ yields $a(b / a)=b$, or $b=b$; hence, $b / a$ is a solution. On the other hand, suppose $x_{0}$ is a solution to $a x=b$, so that $a x_{0}=b$. Multiplying both sides by $1 / a$ yields $x_{0}=b / a$. Hence, $b / a$ is the unique solution of $a x=b$. Thus, (i) is proved.

On the other hand, suppose $a=0$. Then, for any scalar $k$, we have $a k=0 k=0$. If $b \neq 0$, then $a k \neq b$. Accordingly, $k$ is not a solution of $a x=b$, and so (ii) is proved. If $b=0$, then $a k=b$. That is, any scalar $k$ is a solution of $a x=b$, and so (iii) is proved.

3.5. Solve each of the following systems:\\
(a) $\quad \begin{aligned} & 2 x-5 y=11 \\ & 3 x+4 y=5\end{aligned}$\\
(b)\\
$\begin{aligned} 2 x-3 y & =8 \\ -6 x+9 y & =6\end{aligned}$\\
(c)\\
$\begin{aligned} 2 x-3 y & =8 \\ -4 x+6 y & =-16\end{aligned}$

(a) Eliminate $x$ from the equations by forming the new equation $L=-3 L_{1}+2 L_{2}$. This yields the equation

$$
23 y=-23, \quad \text { and so } \quad y=-1
$$

Substitute $y=-1$ in one of the original equations, say $L_{1}$, to get

$$
2 x-5(-1)=11 \quad \text { or } \quad 2 x+5=11 \quad \text { or } \quad 2 x=6 \quad \text { or } \quad x=3
$$

Thus, $x=3, y=-1$ or the pair $u=(3,-1)$ is the unique solution of the system.

(b) Eliminate $x$ from the equations by forming the new equation $L=3 L_{1}+L_{2}$. This yields the equation

$$
0 x+0 y=30
$$

This is a degenerate equation with a nonzero constant; hence, this equation and the system have no solution. (Geometrically, the lines corresponding to the equations are parallel.)

(c) Eliminate $x$ from the equations by forming the new equation $L=2 L_{1}+L_{2}$. This yields the equation

$$
0 x+0 y=0
$$

This is a degenerate equation where the constant term is also zero. Thus, the system has an infinite number of solutions, which correspond to the solution of either equation. (Geometrically, the lines corresponding to the equations coincide.)

To find the general solution, set $y=a$ and substitute in $L_{1}$ to obtain

$$
2 x-3 a=8 \quad \text { or } \quad 2 x=3 a+8 \quad \text { or } \quad x=\frac{3}{2} a+4
$$

Thus, the general solution is

$$
x=\frac{3}{2} a+4, \quad y=a \quad \text { or } \quad u=\left(\frac{3}{2} a+4, a\right)
$$

where $a$ is any scalar.

3.6. Consider the system

$$
\begin{array}{r}
x+a y=4 \\
a x+9 y=b
\end{array}
$$

(a) For which values of $a$ does the system have a unique solution?

(b) Find those pairs of values $(a, b)$ for which the system has more than one solution.

(a) Eliminate $x$ from the equations by forming the new equation $L=-a L_{1}+L_{2}$. This yields the equation


\begin{equation*}
\left(9-a^{2}\right) y=b-4 a \tag{1}
\end{equation*}


The system has a unique solution if and only if the coefficient of $y$ in (1) is not zero-that is, if $9-a^{2} \neq 0$ or if $a \neq \pm 3$.

(b) The system has more than one solution if both sides of (1) are zero. The left-hand side is zero when $a= \pm 3$. When $a=3$, the right-hand side is zero when $b-12=0$ or $b=12$. When $a=-3$, the righthand side is zero when $b+12-0$ or $b=-12$. Thus, $(3,12)$ and $(-3,-12)$ are the pairs for which the system has more than one solution.

\section*{Systems in Triangular and Echelon Form}
3.7. Determine the pivot and free variables in each of the following systems:

$$
\begin{array}{r}
2 x_{1}-3 x_{2}-6 x_{3}-5 x_{4}+2 x_{5}=7 \\
x_{3}+3 x_{4}-7 x_{5}=6 \\
x_{4}-2 x_{5}=1
\end{array}
$$

(a)

$$
\begin{array}{r}
2 x-6 y+7 z=1 \\
4 y+3 z=8 \\
2 z=4
\end{array}
$$

(b)

$$
\begin{array}{r}
x+2 y-3 z=2 \\
2 x+3 y+z=4 \\
3 x+4 y+5 z=8
\end{array}
$$

(c)

(a) In echelon form, the leading unknowns are the pivot variables, and the others are the free variables. Here $x_{1}, x_{3}, x_{4}$ are the pivot variables, and $x_{2}$ and $x_{5}$ are the free variables.\\
(b) The leading unknowns are $x, y, z$, so they are the pivot variables. There are no free variables (as in any triangular system).

(c) The notion of pivot and free variables applies only to a system in echelon form.

3.8. Solve the triangular system in Problem 3.7(b).

Because it is a triangular system, solve by back-substitution.

(i) The last equation gives $z=2$.

(ii) Substitute $z=2$ in the second equation to get $4 y+6=8$ or $y=\frac{1}{2}$.

(iii) Substitute $z=2$ and $y=\frac{1}{2}$ in the first equation to get

$$
2 x-6\left(\frac{1}{2}\right)+7(2)=1 \quad \text { or } \quad 2 x+11=1 \quad \text { or } \quad x=-5
$$

Thus, $\quad x=-5, \quad y=\frac{1}{2}, \quad z=2 \quad$ or $u=\left(-5, \frac{1}{2}, 2\right) \quad$ is the unique solution to the system.

3.9. Solve the echelon system in Problem 3.7(a).

Assign parameters to the free variables, say $x_{2}=a$ and $x_{5}=b$, and solve for the pivot variables by backsubstitution.

(i) Substitute $x_{5}=b$ in the last equation to get $x_{4}-2 b=1$ or $x_{4}=2 b+1$.

(ii) Substitute $x_{5}=b$ and $x_{4}=2 b+1$ in the second equation to get

$$
x_{3}+3(2 b+1)-7 b=6 \quad \text { or } \quad x_{3}-b+3=6 \quad \text { or } \quad x_{3}=b+3
$$

(iii) Substitute $x_{5}=b, x_{4}=2 b+1, x_{3}=b+3, x_{2}=a$ in the first equation to get

$$
\begin{gathered}
2 x_{1}-3 a-6(b+3)-5(2 b+1)+2 b=7 \quad \text { or } \quad 2 x_{1}-3 a-14 b-23=7 \\
\text { or } \quad x_{1}=\frac{3}{2} a+7 b+15
\end{gathered}
$$

Thus,

$$
\begin{gathered}
x_{1}=\frac{3}{2} a+7 b+15, \quad x_{2}=a, \quad x_{3}=b+3, \quad x_{4}=2 b+1, \quad x_{5}=b \\
\text { or } \quad u=\left(\frac{3}{2} a+7 b+15, \quad a, \quad b+3, \quad 2 b+1, \quad b\right)
\end{gathered}
$$

is the parametric form of the general solution.

Alternatively, solving for the pivot variable $x_{1}, x_{3}, x_{4}$ in terms of the free variables $x_{2}$ and $x_{5}$ yields the following free-variable form of the general solution:

$$
x_{1}=\frac{3}{2} x_{2}+7 x_{5}+15, \quad x_{3}=x_{5}+3, \quad x_{4}=2 x_{5}+1
$$

3.10. Prove Theorem 3.6. Consider the system (3.4) of linear equations in echelon form with $r$ equations and $n$ unknowns.

(i) If $r=n$, then the system has a unique solution.

(ii) If $r<n$, then we can arbitrarily assign values to the $n-r$ free variable and solve uniquely for the $r$ pivot variables, obtaining a solution of the system.

(i) Suppose $r=n$. Then we have a square system $A X=B$ where the matrix $A$ of coefficients is (upper) triangular with nonzero diagonal elements. Thus, $A$ is invertible. By Theorem 3.10, the system has a unique solution.

(ii) Assigning values to the $n-r$ free variables yields a triangular system in the pivot variables, which, by (i), has a unique solution.

\section*{Gaussian Elimination}
3.11. Solve each of the following systems:

$$
\begin{aligned}
x+2 y-4 z & =-4 \\
2 x+5 y-9 z & =-10 \\
3 x-2 y+3 z & =11
\end{aligned}
$$

(a)

$$
\begin{array}{rr}
x+2 y-3 z= & -1 \\
-3 x+y-2 z= & -7 \\
5 x+3 y-4 z= & 2
\end{array}
$$

(b)

$$
\begin{array}{r}
x+2 y-3 z=1 \\
2 x+5 y-8 z=4 \\
3 x+8 y-13 z=7
\end{array}
$$

(c)

Reduce each system to triangular or echelon form using Gaussian elimination:

(a) Apply "Replace $L_{2}$ by $-2 L_{1}+L_{2}$ " and "Replace $L_{3}$ by $-3 L_{1}+L_{3}$ " to eliminate $x$ from the second and third equations, and then apply "Replace $L_{3}$ by $8 L_{2}+L_{3}$ " to eliminate $y$ from the third equation. These operations yield

$$
\begin{aligned}
& x+2 y-4 z=-4 \quad x+2 y-4 z=-4 \\
& y-z=-2 \quad \text { and then } \quad y-z=-2 \\
& -8 y+15 z=23 \quad 7 z=7
\end{aligned}
$$

The system is in triangular form. Solve by back-substitution to obtain the unique solution $u=(2,-1,1)$.

(b) Eliminate $x$ from the second and third equations by the operations "Replace $L_{2}$ by $3 L_{1}+L_{2}$ " and "Replace $L_{3}$ by $-5 L_{1}+L_{3}$. " This gives the equivalent system

$$
\begin{aligned}
x+2 y-3 z & =-1 \\
7 y-11 z & =-10 \\
-7 y+11 z & =7
\end{aligned}
$$

The operation "Replace $L_{3}$ by $L_{2}+L_{3}$ " yields the following degenerate equation with a nonzero constant:

$$
0 x+0 y+0 z=-3
$$

This equation and hence the system have no solution.

(c) Eliminate $x$ from the second and third equations by the operations "Replace $L_{2}$ by $-2 L_{1}+L_{2}$ " and "Replace $L_{3}$ by $-3 L_{1}+L_{3}$." This yields the new system

$$
\begin{aligned}
& x+2 y-3 z=1 \\
& y-2 z=2 \quad \text { or } \quad x+2 y-3 z=1 \\
& 2 y-4 z=4 \\
& y-2 z=2
\end{aligned}
$$

(The third equation is deleted, because it is a multiple of the second equation.) The system is in echelon form with pivot variables $x$ and $y$ and free variable $z$.

To find the parametric form of the general solution, set $z=a$ and solve for $x$ and $y$ by backsubstitution. Substitute $z=a$ in the second equation to get $y=2+2 a$. Then substitute $z=a$ and $y=2+2 a$ in the first equation to get

$$
x+2(2+2 a)-3 a=1 \quad \text { or } \quad x+4+a=1 \quad \text { or } \quad x=-3-a
$$

Thus, the general solution is

$$
x=-3-a, \quad y=2+2 a, \quad z=a \quad \text { or } \quad u=(-3-a, \quad 2+2 a, \quad a)
$$

where $a$ is a parameter.

3.12. Solve each of the following systems:

\[
\begin{array}{rlrl}
x_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5} & =2 & x_{1}+2 x_{2}-3 x_{3}+4 x_{4} & =2 \\
3 x_{1}-9 x_{2}+7 x_{3}-x_{4}+3 x_{5} & =7 & 2 x_{1}+5 x_{2}-2 x_{3}+x_{4} & =1 \\
2 x_{1}-6 x_{2}+7 x_{3}+4 x_{4}-5 x_{5} & =7 & 5 x_{1}+12 x_{2}-7 x_{3}+6 x_{4} & =3 \tag{a}
\end{array}
\]

Reduce each system to echelon form using Gaussian elimination:\\
(a) Apply "Replace $L_{2}$ by $-3 L_{1}+L_{2}$ "' and "Replace $L_{3}$ by $-2 L_{1}+L_{3}$ " to eliminate $x$ from the second and third equations. This yields

$$
\begin{aligned}
& x_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5}=2 \\
& x_{3}+2 x_{4}-3 x_{5}=1 \\
& 3 x_{3}+6 x_{4}-9 x_{5}=3 \\
& x_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5}=2 \\
& x_{3}+2 x_{4}-3 x_{5}=1
\end{aligned}
$$

(We delete $L_{3}$, because it is a multiple of $L_{2}$.) The system is in echelon form with pivot variables $x_{1}$ and $x_{3}$ and free variables $x_{2}, x_{4}, x_{5}$.

To find the parametric form of the general solution, set $x_{2}=a, x_{4}=b, x_{5}=c$, where $a, b, c$ are parameters. Back-substitution yields $x_{3}=1-2 b+3 c$ and $x_{1}=3 a+5 b-8 c$. The general solution is

$$
x_{1}=3 a+5 b-8 c, x_{2}=a, x_{3}=1-2 b+3 c, x_{4}=b, x_{5}=c
$$

or, equivalently, $u=(3 a+5 b-8 c, a, 1-2 b+3 c, b, c)$.

(b) Eliminate $x_{1}$ from the second and third equations by the operations "Replace $L_{2}$ by $-2 L_{1}+L_{2}$ " and "Replace $L_{3}$ by $-5 L_{1}+L_{3}$." This yields the system

$$
\begin{aligned}
x_{1}+2 x_{2}-3 x_{3}+4 x_{4} & =2 \\
x_{2}+4 x_{3}-7 x_{4} & =-3 \\
2 x_{2}+8 x_{3}-14 x_{4} & =-7
\end{aligned}
$$

The operation "Replace $L_{3}$ by $-2 L_{2}+L_{3}$ ", yields the degenerate equation $0=-1$. Thus, the system has no solution (even though the system has more unknowns than equations).

3.13. Solve using the condensed format:

$$
\begin{aligned}
2 y+3 z= & 3 \\
x+y+z= & 4 \\
4 x+8 y-3 z= & 35
\end{aligned}
$$

The condensed format follows:

$$
\begin{aligned}
& \text { Number } \\
& \text { Equation } \\
& \text { Operation } \\
& -13 z=13
\end{aligned}
$$

Here (1), (2), and ( $3^{\prime \prime}$ ) form a triangular system. (We emphasize that the interchange of $L_{1}$ and $L_{2}$ is accomplished by simply renumbering $L_{1}$ and $L_{2}$ as above.)

Using back-substitution with the triangular system yields $z=-1$ from $L_{3}, y=3$ from $L_{2}$, and $x=2$ from $L_{1}$. Thus, the unique solution of the system is $x=2, y=3, z=-1$ or the triple $u=(2,3,-1)$.

3.14. Consider the system

$$
\begin{aligned}
x+2 y+z & =3 \\
a y+5 z= & 10 \\
2 x+7 y+a z= & b
\end{aligned}
$$

(a) Find those values of $a$ for which the system has a unique solution.

(b) Find those pairs of values $(a, b)$ for which the system has more than one solution.

Reduce the system to echelon form. That is, eliminate $x$ from the third equation by the operation "Replace $L_{3}$ by $-2 L_{1}+L_{3}$ " and then eliminate $y$ from the third equation by the operation

"'Replace $L_{3}$ by $-3 L_{2}+a L_{3}$." This yields

$$
\begin{aligned}
& x+2 y \quad+z=3 \quad x+2 y+z=3 \\
& \text { ay } \quad+5 z=10 \\
& 3 y+(a-2) z=b-6 \\
& a y+5 z=10 \\
& \left(a^{2}-2 a-15\right) z=a b-6 a-30
\end{aligned}
$$

Examine the last equation $\left(a^{2}-2 a-15\right) z=a b-6 a-30$.

(a) The system has a unique solution if and only if the coefficient of $z$ is not zero; that is, if

$$
a^{2}-2 a-15=(a-5)(a+3) \neq 0 \quad \text { or } \quad a \neq 5 \quad \text { and } \quad a \neq-3
$$

(b) The system has more than one solution if both sides are zero. The left-hand side is zero when $a=5$ or $a=-3$. When $a=5$, the right-hand side is zero when $5 b-60=0$, or $b=12$. When $a=-3$, the righthand side is zero when $-3 b-12=0$, or $b=-4$. Thus, $(5,12)$ and $(-3,-4)$ are the pairs for which the system has more than one solution.

\section*{Echelon Matrices, Row Equivalence, Row Canonical Form}
3.15. Row reduce each of the following matrices to echelon form:\\
(a) $A=\left[\begin{array}{llll}1 & 2 & -3 & 0 \\ 2 & 4 & -2 & 2 \\ 3 & 6 & -4 & 3\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}-4 & 1 & -6 \\ 1 & 2 & -5 \\ 6 & 3 & -4\end{array}\right]$

(a) Use $a_{11}=1$ as a pivot to obtain 0 's below $a_{11}$; that is, apply the row operations "Replace $R_{2}$ by $-2 R_{1}+R_{2}$ "' and 'Replace $R_{3}$ by $-3 R_{1}+R_{3}$." Then use $a_{23}=4$ as a pivot to obtain a 0 below $a_{23}$; that is, apply the row operation "Replace $R_{3}$ by $-5 R_{2}+4 R_{3}$." These operations yield

$$
A \sim\left[\begin{array}{rrrr}
1 & 2 & -3 & 0 \\
0 & 0 & 4 & 2 \\
0 & 0 & 5 & 3
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & -3 & 0 \\
0 & 0 & 4 & 2 \\
0 & 0 & 0 & 2
\end{array}\right]
$$

The matrix is now in echelon form.

(b) Hand calculations are usually simpler if the pivot element equals 1 . Therefore, first interchange $R_{1}$ and $R_{2}$. Next apply the operations 'Replace $R_{2}$ by $4 R_{1}+R_{2}$ "' and 'Replace $R_{3}$ by $-6 R_{1}+R_{3}$ "; and then apply the operation "Replace $R_{3}$ by $R_{2}+R_{3}$." These operations yield

$$
B \sim\left[\begin{array}{rrr}
1 & 2 & -5 \\
-4 & 1 & -6 \\
6 & 3 & -4
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & -5 \\
0 & 9 & -26 \\
0 & -9 & 26
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & -5 \\
0 & 9 & -26 \\
0 & 0 & 0
\end{array}\right]
$$

The matrix is now in echelon form.

3.16. Describe the pivoting row-reduction algorithm. Also describe the advantages, if any, of using this pivoting algorithm.

The row-reduction algorithm becomes a pivoting algorithm if the entry in column $j$ of greatest absolute value is chosen as the pivot $a_{1 j_{1}}$ and if one uses the row operation

$$
\left(-a_{i j_{1}} / a_{1 j_{1}}\right) R_{1}+R_{i} \rightarrow R_{i}
$$

The main advantage of the pivoting algorithm is that the above row operation involves division by the (current) pivot $a_{1 j_{1}}$, and, on the computer, roundoff errors may be substantially reduced when one divides by a number as large in absolute value as possible.

3.17. Let $A=\left[\begin{array}{rrrr}2 & -2 & 2 & 1 \\ -3 & 6 & 0 & -1 \\ 1 & -7 & 10 & 2\end{array}\right]$. Reduce $A$ to echelon form using the pivoting algorithm.

First interchange $R_{1}$ and $R_{2}$ so that -3 can be used as the pivot, and then apply the operations "Replace $R_{2}$ by $\frac{2}{3} R_{1}+R_{2}$ " and "Replace $R_{3}$ by $\frac{1}{3} R_{1}+R_{3}$." These operations yield

$$
A \sim\left[\begin{array}{rrrr}
-3 & 6 & 0 & -1 \\
2 & -2 & 2 & 1 \\
1 & -7 & 10 & 2
\end{array}\right] \sim\left[\begin{array}{rrrr}
-3 & 6 & 0 & -1 \\
0 & 2 & 2 & \frac{1}{3} \\
0 & -5 & 10 & \frac{5}{3}
\end{array}\right]
$$

Now interchange $R_{2}$ and $R_{3}$ so that -5 can be used as the pivot, and then apply the operation "Replace $R_{3}$ by $\frac{2}{5} R_{2}+R_{3}$." We obtain

$$
A \sim\left[\begin{array}{rrrr}
-3 & 6 & 0 & -1 \\
0 & -5 & 10 & \frac{5}{3} \\
0 & 2 & 2 & \frac{1}{3}
\end{array}\right] \sim\left[\begin{array}{rrrr}
-3 & 6 & 0 & -1 \\
0 & -5 & 10 & \frac{5}{3} \\
0 & 0 & 6 & 1
\end{array}\right]
$$

The matrix has been brought to echelon form using partial pivoting.

3.18. Reduce each of the following matrices to row canonical form:\\
(a) $A=\left[\begin{array}{rrrrr}2 & 2 & -1 & 6 & 4 \\ 4 & 4 & 1 & 10 & 13 \\ 8 & 8 & -1 & 26 & 23\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}5 & -9 & 6 \\ 0 & 2 & 3 \\ 0 & 0 & 7\end{array}\right]$

(a) First reduce $A$ to echelon form by applying the operations "Replace $R_{2}$ by $-2 R_{1}+R_{2}$ " and "Replace $R_{3}$ by $-4 R_{1}+R_{3}$," and then applying the operation "Replace $R_{3}$ by $-R_{2}+R_{3}$." These operations yield

$$
A \sim\left[\begin{array}{rrrrr}
2 & 2 & -1 & 6 & 4 \\
0 & 0 & 3 & -2 & 5 \\
0 & 0 & 3 & 2 & 7
\end{array}\right] \sim\left[\begin{array}{rrrrr}
2 & 2 & -1 & 6 & 4 \\
0 & 0 & 3 & -2 & 5 \\
0 & 0 & 0 & 4 & 2
\end{array}\right]
$$

Now use back-substitution on the echelon matrix to obtain the row canonical form of $A$. Specifically, first multiply $R_{3}$ by $\frac{1}{4}$ to obtain the pivot $a_{34}=1$, and then apply the operations "Replace $R_{2}$ by $2 R_{3}+R_{2}$ " and "Replace $R_{1}$ by $-6 R_{3}+R_{1}$." These operations yield

$$
A \sim\left[\begin{array}{rrrrr}
2 & 2 & -1 & 6 & 4 \\
0 & 0 & 3 & -2 & 5 \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right] \sim\left[\begin{array}{rrrrr}
2 & 2 & -1 & 0 & 1 \\
0 & 0 & 3 & 0 & 6 \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right]
$$

Now multiply $R_{2}$ by $\frac{1}{3}$, making the pivot $a_{23}=1$, and then apply "Replace $R_{1}$ by $R_{2}+R_{1}$," yielding

$$
A \sim\left[\begin{array}{rrrrr}
2 & 2 & -1 & 0 & 1 \\
0 & 0 & 1 & 0 & 2 \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right] \sim\left[\begin{array}{lllll}
2 & 2 & 0 & 0 & 3 \\
0 & 0 & 1 & 0 & 2 \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right]
$$

Finally, multiply $R_{1}$ by $\frac{1}{2}$, so the pivot $a_{11}=1$. Thus, we obtain the following row canonical form of $A$ :

$$
A \sim\left[\begin{array}{lllll}
1 & 1 & 0 & 0 & \frac{3}{2} \\
0 & 0 & 1 & 0 & 2 \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right]
$$

(b) Because $B$ is in echelon form, use back-substitution to obtain

$$
B \sim\left[\begin{array}{rrr}
5 & -9 & 6 \\
0 & 2 & 3 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr}
5 & -9 & 0 \\
0 & 2 & 0 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr}
5 & -9 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lll}
5 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

The last matrix, which is the identity matrix $I$, is the row canonical form of $B$. (This is expected, because $B$ is invertible, and so its row canonical form must be $I$.)

3.19. Describe the Gauss-Jordan elimination algorithm, which also row reduces an arbitrary matrix $A$ to its row canonical form.

The Gauss-Jordan algorithm is similar in some ways to the Gaussian elimination algorithm, except that here each pivot is used to place 0 's both below and above the pivot, not just below the pivot, before working with the next pivot. Also, one variation of the algorithm first normalizes each row-that is, obtains a unit pivot-before it is used to produce 0's in the other rows, rather than normalizing the rows at the end of the algorithm.

3.20. Let $A=\left[\begin{array}{rrrrr}1 & -2 & 3 & 1 & 2 \\ 1 & 1 & 4 & -1 & 3 \\ 2 & 5 & 9 & -2 & 8\end{array}\right]$. Use Gauss-Jordan to find the row canonical form of $A$.

Use $a_{11}=1$ as a pivot to obtain 0 's below $a_{11}$ by applying the operations "Replace $R_{2}$ by $-R_{1}+R_{2}$ " and 'Replace $R_{3}$ by $-2 R_{1}+R_{3}$." This yields

$$
A \sim\left[\begin{array}{rrrrr}
1 & -2 & 3 & 1 & 2 \\
0 & 3 & 1 & -2 & 1 \\
0 & 9 & 3 & -4 & 4
\end{array}\right]
$$

Multiply $R_{2}$ by $\frac{1}{3}$ to make the pivot $a_{22}=1$, and then produce 0 's below and above $a_{22}$ by applying the operations "Replace $R_{3}$ by $-9 R_{2}+R_{3}$ " and "Replace $R_{1}$ by $2 R_{2}+R_{1}$." These operations yield

$$
A \sim\left[\begin{array}{rrrrr}
1 & -2 & 3 & 1 & 2 \\
0 & 1 & \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\
0 & 9 & 3 & -4 & 4
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 0 & \frac{11}{3} & -\frac{1}{3} & \frac{8}{3} \\
0 & 1 & \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\
0 & 0 & 0 & 2 & 1
\end{array}\right]
$$

Finally, multiply $R_{3}$ by $\frac{1}{2}$ to make the pivot $a_{34}=1$, and then produce 0 's above $a_{34}$ by applying the operations 'Replace $R_{2}$ by $\frac{2}{3} R_{3}+R_{2}$ "' and 'Replace $R_{1}$ by $\frac{1}{3} R_{3}+R_{1}$." These operations yield

$$
A \sim\left[\begin{array}{rrrrr}
1 & 0 & \frac{11}{3} & -\frac{1}{3} & \frac{8}{3} \\
0 & 1 & \frac{1}{3} & -\frac{2}{3} & \frac{1}{3} \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right] \sim\left[\begin{array}{ccccc}
1 & 0 & \frac{11}{3} & 0 & \frac{17}{6} \\
0 & 1 & \frac{1}{3} & 0 & \frac{2}{3} \\
0 & 0 & 0 & 1 & \frac{1}{2}
\end{array}\right]
$$

which is the row canonical form of $A$.

\section*{Systems of Linear Equations in Matrix Form}
3.21. Find the augmented matrix $M$ and the coefficient matrix $A$ of the following system:

$$
\begin{array}{r}
x+2 y-3 z=4 \\
3 y-4 z+7 x=5 \\
6 z+8 x-9 y=1
\end{array}
$$

First align the unknowns in the system, and then use the aligned system to obtain $M$ and $A$. We have

$$
\begin{aligned}
x+2 y-3 z & =4 \\
7 x+3 y-4 z & =5 ; \\
8 x-9 y+6 z & =1
\end{aligned} \quad \text { then } \quad M=\left[\begin{array}{rrrr}
1 & 2 & -3 & 4 \\
7 & 3 & -4 & 5 \\
8 & -9 & 6 & 1
\end{array}\right] \quad \text { and } \quad A=\left[\begin{array}{rrr}
1 & 2 & -3 \\
7 & 3 & -4 \\
8 & -9 & 6
\end{array}\right]
$$

3.22. Solve each of the following systems using its augmented matrix $M$ :

$$
\begin{aligned}
& x+2 y-z=3 \quad x-2 y+4 z=2 \quad x+y+3 z=1 \\
& x+3 y+z=5 \quad 2 x-3 y+5 z=3 \quad 2 x+3 y-z=3 \\
& 3 x+8 y+4 z=17 \quad 3 x-4 y+6 z=7 \quad 5 x+7 y+z=7
\end{aligned}
$$

(a) Reduce the augmented matrix $M$ to echelon form as follows:

$$
M=\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
1 & 3 & 1 & 5 \\
3 & 8 & 4 & 17
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
0 & 1 & 2 & 2 \\
0 & 2 & 7 & 8
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
0 & 1 & 2 & 2 \\
0 & 0 & 3 & 4
\end{array}\right]
$$

Now write down the corresponding triangular system

$$
\begin{array}{r}
x+2 y-z=3 \\
y+2 z=2 \\
3 z=4
\end{array}
$$

and solve by back-substitution to obtain the unique solution

$$
x=\frac{17}{3}, y=-\frac{2}{3}, z=\frac{4}{3} \quad \text { or } \quad u=\left(\frac{17}{3},-\frac{2}{3}, \frac{4}{3}\right)
$$

Alternately, reduce the echelon form of $M$ to row canonical form, obtaining

$$
M \sim\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
0 & 1 & 2 & 2 \\
0 & 0 & 1 & \frac{4}{3}
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & \frac{13}{3} \\
0 & 1 & 0 & -\frac{2}{3} \\
0 & 0 & 1 & \frac{4}{3}
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 0 & 0 & \frac{17}{3} \\
0 & 1 & 0 & -\frac{2}{3} \\
0 & 0 & 1 & \frac{4}{3}
\end{array}\right]
$$

This also corresponds to the above solution.

(b) First reduce the augmented matrix $M$ to echelon form as follows:

$$
M=\left[\begin{array}{llll}
1 & -2 & 4 & 2 \\
2 & -3 & 5 & 3 \\
3 & -4 & 6 & 7
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & -2 & 4 & 2 \\
0 & 1 & -3 & -1 \\
0 & 2 & -6 & 1
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & -2 & 4 & 2 \\
0 & 1 & -3 & -1 \\
0 & 0 & 0 & 3
\end{array}\right]
$$

The third row corresponds to the degenerate equation $0 x+0 y+0 z=3$, which has no solution. Thus, "DO NOT CONTINUE." The original system also has no solution. (Note that the echelon form indicates whether or not the system has a solution.)

(c) Reduce the augmented matrix $M$ to echelon form and then to row canonical form:

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 3 & 1 \\
2 & 3 & -1 & 3 \\
5 & 7 & 1 & 7
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 3 & 1 \\
0 & 1 & -7 & 1 \\
0 & 2 & -14 & 2
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 0 & 10 & 0 \\
0 & 1 & -7 & 1
\end{array}\right]
$$

(The third row of the second matrix is deleted, because it is a multiple of the second row and will result in a zero row.) Write down the system corresponding to the row canonical form of $M$ and then transfer the free variables to the other side to obtain the free-variable form of the solution:

$$
\begin{aligned}
& x+10 z=0 \\
& y-7 z=1
\end{aligned} \quad \text { and } \quad \begin{gathered}
x=-10 z \\
y=1+7 z
\end{gathered}
$$

Here $z$ is the only free variable. The parametric solution, using $z=a$, is as follows:

$$
x=-10 a, y=1+7 a, z=a \quad \text { or } \quad u=(-10 a, 1+7 a, a)
$$

3.23. Solve the following system using its augmented matrix $M$ :

$$
\begin{array}{r}
x_{1}+2 x_{2}-3 x_{3}-2 x_{4}+4 x_{5}=1 \\
2 x_{1}+5 x_{2}-8 x_{3}-x_{4}+6 x_{5}=4 \\
x_{1}+4 x_{2}-7 x_{3}+5 x_{4}+2 x_{5}=8
\end{array}
$$

Reduce the augmented matrix $M$ to echelon form and then to row canonical form:

$$
\begin{aligned}
& M=\left[\begin{array}{rrrrrr}
1 & 2 & -3 & -2 & 4 & 1 \\
2 & 5 & -8 & -1 & 6 & 4 \\
1 & 4 & -7 & 5 & 2 & 8
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 2 & -3 & -2 & 4 & 1 \\
0 & 1 & -2 & 3 & -2 & 2 \\
0 & 2 & -4 & 7 & -2 & 7
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 2 & -3 & -2 & 4 & 1 \\
0 & 1 & -2 & 3 & -2 & 2 \\
0 & 0 & 0 & 1 & 2 & 3
\end{array}\right] \\
& \sim\left[\begin{array}{rrrrrr}
1 & 2 & -3 & 0 & 8 & 7 \\
0 & 1 & -2 & 0 & -8 & -7 \\
0 & 0 & 0 & 1 & 2 & 3
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 0 & 1 & 0 & 24 & 21 \\
0 & 1 & -2 & 0 & -8 & -7 \\
0 & 0 & 0 & 1 & 2 & 3
\end{array}\right]
\end{aligned}
$$

Write down the system corresponding to the row canonical form of $M$ and then transfer the free variables to the other side to obtain the free-variable form of the solution:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-104}
\end{center}

$$
\begin{aligned}
& x_{4}+2 x_{5}=3 \quad x_{4}=3-2 x_{5}
\end{aligned}
$$

Here $x_{1}, x_{2}, x_{4}$ are the pivot variables and $x_{3}$ and $x_{5}$ are the free variables. Recall that the parametric form of the solution can be obtained from the free-variable form of the solution by simply setting the free variables equal to parameters, say $x_{3}=a, x_{5}=b$. This process yields

or

$$
\begin{gathered}
x_{1}=21-a-24 b, x_{2}=-7+2 a+8 b, x_{3}=a, x_{4}=3-2 b, x_{5}=b \\
u=(21-a-24 b,-7+2 a+8 b, a, 3-2 b, b)
\end{gathered}
$$

which is another form of the solution.

\section*{Linear Combinations, Homogeneous Systems}
3.24. Write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where

(a) $v=(3,10,7)$ and $u_{1}=(1,3,-2), u_{2}=(1,4,2), u_{3}=(2,8,1)$;

(b) $v=(2,7,10)$ and $u_{1}=(1,2,3), u_{2}=(1,3,5), u_{3}=(1,5,9)$;

(c) $v=(1,5,4)$ and $u_{1}=(1,3,-2), u_{2}=(2,7,-1), u_{3}=(1,6,7)$.

Find the equivalent system of linear equations by writing $v=x u_{1}+y u_{2}+z u_{3}$. Alternatively, use the augmented matrix $M$ of the equivalent system, where $M=\left[u_{1}, u_{2}, u_{3}, v\right]$. (Here $u_{1}, u_{2}, u_{3}, v$ are the columns of $M$.)

(a) The vector equation $v=x u_{1}+y u_{2}+z u_{3}$ for the given vectors is as follows:

$$
\left[\begin{array}{r}
3 \\
10 \\
7
\end{array}\right]=x\left[\begin{array}{r}
1 \\
3 \\
-2
\end{array}\right]+y\left[\begin{array}{l}
1 \\
4 \\
2
\end{array}\right]+z\left[\begin{array}{l}
2 \\
8 \\
1
\end{array}\right]=\left[\begin{array}{r}
x+y+2 z \\
3 x+4 y+8 z \\
-2 x+2 y+z
\end{array}\right]
$$

Form the equivalent system of linear equations by setting corresponding entries equal to each other, and then reduce the system to echelon form:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-105}
\end{center}

The system is in triangular form. Back-substitution yields the unique solution $x=2, y=7, z=-3$. Thus, $v=2 u_{1}+7 u_{2}-3 u_{3}$.

Alternatively, form the augmented matrix $M=\left[u_{1}, u_{2}, u_{3}, v\right]$ of the equivalent system, and reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 2 & 3 \\
3 & 4 & 8 & 10 \\
-2 & 2 & 1 & 7
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 2 & 3 \\
0 & 1 & 2 & 1 \\
0 & 4 & 5 & 13
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 2 & 3 \\
0 & 1 & 2 & 1 \\
0 & 0 & -3 & 9
\end{array}\right]
$$

The last matrix corresponds to a triangular system that has a unique solution. Back-substitution yields the solution $x=2, y=7, z=-3$. Thus, $v=2 u_{1}+7 u_{2}-3 u_{3}$.

(b) Form the augmented matrix $M=\left[u_{1}, u_{2}, u_{3}, v\right]$ of the equivalent system, and reduce $M$ to the echelon form:

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 1 & 2 \\
2 & 3 & 5 & 7 \\
3 & 5 & 9 & 10
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 1 & 1 & 2 \\
0 & 1 & 3 & 3 \\
0 & 2 & 6 & 4
\end{array}\right] \sim\left[\begin{array}{cccc}
1 & 1 & 1 & 2 \\
0 & 1 & 3 & 3 \\
0 & 0 & 0 & -2
\end{array}\right]
$$

The third row corresponds to the degenerate equation $0 x+0 y+0 z=-2$, which has no solution. Thus, the system also has no solution, and $v$ cannot be written as a linear combination of $u_{1}, u_{2}, u_{3}$.

(c) Form the augmented matrix $M=\left[u_{1}, u_{2}, u_{3}, v\right]$ of the equivalent system, and reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrr}
1 & 2 & 1 & 1 \\
3 & 7 & 6 & 5 \\
-2 & -1 & 7 & 4
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 2 & 1 & 1 \\
0 & 1 & 3 & 2 \\
0 & 3 & 9 & 6
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 2 & 1 & 1 \\
0 & 1 & 3 & 2 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

The last matrix corresponds to the following system with free variable $z$ :

$$
\begin{array}{r}
x+2 y+z=1 \\
y+3 z=2
\end{array}
$$

Thus, $v$ can be written as a linear combination of $u_{1}, u_{2}, u_{3}$ in many ways. For example, let the free variable $z=1$, and, by back-substitution, we get $y=-2$ and $x=2$. Thus, $v=2 u_{1}-2 u_{2}+u_{3}$.

3.25. Let $u_{1}=(1,2,4), u_{2}=(2,-3,1), u_{3}=(2,1,-1)$ in $\mathbf{R}^{3}$. Show that $u_{1}, u_{2}, u_{3}$ are orthogonal, and write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where (a) $v=(7,16,6)$, (b) $v=(3,5,2)$.

Take the dot product of pairs of vectors to get

$$
u_{1} \cdot u_{2}=2-6+4=0, \quad u_{1} \cdot u_{3}=2+2-4=0, \quad u_{2} \cdot u_{3}=4-3-1=0
$$

Thus, the three vectors in $\mathbf{R}^{3}$ are orthogonal, and hence Fourier coefficients can be used. That is, $v=x u_{1}+y u_{2}+z u_{3}$, where

(a) We have

$$
x=\frac{v \cdot u_{1}}{u_{1} \cdot u_{1}}, \quad y=\frac{v \cdot u_{2}}{u_{2} \cdot u_{2}}, \quad z=\frac{v \cdot u_{3}}{u_{3} \cdot u_{3}}
$$

$x=\frac{7+32+24}{1+4+16}=\frac{63}{21}=3$,

$y=\frac{14-48+6}{4+9+1}=\frac{-28}{14}=-2$,

$$
z=\frac{14+16-6}{4+1+1}=\frac{24}{6}=4
$$

Thus, $v=3 u_{1}-2 u_{2}+4 u_{3}$.

(b) We have

$$
x=\frac{3+10+8}{1+4+16}=\frac{21}{21}=1, \quad y=\frac{6-15+2}{4+9+1}=\frac{-7}{14}=-\frac{1}{2}, \quad z=\frac{6+5-2}{4+1+1}=\frac{9}{6}=\frac{3}{2}
$$

Thus, $v=u_{1}-\frac{1}{2} u_{2}+\frac{3}{2} u_{3}$.

3.26. Find the dimension and a basis for the general solution $W$ of each of the following homogeneous systems:

\[
\begin{array}{rr}
2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4}=0 & x-2 y-3 z=0 \\
3 x_{1}+6 x_{2}-7 x_{3}+4 x_{4}=0 & 2 x+y+3 z=0 \\
5 x_{1}+10 x_{2}-11 x_{3}+6 x_{4}=0 & 3 x-4 y-2 z=0 \tag{a}
\end{array}
\]

(a) Reduce the system to echelon form using the operations "Replace $L_{2}$ by $-3 L_{1}+2 L_{2}$," "Replace $L_{3}$ by $-5 L_{1}+2 L_{3}$," and then "Replace $L_{3}$ by $-2 L_{2}+L_{3}$." These operations yield

$$
\begin{aligned}
2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4} & =0 \\
x_{3}-x_{4} & =0 \\
3 x_{3}-3 x_{4} & =0
\end{aligned} \quad \text { and } \quad \begin{aligned}
2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4} & =0 \\
x_{3}-x_{4} & =0
\end{aligned}
$$

The system in echelon form has two free variables, $x_{2}$ and $x_{4}$, so $\operatorname{dim} W=2$. A basis $\left[u_{1}, u_{2}\right]$ for $W$ may be obtained as follows:

(1) Set $x_{2}=1, x_{4}=0$. Back-substitution yields $x_{3}=0$, and then $x_{1}=-2$. Thus, $u_{1}=(-2,1,0,0)$.

(2) Set $x_{2}=0, x_{4}=1$. Back-substitution yields $x_{3}=1$, and then $x_{1}=1$. Thus, $u_{2}=(1,0,1,1)$.

(b) Reduce the system to echelon form, obtaining

$$
\begin{aligned}
x-2 y-3 z & =0 & & x-2 y-3 z & =0 \\
5 y+9 z & =0 & \text { and } & 5 y+9 z & =0 \\
2 y+7 z & =0 & & 17 z & =0
\end{aligned}
$$

There are no free variables (the system is in triangular form). Hence, $\operatorname{dim} W=0$, and $W$ has no basis. Specifically, $W$ consists only of the zero solution; that is, $W=\{0\}$.

3.27. Find the dimension and a basis for the general solution $W$ of the following homogeneous system using matrix notation:

$$
\begin{array}{r}
x_{1}+2 x_{2}+3 x_{3}-2 x_{4}+4 x_{5}=0 \\
2 x_{1}+4 x_{2}+8 x_{3}+x_{4}+9 x_{5}=0 \\
3 x_{1}+6 x_{2}+13 x_{3}+4 x_{4}+14 x_{5}=0
\end{array}
$$

Show how the basis gives the parametric form of the general solution of the system.

When a system is homogeneous, we represent the system by its coefficient matrix $A$ rather than by its\\
augmented matrix $M$, because the last column of the augmented matrix $M$ is a zero column, and it will remain a zero column during any row-reduction process.

Reduce the coefficient matrix $A$ to echelon form, obtaining

$$
A=\left[\begin{array}{rrrrr}
1 & 2 & 3 & -2 & 4 \\
2 & 4 & 8 & 1 & 9 \\
3 & 6 & 13 & 4 & 14
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & 3 & -2 & 4 \\
0 & 0 & 2 & 5 & 1 \\
0 & 0 & 4 & 10 & 2
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & 3 & -2 & 4 \\
0 & 0 & 2 & 5 & 1
\end{array}\right]
$$

(The third row of the second matrix is deleted, because it is a multiple of the second row and will result in a zero row.) We can now proceed in one of two ways.

(a) Write down the corresponding homogeneous system in echelon form:

$$
\begin{aligned}
x_{1}+2 x_{2}+3 x_{3}-2 x_{4}+4 x_{5} & =0 \\
2 x_{3}+5 x_{4}+x_{5} & =0
\end{aligned}
$$

The system in echelon form has three free variables, $x_{2}, x_{4}, x_{5}$, so $\operatorname{dim} W=3$. A basis $\left[u_{1}, u_{2}, u_{3}\right]$ for $W$ may be obtained as follows:

(1) Set $x_{2}=1, x_{4}=0, x_{5}=0$. Back-substitution yields $x_{3}=0$, and then $x_{1}=-2$. Thus,

$$
u_{1}=(-2,1,0,0,0)
$$

(2) Set $x_{2}=0, x_{4}=1, x_{5}=0$. Back-substitution yields $x_{3}=-\frac{5}{2}$, and then $x_{1}=\frac{19}{2}$. Thus,

$$
u_{2}=\left(\frac{19}{2}, 0,-\frac{5}{2}, 1,0\right) \text {. }
$$

(3) Set $x_{2}=0, x_{4}=0, x_{5}=1$. Back-substitution yields $x_{3}=-\frac{1}{2}$, and then $x_{1}=-\frac{5}{2}$. Thus,

$$
u_{3}=\left(-\frac{5}{2}, 0,-\frac{1}{2}, 0,1\right) \text {. }
$$

[One could avoid fractions in the basis by choosing $x_{4}=2$ in (2) and $x_{5}=2$ in (3), which yields multiples of $u_{2}$ and $u_{3}$.] The parametric form of the general solution is obtained from the following linear combination of the basis vectors using parameters $a, b, c$ :

$$
a u_{1}+b u_{2}+c u_{3}=\left(-2 a+\frac{19}{2} b-\frac{5}{2} c, a,-\frac{5}{2} b-\frac{1}{2} c, b, c\right)
$$

(b) Reduce the echelon form of $A$ to row canonical form:

$$
A \sim\left[\begin{array}{ccccc}
1 & 2 & 3 & -2 & 4 \\
0 & 0 & 1 & \frac{5}{2} & \frac{1}{2}
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & 3 & -\frac{19}{2} & \frac{5}{2} \\
0 & 0 & 1 & \frac{5}{2} & \frac{1}{2}
\end{array}\right]
$$

Write down the corresponding free-variable solution:

$$
\begin{aligned}
& x_{1}=-2 x_{2}+\frac{19}{2} x_{4}-\frac{5}{2} x_{5} \\
& x_{3}=-\frac{5}{2} x_{4}-\frac{1}{2} x_{5}
\end{aligned}
$$

Using these equations for the pivot variables $x_{1}$ and $x_{3}$, repeat the above process to obtain a basis $\left[u_{1}, u_{2}, u_{3}\right]$ for $W$. That is, set $x_{2}=1, x_{4}=0, x_{5}=0$ to get $u_{1}$; set $x_{2}=0, x_{4}=1, x_{5}=0$ to get $u_{2}$; and set $x_{2}=0$, $x_{4}=0, x_{5}=1$ to get $u_{3}$.

3.28. Prove Theorem 3.15. Let $v_{0}$ be a particular solution of $A X=B$, and let $W$ be the general solution of $A X=0$. Then $U=v_{0}+W=\left\{v_{0}+w: w \in W\right\}$ is the general solution of $A X=B$. Let $w$ be a solution of $A X=0$. Then

$$
A\left(v_{0}+w\right)=A v_{0}+A w=B+0=B
$$

Thus, the sum $v_{0}+w$ is a solution of $A X=B$. On the other hand, suppose $v$ is also a solution of $A X=B$. Then

$$
A\left(v-v_{0}\right)=A v-A v_{0}=B-B=0
$$

Therefore, $v-v_{0}$ belongs to $W$. Because $v=v_{0}+\left(v-v_{0}\right)$, we find that any solution of $A X=B$ can be obtained by adding a solution of $A X=0$ to a solution of $A X=B$. Thus, the theorem is proved.

\section*{Elementary Matrices, Applications}
3.29. Let $e_{1}, e_{2}, e_{3}$ denote, respectively, the elementary row operations

"Interchange rows $R_{1}$ and $R_{2}$," "Replace $R_{3}$ by $7 R_{3}$," "Replace $R_{2}$ by $-3 R_{1}+R_{2}$ "

Find the corresponding three-square elementary matrices $E_{1}, E_{2}, E_{3}$. Apply each operation to the $3 \times 3$ identity matrix $I_{3}$ to obtain

$$
E_{1}=\left[\begin{array}{lll}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{array}\right], \quad E_{2}=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 7
\end{array}\right], \quad E_{3}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
-3 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

3.30. Consider the elementary row operations in Problem 3.29.

(a) Describe the inverse operations $e_{1}^{-1}, e_{2}^{-1}, e_{3}^{-1}$.

(b) Find the corresponding three-square elementary matrices $E_{1}^{\prime}, E_{2}^{\prime}, E_{3}^{\prime}$.

(c) What is the relationship between the matrices $E_{1}^{\prime}, E_{2}^{\prime}, E_{3}^{\prime}$ and the matrices $E_{1}, E_{2}, E_{3}$ ?

(a) The inverses of $e_{1}, e_{2}, e_{3}$ are, respectively,

$$
\text { "Interchange rows } R_{1} \text { and } R_{2}, " \quad \text { "Replace } R_{3} \text { by } \frac{1}{7} R_{3}, " \quad \text { "Replace } R_{2} \text { by } 3 R_{1}+R_{2} \text {." }
$$

(b) Apply each inverse operation to the $3 \times 3$ identity matrix $I_{3}$ to obtain

$$
E_{1}^{\prime}=\left[\begin{array}{ccc}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{array}\right], \quad E_{2}^{\prime}=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & \frac{1}{7}
\end{array}\right], \quad E_{3}^{\prime}=\left[\begin{array}{ccc}
1 & 0 & 0 \\
3 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

(c) The matrices $E_{1}^{\prime}, E_{2}^{\prime}, E_{3}^{\prime}$ are, respectively, the inverses of the matrices $E_{1}, E_{2}, E_{3}$.

3.31. Write each of the following matrices as a product of elementary matrices:\\
(a) $A=\left[\begin{array}{rr}1 & -3 \\ -2 & 4\end{array}\right]$\\
(b) $B=\left[\begin{array}{lll}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 1\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rrr}1 & 1 & 2 \\ 2 & 3 & 8 \\ -3 & -1 & 2\end{array}\right]$

The following three steps write a matrix $M$ as a product of elementary matrices:

Step 1. Row reduce $M$ to the identity matrix $I$, keeping track of the elementary row operations.

Step 2. Write down the inverse row operations.

Step 3. Write $M$ as the product of the elementary matrices corresponding to the inverse operations. This gives the desired result.

If a zero row appears in Step 1, then $M$ is not row equivalent to the identity matrix $I$, and $M$ cannot be written as a product of elementary matrices.

(a) (1) We have

$$
A=\left[\begin{array}{rr}
1 & -3 \\
-2 & 4
\end{array}\right] \sim\left[\begin{array}{ll}
1 & -3 \\
0 & -2
\end{array}\right] \sim\left[\begin{array}{rr}
1 & -3 \\
0 & 1
\end{array}\right] \sim\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right]=I
$$

where the row operations are, respectively,

"Replace $R_{2}$ by $2 R_{1}+R_{2}$," "Replace $R_{2}$ by $-\frac{1}{2} R_{2}, " \quad$ "Replace $R_{1}$ by $3 R_{2}+R_{1}$ 

(2) Inverse operations:

"Replace $R_{2}$ by $-2 R_{1}+R_{2}$," $\quad$ "Replace $R_{2}$ by $-2 R_{2}$," $\quad$ "Replace $R_{1}$ by $-3 R_{2}+R_{1}$ "

(3) $A=\left[\begin{array}{rr}1 & 0 \\ -2 & 1\end{array}\right]\left[\begin{array}{rr}1 & 0 \\ 0 & -2\end{array}\right]\left[\begin{array}{rr}1 & -3 \\ 0 & 1\end{array}\right]$\\
(b) (1) We have

$$
B=\left[\begin{array}{lll}
1 & 2 & 3 \\
0 & 1 & 4 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 2 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]=I
$$

where the row operations are, respectively,

"Replace $R_{2}$ by $-4 R_{3}+R_{2}$," "Replace $R_{1}$ by $-3 R_{3}+R_{1}$," "Replace $R_{1}$ by $-2 R_{2}+R_{1}$ "

(2) Inverse operations:

"Replace $R_{2}$ by $4 R_{3}+R_{2}$," "Replace $R_{1}$ by $3 R_{3}+R_{1}$," $\quad$ Replace $R_{1}$ by $2 R_{2}+R_{1}$ "

(3) $B=\left[\begin{array}{lll}1 & 0 & 0 \\ 0 & 1 & 4 \\ 0 & 0 & 1\end{array}\right]\left[\begin{array}{lll}1 & 0 & 3 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right]\left[\begin{array}{lll}1 & 2 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right]$

(c) (1) First row reduce $C$ to echelon form. We have

$$
C=\left[\begin{array}{rrr}
1 & 1 & 2 \\
2 & 3 & 8 \\
-3 & -1 & 2
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 2 \\
0 & 1 & 4 \\
0 & 2 & 8
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 2 \\
0 & 1 & 4 \\
0 & 0 & 0
\end{array}\right]
$$

In echelon form, $C$ has a zero row. "STOP." The matrix $C$ cannot be row reduced to the identity matrix $I$, and $C$ cannot be written as a product of elementary matrices. (We note, in particular, that $C$ has no inverse.)

3.32. Find the inverse of (a) $A=\left[\begin{array}{rrr}1 & 2 & -4 \\ -1 & -1 & 5 \\ 2 & 7 & -3\end{array}\right]$, (b) $B=\left[\begin{array}{rrr}1 & 3 & -4 \\ 1 & 5 & -1 \\ 3 & 13 & -6\end{array}\right]$.

(a) Form the matrix $M=[A, I]$ and row reduce $M$ to echelon form:

$$
\begin{aligned}
M & =\left[\begin{array}{rrr:rrr}
1 & 2 & -4 & 1 & 0 & 0 \\
-1 & -1 & 5 & 0 & 1 & 0 \\
2 & 7 & -3 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 2 & -4 & 1 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 3 & 5 & -2 & 0 & 1
\end{array}\right] \\
& \sim\left[\begin{array}{rrrrrrr}
1 & 2 & -4 & 1 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 0 & 2 & -5 & -3 & 1
\end{array}\right]
\end{aligned}
$$

In echelon form, the left half of $M$ is in triangular form; hence, $A$ has an inverse. Further reduce $M$ to row canonical form:

$$
M \sim\left[\begin{array}{rrr:rrr}
1 & 2 & 0 & -9 & -6 & 2 \\
0 & 1 & 0 & \frac{7}{2} & \frac{5}{2} & -\frac{1}{2} \\
0 & 0 & 1 & -\frac{5}{2} & -\frac{3}{2} & \frac{1}{2}
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & -16 & -11 & 3 \\
0 & 1 & 0 & \frac{7}{2} & \frac{5}{2} & -\frac{1}{2} \\
0 & 0 & 1 & -\frac{5}{2} & -\frac{3}{2} & \frac{1}{2}
\end{array}\right]
$$

The final matrix has the form $\left[I, A^{-1}\right]$; that is, $A^{-1}$ is the right half of the last matrix. Thus,

$$
A^{-1}=\left[\begin{array}{rrr}
-16 & -11 & 3 \\
\frac{7}{2} & \frac{5}{2} & -\frac{1}{2} \\
-\frac{5}{2} & -\frac{3}{2} & \frac{1}{2}
\end{array}\right]
$$

(b) Form the matrix $M=[B, I]$ and row reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrr:rrr}
1 & 3 & -4 & 1 & 0 & 0 \\
1 & 5 & -1 & 0 & 1 & 0 \\
3 & 13 & -6 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 3 & -4 & 1 & 0 & 0 \\
0 & 2 & 3 & -1 & 1 & 0 \\
0 & 4 & 6 & -3 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 3 & -4 & 1 & 0 & 0 \\
0 & 2 & 3 & -1 & 1 & 0 \\
0 & 0 & 0 & -1 & -2 & 1
\end{array}\right]
$$

In echelon form, $M$ has a zero row in its left half; that is, $B$ is not row reducible to triangular form. Accordingly, $B$ has no inverse.

3.33. Show that every elementary matrix $E$ is invertible, and its inverse is an elementary matrix.

Let $E$ be the elementary matrix corresponding to the elementary operation $e$; that is, $e(I)=E$. Let $e^{\prime}$ be the inverse operation of $e$ and let $E^{\prime}$ be the corresponding elementary matrix; that is, $e^{\prime}(I)=E^{\prime}$. Then

$$
I=e^{\prime}(e(I))=e^{\prime}(E)=E^{\prime} E \quad \text { and } \quad I=e\left(e^{\prime}(I)\right)=e\left(E^{\prime}\right)=E E^{\prime}
$$

Therefore, $E^{\prime}$ is the inverse of $E$.

3.34. Prove Theorem 3.16: Let $e$ be an elementary row operation and let $E$ be the corresponding $m$-square elementary matrix; that is, $E=e(I)$. Then $e(A)=E A$, where $A$ is any $m \times n$ matrix.

Let $R_{i}$ be the row $i$ of $A$; we denote this by writing $A=\left[R_{1}, \ldots, R_{m}\right]$. If $B$ is a matrix for which $A B$ is defined then $A B=\left[R_{1} B, \ldots, R_{m} B\right]$. We also let

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-110(1)}
\end{center}

Here ${ }^{\wedge}=i$ means 1 is the $i$ th entry. One can show (Problem 2.45) that $e_{i} A=R_{i}$. We also note that $I=\left[e_{1}, e_{2}, \ldots, e_{m}\right]$ is the identity matrix.

(i) Let $e$ be the elementary row operation "Interchange rows $R_{i}$ and $R_{j}$." Then, for ${ }^{\wedge}=i$ and ${ }^{\hat{}}=j$,

$$
E=e(I)=\left[e_{1}, \ldots, \widehat{e}_{j}, \ldots, \widehat{\widehat{e}}_{i}, \ldots, e_{m}\right]
$$

and

$$
e(A)=\left[R_{1}, \ldots, \widehat{R}_{j}, \ldots, \widehat{\hat{R}}_{i}, \ldots, R_{m}\right]
$$

Thus,

$$
E A=\left[e_{1} A, \ldots, \widehat{e_{j} A}, \ldots, \widehat{e_{i} A}, \ldots, e_{m} A\right]=\left[R_{1}, \ldots, \widehat{R}_{j}, \ldots, \widehat{\widehat{R}}_{i}, \ldots, R_{m}\right]=e(A)
$$

(ii) Let $e$ be the elementary row operation "Replace $R_{i}$ by $k R_{i}(k \neq 0)$." Then, for ${ }^{\wedge}=i$,

$$
E=e(I)=\left[e_{1}, \ldots, \widehat{k e}_{i}, \ldots, e_{m}\right]
$$

and

$$
e(A)=\left[R_{1}, \ldots, \widehat{k R}_{i}, \ldots, R_{m}\right]
$$

Thus,

$$
E A=\left[e_{1} A, \ldots, \widehat{k e_{i} A}, \ldots, e_{m} A\right]=\left[R_{1}, \ldots, \widehat{k R_{i}}, \ldots, R_{m}\right]=e(A)
$$

(iii) Let $e$ be the elementary row operation "Replace $R_{i}$ by $k R_{j}+R_{i}$." Then, for ${ }^{\wedge}=i$,

$$
E=e(I)=\left[e_{1}, \ldots, k \widehat{e}_{j} \widehat{+e}_{i}, \ldots, e_{m}\right]
$$

and

$$
e(A)=\left[R_{1}, \ldots, k R_{j} \widehat{+R} R_{i}, \ldots, R_{m}\right]
$$

Using $\left(k e_{j}+e_{i}\right) A=k\left(e_{j} A\right)+e_{i} A=k R_{j}+R_{i}$, we have

$$
\begin{aligned}
& E A=\left[\begin{array}{lllll}
e_{1} A, & \ldots, & \left(k e_{j}+e_{i}\right) A, & \ldots, & e_{m} A
\end{array}\right]
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-110}
\end{center}

3.35. Prove Theorem 3.17: Let $A$ be a square matrix. Then the following are equivalent:

(a) $A$ is invertible (nonsingular).

(b) $A$ is row equivalent to the identity matrix $I$.

(c) $A$ is a product of elementary matrices.

Suppose $A$ is invertible and suppose $A$ is row equivalent to matrix $B$ in row canonical form. Then there exist elementary matrices $E_{1}, E_{2}, \ldots, E_{s}$ such that $E_{s} \ldots E_{2} E_{1} A=B$. Because $A$ is invertible and each elementary matrix is invertible, $B$ is also invertible. But if $B \neq I$, then $B$ has a zero row; whence $B$ is not invertible. Thus, $B=I$, and (a) implies (b).

If (b) holds, then there exist elementary matrices $E_{1}, E_{2}, \ldots, E_{s}$ such that $E_{s} \ldots E_{2} E_{1} A=I$. Hence, $A=\left(E_{s} \ldots E_{2} E_{1}\right)^{-1}=E_{1}^{-1} E_{2}^{-1} \ldots, E_{s}^{-1}$. But the $E_{i}^{-1}$ are also elementary matrices. Thus (b) implies (c).

If (c) holds, then $A=E_{1} E_{2} \ldots E_{s}$. The $E_{i}$ are invertible matrices; hence, their product $A$ is also invertible. Thus, (c) implies (a). Accordingly, the theorem is proved.

3.36. Prove Theorem 3.18: If $A B=I$, then $B A=I$, and hence $B=A^{-1}$.

Suppose $A$ is not invertible. Then $A$ is not row equivalent to the identity matrix $I$, and so $A$ is row equivalent to a matrix with a zero row. In other words, there exist elementary matrices $E_{1}, \ldots, E_{s}$ such that $E_{s} \ldots E_{2} E_{1} A$ has a zero row. Hence, $E_{s} \ldots E_{2} E_{1} A B=E_{s} \ldots E_{2} E_{1}$, an invertible matrix, also has a zero row. But invertible matrices cannot have zero rows; hence $A$ is invertible, with inverse $A^{-1}$. Then also,

$$
B=I B=\left(A^{-1} A\right) B=A^{-1}(A B)=A^{-1} I=A^{-1}
$$

3.37. Prove Theorem 3.19: $B$ is row equivalent to $A$ (written $B \sim A$ ) if and only if there exists a nonsingular matrix $P$ such that $B=P A$.

If $B \sim A$, then $B=e_{s}\left(\ldots\left(e_{2}\left(e_{1}(A)\right)\right) \ldots\right)=E_{s} \ldots E_{2} E_{1} A=P A$ where $P=E_{s} \ldots E_{2} E_{1}$ is nonsingular. Conversely, suppose $B=P A$, where $P$ is nonsingular. By Theorem 3.17, $P$ is a product of elementary matrices, and so $B$ can be obtained from $A$ by a sequence of elementary row operations; that is, $B \sim A$. Thus, the theorem is proved.

3.38. Prove Theorem 3.21: Every $m \times n$ matrix $A$ is equivalent to a unique block matrix of the form $\left[\begin{array}{cc}I_{r} & 0 \\ 0 & 0\end{array}\right]$, where $I_{r}$ is the $r \times r$ identity matrix.

The proof is constructive, in the form of an algorithm.

Step 1. Row reduce $A$ to row canonical form, with leading nonzero entries $a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}}$.

Step 2. Interchange $C_{1}$ and $C_{1 j_{1}}$, interchange $C_{2}$ and $C_{2 j_{2}}, \ldots$, and interchange $C_{r}$ and $C_{j r}$. This gives a matrix in the form $\left[\begin{array}{ccc}I_{r} & B \\ \hdashline 0 & -1 & 0\end{array}\right]$, with leading nonzero entries $a_{11}, a_{22}, \ldots, a_{r r}$.

Step 3. Use column operations, with the $a_{i i}$ as pivots, to replace each entry in $B$ with a zero; that is, for $i=1,2, \ldots, r$ and $j=r+1, r+2, \ldots, n$, apply the operation $-b_{i j} C_{i}+C_{j} \rightarrow C_{j}$.

The final matrix has the desired form $\left[\begin{array}{c:c}I_{r} & 0 \\ \hdashline 0 & 0\end{array}\right]$.

Lu Factorization\\
3.39. Find the $\mathrm{LU}$ factorization of (a) $A=\left[\begin{array}{rrr}1 & -3 & 5 \\ 2 & -4 & 7 \\ -1 & -2 & 1\end{array}\right]$, (b) $B=\left[\begin{array}{rrr}1 & 4 & -3 \\ 2 & 8 & 1 \\ -5 & -9 & 7\end{array}\right]$.

(a) Reduce $A$ to triangular form by the following operations:

$$
\begin{gathered}
\text { "Replace } R_{2} \text { by }-2 R_{1}+R_{2}, " \quad \text { "Replace } R_{3} \text { by } R_{1}+R_{3}, " \quad \text { and then } \\
\text { "Replace } R_{3} \text { by } \frac{5}{2} R_{2}+R_{3} "
\end{gathered}
$$

These operations yield the following, where the triangular form is $U$ :

$$
A \sim\left[\begin{array}{rrr}
1 & -3 & 5 \\
0 & 2 & -3 \\
0 & -5 & 6
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & -3 & 5 \\
0 & 2 & -3 \\
0 & 0 & -\frac{3}{2}
\end{array}\right]=U \quad \text { and } \quad L=\left[\begin{array}{rrr}
1 & 0 & 0 \\
2 & 1 & 0 \\
-1 & -\frac{5}{2} & 1
\end{array}\right]
$$

The entries 2, $-1,-\frac{5}{2}$ in $L$ are the negatives of the multipliers $-2,1, \frac{5}{2}$ in the above row operations. (As a check, multiply $L$ and $U$ to verify $A=L U$.)\\
(b) Reduce $B$ to triangular form by first applying the operations "Replace $R_{2}$ by $-2 R_{1}+R_{2}$ " and "Replace $R_{3}$ by $5 R_{1}+R_{3}$." These operations yield

$$
B \sim\left[\begin{array}{rrr}
1 & 4 & -3 \\
0 & 0 & 7 \\
0 & 11 & -8
\end{array}\right]
$$

Observe that the second diagonal entry is 0 . Thus, $B$ cannot be brought into triangular form without row interchange operations. Accordingly, $B$ is not $L U$-factorable. (There does exist a $P L U$ factorization of such a matrix $B$, where $P$ is a permutation matrix, but such a factorization lies beyond the scope of this text.)

3.40. Find the $L D U$ factorization of the matrix $A$ in Problem 3.39.

The $A=L D U$ factorization refers to the situation where $L$ is a lower triangular matrix with 1 's on the diagonal (as in the $L U$ factorization of $A$ ), $D$ is a diagonal matrix, and $U$ is an upper triangular matrix with 1 's on the diagonal. Thus, simply factor out the diagonal entries in the matrix $U$ in the above $L U$ factorization of $A$ to obtain $D$ and $L$. That is,

$$
L=\left[\begin{array}{rrr}
1 & 0 & 0 \\
2 & 1 & 0 \\
-1 & -\frac{5}{2} & 1
\end{array}\right], \quad D=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & -\frac{3}{2}
\end{array}\right], \quad U=\left[\begin{array}{rrr}
1 & -3 & 5 \\
0 & 1 & -3 \\
0 & 0 & 1
\end{array}\right]
$$

3.41. Find the $L U$ factorization of the matrix $A=\left[\begin{array}{rrr}1 & 2 & 1 \\ 2 & 3 & 3 \\ -3 & -10 & 2\end{array}\right]$.

Reduce $A$ to triangular form by the following operations:

(1) "Replace $R_{2}$ by $-2 R_{1}+R_{2}$," (2) "Replace $R_{3}$ by $3 R_{1}+R_{3}$," (3) "Replace $R_{3}$ by $-4 R_{2}+R_{3}$ "

These operations yield the following, where the triangular form is $U$ :

$$
A \sim\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & -1 & 1 \\
0 & -4 & 5
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & -1 & 1 \\
0 & 0 & 1
\end{array}\right]=U \quad \text { and } \quad L=\left[\begin{array}{rrr}
1 & 0 & 0 \\
2 & 1 & 0 \\
-3 & 4 & 1
\end{array}\right]
$$

The entries 2,-3,4 in $L$ are the negatives of the multipliers $-2,3,-4$ in the above row operations. (As a check, multiply $L$ and $U$ to verify $A=L U$.)

3.42. Let $A$ be the matrix in Problem 3.41. Find $X_{1}, X_{2}, X_{3}$, where $X_{i}$ is the solution of $A X=B_{i}$ for (a) $B_{1}=(1,1,1)$, (b) $B_{2}=B_{1}+X_{1}$, (c) $B_{3}=B_{2}+X_{2}$.

(a) Find $L^{-1} B_{1}$ by applying the row operations (1), (2), and then (3) in Problem 3.41 to $B_{1}$ :

$$
B_{1}=\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right] \xrightarrow{(1) \text { and }(2)}\left[\begin{array}{r}
1 \\
-1 \\
4
\end{array}\right] \xrightarrow{(3)}\left[\begin{array}{r}
1 \\
-1 \\
8
\end{array}\right]
$$

Solve $U X=B$ for $B=(1,-1,8)$ by back-substitution to obtain $X_{1}=(-25,9,8)$.

(b) First find $B_{2}=B_{1}+X_{1}=(1,1,1)+(-25,9,8)=(-24,10,9)$. Then as above

$$
B_{2}=[-24,10,9]^{T} \xrightarrow{(1) \text { and }(2)}[-24,58,-63]^{T} \xrightarrow{(3)}[-24,58,-295]^{T}
$$

Solve $U X=B$ for $B=(-24,58,-295)$ by back-substitution to obtain $X_{2}=(943,-353,-295)$.

(c) First find $B_{3}=B_{2}+X_{2}=(-24,10,9)+(943,-353,-295)=(919,-343,-286)$. Then, as above

$$
B_{3}=[943,-353,-295]^{T} \xrightarrow{(1) \text { and }(2)}[919,-2181,2671]^{T} \xrightarrow{(3)}[919,-2181,11395]^{T}
$$

Solve $U X=B$ for $B=(919,-2181,11395)$ by back-substitution to obtain

$$
X_{3}=(-37628,13576,11395)
$$

\section*{Miscellaneous Problems}
3.43. Let $L$ be a linear combination of the $m$ equations in $n$ unknowns in the system (3.2). Say $L$ is the equation


\begin{equation*}
\left(c_{1} a_{11}+\cdots+c_{m} a_{m 1}\right) x_{1}+\cdots+\left(c_{1} a_{1 n}+\cdots+c_{m} a_{m n}\right) x_{n}=c_{1} b_{1}+\cdots+c_{m} b_{m} \tag{1}
\end{equation*}


Show that any solution of the system (3.2) is also a solution of $L$.

Let $u=\left(k_{1}, \ldots, k_{n}\right)$ be a solution of (3.2). Then


\begin{equation*}
a_{i 1} k_{1}+a_{i 2} k_{2}+\cdots+a_{i n} k_{n}=b_{i} \quad(i=1,2, \ldots, m) \tag{2}
\end{equation*}


Substituting $u$ in the left-hand side of (1) and using (2), we get

$$
\begin{aligned}
\left(c_{1} a_{11}\right. & \left.+\cdots+c_{m} a_{m 1}\right) k_{1}+\cdots+\left(c_{1} a_{1 n}+\cdots+c_{m} a_{m n}\right) k_{n} \\
& =c_{1}\left(a_{11} k_{1}+\cdots+a_{1 n} k_{n}\right)+\cdots+c_{m}\left(a_{m 1} k_{1}+\cdots+a_{m n} k_{n}\right) \\
& =c_{1} b_{1}+\cdots+c_{m} b_{m}
\end{aligned}
$$

This is the right-hand side of (1); hence, $u$ is a solution of (1).

3.44. Suppose a system $\mathscr{M}$ of linear equations is obtained from a system $\mathscr{L}$ by applying an elementary operation (page 64). Show that $\mathscr{M}$ and $\mathscr{L}$ have the same solutions.

Each equation $L$ in $\mathscr{M}$ is a linear combination of equations in $\mathscr{L}$. Hence, by Problem 3.43, any solution of $\mathscr{L}$ will also be a solution of $\mathscr{M}$. On the other hand, each elementary operation has an inverse elementary operation, so $\mathscr{L}$ can be obtained from $\mathscr{M}$ by an elementary operation. This means that any solution of $\mathscr{M}$ is a solution of $\mathscr{L}$. Thus, $\mathscr{L}$ and $\mathscr{M}$ have the same solutions.

3.45. Prove Theorem 3.4: Suppose a system $\mathscr{M}$ of linear equations is obtained from a system $\mathscr{L}$ by a sequence of elementary operations. Then $\mathscr{M}$ and $\mathscr{L}$ have the same solutions.

Each step of the sequence does not change the solution set (Problem 3.44). Thus, the original system $\mathscr{L}$ and the final system $\mathscr{M}$ (and any system in between) have the same solutions.

3.46. A system $\mathscr{L}$ of linear equations is said to be consistent if no linear combination of its equations is a degenerate equation $L$ with a nonzero constant. Show that $\mathscr{L}$ is consistent if and only if $\mathscr{L}$ is reducible to echelon form.

Suppose $\mathscr{L}$ is reducible to echelon form. Then $\mathscr{L}$ has a solution, which must also be a solution of every linear combination of its equations. Thus, $L$, which has no solution, cannot be a linear combination of the equations in $\mathscr{L}$. Thus, $\mathscr{L}$ is consistent.

On the other hand, suppose $\mathscr{L}$ is not reducible to echelon form. Then, in the reduction process, it must yield a degenerate equation $L$ with a nonzero constant, which is a linear combination of the equations in $\mathscr{L}$. Therefore, $\mathscr{L}$ is not consistent; that is, $\mathscr{L}$ is inconsistent.

3.47. Suppose $u$ and $v$ are distinct vectors. Show that, for distinct scalars $k$, the vectors $u+k(u-v)$ are distinct.

Suppose $u+k_{1}(u-v)=u+k_{2}(u-v)$. We need only show that $k_{1}=k_{2}$. We have

$$
k_{1}(u-v)=k_{2}(u-v), \quad \text { and so } \quad\left(k_{1}-k_{2}\right)(u-v)=0
$$

Because $u$ and $v$ are distinct, $u-v \neq 0$. Hence, $k_{1}-k_{2}=0$, and so $k_{1}=k_{2}$.

3.48. Suppose $A B$ is defined. Prove

(a) Suppose $A$ has a zero row. Then $A B$ has a zero row.

(b) Suppose $B$ has a zero column. Then $A B$ has a zero column.\\
(a) Let $R_{i}$ be the zero row of $A$, and $C_{1}, \ldots, C_{n}$ the columns of $B$. Then the $i$ th row of $A B$ is

$$
\left(R_{i} C_{1}, R_{i} C_{2}, \ldots, R_{i} C_{n}\right)=(0,0,0, \ldots, 0)
$$

(b) $B^{T}$ has a zero row, and so $B^{T} A^{T}=(A B)^{T}$ has a zero row. Hence, $A B$ has a zero column.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Linear Equations, $2 \times 2$ Systems}
3.49. Determine whether each of the following systems is linear:\\
(a) $3 x-4 y+2 y z=8$\\
(b)\\
$e x+3 y=\pi$,\\
$2 x-3 y+k z=4$

3.50. Solve

(a) $\pi x=2$,

(b) $3 x+2=5 x+7-2 x$,

(c) $6 x+2-4 x=5+2 x-3$

3.51. Solve each of the following systems:\\
(a) $2 x+3 y=1$\\
$5 x+7 y=3$\\
(b) $\begin{array}{r}4 x-2 y=5 \\ -6 x+3 y=1\end{array}$\\
(c) $2 x-4=3 y$\\
$5 y-x=5$\\
(d) $\begin{aligned} 2 x-4 y & =10 \\ 3 x-6 y & =15\end{aligned}$

3.52. Consider each of the following systems in unknowns $x$ and $y$ :\\
(a) $\quad x-a y=1$\\
$a x-4 y=b$\\
(b) $\quad a x+3 y=2$\\
$12 x+a y=b$\\
(c) $x+a y=3$\\
$2 x+5 y=b$

For which values of $a$ does each system have a unique solution, and for which pairs of values $(a, b)$ does each system have more than one solution?

\section*{General Systems of Linear Equations}
3.53. Solve\\
(a) $x+y+2 z=4$\\
$2 x+3 y+6 z=10$\\
$3 x+6 y+10 z=17$\\
(b) $x-2 y+3 z=2$\\
$2 x-3 y+8 z=7$\\
$3 x-4 y+13 z=8$\\
(c) $x+2 y+3 z=3$\\
$2 x+3 y+8 z=4$\\
$5 x+8 y+19 z=11$

3.54. Solve\\
(a) $x-2 y=5$\\
$2 x+3 y=3$\\
$3 x+2 y=7$\\
(b) $\begin{aligned} x+2 y-3 z+2 t & =2 \\ 2 x+5 y-8 z+6 t & =5 \\ 3 x+4 y-5 z+2 t & =4\end{aligned}$\\
(c) $x+2 y+4 z-5 t=3$\\
$3 x-y+5 z+2 t=4$\\
$5 x-4 y+6 z+9 t=2$

3.55. Solve\\
(a) $2 x-y-4 z=2$\\
$4 x-2 y-6 z=5$\\
$6 x-3 y-8 z=8$\\
(b) $\begin{aligned} x+2 y-z+3 t & =3 \\ 2 x+4 y+4 z+3 t & =9 \\ 3 x+6 y-z+8 t & =10\end{aligned}$

3.56. Consider each of the following systems in unknowns $x, y, z$ :\\
(a) $x-2 y=1$\\
$x-y+a z=2$\\
(b) $\begin{aligned} x+2 y+2 z & =1 \\ x+a y+3 z & =3 \\ x+11 y+a z & =b\end{aligned}$\\
(c) $x+y+a z=1$\\
$x+a y+z=4$\\
$a x+y+z=b$

For which values of $a$ does the system have a unique solution, and for which pairs of values $(a, b)$ does the system have more than one solution? The value of $b$ does not have any effect on whether the system has a unique solution. Why?

\section*{Linear Combinations, Homogeneous Systems}
3.57. Write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where

(a) $\quad v=(4,-9,2), \quad u_{1}=(1,2,-1), \quad u_{2}=(1,4,2), \quad u_{3}=(1,-3,2)$;

(b) $v=(1,3,2), \quad u_{1}=(1,2,1), \quad u_{2}=(2,6,5), \quad u_{3}=(1,7,8)$;

(c) $v=(1,4,6), \quad u_{1}=(1,1,2), \quad u_{2}=(2,3,5), \quad u_{3}=(3,5,8)$.

3.58. Let $u_{1}=(1,1,2), u_{2}=(1,3,-2), u_{3}=(4,-2,-1)$ in $\mathbf{R}^{3}$. Show that $u_{1}, u_{2}, u_{3}$ are orthogonal, and write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where (a) $v=(5,-5,9)$, (b) $v=(1,-3,3)$, (c) $v=(1,1,1)$.

(Hint: Use Fourier coefficients.)

3.59. Find the dimension and a basis of the general solution $W$ of each of the following homogeneous systems:\\
(a) $x-y+2 z=0$\\
$2 x+y+z=0$\\
$5 x+y+4 z=0$\\
(b) $\begin{aligned} x+2 y-3 z & =0 \\ 2 x+5 y+2 z & =0 \\ 3 x-y-4 z & =0\end{aligned}$\\
(c) $x+2 y+3 z+t=0$\\
$2 x+4 y+7 z+4 t=0$\\
$3 x+6 y+10 z+5 t=0$

3.60. Find the dimension and a basis of the general solution $W$ of each of the following systems:\\
(a) $x_{1}+3 x_{2}+2 x_{3}-x_{4}-x_{5}=0$\\
$2 x_{1}+6 x_{2}+5 x_{3}+x_{4}-x_{5}=0$\\
$5 x_{1}+15 x_{2}+12 x_{3}+x_{4}-3 x_{5}=0$\\
(b) $2 x_{1}-4 x_{2}+3 x_{3}-x_{4}+2 x_{5}=0$\\
$3 x_{1}-6 x_{2}+5 x_{3}-2 x_{4}+4 x_{5}=0$\\
$5 x_{1}-10 x_{2}+7 x_{3}-3 x_{4}+18 x_{5}=0$

\section*{Echelon Matrices, Row Canonical Form}
3.61. Reduce each of the following matrices to echelon form and then to row canonical form:\\
(a) $\left[\begin{array}{rrr}1 & 1 & 2 \\ 2 & 4 & 9 \\ 1 & 5 & 12\end{array}\right]$,\\
(b) $\left[\begin{array}{rrrrr}1 & 2 & -1 & 2 & 1 \\ 2 & 4 & 1 & -2 & 5 \\ 3 & 6 & 3 & -7 & 7\end{array}\right]$,\\
(c) $\left[\begin{array}{rrrrrr}2 & 4 & 2 & -2 & 5 & 1 \\ 3 & 6 & 2 & 2 & 0 & 4 \\ 4 & 8 & 2 & 6 & -5 & 7\end{array}\right]$

3.62. Reduce each of the following matrices to echelon form and then to row canonical form:\\
(a) $\left[\begin{array}{rrrrrr}1 & 2 & 1 & 2 & 1 & 2 \\ 2 & 4 & 3 & 5 & 5 & 7 \\ 3 & 6 & 4 & 9 & 10 & 11 \\ 1 & 2 & 4 & 3 & 6 & 9\end{array}\right]$,\\
(b) $\left[\begin{array}{rrrr}0 & 1 & 2 & 3 \\ 0 & 3 & 8 & 12 \\ 0 & 0 & 4 & 6 \\ 0 & 2 & 7 & 10\end{array}\right]$,\\
(c) $\left[\begin{array}{rrrr}1 & 3 & 1 & 3 \\ 2 & 8 & 5 & 10 \\ 1 & 7 & 7 & 11 \\ 3 & 11 & 7 & 15\end{array}\right]$

3.63. Using only 0 's and 1 's, list all possible $2 \times 2$ matrices in row canonical form.

3.64. Using only 0 's and 1 's, find the number $n$ of possible $3 \times 3$ matrices in row canonical form.

\section*{Elementary Matrices, Applications}
3.65. Let $e_{1}, e_{2}, e_{3}$ denote, respectively, the following elementary row operations:

"Interchange $R_{2}$ and $R_{3}, " \quad$ "Replace $R_{2}$ by $3 R_{2}, " \quad$ "Replace $R_{1}$ by $2 R_{3}+R_{1}$ "

(a) Find the corresponding elementary matrices $E_{1}, E_{2}, E_{3}$.

(b) Find the inverse operations $e_{1}^{-1}, e_{2}^{-1}, e_{3}^{-1}$; their corresponding elementary matrices $E_{1}^{\prime}, E_{2}^{\prime}, E_{3}^{\prime}$; and the relationship between them and $E_{1}, E_{2}, E_{3}$.

(c) Describe the corresponding elementary column operations $f_{1}, f_{2}, f_{3}$.

(d) Find elementary matrices $F_{1}, F_{2}, F_{3}$ corresponding to $f_{1}, f_{2}, f_{3}$, and the relationship between them and $E_{1}, E_{2}, E_{3}$.

3.66. Express each of the following matrices as a product of elementary matrices:

$$
A=\left[\begin{array}{ll}
1 & 2 \\
3 & 4
\end{array}\right], \quad B=\left[\begin{array}{rr}
3 & -6 \\
-2 & 4
\end{array}\right], \quad C=\left[\begin{array}{rr}
2 & 6 \\
-3 & -7
\end{array}\right], \quad D=\left[\begin{array}{lll}
1 & 2 & 0 \\
0 & 1 & 3 \\
3 & 8 & 7
\end{array}\right]
$$

3.67. Find the inverse of each of the following matrices (if it exists):

$$
A=\left[\begin{array}{rrr}
1 & -2 & -1 \\
2 & -3 & 1 \\
3 & -4 & 4
\end{array}\right], \quad B=\left[\begin{array}{rrr}
1 & 2 & 3 \\
2 & 6 & 1 \\
3 & 10 & -1
\end{array}\right], \quad C=\left[\begin{array}{rrr}
1 & 3 & -2 \\
2 & 8 & -3 \\
1 & 7 & 1
\end{array}\right], \quad D=\left[\begin{array}{rrr}
2 & 1 & -1 \\
5 & 2 & -3 \\
0 & 2 & 1
\end{array}\right]
$$

3.68. Find the inverse of each of the following $n \times n$ matrices:

(a) $A$ has 1's on the diagonal and superdiagonal (entries directly above the diagonal) and 0's elsewhere.

(b) $B$ has 1 's on and above the diagonal, and 0 's below the diagonal.

\section*{Lu Factorization}
3.69. Find the $L U$ factorization of each of the following matrices:\\
(a) $\left[\begin{array}{lll}1 & -1 & -1 \\ 3 & -4 & -2 \\ 2 & -3 & -2\end{array}\right]$, (b)\\
(b) $\left[\begin{array}{rrr}1 & 3 & -1 \\ 2 & 5 & 1 \\ 3 & 4 & 2\end{array}\right]$,\\
(c) $\left[\begin{array}{lll}2 & 3 & 6 \\ 4 & 7 & 9 \\ 3 & 5 & 4\end{array}\right]$,\\
(d) $\left[\begin{array}{rrr}1 & 2 & 3 \\ 2 & 4 & 7 \\ 3 & 7 & 10\end{array}\right]$

3.70. Let $A$ be the matrix in Problem 3.69(a). Find $X_{1}, X_{2}, X_{3}, X_{4}$, where

(a) $X_{1}$ is the solution of $A X=B_{1}$, where $B_{1}=(1,1,1)^{T}$.

(b) For $k>1, X_{k}$ is the solution of $A X=B_{k}$, where $B_{k}=B_{k-1}+X_{k-1}$.

3.71. Let $B$ be the matrix in Problem 3.69(b). Find the $L D U$ factorization of $B$.

\section*{Miscellaneous Problems}
3.72. Consider the following systems in unknowns $x$ and $y$ :

$$
\text { (a) } \begin{aligned}
a x+b y & =1 \\
c x+d y & =0
\end{aligned} \quad \text { (b) } \quad \begin{aligned}
a x+b y & =0 \\
c x+d y & =1
\end{aligned}
$$

Suppose $D=a d-b c \neq 0$. Show that each system has the unique solution:\\
(a) $x=d / D, y=-c / D$,\\
(b) $x=-b / D, y=a / D$.

3.73. Find the inverse of the row operation "Replace $R_{i}$ by $k R_{j}+k^{\prime} R_{i}\left(k^{\prime} \neq 0\right)$."

3.74. Prove that deleting the last column of an echelon form (respectively, the row canonical form) of an augmented matrix $M=[A, B]$ yields an echelon form (respectively, the row canonical form) of $A$.

3.75. Let $e$ be an elementary row operation and $E$ its elementary matrix, and let $f$ be the corresponding elementary column operation and $F$ its elementary matrix. Prove\\
(a) $f(A)=\left(e\left(A^{T}\right)\right)^{T}$,\\
(b) $F=E^{T}$,\\
(c) $f(A)=A F$.

3.76. Matrix $A$ is equivalent to matrix $B$, written $A \approx B$, if there exist nonsingular matrices $P$ and $Q$ such that $B=P A Q$. Prove that $\approx$ is an equivalence relation; that is,\\
(a) $A \approx A$,\\
(b) If $A \approx B$, then $B \approx A$,\\
(c) If $A \approx B$ and $B \approx C$, then $A \approx C$.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $A=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ denotes the matrix $A$ with rows $R_{1}, R_{2}, \ldots$. The elements in each row are separated by commas (which may be omitted with single digits), the rows are separated by semicolons, and 0 denotes a zero row. For example,

$$
A=[1,2,3,4 ; \quad 5,-6,7,-8 ; \quad 0]=\left[\begin{array}{rrrr}
1 & 2 & 3 & 4 \\
5 & -6 & 7 & -8 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

3.49. (a) no, (b) yes, (c) linear in $x, y, z$, not linear in $x, y, z, k$

3.50. (a) $x=2 / \pi, \quad$ (b) no solution, (c) every scalar $k$ is a solution

3.51. (a) $(2,-1), \quad$ (b) no solution, (c) $(5,2), \quad$ (d) $(5-2 a, a)$

3.52. (a) $a \neq \pm 2, \quad(2,2), \quad(-2,-2)$,\\
(b) $a \neq \pm 6, \quad(6,4), \quad(-6,-4)$,\\
(c) $a \neq \frac{5}{2}, \quad\left(\frac{5}{2}, 6\right)$

3.53. (a) $\left(2,1, \frac{1}{2}\right)$,

(b) no solution,

(c) $u=(-7 a-1,2 a+2, a)$.

3.54. (a) $(3,-1)$,

(b) $u=(-a+2 b, 1+2 a-2 b, a, b)$,

(c) no solution

3.55. (a) $u=\left(\frac{1}{2} a+2, \quad a, \quad \frac{1}{2}\right)$,

(b) $u=\left(\frac{1}{2}(7-5 b-4 a), \quad a, \frac{1}{2}(1+b), b\right)$

3.56. (a) $a \neq \pm 3, \quad(3,3), \quad(-3,-3)$,

(b) $a \neq 5$ and $a \neq-1, \quad(5,7), \quad(-1,-5)$,

(c) $a \neq 1$ and $a \neq-2, \quad(-2,5)$

3.57. (a) $2,-1,3$,

(b) $6,-3,1$,

(c) not possible

3.58. (a) $3,-2,1$,

(b) $\frac{2}{3},-1, \frac{1}{3}$

(c) $\frac{2}{3}, \frac{1}{7}, \frac{1}{21}$

3.59. (a) $\operatorname{dim} W=1, \quad u_{1}=(-1,1,1), \quad$ (b) $\operatorname{dim} W=0$, no basis,

(c) $\operatorname{dim} W=2, \quad u_{1}=(-2,1,0,0), \quad u_{2}=(5,0,-2,1)$

3.60. (a) $\operatorname{dim} W=3, \quad u_{1}=(-3,1,0,0,0), \quad u_{2}=(7,0,-3,1,0), \quad u_{3}=(3,0,-1,0,1)$,

(b) $\operatorname{dim} W=2, \quad u_{1}=(2,1,0,0,0), \quad u_{2}=(5,0,-5,-3,1)$

3.61. (a) $\left[1,0,-\frac{1}{2} ; 0,1, \frac{5}{2} ; 0\right]$, (b) $[1,2,0,0,2 ; \quad 0,0,1,0,5 ; \quad 0,0,0,1,2]$,

(c) $\left[1,2,0,4,-5,3 ; \quad 0,0,1,-5, \frac{15}{2},-\frac{5}{2} ; 0\right]$

3.62. (a) $[1,2,0,0,-4,-2 ; \quad 0,0,1,0,1,2 ; \quad 0,0,0,1,2,1 ; \quad 0]$,\\
(b) $[0,1,0,0 ; 0,0,1,0 ; 0,0,0,1 ; 0]$,\\
(c) $[1,0,0,4 ; \quad 0,1,0,-1 ; \quad 0,0,1,2 ; \quad 0]$

3.63. $5:[1,0 ; \quad 0,1],[1,1 ; \quad 0,0],[1,0 ; \quad 0,0],[0,1 ; \quad 0,0], 0$

3.64. 16

3.65. (a) $[1,0,0 ; \quad 0,0,1 ; \quad 0,1,0],[1,0,0 ; \quad 0,3,0 ; \quad 0,0,1],[1,0,2 ; \quad 0,1,0 ; \quad 0,0,1]$,

(b) $R_{2} \leftrightarrow R_{3} ; \quad \frac{1}{3} R_{2} \rightarrow R_{2} ; \quad-2 R_{3}+R_{1} \rightarrow R_{1}$; each $E_{i}^{\prime}=E_{i}^{-1}$,

(c) $C_{2} \leftrightarrow C_{3}, 3 C_{2} \rightarrow C_{2}, 2 C_{3}+C_{1} \rightarrow C_{1}, \quad$ (d) each $F_{i}=E_{i}^{T}$.

3.66. $A=[1,0 ; \quad 3,1][1,0 ; \quad 0,-2][1,2 ; \quad 0,1], \quad B$ is not invertible,

$C=\left[1,0 ; \quad-\frac{3}{2}, 1\right][1,0 ; \quad 0,2][1,6 ; \quad 0,1][2,0 ; \quad 0,1]$,

$D=\left[\begin{array}{llll}100 ; & 010 ; & 301\end{array}\right][100 ; \quad 010 ; \quad 021][100 ; \quad 013 ; \quad 001][120 ; \quad 010 ; \quad 001]$

3.67. $A^{-1}=[-8,12,-5 ; \quad-5,7,-3 ; \quad 1,-2,1], \quad B$ has no inverse,

$C^{-1}=\left[\frac{29}{2},-\frac{17}{2}, \frac{7}{2} ; \quad-\frac{5}{2}, \frac{3}{2},-\frac{1}{2} ; \quad 3,-2,1\right], \quad D^{-1}=[8,-3,-1 ; \quad-5,2,1 ; \quad 10,-4,-1]$

3.68. $A^{-1}=[1,-1,1,-1, \ldots ; \quad 0,1,-1,1,-1, \ldots ; \quad 0,0,1,-1,1,-1,1, \ldots ; \quad \ldots ; \quad \ldots ; \quad 0, \ldots 0,1]$ $B^{-1}$ has 1 's on diagonal, -1 's on superdiagonal, and 0 's elsewhere.

3.69. (a) $[100 ; \quad 310 ; 211][1,-1,-1 ; \quad 0,-1,1 ; \quad 0,0,-1]$,

(b) $[100 ; 210 ; 351][1,3,-1 ; \quad 0,-1,3 ; \quad 0,0,-10]$,

(c) $\left[100 ; 210 ; \quad \frac{3}{2}, \frac{1}{2}, 1\right]\left[2,3,6 ; \quad 0,1,-3 ; \quad 0,0,-\frac{7}{2}\right]$,

(d) There is no $L U$ decomposition.

3.70. $X_{1}=[1,1,-1]^{T}, \quad B_{2}=[2,2,0]^{T}, \quad X_{2}=[6,4,0]^{T}, \quad B_{3}=[8,6,0]^{T}, \quad X_{3}=[22,16,-2]^{T}$, $B_{4}=[30,22,-2]^{T}, \quad X_{4}=[86,62,-6]^{T}$

3.71. $B=[100 ; \quad 210 ; \quad 351] \operatorname{diag}(1,-1,-10)[1,3,-1 ; \quad 0,1,3 ; \quad 0,0,1]$

3.73. Replace $R_{i}$ by $-k R_{j}+\left(1 / k^{\prime}\right) R_{i}$.

3.75. (c) $f(A)=\left(e\left(A^{T}\right)\right)^{T}=\left(E A^{T}\right)^{T}=\left(A^{T}\right)^{T} E^{T}=A F$

3.76. (a) $A=I A I$. (b) If $A=P B Q$, then $B=P^{-1} A Q^{-1}$.

(c) If $A=P B Q$ and $B=P^{\prime} C Q^{\prime}$, then $A=\left(P P^{\prime}\right) C\left(Q^{\prime} Q\right)$.

\section*{Vector Spaces}
\subsection*{4.1 Introduction}
This chapter introduces the underlying structure of linear algebra, that of a finite-dimensional vector space. The definition of a vector space $V$, whose elements are called vectors, involves an arbitrary field $K$, whose elements are called scalars. The following notation will be used (unless otherwise stated or implied):

$$
\begin{aligned}
V & \text { the given vector space } \\
u, v, w & \text { vectors in } V \\
K & \text { the given number field } \\
a, b, c, \text { or } k & \text { scalars in } K
\end{aligned}
$$

Almost nothing essential is lost if the reader assumes that $K$ is the real field $\mathbf{R}$ or the complex field $\mathbf{C}$.

The reader might suspect that the real line $\mathbf{R}$ has "dimension" one, the cartesian plane $\mathbf{R}^{2}$ has "dimension" two, and the space $\mathbf{R}^{3}$ has "dimension" three. This chapter formalizes the notion of "dimension," and this definition will agree with the reader's intuition.

Throughout this text, we will use the following set notation:

$$
\begin{array}{rl}
a \in A & \text { Element } a \text { belongs to set } A \\
a, b \in A & \text { Elements } a \text { and } b \text { belong to } A \\
\forall x \in A & \text { For every } x \text { in } A \\
\exists x \in A & \text { There exists an } x \text { in } A \\
A \subseteq B & A \text { is a subset of } B \\
A \cap B & \text { Intersection of } A \text { and } B \\
A \cup B & \text { Union of } A \text { and } B \\
\emptyset & \text { Empty set }
\end{array}
$$

\subsection*{4.2 Vector Spaces}
The following defines the notion of a vector space $V$ where $K$ is the field of scalars.

DEFINITION: Let $V$ be a nonempty set with two operations:

(i) Vector Addition: This assigns to any $u, v \in V$ a sum $u+v$ in $V$.

(ii) Scalar Multiplication: This assigns to any $u \in V, k \in K$ a product $k u \in V$.

Then $V$ is called a vector space (over the field $K$ ) if the following axioms hold for any vectors $u, v, w \in V$ :\\
$\left[\mathrm{A}_{1}\right] \quad(u+v)+w=u+(v+w)$

$\left[A_{2}\right]$ There is a vector in $V$, denoted by 0 and called the zero vector, such that, for any $u \in V$,

$$
u+0=0+u=u
$$

$\left[\mathrm{A}_{3}\right]$ For each $u \in V$, there is a vector in $V$, denoted by $-u$, and called the negative of $u$, such that

$\left[\mathrm{A}_{4}\right] \quad u+v=v+u$.

$$
u+(-u)=(-u)+u=0
$$

$\left[\mathrm{M}_{1}\right] \quad k(u+v)=k u+k v$, for any scalar $k \in K$.

$\left[\mathrm{M}_{2}\right] \quad(a+b) u=a u+b u$, for any scalars $a, b \in K$.

$\left[\mathrm{M}_{3}\right] \quad(a b) u=a(b u)$, for any scalars $a, b \in K$.

$\left[\mathrm{M}_{4}\right] \quad 1 u=u$, for the unit scalar $1 \in K$.

The above axioms naturally split into two sets (as indicated by the labeling of the axioms). The first four are concerned only with the additive structure of $V$ and can be summarized by saying $V$ is a commutative group under addition. This means

(a) Any sum $v_{1}+v_{2}+\cdots+v_{m}$ of vectors requires no parentheses and does not depend on the order of the summands.

(b) The zero vector 0 is unique, and the negative $-u$ of a vector $u$ is unique.

(c) (Cancellation Law) If $u+w=v+w$, then $u=v$.

Also, subtraction in $V$ is defined by $u-v=u+(-v)$, where $-v$ is the unique negative of $v$.

On the other hand, the remaining four axioms are concerned with the "action" of the field $K$ of scalars on the vector space $V$. Using these additional axioms, we prove (Problem 4.2) the following simple properties of a vector space.

THEOREM 4.1: $\quad$ Let $V$ be a vector space over a field $K$.

(i) For any scalar $k \in K$ and $0 \in V, k 0=0$.

(ii) For $0 \in K$ and any vector $u \in V, 0 u=0$.

(iii) If $k u=0$, where $k \in K$ and $u \in V$, then $k=0$ or $u=0$.

(iv) For any $k \in K$ and any $u \in V,(-k) u=k(-u)=-k u$.

\subsection*{4.3 Examples of Vector Spaces}
This section lists important examples of vector spaces that will be used throughout the text.

\section*{Space $\boldsymbol{K}^{\boldsymbol{n}}$}
Let $K$ be an arbitrary field. The notation $K^{n}$ is frequently used to denote the set of all $n$-tuples of elements in $K$. Here $K^{n}$ is a vector space over $K$ using the following operations:

(i) Vector Addition: $\left(a_{1}, a_{2}, \ldots, a_{n}\right)+\left(b_{1}, b_{2}, \ldots, b_{n}\right)=\left(a_{1}+b_{1}, a_{2}+b_{2}, \ldots, a_{n}+b_{n}\right)$

(ii) Scalar Multiplication: $k\left(a_{1}, a_{2}, \ldots, a_{n}\right)=\left(k a_{1}, k a_{2}, \ldots, k a_{n}\right)$

The zero vector in $K^{n}$ is the $n$-tuple of zeros,

$$
0=(0,0, \ldots, 0)
$$

and the negative of a vector is defined by

$$
-\left(a_{1}, a_{2}, \ldots, a_{n}\right)=\left(-a_{1},-a_{2}, \ldots,-a_{n}\right)
$$

Observe that these are the same as the operations defined for $\mathbf{R}^{n}$ in Chapter 1. The proof that $K^{n}$ is a vector space is identical to the proof of Theorem 1.1, which we now regard as stating that $\mathbf{R}^{n}$ with the operations defined there is a vector space over $\mathbf{R}$.

\section*{Polynomial Space $\boldsymbol{P}(\boldsymbol{t})$}
Let $\mathbf{P}(t)$ denote the set of all polynomials of the form

$$
p(t)=a_{0}+a_{1} t+a_{2} t^{2}+\cdots+a_{s} t^{s} \quad(s=1,2, \ldots)
$$

where the coefficients $a_{i}$ belong to a field $K$. Then $\mathbf{P}(t)$ is a vector space over $K$ using the following operations:

(i) Vector Addition: Here $p(t)+q(t)$ in $\mathbf{P}(t)$ is the usual operation of addition of polynomials.

(ii) Scalar Multiplication: Here $k p(t)$ in $\mathbf{P}(t)$ is the usual operation of the product of a scalar $k$ and a polynomial $p(t)$.

The zero polynomial 0 is the zero vector in $\mathbf{P}(t)$.

\section*{Polynomial Space $\boldsymbol{P}_{\boldsymbol{n}}(\boldsymbol{t})$}
Let $\mathbf{P}_{n}(t)$ denote the set of all polynomials $p(t)$ over a field $K$, where the degree of $p(t)$ is less than or equal to $n$; that is,

$$
p(t)=a_{0}+a_{1} t+a_{2} t^{2}+\cdots+a_{s} t^{s}
$$

where $s \leq n$. Then $\mathbf{P}_{n}(t)$ is a vector space over $K$ with respect to the usual operations of addition of polynomials and of multiplication of a polynomial by a constant (just like the vector space $\mathbf{P}(t)$ above). We include the zero polynomial 0 as an element of $\mathbf{P}_{n}(t)$, even though its degree is undefined.

\section*{Matrix Space $\boldsymbol{M}_{m, n}$}
The notation $\mathbf{M}_{m, n}$, or simply $\mathbf{M}$, will be used to denote the set of all $m \times n$ matrices with entries in a field $K$. Then $\mathbf{M}_{m, n}$ is a vector space over $K$ with respect to the usual operations of matrix addition and scalar multiplication of matrices, as indicated by Theorem 2.1.

\section*{Function Space $\boldsymbol{F}(\boldsymbol{X})$}
Let $X$ be a nonempty set and let $K$ be an arbitrary field. Let $F(X)$ denote the set of all functions of $X$ into $K$. [Note that $F(X)$ is nonempty, because $X$ is nonempty.] Then $F(X)$ is a vector space over $K$ with respect to the following operations:

(i) Vector Addition: The sum of two functions $f$ and $g$ in $F(X)$ is the function $f+g$ in $F(X)$ defined by

$$
(f+g)(x)=f(x)+g(x) \quad \forall x \in X
$$

(ii) Scalar Multiplication: The product of a scalar $k \in K$ and a function $f$ in $F(X)$ is the function $k f$ in $F(X)$ defined by

$$
(k f)(x)=k f(x) \quad \forall x \in X
$$

The zero vector in $F(X)$ is the zero function $\mathbf{0}$, which maps every $x \in X$ into the zero element $0 \in K$;

$$
\mathbf{0}(x)=0 \quad \forall x \in X
$$

Also, for any function $f$ in $F(X)$, negative of $f$ is the function $-f$ in $F(X)$ defined by

$$
(-f)(x)=-f(x) \quad \forall x \in X
$$

\section*{Fields and Subfields}
Suppose a field $E$ is an extension of a field $K$; that is, suppose $E$ is a field that contains $K$ as a subfield. Then $E$ may be viewed as a vector space over $K$ using the following operations:

(i) Vector Addition: Here $u+v$ in $E$ is the usual addition in $E$.

(ii) Scalar Multiplication: Here $k u$ in $E$, where $k \in K$ and $u \in E$, is the usual product of $k$ and $u$ as elements of $E$.

That is, the eight axioms of a vector space are satisfied by $E$ and its subfield $K$ with respect to the above two operations.

\subsection*{4.4 Linear Combinations, Spanning Sets}
Let $V$ be a vector space over a field $K$. A vector $v$ in $V$ is a linear combination of vectors $u_{1}, u_{2}, \ldots, u_{m}$ in

$V$ if there exist scalars $a_{1}, a_{2}, \ldots, a_{m}$ in $K$ such that

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{m} u_{m}
$$

Alternatively, $v$ is a linear combination of $u_{1}, u_{2}, \ldots, u_{m}$ if there is a solution to the vector equation

$$
v=x_{1} u_{1}+x_{2} u_{2}+\cdots+x_{m} u_{m}
$$

where $x_{1}, x_{2}, \ldots, x_{m}$ are unknown scalars.

EXAMPLE 4.1 (Linear Combinations in $\mathbf{R}^{n}$ ) Suppose we want to express $v=(3,7,-4)$ in $\mathbf{R}^{3}$ as a linear combination of the vectors

$$
u_{1}=(1,2,3), \quad u_{2}=(2,3,7), \quad u_{3}=(3,5,6)
$$

We seek scalars $x, y, z$ such that $v=x u_{1}+y u_{2}+z u_{3}$; that is,

$$
\left[\begin{array}{r}
3 \\
3 \\
-4
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+y\left[\begin{array}{l}
2 \\
3 \\
7
\end{array}\right]+z\left[\begin{array}{l}
3 \\
5 \\
6
\end{array}\right] \quad \text { or } \quad \begin{array}{r}
x+2 y+3 z=3 \\
2 x+3 y+5 z=7 \\
3 x+7 y+6 z=-4
\end{array}
$$

(For notational convenience, we have written the vectors in $\mathbf{R}^{3}$ as columns, because it is then easier to find the equivalent system of linear equations.) Reducing the system to echelon form yields

$$
\begin{aligned}
& x+2 y+3 z=3 \quad x+2 y+3 z=3 \\
& -y-z=1 \text { and then } \quad-y-z=1 \\
& y-3 z=-13 \quad-4 z=-12
\end{aligned}
$$

Back-substitution yields the solution $x=2, \quad y=-4, \quad z=3$. Thus, $\quad v=2 u_{1}-4 u_{2}+3 u_{3}$.

Remark: Generally speaking, the question of expressing a given vector $v$ in $K^{n}$ as a linear combination of vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $K^{n}$ is equivalent to solving a system $A X=B$ of linear equations, where $v$ is the column $B$ of constants, and the $u$ 's are the columns of the coefficient matrix $A$. Such a system may have a unique solution (as above), many solutions, or no solution. The last case-no solution - means that $v$ cannot be written as a linear combination of the $u$ 's.

EXAMPLE 4.2 (Linear combinations in $\mathbf{P}(t)$ ) Suppose we want to express the polynomial $v=3 t^{2}+5 t-5$ as a linear combination of the polynomials

$$
p_{1}=t^{2}+2 t+1, \quad p_{2}=2 t^{2}+5 t+4, \quad p_{3}=t^{2}+3 t+6
$$

We seek scalars $x, y$, $z$ such that $v=x p_{1}+y p_{2}+z p_{3}$; that is,


\begin{equation*}
3 t^{2}+5 t-5=x\left(t^{2}+2 t+1\right)+y\left(2 t^{2}+5 t+4\right)+z\left(t^{2}+3 t+6\right) \tag{*}
\end{equation*}


There are two ways to proceed from here.

(1) Expand the right-hand side of $\left(^{*}\right)$ obtaining:

$$
\begin{aligned}
3 t^{2}+5 t-5 & =x t^{2}+2 x t+x+2 y t^{2}+5 y t+4 y+z t^{2}+3 z t+6 z \\
& =(x+2 y+z) t^{2}+(2 x+5 y+3 z) t+(x+4 y+6 z)
\end{aligned}
$$

Set coefficients of the same powers of $t$ equal to each other, and reduce the system to echelon form:

$$
\begin{array}{rlrlrl}
x+2 y+z & =3 & & & x \\
2 x+5 y+3 z & =5 & \text { or } & x+z & =3 \\
x+4 y+6 z & =-5 & y+z & =-1 \\
x+5 y & = & \text { or } & x+2 y+z & =3 \\
& 2 y+5 z & =-8 & y+z & =-1 \\
& & 3 z & =-6
\end{array}
$$

The system is in triangular form and has a solution. Back-substitution yields the solution $x=3, y=1, z=-2$. Thus,

$v=3 p_{1}+p_{2}-2 p_{3}$

(2) The equation $(*)$ is actually an identity in the variable $t$; that is, the equation holds for any value of $t$. We can obtain three equations in the unknowns $x, y, z$ by setting $t$ equal to any three values. For example,

Set $t=0$ in (1) to obtain: $\quad x+4 y+6 z=-5$

Set $t=1$ in (1) to obtain: $\quad 4 x+11 y+10 z=3$

Set $t=-1$ in (1) to obtain: $\quad y+4 z=-7$

Reducing this system to echelon form and solving by back-substitution again yields the solution $x=3, y=1$, $z=-2$. Thus (again), $v=3 p_{1}+p_{2}-2 p_{3}$.

\section*{Spanning Sets}
Let $V$ be a vector space over $K$. Vectors $u_{1}, u_{2}, \ldots, u_{m}$ in $V$ are said to span $V$ or to form a spanning set of $V$ if every $v$ in $V$ is a linear combination of the vectors $u_{1}, u_{2}, \ldots, u_{m}$-that is, if there exist scalars $a_{1}, a_{2}, \ldots, a_{m}$ in $K$ such that

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{m} u_{m}
$$

The following remarks follow directly from the definition.

Remark 1: Suppose $u_{1}, u_{2}, \ldots, u_{m}$ span $V$. Then, for any vector $w$, the set $w, u_{1}, u_{2}, \ldots, u_{m}$ also spans $V$.

Remark 2: Suppose $u_{1}, u_{2}, \ldots, u_{m}$ span $V$ and suppose $u_{k}$ is a linear combination of some of the other $u$ 's. Then the $u$ 's without $u_{k}$ also span $V$.

Remark 3: Suppose $u_{1}, u_{2}, \ldots, u_{m}$ span $V$ and suppose one of the $u$ 's is the zero vector. Then the $u$ 's without the zero vector also span $V$.

EXAMPLE 4.3 Consider the vector space $V=\mathbf{R}^{3}$.

(a) We claim that the following vectors form a spanning set of $\mathbf{R}^{3}$ :

$$
e_{1}=(1,0,0), \quad e_{2}=(0,1,0), \quad e_{3}=(0,0,1)
$$

Specifically, if $v=(a, b, c)$ is any vector in $\mathbf{R}^{3}$, then

$$
v=a e_{1}+b e_{2}+c e_{3}
$$

For example, $v=(5,-6,2)=-5 e_{1}-6 e_{2}+2 e_{3}$.

(b) We claim that the following vectors also form a spanning set of $\mathbf{R}^{3}$ :

$$
w_{1}=(1,1,1), \quad w_{2}=(1,1,0), \quad w_{3}=(1,0,0)
$$

Specifically, if $v=(a, b, c)$ is any vector in $\mathbf{R}^{3}$, then (Problem 4.62)

$$
v=(a, b, c)=c w_{1}+(b-c) w_{2}+(a-b) w_{3}
$$

For example, $v=(5,-6,2)=2 w_{1}-8 w_{2}+11 w_{3}$.

(c) One can show (Problem 3.24) that $v=(2,7,8)$ cannot be written as a linear combination of the vectors

$$
u_{1}=(1,2,3), \quad u_{2}=(1,3,5), \quad u_{3}=(1,5,9)
$$

Accordingly, $u_{1}, u_{2}, u_{3}$ do not span $\mathbf{R}^{3}$.

EXAMPLE 4.4 Consider the vector space $V=\mathbf{P}_{n}(t)$ consisting of all polynomials of degree $\leq n$.

(a) Clearly every polynomial in $\mathbf{P}_{n}(t)$ can be expressed as a linear combination of the $n+1$ polynomials

$$
\begin{array}{llllll}
1, & t, & t^{2}, & t^{3}, & \ldots, & t^{n}
\end{array}
$$

Thus, these powers of $t$ (where $1=t^{0}$ ) form a spanning set for $\mathbf{P}_{n}(t)$.

(b) One can also show that, for any scalar $c$, the following $n+1$ powers of $t-c$,

$$
1, \quad t-c, \quad(t-c)^{2}, \quad(t-c)^{3}, \quad \ldots, \quad(t-c)^{n}
$$

(where $(t-c)^{0}=1$ ), also form a spanning set for $\mathbf{P}_{n}(t)$.

EXAMPLE 4.5 Consider the vector space $\mathbf{M}=\mathbf{M}_{2,2}$ consisting of all $2 \times 2$ matrices, and consider the following four matrices in $\mathbf{M}$ :

$$
E_{11}=\left[\begin{array}{cc}
1 & 0 \\
0 & 0
\end{array}\right], \quad E_{12}=\left[\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right], \quad E_{21}=\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right], \quad E_{22}=\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]
$$

Then clearly any matrix $A$ in $\mathbf{M}$ can be written as a linear combination of the four matrices. For example,

$$
A=\left[\begin{array}{rr}
5 & -6 \\
7 & 8
\end{array}\right]=5 E_{11}-6 E_{12}+7 E_{21}+8 E_{22}
$$

Accordingly, the four matrices $E_{11}, E_{12}, E_{21}, E_{22}$ span $\mathbf{M}$.

\subsection*{4.5 Subspaces}
This section introduces the important notion of a subspace.

DEFINITION: $\quad$ Let $V$ be a vector space over a field $K$ and let $W$ be a subset of $V$. Then $W$ is a subspace of $V$ if $W$ is itself a vector space over $K$ with respect to the operations of vector addition and scalar multiplication on $V$.

The way in which one shows that any set $W$ is a vector space is to show that $W$ satisfies the eight axioms of a vector space. However, if $W$ is a subset of a vector space $V$, then some of the axioms automatically hold in $W$, because they already hold in $V$. Simple criteria for identifying subspaces follow.

THEOREM 4.2: $\quad$ Suppose $W$ is a subset of a vector space $V$. Then $W$ is a subspace of $V$ if the following two conditions hold:

(a) The zero vector 0 belongs to $W$.

(b) For every $u, v \in W, k \in K$ : (i) The sum $u+v \in W$. (ii) The multiple $k u \in W$.

Property (i) in (b) states that $W$ is closed under vector addition, and property (ii) in (b) states that $W$ is closed under scalar multiplication. Both properties may be combined into the following equivalent single statement:

( $\left.b^{\prime}\right)$ For every $u, v \in W, a, b \in K$, the linear combination $a u+b v \in W$.

Now let $V$ be any vector space. Then $V$ automatically contains two subspaces: the set $\{0\}$ consisting of the zero vector alone and the whole space $V$ itself. These are sometimes called the trivial subspaces of $V$. Examples of nontrivial subspaces follow.

EXAMPLE 4.6 Consider the vector space $V=\mathbf{R}^{3}$.

(a) Let $U$ consist of all vectors in $\mathbf{R}^{3}$ whose entries are equal; that is,

$$
U=\{(a, b, c): a=b=c\}
$$

For example, $(1,1,1),(-3,-3,-3),(7,7,7),(-2,-2,-2)$ are vectors in $U$. Geometrically, $U$ is the line through the origin $O$ and the point $(1,1,1)$ as shown in Fig. 4-1(a). Clearly $0=(0,0,0)$ belongs to $U$, because\\
all entries in 0 are equal. Further, suppose $u$ and $v$ are arbitrary vectors in $U$, say, $u=(a, a, a)$ and $v=(b, b, b)$. Then, for any scalar $k \in \mathbf{R}$, the following are also vectors in $U$ :

$$
u+v=(a+b, a+b, a+b) \quad \text { and } \quad k u=(k a, k a, k a)
$$

Thus, $U$ is a subspace of $\mathbf{R}^{3}$.

(b) Let $W$ be any plane in $\mathbf{R}^{3}$ passing through the origin, as pictured in Fig. 4-1(b). Then $0=(0,0,0)$ belongs to $W$, because we assumed $W$ passes through, the origin $O$. Further, suppose $u$ and $v$ are vectors in $W$. Then $u$ and $v$ may be viewed as arrows in the plane $W$ emanating from the origin $O$, as in Fig. 4-1(b). The sum $u+v$ and any multiple $k u$ of $u$ also lie in the plane $W$. Thus, $W$ is a subspace of $\mathbf{R}^{3}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-125}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-125(1)}
\end{center}

(b)

Figure 4-1

\section*{EXAMPLE 4.7}
(a) Let $V=\mathbf{M}_{n, n}$, the vector space of $n \times n$ matrices. Let $W_{1}$ be the subset of all (upper) triangular matrices and let $W_{2}$ be the subset of all symmetric matrices. Then $W_{1}$ is a subspace of $V$, because $W_{1}$ contains the zero matrix 0 and $W_{1}$ is closed under matrix addition and scalar multiplication; that is, the sum and scalar multiple of such triangular matrices are also triangular. Similarly, $W_{2}$ is a subspace of $V$.

(b) Let $V=\mathbf{P}(t)$, the vector space $\mathbf{P}(t)$ of polynomials. Then the space $\mathbf{P}_{n}(t)$ of polynomials of degree at most $n$ may be viewed as a subspace of $\mathbf{P}(t)$. Let $\mathbf{Q}(t)$ be the collection of polynomials with only even powers of $t$. For example, the following are polynomials in $\mathbf{Q}(t)$ :

$$
p_{1}=3+4 t^{2}-5 t^{6} \quad \text { and } \quad p_{2}=6-7 t^{4}+9 t^{6}+3 t^{12}
$$

(We assume that any constant $k=k t^{0}$ is an even power of $t$.) Then $\mathbf{Q}(t)$ is a subspace of $\mathbf{P}(t)$.

(c) Let $V$ be the vector space of real-valued functions. Then the collection $W_{1}$ of continuous functions and the collection $W_{2}$ of differentiable functions are subspaces of $V$.

\section*{Intersection of Subspaces}
Let $U$ and $W$ be subspaces of a vector space $V$. We show that the intersection $U \cap W$ is also a subspace of $V$. Clearly, $0 \in U$ and $0 \in W$, because $U$ and $W$ are subspaces; whence $0 \in U \cap W$. Now suppose $u$ and $v$ belong to the intersection $U \cap W$. Then $u, v \in U$ and $u, v \in W$. Further, because $U$ and $W$ are subspaces, for any scalars $a, b \in K$,

$$
a u+b v \in U \quad \text { and } \quad a u+b v \in W
$$

Thus, $a u+b v \in U \cap W$. Therefore, $U \cap W$ is a subspace of $V$.

The above result generalizes as follows.

THEOREM 4.3: $\quad$ The intersection of any number of subspaces of a vector space $V$ is a subspace of $V$.

\section*{Solution Space of a Homogeneous System}
Consider a system $A X=B$ of linear equations in $n$ unknowns. Then every solution $u$ may be viewed as a vector in $K^{n}$. Thus, the solution set of such a system is a subset of $K^{n}$. Now suppose the system is homogeneous; that is, suppose the system has the form $A X=0$. Let $W$ be its solution set. Because $A 0=0$, the zero vector $0 \in W$. Moreover, suppose $u$ and $v$ belong to $W$. Then $u$ and $v$ are solutions of $A X=0$, or, in other words, $A u=0$ and $A v=0$. Therefore, for any scalars $a$ and $b$, we have

$$
A(a u+b v)=a A u+b A v=a 0+b 0=0+0=0
$$

Thus, $a u+b v$ belongs to $W$, because it is a solution of $A X=0$. Accordingly, $W$ is a subspace of $K^{n}$.

We state the above result formally.

THEOREM 4.4: $\quad$ The solution set $W$ of a homogeneous system $A X=0$ in $n$ unknowns is a subspace of $K^{n}$.

We emphasize that the solution set of a nonhomogeneous system $A X=B$ is not a subspace of $K^{n}$. In fact, the zero vector 0 does not belong to its solution set.

\subsection*{4.6 Linear Spans, Row Space of a Matrix}
Suppose $u_{1}, u_{2}, \ldots, u_{m}$ are any vectors in a vector space $V$. Recall (Section 4.4) that any vector of the form $a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{m} u_{m}$, where the $a_{i}$ are scalars, is called a linear combination of $u_{1}, u_{2}, \ldots, u_{m}$. The collection of all such linear combinations, denoted by

$$
\operatorname{span}\left(u_{1}, u_{2}, \ldots, u_{m}\right) \quad \text { or } \quad \operatorname{span}\left(u_{i}\right)
$$

is called the linear span of $u_{1}, u_{2}, \ldots, u_{m}$.

Clearly the zero vector 0 belongs to $\operatorname{span}\left(u_{i}\right)$, because

$$
0=0 u_{1}+0 u_{2}+\cdots+0 u_{m}
$$

Furthermore, suppose $v$ and $v^{\prime}$ belong to $\operatorname{span}\left(u_{i}\right)$, say,

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{m} u_{m} \quad \text { and } \quad v^{\prime}=b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{m} u_{m}
$$

Then,

$$
v+v^{\prime}=\left(a_{1}+b_{1}\right) u_{1}+\left(a_{2}+b_{2}\right) u_{2}+\cdots+\left(a_{m}+b_{m}\right) u_{m}
$$

and, for any scalar $k \in K$,

$$
k v=k a_{1} u_{1}+k a_{2} u_{2}+\cdots+k a_{m} u_{m}
$$

Thus, $v+v^{\prime}$ and $k v$ also belong to $\operatorname{span}\left(u_{i}\right)$. Accordingly, $\operatorname{span}\left(u_{i}\right)$ is a subspace of $V$.

More generally, for any subset $S$ of $V, \operatorname{span}(S)$ consists of all linear combinations of vectors in $S$ or, when $S=\phi, \operatorname{span}(S)=\{0\}$. Thus, in particular, $S$ is a spanning set (Section 4.4) of $\operatorname{span}(S)$.

The following theorem, which was partially proved above, holds.

THEOREM 4.5: $\quad$ Let $S$ be a subset of a vector space $V$.

(i) Then $\operatorname{span}(S)$ is a subspace of $V$ that contains $S$.

(ii) If $W$ is a subspace of $V$ containing $S$, then $\operatorname{span}(S) \subseteq W$.

Condition (ii) in theorem 4.5 may be interpreted as saying that $\operatorname{span}(S)$ is the "smallest" subspace of $V$ containing $S$.

EXAMPLE 4.8 Consider the vector space $V=\mathbf{R}^{3}$.

(a) Let $u$ be any nonzero vector in $\mathbf{R}^{3}$. Then $\operatorname{span}(u)$ consists of all scalar multiples of $u$. Geometrically, $\operatorname{span}(u)$ is the line through the origin $O$ and the endpoint of $u$, as shown in Fig. 4-2(a).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-127(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-127}
\end{center}

(b)

Figure 4-2

(b) Let $u$ and $v$ be vectors in $\mathbf{R}^{3}$ that are not multiples of each other. Then $\operatorname{span}(u, v)$ is the plane through the origin $O$ and the endpoints of $u$ and $v$ as shown in Fig. 4-2(b).

(c) Consider the vectors $e_{1}=(1,0,0), e_{2}=(0,1,0), e_{3}=(0,0,1)$ in $\mathbf{R}^{3}$. Recall [Example 4.1(a)] that every vector in $\mathbf{R}^{3}$ is a linear combination of $e_{1}, e_{2}, e_{3}$. That is, $e_{1}, e_{2}, e_{3}$ form a spanning set of $\mathbf{R}^{3}$. Accordingly, $\operatorname{span}\left(e_{1}, e_{2}, e_{3}\right)=\mathbf{R}^{3}$.

\section*{Row Space of a Matrix}
Let $A=\left[a_{i j}\right]$ be an arbitrary $m \times n$ matrix over a field $K$. The rows of $A$,

$$
R_{1}=\left(a_{11}, a_{12}, \ldots, a_{1 n}\right), \quad R_{2}=\left(a_{21}, a_{22}, \ldots, a_{2 n}\right), \quad \ldots, \quad R_{m}=\left(a_{m 1}, a_{m 2}, \ldots, a_{m n}\right)
$$

may be viewed as vectors in $K^{n}$; hence, they span a subspace of $K^{n}$ called the row space of $A$ and denoted by $\operatorname{rowsp}(\mathrm{A})$. That is,

$$
\operatorname{rowsp}(A)=\operatorname{span}\left(R_{1}, R_{2}, \ldots, R_{m}\right)
$$

Analagously, the columns of $A$ may be viewed as vectors in $K^{m}$ called the column space of $A$ and denoted by colsp(A). Observe that $\operatorname{colsp}(A)=\operatorname{rowsp}\left(A^{T}\right)$.

Recall that matrices $A$ and $B$ are row equivalent, written $A \sim B$, if $B$ can be obtained from $A$ by a sequence of elementary row operations. Now suppose $M$ is the matrix obtained by applying one of the following elementary row operations on a matrix $A$ :\\
(1) Interchange $R_{i}$ and $R_{j}$,\\
(2) Replace $R_{i}$ by $k R_{i}$,\\
(3) Replace $R_{j}$ by $k R_{i}+R_{j}$

Then each row of $M$ is a row of $A$ or a linear combination of rows of $A$. Hence, the row space of $M$ is contained in the row space of $A$. On the other hand, we can apply the inverse elementary row operation on $M$ to obtain $A$; hence, the row space of $A$ is contained in the row space of $M$. Accordingly, $A$ and $M$ have the same row space. This will be true each time we apply an elementary row operation. Thus, we have proved the following theorem.

THEOREM 4.6: Row equivalent matrices have the same row space.

We are now able to prove (Problems 4.45-4.47) basic results on row equivalence (which first appeared as Theorems 3.7 and 3.8 in Chapter 3).

THEOREM 4.7: $\quad$ Suppose $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ are row equivalent echelon matrices with respective pivot entries

$$
a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}} \quad \text { and } \quad b_{1 k_{1}}, b_{2 k_{2}}, \ldots, b_{s k_{s}}
$$

Then $A$ and $B$ have the same number of nonzero rows - that is, $r=s$ - and their pivot entries are in the same positions-that is, $j_{1}=k_{1}, j_{2}=k_{2}, \ldots, j_{r}=k_{r}$.

THEOREM 4.8: $\quad$ Suppose $A$ and $B$ are row canonical matrices. Then $A$ and $B$ have the same row space if and only if they have the same nonzero rows.

COROLLARY 4.9: Every matrix $A$ is row equivalent to a unique matrix in row canonical form.

We apply the above results in the next example.

EXAMPLE 4.9 Consider the following two sets of vectors in $\mathbf{R}^{4}$ :

$$
\begin{gathered}
u_{1}=(1,2,-1,3), \quad u_{2}=(2,4,1,-2), \quad u_{3}=(3,6,3,-7) \\
w_{1}=(1,2,-4,11), \quad w_{2}=(2,4,-5,14)
\end{gathered}
$$

Let $U=\operatorname{span}\left(u_{i}\right)$ and $W=\operatorname{span}\left(w_{i}\right)$. There are two ways to show that $U=W$.

(a) Show that each $u_{i}$ is a linear combination of $w_{1}$ and $w_{2}$, and show that each $w_{i}$ is a linear combination of $u_{1}, u_{2}$, $u_{3}$. Observe that we have to show that six systems of linear equations are consistent.

(b) Form the matrix $A$ whose rows are $u_{1}, u_{2}, u_{3}$ and row reduce $A$ to row canonical form, and form the matrix $B$ whose rows are $w_{1}$ and $w_{2}$ and row reduce $B$ to row canonical form:

$$
\begin{aligned}
& A=\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
2 & 4 & 1 & -2 \\
3 & 6 & 3 & -7
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & -1 & 3 \\
0 & 0 & 3 & -8 \\
0 & 0 & 6 & -16
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & \frac{1}{3} \\
0 & 0 & 1 & -\frac{8}{3} \\
0 & 0 & 0 & 0
\end{array}\right] \\
& B=\left[\begin{array}{rrrr}
1 & 2 & -4 & 11 \\
2 & 4 & -5 & 14
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & -4 & 11 \\
0 & 0 & 3 & -8
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & \frac{1}{3} \\
0 & 0 & 1 & -\frac{8}{3}
\end{array}\right]
\end{aligned}
$$

Because the nonzero rows of the matrices in row canonical form are identical, the row spaces of $A$ and $B$ are equal. Therefore, $U=W$.

Clearly, the method in (b) is more efficient than the method in (a).

\subsection*{4.7 Linear Dependence and Independence}
Let $V$ be a vector space over a field $K$. The following defines the notion of linear dependence and independence of vectors over $K$. (One usually suppresses mentioning $K$ when the field is understood.) This concept plays an essential role in the theory of linear algebra and in mathematics in general.

DEFINITION: We say that the vectors $v_{1}, v_{2}, \ldots, v_{m}$ in $V$ are linearly dependent if there exist scalars $a_{1}, a_{2}, \ldots, a_{m}$ in $K$, not all of them 0 , such that

$$
a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{m} v_{m}=0
$$

Otherwise, we say that the vectors are linearly independent.

The above definition may be restated as follows. Consider the vector equation


\begin{equation*}
x_{1} v_{1}+x_{2} v_{2}+\cdots+x_{m} v_{m}=0 \tag{*}
\end{equation*}


where the $x$ 's are unknown scalars. This equation always has the zero solution $x_{1}=0$, $x_{2}=0, \ldots, x_{m}=0$. Suppose this is the only solution; that is, suppose we can show:

$$
x_{1} v_{1}+x_{2} v_{2}+\cdots+x_{m} v_{m}=0 \quad \text { implies } \quad x_{1}=0, \quad x_{2}=0, \quad \ldots, \quad x_{m}=0
$$

Then the vectors $v_{1}, v_{2}, \ldots, v_{m}$ are linearly independent, On the other hand, suppose the equation $(*)$ has a nonzero solution; then the vectors are linearly dependent.

A set $S=\left\{v_{1}, v_{2}, \ldots, v_{m}\right\}$ of vectors in $V$ is linearly dependent or independent according to whether the vectors $v_{1}, v_{2}, \ldots, v_{m}$ are linearly dependent or independent.

An infinite set $S$ of vectors is linearly dependent or independent according to whether there do or do not exist vectors $v_{1}, v_{2}, \ldots, v_{k}$ in $S$ that are linearly dependent.

Warning: The set $S=\left\{v_{1}, v_{2}, \ldots, v_{m}\right\}$ above represents a list or, in other words, a finite sequence of vectors where the vectors are ordered and repetition is permitted.

The following remarks follow directly from the above definition.

Remark 1: Suppose 0 is one of the vectors $v_{1}, v_{2}, \ldots, v_{m}$, say $v_{1}=0$. Then the vectors must be linearly dependent, because we have the following linear combination where the coefficient of $v_{1} \neq 0$ :

$$
1 v_{1}+0 v_{2}+\cdots+0 v_{m}=1 \cdot 0+0+\cdots+0=0
$$

Remark 2: Suppose $v$ is a nonzero vector. Then $v$, by itself, is linearly independent, because

$$
k v=0, \quad v \neq 0 \quad \text { implies } \quad k=0
$$

Remark 3: Suppose two of the vectors $v_{1}, v_{2}, \ldots, v_{m}$ are equal or one is a scalar multiple of the other, say $v_{1}=k v_{2}$. Then the vectors must be linearly dependent, because we have the following linear combination where the coefficient of $v_{1} \neq 0$ :

$$
v_{1}-k v_{2}+0 v_{3}+\cdots+0 v_{m}=0
$$

Remark 4: Two vectors $v_{1}$ and $v_{2}$ are linearly dependent if and only if one of them is a multiple of the other.

Remark 5: If the set $\left\{v_{1}, \ldots, v_{m}\right\}$ is linearly independent, then any rearrangement of the vectors $\left\{v_{i_{1}}, v_{i_{2}}, \ldots, v_{i_{m}}\right\}$ is also linearly independent.

Remark 6: If a set $S$ of vectors is linearly independent, then any subset of $S$ is linearly independent. Alternatively, if $S$ contains a linearly dependent subset, then $S$ is linearly dependent.

\section*{EXAMPLE 4.10}
(a) Let $u=(1,1,0), v=(1,3,2), w=(4,9,5)$. Then $u, v, w$ are linearly dependent, because

$$
3 u+5 v-2 w=3(1,1,0)+5(1,3,2)-2(4,9,5)=(0,0,0)=0
$$

(b) We show that the vectors $u=(1,2,3), v=(2,5,7), w=(1,3,5)$ are linearly independent. We form the vector equation $x u+y v+z w=0$, where $x, y, z$ are unknown scalars. This yields

$$
x\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+y\left[\begin{array}{l}
2 \\
5 \\
7
\end{array}\right]+z\left[\begin{array}{l}
1 \\
3 \\
5
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0
\end{array}\right] \quad \text { or } \quad \begin{array}{rlr}
x+2 y+z & =0 \\
2 x+5 y+3 z & =0 \\
3 x+7 y+5 z & =0
\end{array} \text { or } \quad \begin{aligned}
x+2 y+z & =0 \\
y+z & =0 \\
2 z & =0
\end{aligned}
$$

Back-substitution yields $x=0, y=0, z=0$. We have shown that

$$
x u+y v+z w=0 \quad \text { implies } \quad x=0, \quad y=0, \quad z=0
$$

Accordingly, $u, v, w$ are linearly independent.

(c) Let $V$ be the vector space of functions from $\mathbf{R}$ into $\mathbf{R}$. We show that the functions $f(t)=\sin t, g(t)=e^{t}$, $h(t)=t^{2}$ are linearly independent. We form the vector (function) equation $x f+y g+z h=0$, where $x, y, z$ are unknown scalars. This function equation means that, for every value of $t$,

$$
x \sin t+y e^{t}+z t^{2}=0
$$

Thus, in this equation, we choose appropriate values of $t$ to easily get $x=0, y=0, z=0$. For example,\\
(i) Substitute $t=0$\\
to obtain $x(0)+y(1)+z(0)=0$\\
or $\quad y=0$\\
(ii) Substitute $t=\pi$\\
to obtain $x(0)+0\left(e^{\pi}\right)+z\left(\pi^{2}\right)=0$\\
or $z=0$\\
(iii) Substitute $t=\pi / 2$\\
to obtain $x(1)+0\left(e^{\pi / 2}\right)+0\left(\pi^{2} / 4\right)=0$\\
or $\quad x=0$

We have shown

$$
x f+y g+z f=0 \quad \text { implies } \quad x=0, \quad y=0, \quad z=0
$$

Accordingly, $u, v, w$ are linearly independent.

\section*{Linear Dependence in $R^{3}$}
Linear dependence in the vector space $V=\mathbf{R}^{3}$ can be described geometrically as follows:

(a) Any two vectors $u$ and $v$ in $\mathbf{R}^{3}$ are linearly dependent if and only if they lie on the same line through the origin $O$, as shown in Fig. 4-3(a).

(b) Any three vectors $u, v, w$ in $\mathbf{R}^{3}$ are linearly dependent if and only if they lie on the same plane through the origin $O$, as shown in Fig. 4-3(b).

Later, we will be able to show that any four or more vectors in $\mathbf{R}^{3}$ are automatically linearly dependent.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-130}
\end{center}

(a) $u$ and $v$ are linearly dependent.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-130(1)}
\end{center}

(b) $u, v$, and $w$ are linearly dependent.

Figure 4-3

\section*{Linear Dependence and Linear Combinations}
The notions of linear dependence and linear combinations are closely related. Specifically, for more than one vector, we show that the vectors $v_{1}, v_{2}, \ldots, v_{m}$ are linearly dependent if and only if one of them is a linear combination of the others.

Suppose, say, $v_{i}$ is a linear combination of the others,

$$
v_{i}=a_{1} v_{1}+\cdots+a_{i-1} v_{i-1}+a_{i+1} v_{i+1}+\cdots+a_{m} v_{m}
$$

Then by adding $-v_{i}$ to both sides, we obtain

$$
a_{1} v_{1}+\cdots+a_{i-1} v_{i-1}-v_{i}+a_{i+1} v_{i+1}+\cdots+a_{m} v_{m}=0
$$

where the coefficient of $v_{i}$ is not 0 . Hence, the vectors are linearly dependent. Conversely, suppose the vectors are linearly dependent, say,

$$
b_{1} v_{1}+\cdots+b_{j} v_{j}+\cdots+b_{m} v_{m}=0, \quad \text { where } \quad b_{j} \neq 0
$$

Then we can solve for $v_{j}$ obtaining

$$
v_{j}=b_{j}^{-1} b_{1} v_{1}-\cdots-b_{j}^{-1} b_{j-1} v_{j-1}-b_{j}^{-1} b_{j+1} v_{j+1}-\cdots-b_{j}^{-1} b_{m} v_{m}
$$

and so $v_{j}$ is a linear combination of the other vectors.

We now state a slightly stronger statement than the one above. This result has many important consequences.

LEMMA 4.10: Suppose two or more nonzero vectors $v_{1}, v_{2}, \ldots, v_{m}$ are linearly dependent. Then one of the vectors is a linear combination of the preceding vectors; that is, there exists $k>1$ such that

$$
v_{k}=c_{1} v_{1}+c_{2} v_{2}+\cdots+c_{k-1} v_{k-1}
$$

\section*{Linear Dependence and Echelon Matrices}
Consider the following echelon matrix $A$, whose pivots have been circled:

$$
A=\left[\begin{array}{ccccccc}
0 & 2 & 3 & 4 & 5 & 6 & 7 \\
0 & 0 & 4 & 3 & 2 & 3 & 4 \\
0 & 0 & 0 & 0 & 7 & 8 & 9 \\
0 & 0 & 0 & 0 & 0 & 6 & 7 \\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Observe that the rows $R_{2}, R_{3}, R_{4}$ have 0 's in the second column below the nonzero pivot in $R_{1}$, and hence any linear combination of $R_{2}, R_{3}, R_{4}$ must have 0 as its second entry. Thus, $R_{1}$ cannot be a linear combination of the rows below it. Similarly, the rows $R_{3}$ and $R_{4}$ have 0 's in the third column below the nonzero pivot in $R_{2}$, and hence $R_{2}$ cannot be a linear combination of the rows below it. Finally, $R_{3}$ cannot be a multiple of $R_{4}$, because $R_{4}$ has a 0 in the fifth column below the nonzero pivot in $R_{3}$. Viewing the nonzero rows from the bottom up, $R_{4}, R_{3}, R_{2}, R_{1}$, no row is a linear combination of the preceding rows. Thus, the rows are linearly independent by Lemma 4.10.

The argument used with the above echelon matrix $A$ can be used for the nonzero rows of any echelon matrix. Thus, we have the following very useful result.

THEOREM 4.11: The nonzero rows of a matrix in echelon form are linearly independent.

\subsection*{4.8 Basis and Dimension}
First we state two equivalent ways to define a basis of a vector space $V$. (The equivalence is proved in Problem 4.28.)

DEFINITION A: A set $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of vectors is a basis of $V$ if it has the following two properties: (1) $S$ is linearly independent. (2) $S$ spans $V$.

DEFINITION B: $\quad$ A set $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of vectors is a basis of $V$ if every $v \in V$ can be written uniquely as a linear combination of the basis vectors.

The following is a fundamental result in linear algebra.

THEOREM 4.12: $\quad$ Let $V$ be a vector space such that one basis has $m$ elements and another basis has $n$ elements. Then $m=n$.

A vector space $V$ is said to be of finite dimension $n$ or $n$-dimensional, written

$$
\operatorname{dim} V=n
$$

if $V$ has a basis with $n$ elements. Theorem 4.12 tells us that all bases of $V$ have the same number of elements, so this definition is well defined.

The vector space $\{0\}$ is defined to have dimension 0 .

Suppose a vector space $V$ does not have a finite basis. Then $V$ is said to be of infinite dimension or to be infinite-dimensional.

The above fundamental Theorem 4.12 is a consequence of the following "replacement lemma" (proved in Problem 4.35).

LEMMA 4.13: Suppose $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ spans $V$, and suppose $\left\{w_{1}, w_{2}, \ldots, w_{m}\right\}$ is linearly independent. Then $m \leq n$, and $V$ is spanned by a set of the form

$$
\left\{w_{1}, w_{2}, \ldots, w_{m}, v_{i_{1}}, v_{i_{2}}, \ldots, v_{i_{n-m}}\right\}
$$

Thus, in particular, $n+1$ or more vectors in $V$ are linearly dependent.

Observe in the above lemma that we have replaced $m$ of the vectors in the spanning set of $V$ by the $m$ independent vectors and still retained a spanning set.

\section*{Examples of Bases}
This subsection presents important examples of bases of some of the main vector spaces appearing in this text.

(a) Vector space $K^{n}$ : Consider the following $n$ vectors in $K^{n}$ :

$$
e_{1}=(1,0,0,0, \ldots, 0,0), e_{2}=(0,1,0,0, \ldots, 0,0), \ldots, e_{n}=(0,0,0,0, \ldots, 0,1)
$$

These vectors are linearly independent. (For example, they form a matrix in echelon form.) Furthermore, any vector $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ in $K^{n}$ can be written as a linear combination of the above vectors. Specifically,

$$
v=a_{1} e_{1}+a_{2} e_{2}+\cdots+a_{n} e_{n}
$$

Accordingly, the vectors form a basis of $K^{n}$ called the usual or standard basis of $K^{n}$. Thus (as one might expect), $K^{n}$ has dimension $n$. In particular, any other basis of $K^{n}$ has $n$ elements.

(b) Vector space $\mathbf{M}=\mathbf{M}_{r, s}$ of all $r \times s$ matrices: The following six matrices form a basis of the vector space $\mathbf{M}_{2,3}$ of all $2 \times 3$ matrices over $K$ :

$$
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 1 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
1 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 1 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

More generally, in the vector space $\mathbf{M}=\mathbf{M}_{r, s}$ of all $r \times s$ matrices, let $E_{i j}$ be the matrix with $i j$-entry 1 and 0's elsewhere. Then all such matrices form a basis of $\mathbf{M}_{r, s}$ called the usual or standard basis of $\mathbf{M}_{r, s}$. Accordingly, $\operatorname{dim} \mathbf{M}_{r, s}=r s$.

(c) Vector space $\mathbf{P}_{n}(t)$ of all polynomials of degree $\leq n$ : The set $S=\left\{1, t, t^{2}, t^{3}, \ldots, t^{n}\right\}$ of $n+1$ polynomials is a basis of $\mathbf{P}_{n}(t)$. Specifically, any polynomial $f(t)$ of degree $\leq n$ can be expessed as a linear combination of these powers of $t$, and one can show that these polynomials are linearly independent. Therefore, $\operatorname{dim} \mathbf{P}_{n}(t)=n+1$.

(d) Vector space $\mathbf{P}(t)$ of all polynomials: Consider any finite set $S=\left\{f_{1}(t), f_{2}(t), \ldots, f_{m}(t)\right\}$ of polynomials in $\mathbf{P}(t)$, and let $m$ denote the largest of the degrees of the polynomials. Then any polynomial $g(t)$ of degree exceeding $m$ cannot be expressed as a linear combination of the elements of $S$. Thus, $S$ cannot be a basis of $\mathbf{P}(t)$. This means that the dimension of $\mathbf{P}(t)$ is infinite. We note that the infinite set $S^{\prime}=\left\{1, t, t^{2}, t^{3}, \ldots\right\}$, consisting of all the powers of $t$, spans $\mathbf{P}(t)$ and is linearly independent. Accordingly, $S^{\prime}$ is an infinite basis of $\mathbf{P}(t)$.

\section*{Theorems on Bases}
The following three theorems (proved in Problems 4.37, 4.38, and 4.39) will be used frequently.

THEOREM 4.14: $\quad$ Let $V$ be a vector space of finite dimension $n$. Then:

(i) Any $n+1$ or more vectors in $V$ are linearly dependent.

(ii) Any linearly independent set $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ with $n$ elements is a basis of $V$.

(iii) Any spanning set $T=\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ of $V$ with $n$ elements is a basis of $V$.

THEOREM 4.15: $\quad$ Suppose $S$ spans a vector space $V$. Then:

(i) Any maximum number of linearly independent vectors in $S$ form a basis of $V$.

(ii) Suppose one deletes from $S$ every vector that is a linear combination of preceding vectors in $S$. Then the remaining vectors form a basis of $V$.

THEOREM 4.16: $\quad$ Let $V$ be a vector space of finite dimension and let $S=\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ be a set of linearly independent vectors in $V$. Then $S$ is part of a basis of $V$; that is, $S$ may be extended to a basis of $V$.

\section*{EXAMPLE 4.11}
(a) The following four vectors in $\mathbf{R}^{4}$ form a matrix in echelon form:

$$
(1,1,1,1),(0,1,1,1),(0,0,1,1),(0,0,0,1)
$$

Thus, the vectors are linearly independent, and, because $\operatorname{dim} \mathbf{R}^{4}=4$, the four vectors form a basis of $\mathbf{R}^{4}$.

(b) The following $n+1$ polynomials in $\mathbf{P}_{n}(t)$ are of increasing degree:

$$
1, t-1,(t-1)^{2}, \ldots,(t-1)^{n}
$$

Therefore, no polynomial is a linear combination of preceding polynomials; hence, the polynomials are linear independent. Furthermore, they form a basis of $\mathbf{P}_{n}(t)$, because $\operatorname{dim} \mathbf{P}_{n}(t)=n+1$.

(c) Consider any four vectors in $\mathbf{R}^{3}$, say

$$
(257,-132,58), \quad(43,0,-17), \quad(521,-317,94), \quad(328,-512,-731)
$$

By Theorem 4.14(i), the four vectors must be linearly dependent, because they come from the three-dimensional vector space $\mathbf{R}^{3}$.

\section*{Dimension and Subspaces}
The following theorem (proved in Problem 4.40) gives the basic relationship between the dimension of a vector space and the dimension of a subspace.

THEOREM 4.17: Let $W$ be a subspace of an $n$-dimensional vector space $V$. Then $\operatorname{dim} W \leq n$. In particular, if $\operatorname{dim} W=n$, then $W=V$.

EXAMPLE 4.12 Let $W$ be a subspace of the real space $\mathbf{R}^{3}$. Note that $\operatorname{dim} \mathbf{R}^{3}=3$. Theorem 4.17 tells us that the dimension of $W$ can only be $0,1,2$, or 3 . The following cases apply:

(a) If $\operatorname{dim} W=0$, then $W=\{0\}$, a point.

(b) If $\operatorname{dim} W=1$, then $W$ is a line through the origin 0 .

(c) If $\operatorname{dim} W=2$, then $W$ is a plane through the origin 0 .

(d) If $\operatorname{dim} W=3$, then $W$ is the entire space $\mathbf{R}^{3}$.

\subsection*{4.9 Application to Matrices, Rank of a Matrix}
Let $A$ be any $m \times n$ matrix over a field $K$. Recall that the rows of $A$ may be viewed as vectors in $K^{n}$ and that the row space of $A$, written rowsp(A), is the subspace of $K^{n}$ spanned by the rows of $A$. The following definition applies.

DEFINITION: The rank of a matrix $A, \operatorname{written} \operatorname{rank}(A)$, is equal to the maximum number of linearly independent rows of $A$ or, equivalently, the dimension of the row space of $A$.

Recall, on the other hand, that the columns of an $m \times n$ matrix $A$ may be viewed as vectors in $K^{m}$ and that the column space of $A$, written colsp(A), is the subspace of $K^{m}$ spanned by the columns of $A$. Although $m$ may not be equal to $n$ - that is, the rows and columns of $A$ may belong to different vector spaces-we have the following fundamental result.

THEOREM 4.18: The maximum number of linearly independent rows of any matrix $A$ is equal to the maximum number of linearly independent columns of $A$. Thus, the dimension of the row space of $A$ is equal to the dimension of the column space of $A$.

Accordingly, one could restate the above definition of the rank of $A$ using columns instead of rows.

\section*{Basis-Finding Problems}
This subsection shows how an echelon form of any matrix $A$ gives us the solution to certain problems about $A$ itself. Specifically, let $A$ and $B$ be the following matrices, where the echelon matrix $B$ (whose pivots are circled) is an echelon form of $A$ :

$$
A=\left[\begin{array}{rrrrrr}
1 & 2 & 1 & 3 & 1 & 2 \\
2 & 5 & 5 & 6 & 4 & 5 \\
3 & 7 & 6 & 11 & 6 & 9 \\
1 & 5 & 10 & 8 & 9 & 9 \\
2 & 6 & 8 & 11 & 9 & 12
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{rrrrrr}
1 & 2 & 1 & 3 & 1 & 2 \\
0 & 1 & 3 & 1 & 2 & 1 \\
0 & 0 & 0 & 1 & 1 & 2 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

We solve the following four problems about the matrix $A$, where $C_{1}, C_{2}, \ldots, C_{6}$ denote its columns:

(a) Find a basis of the row space of $A$.

(b) Find each column $C_{k}$ of $A$ that is a linear combination of preceding columns of $A$.

(c) Find a basis of the column space of $A$.

(d) Find the rank of $A$.

(a) We are given that $A$ and $B$ are row equivalent, so they have the same row space. Moreover, $B$ is in echelon form, so its nonzero rows are linearly independent and hence form a basis of the row space of $B$. Thus, they also form a basis of the row space of $A$. That is,

$$
\text { basis of } \operatorname{rowsp}(A): \quad(1,2,1,3,1,2), \quad(0,1,3,1,2,1), \quad(0,0,0,1,1,2)
$$

(b) Let $M_{k}=\left[C_{1}, C_{2}, \ldots, C_{k}\right]$, the submatrix of $A$ consisting of the first $k$ columns of $A$. Then $M_{k-1}$ and $M_{k}$ are, respectively, the coefficient matrix and augmented matrix of the vector equation

$$
x_{1} C_{1}+x_{2} C_{2}+\cdots+x_{k-1} C_{k-1}=C_{k}
$$

Theorem 3.9 tells us that the system has a solution, or, equivalently, $C_{k}$ is a linear combination of the preceding columns of $A$ if and only if $\operatorname{rank}\left(M_{k}\right)=\operatorname{rank}\left(M_{k-1}\right)$, where $\operatorname{rank}\left(M_{k}\right)$ means the number of pivots in an echelon form of $M_{k}$. Now the first $k$ column of the echelon matrix $B$ is also an echelon form of $M_{k}$. Accordingly,

$$
\operatorname{rank}\left(M_{2}\right)=\operatorname{rank}\left(M_{3}\right)=2 \quad \text { and } \quad \operatorname{rank}\left(M_{4}\right)=\operatorname{rank}\left(M_{5}\right)=\operatorname{rank}\left(M_{6}\right)=3
$$

Thus, $C_{3}, C_{5}, C_{6}$ are each a linear combination of the preceding columns of $A$.

(c) The fact that the remaining columns $C_{1}, C_{2}, C_{4}$ are not linear combinations of their respective preceding columns also tells us that they are linearly independent. Thus, they form a basis of the column space of $A$. That is,

$$
\text { basis of } \operatorname{colsp}(A): \quad[1,2,3,1,2]^{T}, \quad[2,5,7,5,6]^{T}, \quad[3,6,11,8,11]^{T}
$$

Observe that $C_{1}, C_{2}, C_{4}$ may also be characterized as those columns of $A$ that contain the pivots in any echelon form of $A$.

(d) Here we see that three possible definitions of the rank of $A$ yield the same value.

(i) There are three pivots in $B$, which is an echelon form of $A$.

(ii) The three pivots in $B$ correspond to the nonzero rows of $B$, which form a basis of the row space of $A$.

(iii) The three pivots in $B$ correspond to the columns of $A$, which form a basis of the column space of $A$.

Thus, $\operatorname{rank}(A)=3$.

Application to Finding a Basis for $W=\operatorname{span}\left(u_{1}, u_{2}, \ldots, u_{r}\right)$

Frequently, we are given a list $S=\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ of vectors in $K^{n}$ and we want to find a basis for the subspace $W$ of $K^{n}$ spanned by the given vectors - that is, a basis of

$$
W=\operatorname{span}(S)=\operatorname{span}\left(u_{1}, u_{2}, \ldots, u_{r}\right)
$$

The following two algorithms, which are essentially described in the above subsection, find such a basis (and hence the dimension) of $W$.

\section*{Algorithm 4.1 (Row space algorithm)}
Step 1. Form the matrix $M$ whose rows are the given vectors.

Step 2. Row reduce $M$ to echelon form.

Step 3. Output the nonzero rows of the echelon matrix.

Sometimes we want to find a basis that only comes from the original given vectors. The next algorithm accomplishes this task.

\section*{Algorithm 4.2 (Casting-out algorithm)}
Step 1. Form the matrix $M$ whose columns are the given vectors.

Step 2. Row reduce $M$ to echelon form.

Step 3. For each column $C_{k}$ in the echelon matrix without a pivot, delete (cast out) the vector $u_{k}$ from the list $S$ of given vectors.

Step 4. Output the remaining vectors in $S$ (which correspond to columns with pivots).

We emphasize that in the first algorithm we form a matrix whose rows are the given vectors, whereas in the second algorithm we form a matrix whose columns are the given vectors.

EXAMPLE 4.13 Let $W$ be the subspace of $\mathbf{R}^{5}$ spanned by the following vectors:

$$
\begin{gathered}
u_{1}=(1,2,1,3,2), \quad u_{2}=(1,3,3,5,3), \quad u_{3}=(3,8,7,13,8) \\
u_{4}=(1,4,6,9,7), \quad u_{5}=(5,13,13,25,19)
\end{gathered}
$$

Find a basis of $W$ consisting of the original given vectors, and find $\operatorname{dim} W$.

Form the matrix $M$ whose columns are the given vectors, and reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrrr}
1 & 1 & 3 & 1 & 5 \\
2 & 3 & 8 & 4 & 13 \\
1 & 3 & 7 & 6 & 13 \\
3 & 5 & 13 & 9 & 25 \\
2 & 3 & 8 & 7 & 19
\end{array}\right] \sim\left[\begin{array}{lllll}
1 & 1 & 3 & 1 & 5 \\
0 & 1 & 2 & 2 & 3 \\
0 & 0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

The pivots in the echelon matrix appear in columns $C_{1}, C_{2}, C_{4}$. Accordingly, we "cast out" the vectors $u_{3}$ and $u_{5}$ from the original five vectors. The remaining vectors $u_{1}, u_{2}, u_{4}$, which correspond to the columns in the echelon matrix with pivots, form a basis of $W$. Thus, in particular, $\operatorname{dim} W=3$.

Remark: The justification of the casting-out algorithm is essentially described above, but we repeat it again here for emphasis. The fact that column $C_{3}$ in the echelon matrix in Example 4.13 does not have a pivot means that the vector equation

$$
x u_{1}+y u_{2}=u_{3}
$$

has a solution, and hence $u_{3}$ is a linear combination of $u_{1}$ and $u_{2}$. Similarly, the fact that $C_{5}$ does not have a pivot means that $u_{5}$ is a linear combination of the preceding vectors. We have deleted each vector in the original spanning set that is a linear combination of preceding vectors. Thus, the remaining vectors are linearly independent and form a basis of $W$.

\section*{Application to Homogeneous Systems of Linear Equations}
Consider again a homogeneous system $A X=0$ of linear equations over $K$ with $n$ unknowns. By Theorem 4.4, the solution set $W$ of such a system is a subspace of $K^{n}$, and hence $W$ has a dimension. The following theorem, whose proof is postponed until Chapter 5, holds.

THEOREM 4.19: $\quad$ The dimension of the solution space $W$ of a homogeneous system $A X=0$ is $n-r$, where $n$ is the number of unknowns and $r$ is the rank of the coefficient matrix $A$.

In the case where the system $A X=0$ is in echelon form, it has precisely $n-r$ free variables, say $x_{i_{1}}, x_{i_{2}}, \ldots, x_{i_{n-r}}$. Let $v_{j}$ be the solution obtained by setting $x_{i_{j}}=1$ (or any nonzero constant) and the remaining free variables equal to 0 . We show (Problem 4.50) that the solutions $v_{1}, v_{2}, \ldots, v_{n-r}$ are linearly independent; hence, they form a basis of the solution space $W$.

We have already used the above process to find a basis of the solution space $W$ of a homogeneous system $A X=0$ in Section 3.11. Problem 4.48 gives three other examples.

\subsection*{4.10 Sums and Direct Sums}
Let $U$ and $W$ be subsets of a vector space $V$. The sum of $U$ and $W$, written $U+W$, consists of all sums $u+w$ where $u \in U$ and $w \in W$. That is,

$$
U+W=\{v: v=u+w, \text { where } u \in U \text { and } w \in W\}
$$

Now suppose $U$ and $W$ are subspaces of $V$. Then one can easily show (Problem 4.53) that $U+W$ is a subspace of $V$. Recall that $U \cap W$ is also a subspace of $V$. The following theorem (proved in Problem 4.58) relates the dimensions of these subspaces.

THEOREM 4.20: Suppose $U$ and $W$ are finite-dimensional subspaces of a vector space $V$. Then $U+W$ has finite dimension and

$$
\operatorname{dim}(U+W)=\operatorname{dim} U+\operatorname{dim} W-\operatorname{dim}(U \cap W)
$$

EXAMPLE 4.14 Let $V=\mathbf{M}_{2,2}$, the vector space of $2 \times 2$ matrices. Let $U$ consist of those matrices whose second row is zero, and let $W$ consist of those matrices whose second column is zero. Then

$$
U=\left\{\left[\begin{array}{ll}
a & b \\
0 & 0
\end{array}\right]\right\}, \quad W=\left\{\left[\begin{array}{ll}
a & 0 \\
c & 0
\end{array}\right]\right\} \quad \text { and } \quad U+W=\left\{\left[\begin{array}{ll}
a & b \\
c & 0
\end{array}\right]\right\}, \quad U \cap W=\left\{\left[\begin{array}{ll}
a & 0 \\
0 & 0
\end{array}\right]\right\}
$$

That is, $U+W$ consists of those matrices whose lower right entry is 0 , and $U \cap W$ consists of those matrices whose second row and second column are zero. Note that $\operatorname{dim} U=2, \operatorname{dim} W=2, \operatorname{dim}(U \cap W)=1$. Also, $\operatorname{dim}(U+W)=3$, which is expected from Theorem 4.20. That is,

$$
\operatorname{dim}(U+W)=\operatorname{dim} U+\operatorname{dim} V-\operatorname{dim}(U \cap W)=2+2-1=3
$$

\section*{Direct Sums}
The vector space $V$ is said to be the direct sum of its subspaces $U$ and $W$, denoted by

$$
V=U \oplus W
$$

if every $v \in V$ can be written in one and only one way as $v=u+w$ where $u \in U$ and $w \in W$.

The following theorem (proved in Problem 4.59) characterizes such a decomposition.

THEOREM 4.21: The vector space $V$ is the direct sum of its subspaces $U$ and $W$ if and only if: (i) $V=U+W$, (ii) $U \cap W=\{0\}$.

EXAMPLE 4.15 Consider the vector space $V=\mathbf{R}^{3}$.

(a) Let $U$ be the $x y$-plane and let $W$ be the $y z$-plane; that is,

$$
U=\{(a, b, 0): a, b \in \mathbf{R}\} \quad \text { and } \quad W=\{(0, b, c): b, c \in \mathbf{R}\}
$$

Then $\mathbf{R}^{3}=U+W$, because every vector in $\mathbf{R}^{3}$ is the sum of a vector in $U$ and a vector in $W$. However, $\mathbf{R}^{3}$ is not the direct sum of $U$ and $W$, because such sums are not unique. For example,

$$
(3,5,7)=(3,1,0)+(0,4,7) \quad \text { and also } \quad(3,5,7)=(3,-4,0)+(0,9,7)
$$

(b) Let $U$ be the $x y$-plane and let $W$ be the $z$-axis; that is,

$$
U=\{(a, b, 0): a, b \in \mathbf{R}\} \quad \text { and } \quad W=\{(0,0, c): c \in \mathbf{R}\}
$$

Now any vector $(a, b, c) \in \mathbf{R}^{3}$ can be written as the sum of a vector in $U$ and a vector in $V$ in one and only one way:

$$
(a, b, c)=(a, b, 0)+(0,0, c)
$$

Accordingly, $\mathbf{R}^{3}$ is the direct sum of $U$ and $W$; that is, $\mathbf{R}^{3}=U \oplus W$.

\section*{General Direct Sums}
The notion of a direct sum is extended to more than one factor in the obvious way. That is, $V$ is the direct sum of subspaces $W_{1}, W_{2}, \ldots, W_{r}$, written

$$
V=W_{1} \oplus W_{2} \oplus \cdots \oplus W_{r}
$$

if every vector $v \in V$ can be written in one and only one way as

$$
v=w_{1}+w_{2}+\cdots+w_{r}
$$

where $w_{1} \in W_{1}, w_{2} \in W_{2}, \ldots, w_{r} \in W_{r}$.

The following theorems hold.

THEOREM 4.22: Suppose $V=W_{1} \oplus W_{2} \oplus \cdots \oplus W_{r}$. Also, for each $k$, suppose $S_{k}$ is a linearly independent subset of $W_{k}$. Then

(a) The union $S=\bigcup_{k} S_{k}$ is linearly independent in $V$.

(b) If each $S_{k}$ is a basis of $W_{k}$, then $\bigcup_{k} S_{k}$ is a basis of $V$.

(c) $\operatorname{dim} V=\operatorname{dim} W_{1}+\operatorname{dim} W_{2}+\cdots+\operatorname{dim} W_{r}$.

THEOREM 4.23: Suppose $V=W_{1}+W_{2}+\cdots+W_{r}$ and $\operatorname{dim} V=\sum_{k} \operatorname{dim} W_{k}$. Then

$$
V=W_{1} \oplus W_{2} \oplus \cdots \oplus W_{r}
$$

\subsection*{4.11 Coordinates}
Let $V$ be an $n$-dimensional vector space over $K$ with basis $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$. Then any vector $v \in V$ can be expressed uniquely as a linear combination of the basis vectors in $S$, say

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}
$$

These $n$ scalars $a_{1}, a_{2}, \ldots, a_{n}$ are called the coordinates of $v$ relative to the basis $S$, and they form a vector $\left[a_{1}, a_{2}, \ldots, a_{n}\right]$ in $K^{n}$ called the coordinate vector of $v$ relative to $S$. We denote this vector by $[v]_{S}$, or simply $[v]$, when $S$ is understood. Thus,

$$
[v]_{S}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]
$$

For notational convenience, brackets [..], rather than parentheses (...), are used to denote the coordinate vector.

Remark: The above $n$ scalars $a_{1}, a_{2}, \ldots, a_{n}$ also form the coordinate column vector $\left[a_{1}, a_{2}, \ldots, a_{n}\right]^{T}$ of $v$ relative to $S$. The choice of the column vector rather than the row vector to represent $v$ depends on the context in which it is used. The use of such column vectors will become clear later in Chapter 6.

EXAMPLE 4.16 Consider the vector space $\mathbf{P}_{2}(t)$ of polynomials of degree $\leq 2$. The polynomials

$$
p_{1}=t+1, \quad p_{2}=t-1, \quad p_{3}=(t-1)^{2}=t^{2}-2 t+1
$$

form a basis $S$ of $\mathbf{P}_{2}(t)$. The coordinate vector [ $v$ ] of $v=2 t^{2}-5 t+9$ relative to $S$ is obtained as follows.

Set $v=x p_{1}+y p_{2}+z p_{3}$ using unknown scalars $x, y$, $z$, and simplify:

$$
\begin{aligned}
2 t^{2}-5 t+9 & =x(t+1)+y(t-1)+z\left(t^{2}-2 t+1\right) \\
& =x t+x+y t-y+z t^{2}-2 z t+z \\
& =z t^{2}+(x+y-2 z) t+(x-y+z)
\end{aligned}
$$

Then set the coefficients of the same powers of $t$ equal to each other to obtain the system

$$
z=2, \quad x+y-2 z=-5, \quad x-y+z=9
$$

The solution of the system is $x=3, y=-4, z=2$. Thus,

$$
v=3 p_{1}-4 p_{2}+2 p_{3}, \text { and hence, }[v]=[3,-4,2]
$$

EXAMPLE 4.17 Consider real space $\mathbf{R}^{3}$. The following vectors form a basis $S$ of $\mathbf{R}^{3}$ :

$$
u_{1}=(1,-1,0), \quad u_{2}=(1,1,0), \quad u_{3}=(0,1,1)
$$

The coordinates of $v=(5,3,4)$ relative to the basis $S$ are obtained as follows.

Set $v=x v_{1}+y v_{2}+z v_{3}$; that is, set $v$ as a linear combination of the basis vectors using unknown scalars $x, y, z$. This yields

$$
\left[\begin{array}{l}
5 \\
3 \\
4
\end{array}\right]=x\left[\begin{array}{r}
1 \\
-1 \\
0
\end{array}\right]+y\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right]+z\left[\begin{array}{l}
0 \\
1 \\
1
\end{array}\right]
$$

The equivalent system of linear equations is as follows:

$$
x+y=5, \quad-x+y+z=3, \quad z=4
$$

The solution of the system is $x=3, y=2, z=4$. Thus,

$$
v=3 u_{1}+2 u_{2}+4 u_{3}, \quad \text { and so } \quad[v]_{s}=[3,2,4]
$$

Remark 1: There is a geometrical interpretation of the coordinates of a vector $v$ relative to a basis $S$ for the real space $\mathbf{R}^{n}$, which we illustrate using the basis $S$ of $\mathbf{R}^{3}$ in Example 4.17. First consider the space $\mathbf{R}^{3}$ with the usual $x, y, z$ axes. Then the basis vectors determine a new coordinate system of $\mathbf{R}^{3}$, say with $x^{\prime}, y^{\prime}, z^{\prime}$ axes, as shown in Fig. 4-4. That is,

(1) The $x^{\prime}$-axis is in the direction of $u_{1}$ with unit length $\left\|u_{1}\right\|$.

(2) The $y^{\prime}$-axis is in the direction of $u_{2}$ with unit length $\left\|u_{2}\right\|$.

(3) The $z^{\prime}$-axis is in the direction of $u_{3}$ with unit length $\left\|u_{3}\right\|$.

Then each vector $v=(a, b, c)$ or, equivalently, the point $P(a, b, c)$ in $\mathbf{R}^{3}$ will have new coordinates with respect to the new $x^{\prime}, y^{\prime}, z^{\prime}$ axes. These new coordinates are precisely $[v]_{S}$, the coordinates of $v$ with respect to the basis $S$. Thus, as shown in Example 4.17, the coordinates of the point $P(5,3,4)$ with the new axes form the vector $[3,2,4]$.

Remark 2: Consider the usual basis $E=\left\{e_{1}, e_{2}, \ldots, e_{n}\right\}$ of $K^{n}$ defined by

$$
e_{1}=(1,0,0, \ldots, 0,0), \quad e_{2}=(0,1,0, \ldots, 0,0), \quad \ldots, \quad e_{n}=(0,0,0, \ldots, 0,1)
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-139}
\end{center}

$$
v=(5,3,4)=[3,2,4]
$$

Figure 4-4

Let $v=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ be any vector in $K^{n}$. Then one can easily show that

$$
v=a_{1} e_{1}+a_{2} e_{2}+\cdots+a_{n} e_{n}, \quad \text { and so } \quad[v]_{E}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]
$$

That is, the coordinate vector $[v]_{E}$ of any vector $v$ relative to the usual basis $E$ of $K^{n}$ is identical to the original vector $v$.

\section*{Isomorphism of $\boldsymbol{V}$ and $\boldsymbol{K}^{\mathbf{n}}$}
Let $V$ be a vector space of dimension $n$ over $K$, and suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis of $V$. Then each vector $v \in V$ corresponds to a unique $n$-tuple $[v]_{S}$ in $K^{n}$. On the other hand, each $n$-tuple $\left[c_{1}, c_{2}, \ldots, c_{n}\right]$ in $K^{n}$ corresponds to a unique vector $c_{1} u_{1}+c_{2} u_{2}+\cdots+c_{n} u_{n}$ in $V$. Thus, the basis $S$ induces a one-to-one correspondence between $V$ and $K^{n}$. Furthermore, suppose

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n} \quad \text { and } \quad w=b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{n} u_{n}
$$

Then

$$
\begin{aligned}
v+w & =\left(a_{1}+b_{1}\right) u_{1}+\left(a_{2}+b_{2}\right) u_{2}+\cdots+\left(a_{n}+b_{n}\right) u_{n} \\
k v & =\left(k a_{1}\right) u_{1}+\left(k a_{2}\right) u_{2}+\cdots+\left(k a_{n}\right) u_{n}
\end{aligned}
$$

where $k$ is a scalar. Accordingly,

$$
\begin{aligned}
& {[v+w]_{S}=\left[a_{1}+b_{1}, \quad \ldots, \quad a_{n}+b_{n}\right]=\left[a_{1}, \ldots, a_{n}\right]+\left[b_{1}, \ldots, b_{n}\right]=[v]_{S}+[w]_{S}} \\
& {[k v]_{S}=\left[k a_{1}, k a_{2}, \ldots, k a_{n}\right]=k\left[a_{1}, a_{2}, \ldots, a_{n}\right]=k[v]_{S}}
\end{aligned}
$$

Thus, the above one-to-one correspondence between $V$ and $K^{n}$ preserves the vector space operations of vector addition and scalar multiplication. We then say that $V$ and $K^{n}$ are isomorphic, written

$$
V \cong K^{n}
$$

We state this result formally.

THEOREM 4.24: Let $V$ be an $n$-dimensional vector space over a field $K$. Then $V$ and $K^{n}$ are isomorphic.

The next example gives a practical application of the above result.

EXAMPLE 4.18 Suppose we want to determine whether or not the following matrices in $V=\mathbf{M}_{2,3}$ are linearly dependent:

$$
A=\left[\begin{array}{rrr}
1 & 2 & -3 \\
4 & 0 & 1
\end{array}\right], \quad B=\left[\begin{array}{rrr}
1 & 3 & -4 \\
6 & 5 & 4
\end{array}\right], \quad C=\left[\begin{array}{rrr}
3 & 8 & -11 \\
16 & 10 & 9
\end{array}\right]
$$

The coordinate vectors of the matrices in the usual basis of $\mathbf{M}_{2,3}$ are as follows:

$$
[A]=[1,2,-3,4,0,1], \quad[B]=[1,3,-4,6,5,4], \quad[C]=[3,8,-11,16,10,9]
$$

Form the matrix $M$ whose rows are the above coordinate vectors and reduce $M$ to an echelon form:

$$
M=\left[\begin{array}{rrrrrr}
1 & 2 & -3 & 4 & 0 & 1 \\
1 & 3 & -4 & 6 & 5 & 4 \\
3 & 8 & -11 & 16 & 10 & 9
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 2 & -3 & 4 & 0 & 1 \\
0 & 1 & -1 & 2 & 5 & 3 \\
0 & 2 & -2 & 4 & 10 & 6
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 2 & -3 & 4 & 0 & 1 \\
0 & 1 & -1 & 2 & 5 & 3 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Because the echelon matrix has only two nonzero rows, the coordinate vectors $[A],[B],[C]$ span a subspace of dimension 2 and so are linearly dependent. Accordingly, the original matrices $A, B, C$ are linearly dependent.

\section*{SOLVED PROBLEMS}
\section*{Vector Spaces, Linear Combinations}
4.1. Suppose $u$ and $v$ belong to a vector space $V$. Simplify each of the following expressions:\\
(a) $E_{1}=3(2 u-4 v)+5 u+7 v$,\\
(c) $E_{3}=2 u v+3(2 u+4 v)$\\
(b) $E_{2}=3 u-6(3 u-5 v)+7 u$,\\
(d) $E_{4}=5 u-\frac{3}{v}+5 u$

Multiply out and collect terms:

(a) $E_{1}=6 u-12 v+5 u+7 v=11 u-5 v$

(b) $E_{2}=3 u-18 u+30 v+7 u=-8 u+30 v$

(c) $E_{3}$ is not defined because the product $u v$ of vectors is not defined.

(d) $E_{4}$ is not defined because division by a vector is not defined.

4.2. Prove Theorem 4.1: Let $V$ be a vector space over a field $K$.

(i) $k 0=0$. (ii) $0 u=0$. (iii) If $k u=0$, then $k=0$ or $u=0$. (iv) $(-k) u=k(-u)=-k u$.

(i) By Axiom $\left[\mathrm{A}_{2}\right]$ with $u=0$, we have $0+0=0$. Hence, by Axiom $\left[\mathrm{M}_{1}\right]$, we have

$$
k 0=k(0+0)=k 0+k 0
$$

Adding $-k 0$ to both sides gives the desired result.

(ii) For scalars, $0+0=0$. Hence, by Axiom $\left[\mathrm{M}_{2}\right]$, we have

$$
0 u=(0+0) u=0 u+0 u
$$

Adding $-0 u$ to both sides gives the desired result.

(iii) Suppose $k u=0$ and $k \neq 0$. Then there exists a scalar $k^{-1}$ such that $k^{-1} k=1$. Thus,

$$
u=1 u=\left(k^{-1} k\right) u=k^{-1}(k u)=k^{-1} 0=0
$$

(iv) Using $u+(-u)=0$ and $k+(-k)=0$ yields

$$
0=k 0=k[u+(-u)]=k u+k(-u) \quad \text { and } \quad 0=0 u=[k+(-k)] u=k u+(-k) u
$$

Adding $-k u$ to both sides of the first equation gives $-k u=k(-u)$, and adding $-k u$ to both sides of the second equation gives $-k u=(-k) u$. Thus, $(-k) u=k(-u)=-k u$.

4.3. Show that (a) $k(u-v)=k u-k v$, (b) $u+u=2 u$.

(a) Using the definition of subtraction, that $u-v=u+(-v)$, and Theorem 4.1(iv), that $k(-v)=-k v$, we have

$$
k(u-v)=k[u+(-v)]=k u+k(-v)=k u+(-k v)=k u-k v
$$

(b) Using Axiom $\left[\mathrm{M}_{4}\right]$ and then Axiom $\left[\mathrm{M}_{2}\right]$, we have

$$
u+u=1 u+1 u=(1+1) u=2 u
$$

4.4. Express $v=(1,-2,5)$ in $\mathbf{R}^{3}$ as a linear combination of the vectors

$$
u_{1}=(1,1,1), \quad u_{2}=(1,2,3), \quad u_{3}=(2,-1,1)
$$

We seek scalars $x, y, z$, as yet unknown, such that $v=x u_{1}+y u_{2}+z u_{3}$. Thus, we require

$$
\left[\begin{array}{r}
1 \\
-2 \\
5
\end{array}\right]=x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+z\left[\begin{array}{r}
2 \\
-1 \\
1
\end{array}\right] \quad \text { or } \quad \begin{aligned}
& x+y+2 z=1 \\
& x+2 y-z=-2 \\
& x+3 y+z=5
\end{aligned}
$$

(For notational convenience, we write the vectors in $\mathbf{R}^{3}$ as columns, because it is then easier to find the equivalent system of linear equations.) Reducing the system to echelon form yields the triangular system

$$
x+y+2 z=1, \quad y-3 z=-3, \quad 5 z=10
$$

The system is consistent and has a solution. Solving by back-substitution yields the solution $x=-6, y=3$, $z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.

Alternatively, write down the augmented matrix $M$ of the equivalent system of linear equations, where $u_{1}, u_{2}, u_{3}$ are the first three columns of $M$ and $v$ is the last column, and then reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 2 & 1 \\
1 & 2 & -1 & -2 \\
1 & 3 & 1 & 5
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 2 & 1 \\
0 & 1 & -3 & -3 \\
0 & 2 & -1 & 4
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 2 & 1 \\
0 & 1 & -3 & -3 \\
0 & 0 & 5 & 10
\end{array}\right]
$$

The last matrix corresponds to a triangular system, which has a solution. Solving the triangular system by back-substitution yields the solution $x=-6, y=3, z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.

4.5. Express $v=(2,-5,3)$ in $\mathbf{R}^{3}$ as a linear combination of the vectors

$$
u_{1}=(1,-3,2), u_{2}=(2,-4,-1), u_{3}=(1,-5,7)
$$

We seek scalars $x, y, z$, as yet unknown, such that $v=x u_{1}+y u_{2}+z u_{3}$. Thus, we require

$$
\left[\begin{array}{r}
2 \\
-5 \\
3
\end{array}\right]=x\left[\begin{array}{r}
1 \\
-3 \\
2
\end{array}\right]+y\left[\begin{array}{r}
2 \\
-4 \\
-1
\end{array}\right]+z\left[\begin{array}{r}
1 \\
-5 \\
7
\end{array}\right] \quad \text { or } \quad \begin{array}{r}
x+2 y+z=2 \\
-3 x-4 y-5 z=-5 \\
2 x-y+7 z=3
\end{array}
$$

Reducing the system to echelon form yields the system

$$
x+2 y+z=2, \quad 2 y-2 z=1, \quad 0=3
$$

The system is inconsistent and so has no solution. Thus, $v$ cannot be written as a linear combination of $u_{1}, u_{2}, u_{3}$.

4.6. Express the polynomial $v=t^{2}+4 t-3$ in $\mathbf{P}(t)$ as a linear combination of the polynomials

$$
p_{1}=t^{2}-2 t+5, \quad p_{2}=2 t^{2}-3 t, \quad p_{3}=t+1
$$

Set $v$ as a linear combination of $p_{1}, p_{2}, p_{3}$ using unknowns $x, y, z$ to obtain


\begin{equation*}
t^{2}+4 t-3=x\left(t^{2}-2 t+5\right)+y\left(2 t^{2}-3 t\right)+z(t+1) \tag{*}
\end{equation*}


We can proceed in two ways.

Method 1. Expand the right side of $(*)$ and express it in terms of powers of $t$ as follows:

$$
\begin{aligned}
t^{2}+4 t-3 & =x t^{2}-2 x t+5 x+2 y t^{2}-3 y t+z t+z \\
& =(x+2 y) t^{2}+(-2 x-3 y+z) t+(5 x+3 z)
\end{aligned}
$$

Set coefficients of the same powers of $t$ equal to each other, and reduce the system to echelon form. This yields

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-142}
\end{center}

The system is consistent and has a solution. Solving by back-substitution yields the solution $x=-3, y=2$, $z=4$. Thus, $v=-3 p_{1}+2 p_{2}+4 p_{2}$.

Method 2. The equation $\left({ }^{*}\right)$ is an identity in $t$; that is, the equation holds for any value of $t$. Thus, we can set $t$ equal to any numbers to obtain equations in the unknowns.

(a) Set $t=0$ in $\left(^{*}\right)$ to obtain the equation $-3=5 x+z$.

(b) Set $t=1$ in $(*)$ to obtain the equation $2=4 x-y+2 z$.

(c) Set $t=-1$ in (*) to obtain the equation $-6=8 x+5 y$.

Solve the system of the three equations to again obtain the solution $x=-3, y=2, z=4$. Thus, $v=-3 p_{1}+2 p_{2}+4 p_{3}$.

4.7. Express $M$ as a linear combination of the matrices $A, B, C$, where

$M=\left[\begin{array}{ll}4 & 7 \\ 7 & 9\end{array}\right], \quad$ and $\quad A=\left[\begin{array}{ll}1 & 1 \\ 1 & 1\end{array}\right], \quad B=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right], \quad C=\left[\begin{array}{ll}1 & 1 \\ 4 & 5\end{array}\right]$

Set $M$ as a linear combination of $A, B, C$ using unknown scalars $x, y, z$; that is, set $M=x A+y B+z C$. This yields

$$
\left[\begin{array}{ll}
4 & 7 \\
7 & 9
\end{array}\right]=x\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right]+y\left[\begin{array}{ll}
1 & 2 \\
3 & 4
\end{array}\right]+z\left[\begin{array}{ll}
1 & 1 \\
4 & 5
\end{array}\right]=\left[\begin{array}{cc}
x+y+z & x+2 y+z \\
x+3 y+4 z & x+4 y+5 z
\end{array}\right]
$$

Form the equivalent system of equations by setting corresponding entries equal to each other:

$$
x+y+z=4, \quad x+2 y+z=7, \quad x+3 y+4 z=7, \quad x+4 y+5 z=9
$$

Reducing the system to echelon form yields

$$
x+y+z=4, \quad y=3, \quad 3 z=-3, \quad 4 z=-4
$$

The last equation drops out. Solving the system by back-substitution yields $z=-1, y=3, x=2$. Thus, $M=2 A+3 B-C$.

\section*{Subspaces}
4.8. Prove Theorem 4.2: $W$ is a subspace of $V$ if the following two conditions hold:

(a) $0 \in W$. (b) If $u, v \in W$, then $u+v$, $k u \in W$.

By (a), $W$ is nonempty, and, by (b), the operations of vector addition and scalar multiplication are well defined for $W$. Axioms $\left[\mathrm{A}_{1}\right],\left[\mathrm{A}_{4}\right],\left[\mathrm{M}_{1}\right],\left[\mathrm{M}_{2}\right],\left[\mathrm{M}_{3}\right],\left[\mathrm{M}_{4}\right]$ hold in $W$ because the vectors in $W$ belong to $V$. Thus, we need only show that $\left[\mathrm{A}_{2}\right]$ and $\left[\mathrm{A}_{3}\right]$ also hold in $W$. Now $\left[\mathrm{A}_{2}\right]$ holds because the zero vector in $V$ belongs to $W$ by (a). Finally, if $v \in W$, then $(-1) v=-v \in W$, and $v+(-v)=0$. Thus $\left[\mathrm{A}_{3}\right]$ holds.

4.9. Let $V=\mathbf{R}^{3}$. Show that $W$ is not a subspace of $V$, where

(a) $W=\{(a, b, c): a \geq 0\}$, (b) $W=\left\{(a, b, c): a^{2}+b^{2}+c^{2} \leq 1\right\}$.

In each case, show that Theorem 4.2 does not hold.\\
(a) $W$ consists of those vectors whose first entry is nonnegative. Thus, $v=(1,2,3)$ belongs to $W$. Let $k=-3$. Then $k v=(-3,-6,-9)$ does not belong to $W$, because -3 is negative. Thus, $W$ is not a subspace of $V$.

(b) $W$ consists of vectors whose length does not exceed 1. Hence, $u=(1,0,0)$ and $v=(0,1,0)$ belong to $W$, but $u+v=(1,1,0)$ does not belong to $W$, because $1^{2}+1^{2}+0^{2}=2>1$. Thus, $W$ is not a subspace of $V$.

4.10. Let $V=\mathbf{P}(t)$, the vector space of real polynomials. Determine whether or not $W$ is a subspace of $V$, where

(a) $W$ consists of all polynomials with integral coefficients.

(b) $W$ consists of all polynomials with degree $\geq 6$ and the zero polynomial.

(c) $W$ consists of all polynomials with only even powers of $t$.

(a) No, because scalar multiples of polynomials in $W$ do not always belong to $W$. For example,

$$
f(t)=3+6 t+7 t^{2} \in W \quad \text { but } \quad \frac{1}{2} f(t)=\frac{3}{2}+3 t+\frac{7}{2} t^{2} \notin W
$$

(b and c) Yes. In each case, $W$ contains the zero polynomial, and sums and scalar multiples of polynomials in $W$ belong to $W$.

4.11. Let $V$ be the vector space of functions $f: \mathbf{R} \rightarrow \mathbf{R}$. Show that $W$ is a subspace of $V$, where

(a) $W=\{f(x): f(1)=0\}$, all functions whose value at 1 is 0 .

(b) $W=\{f(x): f(3)=f(1)\}$, all functions assigning the same value to 3 and 1 .

(c) $W=\{f(t): f(-x)=-f(x)\}$, all odd functions.

Let $\hat{0}$ denote the zero function, so $\hat{0}(x)=0$ for every value of $x$.

(a) $\hat{0} \in W$, because $\hat{0}(1)=0$. Suppose $f, g \in W$. Then $f(1)=0$ and $g(1)=0$. Also, for scalars $a$ and $b$, we have

$$
(a f+b g)(1)=a f(1)+b g(1)=a 0+b 0=0
$$

Thus, $a f+b g \in W$, and hence $W$ is a subspace.

(b) $\hat{0} \in W$, because $\hat{0}(3)=0=\hat{0}(1)$. Suppose $f, g \in W$. Then $f(3)=f(1)$ and $g(3)=g(1)$. Thus, for any scalars $a$ and $b$, we have

$$
(a f+b g)(3)=a f(3)+b g(3)=a f(1)+b g(1)=(a f+b g)(1)
$$

Thus, $a f+b g \in W$, and hence $W$ is a subspace.

(c) $\hat{0} \in W$, because $\hat{0}(-x)=0=-0=-\hat{0}(x)$. Suppose $f, g \in W$. Then $f(-x)=-f(x)$ and $g(-x)=-g(x)$. Also, for scalars $a$ and $b$,

$$
(a f+b g)(-x)=a f(-x)+b g(-x)=-a f(x)-b g(x)=-(a f+b g)(x)
$$

Thus, $a b+g f \in W$, and hence $W$ is a subspace of $V$.

4.12. Prove Theorem 4.3: The intersection of any number of subspaces of $V$ is a subspace of $V$.

Let $\left\{W_{i}: i \in I\right\}$ be a collection of subspaces of $V$ and let $W=\cap\left(W_{i}: i \in I\right)$. Because each $W_{i}$ is a subspace of $V$, we have $0 \in W_{i}$, for every $i \in I$. Hence, $0 \in W$. Suppose $u, v \in W$. Then $u, v \in W_{i}$, for every $i \in I$. Because each $W_{i}$ is a subspace, $a u+b v \in W_{i}$, for every $i \in I$. Hence, $a u+b v \in W$. Thus, $W$ is a subspace of $V$.

\section*{Linear Spans}
4.13. Show that the vectors $u_{1}=(1,1,1), u_{2}=(1,2,3), u_{3}=(1,5,8)$ span $\mathbf{R}^{3}$.

We need to show that an arbitrary vector $v=(a, b, c)$ in $\mathbf{R}^{3}$ is a linear combination of $u_{1}, u_{2}, u_{3}$. Set $v=x u_{1}+y u_{2}+z u_{3}$; that is, set

$$
(a, b, c)=x(1,1,1)+y(1,2,3)+z(1,5,8)=(x+y+z, \quad x+2 y+5 z, \quad x+3 y+8 z)
$$

Form the equivalent system and reduce it to echelon form:

$$
\begin{aligned}
& x+y+z=a \quad x+y+z=a \quad x+y+z=a \\
& x+2 y+5 z=b \quad \text { or } \quad y+4 z=b-a \quad \text { or } \\
& x+3 y+8 z=c \quad 2 y+7 c=c-a \quad-z=c-2 b+a
\end{aligned}
$$

The above system is in echelon form and is consistent; in fact,

$$
x=-a+5 b-3 c, y=3 a-7 b+4 c, z=a+2 b-c
$$

is a solution. Thus, $u_{1}, u_{2}, u_{3}$ span $\mathbf{R}^{3}$.

4.14. Find conditions on $a, b, c$ so that $v=(a, b, c)$ in $\mathbf{R}^{3}$ belongs to $W=\operatorname{span}\left(u_{1}, u_{2}, u_{3}\right)$, where

$$
u_{1}=(1,2,0), u_{2}=(-1,1,2), u_{3}=(3,0,-4)
$$

yields

Set $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknowns $x, y, z$; that is, set $v=x u_{1}+y u_{2}+z u_{3}$. This

$$
(a, b, c)=x(1,2,0)+y(-1,1,2)+z(3,0,-4)=(x-y+3 z, \quad 2 x+y, \quad 2 y-4 z)
$$

Form the equivalent system of linear equations and reduce it to echelon form:

$$
\begin{aligned}
& x-y+3 z=a \\
& x-y+3 z=a \quad x-y+3 z=a \\
& 2 x+y \quad=b \quad \text { or } \quad 3 y-6 z=b-2 a \quad \text { or } \quad 3 y-6 z=b-2 a \\
& 2 y-4 z=c \quad 2 y-4 z=c \quad 0=4 a-2 b+3 c
\end{aligned}
$$

The vector $v=(a, b, c)$ belongs to $W$ if and only if the system is consistent, and it is consistent if and only if $4 a-2 b+3 c=0$. Note, in particular, that $u_{1}, u_{2}, u_{3}$ do not span the whole space $\mathbf{R}^{3}$.

4.15. Show that the vector space $V=\mathbf{P}(t)$ of real polynomials cannot be spanned by a finite number of polynomials.

Any finite set $S$ of polynomials contains a polynomial of maximum degree, say $m$. Then the linear span $\operatorname{span}(\mathrm{S})$ of $S$ cannot contain a polynomial of degree greater than $m$. Thus, $\operatorname{span}(S) \neq V$, for any finite set $S$.

4.16. Prove Theorem 4.5: Let $S$ be a subset of $V$. (i) Then $\operatorname{span}(\mathrm{S})$ is a subspace of $V$ containing $S$. (ii) If $W$ is a subspace of $V$ containing $S$, then $\operatorname{span}(S) \subseteq W$.

(i) Suppose $S$ is empty. By definition, $\operatorname{span}(S)=\{0\}$. Hence $\operatorname{span}(S)=\{0\}$ is a subspace of $V$ and $S \subseteq \operatorname{span}(S)$. Suppose $S$ is not empty and $v \in S$. Then $v=1 v \in \operatorname{span}(S)$; hence, $S \subseteq \operatorname{span}(S)$. Also $0=0 v \in \operatorname{span}(S)$. Now suppose $u, w \in \operatorname{span}(S)$, say

$$
u=a_{1} u_{1}+\cdots+a_{r} u_{r}=\sum_{i} a_{i} u_{i} \quad \text { and } \quad w=b_{1} w_{1}+\cdots+b_{s} w_{s}=\sum_{j} b_{j} w_{j}
$$

where $u_{i}, w_{j} \in S$ and $a_{i}, b_{j} \in K$. Then

$$
u+v=\sum_{i} a_{i} u_{i}+\sum_{j} b_{j} w_{j} \quad \text { and } \quad k u=k\left(\sum_{i} a_{i} u_{i}\right)=\sum_{i} k a_{i} u_{i}
$$

belong to $\operatorname{span}(\mathrm{S})$ because each is a linear combination of vectors in $S$. Thus, $\operatorname{span}(\mathrm{S})$ is a subspace of $V$.

(ii) Suppose $u_{1}, u_{2}, \ldots, u_{r} \in S$. Then all the $u_{i}$ belong to $W$. Thus, all multiples $a_{1} u_{1}, a_{2} u_{2}, \ldots, a_{r} u_{r} \in W$, and so the sum $a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{r} u_{r} \in W$. That is, $W$ contains all linear combinations of elements in $S$, or, in other words, $\operatorname{span}(S) \subseteq W$, as claimed.

\section*{Linear Dependence}
4.17. Determine whether or not $u$ and $v$ are linearly dependent, where\\
(a) $u=(1,2), v=(3,-5)$\\
(c) $u=(1,2,-3), v=(4,5,-6)$\\
(b) $u=(1,-3), v=(-2,6)$,\\
(d) $u=(2,4,-8), v=(3,6,-12)$

Two vectors $u$ and $v$ are linearly dependent if and only if one is a multiple of the other.\\
(a) No. (b) Yes; for $v=-2 u$.\\
(c) No\\
(d) Yes, for $v=\frac{3}{2} u$.

4.18. Determine whether or not $u$ and $v$ are linearly dependent, where

(a) $u=2 t^{2}+4 t-3, v=4 t^{2}+8 t-6$,

(c) $u=\left[\begin{array}{rrr}1 & 3 & -4 \\ 5 & 0 & -1\end{array}\right], v=\left[\begin{array}{rrr}-4 & -12 & 16 \\ -20 & 0 & 4\end{array}\right]$, (b) $u=2 t^{2}-3 t+4, v=4 t^{2}-3 t+2$,

(d) $u=\left[\begin{array}{lll}1 & 1 & 1 \\ 2 & 2 & 2\end{array}\right], v=\left[\begin{array}{lll}2 & 2 & 2 \\ 3 & 3 & 3\end{array}\right]$

Two vectors $u$ and $v$ are linearly dependent if and only if one is a multiple of the other.

(a) Yes; for $v=2 u$. (b) No. (c) Yes, for $v=-4 u$. (d) No.

4.19. Determine whether or not the vectors $u=(1,1,2), v=(2,3,1), w=(4,5,5)$ in $\mathbf{R}^{3}$ are linearly dependent.

Method 1. Set a linear combination of $u, v, w$ equal to the zero vector using unknowns $x, y, z$ to obtain the equivalent homogeneous system of linear equations and then reduce the system to echelon form. This yields

$$
x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
2 \\
3 \\
1
\end{array}\right]+z\left[\begin{array}{l}
4 \\
5 \\
5
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0
\end{array}\right] \quad \text { or } \quad \begin{aligned}
x+2 y+4 z & =0 \\
x+3 y+5 z & =0 \\
2 x+y+5 z & =0
\end{aligned} \quad \text { or } \quad \begin{aligned}
x+2 y+4 z & =0 \\
y+z & =0
\end{aligned}
$$

The echelon system has only two nonzero equations in three unknowns; hence, it has a free variable and a nonzero solution. Thus, $u, v, w$ are linearly dependent.

Method 2. Form the matrix $A$ whose columns are $u, v, w$ and reduce to echelon form:

$$
A=\left[\begin{array}{lll}
1 & 2 & 4 \\
1 & 3 & 5 \\
2 & 1 & 5
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & 4 \\
0 & 1 & 1 \\
0 & -3 & -3
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 2 & 4 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{array}\right]
$$

The third column does not have a pivot; hence, the third vector $w$ is a linear combination of the first two vectors $u$ and $v$. Thus, the vectors are linearly dependent. (Observe that the matrix $A$ is also the coefficient matrix in Method 1. In other words, this method is essentially the same as the first method.)

Method 3. Form the matrix $B$ whose rows are $u, v, w$, and reduce to echelon form:

$$
B=\left[\begin{array}{lll}
1 & 1 & 2 \\
2 & 3 & 1 \\
4 & 5 & 5
\end{array}\right] \sim\left[\begin{array}{rrr}
0 & 1 & 2 \\
0 & 1 & -3 \\
0 & 1 & -3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 2 \\
0 & 1 & -3 \\
0 & 0 & 0
\end{array}\right]
$$

Because the echelon matrix has only two nonzero rows, the three vectors are linearly dependent. (The three given vectors span a space of dimension 2.)

4.20. Determine whether or not each of the following lists of vectors in $\mathbf{R}^{3}$ is linearly dependent:

(a) $u_{1}=(1,2,5), u_{2}=(1,3,1), u_{3}=(2,5,7), u_{4}=(3,1,4)$,

(b) $u=(1,2,5), v=(2,5,1), w=(1,5,2)$,

(c) $u=(1,2,3), v=(0,0,0), w=(1,5,6)$.

(a) Yes, because any four vectors in $\mathbf{R}^{3}$ are linearly dependent.

(b) Use Method 2 above; that is, form the matrix $A$ whose columns are the given vectors, and reduce the matrix to echelon form:

$$
A=\left[\begin{array}{rrr}
1 & 2 & 1 \\
2 & 5 & 5 \\
5 & 1 & 2
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & 1 & 3 \\
0 & -9 & -3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 2 & 1 \\
0 & 1 & 3 \\
0 & 0 & 24
\end{array}\right]
$$

Every column has a pivot entry; hence, no vector is a linear combination of the previous vectors. Thus, the vectors are linearly independent.

(c) Because $0=(0,0,0)$ is one of the vectors, the vectors are linearly dependent.

4.21. Show that the functions $f(t)=\sin t, g(t) \cos t, h(t)=t$ from $\mathbf{R}$ into $\mathbf{R}$ are linearly independent.

Set a linear combination of the functions equal to the zero function $\mathbf{0}$ using unknown scalars $x, y, z$; that is, set $x f+y g+z h=\mathbf{0}$. Then show $x=0, y=0, z=0$. We emphasize that $x f+y g+z h=\mathbf{0}$ means that, for every value of $t$, we have $x f(t)+y g(t)+z h(t)=0$.

Thus, in the equation $x \sin t+y \cos t+z t=0$ :

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-146}
\end{center}

The three equations have only the zero solution; that is, $x=0, y=0, z=0$. Thus, $f, g, h$ are linearly independent.

4.22. Suppose the vectors $u, v, w$ are linearly independent. Show that the vectors $u+v, u-v$, $u-2 v+w$ are also linearly independent.

Suppose $x(u+v)+y(u-v)+z(u-2 v+w)=0$. Then

$$
x u+x v+y u-y v+z u-2 z v+z w=0
$$

or

$$
(x+y+z) u+(x-y-2 z) v+z w=0
$$

Because $u, v, w$ are linearly independent, the coefficients in the above equation are each 0 ; hence,

$$
x+y+z=0, \quad x-y-2 z=0, \quad z=0
$$

The only solution to the above homogeneous system is $x=0, y=0, z=0$. Thus, $u+v, u-v, u-2 v+w$ are linearly independent.

4.23. Show that the vectors $u=(1+i, 2 i)$ and $w=(1,1+i)$ in $\mathbf{C}^{2}$ are linearly dependent over the complex field $\mathbf{C}$ but linearly independent over the real field $\mathbf{R}$.

Recall that two vectors are linearly dependent (over a field $K$ ) if and only if one of them is a multiple of the other (by an element in $K$ ). Because

$$
(1+i) w=(1+i)(1,1+i)=(1+i, 2 i)=u
$$

$u$ and $w$ are linearly dependent over $\mathbf{C}$. On the other hand, $u$ and $w$ are linearly independent over $\mathbf{R}$, as no real multiple of $w$ can equal $u$. Specifically, when $k$ is real, the first component of $k w=(k, k+k i)$ must be real, and it can never equal the first component $1+i$ of $u$, which is complex.

\section*{Basis and Dimension}
4.24. Determine whether or not each of the following form a basis of $\mathbf{R}^{3}$ :\\
(a) $(1,1,1),(1,0,1)$;\\
(c) $(1,1,1),(1,2,3),(2,-1,1)$;\\
(b) $(1,2,3),(1,3,5),(1,0,1),(2,3,0)$;\\
(d) $(1,1,2),(1,2,5),(5,3,4)$.

(a and b) No, because a basis of $\mathbf{R}^{3}$ must contain exactly three elements because $\operatorname{dim} \mathbf{R}^{3}=3$.

(c) The three vectors form a basis if and only if they are linearly independent. Thus, form the matrix whose rows are the given vectors, and row reduce the matrix to echelon form:

$$
\left[\begin{array}{rrr}
1 & 1 & 1 \\
1 & 2 & 3 \\
2 & -1 & 1
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 1 \\
0 & 1 & 2 \\
0 & -3 & -1
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 1 \\
0 & 1 & 2 \\
0 & 0 & 5
\end{array}\right]
$$

The echelon matrix has no zero rows; hence, the three vectors are linearly independent, and so they do form a basis of $\mathbf{R}^{3}$.\\
(d) Form the matrix whose rows are the given vectors, and row reduce the matrix to echelon form:

$$
\left[\begin{array}{lll}
1 & 1 & 2 \\
1 & 2 & 5 \\
5 & 3 & 4
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 2 \\
0 & 1 & 3 \\
0 & -2 & -6
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 2 \\
0 & 1 & 3 \\
0 & 0 & 0
\end{array}\right]
$$

The echelon matrix has a zero row; hence, the three vectors are linearly dependent, and so they do not form a basis of $\mathbf{R}^{3}$.

4.25. Determine whether $(1,1,1,1),(1,2,3,2),(2,5,6,4),(2,6,8,5)$ form a basis of $\mathbf{R}^{4}$. If not, find the dimension of the subspace they span.

Form the matrix whose rows are the given vectors, and row reduce to echelon form:

$$
B=\left[\begin{array}{llll}
1 & 1 & 1 & 1 \\
1 & 2 & 3 & 2 \\
2 & 5 & 6 & 4 \\
2 & 6 & 8 & 5
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 1 & 1 & 1 \\
0 & 1 & 2 & 1 \\
0 & 3 & 4 & 2 \\
0 & 4 & 6 & 3
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 1 & 1 & 1 \\
0 & 1 & 2 & 1 \\
0 & 0 & -2 & -1 \\
0 & 0 & -2 & -1
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 1 & 1 & 1 \\
0 & 1 & 2 & 1 \\
0 & 0 & 2 & 1 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

The echelon matrix has a zero row. Hence, the four vectors are linearly dependent and do not form a basis of $\mathbf{R}^{4}$. Because the echelon matrix has three nonzero rows, the four vectors span a subspace of dimension 3.

4.26. Extend $\left\{u_{1}=(1,1,1,1), u_{2}=(2,2,3,4)\right\}$ to a basis of $\mathbf{R}^{4}$.

First form the matrix with rows $u_{1}$ and $u_{2}$, and reduce to echelon form:

$$
\left[\begin{array}{llll}
1 & 1 & 1 & 1 \\
2 & 2 & 3 & 4
\end{array}\right] \sim\left[\begin{array}{llll}
1 & 1 & 1 & 1 \\
0 & 0 & 1 & 2
\end{array}\right]
$$

Then $w_{1}=(1,1,1,1)$ and $w_{2}=(0,0,1,2)$ span the same set of vectors as spanned by $u_{1}$ and $u_{2}$. Let $u_{3}=(0,1,0,0)$ and $u_{4}=(0,0,0,1)$. Then $w_{1}, u_{3}, w_{2}, u_{4}$ form a matrix in echelon form. Thus, they are linearly independent, and they form a basis of $\mathbf{R}^{4}$. Hence, $u_{1}, u_{2}, u_{3}, u_{4}$ also form a basis of $\mathbf{R}^{4}$.

4.27. Consider the complex field $\mathbf{C}$, which contains the real field $\mathbf{R}$, which contains the rational field $\mathbf{Q}$. (Thus, $\mathbf{C}$ is a vector space over $\mathbf{R}$, and $\mathbf{R}$ is a vector space over $\mathbf{Q}$.)

(a) Show that $\{1, i\}$ is a basis of $\mathbf{C}$ over $\mathbf{R}$; hence, $\mathbf{C}$ is a vector space of dimension 2 over $\mathbf{R}$.

(b) Show that $\mathbf{R}$ is a vector space of infinite dimension over $\mathbf{Q}$.

(a) For any $v \in \mathbf{C}$, we have $v=a+b i=a(1)+b(i)$, where $a, b \in \mathbf{R}$. Hence, $\{1, i\}$ spans $\mathbf{C}$ over $\mathbf{R}$. Furthermore, if $x(1)+y(i)=0$ or $x+y i=0$, where $x, y \in \mathbf{R}$, then $x=0$ and $y=0$. Hence, $\{1, i\}$ is linearly independent over $\mathbf{R}$. Thus, $\{1, i\}$ is a basis for $\mathbf{C}$ over $\mathbf{R}$.

(b) It can be shown that $\pi$ is a transcendental number; that is, $\pi$ is not a root of any polynomial over $\mathbf{Q}$. Thus, for any $n$, the $n+1$ real numbers $1, \pi, \pi^{2}, \ldots, \pi^{n}$ are linearly independent over $\mathbf{Q}$. $\mathbf{R}$ cannot be of dimension $n$ over $\mathbf{Q}$. Accordingly, $\mathbf{R}$ is of infinite dimension over $\mathbf{Q}$.

4.28. Suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a subset of $V$. Show that the following Definitions A and B of a basis of $V$ are equivalent:

(A) $S$ is linearly independent and spans $V$.

(B) Every $v \in V$ is a unique linear combination of vectors in $S$.

Suppose (A) holds. Because $S$ spans $V$, the vector $v$ is a linear combination of the $u_{i}$, say

$$
u=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n} \quad \text { and } \quad u=b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{n} u_{n}
$$

Subtracting, we get

$$
0=v-v=\left(a_{1}-b_{1}\right) u_{1}+\left(a_{2}-b_{2}\right) u_{2}+\cdots+\left(a_{n}-b_{n}\right) u_{n}
$$

But the $u_{i}$ are linearly independent. Hence, the coefficients in the above relation are each 0 :

$$
a_{1}-b_{1}=0, \quad a_{2}-b_{2}=0, \quad \ldots, \quad a_{n}-b_{n}=0
$$

Therefore, $a_{1}=b_{1}, a_{2}=b_{2}, \ldots, a_{n}=b_{n}$. Hence, the representation of $v$ as a linear combination of the $u_{i}$ is unique. Thus, (A) implies (B).

Suppose (B) holds. Then $S$ spans $V$. Suppose

$$
0=c_{1} u_{1}+c_{2} u_{2}+\cdots+c_{n} u_{n}
$$

However, we do have

$$
0=0 u_{1}+0 u_{2}+\cdots+0 u_{n}
$$

By hypothesis, the representation of 0 as a linear combination of the $u_{i}$ is unique. Hence, each $c_{i}=0$ and the $u_{i}$ are linearly independent. Thus, (B) implies (A).

\section*{Dimension and Subspaces}
4.29. Find a basis and dimension of the subspace $W$ of $\mathbf{R}^{3}$ where\\
(a) $W=\{(a, b, c): a+b+c=0\}$,\\
(b) $W=\{(a, b, c):(a=b=c)\}$

(a) Note that $W \neq \mathbf{R}^{3}$, because, for example, $(1,2,3) \notin W$. Thus, $\operatorname{dim} W<3$. Note that $u_{1}=(1,0,-1)$ and $u_{2}=(0,1,-1)$ are two independent vectors in $W$. Thus, $\operatorname{dim} W=2$, and so $u_{1}$ and $u_{2}$ form a basis of $W$.

(b) The vector $u=(1,1,1) \in W$. Any vector $w \in W$ has the form $w=(k, k, k)$. Hence, $w=k u$. Thus, $u$ spans $W$ and $\operatorname{dim} W=1$.

4.30. Let $W$ be the subspace of $\mathbf{R}^{4}$ spanned by the vectors

$$
u_{1}=(1,-2,5,-3), \quad u_{2}=(2,3,1,-4), \quad u_{3}=(3,8,-3,-5)
$$

(a) Find a basis and dimension of $W$. (b) Extend the basis of $W$ to a basis of $\mathbf{R}^{4}$.

(a) Apply Algorithm 4.1, the row space algorithm. Form the matrix whose rows are the given vectors, and reduce it to echelon form:

$$
A=\left[\begin{array}{rrrr}
1 & -2 & 5 & -3 \\
2 & 3 & 1 & -4 \\
3 & 8 & -3 & -5
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & -2 & 5 & -3 \\
0 & 7 & -9 & 2 \\
0 & 14 & -18 & 4
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & -2 & 5 & -3 \\
0 & 7 & -9 & 2 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

The nonzero rows $(1,-2,5,-3)$ and $(0,7,-9,2)$ of the echelon matrix form a basis of the row space of $A$ and hence of $W$. Thus, in particular, $\operatorname{dim} W=2$.

(b) We seek four linearly independent vectors, which include the above two vectors. The four vectors $(1,-2,5,-3),(0,7,-9,2),(0,0,1,0)$, and $(0,0,0,1)$ are linearly independent (because they form an echelon matrix), and so they form a basis of $\mathbf{R}^{4}$, which is an extension of the basis of $W$.

4.31. Let $W$ be the subspace of $\mathbf{R}^{5}$ spanned by $u_{1}=(1,2,-1,3,4), \quad u_{2}=(2,4,-2,6,8)$, $u_{3}=(1,3,2,2,6), \quad u_{4}=(1,4,5,1,8), \quad u_{5}=(2,7,3,3,9)$. Find a subset of the vectors that form a basis of $W$.

Here we use Algorithm 4.2, the casting-out algorithm. Form the matrix $M$ whose columns (not rows) are the given vectors, and reduce it to echelon form:

$$
M=\left[\begin{array}{rrrrr}
1 & 2 & 1 & 1 & 2 \\
2 & 4 & 3 & 4 & 7 \\
-1 & -2 & 2 & 5 & 3 \\
3 & 6 & 2 & 1 & 3 \\
4 & 8 & 6 & 8 & 9
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & 1 & 1 & 2 \\
0 & 0 & 1 & 2 & 3 \\
0 & 0 & 3 & 6 & 5 \\
0 & 0 & -1 & -2 & -3 \\
0 & 0 & 2 & 4 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 2 & 1 & 1 & 2 \\
0 & 0 & 1 & 2 & 3 \\
0 & 0 & 0 & 0 & -4 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

The pivot positions are in columns $C_{1}, C_{3}, C_{5}$. Hence, the corresponding vectors $u_{1}, u_{3}, u_{5}$ form a basis of $W$, and $\operatorname{dim} W=3$.

4.32. Let $V$ be the vector space of $2 \times 2$ matrices over $K$. Let $W$ be the subspace of symmetric matrices. Show that $\operatorname{dim} W=3$, by finding a basis of $W$.

Recall that a matrix $A=\left[a_{i j}\right]$ is symmetric if $A^{T}=A$, or, equivalently, each $a_{i j}=a_{j i}$. Thus, $A=\left[\begin{array}{ll}a & b \\ b & d\end{array}\right]$ denotes an arbitrary $2 \times 2$ symmetric matrix. Setting (i) $a=1, b=0, d=0$; (ii) $a=0, b=1, d=0$; (iii) $a=0, b=0, d=1$, we obtain the respective matrices:

$$
E_{1}=\left[\begin{array}{cc}
1 & 0 \\
0 & 0
\end{array}\right], \quad E_{2}=\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right], \quad E_{3}=\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]
$$

We claim that $S=\left\{E_{1}, E_{2}, E_{3}\right\}$ is a basis of $W$; that is, (a) $S$ spans $W$ and (b) $S$ is linearly independent.

(a) The above matrix $A=\left[\begin{array}{ll}a & b \\ b & d\end{array}\right]=a E_{1}+b E_{2}+d E_{3}$. Thus, $S$ spans $W$.

(b) Suppose $x E_{1}+y E_{2}+z E_{3}=0$, where $x, y, z$ are unknown scalars. That is, suppose

$$
x\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right]+y\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]+z\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right] \quad \text { or } \quad\left[\begin{array}{ll}
x & y \\
y & z
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right]
$$

Setting corresponding entries equal to each other yields $x=0, y=0, z=0$. Thus, $S$ is linearly independent. Therefore, $S$ is a basis of $W$, as claimed.

\section*{Theorems on Linear Dependence, Basis, and Dimension}
4.33. Prove Lemma 4.10: Suppose two or more nonzero vectors $v_{1}, v_{2}, \ldots, v_{m}$ are linearly dependent. Then one of them is a linear combination of the preceding vectors.

Because the $v_{i}$ are linearly dependent, there exist scalars $a_{1}, \ldots, a_{m}$, not all 0 , such that $a_{1} v_{1}+\cdots+a_{m} v_{m}=0$. Let $k$ be the largest integer such that $a_{k} \neq 0$. Then

$$
a_{1} v_{1}+\cdots+a_{k} v_{k}+0 v_{k+1}+\cdots+0 v_{m}=0 \quad \text { or } \quad a_{1} v_{1}+\cdots+a_{k} v_{k}=0
$$

Suppose $k=1$; then $a_{1} v_{1}=0, a_{1} \neq 0$, and so $v_{1}=0$. But the $v_{i}$ are nonzero vectors. Hence, $k>1$ and

$$
v_{k}=-a_{k}^{-1} a_{1} v_{1}-\cdots-a_{k}^{-1} a_{k-1} v_{k-1}
$$

That is, $v_{k}$ is a linear combination of the preceding vectors.

4.34. Suppose $S=\left\{v_{1}, v_{2}, \ldots, v_{m}\right\}$ spans a vector space $V$.

(a) If $w \in V$, then $\left\{w, v_{1}, \ldots, v_{m}\right\}$ is linearly dependent and spans $V$.

(b) If $v_{i}$ is a linear combination of $v_{1}, \ldots, v_{i-1}$, then $S$ without $v_{i}$ spans $V$.

(a) The vector $w$ is a linear combination of the $v_{i}$, because $\left\{v_{i}\right\}$ spans $V$. Accordingly, $\left\{w, v_{1}, \ldots, v_{m}\right\}$ is linearly dependent. Clearly, $w$ with the $v_{i}$ span $V$, as the $v_{i}$ by themselves span $V$; that is, $\left\{w, v_{1}, \ldots, v_{m}\right\}$ spans $V$.

(b) Suppose $v_{i}=k_{1} v_{1}+\cdots+k_{i-1} v_{i-1}$. Let $u \in V$. Because $\left\{v_{i}\right\}$ spans $V, u$ is a linear combination of the $v_{j}$ 's, say $u=a_{1} v_{1}+\cdots+a_{m} v_{m}$. Substituting for $v_{i}$, we obtain

$$
\begin{aligned}
u & =a_{1} v_{1}+\cdots+a_{i-1} v_{i-1}+a_{i}\left(k_{1} v_{1}+\cdots+k_{i-1} v_{i-1}\right)+a_{i+1} v_{i+1}+\cdots+a_{m} v_{m} \\
& =\left(a_{1}+a_{i} k_{1}\right) v_{1}+\cdots+\left(a_{i-1}+a_{i} k_{i-1}\right) v_{i-1}+a_{i+1} v_{i+1}+\cdots+a_{m} v_{m}
\end{aligned}
$$

Thus, $\left\{v_{1}, \ldots, v_{i-1}, v_{i+1}, \ldots, v_{m}\right\}$ spans $V$. In other words, we can delete $v_{i}$ from the spanning set and still retain a spanning set.

4.35. Prove Lemma 4.13: Suppose $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ spans $V$, and suppose $\left\{w_{1}, w_{2}, \ldots, w_{m}\right\}$ is linearly independent. Then $m \leq n$, and $V$ is spanned by a set of the form

$\left\{w_{1}, w_{2}, \ldots, w_{m}, v_{i_{1}}, v_{i_{2}}, \ldots, v_{i_{n-m}}\right\}$

Thus, any $n+1$ or more vectors in $V$ are linearly dependent.

It suffices to prove the lemma in the case that the $v_{i}$ are all not 0 . (Prove!) Because $\left\{v_{i}\right\}$ spans $V$, we have by Problem 4.34 that


\begin{equation*}
\left\{w_{1}, v_{1}, \ldots, v_{n}\right\} \tag{1}
\end{equation*}


is linearly dependent and also spans $V$. By Lemma 4.10, one of the vectors in (1) is a linear combination of the preceding vectors. This vector cannot be $w_{1}$, so it must be one of the $v$ 's, say $v_{j}$. Thus by Problem 4.34, we can delete $v_{j}$ from the spanning set (1) and obtain the spanning set


\begin{equation*}
\left\{w_{1}, v_{1}, \ldots, v_{j-1}, v_{j+1}, \ldots, v_{n}\right\} \tag{2}
\end{equation*}


Now we repeat the argument with the vector $w_{2}$. That is, because (2) spans $V$, the set


\begin{equation*}
\left\{w_{1}, w_{2}, v_{1}, \ldots, v_{j-1}, v_{j+1}, \ldots, v_{n}\right\} \tag{3}
\end{equation*}


is linearly dependent and also spans $V$. Again by Lemma 4.10, one of the vectors in (3) is a linear combination of the preceding vectors. We emphasize that this vector cannot be $w_{1}$ or $w_{2}$, because $\left\{w_{1}, \ldots, w_{m}\right\}$ is independent; hence, it must be one of the $v$ 's, say $v_{k}$. Thus, by Problem 4.34, we can delete $v_{k}$ from the spanning set (3) and obtain the spanning set

$$
\left\{w_{1}, w_{2}, v_{1}, \ldots, v_{j-1}, v_{j+1}, \ldots, v_{k-1}, v_{k+1}, \ldots, v_{n}\right\}
$$

We repeat the argument with $w_{3}$, and so forth. At each step, we are able to add one of the $w$ 's and delete one of the $v$ 's in the spanning set. If $m \leq n$, then we finally obtain a spanning set of the required form:

$$
\left\{w_{1}, \ldots, w_{m}, v_{i_{1}}, \ldots, v_{i_{n-m}}\right\}
$$

Finally, we show that $m>n$ is not possible. Otherwise, after $n$ of the above steps, we obtain the spanning set $\left\{w_{1}, \ldots, w_{n}\right\}$. This implies that $w_{n+1}$ is a linear combination of $w_{1}, \ldots, w_{n}$, which contradicts the hypothesis that $\left\{w_{i}\right\}$ is linearly independent.

4.36. Prove Theorem 4.12: Every basis of a vector space $V$ has the same number of elements.

Suppose $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis of $V$, and suppose $\left\{v_{1}, v_{2}, \ldots\right\}$ is another basis of $V$. Because $\left\{u_{i}\right\}$ spans $V$, the basis $\left\{v_{1}, v_{2}, \ldots\right\}$ must contain $n$ or less vectors, or else it is linearly dependent by Problem 4.35-Lemma 4.13. On the other hand, if the basis $\left\{v_{1}, v_{2}, \ldots\right\}$ contains less than $n$ elements, then $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is linearly dependent by Problem 4.35. Thus, the basis $\left\{v_{1}, v_{2}, \ldots\right\}$ contains exactly $n$ vectors, and so the theorem is true.

4.37. Prove Theorem 4.14: Let $V$ be a vector space of finite dimension $n$. Then

(i) Any $n+1$ or more vectors must be linearly dependent.

(ii) Any linearly independent set $S=\left\{u_{1}, u_{2}, \ldots u_{n}\right\}$ with $n$ elements is a basis of $V$.

(iii) Any spanning set $T=\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ of $V$ with $n$ elements is a basis of $V$.

Suppose $B=\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ is a basis of $V$.

(i) Because $B$ spans $V$, any $n+1$ or more vectors are linearly dependent by Lemma 4.13.

(ii) By Lemma 4.13, elements from $B$ can be adjoined to $S$ to form a spanning set of $V$ with $n$ elements. Because $S$ already has $n$ elements, $S$ itself is a spanning set of $V$. Thus, $S$ is a basis of $V$.

(iii) Suppose $T$ is linearly dependent. Then some $v_{i}$ is a linear combination of the preceding vectors. By Problem 4.34, $V$ is spanned by the vectors in $T$ without $v_{i}$ and there are $n-1$ of them. By Lemma 4.13, the independent set $B$ cannot have more than $n-1$ elements. This contradicts the fact that $B$ has $n$ elements. Thus, $T$ is linearly independent, and hence $T$ is a basis of $V$.

\subsection*{4.38. Prove Theorem 4.15: Suppose $S$ spans a vector space $V$. Then}
(i) Any maximum number of linearly independent vectors in $S$ form a basis of $V$.

(ii) Suppose one deletes from $S$ every vector that is a linear combination of preceding vectors in $S$. Then the remaining vectors form a basis of $V$.

(i) Suppose $\left\{v_{1}, \ldots, v_{m}\right\}$ is a maximum linearly independent subset of $S$, and suppose $w \in S$. Accordingly, $\left\{v_{1}, \ldots, v_{m}, w\right\}$ is linearly dependent. No $v_{k}$ can be a linear combination of preceding vectors.

Hence, $w$ is a linear combination of the $v_{i}$. Thus, $w \in \operatorname{span}\left(v_{i}\right)$, and hence $S \subseteq \operatorname{span}\left(v_{i}\right)$. This leads to

$$
V=\operatorname{span}(S) \subseteq \operatorname{span}\left(v_{i}\right) \subseteq V
$$

Thus, $\left\{v_{i}\right\}$ spans $V$, and, as it is linearly independent, it is a basis of $V$.

(ii) The remaining vectors form a maximum linearly independent subset of $S$; hence, by (i), it is a basis of $V$.

4.39. Prove Theorem 4.16: Let $V$ be a vector space of finite dimension and let $S=\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ be a set of linearly independent vectors in $V$. Then $S$ is part of a basis of $V$; that is, $S$ may be extended to a basis of $V$.

Suppose $B=\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ is a basis of $V$. Then $B$ spans $V$, and hence $V$ is spanned by

$$
S \cup B=\left\{u_{1}, u_{2}, \ldots, u_{r}, w_{1}, w_{2}, \ldots, w_{n}\right\}
$$

By Theorem 4.15, we can delete from $S \cup B$ each vector that is a linear combination of preceding vectors to obtain a basis $B^{\prime}$ for $V$. Because $S$ is linearly independent, no $u_{k}$ is a linear combination of preceding vectors. Thus, $B^{\prime}$ contains every vector in $S$, and $S$ is part of the basis $B^{\prime}$ for $V$.

4.40. Prove Theorem 4.17: Let $W$ be a subspace of an $n$-dimensional vector space $V$. Then $\operatorname{dim} W \leq n$. In particular, if $\operatorname{dim} W=n$, then $W=V$.

Because $V$ is of dimension $n$, any $n+1$ or more vectors are linearly dependent. Furthermore, because a basis of $W$ consists of linearly independent vectors, it cannot contain more than $n$ elements. Accordingly, $\operatorname{dim} W \leq n$.

In particular, if $\left\{w_{1}, \ldots, w_{n}\right\}$ is a basis of $W$, then, because it is an independent set with $n$ elements, it is also a basis of $V$. Thus, $W=V$ when $\operatorname{dim} W=n$.

\section*{Rank of a Matrix, Row and Column Spaces}
4.41. Find the rank and basis of the row space of each of the following matrices:\\
(a) $A=\left[\begin{array}{rrrr}1 & 2 & 0 & -1 \\ 2 & 6 & -3 & -3 \\ 3 & 10 & -6 & -5\end{array}\right]$\\
(b) $B=\left[\begin{array}{rrrrr}1 & 3 & 1 & -2 & -3 \\ 1 & 4 & 3 & -1 & -4 \\ 2 & 3 & -4 & -7 & -3 \\ 3 & 8 & 1 & -7 & -8\end{array}\right]$.

(a) Row reduce $A$ to echelon form:

$$
A \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & -1 \\
0 & 2 & -3 & -1 \\
0 & 4 & -6 & -2
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 0 & -1 \\
0 & 2 & -3 & -1 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

The two nonzero rows $(1,2,0,-1)$ and $(0,2,-3,-1)$ of the echelon form of $A$ form a basis for rowsp(A). In particular, $\operatorname{rank}(A)=2$.

(b) Row reduce $B$ to echelon form:

$$
B \sim\left[\begin{array}{rrrrr}
1 & 3 & 1 & -2 & -3 \\
0 & 1 & 2 & 1 & -1 \\
0 & -3 & -6 & -3 & 3 \\
0 & -1 & -2 & -1 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 3 & 1 & -2 & -3 \\
0 & 1 & 2 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

The two nonzero rows $(1,3,1,-2,-3)$ and $(0,1,2,1,-1)$ of the echelon form of $B$ form a basis for rowsp(B). In particular, $\operatorname{rank}(B)=2$.

4.42. Show that $U=W$, where $U$ and $W$ are the following subspaces of $\mathbf{R}^{3}$ :

$$
\begin{aligned}
& \left.U=\operatorname{span}\left(u_{1}, u_{2}, u_{3}\right)=\operatorname{span}(1,1,-1),(2,3,-1),(3,1,-5)\right\} \\
& \left.W=\operatorname{span}\left(w_{1}, w_{2}, w_{3}\right)=\operatorname{span}(1,-1,-3),(3,-2,-8),(2,1,-3)\right\}
\end{aligned}
$$

Form the matrix $A$ whose rows are the $u_{i}$, and row reduce $A$ to row canonical form:

$$
A=\left[\begin{array}{lll}
1 & 1 & -1 \\
2 & 3 & -1 \\
3 & 1 & -5
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & -1 \\
0 & 1 & 1 \\
0 & -2 & -2
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 0 & -2 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{array}\right]
$$

Next form the matrix $B$ whose rows are the $w_{j}$, and row reduce $B$ to row canonical form:

$$
B=\left[\begin{array}{rrr}
1 & -1 & -3 \\
3 & -2 & -8 \\
2 & 1 & -3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & -1 & -3 \\
0 & 1 & 1 \\
0 & 3 & 3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 0 & -2 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{array}\right]
$$

Because $A$ and $B$ have the same row canonical form, the row spaces of $A$ and $B$ are equal, and so $U=W$.

4.43. Let $A=\left[\begin{array}{rrrrrr}1 & 2 & 1 & 2 & 3 & 1 \\ 2 & 4 & 3 & 7 & 7 & 4 \\ 1 & 2 & 2 & 5 & 5 & 6 \\ 3 & 6 & 6 & 15 & 14 & 15\end{array}\right]$.

(a) Find $\operatorname{rank}\left(M_{k}\right)$, for $k=1,2, \ldots, 6$, where $M_{k}$ is the submatrix of $A$ consisting of the first $k$ columns $C_{1}, C_{2}, \ldots, C_{k}$ of $A$.

(b) Which columns $C_{k+1}$ are linear combinations of preceding columns $C_{1}, \ldots, C_{k}$ ?

(c) Find columns of $A$ that form a basis for the column space of $A$.

(d) Express column $C_{4}$ as a linear combination of the columns in part (c).

(a) Row reduce $A$ to echelon form:

$$
A \sim\left[\begin{array}{rrrrrr}
1 & 2 & 1 & 2 & 3 & 1 \\
0 & 0 & 1 & 3 & 1 & 2 \\
0 & 0 & 1 & 3 & 2 & 5 \\
0 & 0 & 3 & 9 & 5 & 12
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 2 & 1 & 2 & 3 & 1 \\
0 & 0 & 1 & 3 & 1 & 2 \\
0 & 0 & 0 & 0 & 1 & 3 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Observe that this simultaneously reduces all the matrices $M_{k}$ to echelon form; for example, the first four columns of the echelon form of $A$ are an echelon form of $M_{4}$. We know that $\operatorname{rank}\left(M_{k}\right)$ is equal to the number of pivots or, equivalently, the number of nonzero rows in an echelon form of $M_{k}$. Thus,

$$
\begin{gathered}
\operatorname{rank}\left(M_{1}\right)=\operatorname{rank}\left(M_{2}\right)=1, \quad \operatorname{rank}\left(M_{3}\right)=\operatorname{rank}\left(M_{4}\right)=2 \\
\operatorname{rank}\left(M_{5}\right)=\operatorname{rank}\left(M_{6}\right)=3
\end{gathered}
$$

(b) The vector equation $x_{1} C_{1}+x_{2} C_{2}+\cdots+x_{k} C_{k}=C_{k+1}$ yields the system with coefficient matrix $M_{k}$ and augmented $M_{k+1}$. Thus, $C_{k+1}$ is a linear combination of $C_{1}, \ldots, C_{k}$ if and only if $\operatorname{rank}\left(M_{k}\right)=\operatorname{rank}\left(M_{k+1}\right)$ or, equivalently, if $C_{k+1}$ does not contain a pivot. Thus, each of $C_{2}, C_{4}, C_{6}$ is a linear combination of preceding columns.

(c) In the echelon form of $A$, the pivots are in the first, third, and fifth columns. Thus, columns $C_{1}, C_{3}, C_{5}$ of $A$ form a basis for the columns space of $A$. Alternatively, deleting columns $C_{2}, C_{4}, C_{6}$ from the spanning set of columns (they are linear combinations of other columns), we obtain, again, $C_{1}, C_{3}, C_{5}$.

(d) The echelon matrix tells us that $C_{4}$ is a linear combination of columns $C_{1}$ and $C_{3}$. The augmented matrix $M$ of the vector equation $C_{4}=x C_{1}+y C_{2}$ consists of the columns $C_{1}, C_{3}, C_{4}$ of $A$ which, when reduced to echelon form, yields the matrix (omitting zero rows)

$$
\left[\begin{array}{lll}
1 & 1 & 2 \\
0 & 1 & 3
\end{array}\right] \quad \text { or } \quad \begin{aligned}
x+y & =2 \\
y & =3
\end{aligned} \quad \text { or } \quad x=-1, \quad y=3
$$

Thus, $C_{4}=-C_{1}+3 C_{3}=-C_{1}+3 C_{3}+0 C_{5}$.

4.44. Suppose $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ is a linear combination of the rows $R_{1}, R_{2}, \ldots, R_{m}$ of a matrix $B=\left[b_{i j}\right]$, say $u=k_{1} R_{1}+k_{2} R_{2}+\cdots+k_{m} R_{m}$. Prove that

$$
a_{i}=k_{1} b_{1 i}+k_{2} b_{2 i}+\cdots+k_{m} b_{m i}, \quad i=1,2, \ldots, n
$$

where $b_{1 i}, b_{2 i}, \ldots, b_{m i}$ are the entries in the $i$ th column of $B$.

We are given that $u=k_{1} R_{1}+k_{2} R_{2}+\cdots+k_{m} R_{m}$. Hence,

$$
\begin{aligned}
\left(a_{1}, a_{2}, \ldots, a_{n}\right) & =k_{1}\left(b_{11}, \ldots, b_{1 n}\right)+\cdots+k_{m}\left(b_{m 1}, \ldots, b_{m n}\right) \\
& =\left(k_{1} b_{11}+\cdots+k_{m} b_{m 1}, \ldots, k_{1} b_{1 n}+\cdots+k_{m} b_{m n}\right)
\end{aligned}
$$

Setting corresponding components equal to each other, we obtain the desired result.

4.45. Prove Theorem 4.7: Suppose $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ are row equivalent echelon matrices with respective pivot entries

$$
a_{1 j_{1}}, a_{2 j_{2}}, \ldots, a_{r j_{r}} \quad \text { and } \quad b_{1 k_{1}}, b_{2 k_{2}}, \ldots, b_{s k_{s}}
$$

(pictured in Fig. 4-5). Then $A$ and $B$ have the same number of nonzero rows - that is, $r=s-$ and their pivot entries are in the same positions; that is, $j_{1}=k_{1}, j_{2}=k_{2}, \ldots, j_{r}=k_{r}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-153}
\end{center}

Figure 4-5

Clearly $A=0$ if and only if $B=0$, and so we need only prove the theorem when $r \geq 1$ and $s \geq 1$. We first show that $j_{1}=k_{1}$. Suppose $j_{1}<k_{1}$. Then the $j_{1}$ th column of $B$ is zero. Because the first row $R^{*}$ of $A$ is in the row space of $B$, we have $R^{*}=c_{1} R_{1}+c_{1} R_{2}+\cdots+c_{m} R_{m}$, where the $R_{i}$ are the rows of $B$. Because the $j_{1}$ th column of $B$ is zero, we have

$$
a_{1 j_{1}}=c_{1} 0+c_{2} 0+\cdots+c_{m} 0=0
$$

But this contradicts the fact that the pivot entry $a_{1 j_{1}} \neq 0$. Hence, $j_{1} \geq k_{1}$ and, similarly, $k_{1} \geq j_{1}$. Thus $j_{1}=k_{1}$.

Now let $A^{\prime}$ be the submatrix of $A$ obtained by deleting the first row of $A$, and let $B^{\prime}$ be the submatrix of $B$ obtained by deleting the first row of $B$. We prove that $A^{\prime}$ and $B^{\prime}$ have the same row space. The theorem will then follow by induction, because $A^{\prime}$ and $B^{\prime}$ are also echelon matrices.

Let $R=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ be any row of $A^{\prime}$ and let $R_{1}, \ldots, R_{m}$ be the rows of $B$. Because $R$ is in the row space of $B$, there exist scalars $d_{1}, \ldots, d_{m}$ such that $R=d_{1} R_{1}+d_{2} R_{2}+\cdots+d_{m} R_{m}$. Because $A$ is in echelon form and $R$ is not the first row of $A$, the $j_{1}$ th entry of $R$ is zero: $a_{i}=0$ for $i=j_{1}=k_{1}$. Furthermore, because $B$ is in echelon form, all the entries in the $k_{1}$ th column of $B$ are 0 except the first: $b_{1 k_{1}} \neq 0$, but $b_{2 k_{1}}=0, \ldots, b_{m k_{1}}=0$. Thus,

$$
0=a_{k_{1}}=d_{1} b_{1 k_{1}}+d_{2} 0+\cdots+d_{m} 0=d_{1} b_{1 k_{1}}
$$

Now $b_{1 k_{1}} \neq 0$ and so $d_{1}=0$. Thus, $R$ is a linear combination of $R_{2}, \ldots, R_{m}$ and so is in the row space of $B^{\prime}$. Because $R$ was any row of $A^{\prime}$, the row space of $A^{\prime}$ is contained in the row space of $B^{\prime}$. Similarly, the row space of $B^{\prime}$ is contained in the row space of $A^{\prime}$. Thus, $A^{\prime}$ and $B^{\prime}$ have the same row space, and so the theorem is proved.

4.46. Prove Theorem 4.8: Suppose $A$ and $B$ are row canonical matrices. Then $A$ and $B$ have the same row space if and only if they have the same nonzero rows.

Obviously, if $A$ and $B$ have the same nonzero rows, then they have the same row space. Thus we only have to prove the converse.

Suppose $A$ and $B$ have the same row space, and suppose $R \neq 0$ is the $i$ th row of $A$. Then there exist scalars $c_{1}, \ldots, c_{s}$ such that


\begin{equation*}
R=c_{1} R_{1}+c_{2} R_{2}+\cdots+c_{s} R_{s} \tag{1}
\end{equation*}


where the $R_{i}$ are the nonzero rows of $B$. The theorem is proved if we show that $R=R_{i}$; that is, that $c_{i}=1$ but $c_{k}=0$ for $k \neq i$.

Let $a_{i j}$, be the pivot entry in $R$ - that is, the first nonzero entry of $R$. By (1) and Problem 4.44,


\begin{equation*}
a_{i j_{i}}=c_{1} b_{1 j_{i}}+c_{2} b_{2 j_{i}}+\cdots+c_{s} b_{s j_{i}} \tag{2}
\end{equation*}


But, by Problem 4.45, $b_{i j_{i}}$ is a pivot entry of $B$, and, as $B$ is row reduced, it is the only nonzero entry in the $j$ th column of $B$. Thus, from (2), we obtain $a_{i j_{i}}=c_{i} b_{i j}$. However, $a_{i j_{i}}=1$ and $b_{i j_{i}}=1$, because $A$ and $B$ are row reduced; hence, $c_{i}=1$.

Now suppose $k \neq i$, and $b_{k j_{k}}$ is the pivot entry in $R_{k}$. By (1) and Problem 4.44,


\begin{equation*}
a_{i j_{k}}=c_{1} b_{1 j_{k}}+c_{2} b_{2 j_{k}}+\cdots+c_{s} b_{s j_{k}} \tag{3}
\end{equation*}


Because $B$ is row reduced, $b_{k_{k}}$ is the only nonzero entry in the $j$ th column of $B$. Hence, by (3), $a_{i j_{k}}=c_{k} b_{k j_{k}}$. Furthermore, by Problem 4.45, $a_{k_{k}}$ is a pivot entry of $A$, and because $A$ is row reduced, $a_{i j_{k}}=0$. Thus, $c_{k} b_{k j_{k}}=0$, and as $b_{k j_{k}}=1, c_{k}=0$. Accordingly $R=R_{i}$, and the theorem is proved.

4.47. Prove Corollary 4.9: Every matrix $A$ is row equivalent to a unique matrix in row canonical form.

Suppose $A$ is row equivalent to matrices $A_{1}$ and $A_{2}$, where $A_{1}$ and $A_{2}$ are in row canonical form. Then $\operatorname{rowsp}(A)=\operatorname{rowsp}\left(A_{1}\right)$ and $\operatorname{rowsp}(A)=\operatorname{rowsp}\left(A_{2}\right)$. Hence, $\operatorname{rowsp}\left(A_{1}\right)=\operatorname{rowsp}\left(A_{2}\right)$. Because $A_{1}$ and $A_{2}$ are in row canonical form, $A_{1}=A_{2}$ by Theorem 4.8. Thus, the corollary is proved.

4.48. Suppose $R B$ and $A B$ are defined, where $R$ is a row vector and $A$ and $B$ are matrices. Prove

(a) $R B$ is a linear combination of the rows of $B$.

(b) The row space of $A B$ is contained in the row space of $B$.

(c) The column space of $A B$ is contained in the column space of $A$.

(d) If $C$ is a column vector and $A C$ is defined, then $A C$ is a linear combination of the columns of $A$.

(e) $\operatorname{rank}(A B) \leq \operatorname{rank}(B)$ and $\operatorname{rank}(A B) \leq \operatorname{rank}(A)$.

(a) Suppose $R=\left(a_{1}, a_{2}, \ldots, a_{m}\right)$ and $B=\left[b_{i j}\right]$. Let $B_{1}, \ldots, B_{m}$ denote the rows of $B$ and $B^{1}, \ldots, B^{n}$ its columns. Then

$$
\left.\begin{array}{rl}
R B & =\left(R B^{1}, R B^{2}, \ldots, R B^{n}\right) \\
& =\left(a_{1} b_{11}+a_{2} b_{21}+\cdots+a_{m} b_{m 1}, \quad \ldots, \quad a_{1} b_{1 n}+a_{2} b_{2 n}+\cdots+a_{m} b_{m n}\right.
\end{array}\right)
$$

Thus, $R B$ is a linear combination of the rows of $B$, as claimed.

(b) The rows of $A B$ are $R_{i} B$, where $R_{i}$ is the $i$ th row of $A$. Thus, by part (a), each row of $A B$ is in the row space of $B$. Thus, $\operatorname{rowsp}(A B) \subseteq \operatorname{rowsp}(B)$, as claimed.

(c) Using part (b), we have $\operatorname{colsp}(A B)=\operatorname{rowsp}(A B)^{T}=\operatorname{rowsp}\left(B^{T} A^{T}\right) \subseteq \operatorname{rowsp}\left(A^{T}\right)=\operatorname{colsp}(A)$.

(d) Follows from $(c)$ where $C$ replaces $B$.

(e) The row space of $A B$ is contained in the row space of $B$; hence, $\operatorname{rank}(A B) \leq \operatorname{rank}(B)$. Furthermore, the column space of $A B$ is contained in the column space of $A$; hence, $\operatorname{rank}(A B) \leq \operatorname{rank}(A)$.

4.49. Let $A$ be an $n$-square matrix. Show that $A$ is invertible if and only if $\operatorname{rank}(A)=n$.

Note that the rows of the $n$-square identity matrix $I_{n}$ are linearly independent, because $I_{n}$ is in echelon form; hence, $\operatorname{rank}\left(I_{n}\right)=n$. Now if $A$ is invertible, then $A$ is row equivalent to $I_{n}$; hence, $\operatorname{rank}(A)=n$. But if $A$ is not invertible, then $A$ is row equivalent to a matrix with a zero row; hence, $\operatorname{rank}(A)<n$; that is, $A$ is invertible if and only if $\operatorname{rank}(A)=n$.

\section*{Applications to Linear Equations}
4.50. Find the dimension and a basis of the solution space $W$ of each homogeneous system:

$$
\begin{aligned}
& x+2 y+2 z-s+3 t=0 \quad x+2 y+z-2 t=0 \quad x+y+2 z=0 \\
& x+2 y+3 z+s+t=0 \quad 2 x+4 y+4 z-3 t=0 \quad 2 x+3 y+3 z=0 \\
& 3 x+6 y+8 z+s+5 t=0 \quad 3 x+6 y+7 z-4 t=0 \quad x+3 y+5 z=0 \\
& \begin{aligned}
x+2 y+2 z-s+3 t & =0 & & x+2 y+2 z-s+3 t & =0 \\
z+2 s-2 t & =0 & \text { or } & & =0
\end{aligned} \\
& 2 z+4 s-4 t=0 \quad 2+2 s-2 t=0
\end{aligned}
$$

The system in echelon form has two (nonzero) equations in five unknowns. Hence, the system has $5-2=3$ free variables, which are $y, s, t$. Thus, $\operatorname{dim} W=3$. We obtain a basis for $W$ :

$$
\begin{aligned}
& \text { (1) Set } y=1, s=0, t=0 \quad \text { to obtain the solution } \\
& v_{1}=(-2,1,0,0,0) \text {. } \\
& \text { (2) Set } y=0, s=1, t=0 \quad \text { to obtain the solution } \\
& v_{2}=(5,0,-2,1,0) \text {. } \\
& \text { (3) Set } y=0, s=0, t=1 \\
& \text { to obtain the solution } \quad v_{3}=(-7,0,2,0,1) \text {. }
\end{aligned}
$$

The set $\left\{v_{1}, v_{2}, v_{3}\right\}$ is a basis of the solution space $W$.

(b) (Here we use the matrix format of our homogeneous system.) Reduce the coefficient matrix $A$ to echelon form:

$$
A=\left[\begin{array}{llll}
1 & 2 & 1 & -2 \\
2 & 4 & 4 & -3 \\
3 & 6 & 7 & -4
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 1 & -2 \\
0 & 0 & 2 & 1 \\
0 & 0 & 4 & 2
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 1 & -2 \\
0 & 0 & 2 & 1 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

This corresponds to the system

$$
\begin{aligned}
x+2 y+2 z-2 t & =0 \\
2 z+t & =0
\end{aligned}
$$

The free variables are $y$ and $t$, and $\operatorname{dim} W=2$.

(i) Set $y=1, z=0$ to obtain the solution $u_{1}=(-2,1,0,0)$.

(ii) Set $y=0, z=2$ to obtain the solution $u_{2}=(6,0,-1,2)$.

Then $\left\{u_{1}, u_{2}\right\}$ is a basis of $W$.

(c) Reduce the coefficient matrix $A$ to echelon form:

$$
A=\left[\begin{array}{lll}
1 & 1 & 2 \\
2 & 3 & 3 \\
1 & 3 & 5
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 2 \\
0 & 1 & -1 \\
0 & 2 & 3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 2 \\
0 & 1 & -1 \\
0 & 0 & 5
\end{array}\right]
$$

This corresponds to a triangular system with no free variables. Thus, 0 is the only solution; that is, $W=\{0\}$. Hence, $\operatorname{dim} W=0$.

4.51. Find a homogeneous system whose solution set $W$ is spanned by

$$
\left\{u_{1}, u_{2}, u_{3}\right\}=\{(1,-2,0,3), \quad(1,-1,-1,4), \quad(1,0,-2,5)\}
$$

Let $v=(x, y, z, t)$. Then $v \in W$ if and only if $v$ is a linear combination of the vectors $u_{1}, u_{2}, u_{3}$ that span $W$. Thus, form the matrix $M$ whose first columns are $u_{1}, u_{2}, u_{3}$ and whose last column is $v$, and then row reduce $M$ to echelon form. This yields

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 1 & x \\
-2 & -1 & 0 & y \\
0 & -1 & -2 & z \\
3 & 4 & 5 & t
\end{array}\right] \sim\left[\begin{array}{rrrc}
1 & 1 & 1 & x \\
0 & 1 & 2 & 2 x+y \\
0 & -1 & -2 & z \\
0 & 1 & 2 & -3 x+t
\end{array}\right] \sim\left[\begin{array}{rrcc}
1 & 1 & 1 & x \\
0 & 1 & 2 & 2 x+y \\
0 & 0 & 0 & 2 x+y+z \\
0 & 0 & 0 & -5 x-y+t
\end{array}\right]
$$

Then $v$ is a linear combination of $u_{1}, u_{2}, u_{3}$ if $\operatorname{rank}(M)=\operatorname{rank}(A)$, where $A$ is the submatrix without column $v$. Thus, set the last two entries in the fourth column on the right equal to zero to obtain the required homogeneous system:

$$
\begin{aligned}
& 2 x+y+z=0 \\
& 5 x+y \quad-t=0
\end{aligned}
$$

4.52. Let $x_{i_{1}}, x_{i_{2}}, \ldots, x_{i_{k}}$ be the free variables of a homogeneous system of linear equations with $n$ unknowns. Let $v_{j}$ be the solution for which $x_{i_{j}}=1$, and all other free variables equal 0 . Show that the solutions $v_{1}, v_{2}, \ldots, v_{k}$ are linearly independent.

Let $A$ be the matrix whose rows are the $v_{i}$. We interchange column 1 and column $i_{1}$, then column 2 and column $i_{2}, \ldots$, then column $k$ and column $i_{k}$, and we obtain the $k \times n$ matrix

$$
B=[I, C]=\left[\begin{array}{ccccccccc}
1 & 0 & 0 & \ldots & 0 & 0 & c_{1, k+1} & \ldots & c_{1 n} \\
0 & 1 & 0 & \ldots & 0 & 0 & c_{2, k+1} & \ldots & c_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & 0 & \ldots & 0 & 1 & c_{k, k+1} & \ldots & c_{k n}
\end{array}\right]
$$

The above matrix $B$ is in echelon form, and so its rows are independent; hence, $\operatorname{rank}(B)=k$. Because $A$ and $B$ are column equivalent, they have the same $\operatorname{rank}-\operatorname{rank}(A)=k$. But $A$ has $k$ rows; hence, these rows (i.e., the $v_{i}$ ) are linearly independent, as claimed.

\section*{Sums, Direct Sums, Intersections}
4.53. Let $U$ and $W$ be subspaces of a vector space $V$. Show that

(a) $U+V$ is a subspace of $V$.

(b) $U$ and $W$ are contained in $U+W$.

(c) $U+W$ is the smallest subspace containing $U$ and $W$; that is, $U+W=\operatorname{span}(U, W)$.

(d) $W+W=W$.

(a) Because $U$ and $W$ are subspaces, $0 \in U$ and $0 \in W$. Hence, $0=0+0$ belongs to $U+W$. Now suppose $v, v^{\prime} \in U+W$. Then $v=u+w$ and $v^{\prime}=u^{\prime}+v^{\prime}$, where $u, u^{\prime} \in U$ and $w, w^{\prime} \in W$. Then

$$
a v+b v^{\prime}=\left(a u+b u^{\prime}\right)+\left(a w+b w^{\prime}\right) \in U+W
$$

Thus, $U+W$ is a subspace of $V$.

(b) Let $u \in U$. Because $W$ is a subspace, $0 \in W$. Hence, $u=u+0$ belongs to $U+W$. Thus, $U \subseteq U+W$. Similarly, $W \subseteq U+W$.

(c) Because $U+W$ is a subspace of $V$ containing $U$ and $W$, it must also contain the linear span of $U$ and $W$. That is, $\operatorname{span}(U, W) \subseteq U+W$.

On the other hand, if $v \in U+W$, then $v=u+w=1 u+1 w$, where $u \in U$ and $w \in W$. Thus, $v$ is a linear combination of elements in $U \cup W$, and so $v \in \operatorname{span}(U, W)$. Hence, $U+W \subseteq \operatorname{span}(U, W)$.

The two inclusion relations give the desired result.

(d) Because $W$ is a subspace of $V$, we have that $W$ is closed under vector addition; hence, $W+W \subseteq W$. By part (a), $W \subseteq W+W$. Hence, $W+W=W$.

4.54. Consider the following subspaces of $\mathbf{R}^{5}$ :

$$
\begin{aligned}
& U=\operatorname{span}\left(u_{1}, u_{2}, u_{3}\right)=\operatorname{span}\{(1,3,-2,2,3), \quad(1,4,-3,4,2), \quad(2,3,-1,-2,9)\} \\
& W=\operatorname{span}\left(w_{1}, w_{2}, w_{3}\right)=\operatorname{span}\{(1,3,0,2,1), \quad(1,5,-6,6,3), \quad(2,5,3,2,1)\}
\end{aligned}
$$

Find a basis and the dimension of (a) $U+W$, (b) $U \cap W$.\\
(a) $U+W$ is the space spanned by all six vectors. Hence, form the matrix whose rows are the given six vectors, and then row reduce to echelon form:

$$
\left[\begin{array}{rrrrr}
1 & 3 & -2 & 2 & 3 \\
1 & 4 & -3 & 4 & 2 \\
2 & 3 & -1 & -2 & 9 \\
1 & 3 & 0 & 2 & 1 \\
1 & 5 & -6 & 6 & 3 \\
2 & 5 & 3 & 2 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 3 & -2 & 2 & 3 \\
0 & 1 & -1 & 2 & -1 \\
0 & -3 & 3 & -6 & 3 \\
0 & 0 & 2 & 0 & -2 \\
0 & 2 & -4 & 4 & 0 \\
0 & -1 & 7 & -2 & -5
\end{array}\right] \sim\left[\begin{array}{rrrrr}
1 & 3 & -2 & 2 & 3 \\
0 & 1 & -1 & 2 & -1 \\
0 & 0 & 1 & 0 & -1 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

The following three nonzero rows of the echelon matrix form a basis of $U \cap W$ :

$$
(1,3,-2,2,2,3), \quad(0,1,-1,2,-1), \quad(0,0,1,0,-1)
$$

Thus, $\operatorname{dim}(U+W)=3$.

(b) Let $v=(x, y, z, s, t)$ denote an arbitrary element in $\mathbf{R}^{5}$. First find, say as in Problem 4.49, homogeneous systems whose solution sets are $U$ and $W$, respectively.

Let $M$ be the matrix whose columns are the $u_{i}$ and $v$, and reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrr}
1 & 1 & 2 & x \\
3 & 4 & 3 & y \\
-2 & -3 & -1 & z \\
2 & 4 & -2 & s \\
3 & 2 & 9 & t
\end{array}\right] \sim\left[\begin{array}{cccc}
1 & 1 & 2 & x \\
0 & 1 & -3 & -3 x+y \\
0 & 0 & 0 & -x+y+z \\
0 & 0 & 0 & 4 x-2 y+s \\
0 & 0 & 0 & -6 x+y+t
\end{array}\right]
$$

Set the last three entries in the last column equal to zero to obtain the following homogeneous system whose solution set is $U$ :

$$
-x+y+z=0, \quad 4 x-2 y+s=0, \quad-6 x+y+t=0
$$

Now let $M^{\prime}$ be the matrix whose columns are the $w_{i}$ and $v$, and reduce $M^{\prime}$ to echelon form:

$$
M^{\prime}=\left[\begin{array}{rrrr}
1 & 1 & 2 & x \\
3 & 5 & 5 & y \\
0 & -6 & 3 & z \\
2 & 6 & 2 & s \\
1 & 3 & 1 & t
\end{array}\right] \sim\left[\begin{array}{rrrc}
1 & 1 & 2 & x \\
0 & 2 & -1 & -3 x+y \\
0 & 0 & 0 & -9 x+3 y+z \\
0 & 0 & 0 & 4 x-2 y+s \\
0 & 0 & 0 & 2 x-y+t
\end{array}\right]
$$

Again set the last three entries in the last column equal to zero to obtain the following homogeneous system whose solution set is $W$ :

$$
-9+3+z=0, \quad 4 x-2 y+s=0, \quad 2 x-y+t=0
$$

Combine both of the above systems to obtain a homogeneous system, whose solution space is $U \cap W$, and reduce the system to echelon form, yielding

$$
\begin{aligned}
-x+y+z & =0 \\
2 y+4 z+ & =0 \\
8 z+5 s+2 t & =0 \\
s-2 t & =0
\end{aligned}
$$

There is one free variable, which is $t$; hence, $\operatorname{dim}(U \cap W)=1$. Setting $t=2$, we obtain the solution $u=(1,4,-3,4,2)$, which forms our required basis of $U \cap W$.

4.55. Suppose $U$ and $W$ are distinct four-dimensional subspaces of a vector space $V$, where $\operatorname{dim} V=6$. Find the possible dimensions of $U \cap W$.

Because $U$ and $W$ are distinct, $U+W$ properly contains $U$ and $W$; consequently, $\operatorname{dim}(U+W)>4$. But $\operatorname{dim}(U+W)$ cannot be greater than 6 , as $\operatorname{dim} V=6$. Hence, we have two possibilities: (a) $\operatorname{dim}(U+W)=5$ or (b) $\operatorname{dim}(U+W)=6$. By Theorem 4.20,

$$
\operatorname{dim}(U \cap W)=\operatorname{dim} U+\operatorname{dim} W-\operatorname{dim}(U+W)=8-\operatorname{dim}(U+W)
$$

Thus (a) $\operatorname{dim}(U \cap W)=3$ or (b) $\operatorname{dim}(U \cap W)=2$.

4.56. Let $U$ and $W$ be the following subspaces of $\mathbf{R}^{3}$ :

$U=\{(a, b, c): a=b=c\} \quad$ and $\quad W=\{(0, b, c)\}$

(Note that $W$ is the $y z$-plane.) Show that $\mathbf{R}^{3}=U \oplus W$.

First we show that $U \cap W=\{0\}$. Suppose $v=(a, b, c) \in U \cap W$. Then $a=b=c$ and $a=0$. Hence, $a=0, b=0, c=0$. Thus, $v=0=(0,0,0)$.

Next we show that $\mathbf{R}^{3}=U+W$. For, if $v=(a, b, c) \in \mathbf{R}^{3}$, then

$$
v=(a, a, a)+(0, b-a, c-a) \quad \text { where } \quad(a, a, a) \in U \quad \text { and } \quad(0, b-a, c-a) \in W
$$

Both conditions $U \cap W=\{0\}$ and $U+W=\mathbf{R}^{3}$ imply that $\mathbf{R}^{3}=U \oplus W$.

4.57. Suppose that $U$ and $W$ are subspaces of a vector space $V$ and that $S=\left\{u_{i}\right\}$ spans $U$ and $S^{\prime}=\left\{w_{j}\right\}$ spans $W$. Show that $S \cup S^{\prime}$ spans $U+W$. (Accordingly, by induction, if $S_{i}$ spans $W_{i}$, for $i=1,2, \ldots, n$, then $S_{1} \cup \ldots \cup S_{n}$ spans $W_{1}+\cdots+W_{n}$.)

Let $v \in U+W$. Then $v=u+w$, where $u \in U$ and $w \in W$. Because $S$ spans $U, u$ is a linear combination of $u_{i}$, and as $S^{\prime}$ spans $W, w$ is a linear combination of $w_{j}$; say

$$
u=a_{1} u_{i_{1}}+a_{2} u_{i_{2}}+\cdots+a_{r} u_{i_{r}} \quad \text { and } \quad v=b_{1} w_{j_{1}}+b_{2} w_{j_{2}}+\cdots+b_{s} w_{j_{s}}
$$

where $a_{i}, b_{j} \in K$. Then

$$
v=u+w=a_{1} u_{i_{1}}+a_{2} u_{i_{2}}+\cdots+a_{r} u_{i_{r}}+b_{1} w_{j_{1}}+b_{2} w_{j_{2}}+\cdots+b_{s} w_{j_{s}}
$$

Accordingly, $S \cup S^{\prime}=\left\{u_{i}, w_{j}\right\}$ spans $U+W$.

4.58. Prove Theorem 4.20: Suppose $U$ and $V$ are finite-dimensional subspaces of a vector space $V$. Then $U+W$ has finite dimension and

$$
\operatorname{dim}(U+W)=\operatorname{dim} U+\operatorname{dim} W-\operatorname{dim}(U \cap W)
$$

Observe that $U \cap W$ is a subspace of both $U$ and $W$. Suppose $\operatorname{dim} U=m, \operatorname{dim} W=n$, $\operatorname{dim}(U \cap W)=r$. Suppose $\left\{v_{1}, \ldots, v_{r}\right\}$ is a basis of $U \cap W$. By Theorem 4.16, we can extend $\left\{v_{i}\right\}$ to a basis of $U$ and to a basis of $W$; say

$$
\left\{v_{1}, \ldots, v_{r}, u_{1}, \ldots, u_{m-r}\right\} \quad \text { and } \quad\left\{v_{1}, \ldots, v_{r}, w_{1}, \ldots, w_{n-r}\right\}
$$

are bases of $U$ and $W$, respectively. Let

$$
B=\left\{v_{1}, \ldots, v_{r}, u_{1}, \ldots, u_{m-r}, w_{1}, \ldots, w_{n-r}\right\}
$$

Note that $B$ has exactly $m+n-r$ elements. Thus, the theorem is proved if we can show that $B$ is a basis of $U+W$. Because $\left\{v_{i}, u_{j}\right\}$ spans $U$ and $\left\{v_{i}, w_{k}\right\}$ spans $W$, the union $B=\left\{v_{i}, u_{j}, w_{k}\right\}$ spans $U+W$. Thus, it suffices to show that $B$ is independent.

Suppose


\begin{equation*}
a_{1} v_{1}+\cdots+a_{r} v_{r}+b_{1} u_{1}+\cdots+b_{m-r} u_{m-r}+c_{1} w_{1}+\cdots+c_{n-r} w_{n-r}=0 \tag{1}
\end{equation*}


where $a_{i}, b_{j}, c_{k}$ are scalars. Let


\begin{equation*}
v=a_{1} v_{1}+\cdots+a_{r} v_{r}+b_{1} u_{1}+\cdots+b_{m-r} u_{m-r} \tag{2}
\end{equation*}


By (1), we also have


\begin{equation*}
v=-c_{1} w_{1}-\cdots-c_{n-r} w_{n-r} \tag{3}
\end{equation*}


Because $\left\{v_{i}, u_{j}\right\} \subseteq U, v \in U$ by (2); and as $\left\{w_{k}\right\} \subseteq W, v \in W$ by (3). Accordingly, $v \in U \cap W$. Now $\left\{v_{i}\right\}$ is a basis of $U \cap W$, and so there exist scalars $d_{1}, \ldots, d_{r}$ for which $v=d_{1} v_{1}+\cdots+d_{r} v_{r}$. Thus, by (3), we have

$$
d_{1} v_{1}+\cdots+d_{r} v_{r}+c_{1} w_{1}+\cdots+c_{n-r} w_{n-r}=0
$$

But $\left\{v_{i}, w_{k}\right\}$ is a basis of $W$, and so is independent. Hence, the above equation forces $c_{1}=0, \ldots, c_{n-r}=0$. Substituting this into (1), we obtain

$$
a_{1} v_{1}+\cdots+a_{r} v_{r}+b_{1} u_{1}+\cdots+b_{m-r} u_{m-r}=0
$$

But $\left\{v_{i}, u_{j}\right\}$ is a basis of $U$, and so is independent. Hence, the above equation forces $a_{1}=$ $0, \ldots, a_{r}=0, b_{1}=0, \ldots, b_{m-r}=0$. proved.

Because (1) implies that the $a_{i}, b_{j}, c_{k}$ are all $0, B=\left\{v_{i}, u_{j}, w_{k}\right\}$ is independent, and the theorem is

4.59. Prove Theorem 4.21: $V=U \oplus W$ if and only if (i) $V=U+W$, (ii) $U \cap W=\{0\}$.

Suppose $V=U \oplus W$. Then any $v \in V$ can be uniquely written in the form $v=u+w$, where $u \in U$ and $w \in W$. Thus, in particular, $V=U+W$. Now suppose $v \in U \cap W$. Then

(1) $v=v+0$, where $v \in U, 0 \in W$, (2) $v=0+v$, where $0 \in U, v \in W$.

Thus, $v=0+0=0$ and $U \cap W=\{0\}$.

On the other hand, suppose $V=U+W$ and $U \cap W=\{0\}$. Let $v \in V$. Because $V=U+W$, there exist $u \in U$ and $w \in W$ such that $v=u+w$. We need to show that such a sum is unique. Suppose also that $v=u^{\prime}+w^{\prime}$, where $u^{\prime} \in U$ and $w^{\prime} \in W$. Then

$$
u+w=u^{\prime}+w^{\prime}, \quad \text { and so } \quad u-u^{\prime}=w^{\prime}-w
$$

But $u-u^{\prime} \in U$ and $w^{\prime}-w \in W$; hence, by $U \cap W=\{0\}$,

$$
u-u^{\prime}=0, \quad w^{\prime}-w=0, \quad \text { and so } \quad u=u^{\prime}, \quad w=w^{\prime}
$$

Thus, such a sum for $v \in V$ is unique, and $V=U \oplus W$.

4.60. Prove Theorem 4.22 (for two factors): Suppose $V=U \oplus W$. Also, suppose $S=\left\{u_{1}, \ldots, u_{m}\right\}$ and $S^{\prime}=\left\{w_{1}, \ldots, w_{n}\right\}$ are linearly independent subsets of $U$ and $W$, respectively. Then

(a) The union $S \cup S^{\prime}$ is linearly independent in $V$.

(b) If $S$ and $S^{\prime}$ are bases of $U$ and $W$, respectively, then $S \cup S^{\prime}$ is a basis of $V$.

(c) $\operatorname{dim} V=\operatorname{dim} U+\operatorname{dim} W$.

(a) Suppose $a_{1} u_{1}+\cdots+a_{m} u_{m}+b_{1} w_{1}+\cdots+b_{n} w_{n}=0$, where $a_{i}$, $b_{j}$ are scalars. Then

$$
\left(a_{1} u_{1}+\cdots+a_{m} u_{m}\right)+\left(b_{1} w_{1}+\cdots+b_{n} w_{n}\right)=0=0+0
$$

where $0, a_{1} u_{1}+\cdots+a_{m} u_{m} \in U$ and $0, b_{1} w_{1}+\cdots+b_{n} w_{n} \in W$. Because such a sum for 0 is unique, this leads to

$$
a_{1} u_{1}+\cdots+a_{m} u_{m}=0 \quad \text { and } \quad b_{1} w_{1}+\cdots+b_{n} w_{n}=0
$$

Because $S_{1}$ is linearly independent, each $a_{i}=0$, and because $S_{2}$ is linearly independent, each $b_{j}=0$. Thus, $S=S_{1} \cup S_{2}$ is linearly independent.

(b) By part (a), $S=S_{1} \cup S_{2}$ is linearly independent, and, by Problem 4.55, $S=S_{1} \cup S_{2}$ spans $V=U+W$. Thus, $S=S_{1} \cup S_{2}$ is a basis of $V$.

(c) This follows directly from part (b).

\section*{Coordinates}
4.61. Relative to the basis $S=\left\{u_{1}, u_{2}\right\}=\{(1,1),(2,3)\}$ of $\mathbf{R}^{2}$, find the coordinate vector of $v$, where (a) $v=(4,-3)$, (b) $v=(a, b)$.

In each case, set

$$
v=x u_{1}+y u_{2}=x(1,1)+y(2,3)=(x+2 y, x+3 y)
$$

and then solve for $x$ and $y$.

(a) We have

$$
(4,-3)=(x+2 y, x+3 y) \quad \text { or } \quad \begin{aligned}
x+2 y & =4 \\
x+3 y & =-3
\end{aligned}
$$

The solution is $x=18, y=-7$. Hence, $[v]=[18,-7]$.

(b) We have

$$
(a, b)=(x+2 y, x+3 y) \quad \text { or } \quad \begin{aligned}
& x+2 y=a \\
& x+3 y=b
\end{aligned}
$$

The solution is $x=3 a-2 b, y=-a+b$. Hence, $[v]=\left[\begin{array}{ll}3 a-2 b, & a+b\end{array}\right]$.

4.62. Find the coordinate vector of $v=(a, b, c)$ in $\mathbf{R}^{3}$ relative to

(a) the usual basis $E=\{(1,0,0),(0,1,0),(0,0,1)\}$,

(b) the basis $S=\left\{u_{1}, u_{2}, u_{3}\right\}=\{(1,1,1),(1,1,0),(1,0,0)\}$.

(a) Relative to the usual basis $E$, the coordinates of $[v]_{E}$ are the same as $v$. That is, $[v]_{E}=[a, b, c]$.

(b) Set $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknown scalars $x, y$, $z$. This yields

$$
\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]=x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right]+z\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right] \quad \text { or } \quad \begin{aligned}
& x+y+z=a \\
& x+y=b \\
& x
\end{aligned}
$$

Solving the system yields $x=c, y=b-c, z=a-b$. Thus, $[v]_{S}=[c, b-c, a-b]$.

4.63. Consider the vector space $\mathbf{P}_{3}(t)$ of polynomials of degree $\leq 3$.

(a) Show that $S=\left\{(t-1)^{3},(t-1)^{2}, t-1,1\right\}$ is a basis of $\mathbf{P}_{3}(t)$.

(b) Find the coordinate vector $[v]$ of $v=3 t^{3}-4 t^{2}+2 t-5$ relative to $S$.

(a) The degree of $(t-1)^{k}$ is $k$; writing the polynomials of $S$ in reverse order, we see that no polynomial is a linear combination of preceding polynomials. Thus, the polynomials are linearly independent, and, because $\operatorname{dim} \mathbf{P}_{3}(t)=4$, they form a basis of $\mathbf{P}_{3}(t)$.

(b) Set $v$ as a linear combination of the basis vectors using unknown scalars $x, y, z, s$. We have

$$
\begin{aligned}
v & =3 t^{3}+4 t^{2}+2 t-5=x(t-1)^{3}+y(t-1)^{2}+z(t-1)+s(1) \\
& =x\left(t^{3}-3 t^{2}+3 t-1\right)+y\left(t^{2}-2 t+1\right)+z(t-1)+s(1) \\
& =x t^{3}-3 x t^{2}+3 x t-x+y t^{2}-2 y t+y+z t-z+s \\
& =x t^{3}+(-3 x+y) t^{2}+(3 x-2 y+z) t+(-x+y-z+s)
\end{aligned}
$$

Then set coefficients of the same powers of $t$ equal to each other to obtain

$$
x=3, \quad-3 x+y=4, \quad 3 x-2 y+z=2, \quad-x+y-z+s=-5
$$

Solving the system yields $x=3, y=13, z=19, s=4$. Thus, $[v]=[3,13,19,4]$.

4.64. Find the coordinate vector of $A=\left[\begin{array}{rr}2 & 3 \\ 4 & -7\end{array}\right]$ in the real vector space $\mathbf{M}=\mathbf{M}_{2,2}$ relative to

(a) the basis $S=\left\{\left[\begin{array}{ll}1 & 1 \\ 1 & 1\end{array}\right],\left[\begin{array}{rr}1 & -1 \\ 1 & 0\end{array}\right],\left[\begin{array}{rr}1 & -1 \\ 0 & 0\end{array}\right],\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right]\right\}$,

(b) the usual basis $E=\left\{\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right],\left[\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right],\left[\begin{array}{ll}0 & 0 \\ 1 & 0\end{array}\right],\left[\begin{array}{ll}0 & 0 \\ 0 & 1\end{array}\right]\right\}$

(a) Set $A$ as a linear combination of the basis vectors using unknown scalars $x, y, z, t$ as follows:

$$
A=\left[\begin{array}{rr}
2 & 3 \\
4 & -7
\end{array}\right]=x\left[\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right]+y\left[\begin{array}{rr}
1 & -1 \\
1 & 0
\end{array}\right]+z\left[\begin{array}{rr}
1 & -1 \\
0 & 0
\end{array}\right]+t\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right]=\left[\begin{array}{cc}
x+z+t & x-y-z \\
x+y & x
\end{array}\right]
$$

Set corresponding entries equal to each other to obtain the system

$$
x+z+t=2, \quad x-y-z=3, \quad x+y=4, \quad x=-7
$$

Solving the system yields $x=-7, y=11, z=-21, t=30$. Thus, $[A]_{S}=[-7,11,-21,30]$. (Note that the coordinate vector of $A$ is a vector in $\mathbf{R}^{4}$, because $\operatorname{dim} \mathbf{M}=4$.)

(b) Expressing $A$ as a linear combination of the basis matrices yields

$$
\left[\begin{array}{rr}
2 & 3 \\
4 & -7
\end{array}\right]=x\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right]+y\left[\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right]+z\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right]+t\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]=\left[\begin{array}{ll}
x & y \\
z & t
\end{array}\right]
$$

Thus, $x=2, y=3, z=4, t=-7$. Hence, $[A]=[2,3,4,-7]$, whose components are the elements of $A$ written row by row.

Remark: This result is true in general; that is, if $A$ is any $m \times n$ matrix in $\mathbf{M}=\mathbf{M}_{m, n}$, then the coordinates of $A$ relative to the usual basis of $\mathbf{M}$ are the elements of $A$ written row by row.

4.65. In the space $\mathbf{M}=\mathbf{M}_{2,3}$, determine whether or not the following matrices are linearly dependent:

$$
A=\left[\begin{array}{rrr}
1 & 2 & 3 \\
4 & 0 & 5
\end{array}\right], \quad B=\left[\begin{array}{rrr}
2 & 4 & 7 \\
10 & 1 & 13
\end{array}\right], \quad C=\left[\begin{array}{rrr}
1 & 2 & 5 \\
8 & 2 & 11
\end{array}\right]
$$

If the matrices are linearly dependent, find the dimension and a basis of the subspace $W$ of $\mathbf{M}$ spanned by the matrices.

The coordinate vectors of the above matrices relative to the usual basis of $\mathbf{M}$ are as follows:

$$
[A]=[1,2,3,4,0,5], \quad[B]=[2,4,7,10,1,13], \quad[C]=[1,2,5,8,2,11]
$$

Form the matrix $M$ whose rows are the above coordinate vectors, and reduce $M$ to echelon form:

$$
M=\left[\begin{array}{rrrrrr}
1 & 2 & 3 & 4 & 0 & 5 \\
2 & 4 & 7 & 10 & 1 & 13 \\
1 & 2 & 5 & 8 & 2 & 11
\end{array}\right] \sim\left[\begin{array}{llllll}
1 & 2 & 3 & 4 & 0 & 5 \\
0 & 0 & 1 & 2 & 1 & 3 \\
0 & 0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

Because the echelon matrix has only two nonzero rows, the coordinate vectors $[A],[B],[C]$ span a space of dimension two, and so they are linearly dependent. Thus, $A, B, C$ are linearly dependent. Furthermore, $\operatorname{dim} W=2$, and the matrices

$$
w_{1}=\left[\begin{array}{lll}
1 & 2 & 3 \\
4 & 0 & 5
\end{array}\right] \quad \text { and } \quad w_{2}=\left[\begin{array}{lll}
0 & 0 & 1 \\
2 & 1 & 3
\end{array}\right]
$$

corresponding to the nonzero rows of the echelon matrix form a basis of $W$.

\section*{Miscellaneous Problems}
4.66. Consider a finite sequence of vectors $S=\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$. Let $T$ be the sequence of vectors obtained from $S$ by one of the following "elementary operations": (i) interchange two vectors, (ii) multiply a vector by a nonzero scalar, (iii) add a multiple of one vector to another. Show that $S$ and $T$ span the same space $W$. Also show that $T$ is independent if and only if $S$ is independent.

Observe that, for each operation, the vectors in $T$ are linear combinations of vectors in $S$. On the other hand, each operation has an inverse of the same type (Prove!); hence, the vectors in $S$ are linear combinations of vectors in $T$. Thus $S$ and $T$ span the same space $W$. Also, $T$ is independent if and only if $\operatorname{dim} W=n$, and this is true if and only if $S$ is also independent.

4.67. Let $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ be row equivalent $m \times n$ matrices over a field $K$, and let $v_{1}, \ldots, v_{n}$ be any vectors in a vector space $V$ over $K$. Let

$$
\begin{aligned}
& u_{1}=a_{11} v_{1}+a_{12} v_{2}+\cdots+a_{1 n} v_{n} \quad w_{1}=b_{11} v_{1}+b_{12} v_{2}+\cdots+b_{1 n} v_{n} \\
& u_{2}=a_{21} v_{1}+a_{22} v_{2}+\cdots+a_{2 n} v_{n} \quad w_{2}=b_{21} v_{1}+b_{22} v_{2}+\cdots+b_{2 n} v_{n} \\
& u_{m}=a_{m 1} v_{1}+a_{m 2} v_{2}+\cdots+a_{m n} v_{n} \quad w_{m}=b_{m 1} v_{1}+b_{m 2} v_{2}+\cdots+b_{m n} v_{n}
\end{aligned}
$$

Show that $\left\{u_{i}\right\}$ and $\left\{w_{i}\right\}$ span the same space.

Applying an "elementary operation" of Problem 4.66 to $\left\{u_{i}\right\}$ is equivalent to applying an elementary row operation to the matrix $A$. Because $A$ and $B$ are row equivalent, $B$ can be obtained from $A$ by a sequence of elementary row operations; hence, $\left\{w_{i}\right\}$ can be obtained from $\left\{u_{i}\right\}$ by the corresponding sequence of operations. Accordingly, $\left\{u_{i}\right\}$ and $\left\{w_{i}\right\}$ span the same space.

4.68. Let $v_{1}, \ldots, v_{n}$ belong to a vector space $V$ over $K$, and let $P=\left[a_{i j}\right]$ be an $n$-square matrix over $K$. Let

$$
w_{1}=a_{11} v_{1}+a_{12} v_{2}+\cdots+a_{1 n} v_{n}, \quad \cdots, \quad w_{n}=a_{n 1} v_{1}+a_{n 2} v_{2}+\cdots+a_{n n} v_{n}
$$

(a) Suppose $P$ is invertible. Show that $\left\{w_{i}\right\}$ and $\left\{v_{i}\right\}$ span the same space; hence, $\left\{w_{i}\right\}$ is independent if and only if $\left\{v_{i}\right\}$ is independent.

(b) Suppose $P$ is not invertible. Show that $\left\{w_{i}\right\}$ is dependent.

(c) Suppose $\left\{w_{i}\right\}$ is independent. Show that $P$ is invertible.\\
(a) Because $P$ is invertible, it is row equivalent to the identity matrix $I$. Hence, by Problem 4.67, $\left\{w_{i}\right\}$ and $\left\{v_{i}\right\}$ span the same space. Thus, one is independent if and only if the other is.

(b) Because $P$ is not invertible, it is row equivalent to a matrix with a zero row. This means that $\left\{w_{i}\right\}$ spans a space that has a spanning set of less than $n$ elements. Thus, $\left\{w_{i}\right\}$ is dependent.

(c) This is the contrapositive of the statement of (b), and so it follows from (b).

4.69. Suppose that $A_{1}, A_{2}, \ldots$ are linearly independent sets of vectors, and that $A_{1} \subseteq A_{2} \subseteq \ldots$ Show that the union $A=A_{1} \cup A_{2} \cup \ldots$ is also linearly independent.

Suppose $A$ is linearly dependent. Then there exist vectors $v_{1}, \ldots, v_{n} \in A$ and scalars $a_{1}, \ldots, a_{n} \in K$, not all of them 0 , such that


\begin{equation*}
a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}=0 \tag{1}
\end{equation*}


Because $A=\cup A_{i}$ and the $v_{i} \in A$, there exist sets $A_{i_{1}}, \ldots, A_{i_{n}}$ such that

$$
v_{1} \in A_{i_{1}}, \quad v_{2} \in A_{i_{2}}, \quad \ldots, \quad v_{n} \in A_{i_{n}}
$$

Let $k$ be the maximum index of the sets $A_{i_{j}}: k=\max \left(i_{1}, \ldots, i_{n}\right)$. It follows then, as $A_{1} \subseteq A_{2} \subseteq \ldots$, that each $A_{i_{j}}$ is contained in $A_{k}$. Hence, $v_{1}, v_{2}, \ldots, v_{n} \in A_{k}$, and so, by (1), $A_{k}$ is linearly dependent, which contradicts our hypothesis. Thus, $A$ is linearly independent.

4.70. Let $K$ be a subfield of a field $L$, and let $L$ be a subfield of a field $E$. (Thus, $K \subseteq L \subseteq E$, and $K$ is a subfield of $E$.) Suppose $E$ is of dimension $n$ over $L$, and $L$ is of dimension $m$ over $K$. Show that $E$ is of dimension $m n$ over $K$.

Suppose $\left\{v_{1}, \ldots, v_{n}\right\}$ is a basis of $E$ over $L$ and $\left\{a_{1}, \ldots, a_{m}\right\}$ is a basis of $L$ over $K$. We claim that $\left\{a_{i} v_{j}: i=1, \ldots, m, j=1, \ldots, n\right\}$ is a basis of $E$ over $K$. Note that $\left\{a_{i} v_{j}\right\}$ contains $m n$ elements.

Let $w$ be any arbitrary element in $E$. Because $\left\{v_{1}, \ldots, v_{n}\right\}$ spans $E$ over $L, w$ is a linear combination of the $v_{i}$ with coefficients in $L$ :


\begin{equation*}
w=b_{1} v_{1}+b_{2} v_{2}+\cdots+b_{n} v_{n}, \quad b_{i} \in L \tag{1}
\end{equation*}


Because $\left\{a_{1}, \ldots, a_{m}\right\}$ spans $L$ over $K$, each $b_{i} \in L$ is a linear combination of the $a_{j}$ with coefficients in $K$ :

$$
\begin{aligned}
& b_{1}=k_{11} a_{1}+k_{12} a_{2}+\cdots+k_{1 m} a_{m} \\
& b_{2}=k_{21} a_{1}+k_{22} a_{2}+\cdots+k_{2 m} a_{m} \\
& b_{n}=k_{n 1} a_{1}+k_{n 2} a_{2}+\cdots+k_{m n} a_{m}
\end{aligned}
$$

where $k_{i j} \in K$. Substituting in (1), we obtain

$$
\begin{aligned}
w & =\left(k_{11} a_{1}+\cdots+k_{1 m} a_{m}\right) v_{1}+\left(k_{21} a_{1}+\cdots+k_{2 m} a_{m}\right) v_{2}+\cdots+\left(k_{n 1} a_{1}+\cdots+k_{n m} a_{m}\right) v_{n} \\
& =k_{11} a_{1} v_{1}+\cdots+k_{1 m} a_{m} v_{1}+k_{21} a_{1} v_{2}+\cdots+k_{2 m} a_{m} v_{2}+\cdots+k_{n 1} a_{1} v_{n}+\cdots+k_{n m} a_{m} v_{n} \\
& =\sum_{i, j} k_{j i}\left(a_{i} v_{j}\right)
\end{aligned}
$$

where $k_{j i} \in K$. Thus, $w$ is a linear combination of the $a_{i} v_{j}$ with coefficients in $K$; hence, $\left\{a_{i} v_{j}\right\}$ spans $E$ over K.

The proof is complete if we show that $\left\{a_{i} v_{j}\right\}$ is linearly independent over $K$. Suppose, for scalars $x_{j i} \in K$, we have $\sum_{i, j} x_{j i}\left(a_{i} v_{j}\right)=0$; that is,

$$
\left(x_{11} a_{1} v_{1}+x_{12} a_{2} v_{1}+\cdots+x_{1 m} a_{m} v_{1}\right)+\cdots+\left(x_{n 1} a_{1} v_{n}+x_{n 2} a_{2} v_{n}+\cdots+x_{n m} a_{m} v_{m}\right)=0
$$

or

$$
\left(x_{11} a_{1}+x_{12} a_{2}+\cdots+x_{1 m} a_{m}\right) v_{1}+\cdots+\left(x_{n 1} a_{1}+x_{n 2} a_{2}+\cdots+x_{n m} a_{m}\right) v_{n}=0
$$

Because $\left\{v_{1}, \ldots, v_{n}\right\}$ is linearly independent over $L$ and the above coefficients of the $v_{i}$ belong to $L$, each coefficient must be 0 :

$$
x_{11} a_{1}+x_{12} a_{2}+\cdots+x_{1 m} a_{m}=0, \quad \ldots, \quad x_{n 1} a_{1}+x_{n 2} a_{2}+\cdots+x_{n m} a_{m}=0
$$

But $\left\{a_{1}, \ldots, a_{m}\right\}$ is linearly independent over $K$; hence, because the $x_{j i} \in K$,

$$
x_{11}=0, x_{12}=0, \ldots, x_{1 m}=0, \ldots, x_{n 1}=0, x_{n 2}=0, \ldots, x_{n m}=0
$$

Accordingly, $\left\{a_{i} v_{j}\right\}$ is linearly independent over $K$, and the theorem is proved.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Vector Spaces}
4.71. Suppose $u$ and $v$ belong to a vector space $V$. Simplify each of the following expressions:\\
(a) $E_{1}=4(5 u-6 v)+2(3 u+v)$,\\
(c) $E_{3}=6(3 u+2 v)+5 u-7 v$,\\
(b) $E_{2}=5(2 u-3 v)+4(7 v+8)$,\\
(d) $E_{4}=3(5 u+2 / v)$.

4.72. Let $V$ be the set of ordered pairs $(a, b)$ of real numbers with addition in $V$ and scalar multiplication on $V$ defined by

$$
(a, b)+(c, d)=(a+c, \quad b+d) \quad \text { and } \quad k(a, b)=(k a, 0)
$$

Show that $V$ satisfies all the axioms of a vector space except $\left[\mathrm{M}_{4}\right]$ - that is, except $1 u=u$. Hence, $\left[\mathrm{M}_{4}\right]$ is not a consequence of the other axioms.

4.73. Show that Axiom $\left[\mathrm{A}_{4}\right]$ of a vector space $V$ (that $u+v=v+u$ ) can be derived from the other axioms for $V$.

4.74. Let $V$ be the set of ordered pairs $(a, b)$ of real numbers. Show that $V$ is not a vector space over $\mathbf{R}$ with addition and scalar multiplication defined by

(i) $(a, b)+(c, d)=(a+d, b+c)$ and $k(a, b)=(k a, k b)$,

(ii) $(a, b)+(c, d)=(a+c, b+d)$ and $k(a, b)=(a, b)$,

(iii) $(a, b)+(c, d)=(0,0)$ and $k(a, b)=(k a, k b)$,

(iv) $(a, b)+(c, d)=(a c, b d)$ and $k(a, b)=(k a, k b)$.

4.75. Let $V$ be the set of infinite sequences $\left(a_{1}, a_{2}, \ldots\right)$ in a field $K$. Show that $V$ is a vector space over $K$ with addition and scalar multiplication defined by

$$
\left(a_{1}, a_{2}, \ldots\right)+\left(b_{1}, b_{2}, \ldots\right)=\left(a_{1}+b_{1}, a_{2}+b_{2}, \ldots\right) \quad \text { and } \quad k\left(a_{1}, a_{2}, \ldots\right)=\left(k a_{1}, k a_{2}, \ldots\right)
$$

4.76. Let $U$ and $W$ be vector spaces over a field $K$. Let $V$ be the set of ordered pairs $(u, w)$ where $u \in U$ and $w \in W$. Show that $V$ is a vector space over $K$ with addition in $V$ and scalar multiplication on $V$ defined by

$$
(u, w)+\left(u^{\prime}, w^{\prime}\right)=\left(u+u^{\prime}, w+w^{\prime}\right) \quad \text { and } \quad k(u, w)=(k u, k w)
$$

(This space $V$ is called the external direct product of $U$ and $W$.)

\section*{Subspaces}
4.77. Determine whether or not $W$ is a subspace of $\mathbf{R}^{3}$ where $W$ consists of all vectors $(a, b, c)$ in $\mathbf{R}^{3}$ such that\\
(a) $a=3 b$,\\
(b) $a \leq b \leq c$,\\
(c) $a b=0$,\\
(d) $a+b+c=0$,\\
(e) $b=a^{2}, \quad(f) a=2 b=3 c$.

4.78. Let $V$ be the vector space of $n$-square matrices over a field $K$. Show that $W$ is a subspace of $V$ if $W$ consists of all matrices $A=\left[a_{i j}\right]$ that are\\
(a) symmetric ( $A^{T}=A$ or $a_{i j}=a_{j i}$ ),\\
(b) (upper) triangular,\\
(c) diagonal,\\
(d) scalar.

4.79. Let $A X=B$ be a nonhomogeneous system of linear equations in $n$ unknowns; that is, $B \neq 0$. Show that the solution set is not a subspace of $K^{n}$.

4.80. Suppose $U$ and $W$ are subspaces of $V$ for which $U \cup W$ is a subspace. Show that $U \subseteq W$ or $W \subseteq U$.

4.81. Let $V$ be the vector space of all functions from the real field $\mathbf{R}$ into $\mathbf{R}$. Show that $W$ is a subspace of $V$ where $W$ consists of all: (a) bounded functions, (b) even functions. [Recall that $f: \mathbf{R} \rightarrow \mathbf{R}$ is bounded if $\exists M \in \mathbf{R}$ such that $\forall x \in \mathbf{R}$, we have $|f(x)| \leq M$; and $f(x)$ is even if $f(-x)=f(x), \forall x \in \mathbf{R}$.]

4.82. Let $V$ be the vector space (Problem 4.75) of infinite sequences $\left(a_{1}, a_{2}, \ldots\right)$ in a field $K$. Show that $W$ is a subspace of $V$ if $W$ consists of all sequences with (a) 0 as the first element, (b) only a finite number of nonzero elements.

\section*{Linear Combinations, Linear Spans}
4.83. Consider the vectors $u=(1,2,3)$ and $v=(2,3,1)$ in $\mathbf{R}^{3}$.

(a) Write $w=(1,3,8)$ as a linear combination of $u$ and $v$.

(b) Write $w=(2,4,5)$ as a linear combination of $u$ and $v$.

(c) Find $k$ so that $w=(1, k, 4)$ is a linear combination of $u$ and $v$.

(d) Find conditions on $a, b, c$ so that $w=(a, b, c)$ is a linear combination of $u$ and $v$.

4.84. Write the polynomial $f(t)=a t^{2}+b t+c$ as a linear combination of the polynomials $p_{1}=(t-1)^{2}$, $p_{2}=t-1, p_{3}=1$. [Thus, $p_{1}, p_{2}, p_{3}$ span the space $\mathbf{P}_{2}(t)$ of polynomials of degree $\leq 2$.]

4.85. Find one vector in $\mathbf{R}^{3}$ that spans the intersection of $U$ and $W$ where $U$ is the $x y$-plane-that is, $U=\{(a, b, 0)\}$ - and $W$ is the space spanned by the vectors $(1,1,1)$ and $(1,2,3)$.

4.86. Prove that $\operatorname{span}(S)$ is the intersection of all subspaces of $V$ containing $S$.

4.87. Show that $\operatorname{span}(S)=\operatorname{span}(S \cup\{0\})$. That is, by joining or deleting the zero vector from a set, we do not change the space spanned by the set.

4.88. Show that (a) If $S \subseteq T$, then $\operatorname{span}(S) \subseteq \operatorname{span}(T)$. (b) $\operatorname{span}[\operatorname{span}(S)]=\operatorname{span}(S)$.

\section*{Linear Dependence and Linear Independence}
4.89. Determine whether the following vectors in $\mathbf{R}^{4}$ are linearly dependent or independent:\\
(a) $(1,2,-3,1),(3,7,1,-2),(1,3,7,-4)$;\\
(b) $(1,3,1,-2),(2,5,-1,3),(1,3,7,-2)$.

4.90. Determine whether the following polynomials $u, v, w$ in $\mathbf{P}(t)$ are linearly dependent or independent:

(a) $u=t^{3}-4 t^{2}+3 t+3, \quad v=t^{3}+2 t^{2}+4 t-1, \quad w=2 t^{3}-t^{2}-3 t+5$;

(b) $u=t^{3}-5 t^{2}-2 t+3, v=t^{3}-4 t^{2}-3 t+4, \quad w=2 t^{3}-17 t^{2}-7 t+9$.

4.91. Show that the following functions $f, g, h$ are linearly independent:\\
(a) $f(t)=e^{t}, g(t)=\sin t, h(t)=t^{2}$;\\
(b) $f(t)=e^{t}, g(t)=e^{2 t}, h(t)=t$.

4.92. Show that $u=(a, b)$ and $v=(c, d)$ in $K^{2}$ are linearly dependent if and only if $a d-b c=0$.

4.93. Suppose $u, v, w$ are linearly independent vectors. Prove that $S$ is linearly independent where\\
(a) $S=\{u+v-2 w, u-v-w, u+w\}$;\\
(b) $S=\{u+v-3 w, u+3 v-w, v+w\}$.

4.94. Suppose $\left\{u_{1}, \ldots, u_{r}, w_{1}, \ldots, w_{s}\right\}$ is a linearly independent subset of $V$. Show that

$$
\operatorname{span}\left(u_{i}\right) \cap \operatorname{span}\left(w_{j}\right)=\{0\}
$$

4.95. Suppose $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent. Prove that $S$ is linearly independent where

(a) $S=\left\{a_{1} v_{1}, a_{2} v_{2}, \ldots, a_{n} v_{n}\right\}$ and each $a_{i} \neq 0$.

(b) $S=\left\{v_{1}, \ldots, v_{k-1}, w, v_{k+1}, \ldots, v_{n}\right\}$ and $w=\sum_{i} b_{i} v_{i}$ and $b_{k} \neq 0$.

4.96. Suppose $\left(a_{11}, \ldots, a_{1 n}\right),\left(a_{21}, \ldots, a_{2 n}\right), \ldots,\left(a_{m 1}, \ldots, a_{m n}\right)$ are linearly independent vectors in $K^{n}$, and suppose $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent vectors in a vector space $V$ over $K$. Show that the following\\
vectors are also linearly independent:

$$
w_{1}=a_{11} v_{1}+\cdots+a_{1 n} v_{n}, \quad w_{2}=a_{21} v_{1}+\cdots+a_{2 n} v_{n}, \quad \cdots, \quad w_{m}=a_{m 1} v_{1}+\cdots+a_{m n} v_{n}
$$

\section*{Basis and Dimension}
4.97. Find a subset of $u_{1}, u_{2}, u_{3}, u_{4}$ that gives a basis for $W=\operatorname{span}\left(u_{i}\right)$ of $\mathbf{R}^{5}$, where

(a) $u_{1}=(1,1,1,2,3), \quad u_{2}=(1,2,-1,-2,1), \quad u_{3}=(3,5,-1,-2,5), \quad u_{4}=(1,2,1,-1,4)$

(b) $u_{1}=(1,-2,1,3,-1), \quad u_{2}=(-2,4,-2,-6,2), \quad u_{3}=(1,-3,1,2,1), \quad u_{4}=(3,-7,3,8,-1)$

(c) $u_{1}=(1,0,1,0,1), \quad u_{2}=(1,1,2,1,0), \quad u_{3}=(2,1,3,1,1), \quad u_{4}=(1,2,1,1,1)$

(d) $u_{1}=(1,0,1,1,1), \quad u_{2}=(2,1,2,0,1), \quad u_{3}=(1,1,2,3,4), \quad u_{4}=(4,2,5,4,6)$

4.98. Consider the subspaces $U=\{(a, b, c, d): b-2 c+d=0\}$ and $W=\{(a, b, c, d): a=d, b=2 c\}$ of $\mathbf{R}^{4}$. Find a basis and the dimension of (a) $U$, (b) $W$, (c) $U \cap W$.

4.99. Find a basis and the dimension of the solution space $W$ of each of the following homogeneous systems:\\
(a) $x+2 y-2 z+2 s-t=0$\\
$x+2 y-z+3 s-2 t=0$\\
(b) $x+2 y-z+3 s-4 t=0$\\
$2 x+4 y-7 z+s+t=0$\\
$2 x+4 y-2 z-s+5 t=0$\\
$2 x+4 y-2 z+4 s-2 t=0$

4.100. Find a homogeneous system whose solution space is spanned by the following sets of three vectors:

(a) $(1,-2,0,3,-1),(2,-3,2,5,-3),(1,-2,1,2,-2)$;

(b) $(1,1,2,1,1),(1,2,1,4,3),(3,5,4,9,7)$.

4.101. Determine whether each of the following is a basis of the vector space $\mathbf{P}_{n}(t)$ :

(a) $\left\{1, \quad 1+t, \quad 1+t+t^{2}, \quad 1+t+t^{2}+t^{3}, \quad \ldots, \quad 1+t+t^{2}+\cdots+t^{n-1}+t^{n}\right\}$

(b) $\left\{1+t, \quad t+t^{2}, \quad t^{2}+t^{3}, \quad \ldots, \quad t^{n-2}+t^{n-1}, \quad t^{n-1}+t^{n}\right\}$.

4.102. Find a basis and the dimension of the subspace $W$ of $\mathbf{P}(t)$ spanned by

(a) $u=t^{3}+2 t^{2}-2 t+1, \quad v=t^{3}+3 t^{2}-3 t+4, \quad w=2 t^{3}+t^{2}-7 t-7$,

(b) $u=t^{3}+t^{2}-3 t+2, \quad v=2 t^{3}+t^{2}+t-4, \quad w=4 t^{3}+3 t^{2}-5 t+2$.

4.103. Find a basis and the dimension of the subspace $W$ of $V=\mathbf{M}_{2,2}$ spanned by

$$
A=\left[\begin{array}{rr}
1 & -5 \\
-4 & 2
\end{array}\right], \quad B=\left[\begin{array}{rr}
1 & 1 \\
-1 & 5
\end{array}\right], \quad C=\left[\begin{array}{rr}
2 & -4 \\
-5 & 7
\end{array}\right], \quad D=\left[\begin{array}{rr}
1 & -7 \\
-5 & 1
\end{array}\right]
$$

\section*{Rank of a Matrix, Row and Column Spaces}
4.104. Find the rank of each of the following matrices:\\
(a) $\left[\begin{array}{rrrrr}1 & 3 & -2 & 5 & 4 \\ 1 & 4 & 1 & 3 & 5 \\ 1 & 4 & 2 & 4 & 3 \\ 2 & 7 & -3 & 6 & 13\end{array}\right]$,\\
(b) $\left[\begin{array}{rrrr}1 & 2 & -3 & -2 \\ 1 & 3 & -2 & 0 \\ 3 & 8 & -7 & -2 \\ 2 & 1 & -9 & -10\end{array}\right]$,\\
(c) $\left[\begin{array}{rrr}1 & 1 & 2 \\ 4 & 5 & 5 \\ 5 & 8 & 1 \\ -1 & -2 & 2\end{array}\right]$

4.105. For $k=1,2, \ldots, 5$, find the number $n_{k}$ of linearly independent subsets consisting of $k$ columns for each of the following matrices:\\
(a) $A=\left[\begin{array}{lllll}1 & 1 & 0 & 2 & 3 \\ 1 & 2 & 0 & 2 & 5 \\ 1 & 3 & 0 & 2 & 7\end{array}\right]$,\\
(b) $B=\left[\begin{array}{lllll}1 & 2 & 1 & 0 & 2 \\ 1 & 2 & 3 & 0 & 4 \\ 1 & 1 & 5 & 0 & 6\end{array}\right]$

4.106. Let (a) $A=\left[\begin{array}{rrrrrr}1 & 2 & 1 & 3 & 1 & 6 \\ 2 & 4 & 3 & 8 & 3 & 15 \\ 1 & 2 & 2 & 5 & 3 & 11 \\ 4 & 8 & 6 & 16 & 7 & 32\end{array}\right], \quad$ (b) $B=\left[\begin{array}{rrrrrr}1 & 2 & 2 & 1 & 2 & 1 \\ 2 & 4 & 5 & 4 & 5 & 5 \\ 1 & 2 & 3 & 4 & 4 & 6 \\ 3 & 6 & 7 & 7 & 9 & 10\end{array}\right]$

For each matrix (where $C_{1}, \ldots, C_{6}$ denote its columns):

(i) Find its row canonical form $M$.

(ii) Find the columns that are linear combinations of preceding columns.

(iii) Find columns (excluding $C_{6}$ ) that form a basis for the column space.

(iv) Express $C_{6}$ as a linear combination of the basis vectors obtained in (iii).

4.107. Determine which of the following matrices have the same row space:

$$
A=\left[\begin{array}{ccc}
1 & -2 & -1 \\
3 & -4 & 5
\end{array}\right], \quad B=\left[\begin{array}{ccc}
1 & -1 & 2 \\
2 & 3 & -1
\end{array}\right], \quad C=\left[\begin{array}{ccc}
1 & -1 & 3 \\
2 & -1 & 10 \\
3 & -5 & 1
\end{array}\right]
$$

4.108. Determine which of the following subspaces of $\mathbf{R}^{3}$ are identical:

$$
\begin{gathered}
U_{1}=\operatorname{span}[(1,1,-1), \quad(2,3,-1), \quad(3,1,-5)], \quad U_{2}=\operatorname{span}[(1,-1,-3),(3,-2,-8),(2,1,-3)] \\
U_{3}=\operatorname{span}[(1,1,1),(1,-1,3),(3,-1,7)]
\end{gathered}
$$

4.109. Determine which of the following subspaces of $\mathbf{R}^{4}$ are identical:

$$
\begin{gathered}
U_{1}=\operatorname{span}[(1,2,1,4), \quad(2,4,1,5), \quad(3,6,2,9)], \quad U_{2}=\operatorname{span}[(1,2,1,2), \quad(2,4,1,3)] \\
U_{3}=\operatorname{span}[(1,2,3,10), \quad(2,4,3,11)]
\end{gathered}
$$

4.110. Find a basis for (i) the row space and (ii) the column space of each matrix M:\\
(a) $M=\left[\begin{array}{rrrrr}0 & 0 & 3 & 1 & 4 \\ 1 & 3 & 1 & 2 & 1 \\ 3 & 9 & 4 & 5 & 2 \\ 4 & 12 & 8 & 8 & 7\end{array}\right]$,\\
(b) $M=\left[\begin{array}{rrrrr}1 & 2 & 1 & 0 & 1 \\ 1 & 2 & 2 & 1 & 3 \\ 3 & 6 & 5 & 2 & 7 \\ 2 & 4 & 1 & -1 & 0\end{array}\right]$.

4.111. Show that if any row is deleted from a matrix in echelon (respectively, row canonical) form, then the resulting matrix is still in echelon (respectively, row canonical) form.

4.112. Let $A$ and $B$ be arbitrary $m \times n$ matrices. Show that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A)+\operatorname{rank}(B)$.

4.113. Let $r=\operatorname{rank}(A+B)$. Find $2 \times 2$ matrices $A$ and $B$ such that\\
(a) $r<\operatorname{rank}(A), \operatorname{rank}(\mathrm{B})$;\\
(b) $r=\operatorname{rank}(A)=\operatorname{rank}(B)$;\\
(c) $r>\operatorname{rank}(A), \operatorname{rank}(\mathrm{B})$.

\section*{Sums, Direct Sums, Intersections}
4.114. Suppose $U$ and $W$ are two-dimensional subspaces of $K^{3}$. Show that $U \cap W \neq\{0\}$.

4.115. Suppose $U$ and $W$ are subspaces of $V$ such that $\operatorname{dim} U=4, \operatorname{dim} W=5$, and $\operatorname{dim} V=7$. Find the possible dimensions of $U \cap W$.

4.116. Let $U$ and $W$ be subspaces of $\mathbf{R}^{3}$ for which $\operatorname{dim} U=1, \operatorname{dim} W=2$, and $U \nsubseteq W$. Show that $\mathbf{R}^{3}=U \oplus W$.

4.117. Consider the following subspaces of $\mathbf{R}^{5}$ :

$$
\begin{aligned}
& U=\operatorname{span}[(1,-1,-1,-2,0), \quad(1,-2,-2,0,-3), \quad(1,-1,-2,-2,1)] \\
& W=\operatorname{span}[(1,-2,-3,0,-2), \quad(1,-1,-3,2,-4), \quad(1,-1,-2,2,-5)]
\end{aligned}
$$

(a) Find two homogeneous systems whose solution spaces are $U$ and $W$, respectively.

(b) Find a basis and the dimension of $U \cap W$.

4.118. Let $U_{1}, U_{2}, U_{3}$ be the following subspaces of $\mathbf{R}^{3}$ :

$$
U_{1}=\{(a, b, c): a=c\}, \quad U_{2}=\{(a, b, c): a+b+c=0\}, \quad U_{3}=\{(0,0, c)\}
$$

Show that (a) $\mathbf{R}^{3}=U_{1}+U_{2}$, (b) $\mathbf{R}^{3}=U_{2}+U_{3}$, (c) $\mathbf{R}^{3}=U_{1}+U_{3}$. When is the sum direct?

4.119. Suppose $U, W_{1}, W_{2}$ are subspaces of a vector space $V$. Show that

$$
\left(U \cap W_{1}\right)+\left(U \cap W_{2}\right) \subseteq U \cap\left(W_{1}+W_{2}\right)
$$

Find subspaces of $\mathbf{R}^{2}$ for which equality does not hold.

4.120. Suppose $W_{1}, W_{2}, \ldots, W_{r}$ are subspaces of a vector space $V$. Show that

(a) $\operatorname{span}\left(W_{1}, W_{2}, \ldots, W_{r}\right)=W_{1}+W_{2}+\cdots+W_{r}$.

(b) If $S_{i}$ spans $W_{i}$ for $i=1, \ldots, r$, then $S_{1} \cup S_{2} \cup \cdots \cup S_{r}$ spans $W_{1}+W_{2}+\cdots+W_{r}$.

4.121. Suppose $V=U \oplus W$. Show that $\operatorname{dim} V=\operatorname{dim} U+\operatorname{dim} W$.

4.122. Let $S$ and $T$ be arbitrary nonempty subsets (not necessarily subspaces) of a vector space $V$ and let $k$ be a scalar. The sum $S+T$ and the scalar product $k S$ are defined by

$$
S+T=(u+v: u \in S, \quad v \in T\}, \quad k S=\{k u: u \in S\}
$$

[We also write $w+S$ for $\{w\}+S$.] Let

$$
S=\{(1,2),(2,3)\}, \quad T=\{(1,4),(1,5),(2,5)\}, \quad w=(1,1), \quad k=3
$$

Find: (a) $S+T$, (b) $w+S$, (c) $k S$, (d) $k T$, (e) $k S+k T$, (f) $k(S+T)$.

4.123. Show that the above operations of $S+T$ and $k S$ satisfy

(a) Commutative law: $S+T=T+S$.

(b) Associative law: $\left(S_{1}+S_{2}\right)+S_{3}=S_{1}+\left(S_{2}+S_{3}\right)$.

(c) Distributive law: $k(S+T)=k S+k T$.

(d) $S+\{0\}=\{0\}+S=S$ and $S+V=V+S=V$.

4.124. Let $V$ be the vector space of $n$-square matrices. Let $U$ be the subspace of upper triangular matrices, and let $W$ be the subspace of lower triangular matrices. Find (a) $U \cap W$, (b) $U+W$.

4.125. Let $V$ be the external direct sum of vector spaces $U$ and $W$ over a field $K$. (See Problem 4.76.) Let

$$
\hat{U}=\{(u, 0): u \in U\} \quad \text { and } \quad \hat{W}=\{(0, w): w \in W\}
$$

Show that (a) $\hat{U}$ and $\hat{W}$ are subspaces of $V$, (b) $V=\hat{U} \oplus \hat{W}$.

4.126. Suppose $V=U+W$. Let $\hat{V}$ be the external direct sum of $U$ and $W$. Show that $V$ is isomorphic to $\hat{V}$ under the correspondence $v=u+w \leftrightarrow(u, w)$.

4.127. Use induction to prove (a) Theorem 4.22, (b) Theorem 4.23.

\section*{Coordinates}
4.128. The vectors $u_{1}=(1,-2)$ and $u_{2}=(4,-7)$ form a basis $S$ of $\mathbf{R}^{2}$. Find the coordinate vector $[v]$ of $v$ relative to $S$ where (a) $v=(5,3)$, (b) $v=(a, b)$.

4.129. The vectors $u_{1}=(1,2,0), u_{2}=(1,3,2), u_{3}=(0,1,3)$ form a basis $S$ of $\mathbf{R}^{3}$. Find the coordinate vector $[v]$ of $v$ relative to $S$ where (a) $v=(2,7,-4)$, (b) $v=(a, b, c)$.

4.130. $S=\left\{t^{3}+t^{2}, \quad t^{2}+t, \quad t+1, \quad 1\right\}$ is a basis of $\mathbf{P}_{3}(t)$. Find the coordinate vector $[v]$ of $v$ relative to $S$ where (a) $v=2 t^{3}+t^{2}-4 t+2$, (b) $v=a t^{3}+b t^{2}+c t+d$.

4.131. Let $V=\mathbf{M}_{2,2}$. Find the coordinate vector $[A]$ of $A$ relative to $S$ where\\
$S=\left\{\left[\begin{array}{ll}1 & 1 \\ 1 & 1\end{array}\right], \quad\left[\begin{array}{rr}1 & -1 \\ 1 & 0\end{array}\right], \quad\left[\begin{array}{ll}1 & 1 \\ 0 & 0\end{array}\right], \quad\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right]\right\} \quad$ and\\
(a) $A=\left[\begin{array}{rr}3 & -5 \\ 6 & 7\end{array}\right]$,\\
(b) $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$

4.132. Find the dimension and a basis of the subspace $W$ of $\mathbf{P}_{3}(t)$ spanned by

$$
u=t^{3}+2 t^{2}-3 t+4, \quad v=2 t^{3}+5 t^{2}-4 t+7, \quad w=t^{3}+4 t^{2}+t+2
$$

4.133. Find the dimension and a basis of the subspace $W$ of $\mathbf{M}=\mathbf{M}_{2,3}$ spanned by

$$
A=\left[\begin{array}{lll}
1 & 2 & 1 \\
3 & 1 & 2
\end{array}\right], \quad B=\left[\begin{array}{lll}
2 & 4 & 3 \\
7 & 5 & 6
\end{array}\right], \quad C=\left[\begin{array}{lll}
1 & 2 & 3 \\
5 & 7 & 6
\end{array}\right]
$$

\section*{Miscellaneous Problems}
4.134. Answer true or false. If false, prove it with a counterexample.

(a) If $u_{1}, u_{2}, u_{3}$ span $V$, then $\operatorname{dim} V=3$.

(b) If $A$ is a $4 \times 8$ matrix, then any six columns are linearly dependent.

(c) If $u_{1}, u_{2}, u_{3}$ are linearly independent, then $u_{1}, u_{2}, u_{3}, w$ are linearly dependent.

(d) If $u_{1}, u_{2}, u_{3}, u_{4}$ are linearly independent, then $\operatorname{dim} V \geq 4$.

(e) If $u_{1}, u_{2}, u_{3}$ span $V$, then $w, u_{1}, u_{2}, u_{3}$ span $V$.

(f) If $u_{1}, u_{2}, u_{3}, u_{4}$ are linearly independent, then $u_{1}, u_{2}, u_{3}$ are linearly independent.

4.135. Answer true or false. If false, prove it with a counterexample.

(a) If any column is deleted from a matrix in echelon form, then the resulting matrix is still in echelon form.

(b) If any column is deleted from a matrix in row canonical form, then the resulting matrix is still in row canonical form.

(c) If any column without a pivot is deleted from a matrix in row canonical form, then the resulting matrix is in row canonical form.

4.136. Determine the dimension of the vector space $W$ of the following $n$-square matrices:\\
(a) symmetric matrices,\\
(b) antisymmetric matrices,\\
(d) diagonal matrices,\\
(c) scalar matrices.

4.137. Let $t_{1}, t_{2}, \ldots, t_{n}$ be symbols, and let $K$ be any field. Let $V$ be the following set of expressions where $a_{i} \in K$ :

$$
a_{1} t_{1}+a_{2} t_{2}+\cdots+a_{n} t_{n}
$$

Define addition in $V$ and scalar multiplication on $V$ by

$$
\begin{gathered}
\left(a_{1} t_{1}+\cdots+a_{n} t_{n}\right)+\left(b_{1} t_{1}+\cdots+b_{n} t_{n}\right)=\left(a_{1}+b_{1}\right) t_{1}+\cdots+\left(a_{n} b_{n m}\right) t_{n} \\
k\left(a_{1} t_{1}+a_{2} t_{2}+\cdots+a_{n} t_{n}\right)=k a_{1} t_{1}+k a_{2} t_{2}+\cdots+k a_{n} t_{n}
\end{gathered}
$$

Show that $V$ is a vector space over $K$ with the above operations. Also, show that $\left\{t_{1}, \ldots, t_{n}\right\}$ is a basis of $V$, where

$$
t_{j}=0 t_{1}+\cdots+0 t_{j-1}+1 t_{j}+0 t_{j+1}+\cdots+0 t_{n}
$$


[Some answers, such as bases, need not be unique.]

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}

4.71. (a) $E_{1}=26 u-22 v$;

(b) The sum $7 v+8$ is not defined, so $E_{2}$ is not defined;

(c) $E_{3}=23 u+5 v$

(d) Division by $v$ is not defined, so $E_{4}$ is not defined.

4.77. (a) Yes; (b) No; e.g., $(1,2,3) \in W$ but $-2(1,2,3) \notin W$;\\
(c) No; e.g., $(1,0,0),(0,1,0) \in W$, but not their sum;\\
(d) Yes;

(e) No; e.g., $(1,1,1) \in W$, but $2(1,1,1) \notin W$;

(f) Yes

4.79. The zero vector 0 is not a solution.

4.83. (a) $w=3 u_{1}-u_{2}$,

(b) Impossible,

(c) $k=\frac{11}{5}$,

(d) $7 a-5 b+c=0$

4.84. Using $f=x p_{1}+y p_{2}+z p_{3}$, we get $x=a, y=2 a+b, z=a+b+c$

4.85. $v=(2,1,0)$

4.89. (a) Dependent,

(b) Independent

4.90. (a) Independent,

(b) Dependent

4.97. (a) $u_{1}, u_{2}, u_{4}$;

(b) $u_{1}, u_{2}, u_{3}$;

(c) $u_{1}, u_{2}, u_{4}$;

(d) $u_{1}, u_{2}, u_{3}$

4.98. (a) $\operatorname{dim} U=3$,

(b) $\operatorname{dim} W=2$,

(c) $\operatorname{dim}(U \cap W)=1$

4.99. (a) Basis: $\{(2,-1,0,0,0), \quad(4,0,1,-1,0), \quad(3,0,1,0,1)\} ; \operatorname{dim} W=3$;

(b) Basis: $\{(2,-1,0,0,0), \quad(1,0,1,0,0)\}$; $\operatorname{dim} W=2$

4.100. (a) $5 x+y-z-s=0, \quad x+y-z-t=0$;

(b) $3 x-y-z=0, \quad 2 x-3 y+s=0, \quad x-2 y+t=0$

4.101. (a) Yes, (b) No, because $\operatorname{dim} \mathbf{P}_{n}(t)=n+1$, but the set contains only $n$ elements.

4.102. (a) $\operatorname{dim} W=2, \quad$ (b) $\operatorname{dim} W=3$

4.103. $\operatorname{dim} W=2$

4.104. (a) $3, \quad$ (b) $2, \quad$ (c) 3

4.105. (a) $n_{1}=4, \quad n_{2}=5, \quad n_{3}=n_{4}=n_{5}=0 ; \quad$ (b) $n_{1}=4, \quad n_{2}=6, \quad n_{3}=3, \quad n_{4}=n_{5}=0$

4.106. (a) (i) $M=[1,2,0,1,0,3 ; \quad 0,0,1,2,0,1 ; \quad 0,0,0,0,1,2 ; \quad 0]$;

(ii) $C_{2}, C_{4}, C_{6}$; (iii) $C_{1}, C_{3}, C_{5}$; (iv) $C_{6}=3 C_{1}+C_{3}+2 C_{5}$.

(b) (i) $M=[1,2,0,0,3,1 ; \quad 0,0,1,0,-1,-1 ; \quad 0,0,0,1,1,2 ; \quad 0]$;

(ii) $C_{2}, C_{5}, C_{6}$; (iii) $C_{1}, C_{3}, C_{4}$; (iv) $C_{6}=C_{1}-C_{3}+2 C_{4}$

4.107. $A$ and $C$ are row equivalent to $\left[\begin{array}{lll}1 & 0 & 7 \\ 0 & 1 & 4\end{array}\right]$, but not $B$

4.108. $U_{1}$ and $U_{2}$ are row equivalent to $\left[\begin{array}{rrr}1 & 0 & -2 \\ 0 & 1 & 1\end{array}\right]$, but not $U_{3}$

4.109. $U_{1}$ and $U_{3}$ are row equivalent to $\left[\begin{array}{llll}1 & 2 & 0 & 1 \\ 0 & 0 & 1 & 3\end{array}\right]$, but not $U_{2}$

4.110. (a) (i) $(1,3,1,2,1),(0,0,1,-1,-1),(0,0,0,4,7)$; (ii) $C_{1}, C_{3}, C_{4}$;

(b) (i) $(1,2,1,0,1),(0,0,1,1,2)$; (ii) $C_{1}, C_{3}$

4.113. (a) $A=\left[\begin{array}{ll}1 & 1 \\ 0 & 0\end{array}\right], B=\left[\begin{array}{cr}-1 & -1 \\ 0 & 0\end{array}\right]$;

(b) $A=\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right], B=\left[\begin{array}{ll}0 & 2 \\ 0 & 0\end{array}\right]$;

(c) $A=\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right], B=\left[\begin{array}{ll}0 & 0 \\ 0 & 1\end{array}\right]$

4.115. $\operatorname{dim}(U \cap W)=2,3$, or 4

4.117. (a) (i) $\begin{aligned} 3 x+4 y-z-t & =0 \\ 4 x+2 y+s & =0\end{aligned}$

(ii) $\begin{aligned} 4 x+2 y-s & =0 \\ 9 x+2 y+z+t & =0\end{aligned}$;

(b) Basis: $\{(1,-2,-5,0,0), \quad(0,0,1,0,-1)\} ; \operatorname{dim}(U \cap W)=2$

4.118. The sum is direct in (b) and (c).

4.119. In $\mathbf{R}^{2}$, let $U, V, W$ be, respectively, the line $y=x$, the $x$-axis, the $y$-axis.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-170}
\end{center}

4.124. (a) Diagonal matrices, (b) $V$

4.128. (a) $[-41,11], \quad$ (b) $[-7 a-4 b, 2 a+b]$

4.129. (a) $[-11,13,-10], \quad$ (b) $[c-3 b+7 a, \quad-c+3 b-6 a, \quad c-2 b+4 a]$

4.130. (a) $[2,-1,-2,2], \quad$ (b) $[a, \quad b-c, \quad c-b+a, \quad d-c+b-a]$

4.131. (a) $[7,-1,-13,10], \quad$ (b) $[d, \quad c-d, \quad b+c-2 d, \quad a-b-2 c+2 d]$

4.132. $\operatorname{dim} W=2$; basis: $\left\{t^{3}+2 t^{2}-3 t+4, \quad t^{2}+2 t-1\right\}$

4.133. $\operatorname{dim} W=2$; basis: $\{[1,2,1,3,1,2], \quad[0,0,1,1,3,2]\}$

4.134. (a) False; $(1,1),(1,2),(2,1)$ span $\mathbf{R}^{2}$; (b) True;

(c) False; $(1,0,0,0),(0,1,0,0),(0,0,1,0), w=(0,0,0,1)$;

(d) True; (e) True; (f) True

4.135. (a) True; (b) False; e.g. delete $C_{2}$ from $\left[\begin{array}{lll}1 & 0 & 3 \\ 0 & 1 & 2\end{array}\right]$; (c) True

4.136. (a) $\frac{1}{2} n(n+1)$, (b) $\frac{1}{2} n(n-1)$, (c) $n$, (d) 1

\section*{CHAPTER 5}
\section*{Linear Mappings}
\subsection*{5.1 Introduction}
The main subject matter of linear algebra is the study of linear mappings and their representation by means of matrices. This chapter introduces us to these linear maps and Chapter 6 shows how they can be represented by matrices. First, however, we begin with a study of mappings in general.

\subsection*{5.2 Mappings, Functions}
Let $A$ and $B$ be arbitrary nonempty sets. Suppose to each element in $a \in A$ there is assigned a unique element of $B$; called the image of $a$. The collection $f$ of such assignments is called a mapping (or map) from $A$ into $B$, and it is denoted by

$$
f: A \rightarrow B
$$

The set $A$ is called the domain of the mapping, and $B$ is called the target set. We write $f(a)$, read " $f$ of $a$," for the unique element of $B$ that $f$ assigns to $a \in A$.

One may also view a mapping $f: A \rightarrow B$ as a computer that, for each input value $a \in A$, produces a unique output $f(a) \in B$.

Remark: The term function is used synonymously with the word mapping, although some texts reserve the word "function" for a real-valued or complex-valued mapping.

Consider a mapping $f: A \rightarrow B$. If $A^{\prime}$ is any subset of $A$, then $f\left(A^{\prime}\right)$ denotes the set of images of elements of $A^{\prime}$; and if $B^{\prime}$ is any subset of $B$, then $f^{-1}\left(B^{\prime}\right)$ denotes the set of elements of $A$, each of whose image lies in $B$. That is,

$$
f\left(A^{\prime}\right)=\left\{f(a): a \in A^{\prime}\right\} \quad \text { and } \quad f^{-1}\left(B^{\prime}\right)=\left\{a \in A: f(a) \in B^{\prime}\right\}
$$

We call $f\left(A^{\prime}\right)$ the image of $A^{\prime}$ and $f^{-1}\left(B^{\prime}\right)$ the inverse image or preimage of $B^{\prime}$. In particular, the set of all images (i.e., $f(A)$ ) is called the image or range of $f$.

To each mapping $f: A \rightarrow B$ there corresponds the subset of $A \times B$ given by $\{(a, f(a)): a \in A\}$. We call this set the graph of $f$. Two mappings $f: A \rightarrow B$ and $g: A \rightarrow B$ are defined to be equal, written $f=g$, if $f(a)=g(a)$ for every $a \in A$ - that is, if they have the same graph. Thus, we do not distinguish between a function and its graph. The negation of $f=g$ is written $f \neq g$ and is the statement:

$$
\text { There exists an } a \in A \text { for which } f(a) \neq g(a) \text {. }
$$

Sometimes the "barred" arrow $\mapsto$ is used to denote the image of an arbitrary element $x \in A$ under a mapping $f: A \rightarrow B$ by writing

$$
x \mapsto f(x)
$$

This is illustrated in the following example.

\section*{EXAMPLE 5.1}
(a) Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be the function that assigns to each real number $x$ its square $x^{2}$. We can denote this function by writing

$f(x)=x^{2} \quad$ or $\quad x \mapsto x^{2}$

Here the image of -3 is 9 , so we may write $f(-3)=9$. However, $f^{-1}(9)=\{3,-3\}$. Also, $f(\mathbf{R})=[0, \infty)=\{x: x \geq 0\}$ is the image of $f$.

(b) Let $A=\{a, b, c, d\}$ and $B=\{x, y, z, t\}$. Then the following defines a mapping $f: A \rightarrow B$ :

$f(a)=y, f(b)=x, f(c)=z, f(d)=y \quad$ or $\quad f=\{(a, y),(b, x),(c, z),(d, y)\}$

The first defines the mapping explicitly, and the second defines the mapping by its graph. Here,

$f(\{a, b, d\})=\{f(a), f(b), f(d)\}=\{y, x, y\}=\{x, y\}$

Furthermore, $f(A)=\{x, y, z\}$ is the image of $f$.

EXAMPLE 5.2 Let $V$ be the vector space of polynomials over $\mathbf{R}$, and let $p(t)=3 t^{2}-5 t+2$.

(a) The derivative defines a mapping $\mathbf{D}: V \rightarrow V$ where, for any polynomials $f(t)$, we have $\mathbf{D}(f)=d f / d t$. Thus,

$\mathbf{D}(p)=\mathbf{D}\left(3 t^{2}-5 t+2\right)=6 t-5$

(b) The integral, say from 0 to 1 , defines a mapping $\mathbf{J}: V \rightarrow \mathbf{R}$. That is, for any polynomial $f(t)$,

$$
\mathbf{J}(f)=\int_{0}^{1} f(t) d t, \quad \text { and so } \quad \mathbf{J}(p)=\int_{0}^{1}\left(3 t^{2}-5 t+2\right)=\frac{1}{2}
$$

Observe that the mapping in $(b)$ is from the vector space $V$ into the scalar field $\mathbf{R}$, whereas the mapping in $(a)$ is from the vector space $V$ into itself.

\section*{Matrix Mappings}
Let $A$ be any $m \times n$ matrix over $K$. Then $A$ determines a mapping $F_{A}: K^{n} \rightarrow K^{m}$ by

$$
F_{A}(u)=A u
$$

where the vectors in $K^{n}$ and $K^{m}$ are written as columns. For example, suppose

$$
A=\left[\begin{array}{rrr}
1 & -4 & 5 \\
2 & 3 & -6
\end{array}\right] \quad \text { and } \quad u=\left[\begin{array}{r}
1 \\
3 \\
-5
\end{array}\right]
$$

then

$$
F_{A}(u)=A u=\left[\begin{array}{rrr}
1 & -4 & 5 \\
2 & 3 & -6
\end{array}\right]\left[\begin{array}{r}
1 \\
3 \\
-5
\end{array}\right]=\left[\begin{array}{r}
-36 \\
41
\end{array}\right]
$$

Remark: For notational convenience, we will frequently denote the mapping $F_{A}$ by the letter $A$, the same symbol as used for the matrix.

\section*{Composition of Mappings}
Consider two mappings $f: A \rightarrow B$ and $g: B \rightarrow C$, illustrated below:

$$
A \xrightarrow{f} B \xrightarrow{g} C
$$

The composition of $f$ and $g$, denoted by $g \circ f$, is the mapping $g \circ f: A \rightarrow C$ defined by

$$
(g \circ f)(a) \equiv g(f(a))
$$

That is, first we apply $f$ to $a \in A$, and then we apply $g$ to $f(a) \in B$ to get $g(f(a)) \in C$. Viewing $f$ and $g$ as "computers," the composition means we first input $a \in A$ to get the output $f(a) \in B$ using $f$, and then we input $f(a)$ to get the output $g(f(a)) \in C$ using $g$.

Our first theorem tells us that the composition of mappings satisfies the associative law.

THEOREM 5.1: $\quad$ Let $f: A \rightarrow B, g: B \rightarrow C, h: C \rightarrow D$. Then

$$
h \circ(g \circ f)=(h \circ g) \circ f
$$

We prove this theorem here. Let $a \in A$. Then

$$
\begin{aligned}
& (h \circ(g \circ f))(a)=h((g \circ f)(a))=h(g(f(a))) \\
& ((h \circ g) \circ f)(a)=(h \circ g)(f(a))=h(g(f(a)))
\end{aligned}
$$

Thus, $(h \circ(g \circ f))(a)=((h \circ g) \circ f)(a)$ for every $a \in A$, and so $h \circ(g \circ f)=(h \circ g) \circ f$.

\section*{One-to-One and Onto Mappings}
We formally introduce some special types of mappings.

DEFINITION: A mapping $f: A \rightarrow B$ is said to be one-to-one (or 1-1 or injective) if different elements of $A$ have distinct images; that is,

$$
\text { If } f(a)=f\left(a^{\prime}\right) \text {, then } a=a^{\prime} \text {. }
$$

DEFINITION: A mapping $f: A \rightarrow B$ is said to be onto (or $f$ maps $A$ onto $B$ or surjective) if every $b \in B$ is the image of at least one $a \in A$.

DEFINITION: A mapping $f: A \rightarrow B$ is said to be a one-to-one correspondence between $A$ and $B$ (or bijective) if $f$ is both one-to-one and onto.

EXAMPLE 5.3 Let $f: \mathbf{R} \rightarrow \mathbf{R}, g: \mathbf{R} \rightarrow \mathbf{R}, h: \mathbf{R} \rightarrow \mathbf{R}$ be defined by

$$
f(x)=2^{x}, \quad g(x)=x^{3}-x, \quad h(x)=x^{2}
$$

The graphs of these functions are shown in Fig. 5-1. The function $f$ is one-to-one. Geometrically, this means that each horizontal line does not contain more than one point of $f$. The function $g$ is onto. Geometrically, this means that each horizontal line contains at least one point of $g$. The function $h$ is neither one-to-one nor onto. For example, both 2 and -2 have the same image 4, and -16 has no preimage.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-173(1)}
\end{center}

$f(x)=2^{x}$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-173}
\end{center}

$g(x)=x^{3}-x$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-173(2)}
\end{center}

$h(x)=x^{2}$

Figure 5-1

\section*{Identity and Inverse Mappings}
Let $A$ be any nonempty set. The mapping $f: A \rightarrow A$ defined by $f(a)=a$-that is, the function that assigns to each element in $A$ itself-is called identity mapping. It is usually denoted by $\mathbf{1}_{A}$ or $\mathbf{1}$ or $I$. Thus, for any $a \in A$, we have $\mathbf{1}_{A}(a)=a$.

Now let $f: A \rightarrow B$. We call $g: B \rightarrow A$ the inverse of $f$, written $f^{-1}$, if

$$
f \circ g=\mathbf{1}_{B} \quad \text { and } \quad g \circ f=\mathbf{1}_{A}
$$

We emphasize that $f$ has an inverse if and only if $f$ is a one-to-one correspondence between $A$ and $B$; that is, $f$ is one-to-one and onto (Problem 5.7). Also, if $b \in B$, then $f^{-1}(b)=a$, where $a$ is the unique element of $A$ for which $f(a)=b$

\subsection*{5.3 Linear Mappings (Linear Transformations)}
We begin with a definition.

DEFINITION: $\quad$ Let $V$ and $U$ be vector spaces over the same field $K$. A mapping $F: V \rightarrow U$ is called a linear mapping or linear transformation if it satisfies the following two conditions:

(1) For any vectors $v, w \in V, F(v+w)=F(v)+F(w)$.

(2) For any scalar $k$ and vector $v \in V, F(k v)=k F(v)$.

Namely, $F: V \rightarrow U$ is linear if it "preserves" the two basic operations of a vector space, that of vector addition and that of scalar multiplication.

Substituting $k=0$ into condition (2), we obtain $F(0)=0$. Thus, every linear mapping takes the zero vector into the zero vector.

Now for any scalars $a, b \in K$ and any vector $v, w \in V$, we obtain

$$
F(a v+b w)=F(a v)+F(b w)=a F(v)+b F(w)
$$

More generally, for any scalars $a_{i} \in K$ and any vectors $v_{i} \in V$, we obtain the following basic property of linear mappings:

$$
F\left(a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{m} v_{m}\right)=a_{1} F\left(v_{1}\right)+a_{2} F\left(v_{2}\right)+\cdots+a_{m} F\left(v_{m}\right)
$$

Remark 1: A linear mapping $F: V \rightarrow U$ is completely characterized by the condition


\begin{equation*}
F(a v+b w)=a F(v)+b F(w) \tag{*}
\end{equation*}


and so this condition is sometimes used as its defintion.

Remark 2: The term linear transformation rather than linear mapping is frequently used for linear mappings of the form $F: \mathbf{R}^{n} \rightarrow \mathbf{R}^{m}$.

\section*{EXAMPLE 5.4}
(a) Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the "projection" mapping into the $x y$-plane; that is, $F$ is the mapping defined by $F(x, y, z)=(x, y, 0)$. We show that $F$ is linear. Let $v=(a, b, c)$ and $w=\left(a^{\prime}, b^{\prime}, c^{\prime}\right)$. Then

$$
\begin{aligned}
F(v+w) & =F\left(a+a^{\prime}, b+b^{\prime}, c+c^{\prime}\right)=\left(a+a^{\prime}, b+b^{\prime}, 0\right) \\
& =(a, b, 0)+\left(a^{\prime}, b^{\prime}, 0\right)=F(v)+F(w)
\end{aligned}
$$

and, for any scalar $k$,

$$
F(k v)=F(k a, k b, k c)=(k a, k b, 0)=k(a, b, 0)=k F(v)
$$

Thus, $F$ is linear.

(b) Let $G: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be the "translation" mapping defined by $G(x, y)=(x+1, y+2)$. [That is, $G$ adds the vector $(1,2)$ to any vector $v=(x, y)$ in $\mathbf{R}^{2}$.] Note that

$$
G(0)=G(0,0)=(1,2) \neq 0
$$

Thus, the zero vector is not mapped into the zero vector. Hence, $G$ is not linear.

EXAMPLE 5.5 (Derivative and Integral Mappings) Consider the vector space $V=\mathbf{P}(t)$ of polynomials over the real field $\mathbf{R}$. Let $u(t)$ and $v(t)$ be any polynomials in $V$ and let $k$ be any scalar.

(a) Let $\mathbf{D}: V \rightarrow V$ be the derivative mapping. One proves in calculus that

$$
\frac{d(u+v)}{d t}=\frac{d u}{d t}+\frac{d v}{d t} \quad \text { and } \quad \frac{d(k u)}{d t}=k \frac{d u}{d t}
$$

That is, $\mathbf{D}(u+v)=\mathbf{D}(u)+\mathbf{D}(v)$ and $\mathbf{D}(k u)=k \mathbf{D}(u)$. Thus, the derivative mapping is linear.

(b) Let $\mathbf{J}: V \rightarrow \mathbf{R}$ be an integral mapping, say

$$
\mathbf{J}(f(t))=\int_{0}^{1} f(t) d t
$$

One also proves in calculus that,

$$
\int_{0}^{1}[u(t)+v(t)] d t=\int_{0}^{1} u(t) d t+\int_{0}^{1} v(t) d t
$$

and

$$
\int_{0}^{1} k u(t) d t=k \int_{0}^{1} u(t) d t
$$

That is, $\mathbf{J}(u+v)=\mathbf{J}(u)+\mathbf{J}(v)$ and $\mathbf{J}(k u)=k \mathbf{J}(u)$. Thus, the integral mapping is linear.

EXAMPLE 5.6 (Zero and Identity Mappings)

(a) Let $F: V \rightarrow U$ be the mapping that assigns the zero vector $0 \in U$ to every vector $v \in V$. Then, for any vectors $v, w \in V$ and any scalar $k \in K$, we have

$$
F(v+w)=0=0+0=F(v)+F(w) \quad \text { and } \quad F(k v)=0=k 0=k F(v)
$$

Thus, $F$ is linear. We call $F$ the zero mapping, and we usually denote it by 0 .

(b) Consider the identity mapping $I: V \rightarrow V$, which maps each $v \in V$ into itself. Then, for any vectors $v, w \in V$ and any scalars $a, b \in K$, we have

$$
I(a v+b w)=a v+b w=a I(v)+b I(w)
$$

Thus, $I$ is linear.

Our next theorem (proved in Problem 5.13) gives us an abundance of examples of linear mappings. In particular, it tells us that a linear mapping is completely determined by its values on the elements of a basis.

THEOREM 5.2: $\quad$ Let $V$ and $U$ be vector spaces over a field $K$. Let $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be a basis of $V$ and let $u_{1}, u_{2}, \ldots, u_{n}$ be any vectors in $U$. Then there exists a unique linear mapping $F: V \rightarrow U$ such that $F\left(v_{1}\right)=u_{1}, F\left(v_{2}\right)=u_{2}, \ldots, F\left(v_{n}\right)=u_{n}$.

We emphasize that the vectors $u_{1}, u_{2}, \ldots, u_{n}$ in Theorem 5.2 are completely arbitrary; they may be linearly dependent or they may even be equal to each other.

\section*{Matrices as Linear Mappings}
Let $A$ be any real $m \times n$ matrix. Recall that $A$ determines a mapping $F_{A}: K^{n} \rightarrow K^{m}$ by $F_{A}(u)=A u$ (where the vectors in $K^{n}$ and $K^{m}$ are written as columns). We show $F_{A}$ is linear. By matrix multiplication,

$$
\begin{aligned}
F_{A}(v+w) & =A(v+w)=A v+A w=F_{A}(v)+F_{A}(w) \\
F_{A}(k v) & =A(k v)=k(A v)=k F_{A}(v)
\end{aligned}
$$

In other words, using $A$ to represent the mapping, we have

$$
A(v+w)=A v+A w \quad \text { and } \quad A(k v)=k(A v)
$$

Thus, the matrix mapping $A$ is linear.

\section*{Vector Space Isomorphism}
The notion of two vector spaces being isomorphic was defined in Chapter 4 when we investigated the coordinates of a vector relative to a basis. We now redefine this concept.

DEFINITION: Two vector spaces $V$ and $U$ over $K$ are isomorphic, written $V \cong U$, if there exists a bijective (one-to-one and onto) linear mapping $F: V \rightarrow U$. The mapping $F$ is then called an isomorphism between $V$ and $U$.

Consider any vector space $V$ of dimension $n$ and let $S$ be any basis of $V$. Then the mapping $v \mapsto[v]_{S}$

which maps each vector $v \in V$ into its coordinate vector $[v]_{S}$, is an isomorphism between $V$ and $K^{n}$.

\subsection*{5.4 Kernel and Image of a Linear Mapping}
We begin by defining two concepts.

DEFINITION: Let $F: V \rightarrow U$ be a linear mapping. The kernel of $F$, written $\operatorname{Ker} F$, is the set of elements in $V$ that map into the zero vector 0 in $U$; that is,

$$
\operatorname{Ker} F=\{v \in V: F(v)=0\}
$$

The image (or range) of $F$, written $\operatorname{Im} F$, is the set of image points in $U$; that is,

$$
\operatorname{Im} F=\{u \in U: \text { there exists } v \in V \text { for which } F(v)=u\}
$$

The following theorem is easily proved (Problem 5.22).

THEOREM 5.3: Let $F: V \rightarrow U$ be a linear mapping. Then the kernel of $F$ is a subspace of $V$ and the image of $F$ is a subspace of $U$.

Now suppose that $v_{1}, v_{2}, \ldots, v_{m}$ span a vector space $V$ and that $F: V \rightarrow U$ is linear. We show that $F\left(v_{1}\right), F\left(v_{2}\right), \ldots, F\left(v_{m}\right)$ span $\operatorname{Im} F$. Let $u \in \operatorname{Im} F$. Then there exists $v \in V$ such that $F(v)=u$. Because the $v_{i}$ 's span $V$ and $v \in V$, there exist scalars $a_{1}, a_{2}, \ldots, a_{m}$ for which

$v=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{m} v_{m}$

Therefore,

$$
u=F(v)=F\left(a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{m} v_{m}\right)=a_{1} F\left(v_{1}\right)+a_{2} F\left(v_{2}\right)+\cdots+a_{m} F\left(v_{m}\right)
$$

Thus, the vectors $F\left(v_{1}\right), F\left(v_{2}\right), \ldots, F\left(v_{m}\right)$ span $\operatorname{Im} F$.

We formally state the above result.

PROPOSITION 5.4: Suppose $v_{1}, v_{2}, \ldots, v_{m}$ span a vector space $V$, and suppose $F: V \rightarrow U$ is linear. Then $F\left(v_{1}\right), F\left(v_{2}\right), \ldots, F\left(v_{m}\right)$ span $\operatorname{Im} F$.

\section*{EXAMPLE 5.7}
(a) Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the projection of a vector $v$ into the $x y$-plane [as pictured in Fig. 5-2(a)]; that is,

$$
F(x, y, z)=(x, y, 0)
$$

Clearly the image of $F$ is the entire $x y$-plane-that is, points of the form $(x, y, 0)$. Moreover, the kernel of $F$ is the $z$-axis - that is, points of the form $(0,0, c)$. That is,

$$
\operatorname{Im} F=\{(a, b, c): c=0\}=x y \text {-plane } \quad \text { and } \quad \operatorname{Ker} F=\{(a, b, c): a=0, b=0\}=z \text {-axis }
$$

(b) Let $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the linear mapping that rotates a vector $v$ about the $z$-axis through an angle $\theta$ [as pictured in Fig. 5-2(b)]; that is,

$$
G(x, y, z)=(x \cos \theta-y \sin \theta, x \sin \theta+y \cos \theta, z)
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-177(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-177}
\end{center}

(b)

Figure 5-2

Observe that the distance of a vector $v$ from the origin $O$ does not change under the rotation, and so only the zero vector 0 is mapped into the zero vector 0 . Thus, $\operatorname{Ker} G=\{0\}$. On the other hand, every vector $u$ in $\mathbf{R}^{3}$ is the image of a vector $v$ in $\mathbf{R}^{3}$ that can be obtained by rotating $u$ back by an angle of $\theta$. Thus, $\operatorname{Im} G=\mathbf{R}^{3}$, the entire space.

EXAMPLE 5.8 Consider the vector space $V=\mathbf{P}(t)$ of polynomials over the real field $\mathbf{R}$, and let $H: V \rightarrow V$ be the third-derivative operator; that is, $H[f(t)]=d^{3} f / d t^{3}$. [Sometimes the notation $\mathbf{D}^{3}$ is used for $H$, where $\mathbf{D}$ is the derivative operator.] We claim that

$$
\text { Ker } H=\{\text { polynomials of degree } \leq 2\}=\mathbf{P}_{2}(t) \quad \text { and } \quad \operatorname{Im} H=V
$$

The first comes from the fact that $H\left(a t^{2}+b t+c\right)=0$ but $H\left(t^{n}\right) \neq 0$ for $n \geq 3$. The second comes from that fact that every polynomial $g(t)$ in $V$ is the third derivative of some polynomial $f(t)$ (which can be obtained by taking the antiderivative of $g(t)$ three times).

\section*{Kernel and Image of Matrix Mappings}
Consider, say, a $3 \times 4$ matrix $A$ and the usual basis $\left\{e_{1}, e_{2}, e_{3}, e_{4}\right\}$ of $K^{4}$ (written as columns):

$$
A=\left[\begin{array}{llll}
a_{1} & a_{2} & a_{3} & a_{4} \\
b_{1} & b_{2} & b_{3} & b_{4} \\
c_{1} & c_{2} & c_{3} & c_{4}
\end{array}\right], \quad e_{1}=\left[\begin{array}{l}
1 \\
0 \\
0 \\
0
\end{array}\right], \quad e_{2}=\left[\begin{array}{l}
1 \\
0 \\
0 \\
0
\end{array}\right], \quad e_{3}=\left[\begin{array}{l}
1 \\
0 \\
0 \\
0
\end{array}\right], \quad e_{4}=\left[\begin{array}{l}
1 \\
0 \\
0 \\
0
\end{array}\right]
$$

Recall that $A$ may be viewed as a linear mapping $A: K^{4} \rightarrow K^{3}$, where the vectors in $K^{4}$ and $K^{3}$ are viewed as column vectors. Now the usual basis vectors span $K^{4}$, so their images $A e_{1}, A e_{2}, A e_{3}, A e_{4}$ span the image of $A$. But the vectors $A e_{1}, A e_{2}, A e_{3}, A e_{4}$ are precisely the columns of $A$ :

$$
A e_{1}=\left[a_{1}, b_{1}, c_{1}\right]^{T}, \quad A e_{2}=\left[a_{2}, b_{2}, c_{2}\right]^{T}, \quad A e_{3}=\left[a_{3}, b_{3}, c_{3}\right]^{T}, \quad A e_{4}=\left[a_{4}, b_{4}, c_{4}\right]^{T}
$$

Thus, the image of $A$ is precisely the column space of $A$.

On the other hand, the kernel of $A$ consists of all vectors $v$ for which $A v=0$. This means that the kernel of $A$ is the solution space of the homogeneous system $A X=0$, called the null space of $A$.

We state the above results formally.

PROPOSITION 5.5: $\quad$ Let $A$ be any $m \times n$ matrix over a field $K$ viewed as a linear map $A: K^{n} \rightarrow K^{m}$. Then

$$
\operatorname{Ker} A=\operatorname{nullsp}(A) \quad \text { and } \quad \operatorname{Im} A=\operatorname{colsp}(A)
$$

Here $\operatorname{colsp}(A)$ denotes the column space of $A$, and nullsp $(A)$ denotes the null space of $A$.

\section*{Rank and Nullity of a Linear Mapping}
Let $F: V \rightarrow U$ be a linear mapping. The rank of $F$ is defined to be the dimension of its image, and the nullity of $F$ is defined to be the dimension of its kernel; namely,

$$
\operatorname{rank}(F)=\operatorname{dim}(\operatorname{Im} F) \quad \text { and } \quad \operatorname{nullity}(F)=\operatorname{dim}(\operatorname{Ker} F)
$$

The following important theorem (proved in Problem 5.23) holds.

THEOREM 5.6 Let $V$ be of finite dimension, and let $F: V \rightarrow U$ be linear. Then

$$
\operatorname{dim} V=\operatorname{dim}(\operatorname{Ker} F)+\operatorname{dim}(\operatorname{Im} F)=\operatorname{nullity}(F)+\operatorname{rank}(F)
$$

Recall that the rank of a matrix $A$ was also defined to be the dimension of its column space and row space. If we now view $A$ as a linear mapping, then both definitions correspond, because the image of $A$ is precisely its column space.

EXAMPLE 5.9 Let $F: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$ be the linear mapping defined by

$$
F(x, y, z, t)=(x-y+z+t, \quad 2 x-2 y+3 z+4 t, \quad 3 x-3 y+4 z+5 t)
$$

(a) Find a basis and the dimension of the image of $F$.

First find the image of the usual basis vectors of $\mathbf{R}^{4}$,

$$
\begin{array}{ll}
F(1,0,0,0)=(1,2,3), & F(0,0,1,0)=(1,3,4) \\
F(0,1,0,0)=(-1,-2,-3), & F(0,0,0,1)=(1,4,5)
\end{array}
$$

By Proposition 5.4, the image vectors span $\operatorname{Im} F$. Hence, form the matrix $M$ whose rows are these image vectors and row reduce to echelon form:

$$
M=\left[\begin{array}{rrr}
1 & 2 & 3 \\
-1 & -2 & -3 \\
1 & 3 & 4 \\
1 & 4 & 5
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 2 & 3 \\
0 & 0 & 0 \\
0 & 1 & 1 \\
0 & 2 & 2
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 2 & 3 \\
0 & 1 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $(1,2,3)$ and $(0,1,1)$ form a basis of $\operatorname{Im} F$. Hence, $\operatorname{dim}(\operatorname{Im} F)=2$ and $\operatorname{rank}(F)=2$.

(b) Find a basis and the dimension of the kernel of the map $F$.

Set $F(v)=0$, where $v=(x, y, z, t)$,

$$
F(x, y, z, t)=(x-y+z+t, \quad 2 x-2 y+3 z+4 t, \quad 3 x-3 y+4 z+5 t)=(0,0,0)
$$

Set corresponding components equal to each other to form the following homogeneous system whose solution space is $\operatorname{Ker} F$ :

$$
\begin{aligned}
& x-y+z+t=0 \quad x-y+z+t=0
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-178}
\end{center}

The free variables are $y$ and $t$. Hence, $\operatorname{dim}(\operatorname{Ker} F)=2$ or nullity $(F)=2$.

(i) Set $y=1, t=0$ to obtain the solution $(-1,1,0,0)$,

(ii) Set $y=0, t=1$ to obtain the solution $(1,0,-2,1)$.

Thus, $(-1,1,0,0)$ and $(1,0,-2,1)$ form a basis for $\operatorname{Ker} F$.

As expected from Theorem 5.6, $\operatorname{dim}(\operatorname{Im} F)+\operatorname{dim}(\operatorname{Ker} F)=4=\operatorname{dim} \mathbf{R}^{4}$.

\section*{Application to Systems of Linear Equations}
Let $A X=B$ denote the matrix form of a system of $m$ linear equations in $n$ unknowns. Now the matrix $A$ may be viewed as a linear mapping

$$
A: K^{n} \rightarrow K^{m}
$$

Thus, the solution of the equation $A X=B$ may be viewed as the preimage of the vector $B \in K^{m}$ under the linear mapping $A$. Furthermore, the solution of the associated homogeneous system

$$
A X=0
$$

may be viewed as the kernel of the linear mapping $A$. Applying Theorem 5.6 to this homogeneous system yields

$$
\operatorname{dim}(\operatorname{Ker} A)=\operatorname{dim} K^{n}-\operatorname{dim}(\operatorname{Im} A)=n-\operatorname{rank} A
$$

But $n$ is exactly the number of unknowns in the homogeneous system $A X=0$. Thus, we have proved the following theorem of Chapter 4.

THEOREM 4.19: $\quad$ The dimension of the solution space $W$ of a homogenous system $A X=0$ of linear equations is $s=n-r$, where $n$ is the number of unknowns and $r$ is the rank of the coefficient matrix $A$.

Observe that $r$ is also the number of pivot variables in an echelon form of $A X=0$, so $s=n-r$ is also the number of free variables. Furthermore, the $s$ solution vectors of $A X=0$ described in Theorem 3.14 are linearly independent (Problem 4.52). Accordingly, because $\operatorname{dim} W=s$, they form a basis for the solution space $W$. Thus, we have also proved Theorem 3.14.

\subsection*{5.5 Singular and Nonsingular Linear Mappings, Isomorphisms}
Let $F: V \rightarrow U$ be a linear mapping. Recall that $F(0)=0 . F$ is said to be singular if the image of some nonzero vector $v$ is 0 - that is, if there exists $v \neq 0$ such that $F(v)=0$. Thus, $F: V \rightarrow U$ is nonsingular if the zero vector 0 is the only vector whose image under $F$ is 0 or, in other words, if $\operatorname{Ker} F=\{0\}$.

EXAMPLE 5.10 Consider the projection map $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ and the rotation map $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ appearing in Fig. 5-2. (See Example 5.7.) Because the kernel of $F$ is the $z$-axis, $F$ is singular. On the other hand, the kernel of $G$ consists only of the zero vector 0 . Thus, $G$ is nonsingular.

Nonsingular linear mappings may also be characterized as those mappings that carry independent sets into independent sets. Specifically, we prove (Problem 5.28) the following theorem.

THEOREM 5.7: Let $F: V \rightarrow U$ be a nonsingular linear mapping. Then the image of any linearly independent set is linearly independent.

\section*{Isomorphisms}
Suppose a linear mapping $F: V \rightarrow U$ is one-to-one. Then only $0 \in V$ can map into $0 \in U$, and so $F$ is nonsingular. The converse is also true. For suppose $F$ is nonsingular and $F(v)=F(w)$, then $F(v-w)=F(v)-F(w)=0$, and hence, $v-w=0$ or $v=w$. Thus, $F(v)=F(w)$ implies $v=w-$ that is, $F$ is one-to-one. We have proved the following proposition.

PROPOSITION 5.8: A linear mapping $F: V \rightarrow U$ is one-to-one if and only if $F$ is nonsingular.

Recall that a mapping $F: V \rightarrow U$ is called an isomorphism if $F$ is linear and if $F$ is bijective (i.e., if $F$ is one-to-one and onto). Also, recall that a vector space $V$ is said to be isomorphic to a vector space $U$, written $V \cong U$, if there is an isomorphism $F: V \rightarrow U$.

The following theorem (proved in Problem 5.29) applies.

THEOREM 5.9: $\quad$ Suppose $V$ has finite dimension and $\operatorname{dim} V=\operatorname{dim} U$. Suppose $F: V \rightarrow U$ is linear. Then $F$ is an isomorphism if and only if $F$ is nonsingular.

\subsection*{5.6 Operations with Linear Mappings}
We are able to combine linear mappings in various ways to obtain new linear mappings. These operations are very important and will be used throughout the text.

Let $F: V \rightarrow U$ and $G: V \rightarrow U$ be linear mappings over a field $K$. The sum $F+G$ and the scalar product $k F$, where $k \in K$, are defined to be the following mappings from $V$ into $U$ :

$$
(F+G)(v) \equiv F(v)+G(v) \quad \text { and } \quad(k F)(v) \equiv k F(v)
$$

We now show that if $F$ and $G$ are linear, then $F+G$ and $k F$ are also linear. Specifically, for any vectors $v, w \in V$ and any scalars $a, b \in K$,

$$
\begin{aligned}
(F+G)(a v+b w) & =F(a v+b w)+G(a v+b w) \\
& =a F(v)+b F(w)+a G(v)+b G(w) \\
& =a[F(v)+G(v)]+b[F(w)+G(w)] \\
& =a(F+G)(v)+b(F+G)(w)
\end{aligned}
$$

and

$$
\begin{aligned}
(k F)(a v+b w) & =k F(a v+b w)=k[a F(v)+b F(w)] \\
& =a k F(v)+b k F(w)=a(k F)(v)+b(k F)(w)
\end{aligned}
$$

Thus, $F+G$ and $k F$ are linear.

The following theorem holds.

THEOREM 5.10: Let $V$ and $U$ be vector spaces over a field $K$. Then the collection of all linear mappings from $V$ into $U$ with the above operations of addition and scalar multiplication forms a vector space over $K$.

The vector space of linear mappings in Theorem 5.10 is usually denoted by

$$
\operatorname{Hom}(V, U)
$$

Here Hom comes from the word "homomorphism." We emphasize that the proof of Theorem 5.10 reduces to showing that $\operatorname{Hom}(V, U)$ does satisfy the eight axioms of a vector space. The zero element of $\operatorname{Hom}(V, U)$ is the zero mapping from $V$ into $U$, denoted by $\mathbf{0}$ and defined by

$$
\mathbf{0}(v)=0
$$

for every vector $v \in V$.

Suppose $V$ and $U$ are of finite dimension. Then we have the following theorem.

THEOREM 5.11: $\quad$ Suppose $\operatorname{dim} V=m$ and $\operatorname{dim} U=n$. Then $\operatorname{dim}[\operatorname{Hom}(V, U)]=m n$.

\section*{Composition of Linear Mappings}
Now suppose $V, U$, and $W$ are vector spaces over the same field $K$, and suppose $F: V \rightarrow U$ and $G: U \rightarrow W$ are linear mappings. We picture these mappings as follows:

$$
V \xrightarrow{F} U \xrightarrow{G} W
$$

Recall that the composition function $G \circ F$ is the mapping from $V$ into $W$ defined by $(G \circ F)(v)=G(F(v))$. We show that $G \circ F$ is linear whenever $F$ and $G$ are linear. Specifically, for any vectors $v, w \in V$ and any scalars $a, b \in K$, we have

$$
\begin{aligned}
(G \circ F)(a v+b w) & =G(F(a v+b w))=G(a F(v)+b F(w)) \\
& =a G(F(v))+b G(F(w))=a(G \circ F)(v)+b(G \circ F)(w)
\end{aligned}
$$

Thus, $G \circ F$ is linear.

The composition of linear mappings and the operations of addition and scalar multiplication are related as follows.

THEOREM 5.12: Let $V, U, W$ be vector spaces over $K$. Suppose the following mappings are linear:

$$
F: V \rightarrow U, \quad F^{\prime}: V \rightarrow U \quad \text { and } \quad G: U \rightarrow W, \quad G^{\prime}: U \rightarrow W
$$

Then, for any scalar $k \in K$ :

(i) $G \circ\left(F+F^{\prime}\right)=G \circ F+G \circ F^{\prime}$.

(ii) $\left(G+G^{\prime}\right) \circ F=G \circ F+G^{\prime} \circ F$.

(iii) $k(G \circ F)=(k G) \circ F=G \circ(k F)$.

\subsection*{5.7 Algebra $A(V)$ of Linear Operators}
Let $V$ be a vector space over a field $K$. This section considers the special case of linear mappings from the vector space $V$ into itself-that is, linear mappings of the form $F: V \rightarrow V$. They are also called linear operators or linear transformations on $V$. We will write $A(V)$, instead of $\operatorname{Hom}(V, V)$, for the space of all such mappings.

Now $A(V)$ is a vector space over $K$ (Theorem 5.8), and, if $\operatorname{dim} V=n$, then $\operatorname{dim} A(V)=n^{2}$. Moreover, for any mappings $F, G \in A(V)$, the composition $G \circ F$ exists and also belongs to $A(V)$. Thus, we have a "multiplication" defined in $A(V)$. [We sometimes write $F G$ instead of $G \circ F$ in the space $A(V)$.]

Remark: An algebra $A$ over a field $K$ is a vector space over $K$ in which an operation of multiplication is defined satisfying, for every $F, G, H \in A$ and every $k \in K$ :

(i) $F(G+H)=F G+F H$,

(ii) $(G+H) F=G F+H F$,

(iii) $k(G F)=(k G) F=G(k F)$.

The algebra is said to be associative if, in addition, $(F G) H=F(G H)$.

The above definition of an algebra and previous theorems give us the following result.

THEOREM 5.13: $\quad$ Let $V$ be a vector space over $K$. Then $A(V)$ is an associative algebra over $K$ with respect to composition of mappings. If $\operatorname{dim} V=n$, then $\operatorname{dim} A(V)=n^{2}$.

This is why $A(V)$ is called the algebra of linear operators on $V$.

\section*{Polynomials and Linear Operators}
Observe that the identity mapping $I: V \rightarrow V$ belongs to $A(V)$. Also, for any linear operator $F$ in $A(V)$, we have $F I=I F=F$. We can also form "powers" of $F$. Namely, we define

$$
F^{0}=I, \quad F^{2}=F \circ F, \quad F^{3}=F^{2} \circ F=F \circ F \circ F, \quad F^{4}=F^{3} \circ F, \quad \ldots
$$

Furthermore, for any polynomial $p(t)$ over $K$, say,

$$
p(t)=a_{0}+a_{1} t+a_{2} t^{2}+\cdots+a_{s} t^{2}
$$

we can form the linear operator $p(F)$ defined by

$$
p(F)=a_{0} I+a_{1} F+a_{2} F^{2}+\cdots+a_{s} F^{s}
$$

(For any scalar $k$, the operator $k I$ is sometimes denoted simply by $k$.) In particular, we say $F$ is a zero of the polynomial $p(t)$ if $p(F)=0$.

EXAMPLE 5.11 Let $F: K^{3} \rightarrow K^{3}$ be defined by $F(x, y, z)=(0, x, y)$. For any $(a, b, c) \in K^{3}$,

$$
\begin{aligned}
(F+I)(a, b, c) & =(0, a, b)+(a, b, c)=(a, a+b, b+c) \\
F^{3}(a, b, c) & =F^{2}(0, a, b)=F(0,0, a)=(0,0,0)
\end{aligned}
$$

Thus, $F^{3}=0$, the zero mapping in $A(V)$. This means $F$ is a zero of the polynomial $p(t)=t^{3}$.

\section*{Square Matrices as Linear Operators}
Let $\mathbf{M}=\mathbf{M}_{n, n}$ be the vector space of all square $n \times n$ matrices over $K$. Then any matrix $A$ in $M$ defines a linear mapping $F_{A}: K^{n} \rightarrow K^{n}$ by $F_{A}(u)=A u$ (where the vectors in $K^{n}$ are written as columns). Because the mapping is from $K^{n}$ into itself, the square matrix $A$ is a linear operator, not simply a linear mapping.

Suppose $A$ and $B$ are matrices in $M$. Then the matrix product $A B$ is defined. Furthermore, for any (column) vector $u$ in $K^{n}$,

$$
F_{A B}(u)=(A B) u=A(B u)=A\left(F_{B}(U)\right)=F_{A}\left(F_{B}(u)\right)=\left(F_{A} \circ F_{B}\right)(u)
$$

In other words, the matrix product $A B$ corresponds to the composition of $A$ and $B$ as linear mappings. Similarly, the matrix sum $A+B$ corresponds to the sum of $A$ and $B$ as linear mappings, and the scalar product $k A$ corresponds to the scalar product of $A$ as a linear mapping.

\section*{Invertible Operators in $A(V)$}
Let $F: V \rightarrow V$ be a linear operator. $F$ is said to be invertible if it has an inverse-that is, if there exists $F^{-1}$ in $A(V)$ such that $F F^{-1}=F^{-1} F=I$. On the other hand, $F$ is invertible as a mapping if $F$ is both one-to-one and onto. In such a case, $F^{-1}$ is also linear and $F^{-1}$ is the inverse of $F$ as a linear operator (proved in Problem 5.15).

Suppose $F$ is invertible. Then only $0 \in V$ can map into itself, and so $F$ is nonsingular. The converse is not true, as seen by the following example.

EXAMPLE 5.12 Let $V=\mathbf{P}(t)$, the vector space of polynomials over $K$. Let $F$ be the mapping on $V$ that increases by 1 the exponent of $t$ in each term of a polynomial; that is,

$$
F\left(a_{0}+a_{1} t+a_{2} t^{2}+\cdots+a_{s} t^{s}\right)=a_{0} t+a_{1} t^{2}+a_{2} t^{3}+\cdots+a_{s} t^{s+1}
$$

Then $F$ is a linear mapping and $F$ is nonsingular. However, $F$ is not onto, and so $F$ is not invertible.

The vector space $V=\mathbf{P}(t)$ in the above example has infinite dimension. The situation changes significantly when $V$ has finite dimension. Namely, the following theorem applies.

THEOREM 5.14: $\quad$ Let $F$ be a linear operator on a finite-dimensional vector space $V$. Then the following four conditions are equivalent.\\
(i) $F$ is nonsingular: $\operatorname{Ker} F=\{0\}$.\\
(iii) $F$ is an onto mapping.\\
(ii) $F$ is one-to-one.\\
(iv) $F$ is invertible.

The proof of the above theorem mainly follows from Theorem 5.6, which tells us that

$$
\operatorname{dim} V=\operatorname{dim}(\operatorname{Ker} F)+\operatorname{dim}(\operatorname{Im} F)
$$

By Proposition 5.8, (i) and (ii) are equivalent. Note that (iv) is equivalent to (ii) and (iii). Thus, to prove the theorem, we need only show that (i) and (iii) are equivalent. This we do below.

(a) Suppose (i) holds. Then $\operatorname{dim}(\operatorname{Ker} F)=0$, and so the above equation tells us that $\operatorname{dim} V=\operatorname{dim}(\operatorname{Im} F)$. This means $V=\operatorname{Im} F$ or, in other words, $F$ is an onto mapping. Thus, (i) implies (iii).

(b) Suppose (iii) holds. Then $V=\operatorname{Im} F$, and so $\operatorname{dim} V=\operatorname{dim}(\operatorname{Im} F)$. Therefore, the above equation tells us that $\operatorname{dim}(\operatorname{Ker} F)=0$, and so $F$ is nonsingular. Therefore, (iii) implies (i).

Accordingly, all four conditions are equivalent.

Remark: Suppose $A$ is a square $n \times n$ matrix over $K$. Then $A$ may be viewed as a linear operator on $K^{n}$. Because $K^{n}$ has finite dimension, Theorem 5.14 holds for the square matrix $A$. This is why the terms "nonsingular" and "invertible" are used interchangeably when applied to square matrices.

EXAMPLE 5.13 Let $F$ be the linear operator on $\mathbf{R}^{2}$ defined by $F(x, y)=(2 x+y, 3 x+2 y)$.

(a) To show that $F$ is invertible, we need only show that $F$ is nonsingular. Set $F(x, y)=(0,0)$ to obtain the homogeneous system

$$
2 x+y=0 \quad \text { and } \quad 3 x+2 y=0
$$

Solve for $x$ and $y$ to get $x=0, y=0$. Hence, $F$ is nonsingular and so invertible.

(b) To find a formula for $F^{-1}$, we set $F(x, y)=(s, t)$ and so $F^{-1}(s, t)=(x, y)$. We have

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-183}
\end{center}

Solve for $x$ and $y$ in terms of $s$ and $t$ to obtain $x=2 s-t, y=-3 s+2 t$. Thus,

$$
F^{-1}(s, t)=(2 s-t,-3 s+2 t) \quad \text { or } \quad F^{-1}(x, y)=(2 x-y,-3 x+2 y)
$$

where we rewrite the formula for $F^{-1}$ using $x$ and $y$ instead of $s$ and $t$.

\section*{SOLVED PROBLEMS}
\section*{Mappings}
5.1. State whether each diagram in Fig. 5-3 defines a mapping from $A=\{a, b, c\}$ into $B=\{x, y, z\}$.

(a) No. There is nothing assigned to the element $b \in A$.

(b) No. Two elements, $x$ and $z$, are assigned to $c \in A$.

(c) Yes.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(4)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(1)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(3)}
\end{center}

(c)

Figure 5-3

5.2. Let $f: A \rightarrow B$ and $g: B \rightarrow C$ be defined by Fig. 5-4.

(a) Find the composition mapping $(g \circ f): A \rightarrow C$.

(b) Find the images of the mappings $f, g, g \circ f$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(2)}
\end{center}

Figure 5-4

(a) Use the definition of the composition mapping to compute

$$
\begin{gathered}
(g \circ f)(a)=g(f(a))=g(y)=t, \quad(g \circ f)(b)=g(f(b))=g(x)=s \\
(g \circ f)(c)=g(f(c))=g(y)=t
\end{gathered}
$$

Observe that we arrive at the same answer if we "follow the arrows" in Fig. 5-4:

$$
a \rightarrow y \rightarrow t, \quad b \rightarrow x \rightarrow s, \quad c \rightarrow y \rightarrow t
$$

(b) By Fig. 5-4, the image values under the mapping $f$ are $x$ and $y$, and the image values under $g$ are $r, s, t$.

Hence,

$$
\operatorname{Im} f=\{x, y\} \quad \text { and } \quad \operatorname{Im} g=\{r, s, t\}
$$

Also, by part (a), the image values under the composition mapping $g \circ f$ are $t$ and $s$; accordingly, $\operatorname{Im} g \circ f=\{s, t\}$. Note that the images of $g$ and $g \circ f$ are different.

5.3. Consider the mapping $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=\left(y z, x^{2}\right)$. Find\\
(a) $F(2,3,4)$;\\
(b) $F(5,-2,7)$;\\
(c) $F^{-1}(0,0)$, that is, all $v \in \mathbf{R}^{3}$ such that $F(v)=0$.

(a) Substitute in the formula for $F$ to get $F(2,3,4)=\left(3 \cdot 4,2^{2}\right)=(12,4)$.

(b) $F(5,-2,7)=\left(-2 \cdot 7,5^{2}\right)=(-14,25)$.

(c) Set $F(v)=0$, where $v=(x, y, z)$, and then solve for $x, y, z$ :

$$
F(x, y, z)=\left(y z, x^{2}\right)=(0,0) \quad \text { or } \quad y z=0, x^{2}=0
$$

Thus, $x=0$ and either $y=0$ or $z=0$. In other words, $x=0, y=0$ or $x=0, z=0$ - that is, the $z$-axis and the $y$-axis.

5.4. Consider the mapping $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(3 y, 2 x)$. Let $S$ be the unit circle in $\mathbf{R}^{2}$, that is, the solution set of $x^{2}+y^{2}=1$. (a) Describe $F(S)$. (b) Find $F^{-1}(S)$.

(a) Let $(a, b)$ be an element of $F(S)$. Then there exists $(x, y) \in S$ such that $F(x, y)=(a, b)$. Hence,

$$
(3 y, 2 x)=(a, b) \quad \text { or } \quad 3 y=a, 2 x=b \quad \text { or } \quad y=\frac{a}{3}, x=\frac{b}{2}
$$

Because $(x, y) \in S$-that is, $x^{2}+y^{2}=1$ - we have

$$
\left(\frac{b}{2}\right)^{2}+\left(\frac{a}{3}\right)^{2}=1 \quad \text { or } \quad \frac{a^{2}}{9}+\frac{b^{2}}{4}=1
$$

Thus, $F(S)$ is an ellipse.

(b) Let $F(x, y)=(a, b)$, where $(a, b) \in S$. Then $(3 y, 2 x)=(a, b)$ or $3 y=a, 2 x=b$. Because $(a, b) \in S$, we have $a^{2}+b^{2}=1$. Thus, $(3 y)^{2}+(2 x)^{2}=1$. Accordingly, $F^{-1}(S)$ is the ellipse $4 x^{2}+9 y^{2}=1$.

5.5. Let the mappings $f: A \rightarrow B, g: B \rightarrow C, h: C \rightarrow D$ be defined by Fig. 5-5. Determine whether or not each function is (a) one-to-one; (b) onto; (c) invertible (i.e., has an inverse).

(a) The mapping $f: A \rightarrow B$ is one-to-one, as each element of $A$ has a different image. The mapping $g: B \rightarrow C$ is not one-to one, because $x$ and $z$ both have the same image 4. The mapping $h: C \rightarrow D$ is one-to-one.

(b) The mapping $f: A \rightarrow B$ is not onto, because $z \in B$ is not the image of any element of $A$. The mapping $g: B \rightarrow C$ is onto, as each element of $C$ is the image of some element of $B$. The mapping $h: C \rightarrow D$ is also onto.

(c) A mapping has an inverse if and only if it is one-to-one and onto. Hence, only $h$ has an inverse.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-184}
\end{center}

Figure 5-5

5.6. Suppose $f: A \rightarrow B$ and $g: B \rightarrow C$. Hence, $(g \circ f): A \rightarrow C$ exists. Prove

(a) If $f$ and $g$ are one-to-one, then $g \circ f$ is one-to-one.

(b) If $f$ and $g$ are onto mappings, then $g \circ f$ is an onto mapping.

(c) If $g \circ f$ is one-to-one, then $f$ is one-to-one.

(d) If $g \circ f$ is an onto mapping, then $g$ is an onto mapping.

(a) Suppose $(g \circ f)(x)=(g \circ f)(y)$. Then $g(f(x))=g(f(y))$. Because $g$ is one-to-one, $f(x)=f(y)$. Because $f$ is one-to-one, $x=y$. We have proven that $(g \circ f)(x)=(g \circ f)(y)$ implies $x=y$; hence $g \circ f$ is one-to-one.

(b) Suppose $c \in C$. Because $g$ is onto, there exists $b \in B$ for which $g(b)=c$. Because $f$ is onto, there exists $a \in A$ for which $f(a)=b$. Thus, $(g \circ f)(a)=g(f(a))=g(b)=c$. Hence, $g \circ f$ is onto.

(c) Suppose $f$ is not one-to-one. Then there exist distinct elements $x, y \in A$ for which $f(x)=f(y)$. Thus, $(g \circ f)(x)=g(f(x))=g(f(y))=(g \circ f)(y)$. Hence, $g \circ f$ is not one-to-one. Therefore, if $g \circ f$ is one-toone, then $f$ must be one-to-one.

(d) If $a \in A$, then $(g \circ f)(a)=g(f(a)) \in g(B)$. Hence, $(g \circ f)(A) \subseteq g(B)$. Suppose $g$ is not onto. Then $g(B)$ is properly contained in $C$ and so $(g \circ f)(A)$ is properly contained in $C$; thus, $g \circ f$ is not onto. Accordingly, if $g \circ f$ is onto, then $g$ must be onto.

5.7. Prove that $f: A \rightarrow B$ has an inverse if and only if $f$ is one-to-one and onto.

Suppose $f$ has an inverse-that is, there exists a function $f^{-1}: B \rightarrow A$ for which $f^{-1} \circ f=\mathbf{1}_{A}$ and $f \circ f^{-1}=\mathbf{1}_{B}$. Because $\mathbf{1}_{A}$ is one-to-one, $f$ is one-to-one by Problem 5.6(c), and because $\mathbf{1}_{B}$ is onto, $f$ is onto by Problem 5.6(d); that is, $f$ is both one-to-one and onto.

Now suppose $f$ is both one-to-one and onto. Then each $b \in B$ is the image of a unique element in $A$, say $b^{*}$. Thus, if $f(a)=b$, then $a=b^{*}$; hence, $f\left(b^{*}\right)=b$. Now let $g$ denote the mapping from $B$ to $A$ defined by $b \mapsto b^{*}$. We have

(i) $(g \circ f)(a)=g(f(a))=g(b)=b^{*}=a$ for every $a \in A$; hence, $g \circ f=\mathbf{1}_{A}$.

(ii) $(f \circ g)(b)=f(g(b))=f\left(b^{*}\right)=b$ for every $b \in B$; hence, $f \circ g=\mathbf{1}_{B}$.

Accordingly, $f$ has an inverse. Its inverse is the mapping $g$.

5.8. Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be defined by $f(x)=2 x-3$. Now $f$ is one-to-one and onto; hence, $f$ has an inverse mapping $f^{-1}$. Find a formula for $f^{-1}$.

Let $y$ be the image of $x$ under the mapping $f$; that is, $y=f(x)=2 x-3$. Hence, $x$ will be the image of $y$ under the inverse mapping $f^{-1}$. Thus, solve for $x$ in terms of $y$ in the above equation to obtain $x=\frac{1}{2}(y+3)$. Then the formula defining the inverse function is $f^{-1}(y)=\frac{1}{2}(y+3)$, or, using $x$ instead of $y, f^{-1}(x)=\frac{1}{2}(x+3)$.

\section*{Linear Mappings}
5.9. Suppose the mapping $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ is defined by $F(x, y)=(x+y, x)$. Show that $F$ is linear.

We need to show that $F(v+w)=F(v)+F(w)$ and $F(k v)=k F(v)$, where $u$ and $v$ are any elements of $\mathbf{R}^{2}$ and $k$ is any scalar. Let $v=(a, b)$ and $w=\left(a^{\prime}, b^{\prime}\right)$. Then

$$
v+w=\left(a+a^{\prime}, b+b^{\prime}\right) \quad \text { and } \quad k v=(k a, k b)
$$

We have $F(v)=(a+b, a)$ and $F(w)=\left(a^{\prime}+b^{\prime}, a^{\prime}\right)$. Thus,

$$
\begin{aligned}
F(v+w) & =F\left(a+a^{\prime}, b+b^{\prime}\right)=\left(a+a^{\prime}+b+b^{\prime}, a+a^{\prime}\right) \\
& =(a+b, a)+\left(a^{\prime}+b^{\prime}, a^{\prime}\right)=F(v)+F(w)
\end{aligned}
$$

and

$$
F(k v)=F(k a, k b)=(k a+k b, k a)=k(a+b, a)=k F(v)
$$

Because $v, w, k$ were arbitrary, $F$ is linear.

5.10. Suppose $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ is defined by $F(x, y, z)=(x+y+z, 2 x-3 y+4 z)$. Show that $F$ is linear.

We argue via matrices. Writing vectors as columns, the mapping $F$ may be written in the form $F(v)=A v$, where $v=[x, y, z]^{T}$ and

$$
A=\left[\begin{array}{rrr}
1 & 1 & 1 \\
2 & -3 & 4
\end{array}\right]
$$

Then, using properties of matrices, we have

$$
F(v+w)=A(v+w)=A v+A w=F(v)+F(w)
$$

and

$$
F(k v)=A(k v)=k(A v)=k F(v)
$$

Thus, $F$ is linear.

5.11. Show that the following mappings are not linear:

(a) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(x y, x)$

(b) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{3}$ defined by $F(x, y)=(x+3,2 y, x+y)$

(c) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(|x|, y+z)$

(a) Let $v=(1,2)$ and $w=(3,4)$; then $v+w=(4,6)$. Also,

$$
F(v)=(1(2), 1)=(2,1) \quad \text { and } \quad F(w)=(3(4), 3)=(12,3)
$$

Hence,

$$
F(v+w)=(4(6), 4)=(24,6) \neq F(v)+F(w)
$$

(b) Because $F(0,0)=(3,0,0) \neq(0,0,0), F$ cannot be linear.

(c) Let $v=(1,2,3)$ and $k=-3$. Then $k v=(-3,-6,-9)$. We have

$$
F(v)=(1,5) \text { and } k F(v)=-3(1,5)=(-3,-15) .
$$

Thus,

$$
F(k v)=F(-3,-6,-9)=(3,-15) \neq k F(v)
$$

Accordingly, $F$ is not linear.

5.12. Let $V$ be the vector space of $n$-square real matrices. Let $M$ be an arbitrary but fixed matrix in $V$. Let $F: V \rightarrow V$ be defined by $F(A)=A M+M A$, where $A$ is any matrix in $V$. Show that $F$ is linear.

For any matrices $A$ and $B$ in $V$ and any scalar $k$, we have

$$
\begin{aligned}
F(A+B) & =(A+B) M+M(A+B)=A M+B M+M A+M B \\
& =(A M+M A)=(B M+M B)=F(A)+F(B)
\end{aligned}
$$

and

$$
F(k A)=(k A) M+M(k A)=k(A M)+k(M A)=k(A M+M A)=k F(A)
$$

Thus, $F$ is linear.

5.13. Prove Theorem 5.2: Let $V$ and $U$ be vector spaces over a field $K$. Let $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be a basis of $V$ and let $u_{1}, u_{2}, \ldots, u_{n}$ be any vectors in $U$. Then there exists a unique linear mapping $F: V \rightarrow U$ such that $F\left(v_{1}\right)=u_{1}, F\left(v_{2}\right)=u_{2}, \ldots, F\left(v_{n}\right)=u_{n}$.

There are three steps to the proof of the theorem: (1) Define the mapping $F: V \rightarrow U$ such that $F\left(v_{i}\right)=u_{i}, i=1, \ldots, n$. (2) Show that $F$ is linear. (3) Show that $F$ is unique.

Step 1. Let $v \in V$. Because $\left\{v_{1}, \ldots, v_{n}\right\}$ is a basis of $V$, there exist unique scalars $a_{1}, \ldots, a_{n} \in K$ for which $v=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}$. We define $F: V \rightarrow U$ by

$$
F(v)=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}
$$

(Because the $a_{i}$ are unique, the mapping $F$ is well defined.) Now, for $i=1, \ldots, n$,

$$
v_{i}=0 v_{1}+\cdots+1 v_{i}+\cdots+0 v_{n}
$$

Hence,

$$
F\left(v_{i}\right)=0 u_{1}+\cdots+1 u_{i}+\cdots+0 u_{n}=u_{i}
$$

Thus, the first step of the proof is complete.

Step 2. Suppose $v=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}$ and $w=b_{1} v_{1}+b_{2} v_{2}+\cdots+b_{n} v_{n}$. Then

$$
v+w=\left(a_{1}+b_{1}\right) v_{1}+\left(a_{2}+b_{2}\right) v_{2}+\cdots+\left(a_{n}+b_{n}\right) v_{n}
$$

and, for any $k \in K, k v=k a_{1} v_{1}+k a_{2} v_{2}+\cdots+k a_{n} v_{n}$. By definition of the mapping $F$,

$$
F(v)=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} v_{n} \quad \text { and } \quad F(w)=b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{n} u_{n}
$$

Hence,

$$
\begin{aligned}
F(v+w) & =\left(a_{1}+b_{1}\right) u_{1}+\left(a_{2}+b_{2}\right) u_{2}+\cdots+\left(a_{n}+b_{n}\right) u_{n} \\
& =\left(a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}\right)+\left(b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{n} u_{n}\right) \\
& =F(v)+F(w)
\end{aligned}
$$

and

$$
F(k v)=k\left(a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}\right)=k F(v)
$$

Thus, $F$ is linear.

Step 3. Suppose $G: V \rightarrow U$ is linear and $G\left(v_{1}\right)=u_{i}, i=1, \ldots, n$. Let

$$
v=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}
$$

Then

$$
\begin{aligned}
G(v) & =G\left(a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}\right)=a_{1} G\left(v_{1}\right)+a_{2} G\left(v_{2}\right)+\cdots+a_{n} G\left(v_{n}\right) \\
& =a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}=F(v)
\end{aligned}
$$

Because $G(v)=F(v)$ for every $v \in V, G=F$. Thus, $F$ is unique and the theorem is proved.

5.14. Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be the linear mapping for which $F(1,2)=(2,3)$ and $F(0,1)=(1,4)$. [Note that $\{(1,2),(0,1)\}$ is a basis of $\mathbf{R}^{2}$, so such a linear map $F$ exists and is unique by Theorem 5.2.] Find a formula for $F$; that is, find $F(a, b)$.

Write $(a, b)$ as a linear combination of $(1,2)$ and $(0,1)$ using unknowns $x$ and $y$,

$$
(a, b)=x(1,2)+y(0,1)=(x, 2 x+y), \quad \text { so } \quad a=x, b=2 x+y
$$

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=a, \quad y=-2 a+b$. Then

$$
F(a, b)=x F(1,2)+y F(0,1)=a(2,3)+(-2 a+b)(1,4)=(b,-5 a+4 b)
$$

5.15. Suppose a linear mapping $F: V \rightarrow U$ is one-to-one and onto. Show that the inverse mapping $F^{-1}: U \rightarrow V$ is also linear.

Suppose $u, u^{\prime} \in U$. Because $F$ is one-to-one and onto, there exist unique vectors $v, v^{\prime} \in V$ for which $F(v)=u$ and $F\left(v^{\prime}\right)=u^{\prime}$. Because $F$ is linear, we also have

$$
F\left(v+v^{\prime}\right)=F(v)+F\left(v^{\prime}\right)=u+u^{\prime} \quad \text { and } \quad F(k v)=k F(v)=k u
$$

By definition of the inverse mapping,

$$
F^{-1}(u)=v, F^{-1}\left(u^{\prime}\right)=v^{\prime}, F^{-1}\left(u+u^{\prime}\right)=v+v^{\prime}, F^{-1}(k u)=k v .
$$

Then

$$
F^{-1}\left(u+u^{\prime}\right)=v+v^{\prime}=F^{-1}(u)+F^{-1}\left(u^{\prime}\right) \quad \text { and } \quad F^{-1}(k u)=k v=k F^{-1}(u)
$$

Thus, $F^{-1}$ is linear.

\section*{Kernel and Image of Linear Mappings}
5.16. Let $F: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$ be the linear mapping defined by

$$
F(x, y, z, t)=(x-y+z+t, \quad x+2 z-t, \quad x+y+3 z-3 t)
$$

Find a basis and the dimension of (a) the image of $F$, (b) the kernel of $F$.

(a) Find the images of the usual basis of $\mathbf{R}^{4}$ :

$$
\begin{array}{ll}
F(1,0,0,0)=(1,1,1), & F(0,0,1,0)=(1,2,3) \\
F(0,1,0,0)=(-1,0,1), & F(0,0,0,1)=(1,-1,-3)
\end{array}
$$

By Proposition 5.4, the image vectors span $\operatorname{Im} F$. Hence, form the matrix whose rows are these image vectors, and row reduce to echelon form:

$$
\left[\begin{array}{rrr}
1 & 1 & 1 \\
-1 & 0 & 1 \\
1 & 2 & 3 \\
1 & -1 & -3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 1 \\
0 & 1 & 2 \\
0 & 1 & 2 \\
0 & -2 & -4
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 1 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $(1,1,1)$ and $(0,1,2)$ form a basis for $\operatorname{Im} F$; hence, $\operatorname{dim}(\operatorname{Im} F)=2$.

(b) Set $F(v)=0$, where $v=(x, y, z, t)$; that is, set

$$
F(x, y, z, t)=(x-y+z+t, x+2 z-t, x+y+3 z-3 t)=(0,0,0)
$$

Set corresponding entries equal to each other to form the following homogeneous system whose solution space is $\operatorname{Ker} F$ :

$$
\begin{aligned}
& x-y+z+t=0 \quad x-y+z+t=0 \\
& x+2 z-t=0 \quad \text { or } \quad y+z-2 t=0 \\
& x+y+3 z-3 t=0 \quad 2 y+2 z-4 t=0 \\
& x-y+z+t=0 \\
& y+z-2 t=0
\end{aligned}
$$

The free variables are $z$ and $t$. Hence, $\operatorname{dim}(\operatorname{Ker} F)=2$.

(i) Set $z=-1, t=0$ to obtain the solution $(2,1,-1,0)$.

(ii) Set $z=0, t=1$ to obtain the solution $(1,2,0,1)$.

Thus, $(2,1,-1,0)$ and $(1,2,0,1)$ form a basis of $\operatorname{Ker} F$.

[As expected, $\operatorname{dim}(\operatorname{Im} F)+\operatorname{dim}(\operatorname{Ker} F)=2+2=4=\operatorname{dim} \mathbf{R}^{4}$, the domain of $F$.]

5.17. Let $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the linear mapping defined by

$$
G(x, y, z)=(x+2 y-z, \quad y+z, \quad x+y-2 z)
$$

Find a basis and the dimension of (a) the image of $G$, (b) the kernel of $G$.

(a) Find the images of the usual basis of $\mathbf{R}^{3}$ :

$$
G(1,0,0)=(1,0,1), \quad G(0,1,0)=(2,1,1), \quad G(0,0,1)=(-1,1,-2)
$$

By Proposition 5.4, the image vectors span $\operatorname{Im} G$. Hence, form the matrix $M$ whose rows are these image vectors, and row reduce to echelon form:

$$
M=\left[\begin{array}{rrr}
1 & 0 & 1 \\
2 & 1 & 1 \\
-1 & 1 & -2
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 0 & 1 \\
0 & 1 & -1 \\
0 & 1 & -1
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 0 & 1 \\
0 & 1 & -1 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $(1,0,1)$ and $(0,1,-1)$ form a basis for $\operatorname{Im} G$; hence, $\operatorname{dim}(\operatorname{Im} G)=2$.

(b) Set $G(v)=0$, where $v=(x, y, z)$; that is,

$$
G(x, y, z)=(x+2 y-z, \quad y+z, \quad x+y-2 z)=(0,0,0)
$$

Set corresponding entries equal to each other to form the following homogeneous system whose solution space is $\operatorname{Ker} G$ :

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-189}
\end{center}

The only free variable is $z$; hence, $\operatorname{dim}(\operatorname{Ker} G)=1$. Set $z=1$; then $y=-1$ and $x=3$. Thus, $(3,-1,1)$ forms a basis of $\operatorname{Ker} G$. [As expected, $\operatorname{dim}(\operatorname{Im} G)+\operatorname{dim}(\operatorname{Ker} G)=2+1=3=\operatorname{dim} \mathbf{R}^{3}$, the domain of $G$.]

5.18. Consider the matrix mapping $A: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$, where $A=\left[\begin{array}{rrrr}1 & 2 & 3 & 1 \\ 1 & 3 & 5 & -2 \\ 3 & 8 & 13 & -3\end{array}\right]$. Find a basis and the dimension of (a) the image of $A$, (b) the kernel of $A$.

(a) The column space of $A$ is equal to $\operatorname{Im} A$. Now reduce $A^{T}$ to echelon form:

$$
A^{T}=\left[\begin{array}{rrr}
1 & 1 & 3 \\
2 & 3 & 8 \\
3 & 5 & 13 \\
1 & -2 & -3
\end{array}\right] \sim\left[\begin{array}{rrr}
1 & 1 & 3 \\
0 & 1 & 2 \\
0 & 2 & 4 \\
0 & -3 & -6
\end{array}\right] \sim\left[\begin{array}{lll}
1 & 1 & 3 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $\{(1,1,3),(0,1,2)\}$ is a basis of $\operatorname{Im} A$, and $\operatorname{dim}(\operatorname{Im} A)=2$.

(b) Here $\operatorname{Ker} A$ is the solution space of the homogeneous system $A X=0$, where $X=\{x, y, z, t)^{T}$. Thus, reduce the matrix $A$ of coefficients to echelon form:

$$
\left[\begin{array}{rrrr}
1 & 2 & 3 & 1 \\
0 & 1 & 2 & -3 \\
0 & 2 & 4 & -6
\end{array}\right] \sim\left[\begin{array}{rrrr}
1 & 2 & 3 & 1 \\
0 & 1 & 2 & -3 \\
0 & 0 & 0 & 0
\end{array}\right] \quad \text { or } \quad \begin{array}{r}
x+2 y+3 z+t=0 \\
y+2 z-3 t=0
\end{array}
$$

The free variables are $z$ and $t$. Thus, $\operatorname{dim}(\operatorname{Ker} A)=2$.

(i) Set $z=1, t=0$ to get the solution $(1,-2,1,0)$.

(ii) Set $z=0, t=1$ to get the solution $(-7,3,0,1)$.

Thus, $(1,-2,1,0)$ and $(-7,3,0,1)$ form a basis for $\operatorname{Ker} A$.

5.19. Find a linear map $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{4}$ whose image is spanned by $(1,2,0,-4)$ and $(2,0,-1,-3)$.

Form a $4 \times 3$ matrix whose columns consist only of the given vectors, say

$$
A=\left[\begin{array}{rrr}
1 & 2 & 2 \\
2 & 0 & 0 \\
0 & -1 & -1 \\
-4 & -3 & -3
\end{array}\right]
$$

Recall that $A$ determines a linear map $A: \mathbf{R}^{3} \rightarrow \mathbf{R}^{4}$ whose image is spanned by the columns of $A$. Thus, $A$ satisfies the required condition.

5.20. Suppose $f: V \rightarrow U$ is linear with kernel $W$, and that $f(v)=u$. Show that the "coset" $v+W=\{v+w: w \in W\}$ is the preimage of $u$; that is, $f^{-1}(u)=v+W$.

We must prove that (i) $f^{-1}(u) \subseteq v+W$ and (ii) $v+W \subseteq f^{-1}(u)$.

We first prove (i). Suppose $v^{\prime} \in f^{-1}(u)$. Then $f\left(v^{\prime}\right)=u$, and so

$$
f\left(v^{\prime}-v\right)=f\left(v^{\prime}\right)-f(v)=u-u=0
$$

that is, $v^{\prime}-v \in W$. Thus, $v^{\prime}=v+\left(v^{\prime}-v\right) \in v+W$, and hence $f^{-1}(u) \subseteq v+W$.

Now we prove (ii). Suppose $v^{\prime} \in v+W$. Then $v^{\prime}=v+w$, where $w \in W$. Because $W$ is the kernel of $f$, we have $f(w)=0$. Accordingly,

$$
f\left(v^{\prime}\right)=f(v+w)+f(v)+f(w)=f(v)+0=f(v)=u
$$

Thus, $v^{\prime} \in f^{-1}(u)$, and so $v+W \subseteq f^{-1}(u)$.

Both inclusions imply $f^{-1}(u)=v+W$.

5.21. Suppose $F: V \rightarrow U$ and $G: U \rightarrow W$ are linear. Prove\\
(a) $\operatorname{rank}(G \circ F) \leq \operatorname{rank}(G)$,\\
(b) $\operatorname{rank}(G \circ F) \leq \operatorname{rank}(F)$.

(a) Because $F(V) \subseteq U$, we also have $G(F(V)) \subseteq G(U)$, and so $\operatorname{dim}[G(F(V))] \leq \operatorname{dim}[G(U)]$. Then $\operatorname{rank}(G \circ F)=\operatorname{dim}[(G \circ F)(V)]=\operatorname{dim}[G(F(V))] \leq \operatorname{dim}[G(U)]=\operatorname{rank}(G)$.

(b) We have $\operatorname{dim}[G(F(V))] \leq \operatorname{dim}[F(V)]$. Hence,

$$
\operatorname{rank}(G \circ F)=\operatorname{dim}[(G \circ F)(V)]=\operatorname{dim}[G(F(V))] \leq \operatorname{dim}[F(V)]=\operatorname{rank}(F)
$$

5.22. Prove Theorem 5.3: Let $F: V \rightarrow U$ be linear. Then,

(a) $\operatorname{Im} F$ is a subspace of $U$, (b) $\operatorname{Ker} F$ is a subspace of $V$.

(a) Because $F(0)=0$, we have $0 \in \operatorname{Im} F$. Now suppose $u, u^{\prime} \in \operatorname{Im} F$ and $a, b \in K$. Because $u$ and $u^{\prime}$ belong to the image of $F$, there exist vectors $v, v^{\prime} \in V$ such that $F(v)=u$ and $F\left(v^{\prime}\right)=u^{\prime}$. Then

$$
F\left(a v+b v^{\prime}\right)=a F(v)+b F\left(v^{\prime}\right)=a u+b u^{\prime} \in \operatorname{Im} F
$$

Thus, the image of $F$ is a subspace of $U$.

(b) Because $F(0)=0$, we have $0 \in \operatorname{Ker} F$. Now suppose $v, w \in \operatorname{Ker} F$ and $a, b \in K$. Because $v$ and $w$ belong to the kernel of $F, F(v)=0$ and $F(w)=0$. Thus,

$$
F(a v+b w)=a F(v)+b F(w)=a 0+b 0=0+0=0, \quad \text { and so } \quad a v+b w \in \operatorname{Ker} F
$$

Thus, the kernel of $F$ is a subspace of $V$.

5.23. Prove Theorem 5.6: Suppose $V$ has finite dimension and $F: V \rightarrow U$ is linear. Then

$$
\operatorname{dim} V=\operatorname{dim}(\operatorname{Ker} F)+\operatorname{dim}(\operatorname{Im} F)=\operatorname{nullity}(F)+\operatorname{rank}(F)
$$

Suppose $\operatorname{dim}(\operatorname{Ker} F)=r$ and $\left\{w_{1}, \ldots, w_{r}\right\}$ is a basis of $\operatorname{Ker} F$, and suppose $\operatorname{dim}(\operatorname{Im} F)=s$ and $\left\{u_{1}, \ldots, u_{s}\right\}$ is a basis of $\operatorname{Im} F$. (By Proposition 5.4, $\operatorname{Im} F$ has finite dimension.) Because every $u_{j} \in \operatorname{Im} F$, there exist vectors $v_{1}, \ldots, v_{s}$ in $V$ such that $F\left(v_{1}\right)=u_{1}, \ldots, F\left(v_{s}\right)=u_{s}$. We claim that the set

$$
B=\left\{w_{1}, \ldots, w_{r}, v_{1}, \ldots, v_{s}\right\}
$$

is a basis of $V$; that is, (i) $B$ spans $V$, and (ii) $B$ is linearly independent. Once we prove (i) and (ii), then $\operatorname{dim} V=r+s=\operatorname{dim}(\operatorname{Ker} F)+\operatorname{dim}(\operatorname{Im} F)$.

(i) $B$ spans $V$. Let $v \in V$. Then $F(v) \in \operatorname{Im} F$. Because the $u_{j} \operatorname{span} \operatorname{Im} F$, there exist scalars $a_{1}, \ldots, a_{s}$ such that $F(v)=a_{1} u_{1}+\cdots+a_{s} u_{s}$. Set $\hat{v}=a_{1} v_{1}+\cdots+a_{s} v_{s}-v$. Then

$$
\begin{aligned}
F(\hat{v}) & =F\left(a_{1} v_{1}+\cdots+a_{s} v_{s}-v\right)=a_{1} F\left(v_{1}\right)+\cdots+a_{s} F\left(v_{s}\right)-F(v) \\
& =a_{1} u_{1}+\cdots+a_{s} u_{s}-F(v)=0
\end{aligned}
$$

Thus, $\hat{v} \in \operatorname{Ker} F$. Because the $w_{i}$ span $\operatorname{Ker} F$, there exist scalars $b_{1}, \ldots, b_{r}$, such that

$$
\hat{v}=b_{1} w_{1}+\cdots+b_{r} w_{r}=a_{1} v_{1}+\cdots+a_{s} v_{s}-v
$$

Accordingly,

$$
v=a_{1} v_{1}+\cdots+a_{s} v_{s}-b_{1} w_{1}-\cdots-b_{r} w_{r}
$$

Thus, $B$ spans $V$.\\
(ii) B is linearly independent. Suppose


\begin{equation*}
x_{1} w_{1}+\cdots+x_{r} w_{r}+y_{1} v_{1}+\cdots+y_{s} v_{s}=0 \tag{1}
\end{equation*}


where $x_{i}, y_{j} \in K$. Then


\begin{align*}
0 & =F(0)=F\left(x_{1} w_{1}+\cdots+x_{r} w_{r}+y_{1} v_{1}+\cdots+y_{s} v_{s}\right) \\
& =x_{1} F\left(w_{1}\right)+\cdots+x_{r} F\left(w_{r}\right)+y_{1} F\left(v_{1}\right)+\cdots+y_{s} F\left(v_{s}\right) \tag{2}
\end{align*}


But $F\left(w_{i}\right)=0$, since $w_{i} \in \operatorname{Ker} F$, and $F\left(v_{j}\right)=u_{j}$. Substituting into (2), we will obtain $y_{1} u_{1}+\cdots+y_{s} u_{s}=0$. Since the $u_{j}$ are linearly independent, each $y_{j}=0$. Substitution into (1) gives $x_{1} w_{1}+\cdots+x_{r} w_{r}=0$. Since the $w_{i}$ are linearly independent, each $x_{i}=0$. Thus $B$ is linearly independent.

\section*{Singular and Nonsingular Linear Maps, Isomorphisms}
5.24. Determine whether or not each of the following linear maps is nonsingular. If not, find a nonzero vector $v$ whose image is 0 .

(a) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(x-y, x-2 y)$.

(b) $G: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $G(x, y)=(2 x-4 y, 3 x-6 y)$.

(a) Find Ker $F$ by setting $F(v)=0$, where $v=(x, y)$,

$$
\begin{array}{rlrl}
(x-y, x-2 y)=(0,0) \quad \text { or } & \begin{aligned}
x-y & =0 \\
x-2 y & =0
\end{aligned} \quad \text { or } & x-y & =0 \\
-y & =0
\end{array}
$$

The only solution is $x=0, y=0$. Hence, $F$ is nonsingular.

(b) Set $G(x, y)=(0,0)$ to find $\operatorname{Ker} G$ :

$$
(2 x-4 y, 3 x-6 y)=(0,0) \quad \text { or } \quad \begin{aligned}
& 2 x-4 y=0 \\
& 3 x-6 y=0
\end{aligned} \quad \text { or } \quad x-2 y=0
$$

The system has nonzero solutions, because $y$ is a free variable. Hence, $G$ is singular. Let $y=1$ to obtain the solution $v=(2,1)$, which is a nonzero vector, such that $G(v)=0$.

5.25. The linear map $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(x-y, x-2 y)$ is nonsingular by the previous Problem 5.24. Find a formula for $F^{-1}$.

Set $F(x, y)=(a, b)$, so that $F^{-1}(a, b)=(x, y)$. We have

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-191}
\end{center}

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=2 a-b, y=a-b$. Thus,

$$
F^{-1}(a, b)=(2 a-b, a-b) \quad \text { or } \quad F^{-1}(x, y)=(2 x-y, x-y)
$$

(The second equation is obtained by replacing $a$ and $b$ by $x$ and $y$, respectively.)

5.26. Let $G: \mathbf{R}^{2} \rightarrow \mathbf{R}^{3}$ be defined by $G(x, y)=(x+y, x-2 y, 3 x+y)$.

(a) Show that $G$ is nonsingular. (b) Find a formula for $G^{-1}$.

(a) Set $G(x, y)=(0,0,0)$ to find $\operatorname{Ker} G$. We have

$$
(x+y, x-2 y, 3 x+y)=(0,0,0) \quad \text { or } \quad x+y=0, x-2 y=0,3 x+y=0
$$

The only solution is $x=0, y=0$; hence, $G$ is nonsingular.

(b) Although $G$ is nonsingular, it is not invertible, because $\mathbf{R}^{2}$ and $\mathbf{R}^{3}$ have different dimensions. (Thus, Theorem 5.9 does not apply.) Accordingly, $G^{-1}$ does not exist.

5.27. Suppose that $F: V \rightarrow U$ is linear and that $V$ is of finite dimension. Show that $V$ and the image of $F$ have the same dimension if and only if $F$ is nonsingular. Determine all nonsingular linear mappings $T: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$.

By Theorem 5.6, $\operatorname{dim} V=\operatorname{dim}(\operatorname{Im} F)+\operatorname{dim}(\operatorname{Ker} F)$. Hence, $V$ and $\operatorname{Im} F$ have the same dimension if and only if $\operatorname{dim}(\operatorname{Ker} F)=0$ or $\operatorname{Ker} F=\{0\}$ (i.e., if and only if $F$ is nonsingular).

Because $\operatorname{dim} \mathbf{R}^{3}$ is less than $\operatorname{dim} \mathbf{R}^{4}$, we have that $\operatorname{dim}(\operatorname{Im} T)$ is less than the dimension of the domain $\mathbf{R}^{4}$ of $T$. Accordingly no linear mapping $T: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$ can be nonsingular.

5.28. Prove Theorem 5.7: Let $F: V \rightarrow U$ be a nonsingular linear mapping. Then the image of any linearly independent set is linearly independent.

Suppose $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent vectors in $V$. We claim that $F\left(v_{1}\right), F\left(v_{2}\right), \ldots, F\left(v_{n}\right)$ are also linearly independent. Suppose $a_{1} F\left(v_{1}\right)+a_{2} F\left(v_{2}\right)+\cdots+a_{n} F\left(v_{n}\right)=0$, where $a_{i} \in K$. Because $F$ is linear, $F\left(a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}\right)=0$. Hence,

$$
a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n} \in \operatorname{Ker} F
$$

But $F$ is nonsingular-that is, $\operatorname{Ker} F=\{0\}$. Hence, $a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}=0$. Because the $v_{i}$ are linearly independent, all the $a_{i}$ are 0 . Accordingly, the $F\left(v_{i}\right)$ are linearly independent. Thus, the theorem is proved.

5.29. Prove Theorem 5.9: Suppose $V$ has finite dimension and $\operatorname{dim} V=\operatorname{dim} U$. Suppose $F: V \rightarrow U$ is linear. Then $F$ is an isomorphism if and only if $F$ is nonsingular.

If $F$ is an isomorphism, then only 0 maps to 0 ; hence, $F$ is nonsingular. Conversely, suppose $F$ is nonsingular. Then $\operatorname{dim}(\operatorname{Ker} F)=0$. By Theorem 5.6, $\operatorname{dim} V=\operatorname{dim}(\operatorname{Ker} F)+\operatorname{dim}(\operatorname{Im} F)$. Thus,

$$
\operatorname{dim} U=\operatorname{dim} V=\operatorname{dim}(\operatorname{Im} F)
$$

Because $U$ has finite dimension, $\operatorname{Im} F=U$. This means $F$ maps $V$ onto $U$. Thus, $F$ is one-to-one and onto; that is, $F$ is an isomorphism.

\section*{Operations with Linear Maps}
5.30. Define $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ and $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ by $F(x, y, z)=(2 x, y+z)$ and $G(x, y, z)=(x-z, y)$. Find formulas defining the maps: (a) $F+G$, (b) $3 F$, (c) $2 F-5 G$.

(a) $(F+G)(x, y, z)=F(x, y, z)+G(x, y, z)=(2 x, y+z)+(x-z, y)=(3 x-z, 2 y+z)$

(b) $(3 F)(x, y, z)=3 F(x, y, z)=3(2 x, y+z)=(6 x, 3 y+3 z)$

(c) $(2 F-5 G)(x, y, z)=2 F(x, y, z)-5 G(x, y, z)=2(2 x, y+z)-5(x-z, y)$

$$
=(4 x, 2 y+2 z)+(-5 x+5 z, \quad-5 y)=(-x+5 z,-3 y+2 z)
$$

5.31. Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ and $G: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y, z)=(2 x, y+z)$ and $G(x, y)=(y, x)$. Derive formulas defining the mappings: (a) $G \circ F$, (b) $F \circ G$.

(a) $(G \circ F)(x, y, z)=G(F(x, y, z))=G(2 x, y+z)=(y+z, 2 x)$

(b) The mapping $F \circ G$ is not defined, because the image of $G$ is not contained in the domain of $F$.

5.32. Prove: (a) The zero mapping $\mathbf{0}$, defined by $\mathbf{0}(v)=0 \in U$ for every $v \in V$, is the zero element of $\operatorname{Hom}(V, U)$. (b) The negative of $F \in \operatorname{Hom}(V, U)$ is the mapping $(-1) F$, that is, $-F=(-1) F$.

Let $F \in \operatorname{Hom}(V, U)$. Then, for every $v \in V$ :


\begin{equation*}
(F+\mathbf{0})(v)=F(v)+\mathbf{0}(v)=F(v)+0=F(v) \tag{a}
\end{equation*}


Because $(F+\mathbf{0})(v)=F(v)$ for every $v \in V$, we have $F+\mathbf{0}=F$. Similarly, $\mathbf{0}+F=F$.


\begin{equation*}
(F+(-1) F)(v)=F(v)+(-1) F(v)=F(v)-F(v)=0=\mathbf{0}(v) \tag{b}
\end{equation*}


Thus, $F+(-1) F=\mathbf{0}$. Similarly $(-1) F+F=\mathbf{0}$. Hence, $-F=(-1) F$.

5.33. Suppose $F_{1}, F_{2}, \ldots, F_{n}$ are linear maps from $V$ into $U$. Show that, for any scalars $a_{1}, a_{2}, \ldots, a_{n}$, and for any $v \in V$,

$$
\left(a_{1} F_{1}+a_{2} F_{2}+\cdots+a_{n} F_{n}\right)(v)=a_{1} F_{1}(v)+a_{2} F_{2}(v)+\cdots+a_{n} F_{n}(v)
$$

The mapping $a_{1} F_{1}$ is defined by $\left(a_{1} F_{1}\right)(v)=a_{1} F(v)$. Hence, the theorem holds for $n=1$. Accordingly, by induction,

$$
\begin{aligned}
\left(a_{1} F_{1}+a_{2} F_{2}+\cdots+a_{n} F_{n}\right)(v) & =\left(a_{1} F_{1}\right)(v)+\left(a_{2} F_{2}+\cdots+a_{n} F_{n}\right)(v) \\
& =a_{1} F_{1}(v)+a_{2} F_{2}(v)+\cdots+a_{n} F_{n}(v)
\end{aligned}
$$

5.34. Consider linear mappings $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}, G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}, H: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(x+y+z, x+y), \quad G(x, y, z)=(2 x+z, x+y), \quad H(x, y, z)=(2 y, x)$

Show that $F, G, H$ are linearly independent [as elements of $\left.\operatorname{Hom}\left(\mathbf{R}^{3}, \mathbf{R}^{2}\right)\right]$.

Suppose, for scalars $a, b, c \in K$,


\begin{equation*}
a F+b G+c H=\mathbf{0} \tag{1}
\end{equation*}


(Here $\mathbf{0}$ is the zero mapping.) For $e_{1}=(1,0,0) \in \mathbf{R}^{3}$, we have $\mathbf{0}\left(e_{1}\right)=(0,0)$ and

$$
\begin{aligned}
(a F+b G+c H)\left(e_{1}\right) & =a F(1,0,0)+b G(1,0,0)+c H(1,0,0) \\
& =a(1,1)+b(2,1)+c(0,1)=(a+2 b, \quad a+b+c)
\end{aligned}
$$

Thus by $(1),(a+2 b, \quad a+b+c)=(0,0)$ and so


\begin{equation*}
a+2 b=0 \quad \text { and } \quad a+b+c=0 \tag{2}
\end{equation*}


Similarly for $e_{2}=(0,1,0) \in \mathbf{R}^{3}$, we have $\mathbf{0}\left(e_{2}\right)=(0,0)$ and

$$
\begin{aligned}
(a F+b G+c H)\left(e_{2}\right) & =a F(0,1,0)+b G(0,1,0)+c H(0,1,0) \\
& =a(1,1)+b(0,1)+c(2,0)=(a+2 c, \quad a+b)
\end{aligned}
$$

Thus,


\begin{equation*}
a+2 c=0 \quad \text { and } \quad a+b=0 \tag{3}
\end{equation*}


Using (2) and (3), we obtain


\begin{equation*}
a=0, \quad b=0, \quad c=0 \tag{4}
\end{equation*}


Because (1) implies (4), the mappings $F, G, H$ are linearly independent.

5.35. Let $k$ be a nonzero scalar. Show that a linear map $T$ is singular if and only if $k T$ is singular. Hence, $T$ is singular if and only if $-T$ is singular.

Suppose $T$ is singular. Then $T(v)=0$ for some vector $v \neq 0$. Hence,

$$
(k T)(v)=k T(v)=k 0=0
$$

and so $k T$ is singular.

Now suppose $k T$ is singular. Then $(k T)(w)=0$ for some vector $w \neq 0$. Hence,

$$
T(k w)=k T(w)=(k T)(w)=0
$$

But $k \neq 0$ and $w \neq 0$ implies $k w \neq 0$. Thus, $T$ is also singular.

5.36. Find the dimension $d$ of:\\
(a) $\operatorname{Hom}\left(\mathbf{R}^{3}, \mathbf{R}^{4}\right)$,\\
(b) $\operatorname{Hom}\left(\mathbf{R}^{5}, \mathbf{R}^{3}\right)$\\
(c) $\operatorname{Hom}\left(\mathbf{P}_{3}(t), \mathbf{R}^{2}\right)$,\\
(d) $\operatorname{Hom}\left(\mathbf{M}_{2,3}, \mathbf{R}^{4}\right)$.

Use $\operatorname{dim}[\operatorname{Hom}(V, U)]=m n$, where $\operatorname{dim} V=m$ and $\operatorname{dim} U=n$.\\
(a) $d=3(4)=12$.\\
(c) Because $\operatorname{dim} \mathbf{P}_{3}(t)=4, d=4(2)=8$.\\
(b) $d=5(3)=15$.\\
(d) Because $\operatorname{dim} \mathbf{M}_{2,3}=6, d=6(4)=24$.

5.37. Prove Theorem 5.11. Suppose $\operatorname{dim} V=m$ and $\operatorname{dim} U=n$. Then $\operatorname{dim}[\operatorname{Hom}(V, U)]=m n$.

Suppose $\left\{v_{1}, \ldots, v_{m}\right\}$ is a basis of $V$ and $\left\{u_{1}, \ldots, u_{n}\right\}$ is a basis of $U$. By Theorem 5.2, a linear mapping in $\operatorname{Hom}(V, U)$ is uniquely determined by arbitrarily assigning elements of $U$ to the basis elements $v_{i}$ of $V$. We define

$$
F_{i j} \in \operatorname{Hom}(V, U), \quad i=1, \ldots, m, \quad j=1, \ldots, n
$$

to be the linear mapping for which $F_{i j}\left(v_{i}\right)=u_{j}$, and $F_{i j}\left(v_{k}\right)=0$ for $k \neq i$. That is, $F_{i j}$ maps $v_{i}$ into $u_{j}$ and the other $v$ 's into 0 . Observe that $\left\{F_{i j}\right\}$ contains exactly $m n$ elements; hence, the theorem is proved if we show that it is a basis of $\operatorname{Hom}(V, U)$.

Proof that $\left\{F_{i j}\right\}$ generates $\operatorname{Hom}(V, U)$. Consider an arbitrary function $F \in \operatorname{Hom}(V, U)$. Suppose $F\left(v_{1}\right)=w_{1}, F\left(v_{2}\right)=w_{2}, \ldots, F\left(v_{m}\right)=w_{m}$. Because $w_{k} \in U$, it is a linear combination of the $u$ 's; say,


\begin{equation*}
w_{k}=a_{k 1} u_{1}+a_{k 2} u_{2}+\cdots+a_{k n} u_{n}, \quad k=1, \ldots, m, \quad a_{i j} \in K \tag{1}
\end{equation*}


Consider the linear mapping $G=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} F_{i j}$. Because $G$ is a linear combination of the $F_{i j}$, the proof that $\left\{F_{i j}\right\}$ generates $\operatorname{Hom}(V, U)$ is complete if we show that $F=G$.

We now compute $G\left(v_{k}\right), k=1, \ldots, m$. Because $F_{i j}\left(v_{k}\right)=0$ for $k \neq i$ and $F_{k i}\left(v_{k}\right)=u_{i}$,

$$
\begin{aligned}
G\left(v_{k}\right) & =\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} F_{i j}\left(v_{k}\right)=\sum_{j=1}^{n} a_{k j} F_{k j}\left(v_{k}\right)=\sum_{j=1}^{n} a_{k j} u_{j} \\
& =a_{k 1} u_{1}+a_{k 2} u_{2}+\cdots+a_{k n} u_{n}
\end{aligned}
$$

Thus, by (1), $G\left(v_{k}\right)=w_{k}$ for each $k$. But $F\left(v_{k}\right)=w_{k}$ for each $k$. Accordingly, by Theorem 5.2, $F=G$; hence, $\left\{F_{i j}\right\}$ generates $\operatorname{Hom}(V, U)$.

Proof that $\left\{F_{i j}\right\}$ is linearly independent. Suppose, for scalars $c_{i j} \in K$,

$$
\sum_{i=1}^{m} \sum_{j=1}^{n} c_{i j} F_{i j}=\mathbf{0}
$$

For $v_{k}, k=1, \ldots, m$,

$$
\begin{aligned}
0=\mathbf{0}\left(v_{k}\right) & =\sum_{i=1}^{m} \sum_{j=1}^{n} c_{i j} F_{i j}\left(v_{k}\right)=\sum_{j=1}^{n} c_{k j} F_{k j}\left(v_{k}\right)=\sum_{j=1}^{n} c_{k j} u_{j} \\
& =c_{k 1} u_{1}+c_{k 2} u_{2}+\cdots+c_{k n} u_{n}
\end{aligned}
$$

But the $u_{i}$ are linearly independent; hence, for $k=1, \ldots, m$, we have $c_{k 1}=0, c_{k 2}=0, \ldots, c_{k n}=0$. In other words, all the $c_{i j}=0$, and so $\left\{F_{i j}\right\}$ is linearly independent.

5.38. Prove Theorem 5.12: (i) $G \circ\left(F+F^{\prime}\right)=G \circ F+G \circ F^{\prime}$. (ii) $\left(G+G^{\prime}\right) \circ F=G \circ F+G^{\prime} \circ F$. (iii) $k(G \circ F)=(k G) \circ F=G \circ(k F)$.

(i) For every $v \in V$,

$$
\begin{aligned}
\left(G \circ\left(F+F^{\prime}\right)\right)(v) & =G\left(\left(F+F^{\prime}\right)(v)\right)=G\left(F(v)+F^{\prime}(v)\right) \\
& =G(F(v))+G\left(F^{\prime}(v)\right)=(G \circ F)(v)+\left(G \circ F^{\prime}\right)(v)=\left(G \circ F+G \circ F^{\prime}\right)(v)
\end{aligned}
$$

Thus, $G \circ\left(F+F^{\prime}\right)=G \circ F+G \circ F^{\prime}$.

(ii) For every $v \in V$,

$$
\begin{aligned}
\left(\left(G+G^{\prime}\right) \circ F\right)(v) & =\left(G+G^{\prime}\right)(F(v))=G(F(v))+G^{\prime}(F(v)) \\
& =(G \circ F)(v)+\left(G^{\prime} \circ F\right)(v)=\left(G \circ F+G^{\prime} \circ F\right)(v)
\end{aligned}
$$

Thus, $\left(G+G^{\prime}\right) \circ F=G \circ F+G^{\prime} \circ F$.\\
(iii) For every $v \in V$,

$$
(k(G \circ F))(v)=k(G \circ F)(v)=k(G(F(v)))=(k G)(F(v))=(k G \circ F)(v)
$$

and

$$
(k(G \circ F))(v)=k(G \circ F)(v)=k(G(F(v)))=G(k F(v))=G((k F)(v))=(G \circ k F)(v)
$$

Accordingly, $k(G \circ F)=(k G) \circ F=G \circ(k F)$. (We emphasize that two mappings are shown to be equal by showing that each of them assigns the same image to each point in the domain.)

\section*{Algebra of Linear Maps}
5.39. Let $F$ and $G$ be the linear operators on $\mathbf{R}^{2}$ defined by $F(x, y)=(y, x)$ and $G(x, y)=(0, x)$. Find formulas defining the following operators:\\
(a) $F+G$,\\
(b) $2 F-3 G$,\\
(c) $F G$,\\
(d) $G F$,\\
(e) $F^{2}$,\\
(f) $G^{2}$.

(a) $(F+G)(x, y)=F(x, y)+G(x, y)=(y, x)+(0, x)=(y, 2 x)$.

(b) $(2 F-3 G)(x, y)=2 F(x, y)-3 G(x, y)=2(y, x)-3(0, x)=(2 y,-x)$.

(c) $(F G)(x, y)=F(G(x, y))=F(0, x)=(x, 0)$.

(d) $(G F)(x, y)=G(F(x, y))=G(y, x)=(0, y)$.

(e) $F^{2}(x, y)=F(F(x, y))=F(y, x)=(x, y)$. (Note that $F^{2}=I$, the identity mapping.)

(f) $G^{2}(x, y)=G(G(x, y))=G(0, x)=(0,0)$. (Note that $G^{2}=\mathbf{0}$, the zero mapping.)

5.40. Consider the linear operator $T$ on $\mathbf{R}^{3}$ defined by $T(x, y, z)=(2 x, 4 x-y, 2 x+3 y-z)$.

(a) Show that $T$ is invertible. Find formulas for (b) $T^{-1}$, (c) $T^{2},(d) T^{-2}$.

(a) Let $W=\operatorname{Ker} T$. We need only show that $T$ is nonsingular (i.e., that $W=\{0\}$ ). $\operatorname{Set} T(x, y, z)=(0,0,0)$, which yields

$$
T(x, y, z)=(2 x, 4 x-y, 2 x+3 y-z)=(0,0,0)
$$

Thus, $W$ is the solution space of the homogeneous system

$$
2 x=0, \quad 4 x-y=0, \quad 2 x+3 y-z=0
$$

which has only the trivial solution $(0,0,0)$. Thus, $W=\{0\}$. Hence, $T$ is nonsingular, and so $T$ is invertible.

(b) Set $T(x, y, z)=(r, s, t)$ [and so $\left.T^{-1}(r, s, t)=(x, y, z)\right]$. We have

$$
(2 x, 4 x-y, 2 x+3 y-z)=(r, s, t) \quad \text { or } \quad 2 x=r, \quad 4 x-y=s, \quad 2 x+3 y-z=t
$$

Solve for $x, y, z$ in terms of $r, s, t$ to get $x=\frac{1}{2} r, y=2 r-s, z=7 r-3 s-t$. Thus,

$$
T^{-1}(r, s, t)=\left(\frac{1}{2} r, 2 r-s, 7 r-3 s-t\right) \quad \text { or } \quad T^{-1}(x, y, z)=\left(\frac{1}{2} x, 2 x-y, 7 x-3 y-z\right)
$$

(c) Apply $T$ twice to get

$$
\begin{aligned}
T^{2}(x, y, z) & =T(2 x, \quad 4 x-y, \quad 2 x+3 y-z) \\
& =\left[\begin{array}{ll}
4 x, & 4(2 x)-(4 x-y), \quad 2(2 x)+3(4 x-y)-(2 x+3 y-z)
\end{array}\right] \\
& =(4 x, \quad 4 x+y, \quad 14 x-6 y+z)
\end{aligned}
$$

(d) Apply $T^{-1}$ twice to get

$$
\begin{aligned}
T^{-2}(x, y, z) & =T^{-2}\left(\frac{1}{2} x, \quad 2 x-y, \quad 7 x-3 y-z\right) \\
& =\left[\begin{array}{ll}
\frac{1}{4} x, \quad 2\left(\frac{1}{2} x\right)-(2 x-y), \quad 7\left(\frac{1}{2} x\right)-3(2 x-y)-(7 x-3 y-z)
\end{array}\right] \\
& =\left(\begin{array}{ll}
\frac{1}{4} x, & -x+y, \quad-\frac{19}{2} x+6 y+z
\end{array}\right)
\end{aligned}
$$

5.41. Let $V$ be of finite dimension and let $T$ be a linear operator on $V$ for which $T R=I$, for some operator $R$ on $V$. (We call $R$ a right inverse of $T$.)

(a) Show that $T$ is invertible. (b) Show that $R=T^{-1}$.

(c) Give an example showing that the above need not hold if $V$ is of infinite dimension.

(a) Let $\operatorname{dim} V=n$. By Theorem 5.14, $T$ is invertible if and only if $T$ is onto; hence, $T$ is invertible if and only if $\operatorname{rank}(T)=n$. We have $n=\operatorname{rank}(I)=\operatorname{rank}(T R) \leq \operatorname{rank}(T) \leq n$. Hence, $\operatorname{rank}(T)=n$ and $T$ is invertible.

(b) $T T^{-1}=T^{-1} T=I$. Then $R=I R=\left(T^{-1} T\right) R=T^{-1}(T R)=T^{-1} I=T^{-1}$.

(c) Let $V$ be the space of polynomials in $t$ over $K$; say, $p(t)=a_{0}+a_{1} t+a_{2} t^{2}+\cdots+a_{s} t^{s}$. Let $T$ and $R$ be the operators on $V$ defined by

$$
T(p(t))=0+a_{1}+a_{2} t+\cdots+a_{s} t^{s-1} \quad \text { and } \quad R(p(t))=a_{0} t+a_{1} t^{2}+\cdots+a_{s} t^{s+1}
$$

We have

$$
(T R)(p(t))=T(R(p(t)))=T\left(a_{0} t+a_{1} t^{2}+\cdots+a_{s} t^{s+1}\right)=a_{0}+a_{1} t+\cdots+a_{s} t^{s}=p(t)
$$

and so $T R=I$, the identity mapping. On the other hand, if $k \in K$ and $k \neq 0$, then

$$
(R T)(k)=R(T(k))=R(0)=0 \neq k
$$

Accordingly, $R T \neq I$.

5.42. Let $F$ and $G$ be linear operators on $\mathbf{R}^{2}$ defined by $F(x, y)=(0, x)$ and $G(x, y)=(x, 0)$. Show that

(a) $G F=\mathbf{0}$, the zero mapping, but $F G \neq \mathbf{0}$. (b) $G^{2}=G$.

(a) $(G F)(x, y)=G(F(x, y))=G(0, x)=(0,0)$. Because $G F$ assigns $0=(0,0)$ to every vector $(x, y)$ in $\mathbf{R}^{2}$, it is the zero mapping; that is, $G F=\mathbf{0}$.

On the other hand, $(F G)(x, y)=F(G(x, y))=F(x, 0)=(0, x)$. For example, $(F G)(2,3)=(0,2)$. Thus, $F G \neq \mathbf{0}$, as it does not assign $0=(0,0)$ to every vector in $\mathbf{R}^{2}$.

(b) For any vector $(x, y)$ in $\mathbf{R}^{2}$, we have $G^{2}(x, y)=G(G(x, y))=G(x, 0)=(x, 0)=G(x, y)$. Hence, $G^{2}=G$.

5.43. Find the dimension of (a) $A\left(\mathbf{R}^{4}\right)$, (b) $A\left(\mathbf{P}_{2}(t)\right.$ ), (c) $A\left(\mathbf{M}_{2,3}\right)$.

Use $\operatorname{dim}[A(V)]=n^{2}$ where $\operatorname{dim} V=n$. Hence, (a) $\operatorname{dim}\left[A\left(\mathbf{R}^{4}\right)\right]=4^{2}=16$, (b) $\operatorname{dim}\left[A\left(\mathbf{P}_{2}(t)\right)\right]=3^{2}=9$, (c) $\operatorname{dim}\left[A\left(\mathbf{M}_{2,3}\right)\right]=6^{2}=36$.

5.44. Let $E$ be a linear operator on $V$ for which $E^{2}=E$. (Such an operator is called a projection.) Let $U$ be the image of $E$, and let $W$ be the kernel. Prove

(a) If $u \in U$, then $E(u)=u$ (i.e., $E$ is the identity mapping on $U$ ).

(b) If $E \neq I$, then $E$ is singular - that is, $E(v)=0$ for some $v \neq 0$.

(c) $V=U \oplus W$.

(a) If $u \in U$, the image of $E$, then $E(v)=u$ for some $v \in V$. Hence, using $E^{2}=E$, we have

$$
u=E(v)=E^{2}(v)=E(E(v))=E(u)
$$

(b) If $E \neq I$, then for some $v \in V, E(v)=u$, where $v \neq u$. By (i), $E(u)=u$. Thus,

$$
E(v-u)=E(v)-E(u)=u-u=0, \quad \text { where } \quad v-u \neq 0
$$

(c) We first show that $V=U+W$. Let $v \in V$. Set $u=E(v)$ and $w=v-E(v)$. Then

$$
v=E(v)+v-E(v)=u+w
$$

By deflnition, $u=E(v) \in U$, the image of $E$. We now show that $w \in W$, the kernel of $E$,

$$
E(w)=E(v-E(v))=E(v)-E^{2}(v)=E(v)-E(v)=0
$$

and thus $w \in W$. Hence, $V=U+W$.

We next show that $U \cap W=\{0\}$. Let $v \in U \cap W$. Because $v \in U, E(v)=v$ by part (a). Because $v \in W, E(v)=0$. Thus, $v=E(v)=0$ and so $U \cap W=\{0\}$.

The above two properties imply that $V=U \oplus W$.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Mappings}
5.45. Determine the number of different mappings from $(a)\{1,2\}$ into $\{1,2,3\},(b)\{1,2, \ldots, r\}$ into $\{1,2, \ldots, s\}$.

5.46. Let $f: \mathbf{R} \rightarrow \mathbf{R}$ and $g: \mathbf{R} \rightarrow \mathbf{R}$ be defined by $f(x)=x^{2}+3 x+1$ and $g(x)=2 x-3$. Find formulas defining the composition mappings: (a) $f \circ g$; (b) $g \circ f$; (c) $g \circ g$; (d) $f \circ f$.

5.47. For each mappings $f: \mathbf{R} \rightarrow \mathbf{R}$ find a formula for its inverse: (a) $f(x)=3 x-7$, (b) $f(x)=x^{3}+2$.

5.48. For any mapping $f: A \rightarrow B$, show that $\mathbf{1}_{B} \circ f=f=f \circ \mathbf{1}_{A}$.

\section*{Linear Mappings}
5.49. Show that the following mappings are linear:

(a) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(x+2 y-3 z, 4 x-5 y+6 z)$.

(b) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(a x+b y, c x+d y)$, where $a, b, c, d$ belong to $\mathbf{R}$.

5.50. Show that the following mappings are not linear:

(a) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=\left(x^{2}, y^{2}\right)$.

(b) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(x+1, y+z)$.

(c) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(x y, y)$.

(d) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(|x|, y+z)$.

5.51. Find $F(a, b)$, where the linear map $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ is defined by $F(1,2)=(3,-1)$ and $F(0,1)=(2,1)$.

5.52. Find a $2 \times 2$ matrix $A$ that maps

(a) $(1,3)^{T}$ and $(1,4)^{T}$ into $(-2,5)^{T}$ and $(3,-1)^{T}$, respectively.

(b) $(2,-4)^{T}$ and $(-1,2)^{T}$ into $(1,1)^{T}$ and $(1,3)^{T}$, respectively.

5.53. Find a $2 \times 2$ singular matrix $B$ that maps $(1,1)^{T}$ into $(1,3)^{T}$.

5.54. Let $V$ be the vector space of real $n$-square matrices, and let $M$ be a fixed nonzero matrix in $V$. Show that the first two of the following mappings $T: V \rightarrow V$ are linear, but the third is not:

(a) $T(A)=M A$, (b) $T(A)=A M+M A$, (c) $T(A)=M+A$.

5.55. Give an example of a nonlinear map $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ such that $F^{-1}(0)=\{0\}$ but $F$ is not one-to-one.

5.56. Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y)=(3 x+5 y, 2 x+3 y)$, and let $S$ be the unit circle in $\mathbf{R}^{2}$. ( $S$ consists of all points satisfying $x^{2}+y^{2}=1$.) Find (a) the image $F(S)$, (b) the preimage $F^{-1}(S)$.

5.57. Consider the linear map $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by $G(x, y, z)=(x+y+z, y-2 z, y-3 z)$ and the unit sphere $S_{2}$ in $\mathbf{R}^{3}$, which consists of the points satisfying $x^{2}+y^{2}+z^{2}=1$. Find (a) $G\left(S_{2}\right)$, (b) $G^{-1}\left(S_{2}\right)$.

5.58. Let $H$ be the plane $x+2 y-3 z=4$ in $\mathbf{R}^{3}$ and let $G$ be the linear map in Problem 5.57. Find (a) $G(H)$, (b) $G^{-1}(H)$.

5.59. Let $W$ be a subspace of $V$. The inclusion map, denoted by $i: W \hookrightarrow V$, is defined by $i(w)=w$ for every $w \in W$. Show that the inclusion map is linear.

5.60. Suppose $F: V \rightarrow U$ is linear. Show that $F(-v)=-F(v)$.

\section*{Kernel and Image of Linear Mappings}
5.61. For each linear map $F$ find a basis and the dimension of the kernel and the image of $F$ :

(a) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by $F(x, y, z)=(x+2 y-3 z, 2 x+5 y-4 z, x+4 y+z)$,

(b) $F: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$ defined by $F(x, y, z, t)=(x+2 y+3 z+2 t, 2 x+4 y+7 z+5 t, x+2 y+6 z+5 t)$.

5.62. For each linear map $G$, find a basis and the dimension of the kernel and the image of $G$ :

(a) $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $G(x, y, z)=(x+y+z, 2 x+2 y+2 z)$,

(b) $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $G(x, y, z)=(x+y, y+z)$,

(c) $G: \mathbf{R}^{5} \rightarrow \mathbf{R}^{3}$ defined by

$$
G(x, y, z, s, t)=(x+2 y+2 z+s+t, \quad x+2 y+3 z+2 s-t, \quad 3 x+6 y+8 z+5 s-t) .
$$

5.63. Each of the following matrices determines a linear map from $\mathbf{R}^{4}$ into $\mathbf{R}^{3}$ :

$$
\text { (a) } A=\left[\begin{array}{rrrr}
1 & 2 & 0 & 1 \\
2 & -1 & 2 & -1 \\
1 & -3 & 2 & -2
\end{array}\right] \text {, (b) } B=\left[\begin{array}{rrrr}
1 & 0 & 2 & -1 \\
2 & 3 & -1 & 1 \\
-2 & 0 & -5 & 3
\end{array}\right] \text {. }
$$

Find a basis as well as the dimension of the kernel and the image of each linear map.

5.64. Find a linear mapping $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ whose image is spanned by $(1,2,3)$ and $(4,5,6)$.

5.65. Find a linear mapping $G: \mathbf{R}^{4} \rightarrow \mathbf{R}^{3}$ whose kernel is spanned by $(1,2,3,4)$ and $(0,1,1,1)$.

5.66. Let $V=\mathbf{P}_{10}(t)$, the vector space of polynomials of degree $\leq 10$. Consider the linear map $\mathbf{D}^{4}: V \rightarrow V$, where $\mathbf{D}^{4}$ denotes the fourth derivative $d^{4}(f) / d t^{4}$. Find a basis and the dimension of

(a) the image of $\mathbf{D}^{4}$; (b) the kernel of $\mathbf{D}^{4}$.

5.67. Suppose $F: V \rightarrow U$ is linear. Show that (a) the image of any subspace of $V$ is a subspace of $U$;

(b) the preimage of any subspace of $U$ is a subspace of $V$.

5.68. Show that if $F: V \rightarrow U$ is onto, then $\operatorname{dim} U \leq \operatorname{dim} V$. Determine all linear maps $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{4}$ that are onto.

5.69. Consider the zero mapping $\mathbf{0}: V \rightarrow U$ defined by $\mathbf{0}(v)=0, \forall v \in V$. Find the kernel and the image of $\mathbf{0}$.

\section*{Operations with linear Mappings}
5.70. Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ and $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y, z)=(y, x+z)$ and $G(x, y, z)=(2 z, x-y)$. Find formulas defining the mappings $F+G$ and $3 F-2 G$.

5.71. Let $H: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $H(x, y)=(y, 2 x)$. Using the maps $F$ and $G$ in Problem 5.70 , find formulas defining the mappings: (a) $H \circ F$ and $H \circ G$, (b) $F \circ H$ and $G \circ H$, (c) $H \circ(F+G)$ and $H \circ F+H \circ G$.

5.72. Show that the following mappings $F, G, H$ are linearly independent:

(a) $F, G, H \in \operatorname{Hom}\left(\mathbf{R}^{2}, \mathbf{R}^{2}\right)$ defined by $F(x, y)=(x, 2 y), \quad G(x, y)=(y, x+y), \quad H(x, y)=(0, x)$,

(b) $F, G, H \in \operatorname{Hom}\left(\mathbf{R}^{3}, \mathbf{R}\right)$ defined by $F(x, y, z)=x+y+z, \quad G(x, y, z)=y+z, \quad H(x, y, z)=x-z$.

5.73. For $F, G \in \operatorname{Hom}(V, U)$, show that $\operatorname{rank}(F+G) \leq \operatorname{rank}(F)+\operatorname{rank}(G)$. (Here $V$ has finite dimension.)

5.74. Let $F: V \rightarrow U$ and $G: U \rightarrow V$ be linear. Show that if $F$ and $G$ are nonsingular, then $G \circ F$ is nonsingular. Give an example where $G \circ F$ is nonsingular but $G$ is not. [Hint: Let $\operatorname{dim} V<\operatorname{dim} U$.]

5.75. Find the dimension $d$ of (a) $\operatorname{Hom}\left(\mathbf{R}^{2}, \mathbf{R}^{8}\right)$, (b) $\operatorname{Hom}\left(\mathbf{P}_{4}(t), \mathbf{R}^{3}\right)$, (c) $\operatorname{Hom}\left(\mathbf{M}_{2,4}, \mathbf{P}_{2}(t)\right)$.

5.76. Determine whether or not each of the following linear maps is nonsingular. If not, find a nonzero vector $v$ whose image is 0 ; otherwise find a formula for the inverse map:

(a) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by $F(x, y, z)=(x+y+z, \quad 2 x+3 y+5 z, \quad x+3 y+7 z)$,

(b) $G: \mathbf{R}^{3} \rightarrow \mathbf{P}_{2}(t)$ defined by $G(x, y, z)=(x+y) t^{2}+(x+2 y+2 z) t+y+z$,

(c) $H: \mathbf{R}^{2} \rightarrow \mathbf{P}_{2}(t)$ defined by $H(x, y)=(x+2 y) t^{2}+(x-y) t+x+y$.

5.77. When can $\operatorname{dim}[\operatorname{Hom}(V, U)]=\operatorname{dim} V$ ?

\section*{Algebra of Linear Operators}
5.78. Let $F$ and $G$ be the linear operators on $\mathbf{R}^{2}$ defined by $F(x, y)=(x+y, 0)$ and $G(x, y)=(-y, x)$. Find formulas defining the linear operators: (a) $F+G$, (b) $5 F-3 G$, (c) $F G$, (d) $G F$, (e) $F^{2}$, ( $f$ ) $G^{2}$.

5.79. Show that each linear operator $T$ on $\mathbf{R}^{2}$ is nonsingular and find a formula for $T^{-1}$, where (a) $T(x, y)=(x+2 y, 2 x+3 y)$, (b) $T(x, y)=(2 x-3 y, 3 x-4 y)$.

5.80. Show that each of the following linear operators $T$ on $\mathbf{R}^{3}$ is nonsingular and find a formula for $T^{-1}$, where (a) $T(x, y, z)=(x-3 y-2 z, y-4 z, z)$; (b) $T(x, y, z)=(x+z, x-y, y)$.

5.81. Find the dimension of $A(V)$, where (a) $V=\mathbf{R}^{7}$, (b) $V=\mathbf{P}_{5}(t)$, (c) $V=\mathbf{M}_{3,4}$.

5.82. Which of the following integers can be the dimension of an algebra $A(V)$ of linear maps: $5,9,12,25,28,36,45,64,88,100$ ?

5.83. Let $T$ be the linear operator on $\mathbf{R}^{2}$ defined by $T(x, y)=(x+2 y, 3 x+4 y)$. Find a formula for $f(T)$, where (a) $f(t)=t^{2}+2 t-3$, (b) $f(t)=t^{2}-5 t-2$.

\section*{Miscellaneous Problems}
5.84. Suppose $F: V \rightarrow U$ is linear and $k$ is a nonzero scalar. Prove that the maps $F$ and $k F$ have the same kernel and the same image.

5.85. Suppose $F$ and $G$ are linear operators on $V$ and that $F$ is nonsingular. Assume that $V$ has finite dimension. Show that $\operatorname{rank}(F G)=\operatorname{rank}(G F)=\operatorname{rank}(G)$.

5.86. Suppose $V$ has finite dimension. Suppose $T$ is a linear operator on $V$ such that $\operatorname{rank}\left(T^{2}\right)=\operatorname{rank}(T)$. Show that $\operatorname{Ker} T \cap \operatorname{Im} T=\{0\}$.

5.87. Suppose $V=U \oplus W$. Let $E_{1}$ and $E_{2}$ be the linear operators on $V$ defined by $E_{1}(v)=u, E_{2}(v)=w$, where $v=u+w, u \in U, w \in W$. Show that (a) $E_{1}^{2}=E_{1}$ and $E_{2}^{2}=E_{2}$ (i.e., that $E_{1}$ and $E_{2}$ are projections); (b) $E_{1}+E_{2}=I$, the identity mapping; (c) $E_{1} E_{2}=\mathbf{0}$ and $E_{2} E_{1}=\mathbf{0}$.

5.88. Let $E_{1}$ and $E_{2}$ be linear operators on $V$ satisfying parts (a), (b), (c) of Problem 5.88. Prove

$$
V=\operatorname{Im} E_{1} \oplus \operatorname{Im} E_{2}
$$

5.89. Let $v$ and $w$ be elements of a real vector space $V$. The line segment $L$ from $v$ to $v+w$ is defined to be the set of vectors $v+t w$ for $0 \leq t \leq 1$. (See Fig. 5.6.)

(a) Show that the line segment $L$ between vectors $v$ and $u$ consists of the points:

(i) $(1-t) v+t u$ for $0 \leq t \leq 1$, (ii) $t_{1} v+t_{2} u$ for $t_{1}+t_{2}=1, t_{1} \geq 0, t_{2} \geq 0$.

(b) Let $F: V \rightarrow U$ be linear. Show that the image $F(L)$ of a line segment $L$ in $V$ is a line segment in $U$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-199}
\end{center}

Figure 5-6

5.90. Let $F: V \rightarrow U$ be linear and let $W$ be a subspace of $V$. The restriction of $F$ to $W$ is the map $F \mid W: W \rightarrow U$ defined by $F \mid W(v)=F(v)$ for every $v$ in $W$. Prove the following:

(a) $F \mid W$ is linear; (b) $\operatorname{Ker}(F \mid W)=(\operatorname{Ker} F) \cap W$; (c) $\operatorname{Im}(F \mid W)=F(W)$.

5.91. A subset $X$ of a vector space $V$ is said to be convex if the line segment $L$ between any two points (vectors) $P, Q \in X$ is contained in $X$. (a) Show that the intersection of convex sets is convex; (b) suppose $F: V \rightarrow U$ is linear and $X$ is convex. Show that $F(X)$ is convex.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
5.45. (a) $3^{2}=9$, (b) $s^{r}$

5.46. (a) $(f \circ g)(x)=4 x^{2}+1$, (b) $(g \circ f)(x)=2 x^{2}+6 x-1$, (c) $(g \circ g)(x)=4 x-9$,

(d) $(f \circ f)(x)=x^{4}+6 x^{3}+14 x^{2}+15 x+5$

5.47. (a) $f^{-1}(x)=\frac{1}{3}(x+7)$, (b) $f^{-1}(x)=\sqrt[3]{x-2}$

5.49. $F(x, y, z)=A(x, y, z)^{T}$, where (a) $A=\left[\begin{array}{rrr}1 & 2 & -3 \\ 4 & -5 & 6\end{array}\right]$, (b) $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$

5.50. (a) $u=(2,2), k=3$; then $F(k u)=(36,36)$ but $k F(u)=(12,12)$; (b) $F(0) \neq 0$;

(c) $u=(1,2), v=(3,4)$; then $F(u+v)=(24,6)$ but $F(u)+F(v)=(14,6)$;

(d) $u=(1,2,3), k=-2$; then $F(k u)=(2,-10)$ but $k F(u)=(-2,-10)$.

5.51. $F(a, b)=(-a+2 b,-3 a+b)$

5.52. (a) $A=\left[\begin{array}{cr}-17 & 5 \\ 23 & -6\end{array}\right]$; (b) None. $(2,-4)$ and $(-1,2)$ are linearly dependent but not $(1,1)$ and $(1,3)$.

5.53. $B=\left[\begin{array}{ll}1 & 0 \\ 3 & 0\end{array}\right]\left[\right.$ Hint: Send $(0,1)^{T}$ into $\left.(0,0)^{T}.\right]$

5.55. $F(x, y)=\left(x^{2}, y^{2}\right)$

5.56. (a) $13 x^{2}-42 x y+34 y^{2}=1$, (b) $13 x^{2}+42 x y+34 y^{2}=1$

5.57. (a) $x^{2}-8 x y+26 y^{2}+6 x z-38 y z+14 z^{2}=1$, (b) $x^{2}+2 x y+3 y^{2}+2 x z-8 y z+14 z^{2}=1$

5.58. (a) $x-y+2 z=4$, (b) $x+6 z=4$

5.61. (a) $\operatorname{dim}(\operatorname{Ker} F)=1,\{(7,-2,1)\} ; \operatorname{dim}(\operatorname{Im} F)=2,\{(1,2,1), \quad(0,1,2)\}$;

(b) $\operatorname{dim}(\operatorname{Ker} F)=2,\{(-2,1,0,0), \quad(1,0,-1,1)\} ; \operatorname{dim}(\operatorname{Im} F)=2,\{(1,2,1), \quad(0,1,3)\}$

5.62. (a) $\operatorname{dim}(\operatorname{Ker} G)=2,\{(1,0,-1), \quad(1,-1,0)\} ; \operatorname{dim}(\operatorname{Im} G)=1,\{(1,2)\}$;

(b) $\operatorname{dim}(\operatorname{Ker} G)=1,\{(1,-1,1)\} ; \operatorname{Im} G=\mathbf{R}^{2},\{(1,0), \quad(0,1)\}$;

(c) $\operatorname{dim}(\operatorname{Ker} G)=3,\{(-2,1,0,0,0), \quad(1,0,-1,1,0), \quad(-5,0,2,0,1)\} ; \operatorname{dim}(\operatorname{Im} G)=2$, $\{(1,1,3), \quad(0,1,2)\}$

5.63. (a) $\operatorname{dim}(\operatorname{Ker} A)=2,\{(4,-2,-5,0), \quad(1,-3,0,5)\} ; \operatorname{dim}(\operatorname{Im} A)=2,\{(1,2,1), \quad(0,1,1)\}$;

(b) $\operatorname{dim}(\operatorname{Ker} B)=1,\left\{\left(-1, \frac{2}{3}, 1,1\right)\right\} ; \operatorname{Im} B=\mathbf{R}^{3}$

5.64. $F(x, y, z)=(x+4 y, 2 x+5 y, 3 x+6 y)$

5.65. $F(x, y, z, t)=(x+y-z, 2 x+y-t, 0)$

5.66. (a) $\left\{1, t, t^{2}, \ldots, t^{6}\right\}$, (b) $\left\{1, t, t^{2}, t^{3}\right\}$

5.68. None, because $\operatorname{dim} \mathbf{R}^{4}>\operatorname{dim} \mathbf{R}^{3}$.

5.69. $\operatorname{Ker} \mathbf{0}=V, \operatorname{Im} \mathbf{0}=\{0\}$

5.70. $(F+G)(x, y, z)=(y+2 z, 2 x-y+z),(3 F-2 G)(x, y, z)=(3 y-4 z, x+2 y+3 z)$

5.71. (a) $(H \circ F)(x, y, z)=(x+z, \quad 2 y),(H \circ G)(x, y, z)=(x-y, \quad 4 z)$; (b) not defined;

(c) $(H \circ(F+G))(x, y, z)=(H \circ F+H \circ G)(x, y, z)=(2 x-y+z, \quad 2 y+4 z)$

5.74. $F(x, y)=(x, y, y), G(x, y, z)=(x, y)$

5.75. (a) 16 , (b) 15 , (c) 24

5.76. (a) $v=(2,-3,1)$; (b) $G^{-1}\left(a t^{2}+b t+c\right)=(b-2 c, \quad a-b+2 c, \quad-a+b-c)$;

(c) $H$ is nonsingular, but not invertible, because $\operatorname{dim} \mathbf{P}_{2}(t)>\operatorname{dim} \mathbf{R}^{2}$.

5.77. $\operatorname{dim} U=1$; that is, $U=K$.

5.78. (a) $(F+G)(x, y)=(x, x)$; (b) $(5 F-3 G)(x, y)=(5 x+8 y, \quad-3 x)$; (c) $(F G)(x, y)=(x-y, 0)$;

(d) $(G F)(x, y)=(0, x+y) ;(e) F^{2}(x, y)=(x+y, \quad 0)$ (note that $\left.F^{2}=F\right) ;(f) G^{2}(x, y)=\left(\begin{array}{ll}-x, & -y\end{array}\right)$. [Note that $G^{2}+I=0$; hence, $G$ is a zero of $f(t)=t^{2}+1$.]

5.79. (a) $T^{-1}(x, y)=(-3 x+2 y, 2 x-y)$, (b) $T^{-1}(x, y)=(-4 x+3 y,-3 x+2 y)$

5.80. (a) $T^{-1}(x, y, z)=(x+3 y+14 z, y-4 z, z)$, (b) $T^{-1}(x, y, z)=(y+z, y, x-y-z)$

5.81. (a) 49 , (b) 36 , (c) 144

5.82. Squares: $9,25,36,64,100$

5.83. (a) $T(x, y)=(6 x+14 y, \quad 21 x+27 y)$; (b) $T(x, y)=(0,0)$-that is, $f(T)=0$

\section*{CHAPTER 6}
\section*{Linear Mappings and Matrices}
\subsection*{6.1 Introduction}
Consider a basis $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of a vector space $V$ over a field $K$. For any vector $v \in V$, suppose

$$
v=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}
$$

Then the coordinate vector of $v$ relative to the basis $S$, which we assume to be a column vector (unless otherwise stated or implied), is denoted and defined by

$$
[v]_{S}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]^{T}
$$

Recall (Section 4.11) that the mapping $v \mapsto[v]_{S}$, determined by the basis $S$, is an isomorphism between $V$ and $K^{n}$.

This chapter shows that there is also an isomorphism, determined by the basis $S$, between the algebra $A(V)$ of linear operators on $V$ and the algebra $M$ of $n$-square matrices over $K$. Thus, every linear mapping $F: V \rightarrow V$ will correspond to an $n$-square matrix $[F]_{S}$ determined by the basis $S$. We will also show how our matrix representation changes when we choose another basis.

\subsection*{6.2 Matrix Representation of a Linear Operator}
Let $T$ be a linear operator (transformation) from a vector space $V$ into itself, and suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis of $V$. Now $T\left(u_{1}\right), T\left(u_{2}\right), \ldots, T\left(u_{n}\right)$ are vectors in $V$, and so each is a linear combination of the vectors in the basis $S$; say,

$$
\begin{aligned}
& T\left(u_{1}\right)=a_{11} u_{1}+a_{12} u_{2}+\cdots+a_{1 n} u_{n} \\
& T\left(u_{2}\right)=a_{21} u_{1}+a_{22} u_{2}+\cdots+a_{2 n} u_{n} \\
& T\left(u_{n}\right)=a_{n 1} u_{1}+a_{n 2} u_{2}+\cdots+a_{n n} u_{n}
\end{aligned}
$$

The following definition applies.

DEFINITION: The transpose of the above matrix of coefficients, denoted by $m_{S}(T)$ or $[T]_{S}$, is called the matrix representation of $T$ relative to the basis $S$, or simply the matrix of $T$ in the basis $S$. (The subscript $S$ may be omitted if the basis $S$ is understood.)

Using the coordinate (column) vector notation, the matrix representation of $T$ may be written in the form

$$
m_{S}(T)=[T]_{S}=\left[\left[T\left(u_{1}\right)\right]_{S}, \quad\left[T\left(u_{2}\right)\right]_{S}, \ldots,\left[T\left(u_{1}\right)\right]_{S}\right]
$$

That is, the columns of $m(T)$ are the coordinate vectors of $T\left(u_{1}\right), T\left(u_{2}\right), \ldots, T\left(u_{n}\right)$, respectively.

EXAMPLE 6.1 Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be the linear operator defined by $F(x, y)=(2 x+3 y, 4 x-5 y)$.

(a) Find the matrix representation of $F$ relative to the basis $S=\left\{u_{1}, u_{2}\right\}=\{(1,2),(2,5)\}$.

(1) First find $F\left(u_{1}\right)$, and then write it as a linear combination of the basis vectors $u_{1}$ and $u_{2}$. (For notational convenience, we use column vectors.) We have

$F\left(u_{1}\right)=F\left(\left[\begin{array}{l}1 \\ 2\end{array}\right]\right)=\left[\begin{array}{r}8 \\ -6\end{array}\right]=x\left[\begin{array}{l}1 \\ 2\end{array}\right]+y\left[\begin{array}{l}2 \\ 5\end{array}\right] \quad$ and $\quad \begin{array}{r}x+2 y=8 \\ 2 x+5 y=-6\end{array}$

Solve the system to obtain $x=52, y=-22$. Hence, $F\left(u_{1}\right)=52 u_{1}-22 u_{2}$.

(2) Next find $F\left(u_{2}\right)$, and then write it as a linear combination of $u_{1}$ and $u_{2}$ :

$$
F\left(u_{2}\right)=F\left(\left[\begin{array}{l}
2 \\
5
\end{array}\right]\right)=\left[\begin{array}{r}
19 \\
-17
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2
\end{array}\right]+y\left[\begin{array}{l}
2 \\
5
\end{array}\right] \quad \text { and } \quad \begin{array}{r}
x+2 y=19 \\
2 x+5 y=-17
\end{array}
$$

Solve the system to get $x=129, y=-55$. Thus, $F\left(u_{2}\right)=129 u_{1}-55 u_{2}$.

Now write the coordinates of $F\left(u_{1}\right)$ and $F\left(u_{2}\right)$ as columns to obtain the matrix

$$
[F]_{S}=\left[\begin{array}{rr}
52 & 129 \\
-22 & -55
\end{array}\right]
$$

(b) Find the matrix representation of $F$ relative to the (usual) basis $E=\left\{e_{1}, e_{2}\right\}=\{(1,0),(0,1)\}$.

Find $F\left(e_{1}\right)$ and write it as a linear combination of the usual basis vectors $e_{1}$ and $e_{2}$, and then find $F\left(e_{2}\right)$ and write it as a linear combination of $e_{1}$ and $e_{2}$. We have

$$
\begin{aligned}
& F\left(e_{1}\right)=F(1,0)=(2,2)=2 e_{1}+4 e_{2} \\
& F\left(e_{2}\right)=F(0,1)=(3,-5)=3 e_{1}-5 e_{2}
\end{aligned} \quad \text { and so } \quad[F]_{E}=\left[\begin{array}{rr}
2 & 3 \\
4 & -5
\end{array}\right]
$$

Note that the coordinates of $F\left(e_{1}\right)$ and $F\left(e_{2}\right)$ form the columns, not the rows, of $[F]_{E}$. Also, note that the arithmetic is much simpler using the usual basis of $\mathbf{R}^{2}$.

EXAMPLE 6.2 Let $V$ be the vector space of functions with basis $S=\left\{\sin t, \cos t, e^{3 t}\right\}$, and let $\mathbf{D}: V \rightarrow V$ be the differential operator defined by $\mathbf{D}(f(t))=d(f(t)) / d t$. We compute the matrix representing $\mathbf{D}$ in the basis $S$ :

$$
\begin{aligned}
& \mathbf{D}(\sin t)=\cos t=0(\sin t)+1(\cos t)+0\left(e^{3} t\right) \\
& \mathbf{D}(\cos t)=-\sin t=-1(\sin t)+0(\cos t)+0\left(e^{3 t}\right) \\
& \mathbf{D}\left(e^{3 t}\right)=3 e^{3 t}=0(\sin t)+0(\cos t)+3\left(e^{3 t}\right)
\end{aligned}
$$

and so

$$
[\mathbf{D}]=\left[\begin{array}{rrr}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 3
\end{array}\right]
$$

Note that the coordinates of $\mathbf{D}(\sin t), \mathbf{D}(\cos t), \mathbf{D}\left(e^{3 t}\right)$ form the columns, not the rows, of $[\mathbf{D}]$.

\section*{Matrix Mappings and Their Matrix Representation}
Consider the following matrix $A$, which may be viewed as a linear operator on $\mathbf{R}^{2}$, and basis $S$ of $\mathbf{R}^{2}$ :

$$
A=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right] \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\left\{\left[\begin{array}{l}
1 \\
2
\end{array}\right],\left[\begin{array}{l}
2 \\
5
\end{array}\right]\right\}
$$

(We write vectors as columns, because our map is a matrix.) We find the matrix representation of $A$ relative to the basis $S$.

(1) First we write $A\left(u_{1}\right)$ as a linear combination of $u_{1}$ and $u_{2}$. We have

$$
A\left(u_{1}\right)=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right]\left[\begin{array}{l}
1 \\
2
\end{array}\right]=\left[\begin{array}{l}
-1 \\
-6
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2
\end{array}\right]+y\left[\begin{array}{l}
2 \\
5
\end{array}\right] \quad \text { and so } \quad \begin{array}{r}
x+2 y=-1 \\
2 x+5 y=-6
\end{array}
$$

Solving the system yields $x=7, y=-4$. Thus, $A\left(u_{1}\right)=7 u_{1}-4 u_{2}$.

(2)

$$
A\left(u_{2}\right)=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right]\left[\begin{array}{l}
2 \\
5
\end{array}\right]=\left[\begin{array}{l}
-4 \\
-7
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2
\end{array}\right]+y\left[\begin{array}{l}
2 \\
5
\end{array}\right] \quad \text { and so } \quad \begin{array}{r}
x+2 y=-4 \\
2 x+5 y=-7
\end{array}
$$

Solving the system yields $x=-6, y=1$. Thus, $A\left(u_{2}\right)=-6 u_{1}+u_{2}$. Writing the coordinates of $A\left(u_{1}\right)$ and $A\left(u_{2}\right)$ as columns gives us the following matrix representation of $A$ :

$$
[A]_{S}=\left[\begin{array}{rr}
7 & -6 \\
-4 & 1
\end{array}\right]
$$

Remark: Suppose we want to find the matrix representation of $A$ relative to the usual basis $E=\left\{e_{1}, e_{2}\right\}=\left\{[1,0]^{T},[0,1]^{T}\right\}$ of $\mathbf{R}^{2}$. We have

$$
\begin{aligned}
& A\left(e_{1}\right)=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right]\left[\begin{array}{l}
1 \\
0
\end{array}\right]=\left[\begin{array}{l}
3 \\
4
\end{array}\right]=3 e_{1}+4 e_{2} \\
& A\left(e_{2}\right)=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right]\left[\begin{array}{l}
0 \\
1
\end{array}\right]=\left[\begin{array}{l}
-2 \\
-5
\end{array}\right]=-2 e_{1}-5 e_{2} \\
& \text { and so } \quad[A]_{E}=\left[\begin{array}{ll}
3 & -2 \\
4 & -5
\end{array}\right]
\end{aligned}
$$

Note that $[A]_{E}$ is the original matrix $A$. This result is true in general:

The matrix representation of any $n \times n$ square matrix $A$ over a field $K$ relative to the usual basis $E$ of $K^{n}$ is the matrix $A$ itself; that is,

$$
[A]_{E}=A
$$

\section*{Algorithm for Finding Matrix Representations}
Next follows an algorithm for finding matrix representations. The first Step 0 is optional. It may be useful to use it in Step 1(b), which is repeated for each basis vector.

ALGORITHM 6.1: The input is a linear operator $T$ on a vector space $V$ and a basis $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of $V$. The output is the matrix representation $[T]_{S}$.

Step 0. Find a formula for the coordinates of an arbitrary vector $v$ relative to the basis $S$.

Step 1. Repeat for each basis vector $u_{k}$ in $S$ :

(a) Find $T\left(u_{k}\right)$.

(b) Write $T\left(u_{k}\right)$ as a linear combination of the basis vectors $u_{1}, u_{2}, \ldots, u_{n}$.

Step 2. Form the matrix $[T]_{S}$ whose columns are the coordinate vectors in Step 1(b).

EXAMPLE 6.3 Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y)=(2 x+3 y, 4 x-5 y)$. Find the matrix representation $[F]_{S}$ of $F$ relative to the basis $S=\left\{u_{1}, u_{2}\right\}=\{(1,-2),(2,-5)\}$.

(Step 0) First find the coordinates of $(a, b) \in \mathbf{R}^{2}$ relative to the basis $S$. We have

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-204}
\end{center}

Solving for $x$ and $y$ in terms of $a$ and $b$ yields $x=5 a+2 b, y=-2 a-b$. Thus,

$$
(a, b)=(5 a+2 b) u_{1}+(-2 a-b) u_{2}
$$

(Step 1) Now we find $F\left(u_{1}\right)$ and write it as a linear combination of $u_{1}$ and $u_{2}$ using the above formula for $(a, b)$, and then we repeat the process for $F\left(u_{2}\right)$. We have

$$
\begin{aligned}
& F\left(u_{1}\right)=F(1,-2)=(-4,14)=8 u_{1}-6 u_{2} \\
& F\left(u_{2}\right)=F(2,-5)=(-11,33)=11 u_{1}-11 u_{2}
\end{aligned}
$$

(Step 2) Finally, we write the coordinates of $F\left(u_{1}\right)$ and $F\left(u_{2}\right)$ as columns to obtain the required matrix:

$$
[F]_{S}=\left[\begin{array}{rr}
8 & 11 \\
-6 & -11
\end{array}\right]
$$

\section*{Properties of Matrix Representations}
This subsection gives the main properties of the matrix representations of linear operators $T$ on a vector space $V$. We emphasize that we are always given a particular basis $S$ of $V$.

Our first theorem, proved in Problem 6.9, tells us that the "action" of a linear operator $T$ on a vector $v$ is preserved by its matrix representation.

THEOREM 6.1: $\quad$ Let $T: V \rightarrow V$ be a linear operator, and let $S$ be a (finite) basis of $V$. Then, for any vector $v$ in $V,[T]_{S}[v]_{S}=[T(v)]_{S}$.

EXAMPLE 6.4 Consider the linear operator $F$ on $R^{2}$ and the basis $S$ of Example 6.3; that is,

$$
F(x, y)=(2 x+3 y, \quad 4 x-5 y) \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,-2), \quad(2,-5)\}
$$

Let

$$
v=(5,-7), \quad \text { and so } \quad F(v)=(-11,55)
$$

Using the formula from Example 6.3, we get

$$
[v]=[11,-3]^{T} \quad \text { and } \quad[F(v)]=[55,-33]^{T}
$$

We verify Theorem 6.1 for this vector $v$ (where $[F]$ is obtained from Example 6.3):

$$
[F][v]=\left[\begin{array}{rr}
8 & 11 \\
-6 & -11
\end{array}\right]\left[\begin{array}{r}
11 \\
-3
\end{array}\right]=\left[\begin{array}{r}
55 \\
-33
\end{array}\right]=[F(v)]
$$

Given a basis $S$ of a vector space $V$, we have associated a matrix $[T]$ to each linear operator $T$ in the algebra $A(V)$ of linear operators on $V$. Theorem 6.1 tells us that the "action" of an individual linear operator $T$ is preserved by this representation. The next two theorems (proved in Problems 6.10 and 6.11) tell us that the three basic operations in $A(V)$ with these operators - namely (i) addition, (ii) scalar multiplication, and (iii) composition - are also preserved.

THEOREM 6.2: $\quad$ Let $V$ be an $n$-dimensional vector space over $K$, let $S$ be a basis of $V$, and let $\mathbf{M}$ be the algebra of $n \times n$ matrices over $K$. Then the mapping

$$
m: A(V) \rightarrow \mathbf{M} \quad \text { defined by } \quad m(T)=[T]_{S}
$$

is a vector space isomorphism. That is, for any $F, G \in A(V)$ and any $k \in K$,

(i) $m(F+G)=m(F)+m(G)$ or $[F+G]=[F]+[G]$

(ii) $m(k F)=k m(F)$ or $[k F]=k[F]$

(iii) $m$ is bijective (one-to-one and onto).

THEOREM 6.3: For any linear operators $F, G \in A(V)$,

$$
m(G \circ F)=m(G) m(F) \text { or }[G \circ F]=[G][F]
$$

(Here $G \circ F$ denotes the composition of the maps $G$ and $F$.)

\subsection*{6.3 Change of Basis}
Let $V$ be an $n$-dimensional vector space over a field $K$. We have shown that once we have selected a basis $S$ of $V$, every vector $v \in V$ can be represented by means of an $n$-tuple $[v]_{S}$ in $K^{n}$, and every linear operator $T$ in $A(V)$ can be represented by an $n \times n$ matrix over $K$. We ask the following natural question:

How do our representations change if we select another basis?

In order to answer this question, we first need a definition.

DEFINITION: $\quad$ Let $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ be a basis of a vector space $V$, and let $S^{\prime}=\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be another basis. (For reference, we will call $S$ the "old" basis and $S^{\prime}$ the "new" basis.) Because $S$ is a basis, each vector in the "new" basis $S^{\prime}$ can be written uniquely as a linear combination of the vectors in $S$; say,

$$
\begin{aligned}
& v_{1}=a_{11} u_{1}+a_{12} u_{2}+\cdots+a_{1 n} u_{n} \\
& v_{2}=a_{21} u_{1}+a_{22} u_{2}+\cdots+a_{2 n} u_{n} \\
& \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\
& v_{n}=a_{n 1} u_{1}+a_{n 2} u_{2}+\cdots+a_{n n} u_{n}
\end{aligned}
$$

Let $P$ be the transpose of the above matrix of coefficients; that is, let $P=\left[p_{i j}\right]$, where $p_{i j}=a_{j i}$. Then $P$ is called the change-of-basis matrix (or transition matrix) from the "old" basis $S$ to the "new" basis $S^{\prime}$.

The following remarks are in order.

Remark 1: The above change-of-basis matrix $P$ may also be viewed as the matrix whose columns are, respectively, the coordinate column vectors of the "new" basis vectors $v_{i}$ relative to the "old" basis $S$; namely,

$$
P=\left[\left[v_{1}\right]_{S},\left[v_{2}\right]_{S}, \ldots,\left[v_{n}\right]_{S}\right]
$$

Remark 2: Analogously, there is a change-of-basis matrix $Q$ from the "new" basis $S$ to the "old' basis $S$. Similarly, $Q$ may be viewed as the matrix whose columns are, respectively, the coordinate column vectors of the "old"' basis vectors $u_{i}$ relative to the "new" basis $S^{\prime}$; namely,

$$
Q=\left[\left[u_{1}\right]_{S^{\prime}},\left[u_{2}\right]_{S^{\prime}}, \ldots,\left[u_{n}\right]_{S^{\prime}}\right]
$$

Remark 3: Because the vectors $v_{1}, v_{2}, \ldots, v_{n}$ in the new basis $S^{\prime}$ are linearly independent, the matrix $P$ is invertible (Problem 6.18). Similarly, $Q$ is invertible. In fact, we have the following proposition (proved in Problem 6.18).

PROPOSITION 6.4: $\quad$ Let $P$ and $Q$ be the above change-of-basis matrices. Then $Q=P^{-1}$.

Now suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis of a vector space $V$, and suppose $P=\left[p_{i j}\right]$ is any nonsingular matrix. Then the $n$ vectors

$$
v_{i}=p_{1 i} u_{i}+p_{2 i} u_{2}+\cdots+p_{n i} u_{n}, \quad i=1,2, \ldots, n
$$

corresponding to the columns of $P$, are linearly independent [Problem 6.21(a)]. Thus, they form another basis $S^{\prime}$ of $V$. Moreover, $P$ will be the change-of-basis matrix from $S$ to the new basis $S^{\prime}$.

EXAMPLE 6.5 Consider the following two bases of $\mathbf{R}^{2}$ :

$$
S=\left\{u_{1}, u_{2}\right\}=\{(1,2),(3,5)\} \quad \text { and } \quad S^{\prime}=\left\{v_{1}, v_{2}\right\}=\{(1,-1),(1,-2)\}
$$

(a) Find the change-of-basis matrix $P$ from $S$ to the "new" basis $S^{\prime}$.

Write each of the new basis vectors of $S^{\prime}$ as a linear combination of the original basis vectors $u_{1}$ and $u_{2}$ of $S$. We have

$$
\begin{aligned}
& {\left[\begin{array}{r}
1 \\
-1
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2
\end{array}\right]+y\left[\begin{array}{l}
3 \\
5
\end{array}\right] \quad \text { or } \quad \begin{aligned}
x+3 y & =1 \\
2 x+5 y & =-1 \quad \text { yielding } \quad x=-8, \quad y=3
\end{aligned}} \\
& {\left[\begin{array}{r}
1 \\
-1
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2
\end{array}\right]+y\left[\begin{array}{l}
3 \\
5
\end{array}\right] \quad \text { or } \quad \begin{aligned}
x+3 y & =1 \\
2 x+5 y & =-1 \quad \text { yielding } \quad x=-11, \quad y=4
\end{aligned}}
\end{aligned}
$$

Thus,

$$
\begin{aligned}
& v_{1}=-8 u_{1}+3 u_{2} \\
& v_{2}=-11 u_{1}+4 u_{2}
\end{aligned} \quad \text { and hence, } \quad P=\left[\begin{array}{rr}
-8 & -11 \\
3 & 4
\end{array}\right]
$$

Note that the coordinates of $v_{1}$ and $v_{2}$ are the columns, not rows, of the change-of-basis matrix $P$.

(b) Find the change-of-basis matrix $Q$ from the "new" basis $S^{\prime}$ back to the "old" basis $S$.

Here we write each of the "old" basis vectors $u_{1}$ and $u_{2}$ of $S^{\prime}$ as a linear combination of the "new" basis vectors $v_{1}$ and $v_{2}$ of $S^{\prime}$. This yields

$$
\begin{aligned}
& u_{1}=4 v_{1}-3 v_{2} \\
& u_{2}=11 v_{1}-8 v_{2}
\end{aligned} \quad \text { and hence, } \quad Q=\left[\begin{array}{rr}
4 & 11 \\
-3 & -8
\end{array}\right]
$$

As expected from Proposition 6.4, $Q=P^{-1}$. (In fact, we could have obtained $Q$ by simply finding $P^{-1}$.)

EXAMPLE 6.6 Consider the following two bases of $\mathbf{R}^{3}$ :

and

$$
\begin{array}{lll}
E=\left\{e_{1}, e_{2}, e_{3}\right\}=\{(1,0,0), & (0,1,0), & (0,0,1)\} \\
S=\left\{u_{1}, u_{2}, u_{3}\right\}=\{(1,0,1), & (2,1,2), & (1,2,2)\}
\end{array}
$$

(a) Find the change-of-basis matrix $P$ from the basis $E$ to the basis $S$.

Because $E$ is the usual basis, we can immediately write each basis element of $S$ as a linear combination of the basis elements of $E$. Specifically,

$$
\begin{aligned}
& u_{1}=(1,0,1)=e_{1}+\quad e_{3} \\
& u_{2}=(2,1,2)=2 e_{1}+e_{2}+2 e_{3} \\
& u_{3}=(1,2,2)=e_{1}+2 e_{2}+2 e_{3}
\end{aligned} \quad \text { and hence, } \quad P=\left[\begin{array}{lll}
1 & 2 & 1 \\
0 & 1 & 2 \\
1 & 2 & 2
\end{array}\right]
$$

Again, the coordinates of $u_{1}, u_{2}, u_{3}$ appear as the columns in $P$. Observe that $P$ is simply the matrix whose columns are the basis vectors of $S$. This is true only because the original basis was the usual basis $E$.

(b) Find the change-of-basis matrix $Q$ from the basis $S$ to the basis $E$.

The definition of the change-of-basis matrix $Q$ tells us to write each of the (usual) basis vectors in $E$ as a linear combination of the basis elements of $S$. This yields

$$
\begin{aligned}
& e_{1}=(1,0,0)=-2 u_{1}+2 u_{2}-u_{3} \\
& e_{2}=(0,1,0)=-2 u_{1}+u_{2} \\
& e_{3}=(0,0,1)=3 u_{1}-2 u_{2}+u_{3}
\end{aligned} \quad \text { and hence, } \quad Q=\left[\begin{array}{rrr}
-2 & -2 & 3 \\
2 & 1 & -2 \\
-1 & 0 & 1
\end{array}\right]
$$

We emphasize that to find $Q$, we need to solve three $3 \times 3$ systems of linear equations-one $3 \times 3$ system for each of $e_{1}, e_{2}, e_{3}$.

Alternatively, we can find $Q=P^{-1}$ by forming the matrix $M=[P, I]$ and row reducing $M$ to row canonical form:

thus,

$$
\begin{gathered}
M=\left[\begin{array}{llllll}
1 & 2 & 1 & 1 & 0 & 0 \\
0 & 1 & 2 & 0 & 1 & 0 \\
1 & 2 & 2 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 0 & 0 & -2 & -2 & 3 \\
0 & 1 & 0 & 2 & 1 & -2 \\
0 & 0 & 1 & -1 & 0 & 1
\end{array}\right]=\left[I, P^{-1}\right] \\
Q=P^{-1}=\left[\begin{array}{rrr}
-2 & -2 & 3 \\
2 & 1 & -2 \\
-1 & 0 & 1
\end{array}\right]
\end{gathered}
$$

(Here we have used the fact that $Q$ is the inverse of $P$.)

The result in Example 6.6(a) is true in general. We state this result formally, because it occurs often.

PROPOSITION 6.5: The change-of-basis matrix from the usual basis $E$ of $K^{n}$ to any basis $S$ of $K^{n}$ is the matrix $P$ whose columns are, respectively, the basis vectors of $S$.

\section*{Applications of Change-of-Basis Matrix}
First we show how a change of basis affects the coordinates of a vector in a vector space $V$. The following theorem is proved in Problem 6.22.

THEOREM 6.6: $\quad$ Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\prime}$ in a vector space $V$. Then, for any vector $v \in V$, we have

$$
P[v]_{S^{\prime}}=[v]_{S} \quad \text { and hence, } \quad P^{-1}[v]_{S}=[v]_{S^{\prime}}
$$

Namely, if we multiply the coordinates of $v$ in the original basis $S$ by $P^{-1}$, we get the coordinates of $v$ in the new basis $S^{\prime}$.

Remark 1: Although $P$ is called the change-of-basis matrix from the old basis $S$ to the new basis $S^{\prime}$, we emphasize that $P^{-1}$ transforms the coordinates of $v$ in the original basis $S$ into the coordinates of $v$ in the new basis $S^{\prime}$.

Remark 2: Because of the above theorem, many texts call $Q=P^{-1}$, not $P$, the transition matrix from the old basis $S$ to the new basis $S^{\prime}$. Some texts also refer to $Q$ as the change-of-coordinates matrix.

We now give the proof of the above theorem for the special case that $\operatorname{dim} V=3$. Suppose $P$ is the change-of-basis matrix from the basis $S=\left\{u_{1}, u_{2}, u_{3}\right\}$ to the basis $S^{\prime}=\left\{v_{1}, v_{2}, v_{3}\right\}$; say,

$$
\begin{aligned}
& v_{1}=a_{1} u_{1}+a_{2} u_{2}+a_{3} a_{3} \\
& v_{2}=b_{1} u_{1}+b_{2} u_{2}+b_{3} u_{3} \\
& v_{3}=c_{1} u_{1}+c_{2} u_{2}+c_{3} u_{3}
\end{aligned} \quad \text { and hence, } \quad P=\left[\begin{array}{lll}
a_{1} & b_{1} & c_{1} \\
a_{2} & b_{2} & c_{2} \\
a_{3} & b_{3} & c_{3}
\end{array}\right]
$$

Now suppose $v \in V$ and, say, $v=k_{1} v_{1}+k_{2} v_{2}+k_{3} v_{3}$. Then, substituting for $v_{1}, v_{2}, v_{3}$ from above, we obtain

$$
\begin{aligned}
v & =k_{1}\left(a_{1} u_{1}+a_{2} u_{2}+a_{3} u_{3}\right)+k_{2}\left(b_{1} u_{1}+b_{2} u_{2}+b_{3} u_{3}\right)+k_{3}\left(c_{1} u_{1}+c_{2} u_{2}+c_{3} u_{3}\right) \\
& =\left(a_{1} k_{1}+b_{1} k_{2}+c_{1} k_{3}\right) u_{1}+\left(a_{2} k_{1}+b_{2} k_{2}+c_{2} k_{3}\right) u_{2}+\left(a_{3} k_{1}+b_{3} k_{2}+c_{3} k_{3}\right) u_{3}
\end{aligned}
$$

Thus,

$$
[v]_{S^{\prime}}=\left[\begin{array}{l}
k_{1} \\
k_{2} \\
k_{3}
\end{array}\right] \quad \text { and } \quad[v]_{S}=\left[\begin{array}{l}
a_{1} k_{1}+b_{1} k_{2}+c_{1} k_{3} \\
a_{2} k_{1}+b_{2} k_{2}+c_{2} k_{3} \\
a_{3} k_{1}+b_{3} k_{2}+c_{3} k_{3}
\end{array}\right]
$$

Accordingly,

$$
P[v]_{S^{\prime}}=\left[\begin{array}{lll}
a_{1} & b_{1} & c_{1} \\
a_{2} & b_{2} & c_{2} \\
a_{3} & b_{3} & c_{3}
\end{array}\right]\left[\begin{array}{l}
k_{1} \\
k_{2} \\
k_{3}
\end{array}\right]=\left[\begin{array}{l}
a_{1} k_{1}+b_{1} k_{2}+c_{1} k_{3} \\
a_{2} k_{1}+b_{2} k_{2}+c_{2} k_{3} \\
a_{3} k_{1}+b_{3} k_{2}+c_{3} k_{3}
\end{array}\right]=[v]_{S}
$$

Finally, multiplying the equation $[v]_{S}=P[v]_{S}$, by $P^{-1}$, we get

$$
P^{-1}[v]_{S}=P^{-1} P[v]_{S^{\prime}}=I[v]_{S^{\prime}}=[v]_{S^{\prime}}
$$

The next theorem (proved in Problem 6.26) shows how a change of basis affects the matrix representation of a linear operator.

THEOREM 6.7: $\quad$ Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\prime}$ in a vector space $V$. Then, for any linear operator $T$ on $V$,

$$
[T]_{S^{\prime}}=P^{-1}[T]_{S} P
$$

That is, if $A$ and $B$ are the matrix representations of $T$ relative, respectively, to $S$ and $S^{\prime}$, then

$$
B=P^{-1} A P
$$

EXAMPLE 6.7 Consider the following two bases of $\mathbf{R}^{3}$ :

$$
\begin{aligned}
E & =\left\{e_{1}, e_{2}, e_{3}\right\}=\{(1,0,0), \quad(0,1,0), & (0,0,1)\} \\
\text { and } & S & =\left\{u_{1}, u_{2}, u_{3}\right\}=\{(1,0,1), \quad(2,1,2), \quad(1,2,2)\}
\end{aligned}
$$

The change-of-basis matrix $P$ from $E$ to $S$ and its inverse $P^{-1}$ were obtained in Example 6.6.

(a) Write $v=(1,3,5)$ as a linear combination of $u_{1}, u_{2}, u_{3}$, or, equivalently, find $[v]_{S}$.

One way to do this is to directly solve the vector equation $v=x u_{1}+y u_{2}+z u_{3}$; that is,

$$
\left[\begin{array}{l}
1 \\
3 \\
5
\end{array}\right]=x\left[\begin{array}{l}
1 \\
0 \\
1
\end{array}\right]+y\left[\begin{array}{l}
2 \\
1 \\
2
\end{array}\right]+z\left[\begin{array}{l}
1 \\
2 \\
2
\end{array}\right] \quad \text { or } \quad \begin{array}{r}
x+2 y+z=1 \\
y+2 z=3 \\
x+2 y+2 z=5
\end{array}
$$

The solution is $x=7, \quad y=-5, \quad z=4, \quad$ so $v=7 u_{1}-5 u_{2}+4 u_{3}$.

On the other hand, we know that $[v]_{E}=[1,3,5]^{T}$, because $E$ is the usual basis, and we already know $P^{-1}$. Therefore, by Theorem 6.6,

$$
[v]_{S}=P^{-1}[v]_{E}=\left[\begin{array}{rrr}
-2 & -2 & 3 \\
2 & 1 & -2 \\
-1 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
1 \\
3 \\
5
\end{array}\right]=\left[\begin{array}{r}
7 \\
-5 \\
4
\end{array}\right]
$$

Thus, again, $v=7 u_{1}-5 u_{2}+4 u_{3}$.

(b) Let $A=\left[\begin{array}{rrr}1 & 3 & -2 \\ 2 & -4 & 1 \\ 3 & -1 & 2\end{array}\right]$, which may be viewed as a linear operator on $\mathbf{R}^{3}$. Find the matrix $B$ that represents $A$ relative to the basis $S$.

The definition of the matrix representation of $A$ relative to the basis $S$ tells us to write each of $A\left(u_{1}\right), A\left(u_{2}\right)$, $A\left(u_{3}\right)$ as a linear combination of the basis vectors $u_{1}, u_{2}, u_{3}$ of $S$. This yields

$$
\begin{aligned}
& A\left(u_{1}\right)=(-1,3,5)=11 u_{1}-5 u_{2}+6 u_{3} \\
& A\left(u_{2}\right)=(1,2,9)=21 u_{1}-14 u_{2}+8 u_{3} \\
& A\left(u_{3}\right)=(3,-4,5)=17 u_{1}-8 e_{2}+2 u_{3}
\end{aligned} \quad \text { and hence, } \quad B=\left[\begin{array}{rrr}
11 & 21 & 17 \\
-5 & -14 & -8 \\
6 & 8 & 2
\end{array}\right]
$$

We emphasize that to find $B$, we need to solve three $3 \times 3$ systems of linear equations-one $3 \times 3$ system for each of $A\left(u_{1}\right), A\left(u_{2}\right), A\left(u_{3}\right)$.

On the other hand, because we know $P$ and $P^{-1}$, we can use Theorem 6.7. That is,

$$
B=P^{-1} A P=\left[\begin{array}{rrr}
-2 & -2 & 3 \\
2 & 1 & -2 \\
-1 & 0 & 1
\end{array}\right]\left[\begin{array}{rrr}
1 & 3 & -2 \\
2 & -4 & 1 \\
3 & -1 & 2
\end{array}\right]\left[\begin{array}{lll}
1 & 2 & 1 \\
0 & 1 & 2 \\
1 & 2 & 2
\end{array}\right]=\left[\begin{array}{rrr}
11 & 21 & 17 \\
-5 & -14 & -8 \\
6 & 8 & 2
\end{array}\right]
$$

This, as expected, gives the same result.

\subsection*{6.4 Similarity}
Suppose $A$ and $B$ are square matrices for which there exists an invertible matrix $P$ such that $B=P^{-1} A P$; then $B$ is said to be similar to $A$, or $B$ is said to be obtained from $A$ by a similarity transformation. We show (Problem 6.29) that similarity of matrices is an equivalence relation.

By Theorem 6.7 and the above remark, we have the following basic result.

THEOREM 6.8: Two matrices represent the same linear operator if and only if the matrices are similar.

That is, all the matrix representations of a linear operator $T$ form an equivalence class of similar matrices.

A linear operator $T$ is said to be diagonalizable if there exists a basis $S$ of $V$ such that $T$ is represented by a diagonal matrix; the basis $S$ is then said to diagonalize $T$. The preceding theorem gives us the following result.

THEOREM 6.9: Let $A$ be the matrix representation of a linear operator $T$. Then $T$ is diagonalizable if and only if there exists an invertible matrix $P$ such that $P^{-1} A P$ is a diagonal matrix.

That is, $T$ is diagonalizable if and only if its matrix representation can be diagonalized by a similarity transformation.

We emphasize that not every operator is diagonalizable. However, we will show (Chapter 10) that every linear operator can be represented by certain "standard" matrices called its normal or canonical forms. Such a discussion will require some theory of fields, polynomials, and determinants.

\section*{Functions and Similar Matrices}
Suppose $f$ is a function on square matrices that assigns the same value to similar matrices; that is, $f(A)=f(B)$ whenever $A$ is similar to $B$. Then $f$ induces a function, also denoted by $f$, on linear operators $T$ in the following natural way. We define

$$
f(T)=f\left([T]_{S}\right)
$$

where $S$ is any basis. By Theorem 6.8 , the function is well defined.

The determinant (Chapter 8) is perhaps the most important example of such a function. The trace (Section 2.7) is another important example of such a function.

EXAMPLE 6.8 Consider the following linear operator $F$ and bases $E$ and $S$ of $\mathbf{R}^{2}$ :

$$
F(x, y)=(2 x+3 y, 4 x-5 y), \quad E=\{(1,0),(0,1)\}, \quad S=\{(1,2),(2,5)\}
$$

By Example 6.1, the matrix representations of $F$ relative to the bases $E$ and $S$ are, respectively,

$$
A=\left[\begin{array}{rr}
2 & 3 \\
4 & -5
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{rr}
52 & 129 \\
-22 & -55
\end{array}\right]
$$

Using matrix $A$, we have

(i) Determinant of $F=\operatorname{det}(A)=-10-12=-22$;

(ii) Trace of $F=\operatorname{tr}(A)=2-5=-3$.

On the other hand, using matrix $B$, we have

(i) Determinant of $F=\operatorname{det}(B)=-2860+2838=-22$;

(ii) Trace of $F=\operatorname{tr}(B)=52-55=-3$.

As expected, both matrices yield the same result.

\subsection*{6.5 Matrices and General Linear Mappings}
Last, we consider the general case of linear mappings from one vector space into another. Suppose $V$ and $U$ are vector spaces over the same field $K$ and, say, $\operatorname{dim} V=m$ and $\operatorname{dim} U=n$. Furthermore, suppose

$$
S=\left\{v_{1}, v_{2}, \ldots, v_{m}\right\} \quad \text { and } \quad S^{\prime}=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}
$$

are arbitrary but fixed bases, respectively, of $V$ and $U$.

Suppose $F: V \rightarrow U$ is a linear mapping. Then the vectors $F\left(v_{1}\right), F\left(v_{2}\right), \ldots, F\left(v_{m}\right)$ belong to $U$, and so each is a linear combination of the basis vectors in $S^{\prime}$; say,

$$
\begin{gathered}
F\left(v_{1}\right)=a_{11} u_{1}+a_{12} u_{2}+\cdots+a_{1 n} u_{n} \\
F\left(v_{2}\right)=a_{21} u_{1}+a_{22} u_{2}+\cdots+a_{2 n} u_{n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
F\left(v_{m}\right)=a_{m 1} u_{1}+a_{m 2} u_{2}+\cdots+a_{m n} u_{n}
\end{gathered}
$$

DEFINITION: The transpose of the above matrix of coefficients, denoted by $m_{S, S^{\prime}}(F)$ or $[F]_{S, S^{\prime}}$, is called the matrix representation of $F$ relative to the bases $S$ and $S^{\prime}$. [We will use the simple notation $m(F)$ and $[F]$ when the bases are understood.]

The following theorem is analogous to Theorem 6.1 for linear operators (Problem 6.67).

THEOREM 6.10: For any vector $v \in V,[F]_{S, S^{\prime}}[v]_{S}=[F(v)]_{S^{\prime}}$.

That is, multiplying the coordinates of $v$ in the basis $S$ of $V$ by $[F]$, we obtain the coordinates of $F(v)$ in the basis $S^{\prime}$ of $U$.

Recall that for any vector spaces $V$ and $U$, the collection of all linear mappings from $V$ into $U$ is a vector space and is denoted by $\operatorname{Hom}(V, U)$. The following theorem is analogous to Theorem 6.2 for linear operators, where now we let $\mathbf{M}=\mathbf{M}_{m, n}$ denote the vector space of all $m \times n$ matrices (Problem 6.67).

THEOREM 6.11: The mapping $m: \operatorname{Hom}(V, U) \rightarrow \mathbf{M}$ defined by $m(F)=[F]$ is a vector space isomorphism. That is, for any $F, G \in \operatorname{Hom}(V, U)$ and any scalar $k$,

(i) $m(F+G)=m(F)+m(G)$ or $[F+G]=[F]+[G]$

(ii) $m(k F)=k m(F)$ or $[k F]=k[F]$

(iii) $m$ is bijective (one-to-one and onto).

Our next theorem is analogous to Theorem 6.3 for linear operators (Problem 6.67).

THEOREM 6.12: Let $S, S^{\prime}, S^{\prime \prime}$ be bases of vector spaces $V, U, W$, respectively. Let $F: V \rightarrow U$ and $G \circ U \rightarrow W$ be linear mappings. Then

$$
[G \circ F]_{S, S^{\prime \prime}}=[G]_{S^{\prime}, S^{\prime \prime}}[F]_{S, S^{\prime}}
$$

That is, relative to the appropriate bases, the matrix representation of the composition of two mappings is the matrix product of the matrix representations of the individual mappings.

Next we show how the matrix representation of a linear mapping $F: V \rightarrow U$ is affected when new bases are selected (Problem 6.67).

THEOREM 6.13: $\quad$ Let $P$ be the change-of-basis matrix from a basis $e$ to a basis $e^{\prime}$ in $V$, and let $Q$ be the change-of-basis matrix from a basis $f$ to a basis $f^{\prime}$ in $U$. Then, for any linear map $F: V \rightarrow U$,

$$
[F]_{e^{\prime}, f^{\prime}}=Q^{-1}[F]_{e, f} P
$$

In other words, if $A$ is the matrix representation of a linear mapping $F$ relative to the bases $e$ and $f$, and $B$ is the matrix representation of $F$ relative to the bases $e^{\prime}$ and $f^{\prime}$, then

$$
B=Q^{-1} A P
$$

Our last theorem, proved in Problem 6.36, shows that any linear mapping from one vector space $V$ into another vector space $U$ can be represented by a very simple matrix. We note that this theorem is analogous to Theorem 3.18 for $m \times n$ matrices.

THEOREM 6.14: $\quad$ Let $F: V \rightarrow U$ be linear and, $\operatorname{say}, \operatorname{rank}(F)=r$. Then there exist bases of $V$ and $U$ such that the matrix representation of $F$ has the form

$$
A=\left[\begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array}\right]
$$

where $I_{r}$ is the $r$-square identity matrix.

The above matrix $A$ is called the normal or canonical form of the linear map $F$.

\section*{SOLVED PROBLEMS}
\section*{Matrix Representation of Linear Operators}
6.1. Consider the linear mapping $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(3 x+4 y, \quad 2 x-5 y)$ and the following bases of $\mathbf{R}^{2}$ :

$$
E=\left\{e_{1}, e_{2}\right\}=\{(1,0),(0,1)\} \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,2),(2,3)\}
$$

(a) Find the matrix $A$ representing $F$ relative to the basis $E$.

(b) Find the matrix $B$ representing $F$ relative to the basis $S$.

(a) Because $E$ is the usual basis, the rows of $A$ are simply the coefficients in the components of $F(x, y)$; that is, using $(a, b)=a e_{1}+b e_{2}$, we have

$$
\begin{aligned}
& F\left(e_{1}\right)=F(1,0)=(3,2)=3 e_{1}+2 e_{2} \\
& F\left(e_{2}\right)=F(0,1)=(4,-5)=4 e_{1}-5 e_{2}
\end{aligned} \quad \text { and so } \quad A=\left[\begin{array}{rr}
3 & 4 \\
2 & -5
\end{array}\right]
$$

Note that the coefficients of the basis vectors are written as columns in the matrix representation.\\
(b) First find $F\left(u_{1}\right)$ and write it as a linear combination of the basis vectors $u_{1}$ and $u_{2}$. We have

$$
F\left(u_{1}\right)=F(1,2)=(11,-8)=x(1,2)+y(2,3), \quad \text { and so } \quad \begin{aligned}
x+2 y & =11 \\
2 x+3 y & =-8
\end{aligned}
$$

Solve the system to obtain $x=-49, y=30$. Therefore,

$$
F\left(u_{1}\right)=-49 u_{1}+30 u_{2}
$$

Next find $F\left(u_{2}\right)$ and write it as a linear combination of the basis vectors $u_{1}$ and $u_{2}$. We have

$$
F\left(u_{2}\right)=F(2,3)=(18,-11)=x(1,2)+y(2,3), \quad \text { and so } \quad \begin{aligned}
x+2 y & =18 \\
2 x+3 y & =-11
\end{aligned}
$$

Solve for $x$ and $y$ to obtain $x=-76, y=47$. Hence,

$$
F\left(u_{2}\right)=-76 u_{1}+47 u_{2}
$$

Write the coefficients of $u_{1}$ and $u_{2}$ as columns to obtain $B=\left[\begin{array}{rr}-49 & -76 \\ 30 & 47\end{array}\right]$

(b') Alternatively, one can first find the coordinates of an arbitrary vector $(a, b)$ in $\mathbf{R}^{2}$ relative to the basis $S$. We have

$$
(a, b)=x(1,2)+y(2,3)=(x+2 y, 2 x+3 y), \quad \text { and so } \quad \begin{aligned}
x+2 y & =a \\
2 x+3 y & =b
\end{aligned}
$$

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-3 a+2 b, y=2 a-b$. Thus,

$$
(a, b)=(-3 a+2 b) u_{1}+(2 a-b) u_{2}
$$

Then use the formula for $(a, b)$ to find the coordinates of $F\left(u_{1}\right)$ and $F\left(u_{2}\right)$ relative to $S$ :

$$
\begin{aligned}
& F\left(u_{1}\right)=F(1,2)=(11,-8)=-49 u_{1}+30 u_{2} \\
& F\left(u_{2}\right)=F(2,3)=(18,-11)=-76 u_{1}+47 u_{2}
\end{aligned} \quad \text { and so } \quad B=\left[\begin{array}{rr}
-49 & -76 \\
30 & 47
\end{array}\right]
$$

6.2. Consider the following linear operator $G$ on $\mathbf{R}^{2}$ and basis $S$ :

$$
G(x, y)=(2 x-7 y, 4 x+3 y) \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,3),(2,5)\}
$$

(a) Find the matrix representation $[G]_{S}$ of $G$ relative to $S$.

(b) Verify $[G]_{S}[v]_{S}=[G(v)]_{S}$ for the vector $v=(4,-3)$ in $\mathbf{R}^{2}$. have

First find the coordinates of an arbitrary vector $v=(a, b)$ in $\mathbf{R}^{2}$ relative to the basis $S$. We

$$
\left[\begin{array}{l}
a \\
b
\end{array}\right]=x\left[\begin{array}{l}
1 \\
3
\end{array}\right]+y\left[\begin{array}{l}
2 \\
5
\end{array}\right], \quad \text { and so } \quad \begin{array}{r}
x+2 y=a \\
3 x+5 y=b
\end{array}
$$

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-5 a+2 b, y=3 a-b$. Thus,

$$
(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}, \quad \text { and so } \quad[v]=\left[\begin{array}{ll}
-5 a+2 b, & 3 a-b
\end{array}\right]^{T}
$$

(a) Using the formula for $(a, b)$ and $G(x, y)=(2 x-7 y, 4 x+3 y)$, we have

$$
\begin{aligned}
& G\left(u_{1}\right)=G(1,3)=(-19,13)=121 u_{1}-70 u_{2} \\
& G\left(u_{2}\right)=G(2,5)=(-31,23)=201 u_{1}-116 u_{2}
\end{aligned} \quad \text { and so } \quad[G]_{S}=\left[\begin{array}{rr}
121 & 201 \\
-70 & -116
\end{array}\right]
$$

(We emphasize that the coefficients of $u_{1}$ and $u_{2}$ are written as columns, not rows, in the matrix representation.)

(b) Use the formula $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$ to get

$$
\begin{aligned}
v & =(4,-3)=-26 u_{1}+15 u_{2} \\
G(v) & =G(4,-3)=(20,7)=-131 u_{1}+80 u_{2}
\end{aligned}
$$

Then

$$
[v]_{S}=[-26,15]^{T} \quad \text { and } \quad[G(v)]_{S}=[-131,80]^{T}
$$

Accordingly,

$$
[G]_{S}[v]_{S}=\left[\begin{array}{rr}
121 & 201 \\
-70 & -116
\end{array}\right]\left[\begin{array}{r}
-26 \\
15
\end{array}\right]=\left[\begin{array}{r}
-131 \\
80
\end{array}\right]=[G(v)]_{S}
$$

(This is expected from Theorem 6.1.)

6.3. Consider the following $2 \times 2$ matrix $A$ and basis $S$ of $\mathbf{R}^{2}$ :

$$
A=\left[\begin{array}{ll}
2 & 4 \\
5 & 6
\end{array}\right] \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\left\{\left[\begin{array}{r}
1 \\
-2
\end{array}\right], \quad\left[\begin{array}{r}
3 \\
-7
\end{array}\right]\right\}
$$

The matrix $A$ defines a linear operator on $\mathbf{R}^{2}$. Find the matrix $B$ that represents the mapping $A$ relative to the basis $S$.

First find the coordinates of an arbitrary vector $(a, b)^{T}$ with respect to the basis $S$. We have

$$
\left[\begin{array}{l}
a \\
b
\end{array}\right]=x\left[\begin{array}{r}
1 \\
-2
\end{array}\right]+y\left[\begin{array}{r}
3 \\
-7
\end{array}\right] \quad \text { or } \quad \begin{aligned}
x+3 y & =a \\
-2 x-7 y & =b
\end{aligned}
$$

Solve for $x$ and $y$ in terms of $a$ and $b$ to obtain $x=7 a+3 b, y=-2 a-b$. Thus,

$$
(a, b)^{T}=(7 a+3 b) u_{1}+(-2 a-b) u_{2}
$$

Then use the formula for $(a, b)^{T}$ to find the coordinates of $A u_{1}$ and $A u_{2}$ relative to the basis $S$ :

$$
\begin{aligned}
& A u_{1}=\left[\begin{array}{ll}
2 & 4 \\
5 & 6
\end{array}\right]\left[\begin{array}{r}
1 \\
-2
\end{array}\right]=\left[\begin{array}{l}
-6 \\
-7
\end{array}\right]=-63 u_{1}+19 u_{2} \\
& A u_{2}=\left[\begin{array}{ll}
2 & 4 \\
5 & 6
\end{array}\right]\left[\begin{array}{r}
3 \\
-7
\end{array}\right]=\left[\begin{array}{l}
-22 \\
-27
\end{array}\right]=-235 u_{1}+71 u_{2}
\end{aligned}
$$

Writing the coordinates as columns yields

$$
B=\left[\begin{array}{rr}
-63 & -235 \\
19 & 71
\end{array}\right]
$$

6.4. Find the matrix representation of each of the following linear operators $F$ on $\mathbf{R}^{3}$ relative to the usual basis $E=\left\{e_{1}, e_{2}, e_{3}\right\}$ of $\mathbf{R}^{3}$; that is, find $[F]=[F]_{E}$ :

(a) $F$ defined by $F(x, y, z)=(x+2 y-3 z, 4 x-5 y-6 z, 7 x+8 y+9 z)$.

(b) $F$ defined by the $3 \times 3$ matrix $A=\left[\begin{array}{lll}1 & 1 & 1 \\ 2 & 3 & 4 \\ 5 & 5 & 5\end{array}\right]$.

(c) $F$ defined by $F\left(e_{1}\right)=(1,3,5), F\left(e_{2}\right)=(2,4,6), F\left(e_{3}\right)=(7,7,7)$. (Theorem 5.2 states that a linear map is completely defined by its action on the vectors in a basis.)

(a) Because $E$ is the usual basis, simply write the coefficients of the components of $F(x, y, z)$ as rows:

$$
[F]=\left[\begin{array}{rrr}
1 & 2 & -3 \\
4 & -5 & -6 \\
7 & 8 & 9
\end{array}\right]
$$

(b) Because $E$ is the usual basis, $[F]=A$, the matrix $A$ itself.

(c) Here

$$
\begin{aligned}
& F\left(e_{1}\right)=(1,3,5)=e_{1}+3 e_{2}+5 e_{3} \\
& F\left(e_{2}\right)=(2,4,6)=2 e_{1}+4 e_{2}+6 e_{3} \\
& F\left(e_{3}\right)=(7,7,7)=7 e_{1}+7 e_{2}+7 e_{3}
\end{aligned} \quad \text { and so } \quad[F]=\left[\begin{array}{lll}
1 & 2 & 7 \\
3 & 4 & 7 \\
5 & 6 & 7
\end{array}\right]
$$

That is, the columns of $[F]$ are the images of the usual basis vectors.

6.5. Let $G$ be the linear operator on $\mathbf{R}^{3}$ defined by $G(x, y, z)=(2 y+z, x-4 y, 3 x)$.

(a) Find the matrix representation of $G$ relative to the basis

$$
S=\left\{w_{1}, w_{2}, w_{3}\right\}=\{(1,1,1), \quad(1,1,0), \quad(1,0,0)\}
$$

(b) Verify that $[G][v]=[G(v)]$ for any vector $v$ in $\mathbf{R}^{3}$.

First find the coordinates of an arbitrary vector $(a, b, c) \in \mathbf{R}^{3}$ with respect to the basis $S$. Write $(a, b, c)$ as a linear combination of $w_{1}, w_{2}, w_{3}$ using unknown scalars $x, y$, and $z$ :

$$
(a, b, c)=x(1,1,1)+y(1,1,0)+z(1,0,0)=(x+y+z, x+y, x)
$$

Set corresponding components equal to each other to obtain the system of equations

$$
x+y+z=a, \quad x+y=b, \quad x=c
$$

Solve the system for $x, y, z$ in terms of $a, b, c$ to find $x=c, y=b-c, z=a-b$. Thus,

$(a, b, c)=c w_{1}+(b-c) w_{2}+(a-b) w_{3}, \quad$ or equivalently, $\quad[(a, b, c)]=[c, b-c, a-b]^{T}$

(a) Because $G(x, y, z)=(2 y+z, x-4 y, 3 x)$,

$$
\begin{aligned}
& G\left(w_{1}\right)=G(1,1,1)=(3,-3,3)=3 w_{1}-6 x_{2}+6 x_{3} \\
& G\left(w_{2}\right)=G(1,1,0)=(2,-3,3)=3 w_{1}-6 w_{2}+5 w_{3} \\
& G\left(w_{3}\right)=G(1,0,0)=(0,1,3)=3 w_{1}-2 w_{2}-w_{3}
\end{aligned}
$$

Write the coordinates $G\left(w_{1}\right), G\left(w_{2}\right), G\left(w_{3}\right)$ as columns to get

$$
[G]=\left[\begin{array}{rrr}
3 & 3 & 3 \\
-6 & -6 & -2 \\
6 & 5 & -1
\end{array}\right]
$$

(b) Write $G(v)$ as a linear combination of $w_{1}, w_{2}, w_{3}$, where $v=(a, b, c)$ is an arbitrary vector in $\mathbf{R}^{3}$,

$$
G(v)=G(a, b, c)=(2 b+c, a-4 b, 3 a)=3 a w_{1}+(-2 a-4 b) w_{2}+(-a+6 b+c) w_{3}
$$

or equivalently,

Accordingly,

$$
[G(v)]=[3 a,-2 a-4 b,-a+6 b+c]^{T}
$$

$$
[G][v]=\left[\begin{array}{rrr}
3 & 3 & 3 \\
-6 & -6 & -2 \\
6 & 5 & -1
\end{array}\right]\left[\begin{array}{c}
c \\
b-c \\
a-b
\end{array}\right]=\left[\begin{array}{c}
3 a \\
-2 a-4 b \\
-a+6 b+c
\end{array}\right]=[G(v)]
$$

6.6. Consider the following $3 \times 3$ matrix $A$ and basis $S$ of $\mathbf{R}^{3}$ :

$$
A=\left[\begin{array}{rrr}
1 & -2 & 1 \\
3 & -1 & 0 \\
1 & 4 & -2
\end{array}\right] \quad \text { and } \quad S=\left\{u_{1}, u_{2}, u_{3}\right\}=\left\{\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right], \quad\left[\begin{array}{l}
0 \\
1 \\
1
\end{array}\right], \quad\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]\right\}
$$

The matrix $A$ defines a linear operator on $\mathbf{R}^{3}$. Find the matrix $B$ that represents the mapping $A$ relative to the basis $S$. (Recall that $A$ represents itself relative to the usual basis of $\mathbf{R}^{3}$.)

First find the coordinates of an arbitrary vector $(a, b, c)$ in $\mathbf{R}^{3}$ with respect to the basis $S$. We have

$$
\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]=x\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]+y\left[\begin{array}{l}
0 \\
1 \\
1
\end{array}\right]+z\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right] \quad \text { or } \quad \begin{aligned}
& x+z=a \\
& x+y+2 z=b \\
& x+y+3 z=c
\end{aligned}
$$

Solve for $x, y, z$ in terms of $a, b, c$ to get

$$
\begin{aligned}
x & =a+b-c, \quad y=-a+2 b-c, \quad z=c-b \\
\text { thus, } \quad(a, b, c)^{T} & =(a+b-c) u_{1}+(-a+2 b-c) u_{2}+(c-b) u_{3}
\end{aligned}
$$

Then use the formula for $(a, b, c)^{T}$ to find the coordinates of $A u_{1}, A u_{2}, A u_{3}$ relative to the basis $S$ :

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-216(1)}
\end{center}

6.7. For each of the following linear transformations (operators) $L$ on $\mathbf{R}^{2}$, find the matrix $A$ that represents $L$ (relative to the usual basis of $\mathbf{R}^{2}$ ):

(a) $L$ is defined by $L(1,0)=(2,4)$ and $L(0,1)=(5,8)$.

(b) $L$ is the rotation in $\mathbf{R}^{2}$ counterclockwise by $90^{\circ}$.

(c) $L$ is the reflection in $\mathbf{R}^{2}$ about the line $y=-x$.

(a) Because $\{(1,0),(0,1)\}$ is the usual basis of $\mathbf{R}^{2}$, write their images under $L$ as columns to get

$$
A=\left[\begin{array}{ll}
2 & 5 \\
4 & 8
\end{array}\right]
$$

(b) Under the rotation $L$, we have $L(1,0)=(0,1)$ and $L(0,1)=(-1,0)$. Thus,

$$
A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right]
$$

(c) Under the reflection $L$, we have $L(1,0)=(0,-1)$ and $L(0,1)=(-1,0)$. Thus,

$$
A=\left[\begin{array}{rr}
0 & -1 \\
-1 & 0
\end{array}\right]
$$

6.8. The set $S=\left\{e^{3 t}, t e^{3 t}, t^{2} e^{3 t}\right\}$ is a basis of a vector space $V$ of functions $f: \mathbf{R} \rightarrow \mathbf{R}$. Let $\mathbf{D}$ be the differential operator on $V$; that is, $\mathbf{D}(f)=d f / d t$. Find the matrix representation of $\mathbf{D}$ relative to the basis $S$.

Find the image of each basis function:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-216}
\end{center}

6.9. Prove Theorem 6.1: Let $T: V \rightarrow V$ be a linear operator, and let $S$ be a (finite) basis of $V$. Then, for any vector $v$ in $V,[T]_{S}[v]_{S}=[T(v)]_{S}$.

Suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$, and suppose, for $i=1, \ldots, n$,

$$
T\left(u_{i}\right)=a_{i 1} u_{1}+a_{i 2} u_{2}+\cdots+a_{i n} u_{n}=\sum_{j=1}^{n} a_{i j} u_{j}
$$

Then $[T]_{S}$ is the $n$-square matrix whose $j$ th row is


\begin{equation*}
\left(a_{1 j}, a_{2 j}, \ldots, a_{n j}\right) \tag{1}
\end{equation*}


Now suppose

$$
v=k_{1} u_{1}+k_{2} u_{2}+\cdots+k_{n} u_{n}=\sum_{i=1}^{n} k_{i} u_{i}
$$

Writing a column vector as the transpose of a row vector, we have


\begin{equation*}
[v]_{S}=\left[k_{1}, k_{2}, \ldots, k_{n}\right]^{T} \tag{2}
\end{equation*}


Furthermore, using the linearity of $T$,

$$
\begin{aligned}
T(v) & =T\left(\sum_{i=1}^{n} k_{i} u_{i}\right)=\sum_{i=1}^{n} k_{i} T\left(u_{i}\right)=\sum_{i=1}^{n} k_{i}\left(\sum_{j=1}^{n} a_{i j} u_{j}\right) \\
& =\sum_{j=1}^{n}\left(\sum_{i=1}^{n} a_{i j} k_{i}\right) u_{j}=\sum_{j=1}^{n}\left(a_{1 j} k_{1}+a_{2 j} k_{2}+\cdots+a_{n j} k_{n}\right) u_{j}
\end{aligned}
$$

Thus, $[T(v)]_{S}$ is the column vector whose $j$ th entry is


\begin{equation*}
a_{1 j} k_{1}+a_{2 j} k_{2}+\cdots+a_{n j} k_{n} \tag{3}
\end{equation*}


On the other hand, the $j$ th entry of $[T]_{S}[v]_{S}$ is obtained by multiplying the $j$ th row of $[T]_{S}$ by $[v]_{S}$ - that is (1) by (2). But the product of (1) and (2) is (3). Hence, $[T]_{S}[v]_{S}$ and $[T(v)]_{S}$ have the same entries. Thus, $[T]_{S}[v]_{S}=[T(v)]_{S}$.

6.10. Prove Theorem 6.2: Let $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ be a basis for $V$ over $K$, and let $\mathbf{M}$ be the algebra of $n$-square matrices over $K$. Then the mapping $m: A(V) \rightarrow \mathbf{M}$ defined by $m(T)=[T]_{S}$ is a vector space isomorphism. That is, for any $F, G \in A(V)$ and any $k \in K$, we have\\
(i) $[F+G]=[F]+[G]$,\\
(ii) $[k F]=k[F]$,\\
(iii) $m$ is one-to-one and onto.

(i) Suppose, for $i=1, \ldots, n$,

$$
F\left(u_{i}\right)=\sum_{j=1}^{n} a_{i j} u_{j} \quad \text { and } \quad G\left(u_{i}\right)=\sum_{j=1}^{n} b_{i j} u_{j}
$$

Consider the matrices $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$. Then $[F]=A^{T}$ and $[G]=B^{T}$. We have, for $i=1, \ldots, n$,

$$
(F+G)\left(u_{i}\right)=F\left(u_{i}\right)+G\left(u_{i}\right)=\sum_{j=1}^{n}\left(a_{i j}+b_{i j}\right) u_{j}
$$

Because $A+B$ is the matrix $\left(a_{i j}+b_{i j}\right)$, we have

(ii) Also, for $i=1, \ldots, n$,

$$
[F+G]=(A+B)^{T}=A^{T}+B^{T}=[F]+[G]
$$

$$
(k F)\left(u_{i}\right)=k F\left(u_{i}\right)=k \sum_{j=1}^{n} a_{i j} u_{j}=\sum_{j=1}^{n}\left(k a_{i j}\right) u_{j}
$$

Because $k A$ is the matrix $\left(k a_{i j}\right)$, we have

$$
[k F]=(k A)^{T}=k A^{T}=k[F]
$$

(iii) Finally, $m$ is one-to-one, because a linear mapping is completely determined by its values on a basis. Also, $m$ is onto, because matrix $A=\left[a_{i j}\right]$ in $\mathbf{M}$ is the image of the linear operator,

$$
F\left(u_{i}\right)=\sum_{j=1}^{n} a_{i j} u_{j}, \quad i=1, \ldots, n
$$

Thus, the theorem is proved.

6.11. Prove Theorem 6.3: For any linear operators $G, F \in A(V),[G \circ F]=[G][F]$.

Using the notation in Problem 6.10, we have

$$
\begin{aligned}
(G \circ F)\left(u_{i}\right) & =G\left(F\left(u_{i}\right)\right)=G\left(\sum_{j=1}^{n} a_{i j} u_{j}\right)=\sum_{j=1}^{n} a_{i j} G\left(u_{j}\right) \\
& =\sum_{j=1}^{n} a_{i j}\left(\sum_{k=1}^{n} b_{j k} u_{k}\right)=\sum_{k=1}^{n}\left(\sum_{j=1}^{n} a_{i j} b_{j k}\right) u_{k}
\end{aligned}
$$

Recall that $A B$ is the matrix $A B=\left[c_{i k}\right]$, where $c_{i k}=\sum_{j=1}^{n} a_{i j} b_{j k}$. Accordingly,

$$
[G \circ F]=(A B)^{T}=B^{T} A^{T}=[G][F]
$$

The theorem is proved.

6.12. Let $A$ be the matrix representation of a linear operator $T$. Prove that, for any polynomial $f(t)$, we have that $f(A)$ is the matrix representation of $f(T)$. [Thus, $f(T)=0$ if and only if $f(A)=0$.]

Let $\phi$ be the mapping that sends an operator $T$ into its matrix representation $A$. We need to prove that $\phi(f(T))=f(A)$. Suppose $f(t)=a_{n} t^{n}+\cdots+a_{1} t+a_{0}$. The proof is by induction on $n$, the degree of $f(t)$.

Suppose $n=0$. Recall that $\phi\left(I^{\prime}\right)=I$, where $I^{\prime}$ is the identity mapping and $I$ is the identity matrix. Thus,

$$
\phi(f(T))=\phi\left(a_{0} I^{\prime}\right)=a_{0} \phi\left(I^{\prime}\right)=a_{0} I=f(A)
$$

and so the theorem holds for $n=0$.

Now assume the theorem holds for polynomials of degree less than $n$. Then, because $\phi$ is an algebra isomorphism,

$$
\begin{aligned}
\phi(f(T)) & =\phi\left(a_{n} T^{n}+a_{n-1} T^{n-1}+\cdots+a_{1} T+a_{0} I^{\prime}\right) \\
& =a_{n} \phi(T) \phi\left(T^{n-1}\right)+\phi\left(a_{n-1} T^{n-1}+\cdots+a_{1} T+a_{0} I^{\prime}\right) \\
& =a_{n} A A^{n-1}+\left(a_{n-1} A^{n-1}+\cdots+a_{1} A+a_{0} I\right)=f(A)
\end{aligned}
$$

and the theorem is proved.

\section*{Change of Basis}
The coordinate vector $[v]_{S}$ in this section will always denote a column vector; that is,

$$
[v]_{S}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]^{T}
$$

6.13. Consider the following bases of $\mathbf{R}^{2}$ :

$$
E=\left\{e_{1}, e_{2}\right\}=\{(1,0),(0,1)\} \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,3),(1,4)\}
$$

(a) Find the change-of-basis matrix $P$ from the usual basis $E$ to $S$.

(b) Find the change-of-basis matrix $Q$ from $S$ back to $E$.

(c) Find the coordinate vector $[v]$ of $v=(5,-3)$ relative to $S$.

(a) Because $E$ is the usual basis, simply write the basis vectors in $S$ as columns: $P=\left[\begin{array}{ll}1 & 1 \\ 3 & 4\end{array}\right]$

(b) Method 1. Use the definition of the change-of-basis matrix. That is, express each vector in $E$ as a linear combination of the vectors in $S$. We do this by first finding the coordinates of an arbitrary vector $v=(a, b)$ relative to $S$. We have

$$
(a, b)=x(1,3)+y(1,4)=(x+y, 3 x+4 y) \quad \text { or } \quad \begin{aligned}
x+y & =a \\
3 x+4 y & =b
\end{aligned}
$$

Solve for $x$ and $y$ to obtain $x=4 a-b, y=-3 a+b$. Thus,

$$
v=(4 a-b) u_{1}+(-3 a+b) u_{2} \quad \text { and } \quad[v]_{S}=[(a, b)]_{S}=[4 a-b,-3 a+b]^{T}
$$

Using the above formula for $[v]_{S}$ and writing the coordinates of the $e_{i}$ as columns yields

$$
\begin{aligned}
& e_{1}=(1,0)=4 u_{1}-3 u_{2} \\
& e_{2}=(0,1)=-u_{1}+u_{2}
\end{aligned} \quad \text { and } \quad Q=\left[\begin{array}{rr}
4 & -1 \\
-3 & 1
\end{array}\right]
$$

Method 2. Because $Q=P^{-1}$, find $P^{-1}$, say by using the formula for the inverse of a $2 \times 2$ matrix. Thus,

$$
P^{-1}=\left[\begin{array}{rr}
4 & -1 \\
-3 & 1
\end{array}\right]
$$

(c) Method 1. Write $v$ as a linear combination of the vectors in $S$, say by using the above formula for $v=(a, b)$. We have $v=(5,-3)=23 u_{1}-18 u_{2}$, and so $[v]_{S}=[23,-18]^{T}$.

Method 2. Use, from Theorem 6.6, the fact that $[v]_{S}=P^{-1}[v]_{E}$ and the fact that $[v]_{E}=[5,-3]^{T}$ :

$$
[v]_{S}=P^{-1}[v]_{E}=\left[\begin{array}{rr}
4 & -1 \\
-3 & 1
\end{array}\right]\left[\begin{array}{r}
5 \\
-3
\end{array}\right]=\left[\begin{array}{r}
23 \\
-18
\end{array}\right]
$$

6.14. The vectors $u_{1}=(1,2,0), u_{2}=(1,3,2), u_{3}=(0,1,3)$ form a basis $S$ of $\mathbf{R}^{3}$. Find

(a) The change-of-basis matrix $P$ from the usual basis $E=\left\{e_{1}, e_{2}, e_{3}\right\}$ to $S$.

(b) The change-of-basis matrix $Q$ from $S$ back to $E$.

(a) Because $E$ is the usual basis, simply write the basis vectors of $S$ as columns: $P=\left[\begin{array}{lll}1 & 1 & 0 \\ 2 & 3 & 1 \\ 0 & 2 & 3\end{array}\right]$

(b) Method 1. Express each basis vector of $E$ as a linear combination of the basis vectors of $S$ by first finding the coordinates of an arbitrary vector $v=(a, b, c)$ relative to the basis $S$. We have

$$
\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]=x\left[\begin{array}{l}
1 \\
2 \\
0
\end{array}\right]+y\left[\begin{array}{l}
1 \\
3 \\
2
\end{array}\right]+z\left[\begin{array}{l}
0 \\
1 \\
3
\end{array}\right] \quad \text { or } \begin{array}{rr}
x+y & =a \\
2 x+3 y+z & =b \\
2 y+3 z & =c
\end{array}
$$

Solve for $x, y, z$ to get $x=7 a-3 b+c, y=-6 a+3 b-c, z=4 a-2 b+c$. Thus,

or

$$
\begin{aligned}
v=(a, b, c) & =(7 a-3 b+c) u_{1}+(-6 a+3 b-c) u_{2}+(4 a-2 b+c) u_{3} \\
{[v]_{S}=[(a, b, c)]_{S} } & =[7 a-3 b+c,-6 a+3 b-c, 4 a-2 b+c]^{T}
\end{aligned}
$$

Using the above formula for $[v]_{S}$ and then writing the coordinates of the $e_{i}$ as columns yields

$$
\begin{aligned}
& e_{1}=(1,0,0)=7 u_{1}-6 u_{2}+4 u_{3} \\
& e_{2}=(0,1,0)=-3 u_{1}+3 u_{2}-2 u_{3} \\
& e_{3}=(0,0,1)=u_{1}-u_{2}+u_{3}
\end{aligned} \quad \text { and } \quad Q=\left[\begin{array}{rrr}
7 & -3 & 1 \\
-6 & 3 & -1 \\
4 & -2 & 1
\end{array}\right]
$$

Method 2. Find $P^{-1}$ by row reducing $M=[P, I]$ to the form $\left[I, P^{-1}\right]$ :

$$
\begin{aligned}
M & =\left[\begin{array}{lll:lll}
1 & 1 & 0 & 1 & 0 & 0 \\
2 & 3 & 1 & 0 & 1 & 0 \\
0 & 2 & 3 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{lll:rrr}
1 & 1 & 0 & 1 & 0 & 0 \\
0 & 1 & 1 & -2 & 1 & 0 \\
0 & 2 & 3 & 0 & 0 & 1
\end{array}\right] \\
& \sim\left[\begin{array}{rrr:rrr}
1 & 1 & 0 & 1 & 0 & 0 \\
0 & 1 & 1 & -2 & 1 & 0 \\
0 & 0 & 1 & 4 & -2 & 1
\end{array}\right] \sim\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & 7 & -3 & 1 \\
0 & 1 & 0 & -6 & 3 & -1 \\
0 & 0 & 1 & 4 & -2 & 1
\end{array}\right]=\left[I, P^{-1}\right]
\end{aligned}
$$

Thus, $Q=P^{-1}=\left[\begin{array}{rrr}7 & -3 & 1 \\ -6 & 3 & -1 \\ 4 & -2 & 1\end{array}\right]$.

6.15. Suppose the $x$-axis and $y$-axis in the plane $\mathbf{R}^{2}$ are rotated counterclockwise $45^{\circ}$ so that the new $x^{\prime}$-axis and $y^{\prime}$-axis are along the line $y=x$ and the line $y=-x$, respectively.

(a) Find the change-of-basis matrix $P$.

(b) Find the coordinates of the point $A(5,6)$ under the given rotation.

(a) The unit vectors in the direction of the new $x^{\prime}$ - and $y^{\prime}$-axes are

$$
u_{1}=\left(\frac{1}{2} \sqrt{2}, \frac{1}{2} \sqrt{2}\right) \quad \text { and } \quad u_{2}=\left(-\frac{1}{2} \sqrt{2}, \frac{1}{2} \sqrt{2}\right)
$$

(The unit vectors in the direction of the original $x$ and $y$ axes are the usual basis of $\mathbf{R}^{2}$.) Thus, write the coordinates of $u_{1}$ and $u_{2}$ as columns to obtain

$$
P=\left[\begin{array}{cc}
\frac{1}{2} \sqrt{2} & -\frac{1}{2} \sqrt{2} \\
\frac{1}{2} \sqrt{2} & \frac{1}{2} \sqrt{2}
\end{array}\right]
$$

(b) Multiply the coordinates of the point by $P^{-1}$ :

$$
\left[\begin{array}{rr}
\frac{1}{2} \sqrt{2} & \frac{1}{2} \sqrt{2} \\
-\frac{1}{2} \sqrt{2} & \frac{1}{2} \sqrt{2}
\end{array}\right]\left[\begin{array}{l}
5 \\
6
\end{array}\right]=\left[\begin{array}{l}
\frac{11}{2} \sqrt{2} \\
\frac{1}{2} \sqrt{2}
\end{array}\right]
$$

(Because $P$ is orthogonal, $P^{-1}$ is simply the transpose of $P$.)

6.16. The vectors $u_{1}=(1,1,0), u_{2}=(0,1,1), u_{3}=(1,2,2)$ form a basis $S$ of $\mathbf{R}^{3}$. Find the coordinates of an arbitrary vector $v=(a, b, c)$ relative to the basis $S$.

Method 1. Express $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknowns $x, y, z$. We have

$$
(a, b, c)=x(1,1,0)+y(0,1,1)+z(1,2,2)=(x+z, x+y+2 z, y+2 z)
$$

this yields the system

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-220(1)}
\end{center}

$$
\begin{aligned}
& y+2 z=c \quad y+2 z=c \quad z=a-b+c
\end{aligned}
$$

Solving by back-substitution yields $x=b-c, y=-2 a+2 b-c, z=a-b+c$. Thus,

$$
[v]_{S}=[b-c,-2 a+2 b-c, a-b+c]^{T}
$$

Method 2. Find $P^{-1}$ by row reducing $M=[P, I]$ to the form $\left[I, P^{-1}\right]$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$ or, in other words, the matrix whose columns are the basis vectors of $S$.

We have

$$
\begin{aligned}
M & =\left[\begin{array}{lll:lll}
1 & 0 & 1 & 1 & 0 & 0 \\
1 & 1 & 2 & 0 & 1 & 0 \\
0 & 1 & 2 & 0 & 0 & 1
\end{array}\right] \sim\left[\begin{array}{rrrrrr}
1 & 0 & 1 & 1 & 0 & 0 \\
0 & 1 & 1 & -1 & 1 & 0 \\
0 & 1 & 2 & 0 & 0 & 1
\end{array}\right] \\
& \sim\left[\begin{array}{rrrrrr}
1 & 0 & 1 & 1 & 0 & 0 \\
0 & 1 & 1 & -1 & 1 & 0 \\
0 & 0 & 1 & 1 & -1 & 1
\end{array}\right] \sim\left[\begin{array}{lll:rrr}
1 & 0 & 0 & 0 & 1 & -1 \\
0 & 1 & 0 & -2 & 2 & -1 \\
0 & 0 & 1 & 1 & -1 & 1
\end{array}\right]=\left[I, P^{-1}\right] \\
\text { Thus, } \quad P^{-1} & =\left[\begin{array}{rrrr}
0 & 1 & -1 \\
-2 & 2 & -1 \\
1 & -1 & 1
\end{array}\right] \text { and }[v]_{S}=P^{-1}[v]_{E}=\left[\begin{array}{rrr}
0 & 1 & -1 \\
-2 & 2 & -1 \\
1 & -1 & 1
\end{array}\right]\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]=\left[\begin{array}{c}
b-c \\
-2 a+2 b-c \\
a-b+c
\end{array}\right]
\end{aligned}
$$

6.17. Consider the following bases of $\mathbf{R}^{2}$ :

$$
S=\left\{u_{1}, u_{2}\right\}=\{(1,-2),(3,-4)\} \quad \text { and } \quad S^{\prime}=\left\{v_{1}, v_{2}\right\}=\{(1,3),(3,8)\}
$$

(a) Find the coordinates of $v=(a, b)$ relative to the basis $S$.

(b) Find the change-of-basis matrix $P$ from $S$ to $S^{\prime}$.

(c) Find the coordinates of $v=(a, b)$ relative to the basis $S^{\prime}$.

(d) Find the change-of-basis matrix $Q$ from $S^{\prime}$ back to $S$.

(e) Verify $Q=P^{-1}$.

(f) Show that, for any vector $v=(a, b)$ in $\mathbf{R}^{2}, P^{-1}[v]_{S}=[v]_{S^{\prime}}$. (See Theorem 6.6.)

(a) Let $v=x u_{1}+y u_{2}$ for unknowns $x$ and $y$; that is,

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-220}
\end{center}

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-2 a-\frac{3}{2} b$ and $y=a+\frac{1}{2} b$. Thus,

$$
(a, b)=\left(-2 a-\frac{3}{2}\right) u_{1}+\left(a+\frac{1}{2} b\right) u_{2} \quad \text { or } \quad[(a, b)]_{S}=\left[-2 a-\frac{3}{2} b, a+\frac{1}{2} b\right]^{T}
$$

(b) Use part (a) to write each of the basis vectors $v_{1}$ and $v_{2}$ of $S^{\prime}$ as a linear combination of the basis vectors $u_{1}$ and $u_{2}$ of $S$; that is,

$$
\begin{aligned}
& v_{1}=(1,3)=\left(-2-\frac{9}{2}\right) u_{1}+\left(1+\frac{3}{2}\right) u_{2}=-\frac{13}{2} u_{1}+\frac{5}{2} u_{2} \\
& v_{2}=(3,8)=(-6-12) u_{1}+(3+4) u_{2}=-18 u_{1}+7 u_{2}
\end{aligned}
$$

Then $P$ is the matrix whose columns are the coordinates of $v_{1}$ and $v_{2}$ relative to the basis $S$; that is,

$$
P=\left[\begin{array}{rr}
-\frac{13}{2} & -18 \\
\frac{5}{2} & 7
\end{array}\right]
$$

(c) Let $v=x v_{1}+y v_{2}$ for unknown scalars $x$ and $y$ :

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-221}
\end{center}

Solve for $x$ and $y$ to get $x=-8 a+3 b$ and $y=3 a-b$. Thus,

$$
(a, b)=(-8 a+3 b) v_{1}+(3 a-b) v_{2} \quad \text { or } \quad[(a, b)]_{S^{\prime}}=[-8 a+3 b, \quad 3 a-b]^{T}
$$

(d) Use part (c) to express each of the basis vectors $u_{1}$ and $u_{2}$ of $S$ as a linear combination of the basis vectors $v_{1}$ and $v_{2}$ of $S^{\prime}$ :

$$
\begin{aligned}
& u_{1}=(1,-2)=(-8-6) v_{1}+(3+2) v_{2}=-14 v_{1}+5 v_{2} \\
& u_{2}=(3,-4)=(-24-12) v_{1}+(9+4) v_{2}=-36 v_{1}+13 v_{2}
\end{aligned}
$$

Write the coordinates of $u_{1}$ and $u_{2}$ relative to $S^{\prime}$ as columns to obtain $Q=\left[\begin{array}{rr}-14 & -36 \\ 5 & 13\end{array}\right]$.

(e) $Q P=\left[\begin{array}{rr}-14 & -36 \\ 5 & 13\end{array}\right]\left[\begin{array}{rr}-\frac{13}{2} & -18 \\ \frac{5}{2} & 7\end{array}\right]=\left[\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right]=I$

(f) Use parts (a), (c), and (d) to obtain

$$
P^{-1}[v]_{S}=Q[v]_{S}=\left[\begin{array}{rr}
-14 & -36 \\
5 & 13
\end{array}\right]\left[\begin{array}{c}
-2 a-\frac{3}{2} b \\
a+\frac{1}{2} b
\end{array}\right]=\left[\begin{array}{c}
-8 a+3 b \\
3 a-b
\end{array}\right]=[v]_{S^{\prime}}
$$

6.18. Suppose $P$ is the change-of-basis matrix from a basis $\left\{u_{i}\right\}$ to a basis $\left\{w_{i}\right\}$, and suppose $Q$ is the change-of-basis matrix from the basis $\left\{w_{i}\right\}$ back to $\left\{u_{i}\right\}$. Prove that $P$ is invertible and that $Q=P^{-1}$.

Suppose, for $i=1,2, \ldots, n$, that


\begin{equation*}
w_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\ldots+a_{i n} u_{n}=\sum_{j=1}^{n} a_{i j} u_{j} \tag{1}
\end{equation*}


and, for $j=1,2, \ldots, n$,


\begin{equation*}
u_{j}=b_{j 1} w_{1}+b_{j 2} w_{2}+\cdots+b_{j n} w_{n}=\sum_{k=1}^{n} b_{j k} w_{k} \tag{2}
\end{equation*}


Let $A=\left[a_{i j}\right]$ and $B=\left[b_{j k}\right]$. Then $P=A^{T}$ and $Q=B^{T}$. Substituting (2) into (1) yields

$$
w_{i}=\sum_{j=1}^{n} a_{i j}\left(\sum_{k=1}^{n} b_{j k} w_{k}\right)=\sum_{k=1}^{n}\left(\sum_{j=1}^{n} a_{i j} b_{j k}\right) w_{k}
$$

Because $\left\{w_{i}\right\}$ is a basis, $\sum a_{i j} b_{j k}=\delta_{i k}$, where $\delta_{i k}$ is the Kronecker delta; that is, $\delta_{i k}=1$ if $i=k$ but $\delta_{i k}=0$ if $i \neq k$. Suppose $A B=\left[c_{i k}\right]$. Then $c_{i k}=\delta_{i k}$. Accordingly, $A B=I$, and so

$$
Q P=B^{T} A^{T}=(A B)^{T}=I^{T}=I
$$

Thus, $Q=P^{-1}$.

6.19. Consider a finite sequence of vectors $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$. Let $S^{\prime}$ be the sequence of vectors obtained from $S$ by one of the following "elementary operations":

(1) Interchange two vectors.

(2) Multiply a vector by a nonzero scalar.

(3) Add a multiple of one vector to another vector.

Show that $S$ and $S^{\prime}$ span the same subspace $W$. Also, show that $S^{\prime}$ is linearly independent if and only if $S$ is linearly independent.

Observe that, for each operation, the vectors $S^{\prime}$ are linear combinations of vectors in $S$. Also, because each operation has an inverse of the same type, each vector in $S$ is a linear combination of vectors in $S^{\prime}$. Thus, $S$ and $S^{\prime}$ span the same subspace $W$. Moreover, $S^{\prime}$ is linearly independent if and only if $\operatorname{dim} W=n$, and this is true if and only if $S$ is linearly independent.

6.20. Let $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ be row equivalent $m \times n$ matrices over a field $K$, and let $v_{1}, v_{2}, \ldots, v_{n}$ be any vectors in a vector space $V$ over $K$. For $i=1,2, \ldots, m$, let $u_{i}$ and $w_{i}$ be defined by

$$
u_{i}=a_{i 1} v_{1}+a_{i 2} v_{2}+\cdots+a_{i n} v_{n} \quad \text { and } \quad w_{i}=b_{i 1} v_{1}+b_{i 2} v_{2}+\cdots+b_{i n} v_{n}
$$

Show that $\left\{u_{i}\right\}$ and $\left\{w_{i}\right\}$ span the same subspace of $V$.

Applying an "elementary operation" of Problem 6.19 to $\left\{u_{i}\right\}$ is equivalent to applying an elementary row operation to the matrix $A$. Because $A$ and $B$ are row equivalent, $B$ can be obtained from $A$ by a sequence of elementary row operations. Hence, $\left\{w_{i}\right\}$ can be obtained from $\left\{u_{i}\right\}$ by the corresponding sequence of operations. Accordingly, $\left\{u_{i}\right\}$ and $\left\{w_{i}\right\}$ span the same space.

6.21. Suppose $u_{1}, u_{2}, \ldots, u_{n}$ belong to a vector space $V$ over a field $K$, and suppose $P=\left[a_{i j}\right]$ is an $n$-square matrix over $K$. For $i=1,2, \ldots, n$, let $v_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\cdots+a_{i n} u_{n}$.

(a) Suppose $P$ is invertible. Show that $\left\{u_{i}\right\}$ and $\left\{v_{i}\right\}$ span the same subspace of $V$. Hence, $\left\{u_{i}\right\}$ is linearly independent if and only if $\left\{v_{i}\right\}$ is linearly independent.

(b) Suppose $P$ is singular (not invertible). Show that $\left\{v_{i}\right\}$ is linearly dependent.

(c) Suppose $\left\{v_{i}\right\}$ is linearly independent. Show that $P$ is invertible.

(a) Because $P$ is invertible, it is row equivalent to the identity matrix $I$. Hence, by Problem 6.19, $\left\{v_{i}\right\}$ and $\left\{u_{i}\right\}$ span the same subspace of $V$. Thus, one is linearly independent if and only if the other is linearly independent.

(b) Because $P$ is not invertible, it is row equivalent to a matrix with a zero row. This means $\left\{v_{i}\right\}$ spans a substance that has a spanning set with less than $n$ elements. Thus, $\left\{v_{i}\right\}$ is linearly dependent.

(c) This is the contrapositive of the statement of part (b), and so it follows from part (b).

6.22. Prove Theorem 6.6: Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\prime}$ in a vector space $V$. Then, for any vector $v \in V$, we have $P[v]_{S^{\prime}}=[v]_{S}$, and hence, $P^{-1}[v]_{S}=[v]_{S^{\prime}}$.

Suppose $S=\left\{u_{1}, \ldots, u_{n}\right\}$ and $S^{\prime}=\left\{w_{1}, \ldots, w_{n}\right\}$, and suppose, for $i=1, \ldots, n$,

$$
w_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\cdots+a_{i n} u_{n}=\sum_{j=1}^{n} a_{i j} u_{j}
$$

Then $P$ is the $n$-square matrix whose $j$ th row is


\begin{equation*}
\left(a_{1 j}, a_{2 j}, \ldots, a_{n j}\right) \tag{1}
\end{equation*}


Also suppose $v=k_{1} w_{1}+k_{2} w_{2}+\cdots+k_{n} w_{n}=\sum_{i=1}^{n} k_{i} w_{i}$. Then


\begin{equation*}
[v]_{S^{\prime}}=\left[k_{1}, k_{2}, \ldots, k_{n}\right]^{T} \tag{2}
\end{equation*}


Substituting for $w_{i}$ in the equation for $v$, we obtain

$$
\begin{aligned}
v & =\sum_{i=1}^{n} k_{i} w_{i}=\sum_{i=1}^{n} k_{i}\left(\sum_{j=1}^{n} a_{i j} u_{j}\right)=\sum_{j=1}^{n}\left(\sum_{i=1}^{n} a_{i j} k_{i}\right) u_{j} \\
& =\sum_{j=1}^{n}\left(a_{1 j} k_{1}+a_{2 j} k_{2}+\cdots+a_{n j} k_{n}\right) u_{j}
\end{aligned}
$$

Accordingly, $[v]_{S}$ is the column vector whose $j$ th entry is


\begin{equation*}
a_{1 j} k_{1}+a_{2 j} k_{2}+\cdots+a_{n j} k_{n} \tag{3}
\end{equation*}


On the other hand, the $j$ th entry of $P[v]_{S^{\prime}}$ is obtained by multiplying the $j$ th row of $P$ by $[v]_{S^{\prime}}$-that is, (1) by (2). However, the product of (1) and (2) is (3). Hence, $P[v]_{S^{\prime}}$ and $[v]_{S}$ have the same entries. Thus, $P[v]_{S^{\prime}}=[v]_{S^{\prime}}$, as claimed.

Furthermore, multiplying the above by $P^{-1}$ gives $P^{-1}[v]_{S}=P^{-1} P[v]_{S^{\prime}}=[v]_{S^{\prime}}$.

\section*{Linear Operators and Change of Basis}
6.23. Consider the linear transformation $F$ on $\mathbf{R}^{2}$ defined by $F(x, y)=(5 x-y, 2 x+y)$ and the following bases of $\mathbf{R}^{2}$ :

$$
E=\left\{e_{1}, e_{2}\right\}=\{(1,0),(0,1)\} \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,4),(2,7)\}
$$

(a) Find the change-of-basis matrix $P$ from $E$ to $S$ and the change-of-basis matrix $Q$ from $S$ back to $E$.

(b) Find the matrix $A$ that represents $F$ in the basis $E$.

(c) Find the matrix $B$ that represents $F$ in the basis $S$.

(a) Because $E$ is the usual basis, simply write the vectors in $S$ as columns to obtain the change-of-basis matrix $P$. Recall, also, that $Q=P^{-1}$. Thus,

$$
P=\left[\begin{array}{ll}
1 & 2 \\
4 & 7
\end{array}\right] \quad \text { and } \quad Q=P^{-1}=\left[\begin{array}{rr}
-7 & 2 \\
4 & -1
\end{array}\right]
$$

(b) Write the coefficients of $x$ and $y$ in $F(x, y)=(5 x-y, 2 x+y)$ as rows to get

$$
A=\left[\begin{array}{rr}
5 & -1 \\
2 & 1
\end{array}\right]
$$

(c) Method 1. Find the coordinates of $F\left(u_{1}\right)$ and $F\left(u_{2}\right)$ relative to the basis $S$. This may be done by first finding the coordinates of an arbitrary vector $(a, b)$ in $\mathbf{R}^{2}$ relative to the basis $S$. We have

$$
(a, b)=x(1,4)+y(2,7)=(x+2 y, 4 x+7 y), \quad \text { and so } \quad \begin{aligned}
x+2 y & =a \\
4 x+7 y & =b
\end{aligned}
$$

Solve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-7 a+2 b, y=4 a-b$. Then

$$
(a, b)=(-7 a+2 b) u_{1}+(4 a-b) u_{2}
$$

Now use the formula for $(a, b)$ to obtain

$$
\begin{aligned}
& F\left(u_{1}\right)=F(1,4)=(1,6)=5 u_{1}-2 u_{2} \\
& F\left(u_{2}\right)=F(2,7)=(3,11)=u_{1}+u_{2}
\end{aligned} \quad \text { and so } \quad B=\left[\begin{array}{rr}
5 & 1 \\
-2 & 1
\end{array}\right]
$$

Method 2. By Theorem 6.7, $B=P^{-1} A P$. Thus,

$$
B=P^{-1} A P=\left[\begin{array}{rr}
-7 & 2 \\
4 & -1
\end{array}\right]\left[\begin{array}{rr}
5 & -1 \\
2 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 2 \\
4 & 7
\end{array}\right]=\left[\begin{array}{rr}
5 & 1 \\
-2 & 1
\end{array}\right]
$$

6.24. Let $A=\left[\begin{array}{rr}2 & 3 \\ 4 & -1\end{array}\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to the basis $S=\left\{u_{1}, u_{2}\right\}=\left\{[1,3]^{T},[2,5]^{T}\right\} .\left[\right.$ Recall $A$ defines a linear operator $A: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ relative to the usual basis $E$ of $\mathbf{R}^{2}$ ].

Method 1. Find the coordinates of $A\left(u_{1}\right)$ and $A\left(u_{2}\right)$ relative to the basis $S$ by first finding the coordinates of an arbitrary vector $[a, b]^{T}$ in $\mathbf{R}^{2}$ relative to the basis $S$. By Problem 6.2,

$$
[a, b]^{T}=(-5 a+2 b) u_{1}+(3 a-b) u_{2}
$$

Using the formula for $[a, b]^{T}$, we obtain

and

$$
\begin{gathered}
A\left(u_{1}\right)=\left[\begin{array}{rr}
2 & 3 \\
4 & -1
\end{array}\right]\left[\begin{array}{l}
1 \\
3
\end{array}\right]=\left[\begin{array}{r}
11 \\
1
\end{array}\right]=-53 u_{1}+32 u_{2} \\
A\left(u_{2}\right)=\left[\begin{array}{rr}
2 & 3 \\
4 & -1
\end{array}\right]\left[\begin{array}{l}
2 \\
5
\end{array}\right]=\left[\begin{array}{r}
19 \\
3
\end{array}\right]=-89 u_{1}+54 u_{2} \\
B=\left[\begin{array}{rr}
-53 & -89 \\
32 & 54
\end{array}\right]
\end{gathered}
$$

Thus,

Method 2. Use $B=P^{-1} A P$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$. Thus, simply write the vectors in $S$ (as columns) to obtain the change-of-basis matrix $P$ and then use the formula\\
for $P^{-1}$. This gives

$$
\begin{gathered}
P=\left[\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right] \text { and } P^{-1}=\left[\begin{array}{rr}
-5 & 2 \\
3 & -1
\end{array}\right] \\
\text { Then } \quad B=P^{-1} A P=\left[\begin{array}{lr}
1 & 2 \\
3 & 5
\end{array}\right]\left[\begin{array}{rr}
2 & 3 \\
4 & -1
\end{array}\right]\left[\begin{array}{rr}
-5 & 2 \\
3 & -1
\end{array}\right]=\left[\begin{array}{rr}
-53 & -89 \\
32 & 54
\end{array}\right]
\end{gathered}
$$

6.25. Let $A=\left[\begin{array}{rrr}1 & 3 & 1 \\ 2 & 5 & -4 \\ 1 & -2 & 2\end{array}\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to the basis

$$
S=\left\{u_{1}, u_{2}, u_{3}\right\}=\left\{[1,1,0]^{T}, \quad[0,1,1]^{T}, \quad[1,2,2]^{T}\right\}
$$

[Recall $A$ that defines a linear operator $A: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ relative to the usual basis $E$ of $\mathbf{R}^{3}$.]

Method 1. Find the coordinates of $A\left(u_{1}\right), A\left(u_{2}\right), A\left(u_{3}\right)$ relative to the basis $S$ by first finding the coordinates of an arbitrary vector $v=(a, b, c)$ in $\mathbf{R}^{3}$ relative to the basis $S$. By Problem 6.16,

$$
[v]_{S}=(b-c) u_{1}+(-2 a+2 b-c) u_{2}+(a-b+c) u_{3}
$$

Using this formula for $[a, b, c]^{T}$, we obtain

$$
\begin{gathered}
A\left(u_{1}\right)=[4,7,-1]^{T}=8 u_{1}+7 u_{2}-5 u_{3}, \quad A\left(u_{2}\right)=[4,1,0]^{T}=u_{1}-6 u_{2}+3 u_{3} \\
A\left(u_{3}\right)=[9,4,1]^{T}=3 u_{1}-11 u_{2}+6 u_{3}
\end{gathered}
$$

Writing the coefficients of $u_{1}, u_{2}, u_{3}$ as columns yields

$$
B=\left[\begin{array}{rrr}
8 & 1 & 3 \\
7 & -6 & -11 \\
-5 & 3 & 6
\end{array}\right]
$$

Method 2. Use $B=P^{-1} A P$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$. The matrix $P$ (whose columns are simply the vectors in $S$ ) and $P^{-1}$ appear in Problem 6.16. Thus,

$$
B=P^{-1} A P=\left[\begin{array}{rrr}
0 & 1 & -1 \\
-2 & 2 & -1 \\
1 & -1 & 1
\end{array}\right]\left[\begin{array}{rrr}
1 & 3 & 1 \\
2 & 5 & -4 \\
1 & -2 & 2
\end{array}\right]\left[\begin{array}{lll}
1 & 0 & 1 \\
1 & 1 & 2 \\
0 & 1 & 2
\end{array}\right]=\left[\begin{array}{rrr}
8 & 1 & 3 \\
7 & -6 & -11 \\
-5 & 3 & 6
\end{array}\right]
$$

6.26. Prove Theorem 6.7: Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\prime}$ in a vector space $V$. Then, for any linear operator $T$ on $V,[T]_{S^{\prime}}=P^{-1}[T]_{S} P$.

Let $v$ be a vector in $V$. Then, by Theorem 6.6, $P[v]_{S^{\prime}}=[v]_{S}$. Therefore,

$$
P^{-1}[T]_{S} P[v]_{S^{\prime}}=P^{-1}[T]_{S}[v]_{S}=P^{-1}[T(v)]_{S}=[T(v)]_{S^{\prime}}
$$

But $[T]_{S^{\prime}}[v]_{S^{\prime}}=[T(v)]_{S^{\prime}}$. Hence,

$$
P^{-1}[T]_{S} P[v]_{S^{\prime}}=[T]_{S^{\prime}}[v]_{S^{\prime}}
$$

Because the mapping $v \mapsto[v]_{S^{\prime}}$ is onto $K^{n}$, we have $P^{-1}[T]_{S} P X=[T]_{S^{\prime}} X$ for every $X \in K^{n}$. Thus, $P^{-1}[T]_{S} P=[T]_{S^{\prime}}$, as claimed.

\section*{Similarity of Matrices}
6.27. Let $A=\left[\begin{array}{rr}4 & -2 \\ 3 & 6\end{array}\right]$ and $P=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right]$.\\
(a) Find $B=P^{-1} A P$.\\
(b) Verify $\operatorname{tr}(B)=\operatorname{tr}(A)$.\\
(c) Verify $\operatorname{det}(B)=\operatorname{det}(A)$.

(a) First find $P^{-1}$ using the formula for the inverse of a $2 \times 2$ matrix. We have

$$
P^{-1}=\left[\begin{array}{rr}
-2 & 1 \\
\frac{3}{2} & -\frac{1}{2}
\end{array}\right]
$$

Then

$$
B=P^{-1} A P=\left[\begin{array}{rr}
-2 & 1 \\
\frac{3}{2} & -\frac{1}{2}
\end{array}\right]\left[\begin{array}{rr}
4 & -2 \\
3 & 6
\end{array}\right]\left[\begin{array}{rr}
1 & 2 \\
3 & 4
\end{array}\right]=\left[\begin{array}{rr}
25 & 30 \\
-\frac{27}{2} & -15
\end{array}\right]
$$

(b) $\operatorname{tr}(A)=4+6=10$ and $\operatorname{tr}(B)=25-15=10$. Hence, $\operatorname{tr}(B)=\operatorname{tr}(A)$.

(c) $\operatorname{det}(A)=24+6=30$ and $\operatorname{det}(B)=-375+405=30$. Hence, $\operatorname{det}(B)=\operatorname{det}(A)$.

6.28. Find the trace of each of the linear transformations $F$ on $\mathbf{R}^{3}$ in Problem 6.4.

Find the trace (sum of the diagonal elements) of any matrix representation of $F$ such as the matrix representation $[F]=[F]_{E}$ of $F$ relative to the usual basis $E$ given in Problem 6.4.

(a) $\operatorname{tr}(F)=\operatorname{tr}([F])=1-5+9=5$.

(b) $\operatorname{tr}(F)=\operatorname{tr}([F])=1+3+5=9$.

(c) $\operatorname{tr}(F)=\operatorname{tr}([F])=1+4+7=12$.

6.29. Write $A \approx B$ if $A$ is similar to $B$-that is, if there exists an invertible matrix $P$ such that $A=P^{-1} B P$. Prove that $\approx$ is an equivalence relation (on square matrices); that is,

(a) $A \approx A$, for every $A . \quad$ (b) If $A \approx B$, then $B \approx A$.

(c) If $A \approx B$ and $B \approx C$, then $A \approx C$.

(a) The identity matrix $I$ is invertible, and $I^{-1}=I$. Because $A=I^{-1} A I$, we have $A \approx A$.

(b) Because $A \approx B$, there exists an invertible matrix $P$ such that $A=P^{-1} B P$. Hence, $B=P A P^{-1}=\left(P^{-1}\right)^{-1} A P$ and $P^{-1}$ is also invertible. Thus, $B \approx A$.

(c) Because $A \approx B$, there exists an invertible matrix $P$ such that $A=P^{-1} B P$, and as $B \approx C$, there exists an invertible matrix $Q$ such that $B=Q^{-1} C Q$. Thus,

$$
A=P^{-1} B P=P^{-1}\left(Q^{-1} C Q\right) P=\left(P^{-1} Q^{-1}\right) C(Q P)=(Q P)^{-1} C(Q P)
$$

and $Q P$ is also invertible. Thus, $A \approx C$.

6.30. Suppose $B$ is similar to $A$, say $B=P^{-1} A P$. Prove

(a) $B^{n}=P^{-1} A^{n} P$, and so $B^{n}$ is similar to $A^{n}$.

(b) $f(B)=P^{-1} f(A) P$, for any polynomial $f(x)$, and so $f(B)$ is similar to $f(A)$.

(c) $B$ is a root of a polynomial $g(x)$ if and only if $A$ is a root of $g(x)$.

(a) The proof is by induction on $n$. The result holds for $n=1$ by hypothesis. Suppose $n>1$ and the result holds for $n-1$. Then

$$
B^{n}=B B^{n-1}=\left(P^{-1} A P\right)\left(P^{-1} A^{n-1} P\right)=P^{-1} A^{n} P
$$

(b) Suppose $f(x)=a_{n} x^{n}+\cdots+a_{1} x+a_{0}$. Using the left and right distributive laws and part (a), we have

$$
\begin{aligned}
P^{-1} f(A) P & =P^{-1}\left(a_{n} A^{n}+\cdots+a_{1} A+a_{0} I\right) P \\
& =P^{-1}\left(a_{n} A^{n}\right) P+\cdots+P^{-1}\left(a_{1} A\right) P+P^{-1}\left(a_{0} I\right) P \\
& =a_{n}\left(P^{-1} A^{n} P\right)+\cdots+a_{1}\left(P^{-1} A P\right)+a_{0}\left(P^{-1} I P\right) \\
& =a_{n} B^{n}+\cdots+a_{1} B+a_{0} I=f(B)
\end{aligned}
$$

(c) By part (b), $g(B)=0$ if and only if $P^{-1} g(A) P=0$ if and only if $g(A)=P 0 P^{-1}=0$.

\section*{Matrix Representations of General Linear Mappings}
6.31. Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ be the linear map defined by $F(x, y, z)=(3 x+2 y-4 z, x-5 y+3 z)$.

(a) Find the matrix of $F$ in the following bases of $\mathbf{R}^{3}$ and $\mathbf{R}^{2}$ :


\begin{equation*}
S=\left\{w_{1}, w_{2}, w_{3}\right\}=\{(1,1,1),(1,1,0),(1,0,0)\} \quad \text { and } \quad S^{\prime}=\left\{u_{1}, u_{2}\right\}=\{(1,3) \tag{2,5}
\end{equation*}


(b) Verify Theorem 6.10: The action of $F$ is preserved by its matrix representation; that is, for any $v$ in $\mathbf{R}^{3}$, we have $[F]_{S, S^{\prime}}[v]_{S}=[F(v)]_{S^{\prime}}$.

(a) From Problem 6.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Thus,

$$
\begin{aligned}
& F\left(w_{1}\right)=F(1,1,1)=(1,-1)=-7 u_{1}+4 u_{2} \\
& F\left(w_{2}\right)=F(1,1,0)=(5,-4)=-33 u_{1}+19 u_{2} \\
& F\left(w_{3}\right)=F(1,0,0)=(3,1)=-13 u_{1}+8 u_{2}
\end{aligned}
$$

Write the coordinates of $F\left(w_{1}\right), F\left(w_{2}\right), F\left(w_{3}\right)$ as columns to get

$$
[F]_{S, S^{\prime}}=\left[\begin{array}{rrr}
-7 & -33 & 13 \\
4 & 19 & 8
\end{array}\right]
$$

(b) If $v=(x, y, z)$, then, by Problem 6.5, $v=z w_{1}+(y-z) w_{2}+(x-y) w_{3}$. Also,

$F(v)=(3 x+2 y-4 z, x-5 y+3 z)=(-13 x-20 y+26 z) u_{1}+(8 x+11 y-15 z) u_{2}$

Hence, $\quad[v]_{S}=(z, y-z, x-y)^{T} \quad$ and $\quad[F(v)]_{S^{\prime}}=\left[\begin{array}{c}-13 x-20 y+26 z \\ 8 x+11 y-15 z\end{array}\right]$

Thus, $\quad[F]_{S, S^{\prime}}[v]_{S}=\left[\begin{array}{rrr}-7 & -33 & -13 \\ 4 & 19 & 8\end{array}\right]\left[\begin{array}{c}z \\ y-x \\ x-y\end{array}\right]=\left[\begin{array}{c}-13 x-20 y+26 z \\ 8 x+11 y-15 z\end{array}\right]=[F(v)]_{S^{\prime}}$

6.32. Let $F: \mathbf{R}^{n} \rightarrow \mathbf{R}^{m}$ be the linear mapping defined as follows:

$$
F\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\left(a_{11} x_{1}+\cdots+a_{1 n} x_{n}, a_{21} x_{1}+\cdots+a_{2 n} x_{n}, \ldots, a_{m 1} x_{1}+\cdots+a_{m n} x_{n}\right)
$$

(a) Show that the rows of the matrix $[F]$ representing $F$ relative to the usual bases of $\mathbf{R}^{n}$ and $\mathbf{R}^{m}$ are the coefficients of the $x_{i}$ in the components of $F\left(x_{1}, \ldots, x_{n}\right)$.

(b) Find the matrix representation of each of the following linear mappings relative to the usual basis of $\mathbf{R}^{n}$ :

(i) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{3}$ defined by $F(x, y)=(3 x-y, \quad 2 x+4 y, \quad 5 x-6 y)$.

(ii) $F: \mathbf{R}^{4} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, s, t)=(3 x-4 y+2 s-5 t, \quad 5 x+7 y-s-2 t)$.

(iii) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{4}$ defined by $F(x, y, z)=(2 x+3 y-8 z, \quad x+y+z, \quad 4 x-5 z, \quad 6 y)$.

(a) We have

$$
\begin{aligned}
& F(1,0, \ldots, 0)=\left(a_{11}, a_{21}, \ldots, a_{m 1}\right) \\
& F(0,1, \ldots, 0)=\left(a_{12}, a_{22}, \ldots, a_{m 2}\right) \\
& \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
& F(0,0, \ldots, 1)=\left(a_{1 n}, a_{2 n}, \ldots, a_{m n}\right)
\end{aligned} \quad \text { and thus, } \quad[F]=\left[\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n}
\end{array}\right]
$$

(b) By part (a), we need only look at the coefficients of the unknown $x, y, \ldots$ in $F(x, y, \ldots)$. Thus,\\
(i) $[F]=\left[\begin{array}{rr}3 & -1 \\ 2 & 4 \\ 5 & -6\end{array}\right]$,\\
(ii) $[F]=\left[\begin{array}{rrrr}3 & -4 & 2 & -5 \\ 5 & 7 & -1 & -2\end{array}\right]$,\\
(iii) $[F]=\left[\begin{array}{rrr}2 & 3 & -8 \\ 1 & 1 & 1 \\ 4 & 0 & -5 \\ 0 & 6 & 0\end{array}\right]$

6.33. Let $A=\left[\begin{array}{rrr}2 & 5 & -3 \\ 1 & -4 & 7\end{array}\right]$. Recall that $A$ determines a mapping $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(v)=A v$, where vectors are written as columns. Find the matrix $[F]$ that represents the mapping relative to the following bases of $\mathbf{R}^{3}$ and $\mathbf{R}^{2}$ :

(a) The usual bases of $\mathbf{R}^{3}$ and of $\mathbf{R}^{2}$.

(b) $S=\left\{w_{1}, w_{2}, w_{3}\right\}=\{(1,1,1),(1,1,0),(1,0,0)\}$ and $S^{\prime}=\left\{u_{1}, u_{2}\right\}=\{(1,3),(2,5)\}$.

(a) Relative to the usual bases, $[F]$ is the matrix $A$.\\
(b) From Problem 9.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Thus,

$$
\begin{aligned}
& \qquad F\left(w_{1}\right)=\left[\begin{array}{rrr}
2 & 5 & -3 \\
1 & -4 & 7
\end{array}\right]\left[\begin{array}{l}
1 \\
1 \\
1
\end{array}\right]=\left[\begin{array}{l}
4 \\
4
\end{array}\right]=-12 u_{1}+8 u_{2} \\
& F\left(w_{2}\right)=\left[\begin{array}{rrr}
2 & 5 & -3 \\
1 & -4 & 7
\end{array}\right]\left[\begin{array}{l}
1 \\
1 \\
0
\end{array}\right]=\left[\begin{array}{r}
7 \\
-3
\end{array}\right]=-41 u_{1}+24 u_{2} \\
& F\left(w_{3}\right)=\left[\begin{array}{rrr}
2 & 5 & -3 \\
1 & -4 & 7
\end{array}\right]\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]=\left[\begin{array}{l}
2 \\
1
\end{array}\right]=-8 u_{1}+5 u_{2} \\
& \text { Writing the coefficients of } F\left(w_{1}\right), F\left(w_{2}\right), F\left(w_{3}\right) \text { as columns yields }[F]=\left[\begin{array}{rrr}
-12 & -41 & -8 \\
8 & 24 & 5
\end{array}\right] .
\end{aligned}
$$

6.34. Consider the linear transformation $T$ on $\mathbf{R}^{2}$ defined by $T(x, y)=(2 x-3 y, \quad x+4 y)$ and the following bases of $\mathbf{R}^{2}$ :

$$
E=\left\{e_{1}, e_{2}\right\}=\{(1,0),(0,1)\} \quad \text { and } \quad S=\left\{u_{1}, u_{2}\right\}=\{(1,3),(2,5)\}
$$

(a) Find the matrix $A$ representing $T$ relative to the bases $E$ and $S$.

(b) Find the matrix $B$ representing $T$ relative to the bases $S$ and $E$.

(We can view $T$ as a linear mapping from one space into another, each having its own basis.)

(a) From Problem 6.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Hence,

$$
\begin{aligned}
& T\left(e_{1}\right)=T(1,0)=(2,1)=-8 u_{1}+5 u_{2} \\
& T\left(e_{2}\right)=T(0,1)=(-3,4)=23 u_{1}-13 u_{2}
\end{aligned} \quad \text { and so } \quad A=\left[\begin{array}{rr}
-8 & 23 \\
5 & -13
\end{array}\right]
$$

(b) We have

$$
\begin{aligned}
& T\left(u_{1}\right)=T(1,3)=(-7,13)=-7 e_{1}+13 e_{2} \\
& T\left(u_{2}\right)=T(2,5)=(-11,22)=-11 e_{1}+22 e_{2}
\end{aligned} \quad \text { and so } \quad B=\left[\begin{array}{rr}
-7 & -11 \\
13 & 22
\end{array}\right]
$$

6.35. How are the matrices $A$ and $B$ in Problem 6.34 related?

By Theorem 6.12, the matrices $A$ and $B$ are equivalent to each other; that is, there exist nonsingular matrices $P$ and $Q$ such that $B=Q^{-1} A P$, where $P$ is the change-of-basis matrix from $S$ to $E$, and $Q$ is the change-of-basis matrix from $E$ to $S$. Thus,

and

$$
\begin{gathered}
P=\left[\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right], \quad Q=\left[\begin{array}{rr}
-5 & 2 \\
3 & -1
\end{array}\right], \quad Q^{-1}=\left[\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right] \\
Q^{-1} A P=\left[\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right]\left[\begin{array}{rr}
-8 & -23 \\
5 & -13
\end{array}\right]\left[\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right]=\left[\begin{array}{rr}
-7 & -11 \\
13 & 22
\end{array}\right]=B
\end{gathered}
$$

6.36. Prove Theorem 6.14: Let $F: V \rightarrow U$ be linear and, say, $\operatorname{rank}(F)=r$. Then there exist bases $V$ and of $U$ such that the matrix representation of $F$ has the following form, where $I_{r}$ is the $r$-square identity matrix:

$$
A=\left[\begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array}\right]
$$

Suppose $\operatorname{dim} V=m$ and $\operatorname{dim} U=n$. Let $W$ be the kernel of $F$ and $U^{\prime}$ the image of $F$. We are given that $\operatorname{rank}(F)=r$. Hence, the dimension of the kernel of $F$ is $m-r$. Let $\left\{w_{1}, \ldots, w_{m-r}\right\}$ be a basis of the kernel of $F$ and extend this to a basis of $V$ :

$$
\begin{gathered}
\left\{v_{1}, \ldots, v_{r}, w_{1}, \ldots, w_{m-r}\right\} \\
u_{1}=F\left(v_{1}\right), u_{2}=F\left(v_{2}\right), \ldots, u_{r}=F\left(v_{r}\right)
\end{gathered}
$$

Then $\left\{u_{1}, \ldots, u_{r}\right\}$ is a basis of $U^{\prime}$, the image of $F$. Extend this to a basis of $U$, say

$$
\left\{u_{1}, \ldots, u_{r}, u_{r+1}, \ldots, u_{n}\right\}
$$

Observe that

$$
\begin{aligned}
& F\left(v_{1}\right) \quad=u_{1}=1 u_{1}+0 u_{2}+\cdots+0 u_{r}+0 u_{r+1}+\cdots+0 u_{n} \\
& F\left(v_{2}\right) \quad=u_{2}=0 u_{1}+1 u_{2}+\cdots+0 u_{r}+0 u_{r+1}+\cdots+0 u_{n} \\
& F\left(v_{r}\right) \quad=u_{r}=0 u_{1}+0 u_{2}+\cdots+1 u_{r}+0 u_{r+1}+\cdots+0 u_{n} \\
& F\left(w_{1}\right)=0=0 u_{1}+0 u_{2}+\cdots+0 u_{r}+0 u_{r+1}+\cdots+0 u_{n} \\
& F\left(w_{m-r}\right)=0=0 u_{1}+0 u_{2}+\cdots+0 u_{r}+0 u_{r+1}+\cdots+0 u_{n}
\end{aligned}
$$

Thus, the matrix of $F$ in the above bases has the required form.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Matrices and Linear Operators}
6.37. Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y)=(4 x+5 y, 2 x-y)$.

(a) Find the matrix $A$ representing $F$ in the usual basis $E$.

(b) Find the matrix $B$ representing $F$ in the basis $S=\left\{u_{1}, u_{2}\right\}=\{(1,4),(2,9)\}$.

(c) Find $P$ such that $B=P^{-1} A P$.

(d) For $v=(a, b)$, find $[v]_{S}$ and $[F(v)]_{S}$. Verify that $[F]_{S}[v]_{S}=[F(v)]_{S}$.

6.38. Let $A: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by the matrix $A=\left[\begin{array}{rr}5 & -1 \\ 2 & 4\end{array}\right]$.

(a) Find the matrix $B$ representing $A$ relative to the basis $S=\left\{u_{1}, u_{2}\right\}=\{(1,3),(2,8)\}$. (Recall that $A$ represents the mapping $A$ relative to the usual basis $E$.)

(b) For $v=(a, b)$, find $[v]_{S}$ and $[A(v)]_{S}$.

6.39. For each linear transformation $L$ on $\mathbf{R}^{2}$, find the matrix $A$ representing $L$ (relative to the usual basis of $\mathbf{R}^{2}$ ):

(a) $L$ is the rotation in $\mathbf{R}^{2}$ counterclockwise by $45^{\circ}$.

(b) $L$ is the reflection in $\mathbf{R}^{2}$ about the line $y=x$

(c) $L$ is defined by $L(1,0)=(3,5)$ and $L(0,1)=(7,-2)$.

(d) $L$ is defined by $L(1,1)=(3,7)$ and $L(1,2)=(5,-4)$.

6.40. Find the matrix representing each linear transformation $T$ on $\mathbf{R}^{3}$ relative to the usual basis of $\mathbf{R}^{3}$ :\\
(a) $T(x, y, z)=(x, y, 0)$.\\
(b) $T(x, y, z)=(z, y+z, x+y+z)$.\\
(c) $T(x, y, z)=(2 x-7 y-4 z, 3 x+y+4 z, 6 x-8 y+z)$.

6.41. Repeat Problem 6.40 using the basis $S=\left\{u_{1}, u_{2}, u_{3}\right\}=\{(1,1,0),(1,2,3),(1,3,5)\}$.

6.42. Let $L$ be the linear transformation on $\mathbf{R}^{3}$ defined by

$$
L(1,0,0)=(1,1,1), \quad L(0,1,0)=(1,3,5), \quad L(0,0,1)=(2,2,2)
$$

(a) Find the matrix $A$ representing $L$ relative to the usual basis of $\mathbf{R}^{3}$.

(b) Find the matrix $B$ representing $L$ relative to the basis $S$ in Problem 6.41.

6.43. Let $\mathbf{D}$ denote the differential operator; that is, $\mathbf{D}(f(t))=d f / d t$. Each of the following sets is a basis of a vector space $V$ of functions. Find the matrix representing $\mathbf{D}$ in each basis:\\
(a) $\left\{e^{t}, e^{2 t}, t e^{2 t}\right\}$.\\
(b) $\{1, t, \sin 3 t, \cos 3 t\}$.\\
(c) $\left\{e^{5 t}, t e^{5 t}, t^{2} e^{5 t}\right\}$.

6.44. Let $\mathbf{D}$ denote the differential operator on the vector space $V$ of functions with basis $S=\{\sin \theta, \cos \theta\}$.\\
(a) Find the matrix $A=[\mathbf{D}]_{S}$.\\
(b) Use $A$ to show that $\mathbf{D}$ is a zero of $f(t)=t^{2}+1$.

6.45. Let $V$ be the vector space of $2 \times 2$ matrices. Consider the following matrix $M$ and usual basis $E$ of $V$ :

$M=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right] \quad$ and $\quad E=\left\{\left[\begin{array}{ll}1 & 0 \\ 0 & 0\end{array}\right],\left[\begin{array}{ll}0 & 1 \\ 0 & 0\end{array}\right], \quad\left[\begin{array}{ll}0 & 0 \\ 1 & 0\end{array}\right], \quad\left[\begin{array}{ll}0 & 0 \\ 0 & 1\end{array}\right]\right\}$

Find the matrix representing each of the following linear operators $T$ on $V$ relative to $E$ :\\
(a) $T(A)=M A$.\\
(b) $T(A)=A M$.\\
(c) $T(A)=M A-A M$.

6.46. Let $\mathbf{1}_{V}$ and $\mathbf{0}_{V}$ denote the identity and zero operators, respectively, on a vector space $V$. Show that, for any basis $S$ of $V$, (a) $\left[\mathbf{1}_{V}\right]_{S}=I$, the identity matrix.

(b) $\left[\mathbf{0}_{V}\right]_{S}=0$, the zero matrix.

\section*{Change of Basis}
6.47. Find the change-of-basis matrix $P$ from the usual basis $E$ of $\mathbf{R}^{2}$ to a basis $S$, the change-of-basis matrix $Q$ from $S$ back to $E$, and the coordinates of $v=(a, b)$ relative to $S$, for the following bases $S$ :\\
(a) $S=\{(1,2),(3,5)\}$.\\
(c) $S=\{(2,5),(3,7)\}$.\\
(b) $S=\{(1,-3),(3,-8)\}$.\\
(d) $S=\{(2,3),(4,5)\}$.

6.48. Consider the bases $S=\{(1,2),(2,3)\}$ and $S^{\prime}=\{(1,3),(1,4)\}$ of $\mathbf{R}^{2}$. Find the change-of-basis matrix:\\
(a) $P$ from $S$ to $S^{\prime}$.\\
(b) $Q$ from $S^{\prime}$ back to $S$.

6.49. Suppose that the $x$-axis and $y$-axis in the plane $\mathbf{R}^{2}$ are rotated counterclockwise $30^{\circ}$ to yield new $x^{\prime}$-axis and $y^{\prime}$-axis for the plane. Find

(a) The unit vectors in the direction of the new $x^{\prime}$-axis and $y^{\prime}$-axis.

(b) The change-of-basis matrix $P$ for the new coordinate system.

(c) The new coordinates of the points $A(1,3), B(2,-5), C(a, b)$.

6.50. Find the change-of-basis matrix $P$ from the usual basis $E$ of $\mathbf{R}^{3}$ to a basis $S$, the change-of-basis matrix $Q$ from $S$ back to $E$, and the coordinates of $v=(a, b, c)$ relative to $S$, where $S$ consists of the vectors:

(a) $u_{1}=(1,1,0), u_{2}=(0,1,2), u_{3}=(0,1,1)$.

(b) $u_{1}=(1,0,1), u_{2}=(1,1,2), u_{3}=(1,2,4)$.

(c) $u_{1}=(1,2,1), u_{2}=(1,3,4), u_{3}=(2,5,6)$.

6.51. Suppose $S_{1}, S_{2}, S_{3}$ are bases of $V$. Let $P$ and $Q$ be the change-of-basis matrices, respectively, from $S_{1}$ to $S_{2}$ and from $S_{2}$ to $S_{3}$. Prove that $P Q$ is the change-of-basis matrix from $S_{1}$ to $S_{3}$.

\section*{Linear Operators and Change of Basis}
6.52. Consider the linear operator $F$ on $\mathbf{R}^{2}$ defined by $F(x, y)=(5 x+y, 3 x-2 y)$ and the following bases of $\mathbf{R}^{2}$ :

$$
S=\{(1,2),(2,3)\} \quad \text { and } \quad S^{\prime}=\{(1,3),(1,4)\}
$$

(a) Find the matrix $A$ representing $F$ relative to the basis $S$.

(b) Find the matrix $B$ representing $F$ relative to the basis $S^{\prime}$.

(c) Find the change-of-basis matrix $P$ from $S$ to $S^{\prime}$.

(d) How are $A$ and $B$ related?

6.53. Let $A: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by the matrix $A=\left[\begin{array}{rr}1 & -1 \\ 3 & 2\end{array}\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to each of the following bases: (a) $S=\left\{(1,3)^{T},(2,5)^{T}\right\}$. (b) $S=\left\{(1,3)^{T},(2,4)^{T}\right\}$.

6.54. Let $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y)=(x-3 y, \quad 2 x-4 y)$. Find the matrix $A$ that represents $F$ relative to each of the following bases:\\
(a) $S=\{(2,5),(3,7)\}$\\
(b) $S=\{(2,3),(4,5)\}$.

6.55. Let $A: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be defined by the matrix $A=\left[\begin{array}{lll}1 & 3 & 1 \\ 2 & 7 & 4 \\ 1 & 4 & 3\end{array}\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to the basis $S=\left\{(1,1,1)^{T},(0,1,1)^{T},(1,2,3)^{T}\right\}$.

\section*{Similarity of Matrices}
6.56. Let $A=\left[\begin{array}{rr}1 & 1 \\ 2 & -3\end{array}\right]$ and $P=\left[\begin{array}{ll}1 & -2 \\ 3 & -5\end{array}\right]$.\\
(a) Find $B=P^{-1} A P$.\\
(b) Verify that $\operatorname{tr}(B)=\operatorname{tr}(A)$.\\
(c) Verify that $\operatorname{det}(B)=\operatorname{det}(A)$.

6.57. Find the trace and determinant of each of the following linear maps on $\mathbf{R}^{2}$ :\\
(a) $F(x, y)=(2 x-3 y, 5 x+4 y)$.\\
(b) $G(x, y)=(a x+b y, c x+d y)$.

6.58. Find the trace and determinant of each of the following linear maps on $\mathbf{R}^{3}$ :

(a) $F(x, y, z)=(x+3 y, 3 x-2 z, x-4 y-3 z)$.

(b) $G(x, y, z)=(y+3 z, 2 x-4 z, 5 x+7 y)$.

6.59. Suppose $S=\left\{u_{1}, u_{2}\right\}$ is a basis of $V$, and $T: V \rightarrow V$ is defined by $T\left(u_{1}\right)=3 u_{1}-2 u_{2}$ and $T\left(u_{2}\right)=u_{1}+4 u_{2}$. Suppose $S^{\prime}=\left\{w_{1}, w_{2}\right\}$ is a basis of $V$ for which $w_{1}=u_{1}+u_{2}$ and $w_{2}=2 u_{1}+3 u_{2}$.

(a) Find the matrices $A$ and $B$ representing $T$ relative to the bases $S$ and $S^{\prime}$, respectively.

(b) Find the matrix $P$ such that $B=P^{-1} A P$.

6.60. Let $A$ be a $2 \times 2$ matrix such that only $A$ is similar to itself. Show that $A$ is a scalar matrix, that is, that $A=\left[\begin{array}{ll}a & 0 \\ 0 & a\end{array}\right]$.

6.61. Show that all matrices similar to an invertible matrix are invertible. More generally, show that similar matrices have the same rank.

\section*{Matrix Representation of General Linear Mappings}
6.62. Find the matrix representation of each of the following linear maps relative to the usual basis for $\mathbf{R}^{n}$ :

(a) $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y, z)=(2 x-4 y+9 z, 5 x+3 y-2 z)$.

(b) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{4}$ defined by $F(x, y)=(3 x+4 y, 5 x-2 y, x+7 y, 4 x)$.

(c) $F: \mathbf{R}^{4} \rightarrow \mathbf{R}$ defined by $F\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=2 x_{1}+x_{2}-7 x_{3}-x_{4}$.

6.63. Let $G: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ be defined by $G(x, y, z)=(2 x+3 y-z, 4 x-y+2 z)$.

(a) Find the matrix $A$ representing $G$ relative to the bases

$$
S=\{(1,1,0),(1,2,3),(1,3,5)\} \quad \text { and } \quad S^{\prime}=\{(1,2),(2,3)\}
$$

(b) For any $v=(a, b, c)$ in $\mathbf{R}^{3}$, find $[v]_{S}$ and $[G(v)]_{S^{\prime}}$. (c) Verify that $A[v]_{S}=[G(v)]_{S^{\prime}}$.

6.64. Let $H: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ be defined by $H(x, y)=(2 x+7 y, x-3 y)$ and consider the following bases of $\mathbf{R}^{2}$ :

$$
S=\{(1,1),(1,2)\} \quad \text { and } \quad S^{\prime}=\{(1,4),(1,5)\}
$$

(a) Find the matrix $A$ representing $H$ relative to the bases $S$ and $S^{\prime}$.

(b) Find the matrix $B$ representing $H$ relative to the bases $S^{\prime}$ and $S$.

6.65. Let $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ be defined by $F(x, y, z)=(2 x+y-z, \quad 3 x-2 y+4 z)$.

(a) Find the matrix $A$ representing $F$ relative to the bases

$$
\left.S=\{(1,1,1), \quad(1,1,0), \quad(1,0,0)\} \quad \text { and } \quad S^{\prime}=(1,3), \quad(1,4)\right\}
$$

(b) Verify that, for any $v=(a, b, c)$ in $\mathbf{R}^{3}, A[v]_{S}=[F(v)]_{S^{\prime}}$.

6.66. Let $S$ and $S^{\prime}$ be bases of $V$, and let $\mathbf{1}_{V}$ be the identity mapping on $V$. Show that the matrix $A$ representing $\mathbf{1}_{V}$ relative to the bases $S$ and $S^{\prime}$ is the inverse of the change-of-basis matrix $P$ from $S$ to $S^{\prime}$; that is, $A=P^{-1}$.

6.67. Prove (a) Theorem 6.10, (b) Theorem 6.11, (c) Theorem 6.12, (d) Theorem 6.13. [Hint: See the proofs of the analogous Theorems 6.1 (Problem 6.9), 6.2 (Problem 6.10), 6.3 (Problem 6.11), and 6.7 (Problem 6.26).]

\section*{Miscellaneous Problems}
6.68. Suppose $F: V \rightarrow V$ is linear. A subspace $W$ of $V$ is said to be invariant under $F$ if $F(W) \subseteq W$. Suppose $W$ is invariant under $F$ and $\operatorname{dim} W=r$. Show that $F$ has a block triangular matrix representation $M=\left[\begin{array}{ll}A & B \\ 0 & C\end{array}\right]$\\
where $A$ is an $r \times r$ submatrix.

6.69. Suppose $V=U+W$, and suppose $U$ and $V$ are each invariant under a linear operator $F: V \rightarrow V$. Also, suppose $\operatorname{dim} U=r$ and $\operatorname{dim} W=S$. Show that $F$ has a block diagonal matrix representation $M=\left[\begin{array}{cc}A & 0 \\ 0 & B\end{array}\right]$\\
where $A$ and $B$ are $r \times r$ and $s \times s$ submatrices.

6.70. Two linear operators $F$ and $G$ on $V$ are said to be similar if there exists an invertible linear operator $T$ on $V$ such that $G=T^{-1} \circ F \circ T$. Prove

(a) $F$ and $G$ are similar if and only if, for any basis $S$ of $V,[F]_{S}$ and $[G]_{S}$ are similar matrices.

(b) If $F$ is diagonalizable (similar to a diagonal matrix), then any similar matrix $G$ is also diagonalizable.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $M=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ represents a matrix $M$ with rows $R_{1}, R_{2}, \ldots$

6.37.\\
(a) $A=[4,5 ; \quad 2,-1]$;\\
(b) $B=[220,487 ; \quad-98,-217] ;$\\
(d) $[v]_{S}=\left[\begin{array}{ll}9 a-2 b, & -4 a+b\end{array}\right]^{T}$ and $[F(v)]_{S}=\left[\begin{array}{ll}32 a+47 b, & -14 a-21 b\end{array}\right]^{T}$\\
(c) $P=[1,2 ; \quad 4,9]$;

6.38. (a) $B=[-6,-28 ; 4,15]$;

(b) $[v]_{S}=\left[4 a-b,-\frac{3}{2} a+\frac{1}{2} b\right]^{T}$ and $[A(v)]_{S}=\left[\begin{array}{lll}18 a-8 b, & \frac{1}{2}(-13 a+7 b)\end{array}\right]$

6.39. (a) $[\sqrt{2},-\sqrt{2} ; \quad \sqrt{2}, \sqrt{2}]$;

(d) $[1,2 ; \quad 18,-11]$

(b) $[0,1 ; \quad 1,0]$;

(c) $[3,7 ; \quad 5,-2]$;

6.40. (a) $[1,0,0 ; \quad 0,1,0 ; \quad 0,0,0]$;

(c) $[2,-7,-4 ; 3,1,4 ; \quad 6,-8,1]$

6.41. (a) $[1,3,5 ; \quad 0,-5,-10 ; \quad 0,3,6] ; \quad$ (b) $[0,1,2 ;-1,2,3 ; \quad 1,0,0]$;

(c) $[15,65,104 ;-49,-219,-351 ; 29,130,208]$

6.42. (a) $[1,1,2 ; \quad 1,3,2 ; \quad 1,5,2]$;

(b) $[0 ; \quad 2,14,22 ; \quad 0,-5,-8]$

6.43. (a) $[1,0,0 ; \quad 0,2,1 ; \quad 0,0,2]$;

(c) $[5,1,0 ; 0,5,2 ; 0,0,5]$

(b) $[0,1,0,0 ; 0 ; 0,0,0,-3 ; 0,0,3,0]$;

6.44. (a) $A=[0,-1 ; \quad 1,0] ; \quad$ (b) $A^{2}+I=0$

6.45. (a) $[a, 0, b, 0 ; \quad 0, a, 0, b ; \quad c, 0, d, 0 ; \quad 0, c, 0, d]$;

(b) $[a, c, 0,0 ; \quad b, d, 0,0 ; \quad 0,0, a, c ; \quad 0,0, b, d]$;

(c) $[0,-c, b, 0 ; \quad-b, a-d, 0, b ; \quad c, 0, d-a,-c ; \quad 0, c,-b, 0]$

6.47. (a) $[1,3 ; 2,5], \quad[-5,3 ; 2,-1], \quad[v]=\left[\begin{array}{ll}-5 a+3 b, & 2 a-b\end{array}\right]^{T}$;

(b) $[1,3 ;-3,-8], \quad[-8,-3 ; \quad 3,1], \quad[v]=\left[\begin{array}{ll}-8 a-3 b, & 3 a+b\end{array}\right]^{T}$;

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-232(2)}
\end{center}

6.48. (a) $P=[3,5 ; \quad-1,-2] ; \quad$ (b) $Q=[2,5 ; \quad-1,-3]$

6.49. Here $K=\sqrt{3}$.

(a) $\frac{1}{2}(K, 1), \quad \frac{1}{2}(-1, K)$;

(b) $P=\frac{1}{2}[K,-1 ; \quad 1, K]$;

(c) $\frac{1}{2}[K+3,3 K-1]^{T}, \quad \frac{1}{2}[2 K-5,-5 K-2]^{T}, \quad \frac{1}{2}[a K+b, b K-a]^{T}$

6.50. $P$ is the matrix whose columns are $u_{1}, u_{2}, u_{3}, Q=P^{-1},[v]=Q[a, b, c]^{T}$.

(a) $Q=[1,0,0 ; \quad 1,-1,1 ; \quad-2,2,-1], \quad[v]=\left[\begin{array}{ccc}a, \quad a-b+c, \quad-2 a+2 b-c\end{array}\right]^{T}$;

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-232(3)}
\end{center}

\includegraphics[max width=\textwidth, center]{2024_04_03_de2bde501961f6000cc6g-232(1)}\\
6.52. (a) $[-23,-39 ; \quad 15,26]$;\\
(b) $[35,41 ;-27,-32]$;\\
(c) $[3,5 ; \quad-1,-2]$;\\
(d) $B=P^{-1} A P$\\
6.53. (a) $[28,47 ;-15,-25]$;\\
(b) $\left[13,18 ; \quad-\frac{15}{2},-10\right]$\\
6.54. (a) $[43,60 ;-33,-46]$;\\
(b) $\frac{1}{2}[3,7 ;-5,-9]$

6.55. $[10,8,20 ; \quad 13,11,28 ; \quad-5,-4,-10]$

6.56. (a) $[-34,57 ; \quad-19,32] ; \quad$ (b) $\operatorname{tr}(B)=\operatorname{tr}(A)=-2 ; \quad$ (c) $\operatorname{det}(B)=\operatorname{det}(A)=-5$

6.57. (a) $\operatorname{tr}(F)=6, \operatorname{det}(F)=23 ; \quad$ (b) $\operatorname{tr}(G)=a+d, \operatorname{det}(G)=a d-b c$

6.58. (a) $\operatorname{tr}(F)=-2, \operatorname{det}(F)=13 ; \quad$ (b) $\operatorname{tr}(G)=0, \operatorname{det}(G)=22$

6.59. (a) $A=[3,1 ; \quad-2,4], \quad B=[8,11 ; \quad-2,-1] ; \quad$ (b) $P=[1,2 ; \quad 1,3]$

6.62. (a) $[2,-4,9 ; \quad 5,3,-2] ; \quad$ (b) $[3,5,1,4 ; 4,-2,7,0] ; \quad$ (c) $[2,1,-7,-1]$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-232}
\end{center}

6.64. (a) $A=[47,85 ; \quad-38,-69] ; \quad$ (b) $B=[71,88 ; \quad-41,-51]$

6.65. $A=[3,11,5 ; \quad-1,-8,-3]$

\section*{Inner Product Spaces, Orthogonality}
\subsection*{7.1 Introduction}
The definition of a vector space $V$ involves an arbitrary field $K$. Here we first restrict $K$ to be the real field $\mathbf{R}$, in which case $V$ is called a real vector space; in the last sections of this chapter, we extend our results to the case where $K$ is the complex field $\mathbf{C}$, in which case $V$ is called a complex vector space. Also, we adopt the previous notation that

$$
\begin{array}{ll}
u, v, w & \text { are vectors in } V \\
a, b, c, k & \text { are scalars in } K
\end{array}
$$

Furthermore, the vector spaces $V$ in this chapter have finite dimension unless otherwise stated or implied.

Recall that the concepts of "length" and "orthogonality" did not appear in the investigation of arbitrary vector spaces $V$ (although they did appear in Section 1.4 on the spaces $\mathbf{R}^{n}$ and $\mathbf{C}^{n}$ ). Here we place an additional structure on a vector space $V$ to obtain an inner product space, and in this context these concepts are defined.

\subsection*{7.2 Inner Product Spaces}
We begin with a definition.

DEFINITION: $\quad$ Let $V$ be a real vector space. Suppose to each pair of vectors $u, v \in V$ there is assigned a real number, denoted by $\langle u, v\rangle$. This function is called a (real) inner product on $V$ if it satisfies the following axioms:

$\left[\mathrm{I}_{1}\right]$ (Linear Property): $\left\langle a u_{1}+b u_{2}, v\right\rangle=a\left\langle u_{1}, v\right\rangle+b\left\langle u_{2}, v\right\rangle$.

$\left[\mathrm{I}_{2}\right]$ (Symmetric Property): $\langle u, v\rangle=\langle v, u\rangle$.

$\left[\mathrm{I}_{3}\right]$ (Positive Definite Property): $\langle u, u\rangle \geq 0$.; and $\langle u, u\rangle=0$ if and only if $u=0$.

The vector space $V$ with an inner product is called a (real) inner product space.

Axiom $\left[\mathrm{I}_{1}\right]$ states that an inner product function is linear in the first position. Using $\left[\mathrm{I}_{1}\right]$ and the symmetry axiom $\left[\mathrm{I}_{2}\right]$, we obtain

$$
\left\langle u, \quad c v_{1}+d v_{2}\right\rangle=\left\langle c v_{1}+d v_{2}, \quad u\right\rangle=c\left\langle v_{1}, u\right\rangle+d\left\langle v_{2}, u\right\rangle=c\left\langle u, v_{1}\right\rangle+d\left\langle u, v_{2}\right\rangle
$$

That is, the inner product function is also linear in its second position. Combining these two properties and using induction yields the following general formula:

$$
\left\langle\sum_{i} a_{i} u_{i}, \quad \sum_{j} b_{j} v_{j}\right\rangle=\sum_{i} \sum_{j} a_{i} b_{j}\left\langle u_{i}, v_{j}\right\rangle
$$

That is, an inner product of linear combinations of vectors is equal to a linear combination of the inner products of the vectors.

EXAMPLE 7.1 Let $V$ be a real inner product space. Then, by linearity,

$$
\begin{aligned}
\left\langle 3 u_{1}-4 u_{2}, 2 v_{1}-5 v_{2}+6 v_{3}\right\rangle= & 6\left\langle u_{1}, v_{1}\right\rangle-15\left\langle u_{1}, v_{2}\right\rangle+18\left\langle u_{1}, v_{3}\right\rangle \\
& -8\left\langle u_{2}, v_{1}\right\rangle+20\left\langle u_{2}, v_{2}\right\rangle-24\left\langle u_{2}, v_{3}\right\rangle \\
\langle 2 u-5 v, 4 u+6 v\rangle=8\langle u, u\rangle+ & 12\langle u, v\rangle-20\langle v, u\rangle-30\langle v, v\rangle \\
= & 8\langle u, u\rangle-8\langle v, u\rangle-30\langle v, v\rangle
\end{aligned}
$$

Observe that in the last equation we have used the symmetry property that $\langle u, v\rangle=\langle v, u\rangle$.

Remark: Axiom $\left[\mathrm{I}_{1}\right]$ by itself implies $\langle 0,0\rangle=\langle 0 v, 0\rangle=0\langle v, 0\rangle=0$. Thus, $\left[\mathrm{I}_{1}\right],\left[\mathrm{I}_{2}\right],\left[\mathrm{I}_{3}\right]$ are equivalent to $\left[\mathrm{I}_{1}\right],\left[\mathrm{I}_{2}\right]$, and the following axiom:

$\left[I_{3}^{\prime}\right]$ If $u \neq 0$, then $\langle u, u\rangle$ is positive.

That is, a function satisfying $\left[I_{1}\right],\left[I_{2}\right],\left[I_{3}^{\prime}\right]$ is an inner product.

\section*{Norm of a Vector}
By the third axiom $\left[\mathrm{I}_{3}\right]$ of an inner product, $\langle u, u\rangle$ is nonnegative for any vector $u$. Thus, its positive square root exists. We use the notation

$$
\|u\|=\sqrt{\langle u, u\rangle}
$$

This nonnegative number is called the norm or length of $u$. The relation $\|u\|^{2}=\langle u, u\rangle$ will be used frequently.

Remark: If $\|u\|=1$ or, equivalently, if $\langle u, u\rangle=1$, then $u$ is called a unit vector and it is said to be normalized. Every nonzero vector $v$ in $V$ can be multiplied by the reciprocal of its length to obtain the unit vector

$$
\hat{v}=\frac{1}{\|v\|} v
$$

which is a positive multiple of $v$. This process is called normalizing $v$.

\subsection*{7.3 Examples of Inner Product Spaces}
This section lists the main examples of inner product spaces used in this text.

\section*{Euclidean $\boldsymbol{n}$-Space $\mathbf{R}^{\boldsymbol{n}}$}
Consider the vector space $\mathbf{R}^{n}$. The dot product or scalar product in $\mathbf{R}^{n}$ is defined by

$$
u \cdot v=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}
$$

where $u=\left(a_{i}\right)$ and $v=\left(b_{i}\right)$. This function defines an inner product on $\mathbf{R}^{n}$. The norm $\|u\|$ of the vector $u=\left(a_{i}\right)$ in this space is as follows:

$$
\|u\|=\sqrt{u \cdot u}=\sqrt{a_{1}^{2}+a_{2}^{2}+\cdots+a_{n}^{2}}
$$

On the other hand, by the Pythagorean theorem, the distance from the origin $\mathrm{O}$ in $\mathbf{R}^{3}$ to a point $P(a, b, c)$ is given by $\sqrt{a^{2}+b^{2}+c^{2}}$. This is precisely the same as the above-defined norm of the vector $v=(a, b, c)$ in $\mathbf{R}^{3}$. Because the Pythagorean theorem is a consequence of the axioms of

Euclidean geometry, the vector space $\mathbf{R}^{n}$ with the above inner product and norm is called Euclidean $n$-space. Although there are many ways to define an inner product on $\mathbf{R}^{n}$, we shall assume this inner product unless otherwise stated or implied. It is called the usual (or standard) inner product on $\mathbf{R}^{n}$.

Remark: Frequently the vectors in $\mathbf{R}^{n}$ will be represented by column vectors-that is, by $n \times 1$ column matrices. In such a case, the formula

$$
\langle u, v\rangle=u^{T} v
$$

defines the usual inner product on $\mathbf{R}^{n}$.

EXAMPLE 7.2 Let $u=(1,3,-4,2), v=(4,-2,2,1), w=(5,-1,-2,6)$ in $\mathbf{R}^{4}$.

(a) Show $\langle 3 u-2 v, w\rangle=3\langle u, w\rangle-2\langle v, w\rangle$.

By definition,

$$
\langle u, w\rangle=5-3+8+12=22 \quad \text { and } \quad\langle v, w\rangle=20+2-4+6=24
$$

Note that $3 u-2 v=(-5,13,-16,4)$. Thus,

$$
\langle 3 u-2 v, w\rangle=-25-13+32+24=18
$$

As expected, $3\langle u, w\rangle-2\langle v, w\rangle=3(22)-2(24)=18=\langle 3 u-2 v, w\rangle$.

(b) Normalize $u$ and $v$.

By definition,

$$
\|u\|=\sqrt{1+9+16+4}=\sqrt{30} \quad \text { and } \quad\|v\|=\sqrt{16+4+4+1}=5
$$

We normalize $u$ and $v$ to obtain the following unit vectors in the directions of $u$ and $v$, respectively:

$$
\hat{u}=\frac{1}{\|u\|} u=\left(\frac{1}{\sqrt{30}}, \frac{3}{\sqrt{30}}, \frac{-4}{\sqrt{30}}, \frac{2}{\sqrt{30}}\right) \quad \text { and } \quad \hat{v}=\frac{1}{\|v\|} v=\left(\frac{4}{5}, \frac{-2}{5}, \frac{2}{5}, \frac{1}{5}\right)
$$

\section*{Function Space $\boldsymbol{C}[\mathbf{a}, \boldsymbol{b}]$ and Polynomial Space $\mathbf{P}(\boldsymbol{t})$}
The notation $C[a, b]$ is used to denote the vector space of all continuous functions on the closed interval $[a, b]$ - that is, where $a \leq t \leq b$. The following defines an inner product on $C[a, b]$, where $f(t)$ and $g(t)$ are functions in $C[a, b]$ :

$$
\langle f, g\rangle=\int_{a}^{b} f(t) g(t) d t
$$

It is called the usual inner product on $C[a, b]$.

The vector space $\mathbf{P}(t)$ of all polynomials is a subspace of $C[a, b]$ for any interval $[a, b]$, and hence, the above is also an inner product on $\mathbf{P}(t)$.

\section*{EXAMPLE 7.3}
Consider $f(t)=3 t-5$ and $g(t)=t^{2}$ in the polynomial space $\mathbf{P}(t)$ with inner product

$$
\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t
$$

(a) Find $\langle f, g\rangle$.

We have $f(t) g(t)=3 t^{3}-5 t^{2}$. Hence,

$$
\langle f, g\rangle=\int_{0}^{1}\left(3 t^{3}-5 t^{2}\right) d t=\frac{3}{4} t^{4}-\left.\frac{5}{3} t^{3}\right|_{0} ^{1}=\frac{3}{4}-\frac{5}{3}=-\frac{11}{12}
$$

(b) Find $\|f\|$ and $\|g\|$.

$$
\begin{aligned}
& \text { We have }[f(t)]^{2}=f(t) f(t)=9 t^{2}-30 t+25 \text { and }[g(t)]^{2}=t^{4} \text {. Then } \\
& \|f\|^{2}=\langle f, f\rangle=\int_{0}^{1}\left(9 t^{2}-30 t+25\right) d t=3 t^{3}-15 t^{2}+\left.25 t\right|_{0} ^{1}=13 \\
& \|g\|^{2}=\langle g, g\rangle=\int_{0}^{1} t^{4} d t=\left.\frac{1}{5} t^{5}\right|_{0} ^{1}=\frac{1}{5}
\end{aligned}
$$

Therefore, $\|f\|=\sqrt{13}$ and $\|g\|=\sqrt{\frac{1}{5}}=\frac{1}{5} \sqrt{5}$.

Matrix Space $\mathbf{M}=\mathbf{M}_{\boldsymbol{m}, \boldsymbol{n}}$

Let $\mathbf{M}=\mathbf{M}_{m, n}$, the vector space of all real $m \times n$ matrices. An inner product is defined on $\mathbf{M}$ by

$$
\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)
$$

where, as usual, $\operatorname{tr}()$ is the trace-the sum of the diagonal elements. If $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$, then

$$
\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} b_{i j} \quad \text { and } \quad\|A\|^{2}=\langle A, A\rangle=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j}^{2}
$$

That is, $\langle A, B\rangle$ is the sum of the products of the corresponding entries in $A$ and $B$ and, in particular, $\langle A, A\rangle$ is the sum of the squares of the entries of $A$.

\section*{Hilbert Space}
Let $V$ be the vector space of all infinite sequences of real numbers $\left(a_{1}, a_{2}, a_{3}, \ldots\right)$ satisfying

$$
\sum_{i=1}^{\infty} a_{i}^{2}=a_{1}^{2}+a_{2}^{2}+\cdots<\infty
$$

that is, the sum converges. Addition and scalar multiplication are defined in $V$ componentwise; that is, if

then

$$
\begin{gathered}
u=\left(a_{1}, a_{2}, \ldots\right) \quad \text { and } \quad v=\left(b_{1}, b_{2}, \ldots\right) \\
u+v=\left(a_{1}+b_{1}, \quad a_{2}+b_{2}, \ldots\right) \quad \text { and } \quad k u=\left(k a_{1}, k a_{2}, \ldots\right)
\end{gathered}
$$

An inner product is defined in $v$ by

$$
\langle u, v\rangle=a_{1} b_{1}+a_{2} b_{2}+\cdots
$$

The above sum converges absolutely for any pair of points in $V$. Hence, the inner product is well defined. This inner product space is called $l_{2}$-space or Hilbert space.

\subsection*{7.4 Cauchy-Schwarz Inequality, Applications}
The following formula (proved in Problem 7.8) is called the Cauchy-Schwarz inequality or Schwarz inequality. It is used in many branches of mathematics.

THEOREM 7.1: (Cauchy-Schwarz) For any vectors $u$ and $v$ in an inner product space $V$,

$$
\langle u, v\rangle^{2} \leq\langle u, u\rangle\langle v, v\rangle \quad \text { or } \quad|\langle u, v\rangle| \leq\|u\|\|v\|
$$

Next we examine this inequality in specific cases.

\section*{EXAMPLE 7.4}
(a) Consider any real numbers $a_{1}, \ldots, a_{n}, b_{1}, \ldots, b_{n}$. Then, by the Cauchy-Schwarz inequality,

$$
\left(a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}\right)^{2} \leq\left(a_{1}^{2}+\cdots+a_{n}^{2}\right)\left(b_{1}^{2}+\cdots+b_{n}^{2}\right)
$$

That is, $(u \cdot v)^{2} \leq\|u\|^{2}\|v\|^{2}$, where $u=\left(a_{i}\right)$ and $v=\left(b_{i}\right)$.\\
(b) Let $f$ and $g$ be continuous functions on the unit interval $[0,1]$. Then, by the Cauchy-Schwarz inequality,

$$
\left[\int_{0}^{1} f(t) g(t) d t\right]^{2} \leq \int_{0}^{1} f^{2}(t) d t \int_{0}^{1} g^{2}(t) d t
$$

That is, $(\langle f, g\rangle)^{2} \leq\|f\|^{2}\|v\|^{2}$. Here $V$ is the inner product space $C[0,1]$.

The next theorem (proved in Problem 7.9) gives the basic properties of a norm. The proof of the third property requires the Cauchy-Schwarz inequality.

THEOREM 7.2: Let $V$ be an inner product space. Then the norm in $V$ satisfies the following properties:

$\left[\mathrm{N}_{1}\right] \quad\|v\| \geq 0$; and $\|v\|=0$ if and only if $v=0$.

$\left[\mathrm{N}_{2}\right] \quad\|k v\|=|k|\|v\|$.

$\left[\mathrm{N}_{3}\right] \quad\|u+v\| \leq\|u\|+\|v\|$.

The property $\left[\mathrm{N}_{3}\right]$ is called the triangle inequality, because if we view $u+v$ as the side of the triangle formed with sides $u$ and $v$ (as shown in Fig. 7-1), then $\left[\mathrm{N}_{3}\right]$ states that the length of one side of a triangle cannot be greater than the sum of the lengths of the other two sides.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-237}
\end{center}

Figure 7-1

\section*{Angle Between Vectors}
For any nonzero vectors $u$ and $v$ in an inner product space $V$, the angle between $u$ and $v$ is defined to be the angle $\theta$ such that $0 \leq \theta \leq \pi$ and

$$
\cos \theta=\frac{\langle u, v\rangle}{\|u\|\|v\|}
$$

By the Cauchy-Schwartz inequality, $-1 \leq \cos \theta \leq 1$, and so the angle exists and is unique.

\section*{EXAMPLE 7.5}
(a) Consider vectors $u=(2,3,5)$ and $v=(1,-4,3)$ in $\mathbf{R}^{3}$. Then

$$
\langle u, v\rangle=2-12+15=5, \quad\|u\|=\sqrt{4+9+25}=\sqrt{38}, \quad\|v\|=\sqrt{1+16+9}=\sqrt{26}
$$

Then the angle $\theta$ between $u$ and $v$ is given by

$$
\cos \theta=\frac{5}{\sqrt{38} \sqrt{26}}
$$

Note that $\theta$ is an acute angle, because $\cos \theta$ is positive.

(b) Let $f(t)=3 t-5$ and $g(t)=t^{2}$ in the polynomial space $\mathbf{P}(t)$ with inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$. By Example 7.3,

$$
\langle f, g\rangle=-\frac{11}{12}, \quad \quad\|f\|=\sqrt{13}, \quad\|g\|=\frac{1}{5} \sqrt{5}
$$

Then the "angle" $\theta$ between $f$ and $g$ is given by

$$
\cos \theta=\frac{-\frac{11}{12}}{(\sqrt{13})\left(\frac{1}{5} \sqrt{5}\right)}=-\frac{55}{12 \sqrt{13} \sqrt{5}}
$$

Note that $\theta$ is an obtuse angle, because $\cos \theta$ is negative.

\subsection*{7.5 Orthogonality}
Let $V$ be an inner product space. The vectors $u, v \in V$ are said to be orthogonal and $u$ is said to be orthogonal to $v$ if

$$
\langle u, v\rangle=0
$$

The relation is clearly symmetric - if $u$ is orthogonal to $v$, then $\langle v, u\rangle=0$, and so $v$ is orthogonal to $u$. We note that $0 \in V$ is orthogonal to every $v \in V$, because

$$
\langle 0, v\rangle=\langle 0 v, v\rangle=0\langle v, v\rangle=0
$$

Conversely, if $u$ is orthogonal to every $v \in V$, then $\langle u, u\rangle=0$ and hence $u=0$ by $\left[\mathrm{I}_{3}\right]$. Observe that $u$ and $v$ are orthogonal if and only if $\cos \theta=0$, where $\theta$ is the angle between $u$ and $v$. Also, this is true if and only if $u$ and $v$ are "perpendicular" - that is, $\theta=\pi / 2\left(\right.$ or $\left.\theta=90^{\circ}\right)$.

\section*{EXAMPLE 7.6}
(a) Consider the vectors $u=(1,1,1), v=(1,2,-3)$, $w=(1,-4,3)$ in $\mathbf{R}^{3}$. Then

$$
\langle u, v\rangle=1+2-3=0, \quad\langle u, w\rangle=1-4+3=0, \quad\langle v, w\rangle=1-8-9=-16
$$

Thus, $u$ is orthogonal to $v$ and $w$, but $v$ and $w$ are not orthogonal.

(b) Consider the functions $\sin t$ and $\cos t$ in the vector space $C[-\pi, \pi]$ of continuous functions on the closed interval $[-\pi, \pi]$. Then

$$
\langle\sin t, \cos t\rangle=\int_{-\pi}^{\pi} \sin t \cos t d t=\left.\frac{1}{2} \sin ^{2} t\right|_{-\pi} ^{\pi}=0-0=0
$$

Thus, $\sin t$ and $\cos t$ are orthogonal functions in the vector space $C[-\pi, \pi]$.

Remark: A vector $w=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is orthogonal to $u=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ in $\mathrm{R}^{n}$ if

$$
\langle u, w\rangle=a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=0
$$

That is, $w$ is orthogonal to $u$ if $w$ satisfies a homogeneous equation whose coefficients are the elements of $u$.

EXAMPLE 7.7 Find a nonzero vector $w$ that is orthogonal to $u_{1}=(1,2,1)$ and $u_{2}=(2,5,4)$ in $\mathrm{R}^{3}$.

Let $w=(x, y, z)$. Then we want $\left\langle u_{1}, w\right\rangle=0$ and $\left\langle u_{2}, w\right\rangle=0$. This yields the homogeneous system

$$
\begin{aligned}
& x+2 y+z=0 \quad \text { or } \quad x+2 y+z=0 \\
& 2 x+5 y+4 z=0 \quad y+2 z=0
\end{aligned}
$$

Here $z$ is the only free variable in the echelon system. Set $z=1$ to obtain $y=-2$ and $x=3$. Thus, $w=(3,-2,1)$ is a desired nonzero vector orthogonal to $u_{1}$ and $u_{2}$.

Any multiple of $w$ will also be orthogonal to $u_{1}$ and $u_{2}$. Normalizing $w$, we obtain the following unit vector orthogonal to $u_{1}$ and $u_{2}$ :

$$
\hat{w}=\frac{w}{\|w\|}=\left(\frac{3}{\sqrt{14}},-\frac{2}{\sqrt{14}}, \frac{1}{\sqrt{14}}\right)
$$

\section*{Orthogonal Complements}
Let $S$ be a subset of an inner product space $V$. The orthogonal complement of $S$, denoted by $S^{\perp}$ (read " $S$ perp"') consists of those vectors in $V$ that are orthogonal to every vector $u \in S$; that is,

$$
S^{\perp}=\{v \in V:\langle v, u\rangle=0 \text { for every } u \in S\}
$$

In particular, for a given vector $u$ in $V$, we have

$$
u^{\perp}=\{v \in V:\langle v, u\rangle=0\}
$$

that is, $u^{\perp}$ consists of all vectors in $V$ that are orthogonal to the given vector $u$.

We show that $S^{\perp}$ is a subspace of $V$. Clearly $0 \in S^{\perp}$, because 0 is orthogonal to every vector in $V$. Now suppose $v, w \in S^{\perp}$. Then, for any scalars $a$ and $b$ and any vector $u \in S$, we have

$$
\langle a v+b w, u\rangle=a\langle v, u\rangle+b\langle w, u\rangle=a \cdot 0+b \cdot 0=0
$$

Thus, $a v+b w \in S^{\perp}$, and therefore $S^{\perp}$ is a subspace of $V$.

We state this result formally.

PROPOSITION 7.3: $\quad$ Let $S$ be a subset of a vector space $V$. Then $S^{\perp}$ is a subspace of $V$.

Remark 1: Suppose $u$ is a nonzero vector in $\mathbf{R}^{3}$. Then there is a geometrical description of $u^{\perp}$. Specifically, $u^{\perp}$ is the plane in $\mathbf{R}^{3}$ through the origin $O$ and perpendicular to the vector $u$. This is shown in Fig. 7-2.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-239}
\end{center}

Figure 7-2

Remark 2: $\quad$ Let $W$ be the solution space of an $m \times n$ homogeneous system $A X=0$, where $A=\left[a_{i j}\right]$ and $X=\left[x_{i}\right]$. Recall that $W$ may be viewed as the kernel of the linear mapping $A: \mathbf{R}^{n} \rightarrow \mathbf{R}^{m}$. Now we can give another interpretation of $W$ using the notion of orthogonality. Specifically, each solution vector $w=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is orthogonal to each row of $A$; hence, $W$ is the orthogonal complement of the row space of $A$.

EXAMPLE 7.8 Find a basis for the subspace $u^{\perp}$ of $\mathbf{R}^{3}$, where $u=(1,3,-4)$.

Note that $u^{\perp}$ consists of all vectors $w=(x, y, z)$ such that $\langle u, w\rangle=0$, or $x+3 y-4 z=0$. The free variables are $y$ and $z$.

(1) Set $y=1, z=0$ to obtain the solution $w_{1}=(-3,1,0)$.

(2) Set $y=0, z=1$ to obtain the solution $w_{1}=(4,0,1)$.

The vectors $w_{1}$ and $w_{2}$ form a basis for the solution space of the equation, and hence a basis for $u^{\perp}$.

Suppose $W$ is a subspace of $V$. Then both $W$ and $W^{\perp}$ are subspaces of $V$. The next theorem, whose proof (Problem 7.28) requires results of later sections, is a basic result in linear algebra.

THEOREM 7.4: $\quad$ Let $W$ be a subspace of $V$. Then $V$ is the direct sum of $W$ and $W^{\perp}$; that is, $V=W \oplus W^{\perp}$.

\subsection*{7.6 Orthogonal Sets and Bases}
Consider a set $S=\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ of nonzero vectors in an inner product space $V . S$ is called orthogonal if each pair of vectors in $S$ are orthogonal, and $S$ is called orthonormal if $S$ is orthogonal and each vector in $S$ has unit length. That is,

(i) Orthogonal: $\left\langle u_{i}, u_{j}\right\rangle=0$ for $i \neq j$

(ii) Orthonormal: $\left\langle u_{i}, u_{j}\right\rangle= \begin{cases}0 & \text { for } i \neq j \\ 1 & \text { for } i=j\end{cases}$

Normalizing an orthogonal set $S$ refers to the process of multiplying each vector in $S$ by the reciprocal of its length in order to transform $S$ into an orthonormal set of vectors.

The following theorems apply.

THEOREM 7.5: Suppose $S$ is an orthogonal set of nonzero vectors. Then $S$ is linearly independent.

THEOREM 7.6: (Pythagoras) Suppose $\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ is an orthogonal set of vectors. Then

$$
\left\|u_{1}+u_{2}+\cdots+u_{r}\right\|^{2}=\left\|u_{1}\right\|^{2}+\left\|u_{2}\right\|^{2}+\cdots+\left\|u_{r}\right\|^{2}
$$

These theorems are proved in Problems 7.15 and 7.16, respectively. Here we prove the Pythagorean theorem in the special and familiar case for two vectors. Specifically, suppose $\langle u, v\rangle=0$. Then

$$
\|u+v\|^{2}=\langle u+v, \quad u+v\rangle=\langle u, u\rangle+2\langle u, v\rangle+\langle v, v\rangle=\langle u, u\rangle+\langle v, v\rangle=\|u\|^{2}+\|v\|^{2}
$$

which gives our result.

\section*{EXAMPLE 7.9}
(a) Let $E=\left\{e_{1}, e_{2}, e_{3}\right\}=\{(1,0,0),(0,1,0),(0,0,1)\}$ be the usual basis of Euclidean space $\mathbf{R}^{3}$. It is clear that

$$
\left\langle e_{1}, e_{2}\right\rangle=\left\langle e_{1}, e_{3}\right\rangle=\left\langle e_{2}, e_{3}\right\rangle=0 \quad \text { and } \quad\left\langle e_{1}, e_{1}\right\rangle=\left\langle e_{2}, e_{2}\right\rangle=\left\langle e_{3}, e_{3}\right\rangle=1
$$

Namely, $E$ is an orthonormal basis of $\mathbf{R}^{3}$. More generally, the usual basis of $\mathbf{R}^{n}$ is orthonormal for every $n$.

(b) Let $V=C[-\pi, \pi]$ be the vector space of continuous functions on the interval $-\pi \leq t \leq \pi$ with inner product defined by $\langle f, g\rangle=\int_{-\pi}^{\pi} f(t) g(t) d t$. Then the following is a classical example of an orthogonal set in $V$ :

$$
\{1, \cos t, \cos 2 t, \cos 3 t, \ldots, \sin t, \sin 2 t, \sin 3 t, \ldots\}
$$

This orthogonal set plays a fundamental role in the theory of Fourier series.

\section*{Orthogonal Basis and Linear Combinations, Fourier Coefficients}
Let $S$ consist of the following three vectors in $\mathbf{R}^{3}$ :

$$
u_{1}=(1,2,1), \quad u_{2}=(2,1,-4), \quad u_{3}=(3,-2,1)
$$

The reader can verify that the vectors are orthogonal; hence, they are linearly independent. Thus, $S$ is an orthogonal basis of $\mathbf{R}^{3}$.

Suppose we want to write $v=(7,1,9)$ as a linear combination of $u_{1}, u_{2}, u_{3}$. First we set $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknowns $x_{1}, x_{2}, x_{3}$ as follows:


\begin{equation*}
v=x_{1} u_{1}+x_{2} u_{2}+x_{3} u_{3} \quad \text { or } \quad(7,1,9)=x_{1}(1,2,1)+x_{2}(2,1,-4)+x_{3}(3,-2,1) \tag{*}
\end{equation*}


We can proceed in two ways.

METHOD 1: $\quad$ Expand (*) (as in Chapter 3) to obtain the system

$$
x_{1}+2 x_{2}+3 x_{3}=7, \quad 2 x_{1}+x_{2}-2 x_{3}=1, \quad x_{1}-4 x_{2}+x 3=7
$$

Solve the system by Gaussian elimination to obtain $x_{1}=3, x_{2}=-1, x_{3}=2$. Thus, $v=3 u_{1}-u_{2}+2 u_{3}$.

METHOD 2: (This method uses the fact that the basis vectors are orthogonal, and the arithmetic is much simpler.) If we take the inner product of each side of (*) with respect to $u_{i}$, we get

$$
\left\langle v, u_{i}\right\rangle=\left\langle x_{1} u_{2}+x_{2} u_{2}+x_{3} u_{3}, u_{i}\right\rangle \quad \text { or } \quad\left\langle v, u_{i}\right\rangle=x_{i}\left\langle u_{i}, u_{i}\right\rangle \quad \text { or } \quad x_{i}=\frac{\left\langle v, u_{i}\right\rangle}{\left\langle u_{i}, u_{i}\right\rangle}
$$

Here two terms drop out, because $u_{1}, u_{2}, u_{3}$ are orthogonal. Accordingly,

$$
\begin{gathered}
x_{1}=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle}=\frac{7+2+9}{1+4+1}=\frac{18}{6}=3, \quad x_{2}=\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle}=\frac{14+1-36}{4+1+16}=\frac{-21}{21}=-1 \\
x_{3}=\frac{\left\langle v, u_{3}\right\rangle}{\left\langle u_{3}, u_{3}\right\rangle}=\frac{21-2+9}{9+4+1}=\frac{28}{14}=2
\end{gathered}
$$

Thus, again, we get $v=3 u_{1}-u_{2}+2 u_{3}$.

The procedure in Method 2 is true in general. Namely, we have the following theorem (proved in Problem 7.17).

THEOREM 7.7: Let $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ be an orthogonal basis of $V$. Then, for any $v \in V$,

$$
v=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle} u_{1}+\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle} u_{2}+\cdots+\frac{\left\langle v, u_{n}\right\rangle}{\left\langle u_{n}, u_{n}\right\rangle} u_{n}
$$

Remark: The scalar $k_{i} \equiv \frac{\left\langle v, u_{i}\right\rangle}{\left\langle u_{i}, u_{i}\right\rangle}$ is called the Fourier coefficient of $v$ with respect to $u_{i}$, because it is analogous to a coefficient in the Fourier series of a function. This scalar also has a geometric interpretation, which is discussed below.

\section*{Projections}
Let $V$ be an inner product space. Suppose $w$ is a given nonzero vector in $V$, and suppose $v$ is another vector. We seek the "projection of $v$ along $w$," which, as indicated in Fig. 7-3(a), will be the multiple $c w$ of $w$ such that $v^{\prime}=v-c w$ is orthogonal to $w$. This means

$$
\langle v-c w, \quad w\rangle=0 \quad \text { or } \quad\langle v, w\rangle-c\langle w, w\rangle=0 \quad \text { or } \quad c=\frac{\langle v, w\rangle}{\langle w, w\rangle}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-241}
\end{center}

(b)

Figure 7-3

Accordingly, the projection of $v$ along $w$ is denoted and defined by

$$
\operatorname{proj}(v, w)=c w=\frac{\langle v, w\rangle}{\langle w, w\rangle} w
$$

Such a scalar $c$ is unique, and it is called the Fourier coefficient of $v$ with respect to $w$ or the component of $v$ along $w$.

The above notion is generalized as follows (see Problem 7.25).

THEOREM 7.8: Suppose $w_{1}, w_{2}, \ldots, w_{r}$ form an orthogonal set of nonzero vectors in $V$. Let $v$ be any vector in $V$. Define

$$
v^{\prime}=v-\left(c_{1} w_{1}+c_{2} w_{2}+\cdots+c_{r} w_{r}\right)
$$

where

$$
c_{1}=\frac{\left\langle v, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle}, \quad c_{2}=\frac{\left\langle v, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle}, \quad \ldots, \quad c_{r}=\frac{\left\langle v, w_{r}\right\rangle}{\left\langle w_{r}, w_{r}\right\rangle}
$$

Then $v^{\prime}$ is orthogonal to $w_{1}, w_{2}, \ldots, w_{r}$.

Note that each $c_{i}$ in the above theorem is the component (Fourier coefficient) of $v$ along the given $w_{i}$.

Remark: The notion of the projection of a vector $v \in V$ along a subspace $W$ of $V$ is defined as follows. By Theorem 7.4, $V=W \oplus W^{\perp}$. Hence, $v$ may be expressed uniquely in the form

$$
v=w+w^{\prime}, \quad \text { where } \quad w \in W \quad \text { and } \quad w^{\prime} \in W^{\perp}
$$

We define $w$ to be the projection of $v$ along $W$, and denote it by $\operatorname{proj}(v, W)$, as pictured in Fig. 7-2(b). In particular, if $W=\operatorname{span}\left(w_{1}, w_{2}, \ldots, w_{r}\right)$, where the $w_{i}$ form an orthogonal set, then

$$
\operatorname{proj}(v, W)=c_{1} w_{1}+c_{2} w_{2}+\cdots+c_{r} w_{r}
$$

Here $c_{i}$ is the component of $v$ along $w_{i}$, as above.

\subsection*{7.7 Gram-Schmidt Orthogonalization Process}
Suppose $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ is a basis of an inner product space $V$. One can use this basis to construct an orthogonal basis $\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ of $V$ as follows. Set

$$
\begin{aligned}
& w_{1}=v_{1} \\
& w_{2}=v_{2}-\frac{\left\langle v_{2}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1} \\
& w_{3}=v_{3}-\frac{\left\langle v_{3}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}-\frac{\left\langle v_{3}, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle} w_{2} \\
& \cdots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
& w_{n}=v_{n}-\frac{\left\langle v_{n}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}-\frac{\left\langle v_{n}, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle} w_{2}-\cdots-\frac{\left\langle v_{n-1}\right\rangle}{\left\langle w_{n-1}, w_{n-1}\right\rangle} w_{n-1}
\end{aligned}
$$

In other words, for $k=2,3, \ldots, n$, we define

$$
w_{k}=v_{k}-c_{k 1} w_{1}-c_{k 2} w_{2}-\cdots-c_{k, k-1} w_{k-1}
$$

where $c_{k i}=\left\langle v_{k}, w_{i}\right\rangle /\left\langle w_{i}, w_{i}\right\rangle$ is the component of $v_{k}$ along $w_{i}$. By Theorem 7.8, each $w_{k}$ is orthogonal to the preceeding $w$ 's. Thus, $w_{1}, w_{2}, \ldots, w_{n}$ form an orthogonal basis for $V$ as claimed. Normalizing each $w_{i}$ will then yield an orthonormal basis for $V$.

The above construction is known as the Gram-Schmidt orthogonalization process. The following remarks are in order.

Remark 1: Each vector $w_{k}$ is a linear combination of $v_{k}$ and the preceding $w$ 's. Hence, one can easily show, by induction, that each $w_{k}$ is a linear combination of $v_{1}, v_{2}, \ldots, v_{n}$.

Remark 2: Because taking multiples of vectors does not affect orthogonality, it may be simpler in hand calculations to clear fractions in any new $w_{k}$, by multiplying $w_{k}$ by an appropriate scalar, before obtaining the next $w_{k+1}$.

Remark 3: Suppose $u_{1}, u_{2}, \ldots, u_{r}$ are linearly independent, and so they form a basis for $U=\operatorname{span}\left(u_{i}\right)$. Applying the Gram-Schmidt orthogonalization process to the $u$ 's yields an orthogonal basis for $U$.

The following theorems (proved in Problems 7.26 and 7.27) use the above algorithm and remarks.

THEOREM 7.9: Let $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be any basis of an inner product space $V$. Then there exists an orthonormal basis $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of $V$ such that the change-of-basis matrix from $\left\{v_{i}\right\}$ to $\left\{u_{i}\right\}$ is triangular; that is, for $k=1, \ldots, n$,

$$
u_{k}=a_{k 1} v_{1}+a_{k 2} v_{2}+\cdots+a_{k k} v_{k}
$$

THEOREM 7.10: Suppose $S=\left\{w_{1}, w_{2}, \ldots, w_{r}\right\}$ is an orthogonal basis for a subspace $W$ of a vector space $V$. Then one may extend $S$ to an orthogonal basis for $V$; that is, one may find vectors $w_{r+1}, \ldots, w_{n}$ such that $\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ is an orthogonal basis for $V$.

EXAMPLE 7.10 Apply the Gram-Schmidt orthogonalization process to find an orthogonal basis and then an orthonormal basis for the subspace $U$ of $\mathbf{R}^{4}$ spanned by

$$
v_{1}=(1,1,1,1), \quad v_{2}=(1,2,4,5), \quad v_{3}=(1,-3,-4,-2)
$$

(1) First set $w_{1}=v_{1}=(1,1,1,1)$.

(2) Compute

$$
v_{2}-\frac{\left\langle v_{2}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}=v_{2}-\frac{12}{4} w_{1}=(-2,-1,1,2)
$$

Set $w_{2}=(-2,-1,1,2)$.

(3) Compute

$$
v_{3}-\frac{\left\langle v_{3}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}-\frac{\left\langle v_{3}, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle} w_{2}=v_{3}-\frac{(-8)}{4} w_{1}-\frac{(-7)}{10} w_{2}=\left(\frac{8}{5},-\frac{17}{10},-\frac{13}{10}, \frac{7}{5}\right)
$$

Clear fractions to obtain $w_{3}=(-6,-17,-13,14)$.

Thus, $w_{1}, w_{2}, w_{3}$ form an orthogonal basis for $U$. Normalize these vectors to obtain an orthonormal basis $\left\{u_{1}, u_{2}, u_{3}\right\}$ of $U$. We have $\left\|w_{1}\right\|^{2}=4,\left\|w_{2}\right\|^{2}=10,\left\|w_{3}\right\|^{2}=910$, so

$$
u_{1}=\frac{1}{2}(1,1,1,1), \quad u_{2}=\frac{1}{\sqrt{10}}(-2,-1,1,2), \quad u_{3}=\frac{1}{\sqrt{910}}(16,-17,-13,14)
$$

EXAMPLE 7.11 Let $V$ be the vector space of polynomials $f(t)$ with inner product $\langle f, g\rangle=\int_{-1}^{1} f(t) g(t) d t$. Apply the Gram-Schmidt orthogonalization process to $\left\{1, t, t^{2}, t^{3}\right\}$ to find an orthogonal basis $\left\{f_{0}, f_{1}, f_{2}, f_{3}\right\}$ with integer coefficients for $\mathbf{P}_{3}(t)$.

Here we use the fact that, for $r+s=n$,

$$
\left\langle t^{r}, t^{s}\right\rangle=\int_{-1}^{1} t^{n} d t=\left.\frac{t^{n+1}}{n+1}\right|_{-1} ^{1}= \begin{cases}2 /(n+1) & \text { when } n \text { is even } \\ 0 & \text { when } n \text { is odd }\end{cases}
$$

(1) First set $f_{0}=1$

(2) Compute $t=\frac{\langle t, 1\rangle}{\langle 1,1\rangle}(1)=t-0=t$. Set $f_{1}=t$.

(3) Compute

$$
t^{2}-\frac{\left\langle t^{2}, 1\right\rangle}{\langle 1,1\rangle}(1)-\frac{\left\langle t^{2}, t\right\rangle}{\langle t, t\rangle}(t)=t^{2}-\frac{\frac{2}{3}}{2}(1)+0(t)=t^{2}-\frac{1}{3}
$$

Multiply by 3 to obtain $f_{2}=3 t^{2}=1$.

(4) Compute

$$
\begin{aligned}
t^{3} & -\frac{\left\langle t^{3}, 1\right\rangle}{\langle 1,1\rangle}(1)-\frac{\left\langle t^{3}, t\right\rangle}{\langle t, t\rangle}(t)-\frac{\left\langle t^{3}, 3 t^{2}-1\right\rangle}{\left\langle 3 t^{2}-1,3 t^{2}-1\right\rangle}\left(3 t^{2}-1\right) \\
& =t^{3}-0(1)-\frac{\frac{2}{2}}{\frac{2}{3}}(t)-0\left(3 t^{2}-1\right)=t^{3}-\frac{3}{5} t
\end{aligned}
$$

Multiply by 5 to obtain $f_{3}=5 t^{3}-3 t$.

Thus, $\left\{1, t, 3 t^{2}-1,5 t^{3}-3 t\right\}$ is the required orthogonal basis.

Remark: Normalizing the polynomials in Example 7.11 so that $p(1)=1$ yields the polynomials

$1, t, \frac{1}{2}\left(3 t^{2}-1\right), \frac{1}{2}\left(5 t^{3}-3 t\right)$

These are the first four Legendre polynomials, which appear in the study of differential equations.

\subsection*{7.8 Orthogonal and Positive Definite Matrices}
This section discusses two types of matrices that are closely related to real inner product spaces $V$. Here vectors in $\mathbf{R}^{n}$ will be represented by column vectors. Thus, $\langle u, v\rangle=u^{T} v$ denotes the inner product in Euclidean space $\mathbf{R}^{n}$.

\section*{Orthogonal Matrices}
A real matrix $P$ is orthogonal if $P$ is nonsingular and $P^{-1}=P^{T}$, or, in other words, if $P P^{T}=P^{T} P=I$. First we recall (Theorem 2.6) an important characterization of such matrices.

THEOREM 7.11: $\quad$ Let $P$ be a real matrix. Then the following are equivalent: (a) $P$ is orthogonal; (b) the rows of $P$ form an orthonormal set; (c) the columns of $P$ form an orthonormal set.

(This theorem is true only using the usual inner product on $\mathbf{R}^{n}$. It is not true if $\mathbf{R}^{n}$ is given any other inner product.)

\section*{EXAMPLE 7.12}
(a) Let $P=\left[\begin{array}{crr}1 / \sqrt{3} & 1 / \sqrt{3} & 1 / \sqrt{3} \\ 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\ 2 / \sqrt{6} & -1 / \sqrt{6} & -1 / \sqrt{6}\end{array}\right]$. The rows of $P$ are orthogonal to each other and are unit vectors. Thus $P$ is an orthogonal matrix.

(b) Let $P$ be a $2 \times 2$ orthogonal matrix. Then, for some real number $\theta$, we have

$$
P=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
-\sin \theta & \cos \theta
\end{array}\right] \quad \text { or } \quad P=\left[\begin{array}{rr}
\cos \theta & \sin \theta \\
\sin \theta & -\cos \theta
\end{array}\right]
$$

The following two theorems (proved in Problems 7.37 and 7.38) show important relationships between orthogonal matrices and orthonormal bases of a real inner product space $V$.

THEOREM 7.12: Suppose $E=\left\{e_{i}\right\}$ and $E^{\prime}=\left\{e_{i}^{\prime}\right\}$ are orthonormal bases of $V$. Let $P$ be the changeof-basis matrix from the basis $E$ to the basis $E^{\prime}$. Then $P$ is orthogonal.

THEOREM 7.13: Let $\left\{e_{1}, \ldots, e_{n}\right\}$ be an orthonormal basis of an inner product space $V$. Let $P=\left[a_{i j}\right]$ be an orthogonal matrix. Then the following $n$ vectors form an orthonormal basis for $V$ :

$$
e_{i}^{\prime}=a_{1 i} e_{1}+a_{2 i} e_{2}+\cdots+a_{n i} e_{n}, \quad i=1,2, \ldots, n
$$

\section*{Positive Definite Matrices}
Let $A$ be a real symmetric matrix; that is, $A^{T}=A$. Then $A$ is said to be positive definite if, for every nonzero vector $u$ in $\mathbf{R}^{n}$,

$$
\langle u, A u\rangle=u^{T} A u>0
$$

Algorithms to decide whether or not a matrix $A$ is positive definite will be given in Chapter 12. However, for $2 \times 2$ matrices, we have simple criteria that we state formally in the following theorem (proved in Problem 7.43).

THEOREM 7.14: A $2 \times 2$ real symmetric matrix $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]=\left[\begin{array}{ll}a & b \\ b & d\end{array}\right]$ is positive definite if and only if the diagonal entries $a$ and $d$ are positive and the determinant $|A|=a d-b c=a d-b^{2}$ is positive.

EXAMPLE 7.13 Consider the following symmetric matrices:

$$
A=\left[\begin{array}{ll}
1 & 3 \\
3 & 4
\end{array}\right], \quad B=\left[\begin{array}{rr}
1 & -2 \\
-2 & -3
\end{array}\right], \quad C=\left[\begin{array}{rr}
1 & -2 \\
-2 & 5
\end{array}\right]
$$

$A$ is not positive definite, because $|A|=4-9=-5$ is negative. $B$ is not positive definite, because the diagonal entry -3 is negative. However, $C$ is positive definite, because the diagonal entries 1 and 5 are positive, and the determinant $|C|=5-4=1$ is also positive.

The following theorem (proved in Problem 7.44) holds.

THEOREM 7.15: $\quad$ Let $A$ be a real positive definite matrix. Then the function $\langle u, v\rangle=u^{T} A v$ is an inner product on $\mathbf{R}^{n}$.

\section*{Matrix Representation of an Inner Product (Optional)}
Theorem 7.15 says that every positive definite matrix $A$ determines an inner product on $\mathbf{R}^{n}$. This subsection may be viewed as giving the converse of this result.

Let $V$ be a real inner product space with basis $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$. The matrix

$$
A=\left[a_{i j}\right], \quad \text { where } \quad a_{i j}=\left\langle u_{i}, u_{j}\right\rangle
$$

is called the matrix representation of the inner product on $V$ relative to the basis $S$.

Observe that $A$ is symmetric, because the inner product is symmetric; that is, $\left\langle u_{i}, u_{j}\right\rangle=\left\langle u_{j}, u_{i}\right\rangle$. Also, $A$ depends on both the inner product on $V$ and the basis $S$ for $V$. Moreover, if $S$ is an orthogonal basis, then $A$ is diagonal, and if $S$ is an orthonormal basis, then $A$ is the identity matrix.

EXAMPLE 7.14 The vectors $u_{1}=(1,1,0), u_{2}=(1,2,3), u_{3}=(1,3,5)$ form a basis $S$ for Euclidean space $\mathbf{R}^{3}$. Find the matrix $A$ that represents the inner product in $\mathbf{R}^{3}$ relative to this basis $S$.

First compute each $\left\langle u_{i}, u_{j}\right\rangle$ to obtain

$$
\begin{aligned}
& \left\langle u_{1}, u_{1}\right\rangle=1+1+0=2, \\
& \left\langle u_{2}, u_{2}\right\rangle=1+4+9=14, \\
& \left\langle u_{1}, u_{2}\right\rangle=1+2+0=3, \\
& \left\langle u_{2}, u_{3}\right\rangle=1+6+15=22 \\
& \begin{array}{l}
\left\langle u_{1}, u_{3}\right\rangle=1+3+0=4 \\
\left\langle u_{3}, u_{3}\right\rangle=1+9+25=35
\end{array}
\end{aligned}
$$

Then $A=\left[\begin{array}{rrr}2 & 3 & 4 \\ 3 & 14 & 22 \\ 4 & 22 & 35\end{array}\right]$. As expected, $A$ is symmetric.

The following theorems (proved in Problems 7.45 and 7.46, respectively) hold.

THEOREM 7.16: Let $A$ be the matrix representation of an inner product relative to basis $S$ for $V$. Then, for any vectors $u, v \in V$, we have

$$
\langle u, v\rangle=[u]^{T} A[v]
$$

where $[u]$ and $[v]$ denote the (column) coordinate vectors relative to the basis $S$.

THEOREM 7.17: $\quad$ Let $A$ be the matrix representation of any inner product on $V$. Then $A$ is a positive definite matrix.

\subsection*{7.9 Complex Inner Product Spaces}
This section considers vector spaces over the complex field $\mathbf{C}$. First we recall some properties of the complex numbers (Section 1.7), especially the relations between a complex number $z=a+b i$, where $a, b \in \mathbf{R}$, and its complex conjugate $\bar{z}=a-b i$ :

$$
z \bar{z}=a^{2}+b^{2}, \quad|z|=\sqrt{a^{2}+b^{2}}, \quad \overline{z_{1}+z_{2}}=\overline{z_{1}}+\overline{z_{2}} \quad \overline{z_{1} z_{2}}=\bar{z}_{1} \bar{z}_{2}, \quad \overline{\bar{z}}=z
$$

Also, $z$ is real if and only if $\bar{z}=z$.

The following definition applies.

DEFINITION: Let $V$ be a vector space over C. Suppose to each pair of vectors, $u, v \in V$ there is assigned a complex number, denoted by $\langle u, v\rangle$. This function is called a (complex) inner product on $V$ if it satisfies the following axioms:

$\left[I_{1}^{*}\right] \quad$ (Linear Property) $\left\langle a u_{1}+b u_{2}, v\right\rangle=a\left\langle u_{1}, v\right\rangle+b\left\langle u_{2}, v\right\rangle$

$\left[I_{2}^{*}\right] \quad$ (Conjugate Symmetric Property) $\langle u, v\rangle=\overline{\langle v, u\rangle}$

$\left[I_{3}^{*}\right] \quad$ (Positive Definite Property) $\langle u, u\rangle \geq 0$; and $\langle u, u\rangle=0$ if and only if $u=0$.

The vector space $V$ over $C$ with an inner product is called a (complex) inner product space. Observe that a complex inner product differs from the real case only in the second axiom $\left[I_{2}^{*}\right]$. Axiom $\left[I_{1}^{*}\right]$ (Linear Property) is equivalent to the two conditions:\\
(a) $\left\langle u_{1}+u_{2}, v\right\rangle=\left\langle u_{1}, v\right\rangle+\left\langle u_{2}, v\right\rangle$\\
(b) $\langle k u, v\rangle=k\langle u, v\rangle$

On the other hand, applying $\left[I_{1}^{*}\right]$ and $\left[I_{2}^{*}\right]$, we obtain

$$
\langle u, k v\rangle=\overline{\langle k v, u\rangle}=\overline{k\langle v, u\rangle}=\bar{k} \overline{\langle v, u\rangle}=\bar{k}\langle u, v\rangle
$$

That is, we must take the conjugate of a complex number when it is taken out of the second position of a complex inner product. In fact (Problem 7.47), the inner product is conjugate linear in the second position; that is,

$$
\left\langle u, a v_{1}+b v_{2}\right\rangle=\bar{a}\left\langle u, v_{1}\right\rangle+\bar{b}\left\langle u, v_{2}\right\rangle
$$

Combining linear in the first position and conjugate linear in the second position, we obtain, by induction,

$$
\left\langle\sum_{i} a_{i} u_{i}, \sum_{j} b_{j} v_{j}\right\rangle=\sum_{i, j} a_{i} \overline{b_{j}}\left\langle u_{i}, v_{j}\right\rangle
$$

The following remarks are in order.

Remark 1: Axiom $\left[I_{1}^{*}\right]$ by itself implies that $\langle 0,0\rangle=\langle 0 v, 0\rangle=0\langle v, 0\rangle=0$. Accordingly, $\left[I_{1}^{*}\right],\left[I_{2}^{*}\right]$, and $\left[I_{3}^{*}\right]$ are equivalent to $\left[I_{1}^{*}\right],\left[I_{2}^{*}\right]$, and the following axiom:

$$
\left[I_{3}^{*^{\prime}}\right] \text { If } u \neq 0 \text {, then }\langle u, u\rangle>0 \text {. }
$$

That is, a function satisfying $\left[I_{1}\right],\left[I_{2}^{*}\right]$, and $\left[I_{3}^{* \prime}\right]$ is a (complex) inner product on $V$.

Remark 2: By $\left[I_{2}^{*}\right],\langle u, u\rangle=\overline{\langle u, u\rangle}$. Thus, $\langle u, u\rangle$ must be real. By $\left[I_{3}^{*}\right],\langle u, u\rangle$ must be nonnegative, and hence, its positive real square root exists. As with real inner product spaces, we define $\|u\|=\sqrt{\langle u, u\rangle}$ to be the norm or length of $u$.

Remark 3: In addition to the norm, we define the notions of orthogonality, orthogonal complement, and orthogonal and orthonormal sets as before. In fact, the definitions of distance and Fourier coefficient and projections are the same as in the real case.

EXAMPLE 7.15 (Complex Euclidean Space $\mathbf{C}^{n}$ ). Let $V=\mathbf{C}^{n}$, and let $u=\left(z_{i}\right)$ and $v=\left(w_{i}\right)$ be vectors in $\mathbf{C}^{n}$. Then

$$
\langle u, v\rangle=\sum_{k} z_{k} \overline{w_{k}}=z_{1} \overline{w_{1}}+z_{2} \overline{w_{2}}+\cdots+z_{n} \overline{w_{n}}
$$

is an inner product on $V$, called the usual or standard inner product on $\mathbf{C}^{n} . V$ with this inner product is called Complex Euclidean Space. We assume this inner product on $\mathbf{C}^{n}$ unless otherwise stated or implied. Assuming $u$ and $v$ are column vectors, the above inner product may be defined by

$$
\langle u, v\rangle=u^{T} \bar{v}
$$

where, as with matrices, $\bar{v}$ means the conjugate of each element of $v$. If $u$ and $v$ are real, we have $\overline{w_{i}}=w_{i}$. In this case, the inner product reduced to the analogous one on $\mathbf{R}^{n}$.

\section*{EXAMPLE 7.16}
(a) Let $V$ be the vector space of complex continuous functions on the (real) interval $a \leq t \leq b$. Then the following is the usual inner product on $V$ :

$$
\langle f, g\rangle=\int_{a}^{b} f(t) \overline{g(t)} d t
$$

(b) Let $U$ be the vector space of $m \times n$ matrices over $\mathbf{C}$. Suppose $A=\left(z_{i j}\right)$ and $B=\left(w_{i j}\right)$ are elements of $U$. Then the following is the usual inner product on $U$ :

$$
\langle A, B\rangle=\operatorname{tr}\left(B^{H} A\right)=\sum_{i=1}^{m} \sum_{j=1}^{n} \bar{w}_{i j} z_{i j}
$$

As usual, $B^{H}=\bar{B}^{T}$; that is, $B^{H}$ is the conjugate transpose of $B$.

The following is a list of theorems for complex inner product spaces that are analogous to those for the real case. Here a Hermitian matrix $A$ (i.e., one where $A^{H}=\bar{A}^{T}=A$ ) plays the same role that a symmetric matrix $A$ (i.e., one where $A^{T}=A$ ) plays in the real case. (Theorem 7.18 is proved in Problem 7.50.)

THEOREM 7.18: (Cauchy-Schwarz) Let $V$ be a complex inner product space. Then

$$
|\langle u, v\rangle| \leq\|u\|\|v\|
$$

THEOREM 7.19: $\quad$ Let $W$ be a subspace of a complex inner product space $V$. Then $V=W \oplus W^{\perp}$.

THEOREM 7.20: Suppose $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis for a complex inner product space $V$. Then, for any $v \in V$,

$$
v=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle} u_{1}+\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle} u_{2}+\cdots+\frac{\left\langle v, u_{n}\right\rangle}{\left\langle u_{n}, u_{n}\right\rangle} u_{n}
$$

THEOREM 7.21: Suppose $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis for a complex inner product space $V$. Let $A=\left[a_{i j}\right]$ be the complex matrix defined by $a_{i j}=\left\langle u_{i}, u_{j}\right\rangle$. Then, for any $u, v \in V$,

$$
\langle u, v\rangle=[u]^{T} A \overline{[v]}
$$

where $[u]$ and $[v]$ are the coordinate column vectors in the given basis $\left\{u_{i}\right\}$. (Remark: This matrix $A$ is said to represent the inner product on $V$.)

THEOREM 7.22: Let $A$ be a Hermitian matrix (i.e., $A^{H}=\bar{A}^{T}=A$ ) such that $X^{T} A \bar{X}$ is real and positive for every nonzero vector $X \in \mathbf{C}^{n}$. Then $\langle u, v\rangle=u^{T} A \bar{v}$ is an inner product on $\mathbf{C}^{n}$.

THEOREM 7.23: $\quad$ Let $A$ be the matrix that represents an inner product on $V$. Then $A$ is Hermitian, and $X^{T} A X$ is real and positive for any nonzero vector in $\mathbf{C}^{n}$.

\subsection*{7.10 Normed Vector Spaces (Optional)}
We begin with a definition.

DEFINITION: $\quad$ Let $V$ be a real or complex vector space. Suppose to each $v \in V$ there is assigned a real number, denoted by $\|v\|$. This function $\|\cdot\|$ is called a norm on $V$ if it satisfies the following axioms:

$$
\begin{aligned}
& {\left[\mathrm{N}_{1}\right] \quad\|v\| \geq 0 \text {; and }\|v\|=0 \text { if and only if } v=0 \text {. }} \\
& {\left[\mathrm{N}_{2}\right] \quad\|k v\|=|k|\|v\| .} \\
& {\left[\mathrm{N}_{3}\right] \quad\|u+v\| \leq\|u\|+\|v\| .}
\end{aligned}
$$

A vector space $V$ with a norm is called a normed vector space.

Suppose $V$ is a normed vector space. The distance between two vectors $u$ and $v$ in $V$ is denoted and defined by

$$
d(u, v)=\|u-v\|
$$

The following theorem (proved in Problem 7.56) is the main reason why $d(u, v)$ is called the distance between $u$ and $v$.

THEOREM 7.24: Let $V$ be a normed vector space. Then the function $d(u, v)=\|u-v\|$ satisfies the following three axioms of a metric space:

$\left[\mathrm{M}_{1}\right] \quad d(u, v) \geq 0$; and $d(u, v)=0$ if and only if $u=v$.

$\left[\mathrm{M}_{2}\right] \quad d(u, v)=d(v, u)$.

$\left[\mathrm{M}_{3}\right] \quad d(u, v) \leq d(u, w)+d(w, v)$.

\section*{Normed Vector Spaces and Inner Product Spaces}
Suppose $V$ is an inner product space. Recall that the norm of a vector $v$ in $V$ is defined by

$$
\|v\|=\sqrt{\langle v, v\rangle}
$$

One can prove (Theorem 7.2) that this norm satisfies $\left[\mathrm{N}_{1}\right],\left[\mathrm{N}_{2}\right]$, and $\left[\mathrm{N}_{3}\right]$. Thus, every inner product space $V$ is a normed vector space. On the other hand, there may be norms on a vector space $V$ that do not come from an inner product on $V$, as shown below.

\section*{Norms on $\mathbf{R}^{\boldsymbol{n}}$ and $\mathbf{C}^{\boldsymbol{n}}$}
The following define three important norms on $\mathbf{R}^{n}$ and $\mathbf{C}^{n}$ :

$$
\begin{aligned}
& \left\|\left(a_{1}, \ldots, a_{n}\right)\right\|_{\infty}=\max \left(\left|a_{i}\right|\right) \\
& \left\|\left(a_{1}, \ldots, a_{n}\right)\right\|_{1}=\left|a_{1}\right|+\left|a_{2}\right|+\cdots+\left|a_{n}\right| \\
& \left\|\left(a_{1}, \ldots, a_{n}\right)\right\|_{2}=\sqrt{\left|a_{1}\right|^{2}+\left|a_{2}\right|^{2}+\cdots+\left|a_{n}\right|^{2}}
\end{aligned}
$$

(Note that subscripts are used to distinguish between the three norms.) The norms $\|\cdot\|_{\infty},\|\cdot\|_{1}$, and $\|\cdot\|_{2}$ are called the infinity-norm, one-norm, and two-norm, respectively. Observe that $\|\cdot\|_{2}$ is the norm on $\mathbf{R}^{n}$ (respectively, $\mathbf{C}^{n}$ ) induced by the usual inner product on $\mathbf{R}^{n}$ (respectively, $\mathbf{C}^{n}$ ). We will let $d_{\infty}, d_{1}, d_{2}$ denote the corresponding distance functions.

EXAMPLE 7.17 Consider vectors $u=(1,-5,3)$ and $v=(4,2,-3)$ in $\mathbf{R}^{3}$.

(a) The infinity norm chooses the maximum of the absolute values of the components. Hence,

$$
\|u\|_{\infty}=5 \quad \text { and } \quad\|v\|_{\infty}=4
$$

(b) The one-norm adds the absolute values of the components. Thus,

$$
\|u\|_{1}=1+5+3=9 \quad \text { and } \quad\|v\|_{1}=4+2+3=9
$$

(c) The two-norm is equal to the square root of the sum of the squares of the components (i.e., the norm induced by the usual inner product on $\mathrm{R}^{3}$ ). Thus,

$$
\|u\|_{2}=\sqrt{1+25+9}=\sqrt{35} \quad \text { and } \quad\|v\|_{2}=\sqrt{16+4+9}=\sqrt{29}
$$

(d) Because $u-v=(1-4,-5-2,3+3)=(-3,-7,6)$, we have

$$
d_{\infty}(u, v)=7, \quad d_{1}(u, v)=3+7+6=16, \quad d_{2}(u, v)=\sqrt{9+49+36}=\sqrt{94}
$$

EXAMPLE 7.18 Consider the Cartesian plane $\mathbf{R}^{2}$ shown in Fig. 7-4.

(a) Let $D_{1}$ be the set of points $u=(x, y)$ in $\mathbf{R}^{2}$ such that $\|u\|_{2}=1$. Then $D_{1}$ consists of the points $(x, y)$ such that $\|u\|_{2}^{2}=x^{2}+y^{2}=1$. Thus, $D_{1}$ is the unit circle, as shown in Fig. 7-4.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-249}
\end{center}

Figure 7-4

(b) Let $D_{2}$ be the set of points $u=(x, y)$ in $\mathbf{R}^{2}$ such that $\|u\|_{1}=1$. Then $D_{1}$ consists of the points $(x, y)$ such that $\|u\|_{1}=|x|+|y|=1$. Thus, $D_{2}$ is the diamond inside the unit circle, as shown in Fig. 7-4.

(c) Let $D_{3}$ be the set of points $u=(x, y)$ in $\mathbf{R}^{2}$ such that $\|u\|_{\infty}=1$. Then $D_{3}$ consists of the points $(x, y)$ such that $\|u\|_{\infty}=\max (|x|,|y|)=1$. Thus, $D_{3}$ is the square circumscribing the unit circle, as shown in Fig. 7-4.

\section*{Norms on $\boldsymbol{C}[\boldsymbol{a}, \boldsymbol{b}]$}
Consider the vector space $V=C[a, b]$ of real continuous functions on the interval $a \leq t \leq b$. Recall that the following defines an inner product on $V$ :

$$
\langle f, g\rangle=\int_{a}^{b} f(t) g(t) d t
$$

Accordingly, the above inner product defines the following norm on $V=C[a, b]$ (which is analogous to the $\|\cdot\|_{2}$ norm on $\mathbf{R}^{n}$ ):

$$
\|f\|_{2}=\sqrt{\int_{a}^{b}[f(t)]^{2} d t}
$$

The following define the other norms on $V=C[a, b]$ :

$$
\|f\|_{1}=\int_{a}^{b}|f(t)| d t \quad \text { and } \quad\|f\|_{\infty}=\max (|f(t)|)
$$

There are geometrical descriptions of these two norms and their corresponding distance functions, which are described below.

The first norm is pictured in Fig. 7-5. Here

$$
\begin{aligned}
\|f\|_{1} & =\text { area between the function }|f| \text { and the } t \text {-axis } \\
d_{1}(f, g) & =\text { area between the functions } f \text { and } g
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-250(1)}
\end{center}

(a) $\|f\|_{1}$ is shaded.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-250(3)}
\end{center}

(b) $d_{1}(f, g)$ is shaded.

Figure 7-5

This norm is analogous to the norm $\|\cdot\|_{1}$ on $\mathbf{R}^{n}$.

The second norm is pictured in Fig. 7-6. Here

$$
\begin{aligned}
\|f\|_{\infty} & =\text { maximum distance between } f \text { and the } t \text {-axis } \\
d_{\infty}(f, g) & =\text { maximum distance between } f \text { and } g
\end{aligned}
$$

This norm is analogous to the norms $\|\cdot\|_{\infty}$ on $\mathbf{R}^{n}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-250(2)}
\end{center}

(a) $\|f\|_{\infty}$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-250}
\end{center}

(b) $d_{\infty}(f, g)$

Figure 7-6

\section*{SOLVED PROBLEMS}
\section*{Inner Products}
7.1. Expand:

(a) $\left\langle 5 u_{1}+8 u_{2}, 6 v_{1}-7 v_{2}\right\rangle$

(b) $\langle 3 u+5 v, 4 u-6 v\rangle$,

(c) $\|2 u-3 v\|^{2}$

Use linearity in both positions and, when possible, symmetry, $\langle u, v\rangle=\langle v, u\rangle$.\\
(a) Take the inner product of each term on the left with each term on the right:

$$
\begin{aligned}
\left\langle 5 u_{1}+8 u_{2}, \quad 6 v_{1}-7 v_{2}\right\rangle & =\left\langle 5 u_{1}, 6 v_{1}\right\rangle+\left\langle 5 u_{1},-7 v_{2}\right\rangle+\left\langle 8 u_{2}, 6 v_{1}\right\rangle+\left\langle 8 u_{2},-7 v_{2}\right\rangle \\
& =30\left\langle u_{1}, v_{1}\right\rangle-35\left\langle u_{1}, v_{2}\right\rangle+48\left\langle u_{2}, v_{1}\right\rangle-56\left\langle u_{2}, v_{2}\right\rangle
\end{aligned}
$$

[Remark: Observe the similarity between the above expansion and the expansion $(5 a-8 b)(6 c-7 d)$ in ordinary algebra.]

(b) $\langle 3 u+5 v, 4 u-6 v\rangle=12\langle u, u\rangle-18\langle u, v\rangle+20\langle v, u\rangle-30\langle v, v\rangle$

$$
=12\langle u, u\rangle+2\langle u, v\rangle-30\langle v, v\rangle
$$

(c) $\begin{aligned}\|2 u-3 v\|^{2} & =\langle 2 u-3 v, 2 u-3 v\rangle=4\langle u, u\rangle-6\langle u, v\rangle-6\langle v, u\rangle+9\langle v, v\rangle \\ & =4\|u\|^{2}-12(u, v)+9\|v\|^{2}\end{aligned}$

7.2. Consider vectors $u=(1,2,4), v=(2,-3,5), w=(4,2,-3)$ in $\mathbf{R}^{3}$. Find\\
(a) $u \cdot v$,\\
(b) $u \cdot w$,\\
(c) $v \cdot w$,\\
(d) $\quad(u+v) \cdot w$\\
(e) $\|u\|$, (f)\\
$\|v\|$.

(a) Multiply corresponding components and add to get $u \cdot v=2-6+20=16$.

(b) $u \cdot w=4+4-12=-4$.

(c) $v \cdot w=8-6-15=-13$.

(d) First find $u+v=(3,-1,9)$. Then $(u+v) \cdot w=12-2-27=-17$. Alternatively, using $\left[\mathrm{I}_{1}\right]$, $(u+v) \cdot w=u \cdot w+v \cdot w=-4-13=-17$.

(e) First find $\|u\|^{2}$ by squaring the components of $u$ and adding:

$$
\|u\|^{2}=1^{2}+2^{2}+4^{2}=1+4+16=21, \quad \text { and so } \quad\|u\|=\sqrt{21}
$$

(f) $\|v\|^{2}=4+9+25=38$, and so $\|v\|=\sqrt{38}$.

7.3. Verify that the following defines an inner product in $\mathbf{R}^{2}$ :

$$
\langle u, v\rangle=x_{1} y_{1}-x_{1} y_{2}-x_{2} y_{1}+3 x_{2} y_{2}, \quad \text { where } \quad u=\left(x_{1}, x_{2}\right), \quad v=\left(y_{1}, y_{2}\right)
$$

We argue via matrices. We can write $\langle u, v\rangle$ in matrix notation as follows:

$$
\langle u, v\rangle=u^{T} A v=\left[x_{1}, x_{2}\right]\left[\begin{array}{rr}
1 & -1 \\
-1 & 3
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2}
\end{array}\right]
$$

Because $A$ is real and symmetric, we need only show that $A$ is positive definite. The diagonal elements 1 and 3 are positive, and the determinant $\|A\|=3-1=2$ is positive. Thus, by Theorem 7.14, $A$ is positive definite. Accordingly, by Theorem 7.15, $\langle u, v\rangle$ is an inner product.

7.4. Consider the vectors $u=(1,5)$ and $v=(3,4)$ in $\mathbf{R}^{2}$. Find

(a) $\langle u, v\rangle$ with respect to the usual inner product in $\mathbf{R}^{2}$.

(b) $\langle u, v\rangle$ with respect to the inner product in $\mathbf{R}^{2}$ in Problem 7.3.

(c) $\|v\|$ using the usual inner product in $\mathbf{R}^{2}$.

(d) $\|v\|$ using the inner product in $\mathbf{R}^{2}$ in Problem 7.3.

(a) $\langle u, v\rangle=3+20=23$.

(b) $\langle u, v\rangle=1 \cdot 3-1 \cdot 4-5 \cdot 3+3 \cdot 5 \cdot 4=3-4-15+60=44$.

(c) $\|v\|^{2}=\langle v, v\rangle=\langle(3,4),(3,4)\rangle=9+16=25$; hence, $\mid v \|=5$.

(d) $\|v\|^{2}=\langle v, v\rangle=\langle(3,4),(3,4)\rangle=9-12-12+48=33$; hence, $\|v\|=\sqrt{33}$.

7.5. Consider the following polynomials in $\mathbf{P}(t)$ with the inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$ :

$$
f(t)=t+2, \quad g(t)=3 t-2, \quad h(t)=t^{2}-2 t-3
$$

(a) Find $\langle f, g\rangle$ and $\langle f, h\rangle$.

(b) Find $\|f\|$ and $\|g\|$.

(c) Normalize $f$ and $g$.\\
(a) Integrate as follows:

$$
\begin{aligned}
& \langle f, g\rangle=\int_{0}^{1}(t+2)(3 t-2) d t=\int_{0}^{1}\left(3 t^{2}+4 t-4\right) d t=\left.\left(t^{3}+2 t^{2}-4 t\right)\right|_{0} ^{1}=-1 \\
& \langle f, h\rangle=\int_{0}^{1}(t+2)\left(t^{2}-2 t-3\right) d t=\left.\left(\frac{t^{4}}{4}-\frac{7 t^{2}}{2}-6 t\right)\right|_{0} ^{1}=-\frac{37}{4}
\end{aligned}
$$

(b)

$$
\begin{array}{r}
\langle f, f\rangle=\int_{0}^{1}(t+2)(t+2) d t=\frac{19}{3} ; \quad \text { hence, } \quad\|f\|=\sqrt{\frac{19}{3}}=\frac{1}{3} \sqrt{57} \\
\langle g, g\rangle=\int_{0}^{1}(3 t-2)(3 t-2)=1 ; \quad \text { hence, } \quad\|g\|=\sqrt{1}=1
\end{array}
$$

(c) Because $\|f\|=\frac{1}{3} \sqrt{57}$ and $g$ is already a unit vector, we have

$$
\hat{f}=\frac{1}{\|f\|} f=\frac{3}{\sqrt{57}}(t+2) \quad \text { and } \quad \hat{g}=g=3 t-2
$$

7.6. Find $\cos \theta$ where $\theta$ is the angle between:

(a) $u=(1,3,-5,4)$ and $v=(2,-3,4,1)$ in $\mathbf{R}^{4}$,

(b) $A=\left[\begin{array}{lll}9 & 8 & 7 \\ 6 & 5 & 4\end{array}\right]$ and $B=\left[\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6\end{array}\right]$, where $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)$.

Use $\cos \theta=\frac{\langle u, v\rangle}{\|u\|\|v\|}$

(a) Compute:

$\langle u, v\rangle=2-9-20+4=-23, \quad\|u\|^{2}=1+9+25+16=51, \quad\|v\|^{2}=4+9+16+1=30$

Thus,

$$
\cos \theta=\frac{-23}{\sqrt{51} \sqrt{30}}=\frac{-23}{3 \sqrt{170}}
$$

(b) Use $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} b_{i j}$, the sum of the products of corresponding entries.

$$
\langle A, B\rangle=9+16+21+24+25+24=119
$$

Use $\|A\|^{2}=\langle A, A\rangle=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j}^{2}$, the sum of the squares of all the elements of $A$.

$$
\begin{aligned}
& \|A\|^{2}=\langle A, A\rangle=9^{2}+8^{2}+7^{2}+6^{2}+5^{2}+4^{2}=271, \quad \text { and so } \quad\|A\|=\sqrt{271} \\
& \|B\|^{2}=\langle B, B\rangle=1^{2}+2^{2}+3^{2}+4^{2}+5^{2}+6^{2}=91, \quad \text { and so } \quad\|B\|=\sqrt{91}
\end{aligned}
$$

Thus,

$$
\cos \theta=\frac{119}{\sqrt{271} \sqrt{91}}
$$

7.7. Verify each of the following:

(a) Parallelogram Law (Fig. 7-7): $\|u+v\|^{2}+\|u-v\|^{2}=2\|u\|^{2}+2\|v\|^{2}$.

(b) Polar form for $\langle u, v\rangle$ (which shows the inner product can be obtained from the norm function):

$$
\langle u, v\rangle=\frac{1}{4}\left(\|u+v\|^{2}-\|u-v\|^{2}\right) .
$$

Expand as follows to obtain


\begin{align*}
\|u+v\|^{2} & =\langle u+v, u+v\rangle=\|u\|^{2}+2\langle u, v\rangle+\|v\|^{2}  \tag{1}\\
\|u-v\|^{2} & =\langle u-v, u-v\rangle=\|u\|^{2}-2\langle u, v\rangle+\|v\|^{2} \tag{2}
\end{align*}


Add (1) and (2) to get the Parallelogram Law (a). Subtract (2) from (1) to obtain

$$
\|u+v\|^{2}-\|u-v\|^{2}=4\langle u, v\rangle
$$

Divide by 4 to obtain the (real) polar form (b).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-253}
\end{center}

Figure 7-7

7.8. Prove Theorem 7.1 (Cauchy-Schwarz): For $u$ and $v$ in a real inner product space $V$, $\langle u, u\rangle^{2} \leq\langle u, u\rangle\langle v, v\rangle \quad$ or $\quad|\langle u, v\rangle| \leq\|u\|\|v\|$.

For any real number $t$,

$$
\langle t u+v, \quad t u+v\rangle=t^{2}\langle u, u\rangle+2 t\langle u, v\rangle+\langle v, v\rangle=t^{2}\|u\|^{2}+2 t\langle u, v\rangle+\|v\|^{2}
$$

Let $a=\|u\|^{2}, b=2\langle u, v), c=\|v\|^{2}$. Because $\|t u+v\|^{2} \geq 0$, we have

$$
a t^{2}+b t+c \geq 0
$$

for every value of $t$. This means that the quadratic polynomial cannot have two real roots, which implies that $b^{2}-4 a c \leq 0$ or $b^{2} \leq 4 a c$. Thus,

Dividing by 4 gives our result.

$$
4\langle u, v\rangle^{2} \leq 4\|u\|^{2}\|v\|^{2}
$$

7.9. Prove Theorem 7.2: The norm in an inner product space $V$ satisfies

(a) $\left[\mathrm{N}_{1}\right]\|v\| \geq 0$; and $\|v\|=0$ if and only if $v=0$.

(b) $\left[\mathrm{N}_{2}\right]\|k v\|=|k|\|v\|$.

(c) $\left[\mathrm{N}_{3}\right]\|u+v\| \leq\|u\|+\|v\|$.

(a) If $v \neq 0$, then $\langle v, v\rangle>0$, and hence, $\|v\|=\sqrt{\langle v, v\rangle}>0$. If $v=0$, then $\langle 0,0\rangle=0$. Consequently, $\|0\|=\sqrt{0}=0$. Thus, $\left[\mathrm{N}_{1}\right]$ is true.

(b) We have $\|k v\|^{2}=\langle k v, k v\rangle=k^{2}\langle v, v\rangle=k^{2}\|v\|^{2}$. Taking the square root of both sides gives $\left[\mathrm{N}_{2}\right]$.

(c) Using the Cauchy-Schwarz inequality, we obtain

$$
\begin{gathered}
\|u+v\|^{2}=\langle u+v, \quad u+v\rangle=\langle u, u\rangle+\langle u, v\rangle+\langle u, v\rangle+\langle v, v\rangle \\
\leq\|u\|^{2}+2\|u\|\|v\|+\|v\|^{2}=(\|u\|+\|v\|)^{2}
\end{gathered}
$$

Taking the square root of both sides yields $\left[\mathrm{N}_{3}\right]$.

\section*{Orthogonality, Orthonormal Complements, Orthogonal Sets}
7.10. Find $k$ so that $u=(1,2, k, 3)$ and $v=(3, k, 7,-5)$ in $\mathbf{R}^{4}$ are orthogonal.

First find

$$
\langle u, v\rangle=(1,2, k, 3) \cdot(3, k, 7,-5)=3+2 k+7 k-15=9 k-12
$$

Then set $\langle u, v\rangle=9 k-12=0$ to obtain $k=\frac{4}{3}$.

7.11. Let $W$ be the subspace of $\mathbf{R}^{5}$ spanned by $u=(1,2,3,-1,2)$ and $v=(2,4,7,2,-1)$. Find a basis of the orthogonal complement $W^{\perp}$ of $W$.

We seek all vectors $w=(x, y, z, s, t)$ such that

$$
\begin{aligned}
& \langle w, u\rangle=x+2 y+3 z-s+2 t=0 \\
& \langle w, v\rangle=2 x+4 y+7 z+2 s-t=0
\end{aligned}
$$

Eliminating $x$ from the second equation, we find the equivalent system

$$
\begin{aligned}
x+2 y+3 z-s+2 t & =0 \\
z+4 s-5 t & =0
\end{aligned}
$$

The free variables are $y, s$, and $t$. Therefore,

(1) Set $y=-1, s=0, t=0$ to obtain the solution $w_{1}=(2,-1,0,0,0)$.

(2) Set $y=0, s=1, t=0$ to find the solution $w_{2}=(13,0,-4,1,0)$.

(3) Set $y=0, s=0, t=1$ to obtain the solution $w_{3}=(-17,0,5,0,1)$.

The set $\left\{w_{1}, w_{2}, w_{3}\right\}$ is a basis of $W^{\perp}$.

7.12. Let $w=(1,2,3,1)$ be a vector in $\mathbf{R}^{4}$. Find an orthogonal basis for $w^{\perp}$.

Find a nonzero solution of $x+2 y+3 z+t=0$, say $v_{1}=(0,0,1,-3)$. Now find a nonzero solution of the system

$$
x+2 y+3 z+t=0, \quad z-3 t=0
$$

say $v_{2}=(0,-5,3,1)$. Last, find a nonzero solution of the system

$$
x+2 y+3 z+t=0, \quad-5 y+3 z+t=0, \quad z-3 t=0
$$

say $v_{3}=(-14,2,3,1)$. Thus, $v_{1}, v_{2}, v_{3}$ form an orthogonal basis for $w^{\perp}$.

7.13. Let $\mathrm{S}$ consist of the following vectors in $\mathbf{R}^{4}$ :

$$
u_{1}=(1,1,0,-1), u_{2}=(1,2,1,3), u_{3}=(1,1,-9,2), u_{4}=(16,-13,1,3)
$$

(a) Show that $S$ is orthogonal and a basis of $\mathbf{R}^{4}$.

(b) Find the coordinates of an arbitrary vector $v=(a, b, c, d)$ in $\mathbf{R}^{4}$ relative to the basis $S$.

(a) Compute

$u_{1} \cdot u_{2}=1+2+0-3=0, \quad u_{1} \cdot u_{3}=1+1+0-2=0, \quad u_{1} \cdot u_{4}=16-13+0-3=0$

$u_{2} \cdot u_{3}=1+2-9+6=0, \quad u_{2} \cdot u_{4}=16-26+1+9=0, \quad u_{3} \cdot u_{4}=16-13-9+6=0$

Thus, $S$ is orthogonal, and $S$ is linearly independent. Accordingly, $S$ is a basis for $\mathbf{R}^{4}$ because any four linearly independent vectors form a basis of $\mathbf{R}^{4}$.

(b) Because $S$ is orthogonal, we need only find the Fourier coefficients of $v$ with respect to the basis vectors, as in Theorem 7.7. Thus,

$$
\begin{array}{ll}
k_{1}=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle}=\frac{a+b-d}{3}, & k_{3}=\frac{\left\langle v, u_{3}\right\rangle}{\left\langle u_{3}, u_{3}\right\rangle}=\frac{a+b-9 c+2 d}{87} \\
k_{2}=\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle}=\frac{a+2 b+c+3 d}{15}, & k_{4}=\frac{\left\langle v, u_{4}\right\rangle}{\left\langle u_{4}, u_{4}\right\rangle}=\frac{16 a-13 b+c+3 d}{435}
\end{array}
$$

are the coordinates of $v$ with respect to the basis $S$.

7.14. Suppose $S, S_{1}, S_{2}$ are the subsets of $V$. Prove the following:

(a) $S \subseteq S^{\perp \perp}$.

(b) If $S_{1} \subseteq S_{2}$, then $S_{2}^{\perp} \subseteq S_{1}^{\perp}$.

(c) $S^{\perp}=\operatorname{span}(S)^{\perp}$.

(a) Let $w \in S$. Then $\langle w, v\rangle=0$ for every $v \in S^{\perp}$; hence, $w \in S^{\perp \perp}$. Accordingly, $S \subseteq S^{\perp \perp}$.

(b) Let $w \in S_{2}^{\perp}$. Then $\langle w, v\rangle=0$ for every $v \in S_{2}$. Because $S_{1} \subseteq S_{2},\langle w, v\rangle=0$ for every $v=S_{1}$. Thus, $w \in S_{1}^{\perp}$, and hence, $S_{2}^{\perp} \subseteq S_{1}^{\perp}$.

(c) Because $S \subseteq \operatorname{span}(S)$, part (b) gives us $\operatorname{span}(S)^{\perp} \subseteq S^{\perp}$. Suppose $u \in S^{\perp}$ and $v \in \operatorname{span}(S)$. Then there exist $w_{1}, w_{2}, \ldots, w_{k}$ in $S$ such that $v=a_{1} w_{1}+a_{2} w_{2}+\cdots+a_{k} w_{k}$. Then, using $u \in S^{\perp}$, we have

$$
\begin{aligned}
\langle u, v\rangle & =\left\langle u, a_{1} w_{1}+a_{2} w_{2}+\cdots+a_{k} w_{k}\right\rangle=a_{1}\left\langle u, w_{1}\right\rangle+a_{2}\left\langle u, w_{2}\right\rangle+\cdots+a_{k}\left\langle u, w_{k}\right\rangle \\
& =a_{1}(0)+a_{2}(0)+\cdots+a_{k}(0)=0
\end{aligned}
$$

Thus, $u \in \operatorname{span}(S)^{\perp}$. Accordingly, $S^{\perp} \subseteq \operatorname{span}(S)^{\perp}$. Both inclusions give $S^{\perp}=\operatorname{span}(S)^{\perp}$.

7.15. Prove Theorem 7.5: Suppose $S$ is an orthogonal set of nonzero vectors. Then $S$ is linearly independent.

Suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ and suppose


\begin{equation*}
a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{r} u_{r}=0 \tag{1}
\end{equation*}


Taking the inner product of (1) with $u_{1}$, we get

$$
\begin{aligned}
0 & =\left\langle 0, u_{1}\right\rangle=\left\langle a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{r} u_{r}, u_{1}\right\rangle \\
& =a_{1}\left\langle u_{1}, u_{1}\right\rangle+a_{2}\left\langle u_{2}, u_{1}\right\rangle+\cdots+a_{r}\left\langle u_{r}, u_{1}\right\rangle \\
& =a_{1}\left\langle u_{1}, u_{1}\right\rangle+a_{2} \cdot 0+\cdots+a_{r} \cdot 0=a_{1}\left\langle u_{1}, u_{1}\right\rangle
\end{aligned}
$$

Because $u_{1} \neq 0$, we have $\left\langle u_{1}, u_{1}\right\rangle \neq 0$. Thus, $a_{1}=0$. Similarly, for $i=2, \ldots, r$, taking the inner product of (1) with $u_{i}$,

$$
\begin{aligned}
0 & =\left\langle 0, u_{i}\right\rangle=\left\langle a_{1} u_{1}+\cdots+a_{r} u_{r}, u_{i}\right\rangle \\
& =a_{1}\left\langle u_{1}, u_{i}\right\rangle+\cdots+a_{i}\left\langle u_{i}, u_{i}\right\rangle+\cdots+a_{r}\left\langle u_{r}, u_{i}\right\rangle=a_{i}\left\langle u_{i}, u_{i}\right\rangle
\end{aligned}
$$

But $\left\langle u_{i}, u_{i}\right\rangle \neq 0$, and hence, every $a_{i}=0$. Thus, $S$ is linearly independent.

7.16. Prove Theorem 7.6 (Pythagoras): Suppose $\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ is an orthogonal set of vectors. Then

$$
\left\|u_{1}+u_{2}+\cdots+u_{r}\right\|^{2}=\left\|u_{1}\right\|^{2}+\left\|u_{2}\right\|^{2}+\cdots+\left\|u_{r}\right\|^{2}
$$

Expanding the inner product, we have

$$
\begin{aligned}
\left\|u_{1}+u_{2}+\cdots+u_{r}\right\|^{2} & =\left\langle u_{1}+u_{2}+\cdots+u_{r}, u_{1}+u_{2}+\cdots+u_{r}\right\rangle \\
& =\left\langle u_{1}, u_{1}\right\rangle+\left\langle u_{2}, u_{2}\right\rangle+\cdots+\left\langle u_{r}, u_{r}\right\rangle+\sum_{i \neq j}\left\langle u_{i}, u_{j}\right\rangle
\end{aligned}
$$

The theorem follows from the fact that $\left\langle u_{i}, u_{i}\right\rangle=\left\|u_{i}\right\|^{2}$ and $\left\langle u_{i}, u_{j}\right\rangle=0$ for $i \neq j$.

7.17. Prove Theorem 7.7: Let $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ be an orthogonal basis of $V$. Then for any $v \in V$,

$$
v=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle} u_{1}+\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle} u_{2}+\cdots+\frac{\left\langle v, u_{n}\right\rangle}{\left\langle u_{n}, u_{n}\right\rangle} u_{n}
$$

Suppose $v=k_{1} u_{1}+k_{2} u_{2}+\cdots+k_{n} u_{n}$. Taking the inner product of both sides with $u_{1}$ yields

$$
\begin{aligned}
\left\langle v, u_{1}\right\rangle & =\left\langle k_{1} u_{2}+k_{2} u_{2}+\cdots+k_{n} u_{n}, u_{1}\right\rangle \\
& =k_{1}\left\langle u_{1}, u_{1}\right\rangle+k_{2}\left\langle u_{2}, u_{1}\right\rangle+\cdots+k_{n}\left\langle u_{n}, u_{1}\right\rangle \\
& =k_{1}\left\langle u_{1}, u_{1}\right\rangle+k_{2} \cdot 0+\cdots+k_{n} \cdot 0=k_{1}\left\langle u_{1}, u_{1}\right\rangle
\end{aligned}
$$

Thus, $k_{1}=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle}$. Similarly, for $i=2, \ldots, n$,

$$
\begin{aligned}
\left\langle v, u_{i}\right\rangle & =\left\langle k_{1} u_{i}+k_{2} u_{2}+\cdots+k_{n} u_{n}, u_{i}\right\rangle \\
& =k_{1}\left\langle u_{1}, u_{i}\right\rangle+k_{2}\left\langle u_{2}, u_{i}\right\rangle+\cdots+k_{n}\left\langle u_{n}, u_{i}\right\rangle \\
& =k_{1} \cdot 0+\cdots+k_{i}\left\langle u_{i}, u_{i}\right\rangle+\cdots+k_{n} \cdot 0=k_{i}\left\langle u_{i}, u_{i}\right\rangle
\end{aligned}
$$

Thus, $k_{i}=\frac{\left\langle v, u_{i}\right\rangle}{\left\langle u_{1}, u_{i}\right\rangle}$. Substituting for $k_{i}$ in the equation $v=k_{1} u_{1}+\cdots+k_{n} u_{n}$, we obtain the desired result.

7.18. Suppose $E=\left\{e_{1}, e_{2}, \ldots, e_{n}\right\}$ is an orthonormal basis of $V$. Prove

(a) For any $u \in V$, we have $u=\left\langle u, e_{1}\right\rangle e_{1}+\left\langle u, e_{2}\right\rangle e_{2}+\cdots+\left\langle u, e_{n}\right\rangle e_{n}$.

(b) $\left\langle a_{1} e_{1}+\cdots+a_{n} e_{n}, \quad b_{1} e_{1}+\cdots+b_{n} e_{n}\right\rangle=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}$.

(c) For any $u, v \in V$, we have $\langle u, v\rangle=\left\langle u, e_{1}\right\rangle\left\langle v, e_{1}\right\rangle+\cdots+\left\langle u, e_{n}\right\rangle\left\langle v, e_{n}\right\rangle$.

(a) Suppose $u=k_{1} e_{1}+k_{2} e_{2}+\cdots+k_{n} e_{n}$. Taking the inner product of $u$ with $e_{1}$,

$$
\begin{aligned}
\left\langle u, e_{1}\right\rangle & =\left\langle k_{1} e_{1}+k_{2} e_{2}+\cdots+k_{n} e_{n}, \quad e_{1}\right\rangle \\
& =k_{1}\left\langle e_{1}, e_{1}\right\rangle+k_{2}\left\langle e_{2}, e_{1}\right\rangle+\cdots+k_{n}\left\langle e_{n}, e_{1}\right\rangle \\
& =k_{1}(1)+k_{2}(0)+\cdots+k_{n}(0)=k_{1}
\end{aligned}
$$

Similarly, for $i=2, \ldots, n$,

$$
\begin{aligned}
\left\langle u, e_{i}\right\rangle & =\left\langle k_{1} e_{1}+\cdots+k_{i} e_{i}+\cdots+k_{n} e_{n}, \quad e_{i}\right\rangle \\
& =k_{1}\left\langle e_{1}, e_{i}\right\rangle+\cdots+k_{i}\left\langle e_{i}, e_{i}\right\rangle+\cdots+k_{n}\left\langle e_{n}, e_{i}\right\rangle \\
& =k_{1}(0)+\cdots+k_{i}(1)+\cdots+k_{n}(0)=k_{i}
\end{aligned}
$$

Substituting $\left\langle u, e_{i}\right\rangle$ for $k_{i}$ in the equation $u=k_{1} e_{1}+\cdots+k_{n} e_{n}$, we obtain the desired result.

(b) We have

$$
\left\langle\sum_{i=1}^{n} a_{i} e_{i}, \quad \sum_{j=1}^{n} b_{j} e_{j}\right\rangle=\sum_{i, j=1}^{n} a_{i} b_{j}\left\langle e_{i}, e_{j}\right\rangle=\sum_{i=1}^{n} a_{i} b_{i}\left\langle e_{i}, e_{i}\right\rangle+\sum_{i \neq j} a_{i} b_{j}\left\langle e_{i}, e_{j}\right\rangle
$$

But $\left\langle e_{i}, e_{j}\right\rangle=0$ for $i \neq j$, and $\left\langle e_{i}, e_{j}\right\rangle=1$ for $i=j$. Hence, as required,

$$
\left\langle\sum_{i=1}^{n} a_{i} e_{i}, \quad \sum_{j=1}^{n} b_{j} e_{j}\right\rangle=\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}
$$

(c) By part (a), we have

$$
u=\left\langle u, e_{1}\right\rangle e_{1}+\cdots+\left\langle u, e_{n}\right\rangle e_{n} \quad \text { and } \quad v=\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}
$$

Thus, by part (b),

$$
\langle u, v\rangle=\left\langle u, e_{1}\right\rangle\left\langle v, e_{1}\right\rangle+\left\langle u, e_{2}\right\rangle\left\langle v, e_{2}\right\rangle+\cdots+\left\langle u, e_{n}\right\rangle\left\langle v, e_{n}\right\rangle
$$

\section*{Projections, Gram-Schmidt Algorithm, Applications}
7.19. Suppose $w \neq 0$. Let $v$ be any vector in $V$. Show that

$$
c=\frac{\langle v, w\rangle}{\langle w, w\rangle}=\frac{\langle v, w\rangle}{\|w\|^{2}}
$$

is the unique scalar such that $v^{\prime}=v-c w$ is orthogonal to $w$.

In order for $v^{\prime}$ to be orthogonal to $w$ we must have

$$
\langle v-c w, \quad w\rangle=0 \quad \text { or } \quad\langle v, w\rangle-c\langle w, w\rangle=0 \quad \text { or } \quad\langle v, w\rangle=c\langle w, w\rangle
$$

Thus, $c \frac{\langle v, w\rangle}{\langle w, w\rangle}$. Conversely, suppose $c=\frac{\langle v, w\rangle}{\langle w, w\rangle}$. Then

$$
\langle v-c w, \quad w\rangle=\langle v, w\rangle-c\langle w, w\rangle=\langle v, w\rangle-\frac{\langle v, w\rangle}{\langle w, w\rangle}\langle w, w\rangle=0
$$

7.20. Find the Fourier coefficient $c$ and the projection of $v=(1,-2,3,-4)$ along $w=(1,2,1,2)$ in $\mathbf{R}^{4}$.

Compute $\langle v, w\rangle=1-4+3-8=-8$ and $\|w\|^{2}=1+4+1+4=10$. Then

$$
c=-\frac{8}{10}=-\frac{4}{5} \quad \text { and } \quad \operatorname{proj}(v, w)=c w=\left(-\frac{4}{5},-\frac{8}{5},-\frac{4}{5},-\frac{8}{5}\right)
$$

7.21. Consider the subspace $U$ of $\mathbf{R}^{4}$ spanned by the vectors:

$$
v_{1}=(1,1,1,1), \quad v_{2}=(1,1,2,4), \quad v_{3}=(1,2,-4,-3)
$$

Find (a) an orthogonal basis of $U$; (b) an orthonormal basis of $U$.

(a) Use the Gram-Schmidt algorithm. Begin by setting $w_{1}=u=(1,1,1,1)$. Next find

$$
v_{2}-\frac{\left\langle v_{2}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}=(1,1,2,4)-\frac{8}{4}(1,1,1,1)=(-1,-1,0,2)
$$

Set $w_{2}=(-1,-1,0,2)$. Then find

$$
\begin{aligned}
v_{3}-\frac{\left\langle v_{3}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}-\frac{\left\langle v_{3}, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle} w_{2} & =(1,2,-4,-3)-\frac{(-4)}{4}(1,1,1,1)-\frac{(-9)}{6}(-1,-1,0,2) \\
& =\left(\frac{1}{2}, \frac{3}{2},-3,1\right)
\end{aligned}
$$

Clear fractions to obtain $w_{3}=(1,3,-6,2)$. Then $w_{1}, w_{2}, w_{3}$ form an orthogonal basis of $U$.\\
(b) Normalize the orthogonal basis consisting of $w_{1}, w_{2}, w_{3}$. Because $\left\|w_{1}\right\|^{2}=4,\left\|w_{2}\right\|^{2}=6$, and $\left\|w_{3}\right\|^{2}=50$, the following vectors form an orthonormal basis of $U$ :

$$
u_{1}=\frac{1}{2}(1,1,1,1), \quad u_{2}=\frac{1}{\sqrt{6}}(-1,-1,0,2), \quad u_{3}=\frac{1}{5 \sqrt{2}}(1,3,-6,2)
$$

7.22. Consider the vector space $\mathbf{P}(t)$ with inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$. Apply the GramSchmidt algorithm to the set $\left\{1, t, t^{2}\right\}$ to obtain an orthogonal set $\left\{f_{0}, f_{1}, f_{2}\right\}$ with integer coefficients.

First set $f_{0}=1$. Then find

$$
t-\frac{\langle t, 1\rangle}{\langle 1,1\rangle} \cdot 1=t-\frac{\frac{1}{2}}{1} \cdot 1=t-\frac{1}{2}
$$

Clear fractions to obtain $f_{1}=2 t-1$. Then find

$$
t^{2}-\frac{\left\langle t^{2}, 1\right\rangle}{\langle 1,1\rangle}(1)-\frac{\left\langle t^{2}, 2 t-1\right\rangle}{\langle 2 t-1,2 t-1\rangle}(2 t-1)=t^{2}-\frac{\frac{1}{3}}{1}(1)-\frac{\frac{1}{6}}{\frac{1}{3}}(2 t-1)=t^{2}-t+\frac{1}{6}
$$

Clear fractions to obtain $f_{2}=6 t^{2}-6 t+1$. Thus, $\left\{1,2 t-1,6 t^{2}-6 t+1\right\}$ is the required orthogonal set.

7.23. Suppose $v=(1,3,5,7)$. Find the projection of $v$ onto $W$ or, in other words, find $w \in W$ that minimizes $\|v-w\|$, where $W$ is the subspance of $\mathbf{R}^{4}$ spanned by

(a) $u_{1}=(1,1,1,1)$ and $u_{2}=(1,-3,4,-2)$,

(b) $v_{1}=(1,1,1,1)$ and $v_{2}=(1,2,3,2)$.

(a) Because $u_{1}$ and $u_{2}$ are orthogonal, we need only compute the Fourier coefficients:

$$
\begin{aligned}
& c_{1}=\frac{\left\langle v, u_{1}\right\rangle}{\left\langle u_{1}, u_{1}\right\rangle}=\frac{1+3+5+7}{1+1+1+1}=\frac{16}{4}=4 \\
& c_{2}=\frac{\left\langle v, u_{2}\right\rangle}{\left\langle u_{2}, u_{2}\right\rangle}=\frac{1-9+20-14}{1+9+16+4}=\frac{-2}{30}=-\frac{1}{15}
\end{aligned}
$$

Then $w=\operatorname{proj}(v, W)=c_{1} u_{1}+c_{2} u_{2}=4(1,1,1,1)-\frac{1}{15}(1,-3,4,-2)=\left(\frac{59}{15}, \frac{63}{5}, \frac{56}{15}, \frac{62}{15}\right)$.

(b) Because $v_{1}$ and $v_{2}$ are not orthogonal, first apply the Gram-Schmidt algorithm to find an orthogonal basis for $W$. Set $w_{1}=v_{1}=(1,1,1,1)$. Then find

$$
v_{2}-\frac{\left\langle v_{2}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}=(1,2,3,2)-\frac{8}{4}(1,1,1,1)=(-1,0,1,0)
$$

Set $w_{2}=(-1,0,1,0)$. Now compute

$$
\begin{aligned}
& c_{1}=\frac{\left\langle v, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle}=\frac{1+3+5+7}{1+1+1+1}=\frac{16}{4}=4 \\
& c_{2}=\frac{\left\langle v, w_{2}\right\rangle}{\left\langle w_{2}, w_{2}\right\rangle}-\frac{-1+0+5+0}{1+0+1+0}=\frac{-6}{2}=-3
\end{aligned}
$$

Then $w=\operatorname{proj}(v, W)=c_{1} w_{1}+c_{2} w_{2}=4(1,1,1,1)-3(-1,0,1,0)=(7,4,1,4)$.

7.24. Suppose $w_{1}$ and $w_{2}$ are nonzero orthogonal vectors. Let $v$ be any vector in $V$. Find $c_{1}$ and $c_{2}$ so that $v^{\prime}$ is orthogonal to $w_{1}$ and $w_{2}$, where $v^{\prime}=v-c_{1} w_{1}-c_{2} w_{2}$.

If $v^{\prime}$ is orthogonal to $w_{1}$, then

$$
\begin{aligned}
0 & =\left\langle v-c_{1} w_{1}-c_{2} w_{2}, \quad w_{1}\right\rangle=\left\langle v, w_{1}\right\rangle-c_{1}\left\langle w_{1}, w_{1}\right\rangle-c_{2}\left\langle w_{2}, w_{1}\right\rangle \\
& =\left\langle v, w_{1}\right\rangle-c_{1}\left\langle w_{1}, w_{1}\right\rangle-c_{2} 0=\left\langle v, w_{1}\right\rangle-c_{1}\left\langle w_{1}, w_{1}\right\rangle
\end{aligned}
$$

Thus, $c_{1}=\left\langle v, w_{1}\right\rangle /\left\langle w_{1}, w_{1}\right\rangle$. (That is, $c_{1}$ is the component of $v$ along $w_{1}$.) Similarly, if $v^{\prime}$ is orthogonal to $w_{2}$, then

$$
0=\left\langle v-c_{1} w_{1}-c_{2} w_{2}, \quad w_{2}\right\rangle=\left\langle v, w_{2}\right\rangle-c_{2}\left\langle w_{2}, w_{2}\right\rangle
$$

Thus, $c_{2}=\left\langle v, w_{2}\right\rangle /\left\langle w_{2}, w_{2}\right\rangle$. (That is, $c_{2}$ is the component of $v$ along $w_{2}$.)

7.25. Prove Theorem 7.8: Suppose $w_{1}, w_{2}, \ldots, w_{r}$ form an orthogonal set of nonzero vectors in $V$. Let $v \in V$. Define

$$
v^{\prime}=v-\left(c_{1} w_{1}+c_{2} w_{2}+\cdots+c_{r} w_{r}\right), \quad \text { where } \quad c_{i}=\frac{\left\langle v, w_{i}\right\rangle}{\left\langle w_{i}, w_{i}\right\rangle}
$$

Then $v^{\prime}$ is orthogonal to $w_{1}, w_{2}, \ldots, w_{r}$.

For $i=1,2, \ldots, r$ and using $\left\langle w_{i}, w_{j}\right\rangle=0$ for $i \neq j$, we have

$$
\begin{aligned}
\left\langle v-c_{1} w_{1}-c_{2} x_{2}-\cdots-c_{r} w_{r}, w_{i}\right\rangle & =\left\langle v, w_{i}\right\rangle-c_{1}\left\langle w_{1}, w_{i}\right\rangle-\cdots-c_{i}\left\langle w_{i}, w_{i}\right\rangle-\cdots-c_{r}\left\langle w_{r}, w_{i}\right\rangle \\
& =\left\langle v, w_{i}\right\rangle-c_{1} \cdot 0-\cdots-c_{i}\left\langle w_{i}, w_{i}\right\rangle-\cdots-c_{r} \cdot 0 \\
& =\left\langle v, w_{i}\right\rangle-c_{i}\left\langle w_{i}, w_{i}\right\rangle=\left\langle v, w_{i}\right\rangle-\frac{\left\langle v, w_{i}\right\rangle}{\left\langle w_{i}, w_{i}\right\rangle}\left\langle w_{i}, w_{i}\right\rangle=0
\end{aligned}
$$

The theorem is proved.

7.26. Prove Theorem 7.9: Let $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ be any basis of an inner product space $V$. Then there exists an orthonormal basis $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of $V$ such that the change-of-basis matrix from $\left\{v_{i}\right\}$ to $\left\{u_{i}\right\}$ is triangular; that is, for $k=1,2, \ldots, n$,

$$
u_{k}=a_{k 1} v_{1}+a_{k 2} v_{2}+\cdots+a_{k k} v_{k}
$$

The proof uses the Gram-Schmidt algorithm and Remarks 1 and 3 of Section 7.7. That is, apply the algorithm to $\left\{v_{i}\right\}$ to obtain an orthogonal basis $\left\{w_{i}, \ldots, w_{n}\right\}$, and then normalize $\left\{w_{i}\right\}$ to obtain an orthonormal basis $\left\{u_{i}\right\}$ of $V$. The specific algorithm guarantees that each $w_{k}$ is a linear combination of $v_{1}, \ldots, v_{k}$, and hence, each $u_{k}$ is a linear combination of $v_{1}, \ldots, v_{k}$.

7.27. Prove Theorem 7.10: Suppose $S=\left\{w_{1}, w_{2}, \ldots, w_{r}\right\}$, is an orthogonal basis for a subspace $W$ of $V$. Then one may extend $S$ to an orthogonal basis for $V$; that is, one may find vectors $w_{r+1}, \ldots, w_{r}$ such that $\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ is an orthogonal basis for $V$.

Extend $S$ to a basis $S^{\prime}=\left\{w_{1}, \ldots, w_{r}, v_{r+1}, \ldots, v_{n}\right\}$ for $V$. Applying the Gram-Schmidt algorithm to $S^{\prime}$, we first obtain $w_{1}, w_{2}, \ldots, w_{r}$ because $S$ is orthogonal, and then we obtain vectors $w_{r+1}, \ldots, w_{n}$, where $\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ is an orthogonal basis for $V$. Thus, the theorem is proved.

7.28. Prove Theorem 7.4: Let $W$ be a subspace of $V$. Then $V=W \oplus W^{\perp}$.

By Theorem 7.9, there exists an orthogonal basis $\left\{u_{1}, \ldots, u_{r}\right\}$ of $W$, and by Theorem 7.10 we can extend it to an orthogonal basis $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of $V$. Hence, $u_{r+1}, \ldots, u_{n} \in W^{\perp}$. If $v \in V$, then

$$
v=a_{1} u_{1}+\cdots+a_{n} u_{n}, \text { where } a_{1} u_{1}+\cdots+a_{r} u_{r} \in W \text { and } a_{r+1} u_{r+1}+\cdots+a_{n} u_{n} \in W^{\perp}
$$

Accordingly, $V=W+W^{\perp}$.

On the other hand, if $w \in W \cap W^{\perp}$, then $\langle w, w\rangle=0$. This yields $w=0$. Hence, $W \cap W^{\perp}=\{0\}$.

The two conditions $V=W+W^{\perp}$ and $W \cap W^{\perp}=\{0\}$ give the desired result $V=W \oplus W^{\perp}$.

Remark: Note that we have proved the theorem for the case that $V$ has finite dimension. We remark that the theorem also holds for spaces of arbitrary dimension.

7.29. Suppose $W$ is a subspace of a finite-dimensional space $V$. Prove that $W=W^{\perp \perp}$.

By Theorem 7.4, $V=W \oplus W^{\perp}$, and also $V=W^{\perp} \oplus W^{\perp \perp}$. Hence,

$$
\operatorname{dim} W=\operatorname{dim} V-\operatorname{dim} W^{\perp} \quad \text { and } \quad \operatorname{dim} W^{\perp \perp}=\operatorname{dim} V-\operatorname{dim} W^{\perp}
$$

This yields $\operatorname{dim} W=\operatorname{dim} W^{\perp \perp}$. But $W \subseteq W^{\perp \perp}$ (see Problem 7.14). Hence, $W=W^{\perp \perp}$, as required.

7.30. Prove the following: Suppose $w_{1}, w_{2}, \ldots, w_{r}$ form an orthogonal set of nonzero vectors in $V$. Let $v$ be any vector in $V$ and let $c_{i}$ be the component of $v$ along $w_{i}$. Then, for any scalars $a_{1}, \ldots, a_{r}$, we have

$$
\left\|v-\sum_{k=1}^{r} c_{k} w_{k}\right\| \leq\left\|v-\sum_{k=1}^{r} a_{k} w_{k}\right\|
$$

That is, $\sum c_{i} w_{i}$ is the closest approximation to $v$ as a linear combination of $w_{1}, \ldots, w_{r}$.

By Theorem 7.8, $v-\sum c_{k} w_{k}$ is orthogonal to every $w_{i}$ and hence orthogonal to any linear combination of $w_{1}, w_{2}, \ldots, w_{r}$. Therefore, using the Pythagorean theorem and summing from $k=1$ to $r$,

$$
\begin{aligned}
\left\|v-\sum a_{k} w_{k}\right\|^{2} & =\left\|v-\sum c_{k} w_{k}+\sum\left(c_{k}-a_{k}\right) w_{k}\right\|^{2}=\left\|v-\sum c_{k} w_{k}\right\|^{2}+\left\|\sum\left(c_{k}-a_{k}\right) w_{k}\right\|^{2} \\
& \geq\left\|v-\sum c_{k} w_{k}\right\|^{2}
\end{aligned}
$$

The square root of both sides gives our theorem.

7.31. Suppose $\left\{e_{1}, e_{2}, \ldots, e_{r}\right\}$ is an orthonormal set of vectors in $V$. Let $v$ be any vector in $V$ and let $c_{i}$ be the Fourier coefficient of $v$ with respect to $u_{i}$. Prove Bessel's inequality:

$$
\sum_{k=1}^{r} c_{k}^{2} \leq\|v\|^{2}
$$

Note that $c_{i}=\left\langle v, e_{i}\right\rangle$, because $\left\|e_{i}\right\|=1$. Then, using $\left\langle e_{i}, e_{j}\right\rangle=0$ for $i \neq j$ and summing from $k=1$ to $r$, we get

$$
\begin{aligned}
0 & \leq\left\langle v-\sum c_{k} e_{k}, \quad v-\sum c_{k}, e_{k}\right\rangle=\langle v, v\rangle-2\left\langle v, \quad \sum c_{k} e_{k}\right\rangle+\sum c_{k}^{2}=\langle v, v\rangle-\sum 2 c_{k}\left\langle v, e_{k}\right\rangle+\sum c_{k}^{2} \\
& =\langle v, v\rangle-\sum 2 c_{k}^{2}+\sum c_{k}^{2}=\langle v, v\rangle-\sum c_{k}^{2}
\end{aligned}
$$

This gives us our inequality.

\section*{Orthogonal Matrices}
7.32. Find an orthogonal matrix $P$ whose first row is $u_{1}=\left(\frac{1}{3}, \frac{2}{3}, \frac{2}{3}\right)$.

First find a nonzero vector $w_{2}=(x, y, z)$ that is orthogonal to $u_{1}$-that is, for which

$$
0=\left\langle u_{1}, w_{2}\right\rangle=\frac{x}{3}+\frac{2 y}{3}+\frac{2 z}{3}=0 \quad \text { or } \quad x+2 y+2 z=0
$$

One such solution is $w_{2}=(0,1,-1)$. Normalize $w_{2}$ to obtain the second row of $P$ :

$$
u_{2}=(0,1 / \sqrt{2},-1 / \sqrt{2})
$$

Next find a nonzero vector $w_{3}=(x, y, z)$ that is orthogonal to both $u_{1}$ and $u_{2}$-that is, for which

$$
\begin{array}{lll}
0=\left\langle u_{1}, w_{3}\right\rangle=\frac{x}{3}+\frac{2 y}{3}+\frac{2 z}{3}=0 & \text { or } & x+2 y+2 z=0 \\
0=\left\langle u_{2}, w_{3}\right\rangle=\frac{y}{\sqrt{2}}-\frac{y}{\sqrt{2}}=0 & \text { or } & y-z=0
\end{array}
$$

Set $z=-1$ and find the solution $w_{3}=(4,-1,-1)$. Normalize $w_{3}$ and obtain the third row of $P$; that is,

Thus,

$$
\begin{gathered}
u_{3}=(4 / \sqrt{18},-1 / \sqrt{18},-1 / \sqrt{18}) . \\
P=\left[\begin{array}{ccc}
\frac{1}{3} & \frac{2}{3} & \frac{2}{3} \\
0 & 1 / \sqrt{2} & -1 / \sqrt{2} \\
4 / 3 \sqrt{2} & -1 / 3 \sqrt{2} & -1 / 3 \sqrt{2}
\end{array}\right]
\end{gathered}
$$

We emphasize that the above matrix $P$ is not unique.

7.33. Let $A=\left[\begin{array}{rrr}1 & 1 & -1 \\ 1 & 3 & 4 \\ 7 & -5 & 2\end{array}\right]$. Determine whether or not: (a) the rows of $A$ are orthogonal;

(b) $A$ is an orthogonal matrix; (c) the columns of $A$ are orthogonal.

(a) Yes, because $(1,1,-1) \cdot(1,3,4)=1+3-4=0, \quad(1,1-1) \cdot(7,-5,2)=7-5-2=0$, and $(1,3,4) \cdot(7,-5,2)=7-15+8=0$.

(b) No, because the rows of $A$ are not unit vectors, for example, $(1,1,-1)^{2}=1+1+1=3$.

(c) No; for example, $(1,1,7) \cdot(1,3,-5)=1+3-35=-31 \neq 0$.

7.34. Let $B$ be the matrix obtained by normalizing each row $A$ in Problem 7.33 .

(a) Find $B$.

(b) Is $B$ an orthogonal matrix?

(c) Are the columns of $B$ orthogonal?\\
(a) We have

$$
\begin{aligned}
& \|(1,1,-1)\|^{2}=1+1+1=3, \quad\|(1,3,4)\|^{2}=1+9+16=26 \\
& \|(7,-5,2)\|^{2}=49+25+4=78 \\
& B=\left[\begin{array}{ccc}
1 / \sqrt{3} & 1 / \sqrt{3} & -1 / \sqrt{3} \\
1 / \sqrt{26} & 3 / \sqrt{26} & 4 / \sqrt{26} \\
7 / \sqrt{78} & -5 / \sqrt{78} & 2 / \sqrt{78}
\end{array}\right]
\end{aligned}
$$

Thus,

(b) Yes, because the rows of $B$ are still orthogonal and are now unit vectors.

(c) Yes, because the rows of $B$ form an orthonormal set of vectors. Then, by Theorem 7.11, the columns of $B$ must automatically form an orthonormal set.

7.35. Prove each of the following:

(a) $P$ is orthogonal if and only if $P^{T}$ is orthogonal.

(b) If $P$ is orthogonal, then $P^{-1}$ is orthogonal.

(c) If $P$ and $Q$ are orthogonal, then $P Q$ is orthogonal.

(a) We have $\left(P^{T}\right)^{T}=P$. Thus, $P$ is orthogonal if and only if $P P^{T}=I$ if and only if $P^{T T} P^{T}=I$ if and only if $P^{T}$ is orthogonal.

(b) We have $P^{T}=P^{-1}$, because $P$ is orthogonal. Thus, by part (a), $P^{-1}$ is orthogonal.

(c) We have $P^{T}=P^{-1}$ and $Q^{T}=Q^{-1}$. Thus, $(P Q)(P Q)^{T}=P Q Q^{T} P^{T}=P Q Q^{-1} P^{-1}=I$. Therefore, $(P Q)^{T}=(P Q)^{-1}$, and so $P Q$ is orthogonal.

7.36. Suppose $P$ is an orthogonal matrix. Show that

(a) $\langle P u, P v\rangle=\langle u, v\rangle$ for any $u, v \in V$;

(b) $\|P u\|=\|u\|$ for every $u \in V$.

Use $P^{T} P=I$ and $\langle u, v\rangle=u^{T} v$.

(a) $\langle P u, P v\rangle=(P u)^{T}(P v)=u^{T} P^{T} P v=u^{T} v=\langle u, v\rangle$.

(b) We have

$$
\|P u\|^{2}=\langle P u, P u\rangle=u^{T} P^{T} P u=u^{T} u=\langle u, u\rangle=\|u\|^{2}
$$

Taking the square root of both sides gives our result.

7.37. Prove Theorem 7.12: Suppose $E=\left\{e_{i}\right\}$ and $E^{\prime}=\left\{e_{i}^{\prime}\right\}$ are orthonormal bases of $V$. Let $P$ be the change-of-basis matrix from $E$ to $E^{\prime}$. Then $P$ is orthogonal.

Suppose


\begin{equation*}
e_{i}^{\prime}=b_{i 1} e_{1}+b_{i 2} e_{2}+\cdots+b_{i n} e_{n}, \quad i=1, \ldots, n \tag{1}
\end{equation*}


Using Problem 7.18(b) and the fact that $E^{\prime}$ is orthonormal, we get


\begin{equation*}
\delta_{i j}=\left\langle e_{i}^{\prime}, e_{j}^{\prime}\right\rangle=b_{i 1} b_{j 1}+b_{i 2} b_{j 2}+\cdots+b_{i n} b_{j n} \tag{2}
\end{equation*}


Let $B=\left[b_{i j}\right]$ be the matrix of the coefficients in (1). (Then $P=B^{T}$.) Suppose $B B^{T}=\left[c_{i j}\right]$. Then


\begin{equation*}
c_{i j}=b_{i 1} b_{j 1}+b_{i 2} b_{j 2}+\cdots+b_{i n} b_{j n} \tag{3}
\end{equation*}


By (2) and (3), we have $c_{i j}=\delta_{i j}$. Thus, $B B^{T}=I$. Accordingly, $B$ is orthogonal, and hence, $P=B^{T}$ is orthogonal.

7.38. Prove Theorem 7.13: Let $\left\{e_{1}, \ldots, e_{n}\right\}$ be an orthonormal basis of an inner product space $V$. Let $P=\left[a_{i j}\right]$ be an orthogonal matrix. Then the following $n$ vectors form an orthonormal basis for $V$ :

$$
e_{i}^{\prime}=a_{1 i} e_{1}+a_{2 i} e_{2}+\cdots+a_{n i} e_{n}, \quad i=1,2, \ldots, n
$$

Because $\left\{e_{i}\right\}$ is orthonormal, we get, by Problem 7.18(b),

$$
\left\langle e_{i}^{\prime}, e_{j}^{\prime}\right\rangle=a_{1 i} a_{1 j}+a_{2 i} a_{2 j}+\cdots+a_{n i} a_{n j}=\left\langle C_{i}, C_{j}\right\rangle
$$

where $C_{i}$ denotes the $i$ th column of the orthogonal matrix $P=\left[a_{i j}\right]$. Because $P$ is orthogonal, its columns form an orthonormal set. This implies $\left\langle e_{i}^{\prime}, e_{j}^{\prime}\right\rangle=\left\langle C_{i}, C_{j}\right\rangle=\delta_{i j}$. Thus, $\left\{e_{i}^{\prime}\right\}$ is an orthonormal basis.

\section*{Inner Products And Positive Definite Matrices}
7.39. Which of the following symmetric matrices are positive definite?\\
(a) $A=\left[\begin{array}{ll}3 & 4 \\ 4 & 5\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rr}8 & -3 \\ -3 & 2\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rr}2 & 1 \\ 1 & -3\end{array}\right]$,\\
(d) $D=\left[\begin{array}{ll}3 & 5 \\ 5 & 9\end{array}\right]$

Use Theorem 7.14 that a $2 \times 2$ real symmetric matrix is positive definite if and only if its diagonal entries are positive and if its determinant is positive.

(a) No, because $|A|=15-16=-1$ is negative.

(b) Yes.

(c) No, because the diagonal entry -3 is negative.

(d) Yes.

7.40. Find the values of $k$ that make each of the following matrices positive definite:

(a) $A=\left[\begin{array}{rr}2 & -4 \\ -4 & k\end{array}\right]$, (b) $B=\left[\begin{array}{ll}4 & k \\ k & 9\end{array}\right]$, (c) $C=\left[\begin{array}{rr}k & 5 \\ 5 & -2\end{array}\right]$

(a) First, $k$ must be positive. Also, $|A|=2 k-16$ must be positive; that is, $2 k-16>0$. Hence, $k>8$.

(b) We need $|B|=36-k^{2}$ positive; that is, $36-k^{2}>0$. Hence, $k^{2}<36$ or $-6<k<6$.

(c) $C$ can never be positive definite, because $C$ has a negative diagonal entry -2 .

7.41. Find the matrix $A$ that represents the usual inner product on $\mathbf{R}^{2}$ relative to each of the following bases of $\mathbf{R}^{2}$ : (a) $\left\{v_{1}=(1,3), \quad v_{2}=(2,5)\right\}$; (b) $\left\{w_{1}=(1,2), \quad w_{2}=(4,-2)\right\}$.

(a) Compute $\left\langle v_{1}, v_{1}\right\rangle=1+9=10,\left\langle v_{1}, v_{2}\right\rangle=2+15=17,\left\langle v_{2}, v_{2}\right\rangle=4+25=29$. Thus, $A=\left[\begin{array}{ll}10 & 17 \\ 17 & 29\end{array}\right]$

(b) Compute $\left\langle w_{1}, w_{1}\right\rangle=1+4=5,\left\langle w_{1}, w_{2}\right\rangle=4-4=0,\left\langle w_{2}, w_{2}\right\rangle=16+4=20$. Thus, $A=\left[\begin{array}{rr}5 & 0 \\ 0 & 20\end{array}\right]$.\\
(Because the basis vectors are orthogonal, the matrix $A$ is diagonal.)

7.42. Consider the vector space $\mathbf{P}_{2}(t)$ with inner product $\langle f, g\rangle=\int_{-1}^{1} f(t) g(t) d t$.

(a) Find $\langle f, g\rangle$, where $f(t)=t+2$ and $g(t)=t^{2}-3 t+4$.

(b) Find the matrix $A$ of the inner product with respect to the basis $\left\{1, t, t^{2}\right\}$ of $V$.

(c) Verify Theorem 7.16 by showing that $\langle f, g\rangle=[f]^{T} A[g]$ with respect to the basis $\left\{1, t, t^{2}\right\}$.

(a) $\langle f, g\rangle=\int_{-1}^{1}(t+2)\left(t^{2}-3 t+4\right) d t=\int_{-1}^{1}\left(t^{3}-t^{2}-2 t+8\right) d t=\left.\left(\frac{t^{4}}{4}-\frac{t^{3}}{3}-t^{2}+8 t\right)\right|_{-1} ^{1}=\frac{46}{3}$

(b) Here we use the fact that if $r+s=n$,

$$
\left\langle t^{r}, t^{r}\right\rangle=\int_{-1}^{1} t^{n} d t=\left.\frac{t^{n+1}}{n+1}\right|_{-1} ^{1}= \begin{cases}2 /(n+1) & \text { if } n \text { is even, } \\ 0 & \text { if } n \text { is odd. }\end{cases}
$$

Then $\langle 1,1\rangle=2,\langle 1, t\rangle=0,\left\langle 1, t^{2}\right\rangle=\frac{2}{3},\langle t, t\rangle=\frac{2}{3},\left\langle t, t^{2}\right\rangle=0,\left\langle t^{2}, t^{2}\right\rangle=\frac{2}{5}$. Thus,

$$
A=\left[\begin{array}{ccc}
2 & 0 & \frac{2}{3} \\
0 & \frac{2}{3} & 0 \\
\frac{2}{3} & 0 & \frac{2}{5}
\end{array}\right]
$$

(c) We have $[f]^{T}=(2,1,0)$ and $[g]^{T}=(4,-3,1)$ relative to the given basis. Then

$$
[f]^{T} A[g]=(2,1,0)\left[\begin{array}{lll}
2 & 0 & \frac{2}{3} \\
0 & \frac{2}{3} & 0 \\
\frac{2}{3} & 0 & \frac{2}{5}
\end{array}\right]\left[\begin{array}{r}
4 \\
-3 \\
1
\end{array}\right]=\left(4, \frac{2}{3}, \frac{4}{3}\right)\left[\begin{array}{r}
4 \\
-3 \\
1
\end{array}\right]=\frac{46}{3}=\langle f, g\rangle
$$

7.43. Prove Theorem 7.14: $A=\left[\begin{array}{ll}a & b \\ b & c\end{array}\right]$ is positive definite if and only if $a$ and $d$ are positive and\\
$|A|=a d-b^{2}$ is positive.

Let $u=[x, y]^{T}$. Then

$$
f(u)=u^{T} A u=[x, y]\left[\begin{array}{ll}
a & b \\
b & d
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=a x^{2}+2 b x y+d y^{2}
$$

Suppose $f(u)>0$ for every $u \neq 0$. Then $f(1,0)=a>0$ and $f(0,1)=d>0$. Also, we have $f(b,-a)=a\left(a d-b^{2}\right)>0$. Because $a>0$, we get $a d-b^{2}>0$.

Conversely, suppose $a>0, b=0, a d-b^{2}>0$. Completing the square gives us

$$
f(u)=a\left(x^{2}+\frac{2 b}{a} x y+\frac{b^{2}}{a_{2}} y^{2}\right)+d y^{2}-\frac{b^{2}}{a} y^{2}=a\left(x+\frac{b y}{a}\right)^{2}+\frac{a d-b^{2}}{a} y^{2}
$$

Accordingly, $f(u)>0$ for every $u \neq 0$.

7.44. Prove Theorem 7.15: Let $A$ be a real positive definite matrix. Then the function $\langle u, v\rangle=u^{T} A v$ is an inner product on $\mathbf{R}^{n}$.

For any vectors $u_{1}, u_{2}$, and $v$,

$$
\left\langle u_{1}+u_{2}, \quad v\right\rangle=\left(u_{1}+u_{2}\right)^{T} A v=\left(u_{1}^{T}+u_{2}^{T}\right) A v=u_{1}^{T} A v+u_{2}^{T} A v=\left\langle u_{1}, v\right\rangle+\left\langle u_{2}, v\right\rangle
$$

and, for any scalar $k$ and vectors $u, v$,

$$
\langle k u, v\rangle=(k u)^{T} A v=k u^{T} A v=k\langle u, v\rangle
$$

Thus $\left[\mathrm{I}_{1}\right]$ is satisfied.

Because $u^{T} A v$ is a scalar, $\left(u^{T} A v\right)^{T}=u^{T} A v$. Also, $A^{T}=A$ because $A$ is symmetric. Therefore,

$$
\langle u, v\rangle=u^{T} A v=\left(u^{T} A v\right)^{T}=v^{T} A^{T} u^{T T}=v^{T} A u=\langle v, u\rangle
$$

Thus, $\left[\mathrm{I}_{2}\right]$ is satisfied.

Last, because $A$ is positive definite, $X^{T} A X>0$ for any nonzero $X \in \mathbf{R}^{n}$. Thus, for any nonzero vector $v,\langle v, v\rangle=v^{T} A v>0$. Also, $\langle 0,0\rangle=0^{T} A 0=0$. Thus, $\left[\mathrm{I}_{3}\right]$ is satisfied. Accordingly, the function $\langle u, v\rangle=A v$ is an inner product.

7.45. Prove Theorem 7.16: Let $A$ be the matrix representation of an inner product relative to a basis $S$ of $V$. Then, for any vectors $u, v \in V$, we have

$$
\langle u, v\rangle=[u]^{T} A[v]
$$

Suppose $S=\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$ and $A=\left[k_{i j}\right]$. Hence, $k_{i j}=\left\langle w_{i}, w_{j}\right\rangle$. Suppose

$$
u=a_{1} w_{1}+a_{2} w_{2}+\cdots+a_{n} w_{n} \quad \text { and } \quad v=b_{1} w_{1}+b_{2} w_{2}+\cdots+b_{n} w_{n}
$$

Then


\begin{equation*}
\langle u, v\rangle=\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i} b_{j}\left\langle w_{i}, w_{j}\right\rangle \tag{1}
\end{equation*}


On the other hand,


\begin{align*}
{[u]^{T} A[v] } & =\left(a_{1}, a_{2}, \ldots, a_{n}\right)\left[\begin{array}{cccc}
k_{11} & k_{12} & \ldots & k_{1 n} \\
k_{21} & k_{22} & \ldots & k_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
k_{n 1} & k_{n 2} & \ldots & k_{n n}
\end{array}\right]\left[\begin{array}{c}
b_{1} \\
b_{2} \\
\vdots \\
b_{n}
\end{array}\right] \\
& =\left(\sum_{i=1}^{n} a_{i} k_{i 1}, \sum_{i=1}^{n} a_{i} k_{i 2}, \ldots, \sum_{i=1}^{n} a_{i} k_{i n}\right)\left[\begin{array}{c}
b_{1} \\
b_{2} \\
\vdots \\
b_{n}
\end{array}\right]=\sum_{j=1}^{n} \sum_{i=1}^{n} a_{i} b_{j} k_{i j} \tag{2}
\end{align*}


Equations (1) and (2) give us our result.

7.46. Prove Theorem 7.17: Let $A$ be the matrix representation of any inner product on $V$. Then $A$ is a positive definite matrix.

Because $\left\langle w_{i}, w_{j}\right\rangle=\left\langle w_{j}, w_{i}\right\rangle$ for any basis vectors $w_{i}$ and $w_{j}$, the matrix $A$ is symmetric. Let $X$ be any nonzero vector in $\mathbf{R}^{n}$. Then $[u]=X$ for some nonzero vector $u \in V$. Theorem 7.16 tells us that $X^{T} A X=[u]^{T} A[u]=\langle u, u\rangle>0$. Thus, $A$ is positive definite.

\section*{Complex Inner Product Spaces}
7.47. Let $V$ be a complex inner product space. Verify the relation

$$
\left\langle u, a v_{1}+b v_{2}\right\rangle=\bar{a}\left\langle u, v_{1}\right\rangle+\bar{b}\left\langle u, v_{2}\right\rangle
$$

Using $\left[I_{2}^{*}\right],\left[I_{1}^{*}\right]$, and then $\left[I_{2}^{*}\right]$, we find

$$
\left\langle u, a v_{1}+b v_{2}\right\rangle=\overline{\left\langle a v_{1}+b v_{2}, u\right\rangle}=\overline{a\left\langle v_{1}, u\right\rangle+b\left\langle v_{2}, u\right\rangle}=\bar{a} \overline{\left\langle v_{1}, u\right\rangle}+\bar{b}\left\langle v_{2}, u\right\rangle=\bar{a}\left\langle u, v_{1}\right\rangle+\bar{b}\left\langle u, v_{2}\right\rangle
$$

7.48. Suppose $\langle u, v\rangle=3+2 i$ in a complex inner product space $V$. Find

(a) $\langle(2-4 i) u, v\rangle$; (b) $\langle u,(4+3 i) v\rangle$; (c) $\langle(3-6 i) u$, $(5-2 i) v\rangle$.

(a) $\langle(2-4 i) u, v\rangle=(2-4 i)\langle u, v\rangle=(2-4 i)(3+2 i)=14-8 i$

(b) $\langle u, \quad(4+3 i) v\rangle=\overline{(4+3 i)}\langle u, v\rangle=(4-3 i)(3+2 i)=18-i$

(c) $\langle(3-6 i) u, \quad(5-2 i) v\rangle=(3-6 i) \overline{(5-2 i)}\langle u, v\rangle=(3-6 i)(5+2 i)(3+2 i)=129-18 i$

7.49. Find the Fourier coefficient (component) $c$ and the projection $c w$ of $v=(3+4 i, 2-3 i)$ along $w=(5+i, 2 i)$ in $\mathbf{C}^{2}$.

Recall that $c=\langle v, w\rangle /\langle w, w\rangle$. Compute

$$
\begin{aligned}
\langle v, w\rangle & =(3+4 i)(\overline{5+i})+(2-3 i)(\overline{2 i})=(3+4 i)(5-i)+(2-3 i)(-2 i) \\
& =19+17 i-6-4 i=13+13 i \\
\langle w, w\rangle & =25+1+4=30
\end{aligned}
$$

Thus, $c=(13+13 i) / 30=\frac{13}{30}+\frac{13}{30} i$. Accordingly, $\operatorname{proj}(v, w)=c w=\left(\frac{26}{15}+\frac{39}{15} i,-\frac{13}{15}+\frac{1}{15} i\right)$

7.50. Prove Theorem 7.18 (Cauchy-Schwarz): Let $V$ be a complex inner product space. Then $|\langle u, v\rangle| \leq\|u\|\|v\|$.

If $v=0$, the inequality reduces to $0 \leq 0$ and hence is valid. Now suppose $v \neq 0$. Using $z \bar{z}=|z|^{2}$ (for any complex number $z$ ) and $\langle v, u\rangle=\overline{\langle u, v\rangle}$, we expand $\|u-\langle u, v\rangle t v\|^{2} \geq 0$, where $t$ is any real value:

$$
\begin{aligned}
0 & \leq\|u-\langle u, v\rangle t v\|^{2}=\langle u-\langle u, v\rangle t v, u-\langle u, v\rangle t v\rangle \\
& =\langle u, u\rangle-\overline{\langle u, v\rangle} t\langle u, v\rangle-\langle u, v) t\langle v, u\rangle+\langle u, v\rangle \overline{\langle u, v\rangle} t^{2}\langle v, v\rangle \\
& =\|u\|^{2}-2 t|\langle u, v\rangle|^{2}+|\langle u, v\rangle|^{2} t^{2}\|v\|^{2}
\end{aligned}
$$

Set $t=1 /\|v\|^{2}$ to find $0 \leq\|u\|^{2}-\frac{|\langle u, v\rangle|^{2}}{\|v\|^{2}}$, from which $|\langle u, v\rangle|^{2} \leq\|v\|^{2}\|v\|^{2}$. Taking the square root of both sides, we obtain the required inequality.

7.51. Find an orthogonal basis for $u^{\perp}$ in $C^{3}$ where $u=(1, i, 1+i)$.

Here $u^{\perp}$ consists of all vectors $s=(x, y, z)$ such that

$$
\langle w, u\rangle=x-i y+(1-i) z=0
$$

Find one solution, say $w_{1}=(0,1-i, i)$. Then find a solution of the system

$$
x-i y+(1-i) z=0, \quad(1+i) y-i z=0
$$

Here $z$ is a free variable. Set $z=1$ to obtain $y=i /(1+i)=(1+i) / 2$ and $x=(3 i-3) 2$. Multiplying by 2 yields the solution $w_{2}=(3 i-3,1+i, 2)$. The vectors $w_{1}$ and $w_{2}$ form an orthogonal basis for $u^{\perp}$.

7.52. Find an orthonormal basis of the subspace $W$ of $\mathbf{C}^{3}$ spanned by

$$
v_{1}=(1, i, 0) \quad \text { and } \quad v_{2}=(1,2,1-i) .
$$

Apply the Gram-Schmidt algorithm. Set $w_{1}=v_{1}=(1, i, 0)$. Compute

$$
v_{2}-\frac{\left\langle v_{2}, w_{1}\right\rangle}{\left\langle w_{1}, w_{1}\right\rangle} w_{1}=(1, \quad 2, \quad 1-i)-\frac{1-2 i}{2}(1, i, 0)=\left(\frac{1}{2}+i, \quad 1-\frac{1}{2} i, \quad 1-i\right)
$$

Multiply by 2 to clear fractions, obtaining $w_{2}=(1+2 i, 2-i, 2-2 i)$. Next find $\left\|w_{1}\right\|=\sqrt{2}$ and then $\left\|w_{2}\right\|=\sqrt{18}$. Normalizing $\left\{w_{1}, w_{2}\right\}$, we obtain the following orthonormal basis of $W$ :

$$
\left\{u_{1}=\left(\frac{1}{\sqrt{2}}, \frac{i}{\sqrt{2}}, 0\right), u_{2}=\left(\frac{1+2 i}{\sqrt{18}}, \frac{2-i}{\sqrt{18}}, \frac{2-2 i}{\sqrt{18}}\right)\right\}
$$

7.53. Find the matrix $P$ that represents the usual inner product on $\mathbf{C}^{3}$ relative to the basis $\{1, i, 1-i\}$.

Compute the following six inner products:

$$
\begin{array}{lll}
\langle 1,1\rangle=1, & \langle 1, i\rangle=\bar{i}=-i, & \langle 1,1-i\rangle=\overline{1-i}=1+i \\
\langle i, i\rangle=\bar{i} i=1, & \langle i, 1-i\rangle=i(\overline{1-i})=-1+i, & \langle 1-i, 1-i\rangle=2
\end{array}
$$

Then, using $(u, v)=\overline{\langle v, u\rangle}$, we obtain

$$
P=\left[\begin{array}{ccc}
1 & -i & 1+i \\
i & 1 & -1+i \\
1-i & -1-i & 2
\end{array}\right]
$$

(As expected, $P$ is Hermitian; that is, $P^{H}=P$.)

\section*{Normed Vector Spaces}
7.54. Consider vectors $u=(1,3,-6,4)$ and $v=(3,-5,1,-2)$ in $\mathbf{R}^{4}$. Find

(a) $\|u\|_{\infty}$ and $\|\left. v\right|_{\infty}$, (b) $\|u\|_{1}$ and $\|v\|_{1}$, (c) $\|u\|_{2}$ and $\|v\|_{2}$,

(d) $d_{\infty}(u, v), d_{1}(u, v), d_{2}(u, v)$.

(a) The infinity norm chooses the maximum of the absolute values of the components. Hence,

$$
\|u\|_{\infty}=6 \quad \text { and } \quad\|v\|_{\infty}=5
$$

(b) The one-norm adds the absolute values of the components. Thus,

$$
\|u\|_{1}=1+3+6+4=14 \quad \text { and } \quad\|v\|_{1}=3+5+1+2=11
$$

(c) The two-norm is equal to the square root of the sum of the squares of the components (i.e., the norm induced by the usual inner product on $\mathbf{R}^{3}$ ). Thus,

$$
\|u\|_{2}=\sqrt{1+9+36+16}=\sqrt{62} \quad \text { and } \quad\|v\|_{2}=\sqrt{9+25+1+4}=\sqrt{39}
$$

(d) First find $u-v=(-2,8,-7,6)$. Then

$$
\begin{aligned}
d_{\infty}(u, v) & =\|u-v\|_{\infty}=8 \\
d_{1}(u, v) & =\|u-v\|_{1}=2+8+7+6=23 \\
d_{2}(u, v) & =\|u-v\|_{2}=\sqrt{4+64+49+36}=\sqrt{153}
\end{aligned}
$$

7.55. Consider the function $f(t)=t^{2}-4 t$ in $C[0,3]$.

(a) Find $\|f\|_{\infty}$, (b) Plot $f(t)$ in the plane $\mathbf{R}^{2}$, (c) Find $\|f\|_{1}$, (d) Find $\|f\|_{2}$.

(a) We seek $\|f\|_{\infty}=\max (|f(t)|)$. Because $f(t)$ is differentiable on $[0,3],|f(t)|$ has a maximum at a critical point of $f(t)$ (i.e., when the derivative $f^{\prime}(t)=0$ ), or at an endpoint of $[0,3]$. Because $f^{\prime}(t)=2 t-4$, we set $2 t-4=0$ and obtain $t=2$ as a critical point. Compute

$$
f(2)=4-8=-4, \quad f(0)=0-0=0, \quad f(3)=9-12=-3
$$

Thus, $\|f\|_{\infty}=|f(2)|=|-4|=4$.\\
(b) Compute $f(t)$ for various values of $t$ in $[0,3]$, for example,

$$
\begin{array}{c|rrrr}
t & 0 & 1 & 2 & 3 \\
\hline f(t) & 0 & -3 & -4 & -3
\end{array}
$$

Plot the points in $\mathbf{R}^{2}$ and then draw a continuous curve through the points, as shown in Fig. 7-8.

(c) We seek $\|f\|_{1}=\int_{0}^{3}|f(t)| d t$. As indicated in Fig. 7-3, $f(t)$ is negative in $[0,3]$; hence,

$$
|f(t)|=-\left(t^{2}-4 t\right)=4 t-t^{2}
$$

Thus, $\|f\|_{1}=\int_{0}^{3}\left(4 t-t^{2}\right) d t=\left.\left(2 t^{2}-\frac{t^{3}}{3}\right)\right|_{0} ^{3}=18-9=9$

(d)

$\|f\|_{2}^{2}=\int_{0}^{3} f(t)^{2} d t=\int_{0}^{3}\left(t^{4}-8 t^{3}+16 t^{2}\right) d t=\left.\left(\frac{t^{5}}{5}-2 t^{4}+\frac{16 t^{3}}{3}\right)\right|_{0} ^{3}=\frac{153}{5}$.

Thus, $\|f\|_{2}=\sqrt{\frac{153}{5}}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-265}
\end{center}

Figure 7-8

7.56. Prove Theorem 7.24: Let $V$ be a normed vector space. Then the function $d(u, v)=\|u-v\|$ satisfies the following three axioms of a metric space:

$\left[\mathrm{M}_{1}\right] \quad d(u, v) \geq 0$; and $d(u, v)=0$ iff $u=v$.

$\left[\mathrm{M}_{2}\right] \quad d(u, v)=d(v, u)$.

$\left[\mathrm{M}_{3}\right] \quad d(u, v) \leq d(u, w)+d(w, v)$.

If $u \neq v$, then $u-v \neq 0$, and hence, $d(u, v)=\|u-v\|>0$. Also, $d(u, u)=\|u-u\|=\|0\|=0$. Thus, $\left[\mathrm{M}_{1}\right]$ is satisfied. We also have

$$
d(u, v)=\|u-v\|=\|-1(v-u)\|=|-1|\|v-u\|=\|v-u\|=d(v, u)
$$

and $\quad d(u, v)=\|u-v\|=\|(u-w)+(w-v)\| \leq\|u-w\|+\|w-v\|=d(u, w)+d(w, v)$

Thus, $\left[\mathrm{M}_{2}\right]$ and $\left[\mathrm{M}_{3}\right]$ are satisfied.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Inner Products}
7.57. Verify that the following is an inner product on $\mathbf{R}^{2}$, where $u=\left(x_{1}, x_{2}\right)$ and $v=\left(y_{1}, y_{2}\right)$ :

$$
f(u, v)=x_{1} y_{1}-2 x_{1} y_{2}-2 x_{2} y_{1}+5 x_{2} y_{2}
$$

7.58. Find the values of $k$ so that the following is an inner product on $\mathbf{R}^{2}$, where $u=\left(x_{1}, x_{2}\right)$ and $v=\left(y_{1}, y_{2}\right)$ :

$$
f(u, v)=x_{1} y_{1}-3 x_{1} y_{2}-3 x_{2} y_{1}+k x_{2} y_{2}
$$

7.59. Consider the vectors $u=(1,-3)$ and $v=(2,5)$ in $\mathbf{R}^{2}$. Find

(a) $\langle u, v\rangle$ with respect to the usual inner product in $\mathbf{R}^{2}$.

(b) $\langle u, v\rangle$ with respect to the inner product in $\mathbf{R}^{2}$ in Problem 7.57.

(c) $\|v\|$ using the usual inner product in $\mathbf{R}^{2}$.

(d) $\|v\|$ using the inner product in $\mathbf{R}^{2}$ in Problem 7.57.

7.60. Show that each of the following is not an inner product on $\mathbf{R}^{3}$, where $u=\left(x_{1}, x_{2}, x_{3}\right)$ and $v=\left(y_{1}, y_{2}, y_{3}\right)$ :\\
(a) $\langle u, v\rangle=x_{1} y_{1}+x_{2} y_{2}$,\\
(b) $\langle u, v\rangle=x_{1} y_{2} x_{3}+y_{1} x_{2} y_{3}$.

7.61. Let $V$ be the vector space of $m \times n$ matrices over $\mathbf{R}$. Show that $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)$ defines an inner product in $V$.

7.62. Suppose $|\langle u, v\rangle|=\|u\|\|v\|$. (That is, the Cauchy-Schwarz inequality reduces to an equality.) Show that $u$ and $v$ are linearly dependent.

7.63. Suppose $f(u, v)$ and $g(u, v)$ are inner products on a vector space $V$ over $\mathbf{R}$. Prove

(a) The $\operatorname{sum} f+g$ is an inner product on $V$, where $(f+g)(u, v)=f(u, v)+g(u, v)$.

(b) The scalar product $k f$, for $k>0$, is an inner product on $V$, where $(k f)(u, v)=k f(u, v)$.

\section*{Orthogonality, Orthogonal Complements, Orthogonal Sets}
7.64. Let $V$ be the vector space of polynomials over $\mathbf{R}$ of degree $\leq 2$ with inner product defined by $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$. Find a basis of the subspace $W$ orthogonal to $h(t)=2 t+1$.

7.65. Find a basis of the subspace $W$ of $\mathbf{R}^{4}$ orthogonal to $u_{1}=(1,-2,3,4)$ and $u_{2}=(3,-5,7,8)$.

7.66. Find a basis for the subspace $W$ of $\mathbf{R}^{5}$ orthogonal to the vectors $u_{1}=(1,1,3,4,1)$ and $u_{2}=(1,2,1,2,1)$.

7.67. Let $w=(1,-2,-1,3)$ be a vector in $\mathbf{R}^{4}$. Find

(a) an orthogonal basis for $w^{\perp}$, (b) an orthonormal basis for $w^{\perp}$.

7.68. Let $W$ be the subspace of $\mathbf{R}^{4}$ orthogonal to $u_{1}=(1,1,2,2)$ and $u_{2}=(0,1,2,-1)$. Find

(a) an orthogonal basis for $W$, (b) an orthonormal basis for $W$. (Compare with Problem 7.65.)

7.69. Let $S$ consist of the following vectors in $\mathbf{R}^{4}$ :

$$
u_{1}=(1,1,1,1), \quad u_{2}=(1,1,-1,-1), \quad u_{3}=(1,-1,1,-1), \quad u_{4}=(1,-1,-1,1)
$$

(a) Show that $S$ is orthogonal and a basis of $\mathbf{R}^{4}$.

(b) Write $v=(1,3,-5,6)$ as a linear combination of $u_{1}, u_{2}, u_{3}, u_{4}$.

(c) Find the coordinates of an arbitrary vector $v=(a, b, c, d)$ in $\mathbf{R}^{4}$ relative to the basis $S$.

(d) Normalize $S$ to obtain an orthonormal basis of $\mathbf{R}^{4}$.

7.70. Let $\mathbf{M}=\mathbf{M}_{2,2}$ with inner product $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)$. Show that the following is an orthonormal basis for $\mathbf{M}$ :

$$
\left\{\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]\right\}
$$

7.71. Let $\mathbf{M}=\mathbf{M}_{2,2}$ with inner product $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)$. Find an orthogonal basis for the orthogonal complement of (a) diagonal matrices, (b) symmetric matrices.

7.72. Suppose $\left\{u_{1}, u_{2}, \ldots, u_{r}\right\}$ is an orthogonal set of vectors. Show that $\left\{k_{1} u_{1}, k_{2} u_{2}, \ldots, k_{r} u_{r}\right\}$ is an orthogonal set for any scalars $k_{1}, k_{2}, \ldots, k_{r}$.

7.73. Let $U$ and $W$ be subspaces of a finite-dimensional inner product space $V$. Show that\\
(a) $(U+W)^{\perp}=U^{\perp} \cap W^{\perp}$,\\
(b) $(U \cap W)^{\perp}=U^{\perp}+W^{\perp}$.

\section*{Projections, Gram-Schmidt Algorithm, Applications}
7.74. Find the Fourier coefficient $c$ and projection $c w$ of $v$ along $w$, where

(a) $v=(2,3,-5)$ and $w=(1,-5,2)$ in $\mathbf{R}^{3}$.

(b) $v=(1,3,1,2)$ and $w=(1,-2,7,4)$ in $\mathbf{R}^{4}$.

(c) $v=t^{2}$ and $w=t+3$ in $\mathbf{P}(t)$, with inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$

(d) $\quad v=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right]$ and $w=\left[\begin{array}{ll}1 & 1 \\ 5 & 5\end{array}\right]$ in $\mathbf{M}=\mathbf{M}_{2,2}$, with inner product $\langle A, B\rangle=\operatorname{tr}\left(B^{T} A\right)$.

7.75. Let $U$ be the subspace of $\mathbf{R}^{4}$ spanned by

$$
v_{1}=(1,1,1,1), \quad v_{2}=(1,-1,2,2), \quad v_{3}=(1,2,-3,-4)
$$

(a) Apply the Gram-Schmidt algorithm to find an orthogonal and an orthonormal basis for $U$.

(b) Find the projection of $v=(1,2,-3,4)$ onto $U$.

7.76. Suppose $v=(1,2,3,4,6)$. Find the projection of $v$ onto $W$, or, in other words, find $w \in W$ that minimizes $\|v-w\|$, where $W$ is the subspace of $\mathbf{R}^{5}$ spanned by\\
(a) $u_{1}=(1,2,1,2,1)$ and $u_{2}=(1,-1,2,-1,1)$,\\
(b) $\quad v_{1}=(1,2,1,2,1)$ and $v_{2}=(1,0,1,5,-1)$.

7.77. Consider the subspace $W=\mathbf{P}_{2}(t)$ of $\mathbf{P}(t)$ with inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$. Find the projection of $f(t)=t^{3}$ onto $W$. (Hint: Use the orthogonal polynomials $1,2 t-1,6 t^{2}-6 t+1$ obtained in Problem 7.22.)

7.78. Consider $\mathbf{P}(t)$ with inner product $\langle f, g\rangle=\int_{-1}^{1} f(t) g(t) d t$ and the subspace $W=P_{3}(t)$.

(a) Find an orthogonal basis for $W$ by applying the Gram-Schmidt algorithm to $\left\{1, t, t^{2}, t^{3}\right\}$.

(b) Find the projection of $f(t)=t^{5}$ onto $W$.

\section*{Orthogonal Matrices}
7.79. Find the number and exhibit all $2 \times 2$ orthogonal matrices of the form $\left[\begin{array}{ll}\frac{1}{3} & x \\ y & z\end{array}\right]$.

7.80. Find a $3 \times 3$ orthogonal matrix $P$ whose first two rows are multiples of $u=(1,1,1)$ and $v=(1,-2,3)$, respectively.

7.81. Find a symmetric orthogonal matrix $P$ whose first row is $\left(\frac{1}{3}, \frac{2}{3}, \frac{2}{3}\right)$. (Compare with Problem 7.32.)

7.82. Real matrices $A$ and $B$ are said to be orthogonally equivalent if there exists an orthogonal matrix $P$ such that $B=P^{T} A P$. Show that this relation is an equivalence relation.

\section*{Positive Definite Matrices and Inner Products}
7.83. Find the matrix $A$ that represents the usual inner product on $\mathbf{R}^{2}$ relative to each of the following bases:\\
(a) $\left\{v_{1}=(1,4), \quad v_{2}=(2,-3)\right\}$,\\
(b) $\left\{w_{1}=(1,-3), w_{2}=(6,2)\right\}$.

7.84. Consider the following inner product on $\mathbf{R}^{2}$ :

$$
f(u, v)=x_{1} y_{1}-2 x_{1} y_{2}-2 x_{2} y_{1}+5 x_{2} y_{2}, \quad \text { where } \quad u=\left(x_{1}, x_{2}\right) \quad v=\left(y_{1}, y_{2}\right)
$$

Find the matrix $B$ that represents this inner product on $\mathbf{R}^{2}$ relative to each basis in Problem 7.83.

7.85. Find the matrix $C$ that represents the usual basis on $\mathbf{R}^{3}$ relative to the basis $S$ of $\mathbf{R}^{3}$ consisting of the vectors $u_{1}=(1,1,1), u_{2}=(1,2,1), u_{3}=(1,-1,3)$.

7.86. Let $V=\mathbf{P}_{2}(t)$ with inner product $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$.

(a) Find $\langle f, g\rangle$, where $f(t)=t+2$ and $g(t)=t^{2}-3 t+4$.

(b) Find the matrix $A$ of the inner product with respect to the basis $\left\{1, t, t^{2}\right\}$ of $V$.

(c) Verify Theorem 7.16 that $\langle f, g\rangle=[f]^{T} A[g]$ with respect to the basis $\left\{1, t, t^{2}\right\}$.

7.87. Determine which of the following matrices are positive definite:\\
(a) $\left[\begin{array}{ll}1 & 3 \\ 3 & 5\end{array}\right]$\\
(b) $\left[\begin{array}{ll}3 & 4 \\ 4 & 7\end{array}\right]$\\
(c) $\left[\begin{array}{ll}4 & 2 \\ 2 & 1\end{array}\right]$,\\
(d) $\left[\begin{array}{rr}6 & -7 \\ -7 & 9\end{array}\right]$.

7.88. Suppose $A$ and $B$ are positive definite matrices. Show that:\\
(a) $A+B$ is positive definite and\\
(b) $k A$ is positive definite for $k>0$.

7.89. Suppose $B$ is a real nonsingular matrix. Show that: (a) $B^{T} B$ is symmetric and (b) $B^{T} B$ is positive definite.

\section*{Complex Inner Product Spaces}
7.90. Verify that

$$
\left\langle a_{1} u_{1}+a_{2} u_{2} \quad b_{1} v_{1}+b_{2} v_{2}\right\rangle=a_{1} \bar{b}_{1}\left\langle u_{1}, v_{1}\right\rangle+a_{1} \bar{b}_{2}\left\langle u_{1}, v_{2}\right\rangle+a_{2} \bar{b}_{1}\left\langle u_{2}, v_{1}\right\rangle+a_{2} \bar{b}_{2}\left\langle u_{2}, v_{2}\right\rangle
$$

More generally, prove that $\left\langle\sum_{i=1}^{m} a_{i} u_{i}, \sum_{j=1}^{n} b_{j} v_{j}\right\rangle=\sum_{i, j} a_{i} \overline{b_{j}}\left\langle u_{i}, v_{i}\right\rangle$.

7.91. Consider $u=(1+i, 3,4-i)$ and $v=(3-4 i, 1+i, 2 i)$ in $\mathbf{C}^{3}$. Find\\
(a) $\langle u, v\rangle$,\\
(b) $\langle v, u\rangle$,\\
(c) $\|u\|$,\\
(d) $\|v\|$,\\
(e) $d(u, v)$.

7.92. Find the Fourier coefficient $c$ and the projection $c w$ of

(a) $u=(3+i, 5-2 i)$ along $w=(5+i, 1+i)$ in $\mathbf{C}^{2}$,

(b) $u=(1-i, 3 i, 1+i)$ along $w=(1,2-i, 3+2 i)$ in $\mathbf{C}^{3}$.

7.93. Let $u=\left(z_{1}, z_{2}\right)$ and $v=\left(w_{1}, w_{2}\right)$ belong to $\mathbf{C}^{2}$. Verify that the following is an inner product of $\mathbf{C}^{2}$ :

$$
f(u, v)=z_{1} \bar{w}_{1}+(1+i) z_{1} \bar{w}_{2}+(1-i) z_{2} \bar{w}_{1}+3 z_{2} \bar{w}_{2}
$$

7.94. Find an orthogonal basis and an orthonormal basis for the subspace $W$ of $\mathbf{C}^{3}$ spanned by $u_{1}=(1, i, 1)$ and $u_{2}=(1+i, 0,2)$.

7.95. Let $u=\left(z_{1}, z_{2}\right)$ and $v=\left(w_{1}, w_{2}\right)$ belong to $\mathbf{C}^{2}$. For what values of $a, b, c, d \in \mathbf{C}$ is the following an inner product on $\mathbf{C}^{2}$ ?

$$
f(u, v)=a z_{1} \bar{w}_{1}+b z_{1} \bar{w}_{2}+c z_{2} \bar{w}_{1}+d z_{2} \bar{w}_{2}
$$

7.96. Prove the following form for an inner product in a complex space $V$ :

$$
\langle u, v\rangle=\frac{1}{4}\|u+v\|^{2}-\frac{1}{4}\|u-v\|^{2}+\frac{1}{4}\|u+i v\|^{2}-\frac{1}{4}\|u-i v\|^{2}
$$

[Compare with Problem 7.7(b).]

7.97. Let $V$ be a real inner product space. Show that

(i) $\|u\|=\|v\|$ if and only if $\langle u+v, u-v\rangle=0$;

(ii) $\|u+v\|^{2}=\|u\|^{2}+\|v\|^{2}$ if and only if $\langle u, v\rangle=0$.

Show by counterexamples that the above statements are not true for, say, $\mathbf{C}^{2}$.

7.98. Find the matrix $P$ that represents the usual inner product on $\mathbf{C}^{3}$ relative to the basis $\{1,1+i, 1-2 i\}$.

7.99. A complex matrix $A$ is unitary if it is invertible and $A^{-1}=A^{H}$. Alternatively, $A$ is unitary if its rows (columns) form an orthonormal set of vectors (relative to the usual inner product of $\mathbf{C}^{n}$ ). Find a unitary matrix whose first row is:

(a) a multiple of $(1,1-i)$;

(b) a multiple of $\left(\frac{1}{2}, \frac{1}{2} i, \frac{1}{2}-\frac{1}{2} i\right)$.

\section*{Normed Vector Spaces}
7.100. Consider vectors $u=(1,-3,4,1,-2)$ and $v=(3,1,-2,-3,1)$ in $\mathbf{R}^{5}$. Find\\
(a) $\|u\|_{\infty}$ and $\|v\|_{\infty}$,\\
(b) $\|u\|_{1}$ and $\|v\|_{1}$,\\
(c) $\|u\|_{2}$ and $\|v\|_{2}$,\\
(d) $d_{\infty}(u, v), d_{1}(u, v), d_{2}(u, v)$

7.101. Repeat Problem 7.100 for $u=(1+i, 2-4 i)$ and $v=(1-i, 2+3 i)$ in $\mathbf{C}^{2}$.

7.102. Consider the functions $f(t)=5 t-t^{2}$ and $g(t)=3 t-t^{2}$ in $C[0,4]$. Find\\
(a) $d_{\infty}(f, g)$,\\
(b) $d_{1}(f, g)$,\\
(c) $d_{2}(f, g)$

7.103. Prove (a) $\|\cdot\|_{1}$ is a norm on $\mathbf{R}^{n}$. (b) $\|\cdot\|_{\infty}$ is a norm on $\mathbf{R}^{n}$.

7.104. Prove (a) $\|\cdot\|_{1}$ is a norm on $C[a, b]$. (b) $\|\cdot\|_{\infty}$ is a norm on $C[a, b]$.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $M=\left[R_{1} ; R_{2} ; \ldots\right]$ denotes a matrix $M$ with rows $R_{1}, R_{2}, \ldots$ Also, basis need not be unique.

7.58. $k>9$\\
7.59. (a) -13 ,\\
(b) -71 ,\\
(c) $\sqrt{29}$,\\
(d) $\sqrt{89}$

7.60. Let $u=(0,0,1)$; then $\langle u, u\rangle=0$ in both cases

7.64. $\left\{7 t^{2}-5 t, 12 t^{2}-5\right\}$

7.65. $\{(1,2,1,0),(4,4,0,1)\}$

7.66. $(-1,0,0,0,1),(-6,2,0,1,0),(-5,2,1,0,0)$

7.67. (a) $u_{1}=(0,0,3,1), u_{2}=(0,5,-1,3), u_{3}=(-14,-2,-1,3)$,

(b) $u_{1} / \sqrt{10}, u_{2} / \sqrt{35}, u_{3} / \sqrt{210}$

7.68. (a) $(0,2,-1,0),(-15,1,2,5)$,

(b) $(0,2,-1,0) / \sqrt{5},(-15,1,2,5) / \sqrt{255}$

7.69. (b) $v=\frac{1}{4}\left(5 u_{1}+3 u_{2}-13 u_{3}+9 u_{4}\right)$,

(c) $[v]=\frac{1}{4}[a+b+c+d, a+b-c-d, a-b+c-d, a-b-c+d]$

7.71. (a) $[0,1 ; 0,0], \quad[0,0 ; 1,0], \quad$ (b) $[0,-1 ; 1,0]$

7.74. (a) $c=-\frac{23}{30}$

(b) $c=\frac{1}{7}$,

(c) $c=\frac{15}{148}$,

(d) $c=\frac{19}{26}$

7.75. (a) $w_{1}=(1,1,1,1), w_{2}=(0,-2,1,1), w_{3}=(12,-4,-1,-7)$,

(b) $\operatorname{proj}(v, U)=\frac{1}{5}(-1,12,3,6)$

7.76. (a) $\operatorname{proj}(v, W)=\frac{1}{8}(23,25,30,25,23)$, (b) First find an orthogonal basis for $W$;

say, $w_{1}=(1,2,1,2,1)$ and $w_{2}=(0,2,0,-3,2)$. Then $\operatorname{proj}(v, W)=\frac{1}{17}(34,76,34,56,42)$

7.77. $\operatorname{proj}(f, W)=\frac{3}{2} t^{2}-\frac{3}{5} t+\frac{1}{20}$

7.78. (a) $\left\{1, t, 3 t^{2}-1,5 t^{3}-3 t\right\}, \quad \operatorname{proj}(f, W)=\frac{10}{9} t^{3}-\frac{5}{21} t$

7.79. Four: $[a, b ; \quad b,-a],[a, b ; \quad-b,-a],[a,-b ; \quad b, a],[a,-b ; \quad-b,-a]$, where $a=\frac{1}{3}$ and $b=\frac{1}{3} \sqrt{8}$

7.80. $P=[1 / a, 1 / a, 1 / a ; \quad 1 / b,-2 / b, 3 / b ; \quad 5 / c,-2 / c,-3 / c]$, where $a=\sqrt{3}, b=\sqrt{14}, c=\sqrt{38}$

7.81. $\frac{1}{3}[1,2,2 ; \quad 2,-2,1 ; \quad 2,1,-2]$

7.83. (a) $[17,-10 ;-10,13]$,

(b) $[10,0 ; \quad 0,40]$

7.84. (a) $[65,-68 ;-68,73]$,

(b) $[58,8 ; 8,8]$

7.85. $[3,4,3 ; \quad 4,6,2 ; \quad 3,2,11]$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-270}
\end{center}

7.87. (a) No,

(b) Yes,

(c) No,

(d) Yes

7.91. (a) $-4 i$,

(b) $4 i$,

(c) $\sqrt{28}$,

(d) $\sqrt{31}$,

(e) $\sqrt{59}$

7.92. (a) $c=\frac{1}{28}(19-5 i)$,

(b) $c=\frac{1}{19}(3+6 i)$

7.94. $\left\{v_{1}=(1, i, 1) / \sqrt{3}, \quad v_{2}=(2 i, 1-3 i, 3-i) / \sqrt{24}\right\}$

7.95. $a$ and $d$ real and positive, $c=\bar{b}$ and $a d-b c$ positive.

7.97. $u=(1,2), v=(i, 2 i)$

7.98. $P=[1, \quad 1-i, \quad 1+2 i ; \quad 1+i, 2, \quad-1+3 i ; \quad 1-2 i, \quad-1-3 i, 5]$

7.99. (a) $(1 / \sqrt{3})[1, \quad 1-i ; \quad 1+i,-1]$,

(b) $[a, a i, a-a i ; \quad b i, b, 0 ; a, a i,-a-a i]$, where $a=\frac{1}{2}$ and $b=1 / \sqrt{2}$.

7.100. (a) 4 and 3 ,

(b) 11 and 10 ,

(c) $\sqrt{31}$ and $\sqrt{24}$,

(d) $6,19,9$

7.101. (a) $\sqrt{20}$ and $\sqrt{13}, \quad$ (b) $\sqrt{2}+\sqrt{20}$ and $\sqrt{2}+\sqrt{13}, \quad$ (c) $\sqrt{22}$ and $\sqrt{15}, \quad$ (d) $7,9, \sqrt{53}$

7.102. (a) 8 ,

(b) 16 ,

(c) $16 / \sqrt{3}$

\section*{Determinants}
\subsection*{8.1 Introduction}
Each $n$-square matrix $A=\left[a_{i j}\right]$ is assigned a special scalar called the determinant of $A$, denoted by $\operatorname{det}(A)$ or $|A|$ or

$$
\left|\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
a_{n 1} & a_{n 2} & \ldots & a_{n n}
\end{array}\right|
$$

We emphasize that an $n \times n$ array of scalars enclosed by straight lines, called a determinant of order $n$, is not a matrix but denotes the determinant of the enclosed array of scalars (i.e., the enclosed matrix).

The determinant function was first discovered during the investigation of systems of linear equations. We shall see that the determinant is an indispensable tool in investigating and obtaining properties of square matrices.

The definition of the determinant and most of its properties also apply in the case where the entries of a matrix come from a commutative ring.

We begin with a special case of determinants of orders 1,2 , and 3 . Then we define a determinant of arbitrary order. This general definition is preceded by a discussion of permutations, which is necessary for our general definition of the determinant.

\subsection*{8.2 Determinants of Orders 1 and 2}
Determinants of orders 1 and 2 are defined as follows:

$$
\left|a_{11}\right|=a_{11} \quad \text { and } \quad\left|\begin{array}{ll}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}\right|=a_{11} a_{22}-a_{12} a_{21}
$$

Thus, the determinant of a $1 \times 1$ matrix $A=\left[a_{11}\right]$ is the scalar $a_{11}$; that is, $\operatorname{det}(A)=\left|a_{11}\right|=a_{11}$. The determinant of order two may easily be remembered by using the following diagram:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-271}
\end{center}

That, is, the determinant is equal to the product of the elements along the plus-labeled arrow minus the product of the elements along the minus-labeled arrow. (There is an analogous diagram for determinants of order 3, but not for higher-order determinants.)

\section*{EXAMPLE 8.1}
(a) Because the determinant of order 1 is the scalar itself, we have:

$$
\operatorname{det}(27)=27, \quad \operatorname{det}(-7)=-7, \quad \operatorname{det}(t-3)=t-3
$$

(b) $\left|\begin{array}{ll}5 & 3 \\ 4 & 6\end{array}\right|=5(6)-3(4)=30-12=18, \quad\left|\begin{array}{rr}3 & 2 \\ -5 & 7\end{array}\right|=21+10=31$

\section*{Application to Linear Equations}
Consider two linear equations in two unknowns, say

$$
\begin{aligned}
& a_{1} z+b_{1} y=c_{1} \\
& a_{2} x+b_{2} y=c_{2}
\end{aligned}
$$

Let $D=a_{1} b_{2}-a_{2} b_{1}$, the determinant of the matrix of coefficients. Then the system has a unique solution if and only if $D \neq 0$. In such a case, the unique solution may be expressed completely in terms of determinants as follows:

$$
x=\frac{N_{x}}{D}=\frac{b_{2} c_{1}-b_{1} c_{2}}{a_{1} b_{2}-a_{2} b_{1}}=\frac{\left|\begin{array}{ll}
c_{1} & b_{1} \\
c_{2} & b_{2}
\end{array}\right|}{\left|\begin{array}{ll}
a_{1} & b_{1} \\
a_{2} & b_{2}
\end{array}\right|}, \quad y=\frac{N_{y}}{D}=\frac{a_{1} c_{2}-a_{2} c_{1}}{a_{1} b_{2}-a_{2} b_{1}}=\frac{\left|\begin{array}{ll}
a_{1} & c_{1} \\
a_{2} & c_{2}
\end{array}\right|}{\left|\begin{array}{ll}
a_{1} & b_{1} \\
a_{2} & b_{2}
\end{array}\right|}
$$

Here $D$ appears in the denominator of both quotients. The numerators $N_{x}$ and $N_{y}$ of the quotients for $x$ and $y$, respectively, can be obtained by substituting the column of constant terms in place of the column of coefficients of the given unknown in the matrix of coefficients. On the other hand, if $D=0$, then the system may have no solution or more than one solution.

EXAMPLE 8.2 Solve by determinants the system $\left\{\begin{array}{l}4 x-3 y=15 \\ 2 x+5 y=1\end{array}\right.$

First find the determinant $D$ of the matrix of coefficients:

$$
D=\left|\begin{array}{rr}
4 & -3 \\
2 & 5
\end{array}\right|=4(5)-(-3)(2)=20+6=26
$$

Because $D \neq 0$, the system has a unique solution. To obtain the numerators $N_{x}$ and $N_{y}$, simply replace, in the matrix of coefficients, the coefficients of $x$ and $y$, respectively, by the constant terms, and then take their determinants:

$$
N_{x}=\left|\begin{array}{rr}
15 & -3 \\
1 & 5
\end{array}\right|=75+3=78 \quad N_{y}=\left|\begin{array}{rr}
4 & 15 \\
2 & 1
\end{array}\right|=4-30=-26
$$

Then the unique solution of the system is

$$
x=\frac{N_{x}}{D}=\frac{78}{26}=3, \quad y=\frac{N_{y}}{D}=\frac{-26}{26}=-1
$$

\subsection*{8.3 Determinants of Order 3}
Consider an arbitrary $3 \times 3$ matrix $A=\left[a_{i j}\right]$. The determinant of $A$ is defined as follows:

$$
\operatorname{det}(A)=\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right|=a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31}-a_{12} a_{21} a_{33}-a_{11} a_{23} a_{32}
$$

Observe that there are six products, each product consisting of three elements of the original matrix. Three of the products are plus-labeled (keep their sign) and three of the products are minus-labeled (change their sign).

The diagrams in Fig. 8-1 may help us to remember the above six products in $\operatorname{det}(A)$. That is, the determinant is equal to the sum of the products of the elements along the three plus-labeled arrows in

Fig. 8-1 plus the sum of the negatives of the products of the elements along the three minus-labeled arrows. We emphasize that there are no such diagrammatic devices with which to remember determinants of higher order.\\
\includegraphics[max width=\textwidth, center]{2024_04_03_de2bde501961f6000cc6g-273}

Figure 8-1

EXAMPLE 8.3 Let $A=\left[\begin{array}{rrr}2 & 1 & 1 \\ 0 & 5 & -2 \\ 1 & -3 & 4\end{array}\right]$ and $B=\left[\begin{array}{rrr}3 & 2 & 1 \\ -4 & 5 & -1 \\ 2 & -3 & 4\end{array}\right]$. Find $\operatorname{det}(A)$ and $\operatorname{det}(B)$.

Use the diagrams in Fig. 8-1:

$$
\begin{aligned}
\operatorname{det}(A) & =2(5)(4)+1(-2)(1)+1(-3)(0)-1(5)(1)-(-3)(-2)(2)-4(1)(0) \\
& =40-2+0-5-12-0=21 \\
\operatorname{det}(B) & =60-4+12-10-9+32=81
\end{aligned}
$$

\section*{Alternative Form for a Determinant of Order 3}
The determinant of the $3 \times 3$ matrix $A=\left[a_{i j}\right]$ may be rewritten as follows:

$$
\begin{aligned}
\operatorname{det}(A) & =a_{11}\left(a_{22} a_{23}-a_{23} a_{32}\right)-a_{12}\left(a_{21} a_{33}-a_{23} a_{31}\right)+a_{13}\left(a_{21} a_{32}-a_{22} a_{31}\right) \\
& =a_{11}\left|\begin{array}{ll}
a_{22} & a_{23} \\
a_{32} & a_{33}
\end{array}\right|-a_{12}\left|\begin{array}{ll}
a_{21} & a_{23} \\
a_{31} & a_{33}
\end{array}\right|+a_{13}\left|\begin{array}{ll}
a_{21} & a_{22} \\
a_{31} & a_{32}
\end{array}\right|
\end{aligned}
$$

which is a linear combination of three determinants of order 2 whose coefficients (with alternating signs) form the first row of the given matrix. This linear combination may be indicated in the form

$$
a_{11}\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right|-a_{12}\left|\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right|+a_{13}\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right|
$$

Note that each $2 \times 2$ matrix can be obtained by deleting, in the original matrix, the row and column containing its coefficient.

\section*{EXAMPLE 8.4}
$$
\begin{aligned}
\left|\begin{array}{rrr}
1 & 2 & 3 \\
4 & -2 & 3 \\
0 & 5 & -1
\end{array}\right| & =1\left|\begin{array}{rrr}
1 & 2 & 3 \\
4 & -2 & 3 \\
0 & 5 & -1
\end{array}\right|-2\left|\begin{array}{rrr}
1 & 2 & 3 \\
4 & -2 & 3 \\
0 & 5 & -1
\end{array}\right|+3\left|\begin{array}{rrr}
1 & 2 & 3 \\
4 & -2 & 3 \\
0 & 5 & -1
\end{array}\right| \\
& =1\left|\begin{array}{rr}
-2 & 3 \\
5 & -1
\end{array}\right|-2\left|\begin{array}{rr}
4 & 3 \\
0 & -1
\end{array}\right|+3\left|\begin{array}{rr}
4 & -2 \\
0 & 5
\end{array}\right| \\
& =1(2-15)-2(-4+0)+3(20+0)=-13+8+60=55
\end{aligned}
$$

\subsection*{8.4 Permutations}
A permutation $\sigma$ of the set $\{1,2, \ldots, n\}$ is a one-to-one mapping of the set onto itself or, equivalently, a rearrangement of the numbers $1,2, \ldots, n$. Such a permutation $\sigma$ is denoted by

$$
\sigma=\left(\begin{array}{cccc}
1 & 2 & \cdots & n \\
j_{1} & j_{2} & \cdots & j_{n}
\end{array}\right) \quad \text { or } \quad \sigma=j_{1} j_{2} \cdots j_{n}, \quad \text { where } j_{i}=\sigma(i)
$$

The set of all such permutations is denoted by $S_{n}$, and the number of such permutations is $n$ !. If $\sigma \in S_{n}$, then the inverse mapping $\sigma^{-1} \in S_{n}$; and if $\sigma, \tau \in S_{n}$, then the composition mapping $\sigma \circ \tau \in S_{n}$. Also, the identity mapping $\varepsilon=\sigma \circ \sigma^{-1} \in S_{n}$. (In fact, $\varepsilon=123 \ldots n$.)

\section*{EXAMPLE 8.5}
(a) There are $2 !=2 \cdot 1=2$ permutations in $\mathrm{S}_{2}$; they are 12 and 21 .

(b) There are $3 !=3 \cdot 2 \cdot 1=6$ permutations in $\mathrm{S}_{3}$; they are $123,132,213,231,312,321$.

\section*{Sign (Parity) of a Permutation}
Consider an arbitrary permutation $\sigma$ in $S_{n}$, say $\sigma=j_{1} j_{2} \cdots j_{n}$. We say $\sigma$ is an even or odd permutation according to whether there is an even or odd number of inversions in $\sigma$. By an inversion in $\sigma$ we mean a pair of integers $(i, k)$ such that $i>k$, but $i$ precedes $k$ in $\sigma$. We then define the sign or parity of $\sigma$, written $\operatorname{sgn} \sigma$, by

$$
\operatorname{sgn} \sigma=\left\{\begin{array}{rc}
1 & \text { if } \sigma \text { is even } \\
-1 & \text { if } \sigma \text { is odd }
\end{array}\right.
$$

\section*{EXAMPLE 8.6}
(a) Find the sign of $\sigma=35142$ in $S_{5}$.

For each element $k$, we count the number of elements $i$ such that $i>k$ and $i$ precedes $k$ in $\sigma$. There are 2 numbers ( 3 and 5 ) greater than and preceding 1 ,

3 numbers $(3,5$, and 4$)$ greater than and preceding 2 ,

1 number (5) greater than and preceding 4.

(There are no numbers greater than and preceding either 3 or 5.) Because there are, in all, six inversions, $\sigma$ is even and $\operatorname{sgn} \sigma=1$.

(b) The identity permutation $\varepsilon=123 \ldots n$ is even because there are no inversions in $\varepsilon$.

(c) In $S_{2}$, the permutation 12 is even and 21 is odd. In $S_{3}$, the permutations $123,231,312$ are even and the permutations 132, 213, 321 are odd.

(d) Let $\tau$ be the permutation that interchanges two numbers $i$ and $j$ and leaves the other numbers fixed. That is,

$$
\tau(i)=j, \quad \tau(j)=i, \quad \tau(k)=k, \quad \text { where } \quad k \neq i, j
$$

We call $\tau$ a transposition. If $i<j$, then there are $2(j-i)-1$ inversions in $\tau$, and hence, the transposition $\tau$ is odd.

Remark: One can show that, for any $n$, half of the permutations in $S_{n}$ are even and half of them are odd. For example, 3 of the 6 permutations in $S_{3}$ are even, and 3 are odd.

\subsection*{8.5. Determinants of Arbitrary Order}
Let $A=\left[a_{i j}\right]$ be a square matrix of order $n$ over a field $K$.

Consider a product of $n$ elements of $A$ such that one and only one element comes from each row and one and only one element comes from each column. Such a product can be written in the form

$$
a_{1 j_{1}} a_{2 j_{2}} \cdots a_{n j_{n}}
$$

that is, where the factors come from successive rows, and so the first subscripts are in the natural order $1,2, \ldots, n$. Now because the factors come from different columns, the sequence of second subscripts forms a permutation $\sigma=j_{1} j_{2} \cdots j_{n}$ in $S_{n}$. Conversely, each permutation in $S_{n}$ determines a product of the above form. Thus, the matrix $A$ contains $n$ ! such products.

DEFINITION: The determinant of $A=\left[a_{i j}\right]$, denoted by $\operatorname{det}(A)$ or $|A|$, is the sum of all the above $n$ ! products, where each such product is multiplied by sgn $\sigma$. That is,

or

$$
\begin{gathered}
|A|=\sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 j_{1}} a_{2 j_{2}} \cdots a_{n j_{n}} \\
|A|=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)}
\end{gathered}
$$

The determinant of the $n$-square matrix $A$ is said to be of order $n$.

The next example shows that the above definition agrees with the previous definition of determinants of orders 1,2 , and 3 .

\section*{EXAMPLE 8.7}
(a) Let $A=\left[a_{11}\right]$ be a $1 \times 1$ matrix. Because $S_{1}$ has only one permutation, which is even, $\operatorname{det}(A)=a_{11}$, the number itself.

(b) Let $A=\left[a_{i j}\right]$ be a $2 \times 2$ matrix. In $S_{2}$, the permutation 12 is even and the permutation 21 is odd. Hence,

$$
\operatorname{det}(A)=\left|\begin{array}{ll}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}\right|=a_{11} a_{22}-a_{12} a_{21}
$$

(c) Let $A=\left[a_{i j}\right]$ be a $3 \times 3$ matrix. In $S_{3}$, the permutations 123, 231, 312 are even, and the permutations 321,213 , 132 are odd. Hence,

$$
\operatorname{det}(A)=\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right|=a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31}-a_{12} a_{21} a_{33}-a_{11} a_{23} a_{32}
$$

Remark: As $n$ increases, the number of terms in the determinant becomes astronomical. Accordingly, we use indirect methods to evaluate determinants rather than the definition of the determinant. In fact, we prove a number of properties about determinants that will permit us to shorten the computation considerably. In particular, we show that a determinant of order $n$ is equal to a linear combination of determinants of order $n-1$, as in the case $n=3$ above.

\subsection*{8.6 Properties of Determinants}
We now list basic properties of the determinant.

THEOREM 8.1: The determinant of a matrix $A$ and its transpose $A^{T}$ are equal; that is, $|A|=\left|A^{T}\right|$.

By this theorem (proved in Problem 8.22), any theorem about the determinant of a matrix $A$ that concerns the rows of $A$ will have an analogous theorem concerning the columns of $A$.

The next theorem (proved in Problem 8.24) gives certain cases for which the determinant can be obtained immediately.

THEOREM 8.2: Let $A$ be a square matrix.

(i) If $A$ has a row (column) of zeros, then $|A|=0$.

(ii) If $A$ has two identical rows (columns), then $|A|=0$.\\
(iii) If $A$ is triangular (i.e., $A$ has zeros above or below the diagonal), then $|A|=$ product of diagonal elements. Thus, in particular, $|I|=1$, where $I$ is the identity matrix.

The next theorem (proved in Problems 8.23 and 8.25 ) shows how the determinant of a matrix is affected by the elementary row and column operations.

THEOREM 8.3: $\quad$ Suppose $B$ is obtained from $A$ by an elementary row (column) operation.

(i) If two rows (columns) of $A$ were interchanged, then $|B|=-|A|$.

(ii) If a row (column) of $A$ were multiplied by a scalar $k$, then $|B|=k|A|$.

(iii) If a multiple of a row (column) of $A$ were added to another row (column) of $A$, then $|B|=|A|$.

\section*{Major Properties of Determinants}
We now state two of the most important and useful theorems on determinants.

THEOREM 8.4: The determinant of a product of two matrices $A$ and $B$ is the product of their determinants; that is,

$$
\operatorname{det}(A B)=\operatorname{det}(A) \operatorname{det}(B)
$$

The above theorem says that the determinant is a multiplicative function.

THEOREM 8.5: Let $A$ be a square matrix. Then the following are equivalent:

(i) $A$ is invertible; that is, $A$ has an inverse $A^{-1}$.

(ii) $A X=0$ has only the zero solution.

(iii) The determinant of $A$ is not zero; that is, $\operatorname{det}(A) \neq 0$.

Remark: Depending on the author and the text, a nonsingular matrix $A$ is defined to be an invertible matrix $A$, or a matrix $A$ for which $|A| \neq 0$, or a matrix $A$ for which $A X=0$ has only the zero solution. The above theorem shows that all such definitions are equivalent.

We will prove Theorems 8.4 and 8.5 (in Problems 8.29 and 8.28, respectively) using the theory of elementary matrices and the following lemma (proved in Problem 8.26), which is a special case of Theorem 8.4.

LEMMA 8.6: Let $E$ be an elementary matrix. Then, for any matrix $A,|E A|=|E||A|$.

Recall that matrices $A$ and $B$ are similar if there exists a nonsingular matrix $P$ such that $B=P^{-1} A P$. Using the multiplicative property of the determinant (Theorem 8.4), one can easily prove (Problem 8.31) the following theorem.

THEOREM 8.7: $\quad$ Suppose $A$ and $B$ are similar matrices. Then $|A|=|B|$.

\subsection*{8.7 Minors and Cofactors}
Consider an $n$-square matrix $A=\left[a_{i j}\right]$. Let $M_{i j}$ denote the $(n-1)$-square submatrix of $A$ obtained by deleting its $i$ th row and $j$ th column. The determinant $\left|M_{i j}\right|$ is called the minor of the element $a_{i j}$ of $A$, and we define the cofactor of $a_{i j}$, denoted by $A_{i j}$, to be the "signed" minor:

$$
A_{i j}=(-1)^{i+j}\left|M_{i j}\right|
$$

Note that the "signs" $(-1)^{i+j}$ accompanying the minors form a chessboard pattern with + 's on the main diagonal:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-277}
\end{center}

We emphasize that $M_{i j}$ denotes a matrix, whereas $A_{i j}$ denotes a scalar.

Remark: The sign $(-1)^{i+j}$ of the cofactor $A_{i j}$ is frequently obtained using the checkerboard pattern. Specifically, beginning with + and alternating signs:

$$
+,-,+,-, \ldots
$$

count from the main diagonal to the appropriate square.

EXAMPLE 8.8 Let $A=\left[\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right]$. Find the following minors and cofactors: (a) $\left|M_{23}\right|$ and $A_{23}$,\\
(b) $\left|M_{31}\right|$ and $A_{31}$.

(a) $\left|M_{23}\right|=\left|\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right|=\left|\begin{array}{ll}1 & 2 \\ 7 & 8\end{array}\right|=8-14=-6$, and so $A_{23}=(-1)^{2+3}\left|M_{23}\right|=-(-6)=6$

(b) $\left|M_{31}\right|=\left|\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right|=\left|\begin{array}{ll}2 & 3 \\ 5 & 6\end{array}\right|=12-15=-3$, and so $A_{31}=(-1)^{1+3}\left|M_{31}\right|=+(-3)=-3$

\section*{Laplace Expansion}
The following theorem (proved in Problem 8.32) holds.

THEOREM 8.8: (Laplace) The determinant of a square matrix $A=\left[a_{i j}\right]$ is equal to the sum of the products obtained by multiplying the elements of any row (column) by their respective cofactors:

$$
\begin{aligned}
& |A|=a_{i 1} A_{i 1}+a_{i 2} A_{i 2}+\cdots+a_{i n} A_{i n}=\sum_{j=1}^{n} a_{i j} A_{i j} \\
& |A|=a_{1 j} A_{1 j}+a_{2 j} A_{2 j}+\cdots+a_{n j} A_{n j}=\sum_{i=1}^{n} a_{i j} A_{i j}
\end{aligned}
$$

The above formulas for $|A|$ are called the Laplace expansions of the determinant of $A$ by the $i$ th row and the $j$ th column. Together with the elementary row (column) operations, they offer a method of simplifying the computation of $|A|$, as described below.

\subsection*{8.8 Evaluation of Determinants}
The following algorithm reduces the evaluation of a determinant of order $n$ to the evaluation of a determinant of order $n-1$.

ALGORITHM 8.1: (Reduction of the order of a determinant) The input is a nonzero $n$-square matrix $A=\left[a_{i j}\right]$ with $n>1$.

Step 1. Choose an element $a_{i j}=1$ or, if lacking, $a_{i j} \neq 0$.

Step 2. Using $a_{i j}$ as a pivot, apply elementary row (column) operations to put 0 's in all the other positions in the column (row) containing $a_{i j}$.

Step 3. Expand the determinant by the column (row) containing $a_{i j}$.

The following remarks are in order.

Remark 1: Algorithm 8.1 is usually used for determinants of order 4 or more. With determinants of order less than 4 , one uses the specific formulas for the determinant.

Remark 2: Gaussian elimination or, equivalently, repeated use of Algorithm 8.1 together with row interchanges can be used to transform a matrix $A$ into an upper triangular matrix whose determinant is the product of its diagonal entries. However, one must keep track of the number of row interchanges, because each row interchange changes the sign of the determinant.

EXAMPLE 8.9 Use Algorithm 8.1 to find the determinant of $A=\left[\begin{array}{rrrr}5 & 4 & 2 & 1 \\ 2 & 3 & 1 & -2 \\ -5 & -7 & -3 & 9 \\ 1 & -2 & -1 & 4\end{array}\right]$.

Use $a_{23}=1$ as a pivot to put 0 's in the other positions of the third column; that is, apply the row operations "Replace $R_{1}$ by $-2 R_{2}+R_{1}$," 'Replace $R_{3}$ by $3 R_{2}+R_{3}$," and "Replace $R_{4}$ by $R_{2}+R_{4}$." By Theorem $8.3($ iii), the value of the determinant does not change under these operations. Thus,

$$
|A|=\left|\begin{array}{rrrr}
5 & 4 & 2 & 1 \\
2 & 3 & 1 & -2 \\
-5 & -7 & -3 & 9 \\
1 & -2 & -1 & 4
\end{array}\right|=\left|\begin{array}{rrrr}
1 & -2 & 0 & 5 \\
2 & 3 & 1 & -2 \\
1 & 2 & 0 & 3 \\
3 & 1 & 0 & 2
\end{array}\right|
$$

Now expand by the third column. Specifically, neglect all terms that contain 0 and use the fact that the sign of the minor $M_{23}$ is $(-1)^{2+3}=-1$. Thus,

$$
|A|=-\left|\begin{array}{rrrr}
1 & 2 & 0 & 5 \\
2 & 3 & 1 & -2 \\
1 & 2 & 0 & 3 \\
3 & 1 & 0 & 2
\end{array}\right|=-\left|\begin{array}{rrr}
1 & -2 & 5 \\
1 & 2 & 3 \\
3 & 1 & 2
\end{array}\right|=-(4-18+5-30-3+4)=-(-38)=38
$$

\subsection*{8.9 Classical Adjoint}
Let $A=\left[a_{i j}\right]$ be an $n \times n$ matrix over a field $K$ and let $A_{i j}$ denote the cofactor of $a_{i j}$. The classical adjoint of $A$, denoted by adj $A$, is the transpose of the matrix of cofactors of $A$. Namely,

$$
\operatorname{adj} A=\left[A_{i j}\right]^{T}
$$

We say "classical adjoint" instead of simply "adjoint" because the term "adjoint" is currently used for an entirely different concept.

EXAMPLE 8.10 Let $A=\left[\begin{array}{rrr}2 & 3 & -4 \\ 0 & -4 & 2 \\ 1 & -1 & 5\end{array}\right]$. The cofactors of the nine elements of $A$ follow:

$$
\begin{array}{lll}
A_{11}=+\left|\begin{array}{rr}
-4 & 2 \\
-1 & 5
\end{array}\right|=-18, & A_{12}=-\left|\begin{array}{ll}
0 & 2 \\
1 & 5
\end{array}\right|=2, & A_{13}=+\left|\begin{array}{rr}
0 & -4 \\
1 & -1
\end{array}\right|=4 \\
A_{21}=-\left|\begin{array}{rr}
3 & -4 \\
-1 & 5
\end{array}\right|=-11, & A_{22}=+\left|\begin{array}{rr}
2 & -4 \\
1 & 5
\end{array}\right|=14, & A_{23}=-\left|\begin{array}{rr}
2 & 3 \\
1 & -1
\end{array}\right|=5 \\
A_{31}=+\left|\begin{array}{rr}
3 & -4 \\
-4 & 2
\end{array}\right|=-10, & A_{32}=-\left|\begin{array}{rr}
2 & -4 \\
0 & 2
\end{array}\right|=-4, & A_{33}=+\left|\begin{array}{rr}
2 & 3 \\
0 & -4
\end{array}\right|=-8
\end{array}
$$

The transpose of the above matrix of cofactors yields the classical adjoint of $A$; that is,

$$
\operatorname{adj} A=\left[\begin{array}{rrr}
-18 & -11 & -10 \\
2 & 14 & -4 \\
4 & 5 & -8
\end{array}\right]
$$

The following theorem (proved in Problem 8.34) holds.

THEOREM 8.9: $\quad$ Let $A$ be any square matrix. Then

$$
A(\operatorname{adj} A)=(\operatorname{adj} A) A=|A| I
$$

where $I$ is the identity matrix. Thus, if $|A| \neq 0$,

$$
A^{-1}=\frac{1}{|A|}(\operatorname{adj} A)
$$

EXAMPLE 8.11 Let $A$ be the matrix in Example 8.10. We have

$$
\operatorname{det}(A)=-40+6+0-16+4+0=-46
$$

Thus, $A$ does have an inverse, and, by Theorem 8.9 ,

$$
A^{-1}=\frac{1}{|A|}(\operatorname{adj} A)=-\frac{1}{46}\left[\begin{array}{rrr}
-18 & -11 & -10 \\
2 & 14 & -4 \\
4 & 5 & -8
\end{array}\right]=\left[\begin{array}{rrr}
\frac{9}{23} & \frac{11}{46} & \frac{5}{23} \\
-\frac{1}{23} & -\frac{7}{23} & \frac{2}{23} \\
-\frac{2}{23} & -\frac{5}{46} & \frac{4}{23}
\end{array}\right]
$$

\subsection*{8.10 Applications to Linear Equations, Cramer's Rule}
Consider a system $A X=B$ of $n$ linear equations in $n$ unknowns. Here $A=\left[a_{i j}\right]$ is the (square) matrix of coefficients and $B=\left[b_{i}\right]$ is the column vector of constants. Let $A_{i}$ be the matrix obtained from $A$ by replacing the $i$ th column of $A$ by the column vector $B$. Furthermore, let

$$
D=\operatorname{det}(A), \quad N_{1}=\operatorname{det}\left(A_{1}\right), \quad N_{2}=\operatorname{det}\left(A_{2}\right), \quad \ldots, \quad N_{n}=\operatorname{det}\left(A_{n}\right)
$$

The fundamental relationship between determinants and the solution of the system $A X=B$ follows.

THEOREM 8.10: $\quad$ The (square) system $A X=B$ has a solution if and only if $D \neq 0$. In this case, the unique solution is given by

$$
x_{1}=\frac{N_{1}}{D}, \quad x_{2}=\frac{N_{2}}{D}, \quad \ldots, \quad x_{n}=\frac{N_{n}}{D}
$$

The above theorem (proved in Problem 8.10) is known as Cramer's rule for solving systems of linear equations. We emphasize that the theorem only refers to a system with the same number of equations as unknowns, and that it only gives the solution when $D \neq 0$. In fact, if $D=0$, the theorem does not tell us whether or not the system has a solution. However, in the case of a homogeneous system, we have the following useful result (to be proved in Problem 8.54).

THEOREM 8.11: A square homogeneous system $A X=0$ has a nonzero solution if and only if $D=|A|=0$.

EXAMPLE 8.12 Solve the system using determinants $\left\{\begin{aligned} x+y+z & =5 \\ x-2 y-3 z= & -1 \\ 2 x+y-z & =3\end{aligned}\right.$

First compute the determinant $D$ of the matrix of coefficients:

$$
D=\left|\begin{array}{rrr}
1 & 1 & 1 \\
1 & -2 & -3 \\
2 & 1 & -1
\end{array}\right|=2-6+1+4+3+1=5
$$

Because $D \neq 0$, the system has a unique solution. To compute $N_{x}, N_{y}, N_{z}$, we replace, respectively, the coefficients of $x, y, z$ in the matrix of coefficients by the constant terms. This yields

$$
N_{x}=\left|\begin{array}{rrr}
5 & 1 & 1 \\
-1 & -2 & -3 \\
3 & 1 & -1
\end{array}\right|=20, \quad N_{y}=\left|\begin{array}{rrr}
1 & 5 & 1 \\
1 & -1 & -3 \\
2 & 3 & -1
\end{array}\right|=-10, \quad N_{z}=\left|\begin{array}{rrr}
1 & 1 & 5 \\
1 & -2 & -1 \\
2 & 1 & 3
\end{array}\right|=15
$$

Thus, the unique solution of the system is $x=N_{x} / D=4, y=N_{y} / D=-2, \quad z=N_{z} / D=3$; that is, the vector $u=(4,-2,3)$.

\subsection*{8.11 Submatrices, Minors, Principal Minors}
Let $A=\left[a_{i j}\right]$ be a square matrix of order $n$. Consider any $r$ rows and $r$ columns of A. That is, consider any set $I=\left(i_{1}, i_{2}, \ldots, i_{r}\right)$ of $r$ row indices and any set $J=\left(j_{1}, j_{2}, \ldots, j_{r}\right)$ of $r$ column indices. Then $I$ and $J$ define an $r \times r$ submatrix of $A$, denoted by $A(I ; J)$, obtained by deleting the rows and columns of $A$ whose subscripts do not belong to $I$ or $J$, respectively. That is,

$$
A(I ; J)=\left[a_{s t}: s \in I, t \in J\right]
$$

The determinant $|A(I ; J)|$ is called a minor of $A$ of order $r$ and

$$
(-1)^{i_{1}+i_{2}+\cdots+i_{r}+j_{1}+j_{2}+\cdots+j_{r}}|A(I ; J)|
$$

is the corresponding signed minor. (Note that a minor of order $n-1$ is a minor in the sense of Section 8.7, and the corresponding signed minor is a cofactor.) Furthermore, if $I^{\prime}$ and $J^{\prime}$ denote, respectively, the remaining row and column indices, then

$$
\left|A\left(I^{\prime} ; J^{\prime}\right)\right|
$$

denotes the complementary minor, and its sign (Problem 8.74) is the same sign as the minor.

EXAMPLE 8.13 Let $A=\left[a_{i j}\right]$ be a 5 -square matrix, and let $I=\{1,2,4\}$ and $J=\{2,3,5\}$. Then $I^{\prime}=\{3,5\}$ and $J^{\prime}=\{1,4\}$, and the corresponding minor $|M|$ and complementary minor $\left|M^{\prime}\right|$ are as follows:

$$
|M|=|A(I ; J)|=\left|\begin{array}{ccc}
a_{12} & a_{13} & a_{15} \\
a_{22} & a_{23} & a_{25} \\
a_{42} & a_{43} & a_{45}
\end{array}\right| \quad \text { and } \quad\left|M^{\prime}\right|=\left|A\left(I^{\prime} ; J^{\prime}\right)\right|=\left|\begin{array}{cc}
a_{31} & a_{34} \\
a_{51} & a_{54}
\end{array}\right|
$$

Because $1+2+4+2+3+5=17$ is odd, $-|M|$ is the signed minor, and $-\left|M^{\prime}\right|$ is the signed complementary minor.

\section*{Principal Minors}
A minor is principal if the row and column indices are the same, or equivalently, if the diagonal elements of the minor come from the diagonal of the matrix. We note that the sign of a principal minor is always +1 , because the sum of the row and identical column subscripts must always be even.

EXAMPLE 8.14 Let $A=\left[\begin{array}{rrr}1 & 2 & -1 \\ 3 & 5 & 4 \\ -3 & 1 & -2\end{array}\right]$. Find the sums $C_{1}, C_{2}$, and $C_{3}$ of the principal minors of $A$ of orders 1,2 , and 3 , respectively.

(a) There are three principal minors of order 1. These are

$$
|1|=1, \quad|5|=5, \quad|-2|=-2, \quad \text { and so } \quad C_{1}=1+5-2=4
$$

Note that $C_{1}$ is simply the trace of $A$. Namely, $C_{1}=\operatorname{tr}(A)$.

(b) There are three ways to choose two of the three diagonal elements, and each choice gives a minor of order 2. These are

$$
\left|\begin{array}{ll}
1 & 2 \\
3 & 5
\end{array}\right|=-1, \quad\left|\begin{array}{rr}
1 & -1 \\
-3 & -2
\end{array}\right|=1, \quad\left|\begin{array}{rr}
5 & 4 \\
1 & -2
\end{array}\right|=-14
$$

(Note that these minors of order 2 are the cofactors $A_{33}, A_{22}$, and $A_{11}$ of $A$, respectively.) Thus,

$$
C_{2}=-1+1-14=-14
$$

(c) There is only one way to choose three of the three diagonal elements. Thus, the only minor of order 3 is the determinant of $A$ itself. Thus,

$$
C_{3}=|A|=-10-24-3-15-4+12=-44
$$

\subsection*{8.12 Block Matrices and Determinants}
The following theorem (proved in Problem 8.36) is the main result of this section.

THEOREM 8.12: $\quad$ Suppose $M$ is an upper (lower) triangular block matrix with the diagonal blocks $A_{1}, A_{2}, \ldots, A_{n}$. Then

$$
\operatorname{det}(M)=\operatorname{det}\left(A_{1}\right) \operatorname{det}\left(A_{2}\right) \ldots \operatorname{det}\left(A_{n}\right)
$$

EXAMPLE 8.15 Find $|M|$ where $M=\left[\begin{array}{rr:rrr}2 & 3 & 4 & 7 & 8 \\ -1 & 5 & 3 & 2 & 1 \\ \hdashline 0 & 0 & 2 & 1 & 5 \\ 0 & 0 & 3 & -1 & 4 \\ 0 & 0 & 5 & 2 & 6\end{array}\right]$

Note that $M$ is an upper triangular block matrix. Evaluate the determinant of each diagonal block:

$$
\left|\begin{array}{rr}
2 & 3 \\
-1 & 5
\end{array}\right|=10+3=13, \quad\left|\begin{array}{rrr}
2 & 1 & 5 \\
3 & -1 & 4 \\
5 & 2 & 6
\end{array}\right|=-12+20+30+25-16-18=29
$$

Then $|M|=13(29)=377$.

Remark: Suppose $M=\left[\begin{array}{ll}A & B \\ C & D\end{array}\right]$, where $A, B, C, D$ are square matrices. Then it is not generally true that $|M|=|A||D|-|B||C|$. (See Problem 8.68.)

\subsection*{8.13 Determinants and Volume}
Determinants are related to the notions of area and volume as follows. Let $u_{1}, u_{2}, \ldots, u_{n}$ be vectors in $\mathbf{R}^{n}$. Let $S$ be the (solid) parallelopiped determined by the vectors; that is,

$$
S=\left\{a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}: 0 \leq a_{i} \leq 1 \text { for } i=1, \ldots, n\right\}
$$

(When $n=2, S$ is a parallelogram.) Let $V(S)$ denote the volume of $S$ (or area of $S$ when $n=2$ ). Then $V(S)=$ absolute value of $\operatorname{det}(A)$\\
where $A$ is the matrix with rows $u_{1}, u_{2}, \ldots, u_{n}$. In general, $V(S)=0$ if and only if the vectors $u_{1}, \ldots, u_{n}$ do not form a coordinate system for $\mathbf{R}^{n}$ (i.e., if and only if the vectors are linearly dependent).

EXAMPLE 8.16 Let $u_{1}=(1,1,0), u_{2}=(1,1,1), u_{3}=(0,2,3)$. Find the volume $V(S)$ of the parallelopiped $S$ in $\mathbf{R}^{3}$ (Fig. 8-2) determined by the three vectors.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-282}
\end{center}

Figure 8-2

Evaluate the determinant of the matrix whose rows are $u_{1}, u_{2}, u_{3}$ :

$\left|\begin{array}{lll}1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 2 & 3\end{array}\right|=3+0+0-0-2-3=-2$

Hence, $V(S)=|-2|=2$.

\subsection*{8.14 Determinant of a Linear Operator}
Let $F$ be a linear operator on a vector space $V$ with finite dimension. Let $A$ be the matrix representation of $F$ relative to some basis $S$ of $V$. Then we define the determinant of $F$, written $\operatorname{det}(F)$, by

$$
\operatorname{det}(F)=|A|
$$

If $B$ were another matrix representation of $F$ relative to another basis $S^{\prime}$ of $V$, then $A$ and $B$ are similar matrices (Theorem 6.7) and $|B|=|A|$ (Theorem 8.7). In other words, the above $\operatorname{definition} \operatorname{det}(F)$ is independent of the particular basis $S$ of $V$. (We say that the definition is well defined.)

The next theorem (to be proved in Problem 8.62) follows from analogous theorems on matrices.

THEOREM 8.13: $\quad$ Let $F$ and $G$ be linear operators on a vector space $V$. Then

(i) $\operatorname{det}(F \circ G)=\operatorname{det}(F) \operatorname{det}(G)$.

(ii) $F$ is invertible if and only if $\operatorname{det}(F) \neq 0$.

EXAMPLE 8.17 Let $F$ be the following linear operator on $\mathbf{R}^{3}$ and let $A$ be the matrix that represents $F$ relative to the usual basis of $\mathbf{R}^{3}$ :

$$
F(x, y, z)=(2 x-4 y+z, x-2 y+3 z, 5 x+y-z) \quad \text { and } \quad A=\left[\begin{array}{rrr}
2 & -4 & 1 \\
1 & -2 & 3 \\
5 & 1 & -1
\end{array}\right]
$$

Then

$$
\operatorname{det}(F)=|A|=4-60+1+10-6-4=-55
$$

\subsection*{8.15 Multilinearity and Determinants}
Let $V$ be a vector space over a field $K$. Let $\mathscr{A}=V^{n}$; that is, $\mathscr{A}$ consists of all the $n$-tuples

$$
A=\left(A_{1}, A_{2}, \ldots, A_{n}\right)
$$

where the $A_{i}$ are vectors in $V$. The following definitions apply.

DEFINITION: A function $D: \mathscr{A} \rightarrow K$ is said to be multilinear if it is linear in each component:

(i) If $A_{i}=B+C$, then

$$
D(A)=D(\ldots, B+C, \ldots)=D(\ldots, B, \ldots,)+D(\ldots, C, \ldots)
$$

(ii) If $A_{i}=k B$, where $k \in K$, then

$$
D(A)=D(\ldots, k B, \ldots)=k D(\ldots, B, \ldots)
$$

We also say $n$-linear for multilinear if there are $n$ components.

DEFINITION: A function $D: \mathscr{A} \rightarrow K$ is said to be alternating if $D(A)=0$ whenever $A$ has two identical elements:

$$
D\left(A_{1}, A_{2}, \ldots, A_{n}\right)=0 \quad \text { whenever } \quad A_{i}=A_{j}, \quad i \neq j
$$

Now let $\mathbf{M}$ denote the set of all $n$-square matrices $A$ over a field $K$. We may view $A$ as an $n$-tuple consisting of its row vectors $A_{1}, A_{2}, \ldots, A_{n}$; that is, we may view $A$ in the form $A=\left(A_{1}, A_{2}, \ldots, A_{n}\right)$.

The following theorem (proved in Problem 8.37) characterizes the determinant function.

THEOREM 8.14: There exists a unique function $D: M \rightarrow K$ such that\\
(i) $D$ is multilinear,\\
(ii) $D$ is alternating,\\
(iii) $D(I)=1$.

This function $D$ is the determinant function; that is, $D(A)=|A|$, for any matrix $A \in M$.

\section*{SOLVED PROBLEMS}
\section*{Computation of Determinants}
8.1. Evaluate the determinant of each of the following matrices:\\
(a) $A=\left[\begin{array}{ll}6 & 5 \\ 2 & 3\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rr}2 & -3 \\ 4 & 7\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rr}4 & -5 \\ -1 & -2\end{array}\right]$\\
(d) $D=\left[\begin{array}{cc}t-5 & 6 \\ 3 & t+2\end{array}\right]$

Use the formula $\left|\begin{array}{ll}a & b \\ c & d\end{array}\right|=a d-b c$ :

(a) $|A|=6(3)-5(2)=18-10=8$

(b) $|B|=14+12=26$

(c) $|C|=-8-5=-13$

(d) $|D|=(t-5)(t+2)-18=t^{2}-3 t-10-18=t^{2}-10 t-28$

8.2. Evaluate the determinant of each of the following matrices:\\
(a) $A=\left[\begin{array}{lll}2 & 3 & 4 \\ 5 & 4 & 3 \\ 1 & 2 & 1\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}1 & -2 & 3 \\ 2 & 4 & -1 \\ 1 & 5 & -2\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rrr}1 & 3 & -5 \\ 3 & -1 & 2 \\ 1 & -2 & 1\end{array}\right]$

Use the diagram in Fig. 8-1 to obtain the six products:

(a) $|A|=2(4)(1)+3(3)(1)+4(2)(5)-1(4)(4)-2(3)(2)-1(3)(5)=8+9+40-16-12-15=14$

(b) $|B|=-8+2+30-12+5-8=9$

(c) $|C|=-1+6+30-5+4-9=25$

8.3. Compute the determinant of each of the following matrices:

(a) $A=\left[\begin{array}{rrr}2 & 3 & 4 \\ 5 & 6 & 7 \\ 8 & 9 & 1\end{array}\right]$, (b) $B=\left[\begin{array}{rrrr}4 & -6 & 8 & 9 \\ 0 & -2 & 7 & -3 \\ 0 & 0 & 5 & 6 \\ 0 & 0 & 0 & 3\end{array}\right]$, (c) $C=\left[\begin{array}{rrr}\frac{1}{2} & -1 & -\frac{1}{3} \\ \frac{3}{4} & \frac{1}{2} & -1 \\ 1 & -4 & 1\end{array}\right]$.

(a) One can simplify the entries by first subtracting twice the first row from the second row-that is, by applying the row operation " Replace $R_{2}$ by $-2_{1}+R_{2}$." Then

$$
|A|=\left|\begin{array}{lll}
2 & 3 & 4 \\
5 & 6 & 7 \\
8 & 9 & 1
\end{array}\right|=\left|\begin{array}{rrr}
2 & 3 & 4 \\
1 & 0 & -1 \\
8 & 9 & 1
\end{array}\right|=0-24+36-0+18-3=27
$$

(b) $B$ is triangular, so $|B|=$ product of the diagonal entries $=-120$.

(c) The arithmetic is simpler if fractions are first eliminated. Hence, multiply the first row $R_{1}$ by 6 and the second row $R_{2}$ by 4 . Then

$$
|24 C|=\left|\begin{array}{rrr}
3 & -6 & -2 \\
3 & 2 & -4 \\
1 & -4 & 1
\end{array}\right|=6+24+24+4-48+18=28, \quad \text { so }|C|=\frac{28}{24}=\frac{7}{6}
$$

8.4. Compute the determinant of each of the following matrices:

(a) $A=\left[\begin{array}{rrrr}2 & 5 & -3 & -2 \\ -2 & -3 & 2 & -5 \\ 1 & 3 & -2 & 2 \\ -1 & -6 & 4 & 3\end{array}\right]$, (b) $B=\left[\begin{array}{rrrrr}6 & 2 & 1 & 0 & 5 \\ 2 & 1 & 1 & -2 & 1 \\ 1 & 1 & 2 & -2 & 3 \\ 3 & 0 & 2 & 3 & -1 \\ -1 & -1 & -3 & 4 & 2\end{array}\right]$

(a) Use $a_{31}=1$ as a pivot to put 0 's in the first column, by applying the row operations "Replace $R_{1}$ by $-2 R_{3}+R_{1}$," 'Replace $R_{2}$ by $2 R_{3}+R_{2}$," and "Replace $R_{4}$ by $R_{3}+R_{4}$." Then

$$
\begin{aligned}
|A| & =\left|\begin{array}{rrrr}
2 & 5 & -3 & -2 \\
-2 & -3 & 2 & -5 \\
1 & 3 & -2 & 2 \\
-1 & -6 & 4 & 3
\end{array}\right|=\left|\begin{array}{rrrr}
0 & -1 & 1 & -6 \\
0 & 3 & -2 & -1 \\
1 & 3 & -2 & 2 \\
0 & -3 & 2 & 5
\end{array}\right|=\left|\begin{array}{rrr}
-1 & 1 & -6 \\
3 & -2 & -1 \\
-3 & 2 & 5
\end{array}\right| \\
& =10+3-36+36-2-15=-4
\end{aligned}
$$

(b) First reduce $|B|$ to a determinant of order 4, and then to a determinant of order 3, for which we can use Fig. 8-1. First use $c_{22}=1$ as a pivot to put 0 's in the second column, by applying the row operations "Replace $R_{1}$ by $-2 R_{2}+R_{1}$," "Replace $R_{3}$ by $-R_{2}+R_{3}$," and "Replace $R_{5}$ by $R_{2}+R_{5}$." Then

$$
\begin{aligned}
|B| & =\left|\begin{array}{rrrrr}
2 & 0 & -1 & 4 & 3 \\
2 & 1 & 1 & -2 & 1 \\
-1 & 0 & 1 & 0 & 2 \\
3 & 0 & 2 & 3 & -1 \\
1 & 0 & -2 & 2 & 3
\end{array}\right|=\left|\begin{array}{rrrr}
2 & -1 & 4 & 3 \\
-1 & 1 & 0 & 2 \\
3 & 2 & 3 & -1 \\
1 & -2 & 2 & 3
\end{array}\right|=\left|\begin{array}{rrrr}
1 & 1 & 4 & 5 \\
0 & 1 & 0 & 0 \\
5 & 2 & 3 & -5 \\
-1 & -2 & 2 & 7
\end{array}\right| \\
& =\left|\begin{array}{rrr}
1 & 4 & 5 \\
5 & 3 & -5 \\
-1 & 2 & 7
\end{array}\right|=21+20+50+15+10-140=-34
\end{aligned}
$$

\section*{Cofactors, Classical Adjoints, Minors, Principal Minors}
8.5. Let $A=\left[\begin{array}{rrrr}2 & 1 & -3 & 4 \\ 5 & -4 & 7 & -2 \\ 4 & 0 & 6 & -3 \\ 3 & -2 & 5 & 2\end{array}\right]$.

(a) Find $A_{23}$, the cofactor (signed minor) of 7 in $A$.

(b) Find the minor and the signed minor of the submatrix $M=A(2,4 ; 2,3)$.

(c) Find the principal minor determined by the first and third diagonal entries - that is, by $M=A(1,3 ; \quad 1,3)$.

(a) Take the determinant of the submatrix of $A$ obtained by deleting row 2 and column 3 (those which contain the 7 ), and multiply the determinant by $(-1)^{2+3}$ :

$$
A_{23}=-\left|\begin{array}{rrr}
2 & 1 & 4 \\
4 & 0 & -3 \\
3 & -2 & 2
\end{array}\right|=-(-61)=61
$$

The exponent $2+3$ comes from the subscripts of $A_{23}$-that is, from the fact that 7 appears in row 2 and column 3.

(b) The row subscripts are 2 and 4 and the column subscripts are 2 and 3. Hence, the minor is the determinant

$$
|M|=\left|\begin{array}{ll}
a_{22} & a_{23} \\
a_{42} & a_{43}
\end{array}\right|=\left|\begin{array}{ll}
-4 & 7 \\
-2 & 5
\end{array}\right|=-20+14=-6
$$

and the signed minor is $(-1)^{2+4+2+3}|M|=-|M|=-(-6)=6$.

(c) The principal minor is the determinant

$$
|M|=\left|\begin{array}{ll}
a_{11} & a_{13} \\
a_{31} & a_{33}
\end{array}\right|=\left|\begin{array}{rr}
2 & -3 \\
4 & 6
\end{array}\right|=12+12=24
$$

Note that now the diagonal entries of the submatrix are diagonal entries of the original matrix. Also, the sign of the principal minor is positive.

8.6. Let $B=\left[\begin{array}{lll}1 & 1 & 1 \\ 2 & 3 & 4 \\ 5 & 8 & 9\end{array}\right]$. Find: (a) $|B|, \quad$ (b) $\operatorname{adj} B$, (c) $B^{-1}$ using adj $B$.

(a) $|B|=27+20+16-15-32-18=-2$

(b) Take the transpose of the matrix of cofactors:

$$
\operatorname{adj} B=\left[\begin{array}{rrr}
\left|\begin{array}{ll}
3 & 4 \\
8 & 9
\end{array}\right| & -\left|\begin{array}{ll}
2 & 4 \\
5 & 9
\end{array}\right| & \left|\begin{array}{ll}
2 & 3 \\
5 & 8
\end{array}\right| \\
-\left|\begin{array}{ll}
1 & 1 \\
8 & 9
\end{array}\right| & \left|\begin{array}{ll}
1 & 1 \\
5 & 9
\end{array}\right| & -\left|\begin{array}{ll}
1 & 1 \\
5 & 8
\end{array}\right| \\
\left|\begin{array}{ll}
1 & 1 \\
3 & 4
\end{array}\right| & -\left|\begin{array}{ll}
1 & 1 \\
2 & 4
\end{array}\right| & \left|\begin{array}{ll}
1 & 1 \\
2 & 3
\end{array}\right|
\end{array}\right]^{T}=\left[\begin{array}{rrr}
-5 & 2 & 1 \\
-1 & 4 & -3 \\
1 & -2 & 1
\end{array}\right]^{T}=\left[\begin{array}{rrr}
-5 & -1 & 1 \\
2 & 4 & -2 \\
1 & -3 & 1
\end{array}\right]
$$

(c) Because $|B| \neq 0, B^{-1}=\frac{1}{|B|}(\operatorname{adj} B)=\frac{1}{-2}\left[\begin{array}{rrr}-5 & -1 & 1 \\ 2 & 4 & -2 \\ 1 & -3 & 1\end{array}\right]=\left[\begin{array}{rrr}\frac{5}{2} & \frac{1}{2} & -\frac{1}{2} \\ -1 & -2 & 1 \\ -\frac{1}{2} & \frac{3}{2} & -\frac{1}{2}\end{array}\right]$

8.7. Let $A=\left[\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6 \\ 0 & 7 & 8\end{array}\right]$, and let $S_{k}$ denote the sum of its principal minors of order $k$. Find $S_{k}$ for\\
(a) $k=1$,\\
(b) $k=2$,\\
(c) $k=3$.\\
(a) The principal minors of order 1 are the diagonal elements. Thus, $S_{1}$ is the trace of $A$; that is,

$$
S_{1}=\operatorname{tr}(A)=1+5+8=14
$$

(b) The principal minors of order 2 are the cofactors of the diagonal elements. Thus,

$$
S_{2}=A_{11}+A_{22}+A_{33}=\left|\begin{array}{ll}
5 & 6 \\
7 & 8
\end{array}\right|+\left|\begin{array}{ll}
1 & 3 \\
0 & 8
\end{array}\right|+\left|\begin{array}{ll}
1 & 2 \\
4 & 5
\end{array}\right|=-2+8-3=3
$$

(c) There is only one principal minor of order 3, the determinant of $A$. Then

$$
S_{3}=|A|=40+0+84-0-42-64=18
$$

8.8. Let $A=\left[\begin{array}{rrrr}1 & 3 & 0 & -1 \\ -4 & 2 & 5 & 1 \\ 1 & 0 & 3 & -2 \\ 3 & -2 & 1 & 4\end{array}\right]$. Find the number $N_{k}$ and sum $S_{k}$ of principal minors of order:\\
(a) $k=1$,\\
(b) $k=2$,\\
(c) $k=3$,\\
(d) $k=4$.

Each (nonempty) subset of the diagonal (or equivalently, each nonempty subset of $\{1,2,3,4\}$ ) determines a principal minor of $A$, and $N_{k}=\left(\begin{array}{l}n \\ k\end{array}\right)=\frac{n !}{k !(n-k) !}$ of them are of order $k$.

Thus, $N_{1}=\left(\begin{array}{l}4 \\ 1\end{array}\right)=4, \quad N_{2}=\left(\begin{array}{l}4 \\ 2\end{array}\right)=6, \quad N_{3}=\left(\begin{array}{l}4 \\ 3\end{array}\right)=4, \quad N_{4}=\left(\begin{array}{l}4 \\ 4\end{array}\right)=1$

(a) $S_{1}=|1|+|2|+|3|+|4|=1+2+3+4=10$

(b) $S_{2}=\left|\begin{array}{rr}1 & 3 \\ -4 & 2\end{array}\right|+\left|\begin{array}{ll}1 & 0 \\ 1 & 3\end{array}\right|+\left|\begin{array}{rr}1 & -1 \\ 3 & 4\end{array}\right|+\left|\begin{array}{ll}2 & 5 \\ 0 & 3\end{array}\right|+\left|\begin{array}{rr}2 & 1 \\ -2 & 4\end{array}\right|+\left|\begin{array}{rr}3 & -2 \\ 1 & 4\end{array}\right|$

$$
=14+3+7+6+10+14=54
$$

(c) $S_{3}=\left|\begin{array}{rrr}1 & 3 & 0 \\ -4 & 2 & 5 \\ 1 & 0 & 3\end{array}\right|+\left|\begin{array}{rrr}1 & 3 & -1 \\ -4 & 2 & 1 \\ 3 & -2 & 4\end{array}\right|+\left|\begin{array}{rrr}1 & 0 & -1 \\ 1 & 3 & -2 \\ 3 & 1 & 4\end{array}\right|+\left|\begin{array}{rrr}2 & 5 & 1 \\ 0 & 3 & -2 \\ -2 & 1 & 4\end{array}\right|$

$$
=57+65+22+54=198
$$

(d) $S_{4}=\operatorname{det}(A)=378$

\section*{Determinants and Systems of Linear Equations}
8.9. Use determinants to solve the system $\left\{\begin{array}{l}3 y+2 x=z+1 \\ 3 x+2 z=8-5 y \\ 3 z-1=x-2 y\end{array}\right.$.

First arrange the equation in standard form, then compute the determinant $D$ of the matrix of coefficients:

$$
\begin{aligned}
2 x+3 y-z & =1 \\
3 x+5 y+2 z & =8 \\
x-2 y-3 z & =-1
\end{aligned} \quad \text { and } \quad D=\left|\begin{array}{rrr}
2 & 3 & -1 \\
3 & 5 & 2 \\
1 & -2 & -3
\end{array}\right|=-30+6+6+5+8+27=22
$$

Because $D \neq 0$, the system has a unique solution. To compute $N_{x}, N_{y}, N_{z}$, we replace, respectively, the coefficients of $x, y, z$ in the matrix of coefficients by the constant terms. Then

$$
N_{x}=\left|\begin{array}{rrr}
1 & 3 & -1 \\
8 & 5 & 2 \\
-1 & -2 & -1
\end{array}\right|=66, \quad N_{y}=\left|\begin{array}{rrr}
2 & 1 & -1 \\
3 & 8 & 2 \\
1 & -1 & -3
\end{array}\right|=-22, \quad N_{z}=\left|\begin{array}{rrr}
2 & 3 & 1 \\
3 & 5 & 8 \\
1 & -2 & -1
\end{array}\right|=44
$$

Thus,

$$
x=\frac{N_{x}}{D}=\frac{66}{22}=3, \quad y=\frac{N_{y}}{D}=\frac{-22}{22}=-1, \quad z=\frac{N_{z}}{D}=\frac{44}{22}=2
$$

8.10. Consider the system $\left\{\begin{array}{l}k x+y+z=1 \\ x+k y+z=1 \\ x+y+k z=1\end{array}\right.$

Use determinants to find those values of $k$ for which the system has

(a) a unique solution, (b) more than one solution, (c) no solution.

(a) The system has a unique solution when $D \neq 0$, where $D$ is the determinant of the matrix of coefficients. Compute

$$
D=\left|\begin{array}{lll}
k & 1 & 1 \\
1 & k & 1 \\
1 & 1 & k
\end{array}\right|=k^{3}+1+1-k-k-k=k^{3}-3 k+2=(k-1)^{2}(k+2)
$$

Thus, the system has a unique solution when

$$
(k-1)^{2}(k+2) \neq 0, \quad \text { when } k \neq 1 \text { and } k \neq 2
$$

(b and c) Gaussian elimination shows that the system has more than one solution when $k=1$, and the system has no solution when $k=-2$.

\section*{Miscellaneous Problems}
8.11. Find the volume $V(S)$ of the parallelepiped $S$ in $\mathbf{R}^{3}$ determined by the vectors:

(a) $u_{1}=(1,1,1), u_{2}=(1,3,-4), u_{3}=(1,2,-5)$.

(b) $u_{1}=(1,2,4), u_{2}=(2,1,-3), u_{3}=(5,7,9)$.

$V(S)$ is the absolute value of the determinant of the matrix $M$ whose rows are the given vectors. Thus,

(a) $|M|=\left|\begin{array}{rrr}1 & 1 & 1 \\ 1 & 3 & -4 \\ 1 & 2 & -5\end{array}\right|=-15-4+2-3+8+5=-7$. Hence, $V(S)=|-7|=7$.

(b) $|M|=\left|\begin{array}{rrr}1 & 2 & 4 \\ 2 & 1 & -3 \\ 5 & 7 & 9\end{array}\right|=9-30+56-20+21-36=0$. Thus, $V(S)=0$, or, in other words, $u_{1}, u_{2}, u_{3}$ lie in a plane and are linearly dependent.

8.12. Find $\operatorname{det}(M)$ where $M=\left[\begin{array}{lllll}3 & 4 & 0 & 0 & 0 \\ 2 & 5 & 0 & 0 & 0 \\ 0 & 9 & 2 & 0 & 0 \\ 0 & 5 & 0 & 6 & 7 \\ 0 & 0 & 4 & 3 & 4\end{array}\right]=\left[\begin{array}{cc:c:ccc}3 & 4 & 0 & 0 & 0 \\ 2 & 5 & 0 & 0 & 0 \\ \hdashline 0 & 9 & 2 & 0 & 0 \\ \hdashline 0 & 5 & 0 & 6 & 7 \\ 0 & 0 & 4 & 3 & 4\end{array}\right]$

$M$ is a (lower) triangular block matrix; hence, evaluate the determinant of each diagonal block:

$$
\left|\begin{array}{ll}
3 & 4 \\
2 & 5
\end{array}\right|=15-8=7, \quad|2|=2, \quad\left|\begin{array}{ll}
6 & 7 \\
3 & 4
\end{array}\right|=24-21=3
$$

Thus, $|M|=7(2)(3)=42$.

8.13. Find the determinant of $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by

$$
F(x, y, z)=(x+3 y-4 z, 2 y+7 z, x+5 y-3 z)
$$

The determinant of a linear operator $F$ is equal to the determinant of any matrix that represents $F$. Thus first find the matrix $A$ representing $F$ in the usual basis (whose rows, respectively, consist of the coefficients of $x, y, z$ ). Then

$$
A=\left[\begin{array}{rrr}
1 & 3 & -4 \\
0 & 2 & 7 \\
1 & 5 & -3
\end{array}\right], \quad \text { and so } \quad \operatorname{det}(F)=|A|=-6+21+0+8-35-0=-8
$$

8.14. Write out $g=g\left(x_{1}, x_{2}, x_{3}, x_{4}\right)$ explicitly where

$$
g\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{i<j}\left(x_{i}-x_{j}\right)
$$

The symbol $\prod$ is used for a product of terms in the same way that the symbol $\sum$ is used for a sum of terms. That is, $\prod_{i<j}\left(x_{i}-x_{j}\right)$ means the product of all terms $\left(x_{i}-x_{j}\right)$ for which $i<j$. Hence,

$$
g=g\left(x_{1}, \ldots, x_{4}\right)=\left(x_{1}-x_{2}\right)\left(x_{1}-x_{3}\right)\left(x_{1}-x_{4}\right)\left(x_{2}-x_{3}\right)\left(x_{2}-x_{4}\right)\left(x_{3}-x_{4}\right)
$$

8.15. Let $D$ be a 2-linear, alternating function. Show that $D(A, B)=-D(B, A)$.

Because $D$ is alternating, $D(A, A)=0, D(B, B)=0$. Hence,

$$
D(A+B, A+B)=D(A, A)+D(A, B)+D(B, A)+D(B, B)=D(A, B)+D(B, A)
$$

However, $D(A+B, A+B)=0$. Hence, $D(A, B)=-D(B, A)$, as required.

\section*{Permutations}
8.16. Determine the parity (sign) of the permutation $\sigma=364152$.

Count the number of inversions. That is, for each element $k$, count the number of elements $i$ in $\sigma$ such that $i>k$ and $i$ precedes $k$ in $\sigma$. Namely,

$$
\begin{array}{llll}
k=1: & 3 \text { numbers }(3,6,4) & k=4: & 1 \text { number }(6) \\
k=2: & 4 \text { numbers }(3,6,4,5) & k=5: & 1 \text { number }(6) \\
k=3: & 0 \text { numbers } & k=6: & 0 \text { numbers }
\end{array}
$$

Because $3+4+0+1+1+0=9$ is odd, $\sigma$ is an odd permutation, and $\operatorname{sgn} \sigma=-1$.

8.17. Let $\sigma=24513$ and $\tau=41352$ be permutations in $S_{5}$. Find (a) $\tau \circ \sigma$, (b) $\sigma^{-1}$.

Recall that $\sigma=24513$ and $\tau=41352$ are short ways of writing

$$
\begin{aligned}
& \sigma=\left(\begin{array}{lllll}
1 & 2 & 3 & 4 & 5 \\
2 & 4 & 5 & 1 & 3
\end{array}\right) \quad \text { or } \quad \sigma(1)=2, \quad \sigma(2)=4, \quad \sigma(3)=5, \quad \sigma(4)=1, \quad \sigma(5)=3 \\
& \tau=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5 \\
4 & 1 & 3 & 5 & 2 c
\end{array}\right) \quad \text { or } \quad \tau(1)=4, \quad \tau(2)=1, \quad \tau(3)=3, \quad \tau(4)=5, \quad \tau(5)=2
\end{aligned}
$$

(a) The effects of $\sigma$ and then $\tau$ on $1,2,3,4,5$ are as follows:

$$
1 \rightarrow 2 \rightarrow 1, \quad 2 \rightarrow 4 \rightarrow 5, \quad 3 \rightarrow 5 \rightarrow 2, \quad 4 \rightarrow 1 \rightarrow 4, \quad 5 \rightarrow 3 \rightarrow 3
$$

[That is, for example, $(\tau \circ \sigma)(1)=\tau(\sigma(1))=\tau(2)=1$.] Thus, $\tau \circ \sigma=15243$.

(b) By definition, $\sigma^{-1}(j)=k$ if and only if $\sigma(k)=j$. Hence,

$$
\sigma^{-1}=\left(\begin{array}{lllll}
2 & 4 & 5 & 1 & 3 \\
1 & 2 & 3 & 4 & 5
\end{array}\right)=\left(\begin{array}{lllll}
1 & 2 & 3 & 4 & 5 \\
4 & 1 & 5 & 2 & 3
\end{array}\right) \quad \text { or } \quad \sigma^{-1}=41523
$$

8.18. Let $\sigma=j_{1} j_{2} \ldots j_{n}$ be any permutation in $S_{n}$. Show that, for each inversion $(i, k)$ where $i>k$ but $i$ precedes $k$ in $\sigma$, there is a pair $\left(i^{*}, j^{*}\right)$ such that


\begin{equation*}
i^{*}<k^{*} \quad \text { and } \quad \sigma\left(i^{*}\right)>\sigma\left(j^{*}\right) \tag{1}
\end{equation*}


and vice versa. Thus, $\sigma$ is even or odd according to whether there is an even or an odd number of pairs satisfying (1).

Choose $i^{*}$ and $k^{*}$ so that $\sigma\left(i^{*}\right)=i$ and $\sigma\left(k^{*}\right)=k$. Then $i>k$ if and only if $\sigma\left(i^{*}\right)>\sigma\left(k^{*}\right)$, and $i$ precedes $k$ in $\sigma$ if and only if $i^{*}<k^{*}$.

8.19. Consider the polynomials $g=g\left(x_{1}, \ldots, x_{n}\right)$ and $\sigma(g)$, defined by

$$
g=g\left(x_{1}, \ldots, x_{n}\right)=\prod_{i<j}\left(x_{i}-x_{j}\right) \quad \text { and } \quad \sigma(g)=\prod_{i<j}\left(x_{\sigma(i)}-x_{\sigma(j)}\right)
$$

(See Problem 8.14.) Show that $\sigma(g)=g$ when $\sigma$ is an even permutation, and $\sigma(g)=-g$ when $\sigma$ is an odd permutation. That is, $\sigma(g)=(\operatorname{sgn} \sigma) g$.

Because $\sigma$ is one-to-one and onto,

$$
\sigma(g)=\prod_{i<j}\left(x_{\sigma(i)}-x_{\sigma(j)}\right)=\prod_{i<j \text { or } i>j}\left(x_{i}-x_{j}\right)
$$

Thus, $\sigma(g)$ or $\sigma(g)=-g$ according to whether there is an even or an odd number of terms of the form $x_{i}-x_{j}$, where $i>j$. Note that for each pair $(i, j)$ for which

$$
i<j \quad \text { and } \quad \sigma(i)>\sigma(j)
$$

there is a term $\left(x_{\sigma(i)}-x_{\sigma(j)}\right)$ in $\sigma(g)$ for which $\sigma(i)>\sigma(j)$. Because $\sigma$ is even if and only if there is an even number of pairs satisfying (1), we have $\sigma(g)=g$ if and only if $\sigma$ is even. Hence, $\sigma(g)=-g$ if and only if $\sigma$ is odd.

8.20. Let $\sigma, \tau \in S_{n}$. Show that $\operatorname{sgn}(\tau \circ \sigma)=(\operatorname{sgn} \tau)(\operatorname{sgn} \sigma)$. Thus, the product of two even or two odd permutations is even, and the product of an odd and an even permutation is odd.

Using Problem 8.19, we have

$$
\operatorname{sgn}(\tau \circ \sigma) g=(\tau \circ \sigma)(g)=\tau(\sigma(g))=\tau((\operatorname{sgn} \sigma) g)=(\operatorname{sgn} \tau)(\operatorname{sgn} \sigma) g
$$

Accordingly, $\operatorname{sgn}(\tau \circ \sigma)=(\operatorname{sgn} \tau)(\operatorname{sgn} \sigma)$.

8.21. Consider the permutation $\sigma=j_{1} j_{2} \cdots j_{n}$. Show that $\operatorname{sgn} \sigma^{-1}=\operatorname{sgn} \sigma$ and, for scalars $a_{i j}$, show that

$$
a_{j_{1} 1} a_{j_{2} 2} \cdots a_{j_{n} n}=a_{1 k_{1}} a_{2 k_{2}} \cdots a_{n k_{n}}
$$

where $\sigma^{-1}=k_{1} k_{2} \cdots k_{n}$.

We have $\sigma^{-1} \circ \sigma=\varepsilon$, the identity permutation. Because $\varepsilon$ is even, $\sigma^{-1}$ and $\sigma$ are both even or both odd. Hence $\operatorname{sgn} \sigma^{-1}=\operatorname{sgn} \sigma$.

Because $\sigma=j_{1} j_{2} \cdots j_{n}$ is a permutation, $a_{j_{1} 1} a_{j_{2} 2} \cdots a_{j_{n} n}=a_{1 k_{1}} a_{2 k_{2}} \cdots a_{n k_{n}}$. Then $k_{1}, k_{2}, \ldots, k_{n}$ have the property that

$$
\sigma\left(k_{1}\right)=1, \quad \sigma\left(k_{2}\right)=2, \quad \ldots, \quad \sigma\left(k_{n}\right)=n
$$

Let $\tau=k_{1} k_{2} \cdots k_{n}$. Then, for $i=1, \ldots, n$,

$$
(\sigma \circ \tau)(i)=\sigma(\tau(i))=\sigma\left(k_{i}\right)=i
$$

Thus, $\sigma \circ \tau=\varepsilon$, the identity permutation. Hence, $\tau=\sigma^{-1}$.

\section*{Proofs of Theorems}
8.22. Prove Theorem 8.1: $\left|A^{T}\right|=|A|$.

If $A=\left[a_{i j}\right]$, then $A^{T}=\left[b_{i j}\right]$, with $b_{i j}=a_{j i}$. Hence,

$$
\left|A^{T}\right|=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)}=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) a_{\sigma(1), 1} a_{\sigma(2), 2} \cdots a_{\sigma(n), n}
$$

Let $\tau=\sigma^{-1}$. By Problem $8.21 \operatorname{sgn} \tau=\operatorname{sgn} \sigma$, and $a_{\sigma(1), 1} a_{\sigma(2), 2} \cdots a_{\sigma(n), n}=a_{1 \tau(1)} a_{2 \tau(2)} \cdots a_{n \tau(n)}$. Hence,

$$
\left|A^{T}\right|=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \tau) a_{1 t(1)} a_{2 \tau(2)} \cdots a_{n \tau(n)}
$$

However, as $\sigma$ runs through all the elements of $S_{n}, \tau=\sigma^{-1}$ also runs through all the elements of $S_{n}$. Thus, $\left|A^{T}\right|=|A|$.

8.23. Prove Theorem 8.3(i): If two rows (columns) of $A$ are interchanged, then $|B|=-|A|$.

We prove the theorem for the case that two columns are interchanged. Let $\tau$ be the transposition that interchanges the two numbers corresponding to the two columns of $A$ that are interchanged. If $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$, then $b_{i j}=a_{i \tau(j)}$. Hence, for any permutation $\sigma$,

$$
b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)}=a_{1(\tau \circ \sigma)(1)} a_{2(\tau \circ \sigma)(2)} \cdots a_{n(\tau \circ \sigma)(n)}
$$

Thus,

$$
|B|=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)}=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) a_{1(\tau \circ \sigma)(1)} a_{2(\tau \circ \sigma)(2)} \cdots a_{n(\tau \circ \sigma)(n)}
$$

Because the transposition $\tau$ is an odd permutation, $\operatorname{sgn}(\tau \circ \sigma)=(\operatorname{sgn} \tau)(\operatorname{sgn} \sigma)=-\operatorname{sgn} \sigma$. Accordingly, $\operatorname{sgn} \sigma=-\operatorname{sgn}(\tau \circ \sigma)$, and so

$$
|B|=-\sum_{\sigma \in S_{n}}[\operatorname{sgn}(\tau \circ \sigma)] a_{1(\tau \circ \sigma)(1)} a_{2(\tau \circ \sigma)(2)} \cdots a_{n(\tau \circ \sigma)(n)}
$$

But as $\sigma$ runs through all the elements of $S_{n}, \tau \circ \sigma$ also runs through all the elements of $S_{n}$. Hence, $|B|=-|A|$.

\subsection*{8.24. Prove Theorem 8.2 .}
(i) If $A$ has a row (column) of zeros, then $|A|=0$.

(ii) If $A$ has two identical rows (columns), then $|A|=0$.

(iii) If $A$ is triangular, then $|A|=$ product of diagonal elements. Thus, $|I|=1$.

(i) Each term in $|A|$ contains a factor from every row, and so from the row of zeros. Thus, each term of $|A|$ is zero, and so $|A|=0$.

(ii) Suppose $1+1 \neq 0$ in $K$. If we interchange the two identical rows of $A$, we still obtain the matrix $A$. Hence, by Problem 8.23, $|A|=-|A|$, and so $|A|=0$.

Now suppose $1+1=0$ in $K$. Then $\operatorname{sgn} \sigma=1$ for every $\sigma \in S_{n}$. Because $A$ has two identical rows, we can arrange the terms of $A$ into pairs of equal terms. Because each pair is 0 , the determinant of $A$ is zero.

(iii) Suppose $A=\left[a_{i j}\right]$ is lower triangular; that is, the entries above the diagonal are all zero: $a_{i j}=0$ whenever $i<j$. Consider a term $t$ of the determinant of $A$ :

$$
t=(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots a_{n i_{n}}, \quad \text { where } \quad \sigma=i_{1} i_{2} \cdots i_{n}
$$

Suppose $i_{1} \neq 1$. Then $1<i_{1}$ and so $a_{1 i_{1}}=0$; hence, $t=0$. That is, each term for which $i_{1} \neq 1$ is zero.

Now suppose $i_{1}=1$ but $i_{2} \neq 2$. Then $2<i_{2}$, and so $a_{2 i_{2}}=0$; hence, $t=0$. Thus, each term for which $i_{1} \neq 1$ or $i_{2} \neq 2$ is zero.

Similarly, we obtain that each term for which $i_{1} \neq 1$ or $i_{2} \neq 2$ or $\ldots$ or $i_{n} \neq n$ is zero. Accordingly, $|A|=a_{11} a_{22} \cdots a_{n n}=$ product of diagonal elements.

8.25. Prove Theorem 8.3: $B$ is obtained from $A$ by an elementary operation.

(i) If two rows (columns) of $A$ were interchanged, then $|B|=-|A|$.

(ii) If a row (column) of $A$ were multiplied by a scalar $k$, then $|B|=k|A|$.

(iii) If a multiple of a row (column) of $A$ were added to another row (column) of $A$, then $|B|=|A|$.

(i) This result was proved in Problem 8.23.

(ii) If the $j$ th row of $A$ is multiplied by $k$, then every term in $|A|$ is multiplied by $k$, and so $|B|=k|A|$. That is,

$$
|B|=\sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots\left(k a_{j i_{j}}\right) \cdots a_{n i_{n}}=k \sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots a_{n i_{n}}=k|A|
$$

(iii) Suppose $c$ times the $k$ th row is added to the $j$ th row of $A$. Using the symbol ${ }^{\wedge}$ to denote the $j$ th position in a determinant term, we have

$$
\begin{aligned}
|B| & =\sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots\left(\widehat{c a_{k i_{k}}+a_{j i_{j}}}\right) \ldots a_{n i_{n}} \\
& =c \sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots \widehat{a_{k i_{k}}} \cdots a_{n i_{n}}+\sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots a_{j i_{j}} \cdots a_{n i_{n}}
\end{aligned}
$$

The first sum is the determinant of a matrix whose $k$ th and $j$ th rows are identical. Accordingly, by Theorem 8.2(ii), the sum is zero. The second sum is the determinant of $A$. Thus, $|B|=c \cdot 0+|A|=|A|$.

8.26. Prove Lemma 8.6: Let $E$ be an elementary matrix. Then $|E A|=|E||A|$.

Consider the elementary row operations: (i) Multiply a row by a constant $k \neq 0$,

(ii) Interchange two rows, (iii) Add a multiple of one row to another.

Let $E_{1}, E_{2}, E_{3}$ be the corresponding elementary matrices That is, $E_{1}, E_{2}, E_{3}$ are obtained by applying the above operations to the identity matrix $I$. By Problem 8.25,

$$
\left|E_{1}\right|=k|I|=k, \quad\left|E_{2}\right|=-|I|=-1, \quad\left|E_{3}\right|=|I|=1
$$

Recall (Theorem 3.11) that $E_{i} A$ is identical to the matrix obtained by applying the corresponding operation to $A$. Thus, by Theorem 8.3 , we obtain the following which proves our lemma:

$$
\left|E_{1} A\right|=k|A|=\left|E_{1}\right||A|, \quad\left|E_{2} A\right|=-|A|=\left|E_{2}\right||A|, \quad\left|E_{3} A\right|=|A|=1|A|=\left|E_{3}\right||A|
$$

8.27. Suppose $B$ is row equivalent to a square matrix $A$. Prove that $|B|=0$ if and only if $|A|=0$.

By Theorem 8.3, the effect of an elementary row operation is to change the sign of the determinant or to multiply the determinant by a nonzero scalar. Hence, $|B|=0$ if and only if $|A|=0$.

8.28. Prove Theorem 8.5 : Let $A$ be an $n$-square matrix. Then the following are equivalent:\\
(i) $A$ is invertible,\\
(ii) $A X=0$ has only the zero solution,\\
(iii) $\operatorname{det}(A) \neq 0$.

The proof is by the Gaussian algorithm. If $A$ is invertible, it is row equivalent to $I$. But $|I| \neq 0$. Hence, by Problem 8.27, $|A| \neq 0$. If $A$ is not invertible, it is row equivalent to a matrix with a zero row. Hence, $\operatorname{det}(A)=0$. Thus, (i) and (iii) are equivalent.

If $A X=0$ has only the solution $X=0$, then $A$ is row equivalent to $I$ and $A$ is invertible. Conversely, if $A$ is invertible with inverse $A^{-1}$, then

$$
X=I X=\left(A^{-1} A\right) X=A^{-1}(A X)=A^{-1} 0=0
$$

is the only solution of $A X=0$. Thus, (i) and (ii) are equivalent.

8.29. Prove Theorem 8.4: $|A B|=|A||B|$.

If $A$ is singular, then $A B$ is also singular, and so $|A B|=0=|A||B|$. On the other hand, if $A$ is nonsingular, then $A=E_{n} \cdots E_{2} E_{1}$, a product of elementary matrices. Then, Lemma 8.6 and induction yields

$$
|A B|=\left|E_{n} \cdots E_{2} E_{1} B\right|=\left|E_{n}\right| \cdots\left|E_{2}\right|\left|E_{1}\right||B|=|A||B|
$$

8.30. Suppose $P$ is invertible. Prove that $\left|P^{-1}\right|=|P|^{-1}$.

$$
P^{-1} P=I \text {. Hence, } 1=|I|=\left|P^{-1} P\right|=\left|P^{-1}\right||P| \text {, and so }\left|P^{-1}\right|=|P|^{-1} \text {. }
$$

8.31. Prove Theorem 8.7: Suppose $A$ and $B$ are similar matrices. Then $|A|=|B|$.

Because $A$ and $B$ are similar, there exists an invertible matrix $P$ such that $B=P^{-1} A P$. Therefore, using Problem 8.30, we get $|B|=\left|P^{-1} A P\right|=\left|P^{-1}\right||A||P|=|A|\left|P^{-1}\right||P=| A \mid$.

We remark that although the matrices $P^{-1}$ and $A$ may not commute, their determinants $\left|P^{-1}\right|$ and $|A|$ do commute, because they are scalars in the field $K$.

8.32. Prove Theorem 8.8 (Laplace): Let $A=\left[a_{i j}\right]$, and let $A_{i j}$ denote the cofactor of $a_{i j}$. Then, for any $i$ or $j$

$$
|A|=a_{i 1} A_{i 1}+\cdots+a_{i n} A_{i n} \quad \text { and } \quad|A|=a_{1 j} A_{1 j}+\cdots+a_{n j} A_{n j}
$$

Because $|A|=\left|A^{T}\right|$, we need only prove one of the expansions, say, the first one in terms of rows of $A$. Each term in $|A|$ contains one and only one entry of the $i$ th row $\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right)$ of $A$. Hence, we can write $|A|$ in the form

$$
|A|=a_{i 1} A_{i 1}^{*}+a_{i 2} A_{i 2}^{*}+\cdots+a_{i n} A_{i n}^{*}
$$

(Note that $A_{i j}^{*}$ is a sum of terms involving no entry of the $i$ th row of $A$.) Thus, the theorem is proved if we can show that

$$
A_{i j}^{*}=A_{i j}=(-1)^{i+j}\left|M_{i j}\right|
$$

where $M_{i j}$ is the matrix obtained by deleting the row and column containing the entry $a_{i j}$. (Historically, the expression $A_{i j}^{*}$ was defined as the cofactor of $a_{i j}$, and so the theorem reduces to showing that the two definitions of the cofactor are equivalent.)

First we consider the case that $i=n, j=n$. Then the sum of terms in $|A|$ containing $a_{n n}$ is

$$
a_{n n} A_{n n}^{*}=a_{n n} \sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n-1, \sigma(n-1)}
$$

where we sum over all permutations $\sigma \in S_{n}$ for which $\sigma(n)=n$. However, this is equivalent (Prove!) to summing over all permutations of $\{1, \ldots, n-1\}$. Thus, $A_{n n}^{*}=\left|M_{n n}\right|=(-1)^{n+n}\left|M_{n n}\right|$.

Now we consider any $i$ and $j$. We interchange the $i$ th row with each succeeding row until it is last, and we interchange the $j$ th column with each succeeding column until it is last. Note that the determinant $\left|M_{i j}\right|$ is not affected, because the relative positions of the other rows and columns are not affected by these interchanges. However, the "sign" of $|A|$ and of $A_{i j}^{*}$ is changed $n-1$ and then $n-j$ times. Accordingly,

$$
A_{i j}^{*}=(-1)^{n-i+n-j}\left|M_{i j}\right|=(-1)^{i+j}\left|M_{i j}\right|
$$

8.33. Let $A=\left[a_{i j}\right]$ and let $B$ be the matrix obtained from $A$ by replacing the $i$ th row of $A$ by the row vector $\left(b_{i 1}, \ldots, b_{i n}\right)$. Show that

$$
|B|=b_{i 1} A_{i 1}+b_{i 2} A_{i 2}+\cdots+b_{i n} A_{i n}
$$

Furthermore, show that, for $j \neq i$,

$$
a_{j 1} A_{i 1}+a_{j 2} A_{i 2}+\cdots+a_{j n} A_{i n}=0 \quad \text { and } \quad a_{1 j} A_{1 i}+a_{2 j} A_{2 i}+\cdots+a_{n j} A_{n i}=0
$$

Let $B=\left[b_{i j}\right]$. By Theorem 8.8 ,

$$
|B|=b_{i 1} B_{i 1}+b_{i 2} B_{i 2}+\cdots+b_{i n} B_{i n}
$$

Because $B_{i j}$ does not depend on the $i$ th row of $B$, we get $B_{i j}=A_{i j}$ for $j=1, \ldots, n$. Hence,

$$
|B|=b_{i 1} A_{i 1}+b_{i 2} A_{i 2}+\cdots+b_{i n} A_{i n}
$$

Now let $A^{\prime}$ be obtained from $A$ by replacing the $i$ th row of $A$ by the $j$ th row of $A$. Because $A^{\prime}$ has two identical rows, $\left|A^{\prime}\right|=0$. Thus, by the above result,

$$
\left|A^{\prime}\right|=a_{j 1} A_{i 1}+a_{j 2} A_{i 2}+\cdots+a_{j n} A_{i n}=0
$$

Using $\left|A^{T}\right|=|A|$, we also obtain that $a_{1 j} A_{1 i}+a_{2 j} A_{2 i}+\cdots+a_{n j} A_{n i}=0$.

8.34. Prove Theorem 8.9: $A(\operatorname{adj} A)=(\operatorname{adj} A) A=|A| I$.

Let $A=\left[a_{i j}\right]$ and let $A(\operatorname{adj} A)=\left[b_{i j}\right]$. The $i$ th row of $A$ is


\begin{equation*}
\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right) \tag{1}
\end{equation*}


Because adj $A$ is the transpose of the matrix of cofactors, the $j$ th column of adj $A$ is the tranpose of the cofactors of the $j$ th row of $A$ :


\begin{equation*}
\left(A_{j}, A_{j 2}, \ldots, A_{j n}\right)^{T} \tag{2}
\end{equation*}


Now $b_{i j}$, the $i j$ entry in $A(\operatorname{adj} A)$, is obtained by multiplying expressions (1) and (2):

$$
b_{i j}=a_{i 1} A_{j 1}+a_{i 2} A_{j 2}+\cdots+a_{i n} A_{j n}
$$

By Theorem 8.8 and Problem 8.33 ,

$$
b_{i j}=\left\{\begin{array}{cc}
|A| & \text { if } i=j \\
0 & \text { if } i \neq j
\end{array}\right.
$$

Accordingly, $A(\operatorname{adj} A)$ is the diagonal matrix with each diagonal element $|A|$. In other words, $A(\operatorname{adj} A)=|A| I$. Similarly, $(\operatorname{adj} A) A=|A| I$.

8.35. Prove Theorem 8.10 (Cramer's rule): The (square) system $A X=B$ has a unique solution if and only if $D \neq 0$. In this case, $x_{i}=N_{i} / D$ for each $i$.

By previous results, $A X=B$ has a unique solution if and only if $A$ is invertible, and $A$ is invertible if and only if $D=|A| \neq 0$.

Now suppose $D \neq 0$. By Theorem $8.9, A^{-1}=(1 / D)(\operatorname{adj} A)$. Multiplying $A X=B$ by $A^{-1}$, we obtain


\begin{equation*}
X=A^{-1} A X=(1 / D)(\operatorname{adj} A) B \tag{1}
\end{equation*}


Note that the $i$ th row of $(1 / D)(\operatorname{adj} A)$ is $(1 / D)\left(A_{1 i}, A_{2 i}, \ldots, A_{n i}\right)$. If $B=\left(b_{1}, b_{2}, \ldots, b_{n}\right)^{T}$, then, by (1),

$$
x_{i}=(1 / D)\left(b_{1} A_{1 i}+b_{2} A_{2 i}+\cdots+b_{n} A_{n i}\right)
$$

However, as in Problem 8.33, $b_{1} A_{1 i}+b_{2} A_{2 i}+\cdots+b_{n} A_{n i}=N_{i}$, the determinant of the matrix obtained by replacing the $i$ th column of $A$ by the column vector $B$. Thus, $x_{i}=(1 / D) N_{i}$, as required.

8.36. Prove Theorem 8.12: Suppose $M$ is an upper (lower) triangular block matrix with diagonal blocks $A_{1}, A_{2}, \ldots, A_{n}$. Then

$$
\operatorname{det}(M)=\operatorname{det}\left(A_{1}\right) \operatorname{det}\left(A_{2}\right) \cdots \operatorname{det}\left(A_{n}\right)
$$

We need only prove the theorem for $n=2$ - that is, when $M$ is a square matrix of the form

$M=\left[\begin{array}{cc}A & C \\ 0 & B\end{array}\right]$. The proof of the general theorem follows easily by induction.

Suppose $A=\left[a_{i j}\right]$ is $r$-square, $B=\left[b_{i j}\right]$ is $s$-square, and $M=\left[m_{i j}\right]$ is $n$-square, where $n=r+s$. By definition,

$$
\operatorname{det}(M)=\sum_{\sigma \in S_{n}}(\operatorname{sgn} \sigma) m_{1 \sigma(1)} m_{2 \sigma(2)} \cdots m_{n \sigma(n)}
$$

If $i>r$ and $j \leq r$, then $m_{i j}=0$. Thus, we need only consider those permutations $\sigma$ such that

$$
\sigma\{r+1, r+2, \ldots, r+s\}=\{r+1, r+2, \ldots, r+s\} \quad \text { and } \quad \sigma\{1,2, \ldots, r\}=\{1,2, \ldots, r\}
$$

Let $\sigma_{1}(k)=\sigma(k)$ for $k \leq r$, and let $\sigma_{2}(k)=\sigma(r+k)-r$ for $k \leq s$. Then

$$
(\operatorname{sgn} \sigma) m_{1 \sigma(1)} m_{2 \sigma(2)} \cdots m_{n \sigma(n)}=\left(\operatorname{sgn} \sigma_{1}\right) a_{1 \sigma_{1}(1)} a_{2 \sigma_{1}(2)} \cdots a_{r \sigma_{1}(r)}\left(\operatorname{sgn} \sigma_{2}\right) b_{1 \sigma_{2}(1)} b_{2 \sigma_{2}(2)} \cdots b_{s \sigma_{2}(s)}
$$

which implies $\operatorname{det}(M)=\operatorname{det}(A) \operatorname{det}(B)$.

8.37. Prove Theorem 8.14: There exists a unique function $D: \mathbf{M} \rightarrow K$ such that

(i) $D$ is multilinear, (ii) $D$ is alternating, (iii) $D(I)=1$.

This function $D$ is the determinant function; that is, $D(A)=|A|$.

Let $D$ be the determinant function, $D(A)=|A|$. We must show that $D$ satisfies (i), (ii), and (iii), and that $D$ is the only function satisfying (i), (ii), and (iii).

By Theorem 8.2, $D$ satisfies (ii) and (iii). Hence, we show that it is multilinear. Suppose the $i$ th row of $A=\left[a_{i j}\right]$ has the form $\left(b_{i 1}+c_{i 1}, b_{i 2}+c_{i 2}, \ldots, b_{i n}+c_{i n}\right)$. Then

$$
\begin{aligned}
D(A) & =D\left(A_{1}, \ldots, B_{i}+C_{i}, \ldots, A_{n}\right) \\
& =\sum_{S_{n}}(\operatorname{sgn} \sigma) a_{1 \sigma(1)} \cdots a_{i-1, \sigma(i-1)}\left(b_{i \sigma(i)}+c_{i \sigma(i)}\right) \cdots a_{n \sigma(n)} \\
& =\sum_{S_{n}}(\operatorname{sgn} \sigma) a_{1 \sigma(1)} \cdots b_{i \sigma(i)} \cdots a_{n \sigma(n)}+\sum_{S_{n}}(\operatorname{sgn} \sigma) a_{1 \sigma(1)} \cdots c_{i \sigma(i)} \cdots a_{n \sigma(n)} \\
& =D\left(A_{1}, \ldots, B_{i}, \ldots, A_{n}\right)+D\left(A_{1}, \ldots, C_{i}, \ldots, A_{n}\right)
\end{aligned}
$$

Also, by Theorem 8.3(ii),

$$
D\left(A_{1}, \ldots, k A_{i}, \ldots, A_{n}\right)=k D\left(A_{1}, \ldots, A_{i}, \ldots, A_{n}\right)
$$

Thus, $D$ is multilinear $-D$ satisfies (i).

We next must prove the uniqueness of $D$. Suppose $D$ satisfies (i), (ii), and (iii). If $\left\{e_{1}, \ldots, e_{n}\right\}$ is the usual basis of $K^{n}$, then, by (iii), $D\left(e_{1}, e_{2}, \ldots, e_{n}\right)=D(I)=1$. Using (ii), we also have that


\begin{equation*}
D\left(e_{i_{1}}, e_{i_{2}}, \ldots, e_{i_{n}}\right)=\operatorname{sgn} \sigma, \quad \text { where } \quad \sigma=i_{1} i_{2} \cdots i_{n} \tag{1}
\end{equation*}


Now suppose $A=\left[a_{i j}\right]$. Observe that the $k$ th $\operatorname{row} A_{k}$ of $A$ is

$$
A_{k}=\left(a_{k 1}, a_{k 2}, \ldots, a_{k n}\right)=a_{k 1} e_{1}+a_{k 2} e_{2}+\cdots+a_{k n} e_{n}
$$

Thus,

$$
D(A)=D\left(a_{11} e_{1}+\cdots+a_{1 n} e_{n}, a_{21} e_{1}+\cdots+a_{2 n} e_{n}, \ldots, a_{n 1} e_{1}+\cdots+a_{n n} e_{n}\right)
$$

Using the multilinearity of $D$, we can write $D(A)$ as a sum of terms of the form


\begin{align*}
D(A) & =\sum D\left(a_{1 i_{1}} e_{i_{1}}, a_{2 i_{2}} e_{i_{2}}, \ldots, a_{n i_{n}} e_{i_{n}}\right) \\
& =\sum\left(a_{1 i_{1}} a_{2 i_{2}} \cdots a_{n i_{n}}\right) D\left(e_{i_{1}}, e_{i_{2}}, \ldots, e_{i_{n}}\right) \tag{2}
\end{align*}


where the sum is summed over all sequences $i_{1} i_{2} \ldots i_{n}$, where $i_{k} \in\{1, \ldots, n\}$. If two of the indices are equal, say $i_{j}=i_{k}$ but $j \neq k$, then, by (ii),

$$
D\left(e_{i_{1}}, e_{i_{2}}, \ldots, e_{i_{n}}\right)=0
$$

Accordingly, the sum in (2) need only be summed over all permutations $\sigma=i_{1} i_{2} \cdots i_{n}$. Using (1), we finally have that

$$
\begin{aligned}
D(A) & =\sum_{\sigma}\left(a_{1 i_{1}} a_{2 i_{2}} \cdots a_{n i_{n}}\right) D\left(e_{i_{1}}, e_{i_{2}}, \ldots, e_{i_{n}}\right) \\
& =\sum_{\sigma}(\operatorname{sgn} \sigma) a_{1 i_{1}} a_{2 i_{2}} \cdots a_{n i_{n}}, \quad \text { where } \quad \sigma=i_{1} i_{2} \cdots i_{n}
\end{aligned}
$$

Hence, $D$ is the determinant function, and so the theorem is proved.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Computation of Determinants}
8.38. Evaluate:\\
(a) $\left|\begin{array}{ll}2 & 6 \\ 4 & 1\end{array}\right|$,\\
(b) $\left|\begin{array}{rr}5 & 1 \\ 3 & -2\end{array}\right|$,\\
(c) $\left|\begin{array}{rr}-2 & 8 \\ -5 & -3\end{array}\right|$,\\
(d) $\left|\begin{array}{rr}4 & 9 \\ 1 & -3\end{array}\right|$,\\
(e) $\left|\begin{array}{cc}a+b & a \\ b & a+b\end{array}\right|$

8.39. Find all $t$ such that (a) $\left|\begin{array}{cc}t-4 & 3 \\ 2 & t-9\end{array}\right|=0$, (b) $\left|\begin{array}{cc}t-1 & 4 \\ 3 & t-2\end{array}\right|=0$

8.40. Compute the determinant of each of the following matrices:\\
(a) $\left[\begin{array}{rrr}2 & 1 & 1 \\ 0 & 5 & -2 \\ 1 & -3 & 4\end{array}\right]$,\\
(b) $\left[\begin{array}{rrr}3 & -2 & -4 \\ 2 & 5 & -1 \\ 0 & 6 & 1\end{array}\right]$,\\
(c) $\left[\begin{array}{rrr}-2 & -1 & 4 \\ 6 & -3 & -2 \\ 4 & 1 & 2\end{array}\right]$,\\
(d) $\left[\begin{array}{rrr}7 & 6 & 5 \\ 1 & 2 & 1 \\ 3 & -2 & 1\end{array}\right]$

8.41. Find the determinant of each of the following matrices:\\
(a) $\left[\begin{array}{rrrr}1 & 2 & 2 & 3 \\ 1 & 0 & -2 & 0 \\ 3 & -1 & 1 & -2 \\ 4 & -3 & 0 & 2\end{array}\right]$,\\
(b) $\left[\begin{array}{rrrr}2 & 1 & 3 & 2 \\ 3 & 0 & 1 & -2 \\ 1 & -1 & 4 & 3 \\ 2 & 2 & -1 & 1\end{array}\right]$

8.42. Evaluate:\\
(a) $\left|\begin{array}{rrrr}2 & -1 & 3 & -4 \\ 2 & 1 & -2 & 1 \\ 3 & 3 & -5 & 4 \\ 5 & 2 & -1 & 4\end{array}\right|$,\\
(b) $\left|\begin{array}{rrrr}2 & -1 & 4 & -3 \\ -1 & 1 & 0 & 2 \\ 3 & 2 & 3 & -1 \\ 1 & -2 & 2 & -3\end{array}\right|$,\\
(c) $\left|\begin{array}{rrrr}1 & -2 & 3 & -1 \\ 1 & 1 & -2 & 0 \\ 2 & 0 & 4 & -5 \\ 1 & 4 & 4 & -6\end{array}\right|$

8.43. Evaluate each of the following determinants:\\
(a) $\left|\begin{array}{rrrrr}1 & 2 & -1 & 3 & 1 \\ 2 & -1 & 1 & -2 & 3 \\ 3 & 1 & 0 & 2 & -1 \\ 5 & 1 & 2 & -3 & 4 \\ -2 & 3 & -1 & 1 & -2\end{array}\right|$,\\
(b) $\left|\begin{array}{lllll}1 & 3 & 5 & 7 & 9 \\ 2 & 4 & 2 & 4 & 2 \\ 0 & 0 & 1 & 2 & 3 \\ 0 & 0 & 5 & 6 & 2 \\ 0 & 0 & 2 & 3 & 1\end{array}\right|$,\\
(c) $\left|\begin{array}{lllll}1 & 2 & 3 & 4 & 5 \\ 5 & 4 & 3 & 2 & 1 \\ 0 & 0 & 6 & 5 & 1 \\ 0 & 0 & 0 & 7 & 4 \\ 0 & 0 & 0 & 2 & 3\end{array}\right|$

\section*{Cofactors, Classical Adjoints, Inverses}
8.44. Find $\operatorname{det}(A), \operatorname{adj} A$, and $A^{-1}$, where\\
(a) $A=\left[\begin{array}{lll}1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 2 & 1\end{array}\right]$,\\
(b) $A=\left[\begin{array}{lll}1 & 2 & 2 \\ 3 & 1 & 0 \\ 1 & 1 & 1\end{array}\right]$

8.45. Find the classical adjoint of each matrix in Problem 8.41.

8.46. Let $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$. (a) Find adj $A$, (b) $\quad$ Show that $\operatorname{adj}(\operatorname{adj} A)=A, \quad$ (c) $\quad$ When $\operatorname{does} A=\operatorname{adj} A$ ?

8.47. Show that if $A$ is diagonal (triangular) then adj $A$ is diagonal (triangular).

8.48. Suppose $A=\left[a_{i j}\right]$ is triangular. Show that

(a) $A$ is invertible if and only if each diagonal element $a_{i i} \neq 0$.

(b) The diagonal elements of $A^{-1}$ (if it exists) are $a_{i i}^{-1}$, the reciprocals of the diagonal elements of $A$.

\section*{Minors, Principal Minors}
8.49. Let $A=\left[\begin{array}{rrrr}1 & 2 & 3 & 2 \\ 1 & 0 & -2 & 3 \\ 3 & -1 & 2 & 5 \\ 4 & -3 & 0 & -1\end{array}\right]$ and $B=\left[\begin{array}{rrrr}1 & 3 & -1 & 5 \\ 2 & -3 & 1 & 4 \\ 0 & -5 & 2 & 1 \\ 3 & 0 & 5 & -2\end{array}\right]$. Find the minor and the signed minor corresponding to the following submatrices:\\
(a) $A(1,4 ; \quad 3,4)$,\\
(b) $B(1,4 ; \quad 3,4)$\\
(c) $A(2,3$;\\
$2,4)$,\\
(d) $B(2,3 ; 2,4)$.

8.50. For $k=1,2,3$, find the sum $S_{k}$ of all principal minors of order $k$ for\\
(a) $A=\left[\begin{array}{rrr}1 & 3 & 2 \\ 2 & -4 & 3 \\ 5 & -2 & 1\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrr}1 & 5 & -4 \\ 2 & 6 & 1 \\ 3 & -2 & 0\end{array}\right]$,\\
(c) $C=\left[\begin{array}{rrr}1 & -4 & 3 \\ 2 & 1 & 5 \\ 4 & -7 & 11\end{array}\right]$

8.51. For $k=1,2,3,4$, find the sum $S_{k}$ of all principal minors of order $k$ for\\
(a) $A=\left[\begin{array}{rrrr}1 & 2 & 3 & -1 \\ 1 & -2 & 0 & 5 \\ 0 & 1 & -2 & 2 \\ 4 & 0 & -1 & -3\end{array}\right]$,\\
(b) $B=\left[\begin{array}{llll}1 & 2 & 1 & 2 \\ 0 & 1 & 2 & 3 \\ 1 & 3 & 0 & 4 \\ 2 & 7 & 4 & 5\end{array}\right]$

\section*{Determinants and Linear Equations}
8.52. Solve the following systems by determinants:\\
(a) $\left\{\begin{array}{l}3 x+5 y=8 \\ 4 x-2 y=1\end{array}\right.$,\\
(b) $\left\{\begin{array}{l}2 x-3 y=-1 \\ 4 x+7 y=-1\end{array}\right.$,\\
(c) $\left\{\begin{aligned} a x-2 b y & =c \\ 3 a x-5 b y & =2 c\end{aligned} \quad(a b \neq 0)\right.$

8.53. Solve the following systems by determinants:

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-296}
\end{center}

8.54. Prove Theorem 8.11: The system $A X=0$ has a nonzero solution if and only if $D=|A|=0$.

\section*{Permutations}
8.55. Find the parity of the permutations $\sigma=32154, \tau=13524, \pi=42531$ in $S_{5}$.

8.56. For the permutations in Problem 8.55, find\\
(a) $\tau \circ \sigma$,\\
(b) $\pi \circ \sigma$\\
(c) $\sigma^{-1}$,\\
(d) $\tau^{-1}$.

8.57. Let $\tau \in S_{n}$. Show that $\tau \circ \sigma$ runs through $S_{n}$ as $\sigma$ runs through $S_{n}$, that is, $S_{n}=\left\{\tau \circ \sigma: \sigma \in S_{n}\right\}$.

8.58. Let $\sigma \in S_{n}$ have the property that $\sigma(n)=n$. Let $\sigma^{*} \in S_{n-1}$ be defined by $\sigma^{*}(x)=\sigma(x)$.

(a) Show that $\operatorname{sgn} \sigma^{*}=\operatorname{sgn} \sigma$,

(b) Show that as $\sigma$ runs through $S_{n}$, where $\sigma(n)=n, \sigma^{*}$ runs through $S_{n-1}$; that is,

$$
S_{n-1}=\left\{\sigma^{*}: \sigma \in S_{n}, \sigma(n)=n\right\}
$$

8.59. Consider a permutation $\sigma=j_{1} j_{2} \ldots j_{n}$. Let $\left\{e_{i}\right\}$ be the usual basis of $K^{n}$, and let $A$ be the matrix whose $i$ th row is $e_{j_{i}}$ [i.e., $A=\left(e_{j_{1}}, e_{j_{2}}, \ldots, e_{j_{n}}\right)$ ]. Show that $|A|=\operatorname{sgn} \sigma$.

\section*{Determinant of Linear Operators}
8.60. Find the determinant of each of the following linear transformations:

(a) $T: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $T(x, y)=(2 x-9 y, 3 x-5 y)$,

(b) $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by $T(x, y, z)=(3 x-2 z, 5 y+7 z, x+y+z)$,

(c) $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$ defined by $T(x, y, z)=(2 x+7 y-4 z, 4 x-6 y+2 z)$.

8.61. Let $\mathbf{D}: V \rightarrow V$ be the differential operator; that is, $\mathbf{D}(f(t))=d f / d t$. Find $\operatorname{det}(\mathbf{D})$ if $V$ is the vector space of functions with the following bases: (a) $\left\{1, t, \ldots, t^{5}\right\}$, (b) $\left\{e^{t}, e^{2 t}, e^{3 t}\right\}$, (c) $\{\sin t, \cos t\}$.

8.62. Prove Theorem 8.13: Let $F$ and $G$ be linear operators on a vector space $V$. Then\\
(i) $\operatorname{det}(F \circ G)=\operatorname{det}(F) \operatorname{det}(G)$,\\
(ii) $F$ is invertible if and only if $\operatorname{det}(F) \neq 0$.

8.63. Prove (a) $\operatorname{det}\left(\mathbf{1}_{V}\right)=1$, where $\mathbf{1}_{V}$ is the identity operator, (b) $-\operatorname{det}\left(T^{-1}\right)=\operatorname{det}(T)^{-1}$ when $T$ is invertible.

\section*{Miscellaneous Problems}
8.64. Find the volume $V(S)$ of the parallelopiped $S$ in $\mathbf{R}^{3}$ determined by the following vectors:

(a) $u_{1}=(1,2,-3), u_{2}=(3,4,-1), u_{3}=(2,-1,5)$,

(b) $u_{1}=(1,1,3), u_{2}=(1,-2,-4), u_{3}=(4,1,5)$.

8.65. Find the volume $V(S)$ of the parallelepiped $S$ in $\mathbf{R}^{4}$ determined by the following vectors:

$$
u_{1}=(1,-2,5,-1), u_{2}=(2,1,-2,1), u_{3}=(3,0,1-2), u_{4}=(1,-1,4,-1)
$$

8.66. Let $V$ be the space of $2 \times 2$ matrices $M=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ over $\mathbf{R}$. Determine whether $D: V \rightarrow \mathbf{R}$ is 2-linear (with\\
respect to the rows), where\\
(a) $\mathrm{D}(\mathrm{M})=\mathrm{a}+\mathrm{d}$\\
(c) $\mathrm{D}(\mathrm{M})=\mathrm{ac}-\mathrm{bd}$,\\
(e) $\quad \mathrm{D}(\mathrm{M})=0$\\
(b) $\mathrm{D}(\mathrm{M})=\mathrm{ad}$,\\
(d) $\mathrm{D}(\mathrm{M})=\mathrm{ab}-\mathrm{cd}$,\\
(f) $\quad \mathrm{D}(\mathrm{M})=1$

8.67. Let $A$ be an $n$-square matrix. Prove $|k A|=k^{n}|A|$.

8.68. Let $A, B, C, D$ be commuting $n$-square matrices. Consider the $2 n$-square block matrix $M=\left[\begin{array}{ll}A & B \\ C & D\end{array}\right]$. Prove that $|M|=|A||D|-|B||C|$. Show that the result may not be true if the matrices do not commute.

8.69. Suppose $A$ is orthogonal; that is, $A^{T} A=I$. Show that $\operatorname{det}(A)= \pm 1$.

8.70. Let $V$ be the space of $m$-square matrices viewed as $m$-tuples of row vectors. Suppose $D: V \rightarrow K$ is $m$-linear and alternating. Show that

(a) $D(\ldots, A, \ldots, B, \ldots)=-D(\ldots, B, \ldots, A, \ldots)$; sign changed when two rows are interchanged.

(b) If $A_{1}, A_{2}, \ldots, A_{m}$ are linearly dependent, then $D\left(A_{1}, A_{2}, \ldots, A_{m}\right)=0$.

8.71. Let $V$ be the space of $m$-square matrices (as above), and suppose $D: V \rightarrow K$. Show that the following weaker statement is equivalent to $D$ being alternating:

$$
D\left(A_{1}, A_{2}, \ldots, A_{n}\right)=0 \quad \text { whenever } \quad A_{i}=A_{i+1} \text { for some } i
$$

Let $V$ be the space of $n$-square matrices over $K$. Suppose $B \in V$ is invertible and $\operatorname{sot} \operatorname{det} B) \neq 0$. Define $D: V \rightarrow K$ by $D(A)=\operatorname{det}(A B) / \operatorname{det}(B)$, where $A \in V$. Hence,

$$
D\left(A_{1}, A_{2}, \ldots, A_{n}\right)=\operatorname{det}\left(A_{1} B, A_{2} B, \ldots, A_{n} B\right) / \operatorname{det}(B)
$$

where $A_{i}$ is the $i$ th row of $A$, and so $A_{i} B$ is the $i$ th row of $A B$. Show that $D$ is multilinear and alternating, and that $D(I)=1$. (This method is used by some texts to prove that $|A B|=|A||B|$.)

8.72. Show that $g=g\left(x_{1}, \ldots, x_{n}\right)=(-1)^{n} V_{n-1}(x)$ where $g=g\left(x_{i}\right)$ is the difference product in Problem 8.19, $x=x_{n}$, and $V_{n-1}$ is the Vandermonde determinant defined by

$$
V_{n-1}(x) \equiv\left[\left.\begin{array}{ccccc}
1 & 1 & \ldots & 1 & 1 \\
x_{1} & x_{2} & \ldots & x_{n-1} & x \\
x_{1}^{2} & x_{2}^{2} & \ldots & x_{n-1}^{2} & x^{2} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
x_{1}^{n-1} & x_{2}^{n-1} & \ldots & x_{n-1}^{n-1} & x^{n-1}
\end{array} \right\rvert\,\right.
$$

8.73. Let $A$ be any matrix. Show that the signs of a minor $A[I, J]$ and its complementary minor $A\left[I^{\prime}, J^{\prime}\right]$ are equal.

8.74. Let $A$ be an $n$-square matrix. The determinantal rank of $A$ is the order of the largest square submatrix of $A$ (obtained by deleting rows and columns of $A$ ) whose determinant is not zero. Show that the determinantal rank of $A$ is equal to its rank-the maximum number of linearly independent rows (or columns).

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $M=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ denotes a matrix with rows $R_{1}, R_{2}, \ldots$\\
8.38. (a) -22 ,\\
(b) -13 ,\\
(c) 46 ,\\
(d) -21 ,\\
(e) $a^{2}+a b+b^{2}$

8.39. (a) 3,10 ;

(b) $5,-2$

8.40. (a) 21 ,

(b) -11 ,

(c) 100 ,

(d) 0

8.41. (a) -131 ,

(b) -55

8.42. (a) 33 ,

(b) 0 ,

(c) 45

8.43. (a) -32 ,

(b) -14 ,

(c) -468

8.44. (a) $|A|=-2, \quad$ adj $A=[-1,-1,1 ; \quad-1,1,-1 ; \quad 2,-2,0]$,

(b) $|A|=-1, \quad$ adj $A=[1,0,-2 ; \quad-3,-1,6 ; \quad 2,1,-5]$. Also, $A^{-1}=(\operatorname{adj} A) /|A|$

8.45. (a) $[-16,-29,-26,-2 ; \quad-30,-38,-16,29 ; \quad-8,51,-13,-1 ; \quad-13,1,28,-18]$,

(b) $[21,-14,-17,-19 ; \quad-44,11,33,11 ; \quad-29,1,13,21 ; \quad 17,7,-19,-18]$

8.46. (a) $\operatorname{adj} A=[d,-b ; \quad-c, a]$,

(c) $A=k I$

8.49. (a) $-3,-3$,

(b) $-23,-23$,

(c) $3,-3$,

(d) $17,-17$

8.50. (a) $-2,-17,73$,

(b) 7,10,105,

(c) $13,54,0$

8.51. (a) $-6,13,62,-219$;

(b) $7,-37,30,20$

8.52. (a) $x=\frac{21}{26}, y=\frac{29}{26}$;

(b) $x=-\frac{5}{13}, y=\frac{1}{13}$;

(c) $x=-\frac{c}{a}, y=-\frac{c}{b}$

8.53. (a) $x=5, y=2, z=1$,

(b) Because $D=0$, the system cannot be solved by determinants.

8.55. (a) $\operatorname{sgn} \sigma=1, \operatorname{sgn} \tau=-1, \operatorname{sgn} \pi=-1$\\
8.56. (a) $\tau \circ \sigma=53142$,\\
(b) $\pi \circ \sigma=52413$,\\
(c) $\sigma^{-1}=32154$,\\
(d) $\tau^{-1}=14253$

8.60. (a) $\operatorname{det}(T)=17$,

(b) $\operatorname{det}(T)=4$,

(c) not defined

8.61. (a) 0 ,

(b) 6 ,

(c) 1

8.64. (a) 18 ,

(b) 0

8.65. 17\\
8.66. (a) no,\\
(b) yes,\\
(c) yes,\\
(d) no,\\
(e) yes,\\
(f) no

\section*{Diagonalization: Eigenvalues and Eigenvectors}
\subsection*{9.1 Introduction}
The ideas in this chapter can be discussed from two points of view.

\section*{Matrix Point of View}
Suppose an $n$-square matrix $A$ is given. The matrix $A$ is said to be diagonalizable if there exists a nonsingular matrix $P$ such that

$$
B=P^{-1} A P
$$

is diagonal. This chapter discusses the diagonalization of a matrix $A$. In particular, an algorithm is given to find the matrix $P$ when it exists.

\section*{Linear Operator Point of View}
Suppose a linear operator $T: V \rightarrow V$ is given. The linear operator $T$ is said to be diagonalizable if there exists a basis $S$ of $V$ such that the matrix representation of $T$ relative to the basis $S$ is a diagonal matrix $D$. This chapter discusses conditions under which the linear operator $T$ is diagonalizable.

\section*{Equivalence of the Two Points of View}
The above two concepts are essentially the same. Specifically, a square matrix $A$ may be viewed as a linear operator $F$ defined by

$$
F(X)=A X
$$

where $X$ is a column vector, and $B=P^{-1} A P$ represents $F$ relative to a new coordinate system (basis) $S$ whose elements are the columns of $P$. On the other hand, any linear operator $T$ can be represented by a matrix $A$ relative to one basis and, when a second basis is chosen, $T$ is represented by the matrix

$$
B=P^{-1} A P
$$

where $P$ is the change-of-basis matrix.

Most theorems will be stated in two ways: one in terms of matrices $A$ and again in terms of linear mappings $T$.

\section*{Role of Underlying Field $\boldsymbol{K}$}
The underlying number field $K$ did not play any special role in our previous discussions on vector spaces and linear mappings. However, the diagonalization of a matrix $A$ or a linear operator $T$ will depend on the\\
roots of a polynomial $\Delta(t)$ over $K$, and these roots do depend on $K$. For example, suppose $\Delta(t)=t^{2}+1$. Then $\Delta(t)$ has no roots if $K=\mathbf{R}$, the real field; but $\Delta(t)$ has roots $\pm i$ if $K=\mathbf{C}$, the complex field. Furthermore, finding the roots of a polynomial with degree greater than two is a subject unto itself (frequently discussed in numerical analysis courses). Accordingly, our examples will usually lead to those polynomials $\Delta(t)$ whose roots can be easily determined.

\subsection*{9.2 Polynomials of Matrices}
Consider a polynomial $f(t)=a_{n} t^{n}+\cdots+a_{1} t+a_{0}$ over a field $K$. Recall (Section 2.8) that if $A$ is any square matrix, then we define

$$
f(A)=a_{n} A^{n}+\cdots+a_{1} A+a_{0} I
$$

where $I$ is the identity matrix. In particular, we say that $A$ is a root of $f(t)$ if $f(A)=0$, the zero matrix.

EXAMPLE 9.1 Let $A=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right]$. Then $A^{2}=\left[\begin{array}{rr}7 & 10 \\ 15 & 22\end{array}\right]$. Let

$$
f(t)=2 t^{2}-3 t+5 \quad \text { and } \quad g(t)=t^{2}-5 t-2
$$

Then

$$
f(A)=2 A^{2}-3 A+5 I=\left[\begin{array}{ll}
14 & 20 \\
30 & 44
\end{array}\right]+\left[\begin{array}{rr}
-3 & -6 \\
-9 & -12
\end{array}\right]+\left[\begin{array}{ll}
5 & 0 \\
0 & 5
\end{array}\right]=\left[\begin{array}{ll}
16 & 14 \\
21 & 37
\end{array}\right]
$$

and

$$
g(A)=A^{2}-5 A-2 I=\left[\begin{array}{rr}
7 & 10 \\
15 & 22
\end{array}\right]+\left[\begin{array}{rr}
-5 & -10 \\
-15 & -20
\end{array}\right]+\left[\begin{array}{rr}
-2 & 0 \\
0 & -2
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right]
$$

Thus, $A$ is a zero of $g(t)$.

The following theorem (proved in Problem 9.7) applies.

THEOREM 9.1: $\quad$ Let $f$ and $g$ be polynomials. For any square matrix $A$ and scalar $k$,

$$
\begin{array}{ll}
\text { (i) } \quad(f+g)(A)=f(A)+g(A) & \text { (iii) } \quad(k f)(A)=k f(A) \\
\text { (ii) } \quad(f g)(A)=f(A) g(A) & \text { (iv) } f(A) g(A)=g(A) f(A)
\end{array}
$$

Observe that (iv) tells us that any two polynomials in $A$ commute.

\section*{Matrices and Linear Operators}
Now suppose that $T: V \rightarrow V$ is a linear operator on a vector space $V$. Powers of $T$ are defined by the composition operation:

$$
T^{2}=T \circ T, \quad T^{3}=T^{2} \circ T, \quad \ldots
$$

Also, for any polynomial $f(t)=a_{n} t^{n}+\cdots+a_{1} t+a_{0}$, we define $f(T)$ in the same way as we did for matrices:

$$
f(T)=a_{n} T^{n}+\cdots+a_{1} T+a_{0} I
$$

where $I$ is now the identity mapping. We also say that $T$ is a zero or root of $f(t)$ if $f(T)=0$, the zero mapping. We note that the relations in Theorem 9.1 hold for linear operators as they do for matrices.

Remark: Suppose $A$ is a matrix representation of a linear operator $T$. Then $f(A)$ is the matrix representation of $f(T)$, and, in particular, $f(T)=0$ if and only if $f(A)=0$.

\subsection*{9.3 Characteristic Polynomial, Cayley-Hamilton Theorem}
Let $A=\left[a_{i j}\right]$ be an $n$-square matrix. The matrix $M=A-t I_{n}$, where $I_{n}$ is the $n$-square identity matrix and $t$ is an indeterminate, may be obtained by subtracting $t$ down the diagonal of $A$. The negative of $M$ is the matrix $t I_{n}-A$, and its determinant

$$
\Delta(t)=\operatorname{det}\left(t I_{n}-A\right)=(-1)^{n} \operatorname{det}\left(A-t I_{n}\right)
$$

which is a polynomial in $t$ of degree $n$ and is called the characteristic polynomial of $A$.

We state an important theorem in linear algebra (proved in Problem 9.8).

THEOREM 9.2: (Cayley-Hamilton) Every matrix $A$ is a root of its characteristic polynomial.

Remark: Suppose $A=\left[a_{i j}\right]$ is a triangular matrix. Then $t I-A$ is a triangular matrix with diagonal entries $t-a_{i i}$; hence,

$$
\Delta(t)=\operatorname{det}(t I-A)=\left(t-a_{11}\right)\left(t-a_{22}\right) \cdots\left(t-a_{n n}\right)
$$

Observe that the roots of $\Delta(t)$ are the diagonal elements of $A$.

EXAMPLE 9.2 Let $A=\left[\begin{array}{ll}1 & 3 \\ 4 & 5\end{array}\right]$. Its characteristic polynomial is

$$
\Delta(t)=|t I-A|=\left|\begin{array}{rr}
t-1 & -3 \\
-4 & t-5
\end{array}\right|=(t-1)(t-5)-12=t^{2}-6 t-7
$$

As expected from the Cayley-Hamilton theorem, $A$ is a root of $\Delta(t)$; that is,

$$
\Delta(A)=A^{2}-6 A-7 I=\left[\begin{array}{ll}
13 & 18 \\
24 & 37
\end{array}\right]+\left[\begin{array}{rr}
-6 & -18 \\
-24 & -30
\end{array}\right]+\left[\begin{array}{rr}
-7 & 0 \\
0 & -7
\end{array}\right]=\left[\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right]
$$

Now suppose $A$ and $B$ are similar matrices, say $B=P^{-1} A P$, where $P$ is invertible. We show that $A$ and $B$ have the same characteristic polynomial. Using $t I=P^{-1} t I P$, we have

$$
\begin{aligned}
\Delta_{B}(t) & =\operatorname{det}(t I-B)=\operatorname{det}\left(t I-P^{-1} A P\right)=\operatorname{det}\left(P^{-1} t I P-P^{-1} A P\right) \\
& =\operatorname{det}\left[P^{-1}(t I-A) P\right]=\operatorname{det}\left(P^{-1}\right) \operatorname{det}(t I-A) \operatorname{det}(P)
\end{aligned}
$$

Using the fact that determinants are scalars and commute and that $\operatorname{det}\left(P^{-1}\right) \operatorname{det}(P)=1$, we finally obtain

$$
\Delta_{B}(t)=\operatorname{det}(t I-A)=\Delta_{A}(t)
$$

Thus, we have proved the following theorem.

THEOREM 9.3: Similar matrices have the same characteristic polynomial.

\section*{Characteristic Polynomials of Degrees 2 and 3}
There are simple formulas for the characteristic polynomials of matrices of orders 2 and 3.

(a) Suppose $A=\left[\begin{array}{ll}a_{11} & a_{12} \\ a_{21} & a_{22}\end{array}\right]$. Then

$$
\Delta(t)=t^{2}-\left(a_{11}+a_{22}\right) t+\operatorname{det}(A)=t^{2}-\operatorname{tr}(A) t+\operatorname{det}(A)
$$

Here $\operatorname{tr}(A)$ denotes the trace of $A$-that is, the sum of the diagonal elements of $A$.

(b) Suppose $A=\left[\begin{array}{lll}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{array}\right]$. Then

$$
\Delta(t)=t^{3}-\operatorname{tr}(A) t^{2}+\left(A_{11}+A_{22}+A_{33}\right) t-\operatorname{det}(A)
$$

(Here $A_{11}, A_{22}, A_{33}$ denote, respectively, the cofactors of $a_{11}, a_{22}, a_{33}$.)

EXAMPLE 9.3 Find the characteristic polynomial of each of the following matrices:

(a) $A=\left[\begin{array}{rr}5 & 3 \\ 2 & 10\end{array}\right]$, (b) $B=\left[\begin{array}{rr}7 & -1 \\ 6 & 2\end{array}\right]$, (c) $C=\left[\begin{array}{rr}5 & -2 \\ 4 & -4\end{array}\right]$.

(a) We have $\operatorname{tr}(A)=5+10=15$ and $|A|=50-6=44$; hence, $\Delta(t)+t^{2}-15 t+44$.

(b) We have $\operatorname{tr}(B)=7+2=9$ and $|B|=14+6=20$; hence, $\Delta(t)=t^{2}-9 t+20$.

(c) We have $\operatorname{tr}(C)=5-4=1$ and $|C|=-20+8=-12$; hence, $\Delta(t)=t^{2}-t-12$.

EXAMPLE 9.4 Find the characteristic polynomial of $A=\left[\begin{array}{lll}1 & 1 & 2 \\ 0 & 3 & 2 \\ 1 & 3 & 9\end{array}\right]$.

We have $\operatorname{tr}(A)=1+3+9=13$. The cofactors of the diagonal elements are as follows:

$$
A_{11}=\left|\begin{array}{ll}
3 & 2 \\
3 & 9
\end{array}\right|=21, \quad A_{22}=\left|\begin{array}{ll}
1 & 2 \\
1 & 9
\end{array}\right|=7, \quad A_{33}=\left|\begin{array}{ll}
1 & 1 \\
0 & 3
\end{array}\right|=3
$$

Thus, $A_{11}+A_{22}+A_{33}=31$. Also, $|A|=27+2+0-6-6-0=17$. Accordingly,

$$
\Delta(t)=t^{3}-13 t^{2}+31 t-17
$$

Remark: The coefficients of the characteristic polynomial $\Delta(t)$ of the 3 -square matrix $A$ are, with alternating signs, as follows:

$$
S_{1}=\operatorname{tr}(A), \quad S_{2}=A_{11}+A_{22}+A_{33}, \quad S_{3}=\operatorname{det}(A)
$$

We note that each $S_{k}$ is the sum of all principal minors of $A$ of order $k$. general.

The next theorem, whose proof lies beyond the scope of this text, tells us that this result is true in

THEOREM 9.4: Let $A$ be an $n$-square matrix. Then its characteristic polynomial is

$$
\Delta(t)=t^{n}-S_{1} t^{n-1}+S_{2} t^{n-2}+\cdots+(-1)^{n} S_{n}
$$

where $S_{k}$ is the sum of the principal minors of order $k$.

\section*{Characteristic Polynomial of a Linear Operator}
Now suppose $T: V \rightarrow V$ is a linear operator on a vector space $V$ of finite dimension. We define the characteristic polynomial $\Delta(t)$ of $T$ to be the characteristic polynomial of any matrix representation of $T$. Recall that if $A$ and $B$ are matrix representations of $T$, then $B=P^{-1} A P$, where $P$ is a change-of-basis matrix. Thus, $A$ and $B$ are similar, and by Theorem 9.3, $A$ and $B$ have the same characteristic polynomial. Accordingly, the characteristic polynomial of $T$ is independent of the particular basis in which the matrix representation of $T$ is computed.

Because $f(T)=0$ if and only if $f(A)=0$, where $f(t)$ is any polynomial and $A$ is any matrix representation of $T$, we have the following analogous theorem for linear operators.

THEOREM 9.2': (Cayley-Hamilton) A linear operator $T$ is a zero of its characteristic polynomial.

\subsection*{9.4 Diagonalization, Eigenvalues and Eigenvectors}
Let $A$ be any $n$-square matrix. Then $A$ can be represented by (or is similar to) a diagonal matrix $D=\operatorname{diag}\left(k_{1}, k_{2}, \ldots, k_{n}\right)$ if and only if there exists a basis $S$ consisting of (column) vectors $u_{1}, u_{2}, \ldots, u_{n}$ such that

$$
\begin{array}{ccc}
A u_{1} & =k_{1} u_{1} \\
A u_{2} & =\quad k_{2} u_{2} & \\
\quad \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
A u_{n} & = & k_{n} u_{n}
\end{array}
$$

In such a case, $A$ is said to be diagonizable. Furthermore, $D=P^{-1} A P$, where $P$ is the nonsingular matrix whose columns are, respectively, the basis vectors $u_{1}, u_{2}, \ldots, u_{n}$.

The above observation leads us to the following definition.

DEFINITION: $\quad$ Let $A$ be any square matrix. A scalar $\lambda$ is called an eigenvalue of $A$ if there exists a nonzero (column) vector $v$ such that

$$
A v=\lambda v
$$

Any vector satisfying this relation is called an eigenvector of $A$ belonging to the eigenvalue $\lambda$.

We note that each scalar multiple $k v$ of an eigenvector $v$ belonging to $\lambda$ is also such an eigenvector, because

$$
A(k v)=k(A v)=k(\lambda v)=\lambda(k v)
$$

The set $E_{\lambda}$ of all such eigenvectors is a subspace of $V$ (Problem 9.19), called the eigenspace of $\lambda$. (If $\operatorname{dim} E_{\lambda}=1$, then $E_{\lambda}$ is called an eigenline and $\lambda$ is called a scaling factor.)

The terms characteristic value and characteristic vector (or proper value and proper vector) are sometimes used instead of eigenvalue and eigenvector.

The above observation and definitions give us the following theorem.

THEOREM 9.5: An $n$-square matrix $A$ is similar to a diagonal matrix $D$ if and only if $A$ has $n$ linearly independent eigenvectors. In this case, the diagonal elements of $D$ are the corresponding eigenvalues and $D=P^{-1} A P$, where $P$ is the matrix whose columns are the eigenvectors.

Suppose a matrix $A$ can be diagonalized as above, say $P^{-1} A P=D$, where $D$ is diagonal. Then $A$ has the extremely useful diagonal factorization:

$$
A=P D P^{-1}
$$

Using this factorization, the algebra of $A$ reduces to the algebra of the diagonal matrix $D$, which can be easily calculated. Specifically, suppose $D=\operatorname{diag}\left(k_{1}, k_{2}, \ldots, k_{n}\right)$. Then

$$
A^{m}=\left(P D P^{-1}\right)^{m}=P D^{m} P^{-1}=P \operatorname{diag}\left(k_{1}^{m}, \ldots, k_{n}^{m}\right) P^{-1}
$$

More generally, for any polynomial $f(t)$,

$$
f(A)=f\left(P D P^{-1}\right)=P f(D) P^{-1}=P \operatorname{diag}\left(f\left(k_{1}\right), f\left(k_{2}\right), \ldots, f\left(k_{n}\right)\right) P^{-1}
$$

Furthermore, if the diagonal entries of $D$ are nonnegative, let

$$
B=P \operatorname{diag}\left(\sqrt{k_{1}}, \sqrt{k_{2}}, \ldots, \sqrt{k_{n}}\right) P^{-1}
$$

Then $B$ is a nonnegative square root of $A$; that is, $B^{2}=A$ and the eigenvalues of $B$ are nonnegative.

EXAMPLE 9.5 Let $A=\left[\begin{array}{ll}3 & 1 \\ 2 & 2\end{array}\right]$ and let $v_{1}=\left[\begin{array}{r}1 \\ -2\end{array}\right]$ and $v_{2}=\left[\begin{array}{l}1 \\ 1\end{array}\right]$. Then

$$
A v_{1}=\left[\begin{array}{ll}
3 & 1 \\
2 & 2
\end{array}\right]\left[\begin{array}{r}
1 \\
-2
\end{array}\right]=\left[\begin{array}{r}
1 \\
-2
\end{array}\right]=v_{1} \quad \text { and } \quad A v_{2}=\left[\begin{array}{ll}
3 & 1 \\
2 & 2
\end{array}\right]\left[\begin{array}{l}
1 \\
1
\end{array}\right]=\left[\begin{array}{l}
4 \\
4
\end{array}\right]=4 v_{2}
$$

Thus, $v_{1}$ and $v_{2}$ are eigenvectors of $A$ belonging, respectively, to the eigenvalues $\lambda_{1}=1$ and $\lambda_{2}=4$. Observe that $v_{1}$ and $v_{2}$ are linearly independent and hence form a basis of $\mathbf{R}^{2}$. Accordingly, $A$ is diagonalizable. Furthermore, let $P$ be the matrix whose columns are the eigenvectors $v_{1}$ and $v_{2}$. That is, let

$$
P=\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right], \quad \text { and so } \quad P^{-1}=\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]
$$

Then $A$ is similar to the diagonal matrix

$$
D=P^{-1} A P=\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]\left[\begin{array}{ll}
3 & 1 \\
2 & 2
\end{array}\right]\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
0 & 4
\end{array}\right]
$$

As expected, the diagonal elements 1 and 4 in $D$ are the eigenvalues corresponding, respectively, to the eigenvectors $v_{1}$ and $v_{2}$, which are the columns of $P$. In particular, $A$ has the factorization

$$
A=P D P^{-1}=\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 0 \\
0 & 4
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]
$$

Accordingly,

$$
A^{4}=\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right]\left[\begin{array}{rr}
1 & 0 \\
0 & 256
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]=\left[\begin{array}{ll}
171 & 85 \\
170 & 86
\end{array}\right]
$$

Moreover, suppose $f(t)=t^{3}-5 t^{2}+3 t+6$; hence, $f(1)=5$ and $f(4)=2$. Then

$$
f(A)=P f(D) P^{-1}=\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right]\left[\begin{array}{ll}
5 & 0 \\
0 & 2
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]=\left[\begin{array}{rr}
3 & -1 \\
-2 & 4
\end{array}\right]
$$

Last, we obtain a "positive square root" of $A$. Specifically, using $\sqrt{1}=1$ and $\sqrt{4}=2$, we obtain the matrix

$$
B=P \sqrt{D} P^{-1}=\left[\begin{array}{rr}
1 & 1 \\
-2 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 0 \\
0 & 2
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{2}{3} & \frac{1}{3}
\end{array}\right]=\left[\begin{array}{ll}
\frac{5}{3} & \frac{1}{3} \\
\frac{2}{3} & \frac{4}{3}
\end{array}\right]
$$

where $B^{2}=A$ and where $B$ has positive eigenvalues 1 and 2 .

Remark: Throughout this chapter, we use the following fact:

$$
\text { If } P=\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right] \text {, then } P^{-1}=\left[\begin{array}{rr}
d /|P| & -b /|P| \\
-c /|P| & a /|P|
\end{array}\right] \text {. }
$$

That is, $P^{-1}$ is obtained by interchanging the diagonal elements $a$ and $d$ of $P$, taking the negatives of the nondiagonal elements $b$ and $c$, and dividing each element by the determinant $|P|$.

\section*{Properties of Eigenvalues and Eigenvectors}
Example 9.5 indicates the advantages of a diagonal representation (factorization) of a square matrix. In the following theorem (proved in Problem 9.20), we list properties that help us to find such a representation.

THEOREM 9.6: Let $A$ be a square matrix. Then the following are equivalent.

(i) A scalar $\lambda$ is an eigenvalue of $A$.

(ii) The matrix $M=A-\lambda I$ is singular.

(iii) The scalar $\lambda$ is a root of the characteristic polynomial $\Delta(t)$ of $A$.

The eigenspace $E_{\lambda}$ of an eigenvalue $\lambda$ is the solution space of the homogeneous system $M X=0$, where $M=A-\lambda I$; that is, $M$ is obtained by subtracting $\lambda$ down the diagonal of $A$.

Some matrices have no eigenvalues and hence no eigenvectors. However, using Theorem 9.6 and the Fundamental Theorem of Algebra (every polynomial over the complex field $\mathbf{C}$ has a root), we obtain the following result.

THEOREM 9.7: Let $A$ be a square matrix over the complex field $\mathbf{C}$. Then $A$ has at least one eigenvalue.

The following theorems will be used subsequently. (The theorem equivalent to Theorem 9.8 for linear operators is proved in Problem 9.21, and Theorem 9.9 is proved in Problem 9.22.)

THEOREM 9.8: Suppose $v_{1}, v_{2}, \ldots, v_{n}$ are nonzero eigenvectors of a matrix $A$ belonging to distinct eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Then $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent.

THEOREM 9.9: Suppose the characteristic polynomial $\Delta(t)$ of an $n$-square matrix $A$ is a product of $n$ distinct factors, say, $\Delta(t)=\left(t-a_{1}\right)\left(t-a_{2}\right) \cdots\left(t-a_{n}\right)$. Then $A$ is similar to the diagonal matrix $D=\operatorname{diag}\left(a_{1}, a_{2}, \ldots, a_{n}\right)$.

If $\lambda$ is an eigenvalue of a matrix $A$, then the algebraic multiplicity of $\lambda$ is defined to be the multiplicity of $\lambda$ as a root of the characteristic polynomial of $A$, and the geometric multiplicity of $\lambda$ is defined to be the dimension of its eigenspace, $\operatorname{dim} E_{\lambda}$. The following theorem (whose equivalent for linear operators is proved in Problem 9.23) holds.

THEOREM 9.10: The geometric multiplicity of an eigenvalue $\lambda$ of a matrix $A$ does not exceed its algebraic multiplicity.

\section*{Diagonalization of Linear Operators}
Consider a linear operator $T: V \rightarrow V$. Then $T$ is said to be diagonalizable if it can be represented by a diagonal matrix $D$. Thus, $T$ is diagonalizable if and only if there exists a basis $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ of $V$ for which

$$
\begin{array}{lr}
T\left(u_{1}\right)=k_{1} u_{1} \\
T\left(u_{2}\right)=\quad k_{2} u_{2} & \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
T\left(u_{n}\right)= & k_{n} u_{n}
\end{array}
$$

In such a case, $T$ is represented by the diagonal matrix

$$
D=\operatorname{diag}\left(k_{1}, k_{2}, \ldots, k_{n}\right)
$$

relative to the basis $S$.

The above observation leads us to the following definitions and theorems, which are analogous to the definitions and theorems for matrices discussed above.

DEFINITION: Let $T$ be a linear operator. A scalar $\lambda$ is called an eigenvalue of $T$ if there exists a nonzero vector $v$ such that $T(v)=\lambda v$.

Every vector satisfying this relation is called an eigenvector of $T$ belonging to the eigenvalue $\lambda$.

The set $E_{\lambda}$ of all eigenvectors belonging to an eigenvalue $\lambda$ is a subspace of $V$, called the eigenspace of $\lambda$. (Alternatively, $\lambda$ is an eigenvalue of $T$ if $\lambda I-T$ is singular, and, in this case, $E_{\lambda}$ is the kernel of $\lambda I-T$.) The algebraic and geometric multiplicities of an eigenvalue $\lambda$ of a linear operator $T$ are defined in the same way as those of an eigenvalue of a matrix $A$.

The following theorems apply to a linear operator $T$ on a vector space $V$ of finite dimension.

THEOREM 9.5': $\quad T$ can be represented by a diagonal matrix $D$ if and only if there exists a basis $S$ of $V$ consisting of eigenvectors of $T$. In this case, the diagonal elements of $D$ are the corresponding eigenvalues.

THEOREM 9.6': Let $T$ be a linear operator. Then the following are equivalent:

(i) A scalar $\lambda$ is an eigenvalue of $T$.

(ii) The linear operator $\lambda I-T$ is singular.

(iii) The scalar $\lambda$ is a root of the characteristic polynomial $\Delta(t)$ of $T$.

THEOREM 9.7': Suppose $V$ is a complex vector space. Then $T$ has at least one eigenvalue.

THEOREM 9.8': Suppose $v_{1}, v_{2}, \ldots, v_{n}$ are nonzero eigenvectors of a linear operator $T$ belonging to distinct eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Then $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent.

THEOREM 9.9': Suppose the characteristic polynomial $\Delta(t)$ of $T$ is a product of $n$ distinct factors, say, $\Delta(t)=\left(t-a_{1}\right)\left(t-a_{2}\right) \cdots\left(t-a_{n}\right)$. Then $T$ can be represented by the diagonal matrix $D=\operatorname{diag}\left(a_{1}, a_{2}, \ldots, a_{n}\right)$.

THEOREM 9.10': The geometric multiplicity of an eigenvalue $\lambda$ of $T$ does not exceed its algebraic multiplicity.

Remark: The following theorem reduces the investigation of the diagonalization of a linear operator $T$ to the diagonalization of a matrix $A$.

THEOREM 9.11: $\quad$ Suppose $A$ is a matrix representation of $T$. Then $T$ is diagonalizable if and only if $A$ is diagonalizable.

\subsection*{9.5 Computing Eigenvalues and Eigenvectors, Diagonalizing Matrices}
This section gives an algorithm for computing eigenvalues and eigenvectors for a given square matrix $A$ and for determining whether or not a nonsingular matrix $P$ exists such that $P^{-1} A P$ is diagonal.

ALGORITHM 9.1: (Diagonalization Algorithm) The input is an $n$-square matrix $A$.

Step 1. Find the characteristic polynomial $\Delta(t)$ of $A$.

Step 2. Find the roots of $\Delta(t)$ to obtain the eigenvalues of $A$.

Step 3. Repeat (a) and (b) for each eigenvalue $\lambda$ of $A$.

(a) Form the matrix $M=A-\lambda I$ by subtracting $\lambda$ down the diagonal of $A$.

(b) Find a basis for the solution space of the homogeneous system $M X=0$. (These basis vectors are linearly independent eigenvectors of $A$ belonging to $\lambda$.)

Step 4. Consider the collection $S=\left\{v_{1}, v_{2}, \ldots, v_{m}\right\}$ of all eigenvectors obtained in Step 3.

(a) If $m \neq n$, then $A$ is not diagonalizable.

(b) If $m=n$, then $A$ is diagonalizable. Specifically, let $P$ be the matrix whose columns are the eigenvectors $v_{1}, v_{2}, \ldots, v_{n}$. Then

$$
D=P^{-1} A P=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right)
$$

where $\lambda_{i}$ is the eigenvalue corresponding to the eigenvector $v_{i}$.

EXAMPLE 9.6 The diagonalizable algorithm is applied to $A=\left[\begin{array}{rr}4 & 2 \\ 3 & -1\end{array}\right]$.

(1) The characteristic polynomial $\Delta(t)$ of $A$ is computed. We have

$$
\operatorname{tr}(A)=4-1=-3, \quad|A|=-4-6=-10
$$

hence,

$$
\Delta(t)=t^{2}-3 t-10=(t-5)(t+2)
$$

(2) Set $\Delta(t)=(t-5)(t+2)=0$. The roots $\lambda_{1}=5$ and $\lambda_{2}=-2$ are the eigenvalues of $A$.

(3) (i) We find an eigenvector $v_{1}$ of $A$ belonging to the eigenvalue $\lambda_{1}=5$. Subtract $\lambda_{1}=5$ down the diagonal of $A$ to obtain the matrix $M=\left[\begin{array}{rr}-1 & 2 \\ 3 & -6\end{array}\right]$. The eigenvectors belonging to $\lambda_{1}=5$ form the solution of the homogeneous system $M X=0$; that is,

$$
\left[\begin{array}{rr}
-1 & 2 \\
3 & -6
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right] \quad \text { or } \quad \begin{array}{r}
-x+2 y=0 \\
3 x-6 y=0
\end{array} \quad \text { or } \quad-x+2 y=0
$$

The system has only one free variable. Thus, a nonzero solution, for example, $v_{1}=(2,1)$, is an eigenvector that spans the eigenspace of $\lambda_{1}=5$.

(ii) We find an eigenvector $v_{2}$ of $A$ belonging to the eigenvalue $\lambda_{2}=-2$. Subtract -2 (or add 2) down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ll}
6 & 2 \\
3 & 1
\end{array}\right] \text { and the homogenous system } \quad \begin{array}{r}
6 x+2 y=0 \\
3 x+y=0
\end{array} \quad \text { or } \quad 3 x+y=0 .
$$

The system has only one independent solution. Thus, a nonzero solution, say $v_{2}=(-1,3)$, is an eigenvector that spans the eigenspace of $\lambda_{2}=-2$.

(4) Let $P$ be the matrix whose columns are the eigenvectors $v_{1}$ and $v_{2}$. Then

$$
P=\left[\begin{array}{rr}
2 & -1 \\
1 & 3
\end{array}\right], \quad \text { and so } \quad P^{-1}=\left[\begin{array}{rr}
\frac{3}{7} & \frac{1}{7} \\
-\frac{1}{7} & \frac{2}{7}
\end{array}\right]
$$

Accordingly, $D=P^{-1} A P$ is the diagonal matrix whose diagonal entries are the corresponding eigenvalues; that is,

$$
D=P^{-1} A P=\left[\begin{array}{rr}
\frac{3}{7} & \frac{1}{7} \\
-\frac{1}{7} & \frac{2}{7}
\end{array}\right]\left[\begin{array}{rr}
4 & 2 \\
3 & -1
\end{array}\right]\left[\begin{array}{rr}
2 & -1 \\
1 & 3
\end{array}\right]=\left[\begin{array}{rr}
5 & 0 \\
0 & -2
\end{array}\right]
$$

EXAMPLE 9.7 Consider the matrix $B=\left[\begin{array}{rr}5 & -1 \\ 1 & 3\end{array}\right]$. We have

$$
\operatorname{tr}(B)=5+3=8, \quad|B|=15+1=16 ; \quad \text { so } \quad \Delta(t)=t^{2}-8 t+16=(t-4)^{2}
$$

Accordingly, $\lambda=4$ is the only eigenvalue of $B$.

Subtract $\lambda=4$ down the diagonal of $B$ to obtain the matrix

$$
M=\left[\begin{array}{ll}
1 & -1 \\
1 & -1
\end{array}\right] \text { and the homogeneous system } \begin{aligned}
& x-y=0 \\
& x-y=0
\end{aligned} \text { or } \quad x-y=0
$$

The system has only one independent solution; for example, $x=1, y=1$. Thus, $v=(1,1)$ and its multiples are the only eigenvectors of $B$. Accordingly, $B$ is not diagonalizable, because there does not exist a basis consisting of eigenvectors of $B$.

EXAMPLE 9.8 Consider the matrix $A=\left[\begin{array}{ll}3 & -5 \\ 2 & -3\end{array}\right]$. Here $\operatorname{tr}(A)=3-3=0$ and $|A|=-9+10=1$. Thus, $\Delta(t)=t^{2}+1$ is the characteristic polynomial of $A$. We consider two cases:

(a) $A$ is a matrix over the real field $\mathbf{R}$. Then $\Delta(t)$ has no (real) roots. Thus, $A$ has no eigenvalues and no eigenvectors, and so $A$ is not diagonalizable.

(b) $A$ is a matrix over the complex field $\mathbf{C}$. Then $\Delta(t)=(t-i)(t+i)$ has two roots, $i$ and $-i$. Thus, $A$ has two distinct eigenvalues $i$ and $-i$, and hence, $A$ has two independent eigenvectors. Accordingly there exists a nonsingular matrix $P$ over the complex field $\mathbf{C}$ for which

$$
P^{-1} A P=\left[\begin{array}{rr}
i & 0 \\
0 & -i
\end{array}\right]
$$

Therefore, $A$ is diagonalizable (over $\mathbf{C}$ ).

\subsection*{9.6 Diagonalizing Real Symmetric Matrices and Quadratic Forms}
There are many real matrices $A$ that are not diagonalizable. In fact, some real matrices may not have any (real) eigenvalues. However, if $A$ is a real symmetric matrix, then these problems do not exist. Namely, we have the following theorems.

THEOREM 9.12: $\quad$ Let $A$ be a real symmetric matrix. Then each $\operatorname{root} \lambda$ of its characteristic polynomial is real.

THEOREM 9.13: $\quad$ Let $A$ be a real symmetric matrix. Suppose $u$ and $v$ are eigenvectors of $A$ belonging to distinct eigenvalues $\lambda_{1}$ and $\lambda_{2}$. Then $u$ and $v$ are orthogonal, that; is, $\langle u, v\rangle=0$.

The above two theorems give us the following fundamental result.

THEOREM 9.14: $\quad$ Let $A$ be a real symmetric matrix. Then there exists an orthogonal matrix $P$ such that $D=P^{-1} A P$ is diagonal.

The orthogonal matrix $P$ is obtained by normalizing a basis of orthogonal eigenvectors of $A$ as illustrated below. In such a case, we say that $A$ is "orthogonally diagonalizable."

EXAMPLE 9.9 Let $A=\left[\begin{array}{rr}2 & -2 \\ -2 & 5\end{array}\right]$, a real symmetric matrix. Find an orthogonal matrix $P$ such that $P^{-1} A P$ is\\
diagonal. First we find the characteristic polynomial $\Delta(t)$ of $A$. We have

$$
\operatorname{tr}(A)=2+5=7, \quad|A|=10-4=6 ; \quad \text { so } \quad \Delta(t)=t^{2}-7 t+6=(t-6)(t-1)
$$

Accordingly, $\lambda_{1}=6$ and $\lambda_{2}=1$ are the eigenvalues of $A$.

(a) Subtracting $\lambda_{1}=6$ down the diagonal of $A$ yields the matrix

$$
M=\left[\begin{array}{rr}
-4 & -2 \\
-2 & -1
\end{array}\right] \quad \text { and the homogeneous system } \quad \begin{array}{r}
-4 x-2 y=0 \\
-2 x-y=0
\end{array} \quad \text { or } \quad 2 x+y=0
$$

A nonzero solution is $u_{1}=(1,-2)$.\\
(b) Subtracting $\lambda_{2}=1$ down the diagonal of $A$ yields the matrix

$M=\left[\begin{array}{rr}1 & -2 \\ -2 & 4\end{array}\right] \quad$ and the homogeneous system $\quad x-2 y=0$

(The second equation drops out, because it is a multiple of the first equation.) A nonzero solution is $u_{2}=(2,1)$.

As expected from Theorem 9.13, $u_{1}$ and $u_{2}$ are orthogonal. Normalizing $u_{1}$ and $u_{2}$ yields the orthonormal vectors

$$
\hat{u}_{1}=(1 / \sqrt{5},-2 / \sqrt{5}) \quad \text { and } \quad \hat{u}_{2}=(2 / \sqrt{5}, 1 / \sqrt{5})
$$

Finally, let $P$ be the matrix whose columns are $\hat{u}_{1}$ and $\hat{u}_{2}$, respectively. Then

$$
P=\left[\begin{array}{rr}
1 / \sqrt{5} & 2 / \sqrt{5} \\
-2 / \sqrt{5} & 1 / \sqrt{5}
\end{array}\right] \quad \text { and } \quad P^{-1} A P=\left[\begin{array}{ll}
6 & 0 \\
0 & 1
\end{array}\right]
$$

As expected, the diagonal entries of $P^{-1} A P$ are the eigenvalues corresponding to the columns of $P$.

The procedure in the above Example 9.9 is formalized in the following algorithm, which finds an orthogonal matrix $P$ such that $P^{-1} A P$ is diagonal.

ALGORITHM 9.2: (Orthogonal Diagonalization Algorithm) The input is a real symmetric matrix $A$.

Step 1. Find the characteristic polynomial $\Delta(t)$ of $A$.

Step 2. Find the eigenvalues of $A$, which are the roots of $\Delta(t)$.

Step 3. For each eigenvalue $\lambda$ of $A$ in Step 2, find an orthogonal basis of its eigenspace.

Step 4. Normalize all eigenvectors in Step 3, which then forms an orthonormal basis of $\mathbf{R}^{n}$.

Step 5. Let $P$ be the matrix whose columns are the normalized eigenvectors in Step 4 .

\section*{Application to Quadratic Forms}
Let $q$ be a real polynomial in variables $x_{1}, x_{2}, \ldots, x_{n}$ such that every term in $q$ has degree two; that is,

$$
q\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\sum_{i} c_{i} x_{i}^{2}+\sum_{i<j} d_{i j} x_{i} x_{j}, \quad \text { where } \quad c_{i}, d_{i j} \in \mathbf{R}
$$

Then $q$ is called a quadratic form. If there are no cross-product terms $x_{i} x_{j}$ (i.e., all $d_{i j}=0$ ), then $q$ is said to be diagonal.

The above quadratic form $q$ determines a real symmetric matrix $A=\left[a_{i j}\right]$, where $a_{i i}=c_{i}$ and $a_{i j}=a_{j i}=\frac{1}{2} d_{i j}$. Namely, $q$ can be written in the matrix form

$$
q(X)=X^{T} A X
$$

where $X=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{T}$ is the column vector of the variables. Furthermore, suppose $X=P Y$ is a linear substitution of the variables. Then substitution in the quadratic form yields

$$
q(Y)=(P Y)^{T} A(P Y)=Y^{T}\left(P^{T} A P\right) Y
$$

Thus, $P^{T} A P$ is the matrix representation of $q$ in the new variables.

We seek an orthogonal matrix $P$ such that the orthogonal substitution $X=P Y$ yields a diagonal quadratic form for which $P^{T} A P$ is diagonal. Because $P$ is orthogonal, $P^{T}=P^{-1}$, and hence, $P^{T} A P=P^{-1} A P$. The above theory yields such an orthogonal matrix $P$.

EXAMPLE 9.10 Consider the quadratic form

$$
q(x, y)=2 x^{2}-4 x y+5 y^{2}=X^{T} A X, \quad \text { where } \quad A=\left[\begin{array}{rr}
2 & -2 \\
-2 & 5
\end{array}\right] \quad \text { and } \quad X=\left[\begin{array}{l}
x \\
y
\end{array}\right]
$$

By Example 9.9,

$$
P^{-1} A P=\left[\begin{array}{ll}
6 & 0 \\
0 & 1
\end{array}\right]=P^{T} A P, \quad \text { where } \quad P=\left[\begin{array}{rr}
1 / \sqrt{5} & 2 / \sqrt{5} \\
-2 / \sqrt{5} & 1 / \sqrt{5}
\end{array}\right]
$$

Let $Y=[s, t]^{T}$. Then matrix $P$ corresponds to the following linear orthogonal substitution $x=P Y$ of the variables $x$ and $y$ in terms of the variables $s$ and $t$ :

$$
x=\frac{1}{\sqrt{5}} s+\frac{2}{\sqrt{5}} t, \quad y=-\frac{2}{\sqrt{5}} s+\frac{1}{\sqrt{5}} t
$$

This substitution in $q(x, y)$ yields the diagonal quadratic form $q(s, t)=6 s^{2}+t^{2}$.

\subsection*{9.7 Minimal Polynomial}
Let $A$ be any square matrix. Let $J(A)$ denote the collection of all polynomials $f(t)$ for which $A$ is a rootthat is, for which $f(A)=0$. The set $J(A)$ is not empty, because the Cayley-Hamilton Theorem 9.1 tells us that the characteristic polynomial $\Delta_{A}(t)$ of $A$ belongs to $J(A)$. Let $m(t)$ denote the monic polynomial of lowest degree in $J(A)$. (Such a polynomial $m(t)$ exists and is unique.) We call $m(t)$ the minimal polynomial of the matrix $A$.

Remark: A polynomial $f(t) \neq 0$ is monic if its leading coefficient equals one.

The following theorem (proved in Problem 9.33) holds.

THEOREM 9.15: The minimal polynomial $m(t)$ of a matrix (linear operator) $A$ divides every polynomial that has $A$ as a zero. In particular, $m(t)$ divides the characteristic polynomial $\Delta(t)$ of $A$.

There is an even stronger relationship between $m(t)$ and $\Delta(t)$.

THEOREM 9.16: The characteristic polynomial $\Delta(t)$ and the minimal polynomial $m(t)$ of a matrix $A$ have the same irreducible factors.

This theorem (proved in Problem 9.35) does not say that $m(t)=\Delta(t)$, only that any irreducible factor of one must divide the other. In particular, because a linear factor is irreducible, $m(t)$ and $\Delta(t)$ have the same linear factors. Hence, they have the same roots. Thus, we have the following theorem.

THEOREM 9.17: A scalar $\lambda$ is an eigenvalue of the matrix $A$ if and only if $\lambda$ is a root of the minimal polynomial of $A$.

EXAMPLE 9.11 Find the minimal polynomial $m(t)$ of $A=\left[\begin{array}{rrr}2 & 2 & -5 \\ 3 & 7 & -15 \\ 1 & 2 & -4\end{array}\right]$.

First find the characteristic polynomial $\Delta(t)$ of $A$. We have

$$
\operatorname{tr}(A)=5, \quad A_{11}+A_{22}+A_{33}=2-3+8=7, \quad \text { and } \quad|A|=3
$$

Hence,

$$
\Delta(t)=t^{3}-5 t^{2}+7 t-3=(t-1)^{2}(t-3)
$$

The minimal polynomial $m(t)$ must divide $\Delta(t)$. Also, each irreducible factor of $\Delta(t)$ (i.e., $t-1$ and $t-3$ ) must also be a factor of $m(t)$. Thus, $m(t)$ is exactly one of the following:

$$
f(t)=(t-3)(t-1) \quad \text { or } \quad g(t)=(t-3)(t-1)^{2}
$$

We know, by the Cayley-Hamilton theorem, that $g(A)=\Delta(A)=0$. Hence, we need only test $f(t)$. We have

$$
f(A)=(A-I)(A-3 I)=\left[\begin{array}{rrr}
1 & 2 & -5 \\
3 & 6 & -15 \\
1 & 2 & -5
\end{array}\right]\left[\begin{array}{rrr}
-1 & 2 & -5 \\
3 & 4 & -15 \\
1 & 2 & -7
\end{array}\right]=\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $f(t)=m(t)=(t-1)(t-3)=t^{2}-4 t+3$ is the minimal polynomial of $A$.

\section*{EXAMPLE 9.12}
(a) Consider the following two $r$-square matrices, where $a \neq 0$ :

$$
J(\lambda, r)=\left[\begin{array}{cccccc}
\lambda & 1 & 0 & \ldots & 0 & 0 \\
0 & \lambda & 1 & \ldots & 0 & 0 \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots . . . \ldots \\
0 & 0 & 0 & \ldots & \lambda & 1 \\
0 & 0 & 0 & \ldots & 0 & \lambda
\end{array}\right] \quad \text { and } \quad A=\left[\begin{array}{cccccc}
\lambda & a & 0 & \ldots & 0 & 0 \\
0 & \lambda & a & \ldots & 0 & 0 \\
\ldots & \ldots \ldots \ldots \ldots \ldots \ldots \ldots . . \ldots \\
0 & 0 & 0 & \ldots & \lambda & a \\
0 & 0 & 0 & \ldots & 0 & \lambda
\end{array}\right]
$$

The first matrix, called a Jordan Block, has $\lambda$ 's on the diagonal, 1's on the superdiagonal (consisting of the entries above the diagonal entries), and 0 's elsewhere. The second matrix $A$ has $\lambda$ 's on the diagonal, $a$ 's on the superdiagonal, and 0 's elsewhere. [Thus, $A$ is a generalization of $J(\lambda, r)$.] One can show that

$$
f(t)=(t-\lambda)^{r}
$$

is both the characteristic and minimal polynomial of both $J(\lambda, r)$ and $A$.

(b) Consider an arbitrary monic polynomial:

$$
f(t)=t^{n}+a_{n-1} t^{n-1}+\cdots+a_{1} t+a_{0}
$$

Let $C(f)$ be the $n$-square matrix with 1's on the subdiagonal (consisting of the entries below the diagonal entries), the negatives of the coefficients in the last column, and 0 's elsewhere as follows:

$$
C(f)=\left[\begin{array}{ccccc}
0 & 0 & \ldots & 0 & -a_{0} \\
1 & 0 & \ldots & 0 & -a_{1} \\
0 & 1 & \ldots & 0 & -a_{2} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & \ldots & 1 & -a_{n-1}
\end{array}\right]
$$

Then $C(f)$ is called the companion matrix of the polynomial $f(t)$. Moreover, the minimal polynomial $m(t)$ and the characteristic polynomial $\Delta(t)$ of the companion matrix $C(f)$ are both equal to the original polynomial $f(t)$.

\section*{Minimal Polynomial of a Linear Operator}
The minimal polynomial $m(t)$ of a linear operator $T$ is defined to be the monic polynomial of lowest degree for which $T$ is a root. However, for any polynomial $f(t)$, we have

$$
f(T)=0 \quad \text { if and only if } \quad f(A)=0
$$

where $A$ is any matrix representation of $T$. Accordingly, $T$ and $A$ have the same minimal polynomials. Thus, the above theorems on the minimal polynomial of a matrix also hold for the minimal polynomial of a linear operator. That is, we have the following theorems.

THEOREM 9.15': $\quad$ The minimal polynomial $m(t)$ of a linear operator $T$ divides every polynomial that has $T$ as a root. In particular, $m(t)$ divides the characteristic polynomial $\Delta(t)$ of $T$.

THEOREM 9.16': The characteristic and minimal polynomials of a linear operator $T$ have the same irreducible factors.

THEOREM 9.17': A scalar $\lambda$ is an eigenvalue of a linear operator $T$ if and only if $\lambda$ is a root of the minimal polynomial $m(t)$ of $T$.

\subsection*{9.8 Characteristic and Minimal Polynomials of Block Matrices}
This section discusses the relationship of the characteristic polynomial and the minimal polynomial to certain (square) block matrices.

\section*{Characteristic Polynomial and Block Triangular Matrices}
Suppose $M$ is a block triangular matrix, say $M=\left[\begin{array}{cc}A_{1} & B \\ 0 & A_{2}\end{array}\right]$, where $A_{1}$ and $A_{2}$ are square matrices. Then $t I-M$ is also a block triangular matrix, with diagonal blocks $t I-A_{1}$ and $t I-A_{2}$. Thus,

$$
|t I-M|=\left|\begin{array}{cc}
t I-A_{1} & -B \\
0 & t I-A_{2}
\end{array}\right|=\left|t I-A_{1}\right|\left|t I-A_{2}\right|
$$

That is, the characteristic polynomial of $M$ is the product of the characteristic polynomials of the diagonal blocks $A_{1}$ and $A_{2}$.

By induction, we obtain the following useful result.

THEOREM 9.18: $\quad$ Suppose $M$ is a block triangular matrix with diagonal blocks $A_{1}, A_{2}, \ldots, A_{r}$. Then the characteristic polynomial of $M$ is the product of the characteristic polynomials of the diagonal blocks $A_{i}$; that is,

$$
\Delta_{M}(t)=\Delta_{A_{1}}(t) \Delta_{A_{2}}(t) \ldots \Delta_{A_{r}}(t)
$$

EXAMPLE 9.13 Consider the matrix $M=\left[\begin{array}{lrlrr}9 & -1 & 1 & 5 & 7 \\ 8 & - & 3 & 1 & 2 \\ 0 & 0 & - & -4 & 6 \\ 0 & 0 & 1 & -1 & 8\end{array}\right]$.

Then $M$ is a block triangular matrix with diagonal blocks $A=\left[\begin{array}{rr}9 & -1 \\ 8 & 3\end{array}\right]$ and $B=\left[\begin{array}{rr}3 & 6 \\ -1 & 8\end{array}\right]$. Here

$$
\begin{array}{llll}
\operatorname{tr}(A)=9+3=12, & \operatorname{det}(A)=27+8=35, & \text { and so } \quad \Delta_{A}(t)=t^{2}-12 t+35=(t-5)(t-7) \\
\operatorname{tr}(B)=3+8=11, & \operatorname{det}(B)=24+6=30, & \text { and so } \quad & \Delta_{B}(t)=t^{2}-11 t+30=(t-5)(t-6)
\end{array}
$$

Accordingly, the characteristic polynomial of $M$ is the product

$$
\Delta_{M}(t)=\Delta_{A}(t) \Delta_{B}(t)=(t-5)^{2}(t-6)(t-7)
$$

\section*{Minimal Polynomial and Block Diagonal Matrices}
The following theorem (proved in Problem 9.36) holds.

THEOREM 9.19: $\quad$ Suppose $M$ is a block diagonal matrix with diagonal blocks $A_{1}, A_{2}, \ldots, A_{r}$. Then the minimal polynomial of $M$ is equal to the least common multiple (LCM) of the minimal polynomials of the diagonal blocks $A_{i}$.

Remark: We emphasize that this theorem applies to block diagonal matrices, whereas the analogous Theorem 9.18 on characteristic polynomials applies to block triangular matrices.

EXAMPLE 9.14 Find the characteristic polynomal $\Delta(t)$ and the minimal polynomial $m(t)$ of the block diagonal matrix:

$$
M=\left[\begin{array}{cc:cc:c}
2 & 5 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 & 0 \\
0 & 0 & 4 & 2 & 0 \\
0 & 0 & 3 & 5 & 0 \\
\hdashline 0 & 0 & 0 & 0 & 7
\end{array}\right]=\operatorname{diag}\left(A_{1}, A_{2}, A_{3}\right), \text { where } A_{1}=\left[\begin{array}{ll}
2 & 5 \\
0 & 2
\end{array}\right], A_{2}=\left[\begin{array}{ll}
4 & 2 \\
3 & 5
\end{array}\right], A_{3}=[7]
$$

Then $\Delta(t)$ is the product of the characterization polynomials $\Delta_{1}(t), \Delta_{2}(t), \Delta_{3}(t)$ of $A_{1}, A_{2}, A_{3}$, respectively. One can show that

$$
\Delta_{1}(t)=(t-2)^{2}, \quad \Delta_{2}(t)=(t-2)(t-7), \quad \Delta_{3}(t)=t-7
$$

Thus, $\Delta(t)=(t-2)^{3}(t-7)^{2}$. [As expected, $\operatorname{deg} \Delta(t)=5$.]

The minimal polynomials $m_{1}(t), m_{2}(t), m_{3}(t)$ of the diagonal blocks $A_{1}, A_{2}, A_{3}$, respectively, are equal to the characteristic polynomials; that is,

$$
m_{1}(t)=(t-2)^{2}, \quad m_{2}(t)=(t-2)(t-7), \quad m_{3}(t)=t-7
$$

But $m(t)$ is equal to the least common multiple of $m_{1}(t), m_{2}(t), m_{3}(t)$. Thus, $m(t)=(t-2)^{2}(t-7)$.

\section*{SOLVED PROBLEMS}
\section*{Polynomials of Matrices, Characteristic Polynomials}
9.1. Let $A=\left[\begin{array}{rr}1 & -2 \\ 4 & 5\end{array}\right]$. Find $f(A)$, where\\
(a) $f(t)=t^{2}-3 t+7$,\\
(b) $f(t)=t^{2}-6 t+13$

First find $A^{2}=\left[\begin{array}{rr}1 & -2 \\ 4 & 5\end{array}\right]\left[\begin{array}{rr}1 & -2 \\ 4 & 5\end{array}\right]=\left[\begin{array}{rr}-7 & -12 \\ 24 & 17\end{array}\right]$. Then

(a) $f(A)=A^{2}-3 A+7 I=\left[\begin{array}{rr}-7 & -12 \\ 24 & 17\end{array}\right]+\left[\begin{array}{rr}-3 & 6 \\ -12 & -15\end{array}\right]+\left[\begin{array}{ll}7 & 0 \\ 0 & 7\end{array}\right]=\left[\begin{array}{rr}-3 & -6 \\ 12 & 9\end{array}\right]$

(b) $f(A)=A^{2}-6 A+13 I=\left[\begin{array}{rr}-7 & -12 \\ 24 & 17\end{array}\right]+\left[\begin{array}{rr}-6 & 12 \\ -24 & -30\end{array}\right]+\left[\begin{array}{rr}13 & 0 \\ 0 & 13\end{array}\right]=\left[\begin{array}{ll}0 & 0 \\ 0 & 0\end{array}\right]$

[Thus, $A$ is a root of $f(t)$.]

9.2. Find the characteristic polynomial $\Delta(t)$ of each of the following matrices:\\
(a) $A=\left[\begin{array}{ll}2 & 5 \\ 4 & 1\end{array}\right]$,\\
(b) $B=\left[\begin{array}{ll}7 & -3 \\ 5 & -2\end{array}\right]$\\
(c) $C=\left[\begin{array}{ll}3 & -2 \\ 9 & -3\end{array}\right]$

Use the formula $(t)=t^{2}-\operatorname{tr}(M) t+|M|$ for a $2 \times 2$ matrix $M$ :\\
(a) $\operatorname{tr}(A)=2+1=3, \quad|A|=2-20=-18$,\\
so $\quad \Delta(t)=t^{2}-3 t-18$\\
(b) $\operatorname{tr}(B)=7-2=5, \quad|B|=-14+15=1$,\\
$\Delta(t)=t^{2}-5 t+1$\\
(c) $\operatorname{tr}(C)=3-3=0$,\\
$|C|=-9+18=9, \quad$ so $\quad \Delta(t)=t^{2}+9$

9.3. Find the characteristic polynomial $\Delta(t)$ of each of the following matrices:

(a) $A=\left[\begin{array}{lll}1 & 2 & 3 \\ 3 & 0 & 4 \\ 6 & 4 & 5\end{array}\right]$, (b) $B=\left[\begin{array}{rrr}1 & 6 & -2 \\ -3 & 2 & 0 \\ 0 & 3 & -4\end{array}\right]$

Use the formula $\Delta(t)=t^{3}-\operatorname{tr}(A) t^{2}+\left(A_{11}+A_{22}+A_{33}\right) t-|A|$, where $A_{i i}$ is the cofactor of $a_{i i}$ in the $3 \times 3$ matrix $A=\left[a_{i j}\right]$.

(a) $\operatorname{tr}(A)=1+0+5=6$,

$$
\begin{gathered}
A_{11}=\left|\begin{array}{ll}
0 & 4 \\
4 & 5
\end{array}\right|=-16, \quad A_{22}=\left|\begin{array}{ll}
1 & 3 \\
6 & 5
\end{array}\right|=-13, \quad A_{33}=\left|\begin{array}{ll}
1 & 2 \\
3 & 0
\end{array}\right|=-6 \\
A_{11}+A_{22}+A_{33}=-35, \quad \text { and } \quad|A|=48+36-16-30=38
\end{gathered}
$$

Thus,

$$
\Delta(t)=t^{3}-6 t^{2}-35 t-38
$$

(b) $\operatorname{tr}(B)=1+2-4=-1$

$$
\begin{gathered}
B_{11}=\left|\begin{array}{rr}
2 & 0 \\
3 & -4
\end{array}\right|=-8, \quad B_{22}=\left|\begin{array}{rr}
1 & -2 \\
0 & -4
\end{array}\right|=-4, \quad B_{33}=\left|\begin{array}{rr}
1 & 6 \\
-3 & 2
\end{array}\right|=20 \\
B_{11}+B_{22}+B_{33}=8, \quad \text { and } \quad|B|=-8+18-72=-62
\end{gathered}
$$

Thus,

$$
\Delta(t)=t^{3}+t^{2}-8 t+62
$$

9.4. Find the characteristic polynomial $\Delta(t)$ of each of the following matrices:

(a) $A=\left[\begin{array}{rrrr}2 & 5 & 1 & 1 \\ 1 & 4 & 2 & 2 \\ 0 & 0 & 6 & -5 \\ 0 & 0 & 2 & 3\end{array}\right]$, (b) $B=\left[\begin{array}{llll}1 & 1 & 2 & 2 \\ 0 & 3 & 3 & 4 \\ 0 & 0 & 5 & 5 \\ 0 & 0 & 0 & 6\end{array}\right]$

(a) $A$ is block triangular with diagonal blocks

$$
A_{1}=\left[\begin{array}{ll}
2 & 5 \\
1 & 4
\end{array}\right] \quad \text { and } \quad A_{2}=\left[\begin{array}{rr}
6 & -5 \\
2 & 3
\end{array}\right]
$$

Thus,

$$
\Delta(t)=\Delta_{A_{1}}(t) \Delta_{A_{2}}(t)=\left(t^{2}-6 t+3\right)\left(t^{2}-9 t+28\right)
$$

(b) Because $B$ is triangular, $\Delta(t)=(t-1)(t-3)(t-5)(t-6)$.

9.5. Find the characteristic polynomial $\Delta(t)$ of each of the following linear operators:

(a) $F: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$ defined by $F(x, y)=(3 x+5 y, \quad 2 x-7 y)$.

(b) $\mathbf{D}: V \rightarrow V$ defined by $\mathbf{D}(f)=d f / d t$, where $V$ is the space of functions with basis $S=\{\sin t, \cos t\}$.

The characteristic polynomial $\Delta(t)$ of a linear operator is equal to the characteristic polynomial of any matrix $A$ that represents the linear operator.

(a) Find the matrix $A$ that represents $T$ relative to the usual basis of $\mathbf{R}^{2}$. We have

$$
A=\left[\begin{array}{rr}
3 & 5 \\
2 & -7
\end{array}\right], \quad \text { so } \quad \Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}+4 t-31
$$

(b) Find the matrix $A$ representing the differential operator $\mathbf{D}$ relative to the basis $S$. We have

$$
\begin{aligned}
\mathbf{D}(\sin t) & =\cos t \quad=0(\sin t)+1(\cos t) \\
\mathbf{D}(\cos t) & =-\sin t=-1(\sin t)+0(\cos t)
\end{aligned} \quad \text { and so } \quad A=\left[\begin{array}{rr}
0 & -1 \\
1 & 0
\end{array}\right]
$$

Therefore,

$$
\Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}+1
$$

9.6. Show that a matrix $A$ and its transpose $A^{T}$ have the same characteristic polynomial.

By the transpose operation, $(t I-A)^{T}=t I^{T}-A^{T}=t I-A^{T}$. Because a matrix and its transpose have the same determinant,

$$
\Delta_{A}(t)=|t I-A|=\left|(t I-A)^{T}\right|=\left|t I-A^{T}\right|=\Delta_{A^{T}}(t)
$$

9.7. Prove Theorem 9.1: Let $f$ and $g$ be polynomials. For any square matrix $A$ and scalar $k$,

(i) $(f+g)(A)=f(A)+g(A)$,

(ii) $(f g)(A)=f(A) g(A)$,

(iii) $(k f)(A)=k f(A)$,

(iv) $f(A) g(A)=g(A) f(A)$.

Suppose $f=a_{n} t^{n}+\cdots+a_{1} t+a_{0}$ and $g=b_{m} t^{m}+\cdots+b_{1} t+b_{0}$. Then, by definition,

$$
f(A)=a_{n} A^{n}+\cdots+a_{1} A+a_{0} I \quad \text { and } \quad g(A)=b_{m} A^{m}+\cdots+b_{1} A+b_{0} I
$$

(i) Suppose $m \leq n$ and let $b_{i}=0$ if $i>m$. Then

$$
f+g=\left(a_{n}+b_{n}\right) t^{n}+\cdots+\left(a_{1}+b_{1}\right) t+\left(a_{0}+b_{0}\right)
$$

Hence,

$$
\begin{aligned}
(f+g)(A) & =\left(a_{n}+b_{n}\right) A^{n}+\cdots+\left(a_{1}+b_{1}\right) A+\left(a_{0}+b_{0}\right) I \\
& =a_{n} A^{n}+b_{n} A^{n}+\cdots+a_{1} A+b_{1} A+a_{0} I+b_{0} I=f(A)+g(A)
\end{aligned}
$$

(ii) By definition, $f g=c_{n+m} t^{n+m}+\cdots+c_{1} t+c_{0}=\sum_{k=0}^{n+m} c_{k} t^{k}$, where

$$
c_{k}=a_{0} b_{k}+a_{1} b_{k-1}+\cdots+a_{k} b_{0}=\sum_{i=0}^{k} a_{i} b_{k-i}
$$

Hence, $(f g)(A)=\sum_{k=0}^{n+m} c_{k} A^{k}$ and

$$
f(A) g(A)=\left(\sum_{i=0}^{n} a_{i} A^{i}\right)\left(\sum_{j=0}^{m} b_{j} A^{j}\right)=\sum_{i=0}^{n} \sum_{j=0}^{m} a_{i} b_{j} A^{i+j}=\sum_{k=0}^{n+m} c_{k} A^{k}=(f g)(A)
$$

(iii) By definition, $k f=k a_{n} t^{n}+\cdots+k a_{1} t+k a_{0}$, and so

$$
(k f)(A)=k a_{n} A^{n}+\cdots+k a_{1} A+k a_{0} I=k\left(a_{n} A^{n}+\cdots+a_{1} A+a_{0} I\right)=k f(A)
$$

(iv) By (ii), $g(A) f(A)=(g f)(A)=(f g)(A)=f(A) g(A)$.

9.8. Prove the Cayley-Hamilton Theorem 9.2: Every matrix $A$ is a root of its characterstic polynomial $\Delta(t)$.

Let $A$ be an arbitrary $n$-square matrix and let $\Delta(t)$ be its characteristic polynomial, say,

$$
\Delta(t)=|t I-A|=t^{n}+a_{n-1} t^{n-1}+\cdots+a_{1} t+a_{0}
$$

Now let $B(t)$ denote the classical adjoint of the matrix $t I-A$. The elements of $B(t)$ are cofactors of the matrix $t I-A$ and hence are polynomials in $t$ of degree not exceeding $n-1$. Thus,

$$
B(t)=B_{n-1} t^{n-1}+\cdots+B_{1} t+B_{0}
$$

where the $B_{i}$ are $n$-square matrices over $K$ which are independent of $t$. By the fundamental property of the classical adjoint (Theorem 8.9), $(t I-A) B(t)=|t I-A| I$, or

$$
(t I-A)\left(B_{n-1} t^{n-1}+\cdots+B_{1} t+B_{0}\right)=\left(t^{n}+a_{n-1} t^{n-1}+\cdots+a_{1} t+a_{0}\right) I
$$

Removing the parentheses and equating corresponding powers of $t$ yields

$$
B_{n-1}=I, \quad B_{n-2}-A B_{n-1}=a_{n-1} I, \quad \ldots, \quad B_{0}-A B_{1}=a_{1} I, \quad-A B_{0}=a_{0} I
$$

Multiplying the above equations by $A^{n}, A^{n-1}, \ldots, A, I$, respectively, yields

$$
A^{n} B_{n-1}=A_{n} I, \quad A^{n-1} B_{n-2}-A^{n} B_{n-1}=a_{n-1} A^{n-1}, \quad \ldots, \quad A B_{0}-A^{2} B_{1}=a_{1} A, \quad-A B_{0}=a_{0} I
$$

Adding the above matrix equations yields 0 on the left-hand side and $\Delta(A)$ on the right-hand side; that is,

$$
0=A^{n}+a_{n-1} A^{n-1}+\cdots+a_{1} A+a_{0} I
$$

Therefore, $\Delta(A)=0$, which is the Cayley-Hamilton theorem.

\section*{Eigenvalues and Eigenvectors of $\mathbf{2} \times \mathbf{2}$ Matrices}
9.9. Let $A=\left[\begin{array}{ll}3 & -4 \\ 2 & -6\end{array}\right]$.

(a) Find all eigenvalues and corresponding eigenvectors.

(b) Find matrices $P$ and $D$ such that $P$ is nonsingular and $D=P^{-1} A P$ is diagonal.

(a) First find the characteristic polynomial $\Delta(t)$ of $A$ :

$$
\Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}+3 t-10=(t-2)(t+5)
$$

The roots $\lambda=2$ and $\lambda=-5$ of $\Delta(t)$ are the eigenvalues of $A$. We find corresponding eigenvectors.

(i) Subtract $\lambda=2$ down the diagonal of $A$ to obtain the matrix $M=A-2 I$, where the corresponding homogeneous system $M X=0$ yields the eigenvectors corresponding to $\lambda=2$. We have

$$
M=\left[\begin{array}{rr}
1 & -4 \\
2 & -8
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
x-4 y & =0 \\
2 x-8 y & =0
\end{aligned} \quad \text { or } \quad x-4 y=0
$$

The system has only one free variable, and $v_{1}=(4,1)$ is a nonzero solution. Thus, $v_{1}=(4,1)$ is an eigenvector belonging to (and spanning the eigenspace of) $\lambda=2$.

(ii) Subtract $\lambda=-5$ (or, equivalently, add 5) down the diagonal of $A$ to obtain

$$
M=\left[\begin{array}{ll}
8 & -4 \\
2 & -1
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
& 8 x-4 y=0 \\
& 2 x-y=0
\end{aligned} \quad \text { or } \quad 2 x-y=0
$$

The system has only one free variable, and $v_{2}=(1,2)$ is a nonzero solution. Thus, $v_{2}=(1,2)$ is an eigenvector belonging to $\lambda=5$.

(b) Let $P$ be the matrix whose columns are $v_{1}$ and $v_{2}$. Then

$$
P=\left[\begin{array}{ll}
4 & 1 \\
1 & 2
\end{array}\right] \quad \text { and } \quad D=P^{-1} A P=\left[\begin{array}{rr}
2 & 0 \\
0 & -5
\end{array}\right]
$$

Note that $D$ is the diagonal matrix whose diagonal entries are the eigenvalues of $A$ corresponding to the eigenvectors appearing in $P$.

Remark: Here $P$ is the change-of-basis matrix from the usual basis of $\mathbf{R}^{2}$ to the basis $S=\left\{v_{1}, v_{2}\right\}$, and $D$ is the matrix that represents (the matrix function) $A$ relative to the new basis $S$.

9.10. Let $A=\left[\begin{array}{ll}2 & 2 \\ 1 & 3\end{array}\right]$.

(a) Find all eigenvalues and corresponding eigenvectors.

(b) Find a nonsingular matrix $P$ such that $D=P^{-1} A P$ is diagonal, and $P^{-1}$.

(c) Find $A^{6}$ and $f(A)$, where $t^{4}-3 t^{3}-6 t^{2}+7 t+3$.

(d) Find a "real cube root" of $B$ - that is, a matrix $B$ such that $B^{3}=A$ and $B$ has real eigenvalues.

(a) First find the characteristic polynomial $\Delta(t)$ of $A$ :

$$
\Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}-5 t+4=(t-1)(t-4)
$$

The roots $\lambda=1$ and $\lambda=4$ of $\Delta(t)$ are the eigenvalues of $A$. We find corresponding eigenvectors.

(i) Subtract $\lambda=1$ down the diagonal of $A$ to obtain the matrix $M=A-\lambda I$, where the corresponding homogeneous system $M X=0$ yields the eigenvectors belonging to $\lambda=1$. We have

$$
M=\left[\begin{array}{ll}
1 & 2 \\
1 & 2
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
& x+2 y=0 \\
& x+2 y=0
\end{aligned} \quad \text { or } \quad x+2 y=0
$$

The system has only one independent solution; for example, $x=2, y=-1$. Thus, $v_{1}=(2,-1)$ is an eigenvector belonging to (and spanning the eigenspace of) $\lambda=1$.

(ii) Subtract $\lambda=4$ down the diagonal of $A$ to obtain

$$
M=\left[\begin{array}{rr}
-2 & 2 \\
1 & -1
\end{array}\right], \quad \text { corresponding to } \begin{aligned}
-2 x+2 y & =0 \\
x-y & =0
\end{aligned} \quad \text { or } \quad x-y=0
$$

The system has only one independent solution; for example, $x=1, y=1$. Thus, $v_{2}=(1,1)$ is an eigenvector belonging to $\lambda=4$.

(b) Let $P$ be the matrix whose columns are $v_{1}$ and $v_{2}$. Then

$$
P=\left[\begin{array}{rr}
2 & 1 \\
-1 & 1
\end{array}\right] \quad \text { and } \quad D=P^{-1} A P=\left[\begin{array}{ll}
1 & 0 \\
0 & 4
\end{array}\right], \quad \text { where } \quad P^{-1}=\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{1}{3} & \frac{2}{3}
\end{array}\right]
$$

(c) Using the diagonal factorization $A=P D P^{-1}$, and $1^{6}=1$ and $4^{6}=4096$, we get

$$
A^{6}=P D^{6} P^{-1}=\left[\begin{array}{rr}
2 & 1 \\
-1 & 1
\end{array}\right]\left[\begin{array}{rr}
1 & 0 \\
0 & 4096
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{1}{3} & \frac{2}{3}
\end{array}\right]=\left[\begin{array}{ll}
1366 & 2230 \\
1365 & 2731
\end{array}\right]
$$

Also, $f(1)=2$ and $f(4)=-1$. Hence,

$$
f(A)=\operatorname{Pf}(D) P^{-1}=\left[\begin{array}{rr}
2 & 1 \\
-1 & 1
\end{array}\right]\left[\begin{array}{rr}
2 & 0 \\
0 & -1
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{1}{3} & \frac{2}{3}
\end{array}\right]=\left[\begin{array}{rr}
1 & 2 \\
-1 & 0
\end{array}\right]
$$

(d) Here $\left[\begin{array}{cc}1 & 0 \\ 0 & \sqrt[3]{4}\end{array}\right]$ is the real cube root of $D$. Hence the real cube root of $A$ is

$$
B=P \sqrt[3]{D} P^{-1}=\left[\begin{array}{rr}
2 & 1 \\
-1 & 1
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & \sqrt[3]{4}
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{3} & -\frac{1}{3} \\
\frac{1}{3} & \frac{2}{3}
\end{array}\right]=\frac{1}{3}\left[\begin{array}{rr}
2+\sqrt[3]{4} & -2+2 \sqrt[3]{4} \\
-1+\sqrt[3]{4} & 1+2 \sqrt[3]{4}
\end{array}\right]
$$

9.11. Each of the following real matrices defines a linear transformation on $\mathbf{R}^{2}$ :\\
(a) $A=\left[\begin{array}{rr}5 & 6 \\ 3 & -2\end{array}\right]$,\\
(b) $B=\left[\begin{array}{ll}1 & -1 \\ 2 & -1\end{array}\right]$\\
(c) $C=\left[\begin{array}{rr}5 & -1 \\ 1 & 3\end{array}\right]$

Find, for each matrix, all eigenvalues and a maximum set $S$ of linearly independent eigenvectors. Which of these linear operators are diagonalizable - that is, which can be represented by a diagonal matrix?

(a) First find $\Delta(t)=t^{2}-3 t-28=(t-7)(t+4)$. The roots $\lambda=7$ and $\lambda=-4$ are the eigenvalues of $A$. We find corresponding eigenvectors.

(i) Subtract $\lambda=7$ down the diagonal of $A$ to obtain

$$
M=\left[\begin{array}{rr}
-2 & 6 \\
3 & -9
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
-2 x+6 y & =0 \\
3 x-9 y & =0
\end{aligned} \quad \text { or } \quad x-3 y=0
$$

Here $v_{1}=(3,1)$ is a nonzero solution.

(ii) Subtract $\lambda=-4$ (or add 4) down the diagonal of $A$ to obtain

$$
M=\left[\begin{array}{ll}
9 & 6 \\
3 & 2
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
& 9 x+6 y=0 \\
& 3 x+2 y=0
\end{aligned} \quad \text { or } \quad 3 x+2 y=0
$$

Here $v_{2}=(2,-3)$ is a nonzero solution.

Then $S=\left\{v_{1}, v_{2}\right\}=\{(3,1),(2,-3)\}$ is a maximal set of linearly independent eigenvectors. Because $S$ is a basis of $\mathbf{R}^{2}, A$ is diagonalizable. Using the basis $S, A$ is represented by the diagonal matrix $D=\operatorname{diag}(7,-4)$.

(b) First find the characteristic polynomial $\Delta(t)=t^{2}+1$. There are no real roots. Thus $B$, a real matrix representing a linear transformation on $\mathbf{R}^{2}$, has no eigenvalues and no eigenvectors. Hence, in particular, $B$ is not diagonalizable.\\
(c) First find $\Delta(t)=t^{2}-8 t+16=(t-4)^{2}$. Thus, $\lambda=4$ is the only eigenvalue of $C$. Subtract $\lambda=4$ down the diagonal of $C$ to obtain

$$
M=\left[\begin{array}{cc}
1 & -1 \\
1 & -1
\end{array}\right], \quad \text { corresponding to } \quad x-y=0
$$

The homogeneous system has only one independent solution; for example, $x=1, y=1$. Thus, $v=(1,1)$ is an eigenvector of $C$. Furthermore, as there are no other eigenvalues, the singleton set $S=\{v\}=\{(1,1)\}$ is a maximal set of linearly independent eigenvectors of $C$. Furthermore, because $S$ is not a basis of $\mathbf{R}^{2}, C$ is not diagonalizable.

9.12. Suppose the matrix $B$ in Problem 9.11 represents a linear operator on complex space $\mathbf{C}^{2}$. Show that, in this case, $B$ is diagonalizable by finding a basis $S$ of $\mathbf{C}^{2}$ consisting of eigenvectors of $B$.

The characteristic polynomial of $B$ is still $\Delta(t)=t^{2}+1$. As a polynomial over $\mathbf{C}, \Delta(t)$ does factor; specifically, $\Delta(t)=(t-i)(t+i)$. Thus, $\lambda=i$ and $\lambda=-i$ are the eigenvalues of $B$.

(i) Subtract $\lambda=i$ down the diagonal of $B$ to obtain the homogeneous system

$$
\begin{aligned}
(1-i) x-y & =0 \\
2 x+(-1-i) y & =0
\end{aligned} \quad \text { or } \quad(1-i) x-y=0
$$

The system has only one independent solution; for example, $x=1, y=1-i$. Thus, $v_{1}=(1,1-i)$ is an eigenvector that spans the eigenspace of $\lambda=i$.

(ii) Subtract $\lambda=-i$ (or add $i$ ) down the diagonal of $B$ to obtain the homogeneous system

$$
\begin{aligned}
(1+i) x-y & =0 \\
2 x+(-1+i) y & =0
\end{aligned} \quad \text { or } \quad(1+i) x-y=0
$$

The system has only one independent solution; for example, $x=1, y=1+i$. Thus, $v_{2}=(1,1+i)$ is an eigenvector that spans the eigenspace of $\lambda=-i$.

As a complex matrix, $B$ is diagonalizable. Specifically, $S=\left\{v_{1}, v_{2}\right\}=\{(1,1-i),(1,1+i)\}$ is a basis of $C^{2}$ consisting of eigenvectors of $B$. Using this basis $S, B$ is represented by the diagonal matrix $D=\operatorname{diag}(i,-i)$.

9.13. Let $L$ be the linear transformation on $\mathbf{R}^{2}$ that reflects each point $P$ across the line $y=k x$, where $k>0$. (See Fig. 9-1.)

(a) Show that $v_{1}=(k, 1)$ and $v_{2}=(1,-k)$ are eigenvectors of $L$.

(b) Show that $L$ is diagonalizable, and find a diagonal representation $D$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-318}
\end{center}

Figure 9-1

(a) The vector $v_{1}=(k, 1)$ lies on the line $y=k x$, and hence is left fixed by $L$; that is, $L\left(v_{1}\right)=v_{1}$. Thus, $v_{1}$ is an eigenvector of $L$ belonging to the eigenvalue $\lambda_{1}=1$.

The vector $v_{2}=(1,-k)$ is perpendicular to the line $y=k x$, and hence, $L$ reflects $v_{2}$ into its negative; that is, $L\left(v_{2}\right)=-v_{2}$. Thus, $v_{2}$ is an eigenvector of $L$ belonging to the eigenvalue $\lambda_{2}=-1$.\\
(b) Here $S=\left\{v_{1}, v_{2}\right\}$ is a basis of $\mathbf{R}^{2}$ consisting of eigenvectors of $L$. Thus, $L$ is diagonalizable, with the diagonal representation $D=\left[\begin{array}{rr}1 & 0 \\ 0 & -1\end{array}\right]$ (relative to the basis $S$ ).

\section*{Eigenvalues and Eigenvectors}
9.14. Let $A=\left[\begin{array}{rrr}4 & 1 & -1 \\ 2 & 5 & -2 \\ 1 & 1 & 2\end{array}\right]$. (a) Find all eigenvalues of $A$.

(b) Find a maximum set $S$ of linearly independent eigenvectors of $A$.

(c) Is $A$ diagonalizable? If yes, find $P$ such that $D=P^{-1} A P$ is diagonal.

(a) First find the characteristic polynomial $\Delta(t)$ of $A$. We have

$$
\operatorname{tr}(A)=4+5+2=11 \quad \text { and } \quad|A|=40-2-2+5+8-4=45
$$

Also, find each cofactor $A_{i i}$ of $a_{i i}$ in $A$ :

$$
A_{11}=\left|\begin{array}{rr}
5 & -2 \\
1 & 2
\end{array}\right|=12, \quad A_{22}=\left|\begin{array}{rr}
4 & -1 \\
1 & 2
\end{array}\right|=9, \quad A_{33}=\left|\begin{array}{ll}
4 & 1 \\
2 & 5
\end{array}\right|=18
$$

Hence,

$$
\Delta(t)=t^{3}-\operatorname{tr}(A) t^{2}+\left(A_{11}+A_{22}+A_{33}\right) t-|A|=t^{3}-11 t^{2}+39 t-45
$$

Assuming $\Delta t$ has a rational root, it must be among $\pm 1, \pm 3, \pm 5, \pm 9, \pm 15, \pm 45$. Testing, by synthetic division, we get

$$
3 \begin{array}{r}
1-11+39-45 \\
3-24+45 \\
1-8+15+0
\end{array}
$$

Thus, $t=3$ is a root of $\Delta(t)$. Also, $t-3$ is a factor and $t^{2}-8 t+15$ is a factor. Hence,

$$
\Delta(t)=(t-3)\left(t^{2}-8 t+15\right)=(t-3)(t-5)(t-3)=(t-3)^{2}(t-5)
$$

Accordingly, $\lambda=3$ and $\lambda=5$ are eigenvalues of $A$.

(b) Find linearly independent eigenvectors for each eigenvalue of $A$.

(i) Subtract $\lambda=3$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ccc}
1 & 1 & -1 \\
2 & 2 & -2 \\
1 & 1 & -1
\end{array}\right], \quad \text { corresponding to } \quad x+y-z=0
$$

Here $u=(1,-1,0)$ and $v=(1,0,1)$ are linearly independent solutions.

(ii) Subtract $\lambda=5$ down the diagonal of $A$ to obtain the matrix

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-319}
\end{center}

Only $z$ is a free variable. Here $w=(1,2,1)$ is a solution.

Thus, $S=\{u, v, w\}=\{(1,-1,0),(1,0,1),(1,2,1)\}$ is a maximal set of linearly independent eigenvectors of $A$.

Remark: The vectors $u$ and $v$ were chosen so that they were independent solutions of the system $x+y-z=0$. On the other hand, $w$ is automatically independent of $u$ and $v$ because $w$ belongs to a different eigenvalue of $A$. Thus, the three vectors are linearly independent.\\
(c) $A$ is diagonalizable, because it has three linearly independent eigenvectors. Let $P$ be the matrix with columns $u, v, w$. Then

$$
P=\left[\begin{array}{rrr}
1 & 1 & 1 \\
-1 & 0 & 2 \\
0 & 1 & 1
\end{array}\right] \quad \text { and } \quad D=P^{-1} A P=\left[\begin{array}{lll}
3 & & \\
& 3 & \\
& & 5
\end{array}\right]
$$

9.15. Repeat Problem 9.14 for the matrix $B=\left[\begin{array}{lll}3 & -1 & 1 \\ 7 & -5 & 1 \\ 6 & -6 & 2\end{array}\right]$.

(a) First find the characteristic polynomial $\Delta(t)$ of $B$. We have

$\operatorname{tr}(B)=0, \quad|B|=-16, \quad B_{11}=-4, \quad B_{22}=0, \quad B_{33}=-8, \quad$ so $\quad \sum_{i} B_{i i}=-12$

Therefore, $\Delta(t)=t^{3}-12 t+16=(t-2)^{2}(t+4)$. Thus, $\lambda_{1}=2$ and $\lambda_{2}=-4$ are the eigenvalues of $B$.

(b) Find a basis for the eigenspace of each eigenvalue of $B$.

(i) Subtract $\lambda_{1}=2$ down the diagonal of $B$ to obtain

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-320}
\end{center}

The system has only one independent solution; for example, $x=1, y=1, z=0$. Thus, $u=(1,1,0)$ forms a basis for the eigenspace of $\lambda_{1}=2$.

(ii) Subtract $\lambda_{2}=-4$ (or add 4) down the diagonal of $B$ to obtain

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-320(1)}
\end{center}

The system has only one independent solution; for example, $x=0, y=1, z=1$. Thus, $v=(0,1,1)$ forms a basis for the eigenspace of $\lambda_{2}=-4$.

Thus $S=\{u, v\}$ is a maximal set of linearly independent eigenvectors of $B$.

(c) Because $B$ has at most two linearly independent eigenvectors, $B$ is not similar to a diagonal matrix; that is, $B$ is not diagonalizable.

9.16. Find the algebraic and geometric multiplicities of the eigenvalue $\lambda_{1}=2$ of the matrix $B$ in Problem 9.15.

The algebraic multiplicity of $\lambda_{1}=2$ is 2 , because $t-2$ appears with exponent 2 in $\Delta(t)$. However, the geometric multiplicity of $\lambda_{1}=2$ is 1 , because $\operatorname{dim} E_{\lambda_{1}}=1$ (where $E_{\lambda_{1}}$ is the eigenspace of $\lambda_{1}$ ).

9.17. Let $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be defined by $T(x, y, z)=(2 x+y-2 z, \quad 2 x+3 y-4 z, \quad x+y-z)$. Find all eigenvalues of $T$, and find a basis of each eigenspace. Is $T$ diagonalizable? If so, find the basis $S$ of $\mathbf{R}^{3}$ that diagonalizes $T$, and find its diagonal representation $D$.

First find the matrix $A$ that represents $T$ relative to the usual basis of $\mathbf{R}^{3}$ by writing down the coefficients of $x, y, z$ as rows, and then find the characteristic polynomial of $A$ (and $T$ ). We have

$$
A=[T]=\left[\begin{array}{ccc}
2 & 1 & -2 \\
2 & 3 & -4 \\
1 & 1 & -1
\end{array}\right] \quad \text { and } \quad \begin{gathered}
\operatorname{tr}(A)=4, \quad|A|=2 \\
A_{11}=1, \quad A_{22}=0, \quad A_{33}=4 \\
\sum_{i} A_{i i}=5
\end{gathered}
$$

Therefore, $\Delta(t)=t^{3}-4 t^{2}+5 t-2=(t-1)^{2}(t-2)$, and so $\lambda=1$ and $\lambda=2$ are the eigenvalues of $A$ (and $T$ ). We next find linearly independent eigenvectors for each eigenvalue of $A$.\\
(i) Subtract $\lambda=1$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ccc}
1 & 1 & -2 \\
2 & 2 & -4 \\
1 & 1 & -2
\end{array}\right], \quad \text { corresponding to } \quad x+y-2 z=0
$$

Here $y$ and $z$ are free variables, and so there are two linearly independent eigenvectors belonging to $\lambda=1$. For example, $u=(1,-1,0)$ and $v=(2,0,1)$ are two such eigenvectors.

(ii) Subtract $\lambda=2$ down the diagonal of $A$ to obtain

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-321}
\end{center}

Only $z$ is a free variable. Here $w=(1,2,1)$ is a solution.

Thus, $T$ is diagonalizable, because it has three independent eigenvectors. Specifically, choosing

$$
S=\{u, v, w\}=\{(1,-1,0), \quad(2,0,1), \quad(1,2,1)\}
$$

as a basis, $T$ is represented by the diagonal matrix $D=\operatorname{diag}(1,1,2)$.

9.18. Prove the following for a linear operator (matrix) $T$ :

(a) The scalar 0 is an eigenvalue of $T$ if and only if $T$ is singular.

(b) If $\lambda$ is an eigenvalue of $T$, where $T$ is invertible, then $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.

(a) We have that 0 is an eigenvalue of $T$ if and only if there is a vector $v \neq 0$ such that $T(v)=0 v$ - that is, if and only if $T$ is singular.

(b) Because $T$ is invertible, it is nonsingular; hence, by (a), $\lambda \neq 0$. By definition of an eigenvalue, there exists $v \neq 0$ such that $T(v)=\lambda v$. Applying $T^{-1}$ to both sides, we obtain

$$
v=T^{-1}(\lambda v)=\lambda T^{-1}(v), \quad \text { and so } \quad T^{-1}(v)=\lambda^{-1} v
$$

Therefore, $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.

9.19. Let $\lambda$ be an eigenvalue of a linear operator $T: V \rightarrow V$, and let $E_{\lambda}$ consists of all the eigenvectors belonging to $\lambda$ (called the eigenspace of $\lambda$ ). Prove that $E_{\lambda}$ is a subspace of $V$. That is, prove

(a) If $u \in E_{\lambda}$, then $k u \in E_{\lambda}$ for any scalar $k$. (b) If $u, v, \in E_{\lambda}$, then $u+v \in E_{\lambda}$.

(a) Because $u \in E_{\lambda}$, we have $T(u)=\lambda u$. Then $T(k u)=k T(u)=k(\lambda u)=\lambda(k u)$, and so $k u \in E_{\lambda}$.

(We view the zero vector $0 \in V$ as an "eigenvector" of $\lambda$ in order for $E_{\lambda}$ to be a subspace of $V$.)

(b) As $u, v \in E_{\lambda}$, we have $T(u)=\lambda u$ and $T(v)=\lambda v$. Then

$T(u+v)=T(u)+T(v)=\lambda u+\lambda v=\lambda(u+v)$, and so $u+v \in E_{\lambda}$

9.20. Prove Theorem 9.6: The following are equivalent: (i) The scalar $\lambda$ is an eigenvalue of $A$.

(ii) The matrix $\lambda I-A$ is singular.

(iii) The scalar $\lambda$ is a root of the characteristic polynomial $\Delta(t)$ of $A$.

The scalar $\lambda$ is an eigenvalue of $A$ if and only if there exists a nonzero vector $v$ such that

$$
A v=\lambda v \quad \text { or } \quad(\lambda I) v-A v=0 \quad \text { or } \quad(\lambda I-A) v=0
$$

or $\lambda I-A$ is singular. In such a case, $\lambda$ is a root of $\Delta(t)=|t I-A|$. Also, $v$ is in the eigenspace $E_{\lambda}$ of $\lambda$ if and only if the above relations hold. Hence, $v$ is a solution of $(\lambda I-A) X=0$.

9.21. Prove Theorem 9.8': Suppose $v_{1}, v_{2}, \ldots, v_{n}$ are nonzero eigenvectors of $T$ belonging to distinct eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Then $v_{1}, v_{2}, \ldots, v_{n}$ are linearly independent.

Suppose the theorem is not true. Let $v_{1}, v_{2}, \ldots, v_{s}$ be a minimal set of vectors for which the theorem is not true. We have $s>1$, because $v_{1} \neq 0$. Also, by the minimality condition, $v_{2}, \ldots, v_{s}$ are linearly independent. Thus, $v_{1}$ is a linear combination of $v_{2}, \ldots, v_{s}$, say,


\begin{equation*}
v_{1}=a_{2} v_{2}+a_{3} v_{3}+\cdots+a_{s} v_{s} \tag{1}
\end{equation*}


(where some $a_{k} \neq 0$ ). Applying $T$ to (1) and using the linearity of $T$ yields


\begin{equation*}
T\left(v_{1}\right)=T\left(a_{2} v_{2}+a_{3} v_{3}+\cdots+a_{s} v_{s}\right)=a_{2} T\left(v_{2}\right)+a_{3} T\left(v_{3}\right)+\cdots+a_{s} T\left(v_{s}\right) \tag{2}
\end{equation*}


Because $v_{j}$ is an eigenvector of $T$ belonging to $\lambda_{j}$, we have $T\left(v_{j}\right)=\lambda_{j} v_{j}$. Substituting in (2) yields


\begin{equation*}
\lambda_{1} v_{1}=a_{2} \lambda_{2} v_{2}+a_{3} \lambda_{3} v_{3}+\cdots+a_{s} \lambda_{s} v_{s} \tag{3}
\end{equation*}


Multiplying (1) by $\lambda_{1}$ yields


\begin{equation*}
\lambda_{1} v_{1}=a_{2} \lambda_{1} v_{2}+a_{3} \lambda_{1} v_{3}+\cdots+a_{s} \lambda_{1} v_{s} \tag{4}
\end{equation*}


Setting the right-hand sides of (3) and (4) equal to each other, or subtracting (3) from (4) yields


\begin{equation*}
a_{2}\left(\lambda_{1}-\lambda_{2}\right) v_{2}+a_{3}\left(\lambda_{1}-\lambda_{3}\right) v_{3}+\cdots+a_{s}\left(\lambda_{1}-\lambda_{s}\right) v_{s}=0 \tag{5}
\end{equation*}


Because $v_{2}, v_{3}, \ldots, v_{s}$ are linearly independent, the coefficients in (5) must all be zero. That is,

$$
a_{2}\left(\lambda_{1}-\lambda_{2}\right)=0, \quad a_{3}\left(\lambda_{1}-\lambda_{3}\right)=0, \quad \ldots, \quad a_{s}\left(\lambda_{1}-\lambda_{s}\right)=0
$$

However, the $\lambda_{i}$ are distinct. Hence $\lambda_{1}-\lambda_{j} \neq 0$ for $j>1$. Hence, $a_{2}=0, a_{3}=0, \ldots, a_{s}=0$. This contradicts the fact that some $a_{k} \neq 0$. The theorem is proved.

9.22. Prove Theorem 9.9. Suppose $\Delta(t)=\left(t-a_{1}\right)\left(t-a_{2}\right) \ldots\left(t-a_{n}\right)$ is the characteristic polynomial of an $n$-square matrix $A$, and suppose the $n$ roots $a_{i}$ are distinct. Then $A$ is similar to the diagonal matrix $D=\operatorname{diag}\left(a_{1}, a_{2}, \ldots, a_{n}\right)$.

Let $v_{1}, v_{2}, \ldots, v_{n}$ be (nonzero) eigenvectors corresponding to the eigenvalues $a_{i}$. Then the $n$ eigenvectors $v_{i}$ are linearly independent (Theorem 9.8), and hence form a basis of $K^{n}$. Accordingly, $A$ is diagonalizable (i.e., $A$ is similar to a diagonal matrix $D$ ), and the diagonal elements of $D$ are the eigenvalues $a_{i}$.

9.23. Prove Theorem 9.10': The geometric multiplicity of an eigenvalue $\lambda$ of $T$ does not exceed its algebraic multiplicity.

Suppose the geometric multiplicity of $\lambda$ is $r$. Then its eigenspace $E_{\lambda}$ contains $r$ linearly independent eigenvectors $v_{1}, \ldots, v_{r}$. Extend the set $\left\{v_{i}\right\}$ to a basis of $V$, say, $\left\{v_{i}, \ldots, v_{r}, w_{1}, \ldots, w_{s}\right\}$. We have

$$
\begin{aligned}
& T\left(v_{1}\right)=\lambda v_{1}, \quad T\left(v_{2}\right)=\lambda v_{2}, \quad \ldots, \quad T\left(v_{r}\right)=\lambda v_{r} \\
& T\left(w_{1}\right)=a_{11} v_{1}+\cdots+a_{1 r} v_{r}+b_{11} w_{1}+\cdots+b_{1 s} w_{s} \\
& T\left(w_{2}\right)=a_{21} v_{1}+\cdots+a_{2 r} v_{r}+b_{21} w_{1}+\cdots+b_{2 s} w_{s} \\
& T\left(w_{s}\right)=a_{s 1} v_{1}+\cdots+a_{s r} v_{r}+b_{s 1} w_{1}+\cdots+b_{s s} w_{s}
\end{aligned}
$$

Then $M=\left[\begin{array}{cc}\lambda I_{r} & A \\ 0 & B\end{array}\right]$ is the matrix of $T$ in the above basis, where $A=\left[a_{i j}\right]^{T}$ and $B=\left[b_{i j}\right]^{T}$.

Because $M$ is block diagonal, the characteristic polynomial $(t-\lambda)^{r}$ of the block $\lambda I_{r}$ must divide the characteristic polynomial of $M$ and hence of $T$. Thus, the algebraic multiplicity of $\lambda$ for $T$ is at least $r$, as required.

\section*{Diagonalizing Real Symmetric Matrices and Quadratic Forms}
9.24. Let $A=\left[\begin{array}{rr}7 & 3 \\ 3 & -1\end{array}\right]$. Find an orthogonal matrix $P$ such that $D=P^{-1} A P$ is diagonal.

First find the characteristic polynomial $\Delta(t)$ of $A$. We have

$$
\Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}-6 t-16=(t-8)(t+2)
$$

Thus, the eigenvalues of $A$ are $\lambda=8$ and $\lambda=-2$. We next find corresponding eigenvectors.

Subtract $\lambda=8$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{rr}
-1 & 3 \\
3 & -9
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
-x+3 y & =0 \\
3 x-9 y & =0
\end{aligned} \quad \text { or } \quad x-3 y=0
$$

A nonzero solution is $u_{1}=(3,1)$.

Subtract $\lambda=-2$ (or add 2) down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ll}
9 & 3 \\
3 & 1
\end{array}\right], \quad \text { corresponding to } \quad \begin{aligned}
& 9 x+3 y=0 \\
& 3 x+y=0
\end{aligned} \quad \text { or } \quad 3 x+y=0
$$

A nonzero solution is $u_{2}=(1,-3)$.

As expected, because $A$ is symmetric, the eigenvectors $u_{1}$ and $u_{2}$ are orthogonal. Normalize $u_{1}$ and $u_{2}$ to obtain, respectively, the unit vectors

$$
\hat{u}_{1}=(3 / \sqrt{10}, 1 / \sqrt{10}) \quad \text { and } \quad \hat{u}_{2}=(1 / \sqrt{10},-3 / \sqrt{10})
$$

Finally, let $P$ be the matrix whose columns are the unit vectors $\hat{u}_{1}$ and $\hat{u}_{2}$, respectively. Then

$$
P=\left[\begin{array}{rr}
3 / \sqrt{10} & 1 / \sqrt{10} \\
1 / \sqrt{10} & -3 / \sqrt{10}
\end{array}\right] \quad \text { and } \quad D=P^{-1} A P=\left[\begin{array}{rr}
8 & 0 \\
0 & -2
\end{array}\right]
$$

As expected, the diagonal entries in $D$ are the eigenvalues of $A$.

9.25. Let $B=\left[\begin{array}{rrr}11 & -8 & 4 \\ -8 & -1 & -2 \\ 4 & -2 & -4\end{array}\right]$. (a) Find all eigenvalues of $B$.

(b) Find a maximal set $S$ of nonzero orthogonal eigenvectors of $B$.

(c) Find an orthogonal matrix $P$ such that $D=P^{-1} B P$ is diagonal.

(a) First find the characteristic polynomial of $B$. We have

$$
\operatorname{tr}(B)=6, \quad|B|=400, \quad B_{11}=0, \quad B_{22}=-60, \quad B_{33}=-75, \quad \text { so } \quad \sum_{i} B_{i i}=-135
$$

Hence, $\Delta(t)=t^{3}-6 t^{2}-135 t-400$. If $\Delta(t)$ has an integer root it must divide 400 . Testing $t=-5$, by synthetic division, yields

$$
-5 \left\lvert\, \begin{array}{r}
1-6-135-400 \\
-5+55+400
\end{array}\right.
$$

Thus, $t+5$ is a factor of $\Delta(t)$, and $t^{2}-11 t-80$ is a factor. Thus,

$$
\Delta(t)=(t+5)\left(t^{2}-11 t-80\right)=(t+5)^{2}(t-16)
$$

The eigenvalues of $B$ are $\lambda=-5$ (multiplicity 2), and $\lambda=16$ (multiplicity 1).

(b) Find an orthogonal basis for each eigenspace. Subtract $\lambda=-5$ (or, add 5) down the diagonal of $B$ to obtain the homogeneous system

$$
16 x-8 y+4 z=0, \quad-8 x+4 y-2 z=0, \quad 4 x-2 y+z=0
$$

That is, $4 x-2 y+z=0$. The system has two independent solutions. One solution is $v_{1}=(0,1,2)$. We seek a second solution $v_{2}=(a, b, c)$, which is orthogonal to $v_{1}$, such that

$$
4 a-2 b+c=0, \quad \text { and also } \quad b-2 c=0
$$

One such solution is $v_{2}=(-5,-8,4)$.

Subtract $\lambda=16$ down the diagonal of $B$ to obtain the homogeneous system

$$
-5 x-8 y+4 z=0, \quad-8 x-17 y-2 z=0, \quad 4 x-2 y-20 z=0
$$

This system yields a nonzero solution $v_{3}=(4,-2,1)$. (As expected from Theorem 9.13, the eigenvector $v_{3}$ is orthogonal to $v_{1}$ and $v_{2}$.)

Then $v_{1}, v_{2}, v_{3}$ form a maximal set of nonzero orthogonal eigenvectors of $B$.

(c) Normalize $v_{1}, v_{2}, v_{3}$ to obtain the orthonormal basis:

$$
\hat{v}_{1}=v_{1} / \sqrt{5}, \quad \hat{v}_{2}=v_{2} / \sqrt{105}, \quad \hat{v}_{3}=v_{3} / \sqrt{21}
$$

Then $P$ is the matrix whose columns are $\hat{v}_{1}, \hat{v}_{2}, \hat{v}_{3}$. Thus,

$$
P=\left[\begin{array}{crr}
0 & -5 / \sqrt{105} & 4 / \sqrt{21} \\
1 / \sqrt{5} & -8 / \sqrt{105} & -2 / \sqrt{21} \\
2 / \sqrt{5} & 4 / \sqrt{105} & 1 / \sqrt{21}
\end{array}\right] \quad \text { and } \quad D=P^{-1} B P=\left[\begin{array}{lll}
-5 & & \\
& -5 & \\
& & 16
\end{array}\right]
$$

9.26. Let $q(x, y)=x^{2}+6 x y-7 y^{2}$. Find an orthogonal substitution that diagonalizes $q$.

Find the symmetric matrix $A$ that represents $q$ and its characteristic polynomial $\Delta(t)$. We have

$$
A=\left[\begin{array}{rr}
1 & 3 \\
3 & -7
\end{array}\right] \quad \text { and } \quad \Delta(t)=t^{2}+6 t-16=(t-2)(t+8)
$$

The eigenvalues of $A$ are $\lambda=2$ and $\lambda=-8$. Thus, using $s$ and $t$ as new variables, a diagonal form of $q$ is

$$
q(s, t)=2 s^{2}-8 t^{2}
$$

The corresponding orthogonal substitution is obtained by finding an orthogonal set of eigenvectors of $A$.

(i) Subtract $\lambda=2$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{rr}
-1 & 3 \\
3 & -9
\end{array}\right], \quad \text { corresponding to } \quad \begin{array}{r}
-x+3 y=0 \\
3 x-9 y=0
\end{array} \quad \text { or } \quad-x+3 y=0
$$

A nonzero solution is $u_{1}=(3,1)$.

(ii) Subtract $\lambda=-8$ (or add 8 ) down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ll}
9 & 3 \\
3 & 1
\end{array}\right], \quad \text { corresponding to } \quad \begin{array}{r}
9 x+3 y=0 \\
3 x+y=0
\end{array} \quad \text { or } \quad 3 x+y=0
$$

A nonzero solution is $u_{2}=(-1,3)$.

As expected, because $A$ is symmetric, the eigenvectors $u_{1}$ and $u_{2}$ are orthogonal.

Now normalize $u_{1}$ and $u_{2}$ to obtain, respectively, the unit vectors

$$
\hat{u}_{1}=(3 / \sqrt{10}, 1 / \sqrt{10}) \quad \text { and } \quad \hat{u}_{2}=(-1 / \sqrt{10}, 3 / \sqrt{10}) \text {. }
$$

Finally, let $P$ be the matrix whose columns are the unit vectors $\hat{u}_{1}$ and $\hat{u}_{2}$, respectively, and then $[x, y]^{T}=P[s, t]^{T}$ is the required orthogonal change of coordinates. That is,

$$
\left.P=\left\lvert\, \begin{array}{rr}
3 / \sqrt{10} & -1 / \sqrt{10} \\
1 / \sqrt{10} & 3 / \sqrt{10}
\end{array}\right.\right] \quad \text { and } \quad x=\frac{3 s-t}{\sqrt{10}}, \quad y=\frac{s+3 t}{\sqrt{10}}
$$

One can also express $s$ and $t$ in terms of $x$ and $y$ by using $P^{-1}=P^{T}$. That is,

$$
s=\frac{3 x+y}{\sqrt{10}}, \quad t=\frac{-x+3 t}{\sqrt{10}}
$$

\section*{Minimal Polynomial}
9.27. Let $A=\left[\begin{array}{lll}4 & -2 & 2 \\ 6 & -3 & 4 \\ 3 & -2 & 3\end{array}\right]$ and $B=\left[\begin{array}{lll}3 & -2 & 2 \\ 4 & -4 & 6 \\ 2 & -3 & 5\end{array}\right]$. The characteristic polynomial of both matrices is $\Delta(t)=(t-2)(t-1)^{2}$. Find the minimal polynomial $m(t)$ of each matrix.

The minimal polynomial $m(t)$ must divide $\Delta(t)$. Also, each factor of $\Delta(t)$ (i.e., $t-2$ and $t-1$ ) must also be a factor of $m(t)$. Thus, $m(t)$ must be exactly one of the following:

$$
f(t)=(t-2)(t-1) \quad \text { or } \quad g(t)=(t-2)(t-1)^{2}
$$

(a) By the Cayley-Hamilton theorem, $g(A)=\Delta(A)=0$, so we need only test $f(t)$. We have

$$
f(A)=(A-2 I)(A-I)=\left[\begin{array}{lll}
2 & -2 & 2 \\
6 & -5 & 4 \\
3 & -2 & 1
\end{array}\right]\left[\begin{array}{lll}
3 & -2 & 2 \\
6 & -4 & 4 \\
3 & -2 & 2
\end{array}\right]=\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]
$$

Thus, $m(t)=f(t)=(t-2)(t-1)=t^{2}-3 t+2$ is the minimal polynomial of $A$.

(b) Again $g(B)=\Delta(B)=0$, so we need only test $f(t)$. We get

$$
f(B)=(B-2 I)(B-I)=\left[\begin{array}{lll}
1 & -2 & 2 \\
4 & -6 & 6 \\
2 & -3 & 3
\end{array}\right]\left[\begin{array}{lll}
2 & -2 & 2 \\
4 & -5 & 6 \\
2 & -3 & 4
\end{array}\right]=\left[\begin{array}{lll}
-2 & 2 & -2 \\
-4 & 4 & -4 \\
-2 & 2 & -2
\end{array}\right] \neq 0
$$

Thus, $m(t) \neq f(t)$. Accordingly, $m(t)=g(t)=(t-2)(t-1)^{2}$ is the minimal polynomial of $B$. [We emphasize that we do not need to compute $g(B)$; we know $g(B)=0$ from the Cayley-Hamilton theorem.]

9.28. Find the minimal polynomial $m(t)$ of each of the following matrices:

(a) $A=\left[\begin{array}{ll}5 & 1 \\ 3 & 7\end{array}\right]$, (b) $B=\left[\begin{array}{lll}1 & 2 & 3 \\ 0 & 2 & 3 \\ 0 & 0 & 3\end{array}\right]$, (c) $C=\left[\begin{array}{rr}4 & -1 \\ 1 & 2\end{array}\right]$

(a) The characteristic polynomial of $A$ is $\Delta(t)=t^{2}-12 t+32=(t-4)(t-8)$. Because $\Delta(t)$ has distinct factors, the minimal polynomial $m(t)=\Delta(t)=t^{2}-12 t+32$.

(b) Because $B$ is triangular, its eigenvalues are the diagonal elements $1,2,3$; and so its characteristic polynomial is $\Delta(t)=(t-1)(t-2)(t-3)$. Because $\Delta(t)$ has distinct factors, $m(t)=\Delta(t)$.

(c) The characteristic polynomial of $C$ is $\Delta(t)=t^{2}-6 t+9=(t-3)^{2}$. Hence the minimal polynomial of $C$ is $f(t)=t-3$ or $g(t)=(t-3)^{2}$. However, $f(C) \neq 0$; that is, $C-3 I \neq 0$. Hence,

$$
m(t)=g(t)=\Delta(t)=(t-3)^{2}
$$

9.29. Suppose $S=\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is a basis of $V$, and suppose $F$ and $G$ are linear operators on $V$ such that $[F]$ has 0 's on and below the diagonal, and $[G]$ has $a \neq 0$ on the superdiagonal and 0 's elsewhere. That is,

$$
[F]=\left[\begin{array}{ccccc}
0 & a_{21} & a_{31} & \ldots & a_{n 1} \\
0 & 0 & a_{32} & \ldots & a_{n 2} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & 0 & \ldots & a_{n, n-1} \\
0 & 0 & 0 & \ldots & 0
\end{array}\right], \quad[G]=\left[\begin{array}{ccccc}
0 & a & 0 & \ldots & 0 \\
0 & 0 & a & \ldots & 0 \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & 0 & \ldots & a \\
0 & 0 & 0 & \ldots & 0
\end{array}\right]
$$

Show that (a) $F^{n}=0$, (b) $G^{n-1} \neq 0$, but $G^{n}=0$. (These conditions also hold for $[F]$ and $[G]$.)

(a) We have $F\left(u_{1}\right)=0$ and, for $r>1, F\left(u_{r}\right)$ is a linear combination of vectors preceding $u_{r}$ in $S$. That is,

$$
F\left(u_{r}\right)=a_{r 1} u_{1}+a_{r 2} u_{2}+\cdots+a_{r, r-1} u_{r-1}
$$

Hence, $F^{2}\left(u_{r}\right)=F\left(F\left(u_{r}\right)\right)$ is a linear combination of vectors preceding $u_{r-1}$, and so on. Hence, $F^{r}\left(u_{r}\right)=0$ for each $r$. Thus, for each $r, F^{n}\left(u_{r}\right)=F^{n-r}(0)=0$, and so $F^{n}=0$, as claimed.

(b) We have $G\left(u_{1}\right)=0$ and, for each $k>1, G\left(u_{k}\right)=a u_{k-1}$. Hence, $G^{r}\left(u_{k}\right)=a^{r} u_{k-r}$ for $r<k$. Because $a \neq 0$, $a^{n-1} \neq 0$. Therefore, $G^{n-1}\left(u_{n}\right)=a^{n-1} u_{1} \neq 0$, and so $G^{n-1} \neq 0$. On the other hand, by (a), $G^{n}=0$.

9.30. Let $B$ be the matrix in Example 9.12(a) that has 1's on the diagonal, $a$ 's on the superdiagonal, where $a \neq 0$, and 0 's elsewhere. Show that $f(t)=(t-\lambda)^{n}$ is both the characteristic polynomial $\Delta(t)$ and the minimum polynomial $m(t)$ of $A$.

Because $A$ is triangular with $\lambda$ 's on the diagonal, $\Delta(t)=f(t)=(t-\lambda)^{n}$ is its characteristic polynomial. Thus, $m(t)$ is a power of $t-\lambda$. By Problem 9.29, $(A-\lambda I)^{r-1} \neq 0$. Hence, $m(t)=\Delta(t)=(t-\lambda)^{n}$.

9.31. Find the characteristic polynomial $\Delta(t)$ and minimal polynomial $m(t)$ of each matrix:

(a) $M=\left[\begin{array}{lllll}4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 1 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & 4 & 1 \\ 0 & 0 & 0 & 0 & 4\end{array}\right]$, (b) $\quad M^{\prime}=\left[\begin{array}{rrrr}2 & 7 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & -2 & 4\end{array}\right]$

(a) $M$ is block diagonal with diagonal blocks

$$
A=\left[\begin{array}{lll}
4 & 1 & 0 \\
0 & 4 & 1 \\
0 & 0 & 4
\end{array}\right] \quad \text { and } \quad B=\left[\begin{array}{ll}
4 & 1 \\
0 & 4
\end{array}\right]
$$

The characteristic and minimal polynomial of $A$ is $f(t)=(t-4)^{3}$ and the characteristic and minimal polynomial of $B$ is $g(t)=(t-4)^{2}$. Then

$$
\Delta(t)=f(t) g(t)=(t-4)^{5} \quad \text { but } \quad m(t)=\operatorname{LCM}[f(t), g(t)]=(t-4)^{3}
$$

(where LCM means least common multiple). We emphasize that the exponent in $m(t)$ is the size of the largest block.

(b) Here $M^{\prime}$ is block diagonal with diagonal blocks $A^{\prime}=\left[\begin{array}{ll}2 & 7 \\ 0 & 2\end{array}\right]$ and $\quad B^{\prime}=\left[\begin{array}{rr}1 & 1 \\ -2 & 4\end{array}\right]$ The characteristic and minimal polynomial of $A^{\prime}$ is $f(t)=(t-2)^{2}$. The characteristic polynomial of $B^{\prime}$ is $g(t)=t^{2}-5 t+6=(t-2)(t-3)$, which has distinct factors. Hence, $g(t)$ is also the minimal polynomial of $B$. Accordingly,

$$
\Delta(t)=f(t) g(t)=(t-2)^{3}(t-3) \quad \text { but } \quad m(t)=\operatorname{LCM}[f(t), g(t)]=(t-2)^{2}(t-3)
$$

9.32. Find a matrix $A$ whose minimal polynomial is $f(t)=t^{3}-8 t^{2}+5 t+7$.

Simply let $A=\left[\begin{array}{rrr}0 & 0 & -7 \\ 1 & 0 & -5 \\ 0 & 1 & 8\end{array}\right]$, the companion matrix of $f(t)$ [defined in Example 9.12(b)].

9.33. Prove Theorem 9.15: The minimal polynomial $m(t)$ of a matrix (linear operator) $A$ divides every polynomial that has $A$ as a zero. In particular (by the Cayley-Hamilton theorem), $m(t)$ divides the characteristic polynomial $\Delta(t)$ of $A$.

Suppose $f(t)$ is a polynomial for which $f(A)=0$. By the division algorithm, there exist polynomials $q(t)$ and $r(t)$ for which $f(t)=m(t) q(t)+r(t)$ and $r(t)=0$ or $\operatorname{deg} r(t)<\operatorname{deg} m(t)$. Substituting $t=A$ in this equation, and using that $f(A)=0$ and $m(A)=0$, we obtain $r(A)=0$. If $r(t) \neq 0$, then $r(t)$ is a polynomial of degree less than $m(t)$ that has $A$ as a zero. This contradicts the definition of the minimal polynomial. Thus, $r(t)=0$, and so $f(t)=m(t) q(t)$; that is, $m(t)$ divides $f(t)$.

9.34. Let $m(t)$ be the minimal polynomial of an $n$-square matrix $A$. Prove that the characteristic polynomial $\Delta(t)$ of $A$ divides $[m(t)]^{n}$.

Suppose $m(t)=t^{r}+c_{1} t^{r-1}+\cdots+c_{r-1} t+c_{r}$. Define matrices $B_{j}$ as follows:

$$
\begin{aligned}
& B_{0}=I \quad \text { so } \quad I=B_{0} \\
& B_{1}=A+c_{1} I \quad \text { so } \quad c_{1} I=B_{1}-A=B_{1}-A B_{0} \\
& B_{2}=A^{2}+c_{1} A+c_{2} I \quad \text { so } \quad c_{2} I=B_{2}-A\left(A+c_{1} I\right)=B_{2}-A B_{1} \\
& B_{r-1}=A^{r-1}+c_{1} A^{r-2}+\cdots+c_{r-1} I \quad \text { so } \quad c_{r-1} I=B_{r-1}-A B_{r-2}
\end{aligned}
$$

Then

Set

$$
\begin{gathered}
-A B_{r-1}=c_{r} I-\left(A^{r}+c_{1} A^{r-1}+\cdots+c_{r-1} A+c_{r} I\right)=c_{r} I-m(A)=c_{r} I \\
B(t)=t^{r-1} B_{0}+t^{r-2} B_{1}+\cdots+t B_{r-2}+B_{r-1}
\end{gathered}
$$

Then

$$
\begin{aligned}
(t I-A) B(t) & =\left(t^{r} B_{0}+t^{r-1} B_{1}+\cdots+t B_{r-1}\right)-\left(t^{r-1} A B_{0}+t^{r-2} A B_{1}+\cdots+A B_{r-1}\right) \\
& =t^{r} B_{0}+t^{r-1}\left(B_{1}-A B_{0}\right)+t^{r-2}\left(B_{2}-A B_{1}\right)+\cdots+t\left(B_{r-1}-A B_{r-2}\right)-A B_{r-1} \\
& =t^{r} I+c_{1} t^{r-1} I+c_{2} t^{r-2} I+\cdots+c_{r-1} t I+c_{r} I=m(t) I
\end{aligned}
$$

Taking the determinant of both sides gives $|t I-A||B(t)|=|m(t) I|=[m(t)]^{n}$. Because $|B(t)|$ is a polynomial, $|t I-A|$ divides $[m(t)]^{n}$; that is, the characteristic polynomial of $A$ divides $[m(t)]^{n}$.

9.35. Prove Theorem 9.16: The characteristic polynomial $\Delta(t)$ and the minimal polynomial $m(t)$ of $A$ have the same irreducible factors.

Suppose $f(t)$ is an irreducible polynomial. If $f(t)$ divides $m(t)$, then $f(t)$ also divides $\Delta(t)$ [because $m(t)$ divides $\Delta(t)]$. On the other hand, if $f(t)$ divides $\Delta(t)$, then by Problem 9.34, $f(t)$ also divides $[m(t)]^{n}$. But $f(t)$ is irreducible; hence, $f(t)$ also divides $m(t)$. Thus, $m(t)$ and $\Delta(t)$ have the same irreducible factors.

9.36. Prove Theorem 9.19: The minimal polynomial $m(t)$ of a block diagonal matrix $M$ with diagonal blocks $A_{i}$ is equal to the least common multiple (LCM) of the minimal polynomials of the diagonal blocks $A_{i}$.

We prove the theorem for the case $r=2$. The general theorem follows easily by induction. Suppose $M=\left[\begin{array}{cc}A & 0 \\ 0 & B\end{array}\right]$, where $A$ and $B$ are square matrices. We need to show that the minimal polynomial $m(t)$ of $M$ is the LCM of the minimal polynomials $g(t)$ and $h(t)$ of $A$ and $B$, respectively.

Because $m(t)$ is the minimal polynomial of $M, m(M)=\left[\begin{array}{cc}m(A) & 0 \\ 0 & m(B)\end{array}\right]=0$, and $m(A)=0$ and $m(B)=0$. Because $g(t)$ is the minimal polynomial of $A, g(t)$ divides $m(t)$. Similarly, $h(t)$ divides $m(t)$. Thus $m(t)$ is a multiple of $g(t)$ and $h(t)$.

Now let $f(t)$ be another multiple of $g(t)$ and $h(t)$. Then $f(M)=\left[\begin{array}{cc}f(A) & 0 \\ 0 & f(B)\end{array}\right]=\left[\begin{array}{ll}0 & 0 \\ 0 & 0\end{array}\right]=0$. But $m(t)$ is the minimal polynomial of $M$; hence, $m(t)$ divides $f(t)$. Thus, $m(t)$ is the LCM of $g(t)$ and $h(t)$.

9.37. Suppose $m(t)=t^{r}+a_{r-1} t^{r-1}+\cdots+a_{1} t+a_{0}$ is the minimal polynomial of an $n$-square matrix $A$. Prove the following:

(a) $A$ is nonsingular if and only if the constant term $a_{0} \neq 0$.

(b) If $A$ is nonsingular, then $A^{-1}$ is a polynomial in $A$ of degree $r-1<n$.

(a) The following are equivalent: (i) $A$ is nonsingular, (ii) 0 is not a root of $m(t)$, (iii) $a_{0} \neq 0$. Thus, the statement is true.\\
(b) Because $A$ is nonsingular, $a_{0} \neq 0$ by (a). We have

Thus,

$$
m(A)=A^{r}+a_{r-1} A^{r-1}+\cdots+a_{1} A+a_{0} I=0
$$

$$
-\frac{1}{a_{0}}\left(A^{r-1}+a_{r-1} A^{r-2}+\cdots+a_{1} I\right) A=I
$$

Accordingly,

$$
A^{-1}=-\frac{1}{a_{0}}\left(A^{r-1}+a_{r-1} A^{r-2}+\cdots+a_{1} I\right)
$$

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Polynomials of Matrices}
9.38. Let $A=\left[\begin{array}{rr}2 & -3 \\ 5 & 1\end{array}\right]$ and $B=\left[\begin{array}{ll}1 & 2 \\ 0 & 3\end{array}\right]$. Find $f(A), g(A), f(B), g(B)$, where $f(t)=2 t^{2}-5 t+6$ and $g(t)=t^{3}-2 t^{2}+t+3$.

9.39. Let $A=\left[\begin{array}{ll}1 & 2 \\ 0 & 1\end{array}\right]$. Find $A^{2}, A^{3}, A^{n}$, where $n>3$, and $A^{-1}$.

9.40. Let $B=\left[\begin{array}{rrr}8 & 12 & 0 \\ 0 & 8 & 12 \\ 0 & 0 & 8\end{array}\right]$. Find a real matrix $A$ such that $B=A^{3}$.

9.41. For each matrix, find a polynomial having the following matrix as a root:\\
(a) $A=\left[\begin{array}{rr}2 & 5 \\ 1 & -3\end{array}\right]$,\\
(b) $B=\left[\begin{array}{ll}2 & -3 \\ 7 & -4\end{array}\right]$,\\
(c) $C=\left[\begin{array}{lll}1 & 1 & 2 \\ 1 & 2 & 3 \\ 2 & 1 & 4\end{array}\right]$

9.42. Let $A$ be any square matrix and let $f(t)$ be any polynomial. Prove (a) $\left(P^{-1} A P\right)^{n}=P^{-1} A^{n} P$.\\
(b) $f\left(P^{-1} A P\right)=P^{-1} f(A) P$.\\
(c) $f\left(A^{T}\right)=[f(A)]^{T}$.\\
(d) If $A$ is symmetric, then $f(A)$ is symmetric.

9.43. Let $M=\operatorname{diag}\left[A_{1}, \ldots, A_{r}\right]$ be a block diagonal matrix, and let $f(t)$ be any polynomial. Show that $f(M)$ is block diagonal and $f(M)=\operatorname{diag}\left[f\left(A_{1}\right), \ldots, f\left(A_{r}\right)\right]$.

9.44. Let $M$ be a block triangular matrix with diagonal blocks $A_{1}, \ldots, A_{r}$, and let $f(t)$ be any polynomial. Show that $f(M)$ is also a block triangular matrix, with diagonal blocks $f\left(A_{1}\right), \ldots, f\left(A_{r}\right)$.

\section*{Eigenvalues and Eigenvectors}
9.45. For each of the following matrices, find all eigenvalues and corresponding linearly independent eigenvectors:\\
(a) $A=\left[\begin{array}{ll}2 & -3 \\ 2 & -5\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rr}2 & 4 \\ -1 & 6\end{array}\right]$,\\
(c) $C=\left[\begin{array}{ll}1 & -4 \\ 3 & -7\end{array}\right]$

When possible, find the nonsingular matrix $P$ that diagonalizes the matrix.

9.46. Let $A=\left[\begin{array}{rr}2 & -1 \\ -2 & 3\end{array}\right]$.

(a) Find eigenvalues and corresponding eigenvectors.

(b) Find a nonsingular matrix $P$ such that $D=P^{-1} A P$ is diagonal.

(c) Find $A^{8}$ and $f(A)$ where $f(t)=t^{4}-5 t^{3}+7 t^{2}-2 t+5$.

(d) Find a matrix $B$ such that $B^{2}=A$.

9.47. Repeat Problem 9.46 for $A=\left[\begin{array}{rr}5 & 6 \\ -2 & -2\end{array}\right]$.

9.48. For each of the following matrices, find all eigenvalues and a maximum set $S$ of linearly independent eigenvectors:

(a) $A=\left[\begin{array}{lll}1 & -3 & 3 \\ 3 & -5 & 3 \\ 6 & -6 & 4\end{array}\right]$, (b) $B=\left[\begin{array}{lll}3 & -1 & 1 \\ 7 & -5 & 1 \\ 6 & -6 & 2\end{array}\right]$, (c) $C=\left[\begin{array}{rrr}1 & 2 & 2 \\ 1 & 2 & -1 \\ -1 & 1 & 4\end{array}\right]$

Which matrices can be diagonalized, and why?

9.49. For each of the following linear operators $T: \mathbf{R}^{2} \rightarrow \mathbf{R}^{2}$, find all eigenvalues and a basis for each eigenspace:\\
(a) $T(x, y)=(3 x+3 y, x+5 y)$,\\
(b) $T(x, y)=(3 x-13 y, x-3 y)$.

9.50. Let $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ be a real matrix. Find necessary and sufficient conditions on $a, b, c, d$ so that $A$ is diagonalizable - that is, so that $A$ has two (real) linearly independent eigenvectors.

9.51. Show that matrices $A$ and $A^{T}$ have the same eigenvalues. Give an example of a $2 \times 2$ matrix $A$ where $A$ and $A^{T}$ have different eigenvectors.

9.52. Suppose $v$ is an eigenvector of linear operators $F$ and $G$. Show that $v$ is also an eigenvector of the linear operator $k F+k^{\prime} G$, where $k$ and $k^{\prime}$ are scalars.

9.53. Suppose $v$ is an eigenvector of a linear operator $T$ belonging to the eigenvalue $\lambda$. Prove

(a) For $n>0, v$ is an eigenvector of $T^{n}$ belonging to $\lambda^{n}$.

(b) $f(\lambda)$ is an eigenvalue of $f(T)$ for any polynomial $f(t)$.

9.54. Suppose $\lambda \neq 0$ is an eigenvalue of the composition $F \circ G$ of linear operators $F$ and $G$. Show that $\lambda$ is also an eigenvalue of the composition $G \circ F$. [Hint: Show that $G(v)$ is an eigenvector of $G \circ F$.]

9.55. Let $E: V \rightarrow V$ be a projection mapping; that is, $E^{2}=E$. Show that $E$ is diagonalizable and, in fact, can be represented by the diagonal matrix $M=\left[\begin{array}{rr}I_{r} & 0 \\ 0 & 0\end{array}\right]$, where $r$ is the rank of $E$.

\section*{Diagonalizing Real Symmetric Matrices and Quadratic Forms}
9.56. For each of the following symmetric matrices $A$, find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $D=P^{-1} A P$ :\\
(a) $A=\left[\begin{array}{rr}5 & 4 \\ 4 & -1\end{array}\right]$,\\
(b) $A=\left[\begin{array}{rr}4 & -1 \\ -1 & 4\end{array}\right]$,\\
(c) $A=\left[\begin{array}{rr}7 & 3 \\ 3 & -1\end{array}\right]$

9.57. For each of the following symmetric matrices $B$, find its eigenvalues, a maximal orthogonal set $S$ of eigenvectors, and an orthogonal matrix $P$ such that $D=P^{-1} B P$ is diagonal:

(a) $B=\left[\begin{array}{lll}0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0\end{array}\right], \quad$ (b) $B=\left[\begin{array}{rrr}2 & 2 & 4 \\ 2 & 5 & 8 \\ 4 & 8 & 17\end{array}\right]$

9.58. Using variables $s$ and $t$, find an orthogonal substitution that diagonalizes each of the following quadratic forms:\\
(a) $q(x, y)=4 x^{2}+8 x y-11 y^{2}$,\\
(b) $q(x, y)=2 x^{2}-6 x y+10 y^{2}$

9.59. For each of the following quadratic forms $q(x, y, z)$, find an orthogonal substitution expressing $x, y, z$ in terms of variables $r, s, t$, and find $q(r, s, t)$ :\\
(a) $q(x, y, z)=5 x^{2}+3 y^{2}+12 x z$,\\
(b) $q(x, y, z)=3 x^{2}-4 x y+6 y^{2}+2 x z-4 y z+3 z^{2}$

9.60. Find a real $2 \times 2$ symmetric matrix $A$ with eigenvalues:

(a) $\lambda=1$ and $\lambda=4$ and eigenvector $u=(1,1)$ belonging to $\lambda=1$;

(b) $\lambda=2$ and $\lambda=3$ and eigenvector $u=(1,2)$ belonging to $\lambda=2$.

In each case, find a matrix $B$ for which $B^{2}=A$.

\section*{Characteristic and Minimal Polynomials}
9.61. Find the characteristic and minimal polynomials of each of the following matrices:

(a) $A=\left[\begin{array}{rrr}3 & 1 & -1 \\ 2 & 4 & -2 \\ -1 & -1 & 3\end{array}\right]$, (b) $B=\left[\begin{array}{rrr}3 & 2 & -1 \\ 3 & 8 & -3 \\ 3 & 6 & -1\end{array}\right]$

9.62. Find the characteristic and minimal polynomials of each of the following matrices:\\
(a) $A=\left[\begin{array}{lllll}2 & 5 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 & 0 \\ 0 & 0 & 4 & 2 & 0 \\ 0 & 0 & 3 & 5 & 0 \\ 0 & 0 & 0 & 0 & 7\end{array}\right]$,\\
(b) $B=\left[\begin{array}{rrrrr}4 & -1 & 0 & 0 & 0 \\ 1 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 \\ 0 & 0 & 0 & 3 & 1 \\ 0 & 0 & 0 & 0 & 3\end{array}\right]$,\\
(c) $C=\left[\begin{array}{lllll}3 & 2 & 0 & 0 & 0 \\ 1 & 4 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 \\ 0 & 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 & 4\end{array}\right]$

9.63. Let $A=\left[\begin{array}{lll}1 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1\end{array}\right]$ and $B=\left[\begin{array}{lll}2 & 0 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 1\end{array}\right]$. Show that $A$ and $B$ have different characteristic polynomials (and so are not similar) but have the same minimal polynomial. Thus, nonsimilar matrices may have the same minimal polynomial.

9.64. Let $A$ be an $n$-square matrix for which $A^{k}=0$ for some $k>n$. Show that $A^{n}=0$.

9.65. Show that a matrix $A$ and its transpose $A^{T}$ have the same minimal polynomial.

9.66. Suppose $f(t)$ is an irreducible monic polynomial for which $f(A)=0$ for a matrix $A$. Show that $f(t)$ is the minimal polynomial of $A$.

9.67. Show that $A$ is a scalar matrix $k I$ if and only if the minimal polynomial of $A$ is $m(t)=t-k$.

9.68. Find a matrix $A$ whose minimal polynomial is (a) $t^{3}-5 t^{2}+6 t+8$, (b) $t^{4}-5 t^{3}-2 t+7 t+4$.

9.69. Let $f(t)$ and $g(t)$ be monic polynomials (leading coefficient one) of minimal degree for which $A$ is a root. Show $f(t)=g(t)$. [Thus, the minimal polynomial of $A$ is unique.]

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $M=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ denotes a matrix $M$ with rows $R_{1}, R_{2}, \ldots$

9.38. $f(A)=[-26,-3 ; \quad 5,-27], \quad g(A)=[-40,39 ; \quad-65,-27]$, $f(B)=[3,6 ; \quad 0,9], \quad g(B)=[3,12 ; \quad 0,15]$

9.39. $A^{2}=[1,4 ; \quad 0,1], \quad A^{3}=[1,6 ; \quad 0,1], \quad A^{n}=[1,2 n ; \quad 0,1], \quad A^{-1}=[1,-2 ; \quad 0,1]$

9.40. Let $A=[2, a, b ; \quad 0,2, c ; \quad 0,0,2]$. Set $B=A^{3}$ and then $a=1, b=-\frac{1}{2}, c=1$

9.41. Find $\Delta(t)$ : (a) $t^{2}+t-11$,

(b) $t^{2}+2 t+13$

(c) $t^{3}-7 t^{2}+6 t-1$

9.45. (a) $\lambda=1, u=(3,1) ; \quad \lambda=-4, v=(1,2), \quad$ (b) $\quad \lambda=4, u=(2,1)$,

(c) $\lambda=-1, u=(2,1) ; \quad \lambda=-5, v=(2,3)$. Only $A$ and $C$ can be diagonalized; use $P=[u, v]$.

9.46. (a) $\lambda=1, u=(1,1) ; \quad \lambda=4, v=(1,-2)$,

(b) $P=[u, v]$,

(c) $f(A)=[3,1 ; 2,1], \quad A^{8}=[21846,-21845 ; \quad-43690,43691]$,

(d) $B=\left[\frac{4}{3},-\frac{1}{3} ; \quad-\frac{2}{3}, \frac{5}{3}\right]$

9.47. (a) $\lambda=1, u=(3,-2) ; \quad \lambda=2, v=(2,-1)$, (b) $P=[u, v]$,

(c) $f(A)=[2,-6 ; 2,9], \quad A^{8}=[1021,1530 ;-510,-764]$,

(d) $B=\left[\begin{array}{ll}-3+4 \sqrt{2}, & -6+6 \sqrt{2} ; \quad 2-2 \sqrt{2}, \quad 4-3 \sqrt{2}]\end{array}\right.$

9.48. (a) $\lambda=-2, u=(1,1,0), v=(1,0,-1) ; \lambda=4, w=(1,1,2)$,

(b) $\lambda=2, u=(1,1,0) ; \quad \lambda=-4, v=(0,1,1)$,

(c) $\lambda=3, u=(1,1,0), v=(1,0,1) ; \quad \lambda=1, w=(2,-1,1)$. Only $A$ and $C$ can be diagonalized; use $P=[u, v, w]$.

9.49. (a) $\lambda=2, u=(3,-1) ; \quad \lambda=6, v=(1,1), \quad$ (b) No real eigenvalues

9.50. We need $[-\operatorname{tr}(A)]^{2}-4[\operatorname{det}(A)] \geq 0$ or $(a-d)^{2}+4 b c \geq 0$.

9.51. $A=[1,1 ; \quad 0,1]$

9.56. (a) $P=[2,-1 ; \quad 1,2] / \sqrt{5}, \quad D=[7,0 ; \quad 0,3]$,

(b) $P=[1,1 ; 1,-1] / \sqrt{2}, \quad D=[3,0 ; \quad 0,5]$,

(c) $P=[3,-1 ; \quad 1,3] / \sqrt{10}, \quad D=[8,0 ; \quad 0,2]$

9.57. (a) $\lambda=-1, \quad u=(1,-1,0), \quad v=(1,1,-2) ; \quad \lambda=2, \quad w=(1,1,1)$,

(b) $\quad \lambda=1, \quad u=(2,1,-1), \quad v=(2,-3,1) ; \quad \lambda=22, \quad w=(1,2,4)$;

Normalize $u, v, w$, obtaining $\hat{u}, \hat{v}, \hat{w}$, and set $P=[\hat{u}, \hat{v}, \hat{w}]$. (Remark: $u$ and $v$ are not unique.)

9.58. (a) $x=(4 s+t) / \sqrt{17}, \quad y=(-s+4 t) / \sqrt{17}, \quad q(s, t)=5 s^{2}-12 t^{2}$,

(b) $x=(3 s-t) / \sqrt{10}, \quad y=(s+3 t) / \sqrt{10}, \quad q(s, t)=s^{2}+11 t^{2}$

9.59. (a) $x=(3 s+2 t) / \sqrt{13}, \quad y=r, \quad z=(2 s-3 t) / \sqrt{13}, \quad q(r, s, t)=3 r^{2}+9 s^{2}-4 t^{2}$,

(b) $\quad x=5 K s+L t, \quad y=J r+2 K s-2 L t, \quad z=2 J r-K s-L t, \quad$ where $\quad J=1 / \sqrt{5}, \quad K=1 / \sqrt{30}$, $L=1 / \sqrt{6} ; \quad q(r, s, t)=2 r^{2}+2 s^{2}+8 t^{2}$

9.60. (a) $A=\frac{1}{2}[5,-3 ;-3,5], \quad B=\frac{1}{2}[3,-1 ;-1,3]$,

(b) $A=\frac{1}{5}[14,-2 ;-2,11], \quad B=\frac{1}{5}[\sqrt{2}+4 \sqrt{3}, 2 \sqrt{2}-2 \sqrt{3} ; 2 \sqrt{2}-2 \sqrt{3}, 4 \sqrt{2}+\sqrt{3}]$

9.61. (a) $\Delta(t)=m(t)=(t-2)^{2}(t-6), \quad$ (b) $\Delta(t)=(t-2)^{2}(t-6), \quad m(t)=(t-2)(t-6)$

9.62. (a) $\Delta(t)=(t-2)^{3}(t-7)^{2}, \quad m(t)=(t-2)^{2}(t-7)$,

(b) $\Delta(t)=(t-3)^{5}, \quad m(t)=(t-3)^{3}$,

(c) $\Delta(t)=(t-2)^{2}(t-4)^{2}(t-5), \quad m(t)=(t-2)(t-4)(t-5)$

9.68. Let $A$ be the companion matrix [Example 9.12(b)] with last column: (a) $[-8,-6,5]^{T}$, (b) $[-4,-7,2,5]^{T}$

9.69. Hint: $A$ is a root of $h(t)=f(t)-g(t)$, where $h(t) \equiv 0$ or the degree of $h(t)$ is less than the degree of $f(t)$.

\section*{Canonical Forms}
\subsection*{10.1 Introduction}
Let $T$ be a linear operator on a vector space of finite dimension. As seen in Chapter 6, $T$ may not have a diagonal matrix representation. However, it is still possible to "simplify" the matrix representation of $T$ in a number of ways. This is the main topic of this chapter. In particular, we obtain the primary decomposition theorem, and the triangular, Jordan, and rational canonical forms.

We comment that the triangular and Jordan canonical forms exist for $T$ if and only if the characteristic polynomial $\Delta(t)$ of $T$ has all its roots in the base field $K$. This is always true if $K$ is the complex field $\mathbf{C}$ but may not be true if $K$ is the real field $\mathbf{R}$.

We also introduce the idea of a quotient space. This is a very powerful tool, and it will be used in the proof of the existence of the triangular and rational canonical forms.

\subsection*{10.2 Triangular Form}
Let $T$ be a linear operator on an $n$-dimensional vector space $V$. Suppose $T$ can be represented by the triangular matrix

$$
A=\left[\begin{array}{llll}
a_{11} & a_{12} & \ldots & a_{1 n} \\
& a_{22} & \ldots & a_{2 n} \\
& & \cdots & \cdots \\
& & & a_{n n}
\end{array}\right]
$$

Then the characteristic polynomial $\Delta(t)$ of $T$ is a product of linear factors; that is,

$$
\Delta(t)=\operatorname{det}(t I-A)=\left(t-a_{11}\right)\left(t-a_{22}\right) \cdots\left(t-a_{n n}\right)
$$

The converse is also true and is an important theorem (proved in Problem 10.28).

THEOREM 10.1: Let $T: V \rightarrow V$ be a linear operator whose characteristic polynomial factors into linear polynomials. Then there exists a basis of $V$ in which $T$ is represented by a triangular matrix.

THEOREM 10.1: (Alternative Form) Let $A$ be a square matrix whose characteristic polynomial factors into linear polynomials. Then $A$ is similar to a triangular matrix-that is, there exists an invertible matrix $P$ such that $P^{-1} A P$ is triangular.

We say that an operator $T$ can be brought into triangular form if it can be represented by a triangular matrix. Note that in this case, the eigenvalues of $T$ are precisely those entries appearing on the main diagonal. We give an application of this remark.

EXAMPLE 10.1 Let $A$ be a square matrix over the complex field $\mathbf{C}$. Suppose $\lambda$ is an eigenvalue of $A^{2}$. Show that $\sqrt{\lambda}$ or $-\sqrt{\lambda}$ is an eigenvalue of $A$.

By Theorem 10.1, $A$ and $A^{2}$ are similar, respectively, to triangular matrices of the form

$$
B=\left[\begin{array}{cccc}
\mu_{1} & * & \ldots & * \\
& \mu_{2} & \ldots & * \\
& & \ldots & \ldots \\
& & & \mu_{n}
\end{array}\right] \quad \text { and } \quad B^{2}=\left[\begin{array}{cccc}
\mu_{1}^{2} & * & \ldots & * \\
& \mu_{2}^{2} & \ldots & * \\
& & \ldots & \ldots \\
& & & \mu_{n}^{2}
\end{array}\right]
$$

Because similar matrices have the same eigenvalues, $\lambda=\mu_{i}^{2}$ for some $i$. Hence, $\mu_{i}=\sqrt{\lambda}$ or $\mu_{i}=-\sqrt{\lambda}$ is an eigenvalue of $A$.

\subsection*{10.3 Invariance}
Let $T: V \rightarrow V$ be linear. A subspace $W$ of $V$ is said to be invariant under $T$ or $T$-invariant if $T$ maps $W$ into itself-that is, if $v \in W$ implies $T(v) \in W$. In this case, $T$ restricted to $W$ defines a linear operator on $W$; that is, $T$ induces a linear operator $\hat{T}: W \rightarrow W$ defined by $\hat{T}(w)=T(w)$ for every $w \in W$.

\section*{EXAMPLE 10.2}
(a) Let $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the following linear operator, which rotates each vector $v$ about the $z$-axis by angle $\theta$ (shown in Fig. 10-1):

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-333}
\end{center}

Figure 10-1

Observe that each vector $w=(a, b, 0)$ in the $x y$-plane $W$ remains in $W$ under the mapping $T$; hence, $W$ is $T$-invariant. Observe also that the $z$-axis $U$ is invariant under $T$. Furthermore, the restriction of $T$ to $W$ rotates each vector about the origin $O$, and the restriction of $T$ to $U$ is the identity mapping of $U$.

(b) Nonzero eigenvectors of a linear operator $T: V \rightarrow V$ may be characterized as generators of $T$-invariant one-dimensional subspaces. Suppose $T(v)=\lambda v, v \neq 0$. Then $W=\{k v, k \in K\}$, the one-dimensional subspace generated by $v$, is invariant under $T$ because

$$
T(k v)=k T(v)=k(\lambda v)=k \lambda v \in W
$$

Conversely, suppose $\operatorname{dim} U=1$ and $u \neq 0$ spans $U$, and $U$ is invariant under $T$. Then $T(u) \in U$ and so $T(u)$ is a multiple of $u$ - that is, $T(u)=\mu u$. Hence, $u$ is an eigenvector of $T$.

The next theorem (proved in Problem 10.3) gives us an important class of invariant subspaces.

THEOREM 10.2: Let $T: V \rightarrow V$ be any linear operator, and let $f(t)$ be any polynomial. Then the kernel of $f(T)$ is invariant under $T$.

The notion of invariance is related to matrix representations (Problem 10.5) as follows.

THEOREM 10.3: Suppose $W$ is an invariant subspace of $T: V \rightarrow V$. Then $T$ has a block matrix representation $\left[\begin{array}{cc}A & B \\ 0 & C\end{array}\right]$, where $A$ is a matrix representation of the restriction $\hat{T}$ of $T$ to $W$.

\subsection*{10.4 Invariant Direct-Sum Decompositions}
A vector space $V$ is termed the direct sum of subspaces $W_{1}, \ldots, W_{r}$, written

$$
V=W_{1} \oplus W_{2} \oplus \ldots \oplus W_{r}
$$

if every vector $v \in V$ can be written uniquely in the form

$$
v=w_{1}+w_{2}+\ldots+w_{r}, \quad \text { with } \quad w_{i} \in W_{i}
$$

The following theorem (proved in Problem 10.7) holds.

THEOREM 10.4: $\quad$ Suppose $W_{1}, W_{2}, \ldots, W_{r}$ are subspaces of $V$, and suppose

$$
B_{1}=\left\{w_{11}, w_{12}, \ldots, w_{1 n_{1}}\right\}, \quad \ldots, \quad B_{r}=\left\{w_{r 1}, w_{r 2}, \ldots, w_{r n_{r}}\right\}
$$

are bases of $W_{1}, W_{2}, \ldots, W_{r}$, respectively. Then $V$ is the direct sum of the $W_{i}$ if and only if the union $B=B_{1} \cup \ldots \cup B_{r}$ is a basis of $V$.

Now suppose $T: V \rightarrow V$ is linear and $V$ is the direct sum of (nonzero) $T$-invariant subspaces $W_{1}, W_{2}, \ldots, W_{r}$; that is,

$$
V=W_{1} \oplus \ldots \oplus W_{r} \quad \text { and } \quad T\left(W_{i}\right) \subseteq W_{i}, \quad i=1, \ldots, r
$$

Let $T_{i}$ denote the restriction of $T$ to $W_{i}$. Then $T$ is said to be decomposable into the operators $T_{i}$ or $T$ is said to be the direct sum of the $T_{i}$, written $T=T_{1} \oplus \ldots \oplus T_{r}$. Also, the subspaces $W_{1}, \ldots, W_{r}$ are said to reduce $T$ or to form a $T$-invariant direct-sum decomposition of $V$.

Consider the special case where two subspaces $U$ and $W$ reduce an operator $T: V \rightarrow V$; say $\operatorname{dim} U=2$ and $\operatorname{dim} W=3$, and suppose $\left\{u_{1}, u_{2}\right\}$ and $\left\{w_{1}, w_{2}, w_{3}\right\}$ are bases of $U$ and $W$, respectively. If $T_{1}$ and $T_{2}$ denote the restrictions of $T$ to $U$ and $W$, respectively, then

$$
\begin{array}{ll}
T_{1}\left(u_{1}\right)=a_{11} u_{1}+a_{12} u_{2} & T_{2}\left(w_{1}\right)=b_{11} w_{1}+b_{12} w_{2}+b_{13} w_{3} \\
T_{1}\left(u_{2}\right)=a_{21} u_{1}+a_{22} u_{2} & T_{2}\left(w_{2}\right)=b_{21} w_{1}+b_{22} w_{2}+b_{23} w_{3} \\
T_{2}\left(w_{2}\right)=b_{21} w_{1}+b_{22} w_{2}+b_{23} w_{3}
\end{array}
$$

Accordingly, the following matrices $A, B, M$ are the matrix representations of $T_{1}, T_{2}, T$, respectively,

$$
A=\left[\begin{array}{ll}
a_{11} & a_{21} \\
a_{12} & a_{22}
\end{array}\right], \quad B=\left[\begin{array}{lll}
b_{11} & b_{21} & b_{31} \\
b_{12} & b_{22} & b_{32} \\
b_{13} & b_{23} & b_{33}
\end{array}\right], \quad M=\left[\begin{array}{cc}
A & 0 \\
0 & B
\end{array}\right]
$$

The block diagonal matrix $M$ results from the fact that $\left\{u_{1}, u_{2}, w_{1}, w_{2}, w_{3}\right\}$ is a basis of $V$ (Theorem 10.4), and that $T\left(u_{i}\right)=T_{1}\left(u_{i}\right)$ and $T\left(w_{j}\right)=T_{2}\left(w_{j}\right)$.

A generalization of the above argument gives us the following theorem.

THEOREM 10.5: Suppose $T: V \rightarrow V$ is linear and suppose $V$ is the direct sum of $T$-invariant subspaces, say, $W_{1}, \ldots, W_{r}$. If $A_{i}$ is a matrix representation of the restriction of $T$ to $W_{i}$, then $T$ can be represented by the block diagonal matrix:

$$
M=\operatorname{diag}\left(A_{1}, A_{2}, \ldots, A_{r}\right)
$$

\subsection*{10.5 Primary Decomposition}
The following theorem shows that any operator $T: V \rightarrow V$ is decomposable into operators whose minimum polynomials are powers of irreducible polynomials. This is the first step in obtaining a canonical form for $T$.

THEOREM 10.6: (Primary Decomposition Theorem) Let $T: V \rightarrow V$ be a linear operator with minimal polynomial

$$
m(t)=f_{1}(t)^{n_{1}} f_{2}(t)^{n_{2}} \cdots f_{r}(t)^{n_{r}}
$$

where the $f_{i}(t)$ are distinct monic irreducible polynomials. Then $V$ is the direct sum of $T$-invariant subspaces $W_{1}, \ldots, W_{r}$, where $W_{i}$ is the kernel of $f_{i}(T)^{n_{i}}$. Moreover, $f_{i}(t)^{n_{i}}$ is the minimal polynomial of the restriction of $T$ to $W_{i}$.

The above polynomials $f_{i}(t)^{n_{i}}$ are relatively prime. Therefore, the above fundamental theorem follows (Problem 10.11) from the next two theorems (proved in Problems 10.9 and 10.10, respectively).

THEOREM 10.7: Suppose $T: V \rightarrow V$ is linear, and suppose $f(t)=g(t) h(t)$ are polynomials such that $f(T)=\mathbf{0}$ and $g(t)$ and $h(t)$ are relatively prime. Then $V$ is the direct sum of the $T$-invariant subspace $U$ and $W$, where $U=\operatorname{Ker} g(T)$ and $W=\operatorname{Ker} h(T)$.

THEOREM 10.8: In Theorem 10.7, if $f(t)$ is the minimal polynomial of $T$ [and $g(t)$ and $h(t)$ are monic], then $g(t)$ and $h(t)$ are the minimal polynomials of the restrictions of $T$ to $U$ and $W$, respectively.

We will also use the primary decomposition theorem to prove the following useful characterization of diagonalizable operators (see Problem 10.12 for the proof).

THEOREM 10.9: A linear operator $T: V \rightarrow V$ is diagonalizable if and only if its minimal polynomial $m(t)$ is a product of distinct linear polynomials.

THEOREM 10.9: (Alternative Form) A matrix $A$ is similar to a diagonal matrix if and only if its minimal polynomial is a product of distinct linear polynomials.

EXAMPLE 10.3 Suppose $A \neq I$ is a square matrix for which $A^{3}=I$. Determine whether or not $A$ is similar to a diagonal matrix if $A$ is a matrix over: (i) the real field $\mathbf{R}$, (ii) the complex field $\mathbf{C}$.

Because $A^{3}=I, A$ is a zero of the polynomial $f(t)=t^{3}-1=(t-1)\left(t^{2}+t+1\right)$. The minimal polynomial $m(t)$ of $A$ cannot be $t-1$, because $A \neq I$. Hence,

$$
m(t)=t^{2}+t+1 \quad \text { or } \quad m(t)=t^{3}-1
$$

Because neither polynomial is a product of linear polynomials over $\mathbf{R}, A$ is not diagonalizable over $\mathbf{R}$. On the other hand, each of the polynomials is a product of distinct linear polynomials over $\mathbf{C}$. Hence, $A$ is diagonalizable over $\mathbf{C}$.

\subsection*{10.6 Nilpotent Operators}
A linear operator $T: V \rightarrow V$ is termed nilpotent if $T^{n}=\mathbf{0}$ for some positive integer $n$; we call $k$ the index of nilpotency of $T$ if $T^{k}=\mathbf{0}$ but $T^{k-1} \neq \mathbf{0}$. Analogously, a square matrix $A$ is termed nilpotent if $A^{n}=0$ for some positive integer $n$, and of index $k$ if $A^{k}=0$ but $A^{k-1} \neq 0$. Clearly the minimum polynomial of a nilpotent operator (matrix) of index $k$ is $m(t)=t^{k}$; hence, 0 is its only eigenvalue.

EXAMPLE 10.4 The following two $r$-square matrices will be used throughout the chapter:

$$
N=N(r)=\left[\begin{array}{cccccc}
0 & 1 & 0 & \ldots & 0 & 0 \\
0 & 0 & 1 & \ldots & 0 & 0 \\
& \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & 0 & \ldots & 0 & 1 \\
0 & 0 & 0 & \ldots & 0 & 0
\end{array}\right] \quad \text { and } \quad J(\lambda)=\left[\begin{array}{cccccc}
\lambda & 1 & 0 & \ldots & 0 & 0 \\
0 & \lambda & 1 & \ldots & 0 & 0 \\
\ldots & \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
0 & 0 & 0 & \ldots & \lambda & 1 \\
0 & 0 & 0 & \ldots & 0 & \lambda
\end{array}\right]
$$

The first matrix $N$, called a Jordan nilpotent block, consists of 1's above the diagonal (called the superdiagonal), and 0 's elsewhere. It is a nilpotent matrix of index $r$. (The matrix $N$ of order 1 is just the $1 \times 1$ zero matrix [0].)

The second matrix $J(\lambda)$, called a Jordan block belonging to the eigenvalue $\lambda$, consists of $\lambda$ 's on the diagonal, 1's on the superdiagonal, and 0's elsewhere. Observe that

$$
J(\lambda)=\lambda I+N
$$

In fact, we will prove that any linear operator $T$ can be decomposed into operators, each of which is the sum of a scalar operator and a nilpotent operator.

The following (proved in Problem 10.16) is a fundamental result on nilpotent operators.

THEOREM 10.10: Let $T: V \rightarrow V$ be a nilpotent operator of index $k$. Then $T$ has a block diagonal matrix representation in which each diagonal entry is a Jordan nilpotent block $N$.

There is at least one $N$ of order $k$, and all other $N$ are of orders $\leq k$. The number of $N$ of each possible order is uniquely determined by $T$. The total number of $N$ of all orders is equal to the nullity of $T$.

The proof of Theorem 10.10 shows that the number of $N$ of order $i$ is equal to $2 m_{i}-m_{i+1}-m_{i-1}$, where $m_{i}$ is the nullity of $T^{i}$.

\subsection*{10.7 Jordan Canonical Form}
An operator $T$ can be put into Jordan canonical form if its characteristic and minimal polynomials factor into linear polynomials. This is always true if $K$ is the complex field $\mathbf{C}$. In any case, we can always extend the base field $K$ to a field in which the characteristic and minimal polynomials do factor into linear factors; thus, in a broad sense, every operator has a Jordan canonical form. Analogously, every matrix is similar to a matrix in Jordan canonical form.

The following theorem (proved in Problem 10.18) describes the Jordan canonical form $J$ of a linear operator $T$.

THEOREM 10.11: Let $T: V \rightarrow V$ be a linear operator whose characteristic and minimal polynomials are, respectively,

$$
\Delta(t)=\left(t-\lambda_{1}\right)^{n_{1}} \cdots\left(t-\lambda_{r}\right)^{n_{r}} \quad \text { and } \quad m(t)=\left(t-\lambda_{1}\right)^{m_{1}} \cdots\left(t-\lambda_{r}\right)^{m_{r}}
$$

where the $\lambda_{i}$ are distinct scalars. Then $T$ has a block diagonal matrix representation $J$ in which each diagonal entry is a Jordan block $J_{i j}=J\left(\lambda_{i}\right)$. For each $\lambda_{i j}$, the corresponding $J_{i j}$ have the following properties:

(i) There is at least one $J_{i j}$ of order $m_{i}$; all other $J_{i j}$ are of order $\leq m_{i}$.

(ii) The sum of the orders of the $J_{i j}$ is $n_{i}$.

(iii) The number of $J_{i j}$ equals the geometric multiplicity of $\lambda_{i}$.

(iv) The number of $J_{i j}$ of each possible order is uniquely determined by $T$.

EXAMPLE 10.5 Suppose the characteristic and minimal polynomials of an operator $T$ are, respectively,

$$
\Delta(t)=(t-2)^{4}(t-5)^{3} \quad \text { and } \quad m(t)=(t-2)^{2}(t-5)^{3}
$$

Then the Jordan canonical form of $T$ is one of the following block diagonal matrices:

$$
\operatorname{diag}\left(\left[\begin{array}{ll}
2 & 1 \\
0 & 2
\end{array}\right],\left[\begin{array}{ll}
2 & 1 \\
0 & 2
\end{array}\right],\left[\begin{array}{lll}
5 & 1 & 0 \\
0 & 5 & 1 \\
0 & 0 & 5
\end{array}\right]\right) \quad \text { or } \quad \operatorname{diag}\left(\left[\begin{array}{ll}
2 & 1 \\
0 & 2
\end{array}\right],[2],[2],\left[\begin{array}{lll}
5 & 1 & 0 \\
0 & 5 & 1 \\
0 & 0 & 5
\end{array}\right]\right)
$$

The first matrix occurs if $T$ has two independent eigenvectors belonging to the eigenvalue 2 ; and the second matrix occurs if $T$ has three independent eigenvectors belonging to the eigenvalue 2 .

\subsection*{10.8 Cyclic Subspaces}
Let $T$ be a linear operator on a vector space $V$ of finite dimension over $K$. Suppose $v \in V$ and $v \neq 0$. The set of all vectors of the form $f(T)(v)$, where $f(t)$ ranges over all polynomials over $K$, is a $T$-invariant subspace of $V$ called the $T$-cyclic subspace of $V$ generated by $v$; we denote it by $Z(v, T)$ and denote the restriction of $T$ to $Z(v, T)$ by $T_{v}$. By Problem 10.56, we could equivalently define $Z(v, T)$ as the intersection of all $T$-invariant subspaces of $V$ containing $v$.

Now consider the sequence

$$
v, \quad T(v), \quad T^{2}(v), \quad T^{3}(v), \ldots
$$

of powers of $T$ acting on $v$. Let $k$ be the least integer such that $T^{k}(v)$ is a linear combination of those vectors that precede it in the sequence, say,

$$
T^{k}(v)=-a_{k-1} T^{k-1}(v)-\cdots-a_{1} T(v)-a_{0} v
$$

Then

$$
m_{v}(t)=t^{k}+a_{k-1} t^{k-1}+\cdots+a_{1} t+a_{0}
$$

is the unique monic polynomial of lowest degree for which $m_{v}(T)(v)=0$. We call $m_{v}(t)$ the $T$-annihilator of $v$ and $Z(v, T)$.

The following theorem (proved in Problem 10.29) holds.

THEOREM 10.12: Let $Z(v, T), T_{v}, m_{v}(t)$ be defined as above. Then

(i) The set $\left\{v, T(v), \ldots, T^{k-1}(v)\right\}$ is a basis of $Z(v, T)$; hence, $\operatorname{dim} Z(v, T)=k$.

(ii) The minimal polynomial of $T_{v}$ is $m_{v}(t)$.

(iii) The matrix representation of $T_{v}$ in the above basis is just the companion matrix $C\left(m_{v}\right)$ of $m_{v}(t)$; that is,

$$
C\left(m_{v}\right)=\left[\begin{array}{rrrrrr}
0 & 0 & 0 & \ldots & 0 & -a_{0} \\
1 & 0 & 0 & \ldots & 0 & -a_{1} \\
0 & 1 & 0 & \ldots & 0 & -a_{2} \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots . . \ldots \ldots \\
0 & 0 & 0 & \ldots & 0 & -a_{k-2} \\
0 & 0 & 0 & \ldots & 1 & -a_{k-1}
\end{array}\right]
$$

\subsection*{10.9 Rational Canonical Form}
In this section, we present the rational canonical form for a linear operator $T: V \rightarrow V$. We emphasize that this form exists even when the minimal polynomial cannot be factored into linear polynomials. (Recall that this is not the case for the Jordan canonical form.)

LEMMA 10.13: $\quad$ Let $T: V \rightarrow V$ be a linear operator whose minimal polynomial is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial. Then $V$ is the direct sum

$$
V=Z\left(v_{1}, T\right) \oplus \cdots \oplus Z\left(v_{r}, T\right)
$$

of $T$-cyclic subspaces $Z\left(v_{i}, T\right)$ with corresponding $T$-annihilators

$$
f(t)^{n_{1}}, f(t)^{n_{2}}, \ldots, f(t)^{n_{r}}, \quad n=n_{1} \geq n_{2} \geq \ldots \geq n_{r}
$$

Any other decomposition of $V$ into $T$-cyclic subspaces has the same number of components and the same set of $T$-annihilators.

We emphasize that the above lemma (proved in Problem 10.31) does not say that the vectors $v_{i}$ or other $T$-cyclic subspaces $Z\left(v_{i}, T\right)$ are uniquely determined by $T$, but it does say that the set of $T$-annihilators is uniquely determined by $T$. Thus, $T$ has a unique block diagonal matrix representation:

$$
M=\operatorname{diag}\left(C_{1}, C_{2}, \ldots, C_{r}\right)
$$

where the $C_{i}$ are companion matrices. In fact, the $C_{i}$ are the companion matrices of the polynomials $f(t)^{n_{i}}$.

Using the Primary Decomposition Theorem and Lemma 10.13, we obtain the following result.

THEOREM 10.14: Let $T: V \rightarrow V$ be a linear operator with minimal polynomial

$$
m(t)=f_{1}(t)^{m_{1}} f_{2}(t)^{m_{2}} \cdots f_{s}(t)^{m_{s}}
$$

where the $f_{i}(t)$ are distinct monic irreducible polynomials. Then $T$ has a unique block diagonal matrix representation:

$$
M=\operatorname{diag}\left(C_{11}, C_{12}, \ldots, C_{1 r_{1}}, \ldots, C_{s 1}, C_{s 2}, \ldots, C_{s r_{s}}\right)
$$

where the $C_{i j}$ are companion matrices. In particular, the $C_{i j}$ are the companion matrices of the polynomials $f_{i}(t)^{n_{i j}}$, where

$$
m_{1}=n_{11} \geq n_{12} \geq \cdots \geq n_{1 r_{1}}, \quad \ldots, \quad m_{s}=n_{s 1} \geq n_{s 2} \geq \cdots \geq n_{s r_{s}}
$$

The above matrix representation of $T$ is called its rational canonical form. The polynomials $f_{i}(t)^{n_{i j}}$ are called the elementary divisors of $T$.

EXAMPLE 10.6 Let $V$ be a vector space of dimension 8 over the rational field $\mathbf{Q}$, and let $T$ be a linear operator on $V$ whose minimal polynomial is

$$
m(t)=f_{1}(t) f_{2}(t)^{2}=\left(t^{4}-4 t^{3}+6 t^{2}-4 t-7\right)(t-3)^{2}
$$

Thus, because $\operatorname{dim} V=8$, the characteristic polynomial $\Delta(t)=f_{1}(t) f_{2}(t)^{4}$. Also, the rational canonical form $M$ of $T$ must have one block the companion matrix of $f_{1}(t)$ and one block the companion matrix of $f_{2}(t)^{2}$. There are two possibilities:

(a) $\operatorname{diag}\left[C\left(t^{4}-4 t^{3}+6 t^{2}-4 t-7\right), \quad C\left((t-3)^{2}\right), \quad C\left((t-3)^{2}\right)\right]$

(b) $\operatorname{diag}\left[C\left(t^{4}-4 t^{3}+6 t^{2}-4 t-7\right), \quad C\left((t-3)^{2}\right), \quad C(t-3), C(t-3)\right]$

That is,

(a) $\operatorname{diag}\left(\left[\begin{array}{rrrr}0 & 0 & 0 & 7 \\ 1 & 0 & 0 & 4 \\ 0 & 1 & 0 & -6 \\ 0 & 0 & 1 & 4\end{array}\right],\left[\begin{array}{rr}0 & -9 \\ 1 & 6\end{array}\right],\left[\begin{array}{rr}0 & -9 \\ 1 & 6\end{array}\right]\right)$, (b) $\operatorname{diag}\left(\left[\begin{array}{rrrr}0 & 0 & 0 & 7 \\ 1 & 0 & 0 & 4 \\ 0 & 1 & 0 & -6 \\ 0 & 0 & 1 & 4\end{array}\right],\left[\begin{array}{rr}0 & -9 \\ 1 & 6\end{array}\right],[3],[3]\right)$

\subsection*{10.10 Quotient Spaces}
Let $V$ be a vector space over a field $K$ and let $W$ be a subspace of $V$. If $v$ is any vector in $V$, we write $v+W$ for the set of sums $v+w$ with $w \in W$; that is,

$$
v+W=\{v+w: w \in W\}
$$

These sets are called the cosets of $W$ in $V$. We show (Problem 10.22) that these cosets partition $V$ into mutually disjoint subsets.

EXAMPLE 10.7 Let $W$ be the subspace of $\mathbf{R}^{2}$ defined by

$$
W=\{(a, b): a=b\}
$$

that is, $W$ is the line given by the equation $x-y=0$. We can view $v+W$ as a translation of the line obtained by adding the vector $v$ to each point in $W$. As shown in Fig. 10-2, the coset $v+W$ is also a line, and it is parallel to $W$. Thus, the cosets of $W$ in $\mathbf{R}^{2}$ are precisely all the lines parallel to $W$.

In the following theorem, we use the cosets of a subspace $W$ of a vector space $V$ to define a new vector space; it is called the quotient space of $V$ by $W$ and is denoted by $V / W$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-339}
\end{center}

Figure 10-2

THEOREM 10.15: $\quad$ Let $W$ be a subspace of a vector space over a field $K$. Then the cosets of $W$ in $V$ form a vector space over $K$ with the following operations of addition and scalar multiplication:

(i) $(u+w)+(v+W)=(u+v)+W$, (ii) $k(u+W)=k u+W$, where $k \in K$

We note that, in the proof of Theorem 10.15 (Problem 10.24), it is first necessary to show that the operations are well defined; that is, whenever $u+W=u^{\prime}+W$ and $v+W=v^{\prime}+W$, then\\
(i) $(u+v)+W=\left(u^{\prime}+v^{\prime}\right)+W$\\
and\\
(ii) $k u+W=k u^{\prime}+W \quad$ for any $k \in K$

In the case of an invariant subspace, we have the following useful result (proved in Problem 10.27).

THEOREM 10.16: Suppose $W$ is a subspace invariant under a linear operator $T: V \rightarrow V$. Then $T$ induces a linear operator $\bar{T}$ on $V / W$ defined by $\bar{T}(v+W)=T(v)+W$. Moreover, if $T$ is a zero of any polynomial, then so is $\bar{T}$. Thus, the minimal polynomial of $\bar{T}$ divides the minimal polynomial of $T$.

\section*{SOLVED PROBLEMS}
\section*{Invariant Subspaces}
10.1. Suppose $T: V \rightarrow V$ is linear. Show that each of the following is invariant under $T$ :\\
(a) $\{0\}$,\\
(b) $V$,\\
(c) kernel of $T$,\\
(d) image of $T$.

(a) We have $T(0)=0 \in\{0\}$; hence, $\{0\}$ is invariant under $T$.

(b) For every $v \in V, T(v) \in V$; hence, $V$ is invariant under $T$.

(c) Let $u \in \operatorname{Ker} T$. Then $T(u)=0 \in \operatorname{Ker} T$ because the kernel of $T$ is a subspace of $V$. Thus, Ker $T$ is invariant under $T$.

(d) Because $T(v) \in \operatorname{Im} T$ for every $v \in V$, it is certainly true when $v \in \operatorname{Im} T$. Hence, the image of $T$ is invariant under $T$.

10.2. Suppose $\left\{W_{i}\right\}$ is a collection of $T$-invariant subspaces of a vector space $V$. Show that the intersection $W=\bigcap_{i} W_{i}$ is also $T$-invariant.

Suppose $v \in W$; then $v \in W_{i}$ for every $i$. Because $W_{i}$ is $T$-invariant, $T(v) \in W_{i}$ for every $i$. Thus, $T(v) \in W$ and so $W$ is $T$-invariant.

10.3. Prove Theorem 10.2: Let $T: V \rightarrow V$ be linear. For any polynomial $f(t)$, the kernel of $f(T)$ is invariant under $T$.

Suppose $v \in \operatorname{Ker} f(T)$ - that is, $f(T)(v)=0$. We need to show that $T(v)$ also belongs to the kernel of $f(T)$-that is, $f(T)(T(v))=(f(T) \circ T)(v)=0$. Because $f(t) t=t f(t)$, we have $f(T) \circ T=T \circ f(T)$. Thus, as required,

$$
(f(T) \circ T)(v)=(T \circ f(T))(v)=T(f(T)(v))=T(0)=0
$$

10.4. Find all invariant subspaces of $A=\left[\begin{array}{ll}2 & -5 \\ 1 & -2\end{array}\right]$ viewed as an operator on $\mathbf{R}^{2}$.

By Problem 10.1, $\mathbf{R}^{2}$ and $\{0\}$ are invariant under $A$. Now if $A$ has any other invariant subspace, it must be one-dimensional. However, the characteristic polynomial of $A$ is

$$
\Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}+1
$$

Hence, $A$ has no eigenvalues (in $\mathbf{R}$ ) and so $A$ has no eigenvectors. But the one-dimensional invariant subspaces correspond to the eigenvectors; thus, $\mathbf{R}^{2}$ and $\{0\}$ are the only subspaces invariant under $A$.

10.5. Prove Theorem 10.3: Suppose $W$ is $T$-invariant. Then $T$ has a triangular block representation $\left[\begin{array}{ll}A & B \\ 0 & C\end{array}\right]$, where $A$ is the matrix representation of the restriction $\hat{T}$ of $T$ to $W$.

We choose a basis $\left\{w_{1}, \ldots, w_{r}\right\}$ of $W$ and extend it to a basis $\left\{w_{1}, \ldots, w_{r}, v_{1}, \ldots, v_{s}\right\}$ of $V$. We have

$$
\begin{aligned}
& \hat{T}\left(w_{1}\right)=T\left(w_{1}\right)=a_{11} w_{1}+\cdots+a_{1 r} w_{r} \\
& \hat{T}\left(w_{2}\right)=T\left(w_{2}\right)=a_{21} w_{1}+\cdots+a_{2 r} w_{r} \\
& \hat{T}\left(w_{r}\right)=T\left(w_{r}\right)=a_{r 1} w_{1}+\cdots+a_{r r} w_{r} \\
& T\left(v_{1}\right)=b_{11} w_{1}+\cdots+b_{1 r} w_{r}+c_{11} v_{1}+\cdots+c_{1 s} v_{s} \\
& T\left(v_{2}\right)=b_{21} w_{1}+\cdots+b_{2 r} w_{r}+c_{21} v_{1}+\cdots+c_{2 s} v_{s} \\
& T\left(v_{s}\right)=b_{s 1} w_{1}+\cdots+b_{s r} w_{r}+c_{s 1} v_{1}+\cdots+c_{s s} v_{s}
\end{aligned}
$$

But the matrix of $T$ in this basis is the transpose of the matrix of coefficients in the above system of equations (Section 6.2). Therefore, it has the form $\left[\begin{array}{cc}A & B \\ 0 & C\end{array}\right]$, where $A$ is the transpose of the matrix of coefficients for the obvious subsystem. By the same argument, $A$ is the matrix of $\hat{T}$ relative to the basis $\left\{w_{i}\right\}$ of $W$.

10.6. Let $\hat{T}$ denote the restriction of an operator $T$ to an invariant subspace $W$. Prove

(a) For any polynomial $f(t), f(\hat{T})(w)=f(T)(w)$.

(b) The minimal polynomial of $\hat{T}$ divides the minimal polynomial of $T$.

(a) If $f(t)=0$ or if $f(t)$ is a constant (i.e., of degree 1 ), then the result clearly holds.

Assume $\operatorname{deg} f=n>1$ and that the result holds for polynomials of degree less than $n$. Suppose that

$$
f(t)=a_{n} t^{n}+a_{n-1} t^{n-1}+\cdots+a_{1} t+a_{0}
$$

Then

$$
\begin{aligned}
f(\hat{T})(w) & =\left(a_{n} \hat{T}^{n}+a_{n-1} \hat{T}^{n-1}+\cdots+a_{0} I\right)(w) \\
& =\left(a_{n} \hat{T}^{n-1}\right)(\hat{T}(w))+\left(a_{n-1} \hat{T}^{n-1}+\cdots+a_{0} I\right)(w) \\
& =\left(a_{n} T^{n-1}\right)(T(w))+\left(a_{n-1} T^{n-1}+\cdots+a_{0} I\right)(w)=f(T)(w)
\end{aligned}
$$

(b) Let $m(t)$ denote the minimal polynomial of $T$. Then by (a), $m(\hat{T})(w)=m(T)(w)=\mathbf{0}(w)=0$ for every $w \in W$; that is, $\hat{T}$ is a zero of the polynomial $m(t)$. Hence, the minimal polynomial of $\hat{T}$ divides $m(t)$.

\section*{Invariant Direct-Sum Decompositions}
10.7. Prove Theorem 10.4: Suppose $W_{1}, W_{2}, \ldots, W_{r}$ are subspaces of $V$ with respective bases

$$
B_{1}=\left\{w_{11}, w_{12}, \ldots, w_{1 n_{1}}\right\}, \quad \ldots, \quad B_{r}=\left\{w_{r 1}, w_{r 2}, \ldots, w_{r n_{r}}\right\}
$$

Then $V$ is the direct sum of the $W_{i}$ if and only if the union $B=\bigcup_{i} B_{i}$ is a basis of $V$.

Suppose $B$ is a basis of $V$. Then, for any $v \in V$,

$$
v=a_{11} w_{11}+\cdots+a_{1 n_{1}} w_{1 n_{1}}+\cdots+a_{r 1} w_{r 1}+\cdots+a_{r n_{r}} w_{r n_{r}}=w_{1}+w_{2}+\cdots+w_{r}
$$

where $w_{i}=a_{i 1} w_{i 1}+\cdots+a_{i n_{i}} w_{i n_{i}} \in W_{i}$. We next show that such a sum is unique. Suppose

$$
v=w_{1}^{\prime}+w_{2}^{\prime}+\cdots+w_{r}^{\prime}, \quad \text { where } \quad w_{i}^{\prime} \in W_{i}
$$

Because $\left\{w_{i 1}, \ldots, w_{i n_{i}}\right\}$ is a basis of $W_{i}, w_{i}^{\prime}=b_{i 1} w_{i 1}+\cdots+b_{i n_{i}} w_{i n_{i}}$, and so

$$
v=b_{11} w_{11}+\cdots+b_{1 n_{1}} w_{1 n_{1}}+\cdots+b_{r 1} w_{r 1}+\cdots+b_{r n_{r}} w_{r n_{r}}
$$

Because $B$ is a basis of $V, a_{i j}=b_{i j}$, for each $i$ and each $j$. Hence, $w_{i}=w_{i}^{\prime}$, and so the sum for $v$ is unique. Accordingly, $V$ is the direct sum of the $W_{i}$.

Conversely, suppose $V$ is the direct sum of the $W_{i}$. Then for any $v \in V, v=w_{1}+\cdots+w_{r}$, where $w_{i} \in W_{i}$. Because $\left\{w_{i j_{i}}\right\}$ is a basis of $W_{i}$, each $w_{i}$ is a linear combination of the $w_{i j}$, and so $v$ is a linear combination of the elements of $B$. Thus, $B$ spans $V$. We now show that $B$ is linearly independent. Suppose

$$
a_{11} w_{11}+\cdots+a_{1 n_{1}} w_{1 n_{1}}+\cdots+a_{r 1} w_{r 1}+\cdots+a_{r n_{r}} w_{r n_{r}}=0
$$

Note that $a_{i 1} w_{i 1}+\cdots+a_{i n_{i}} w_{i n_{i}} \in W_{i}$. We also have that $0=0+0 \cdots 0 \in W_{i}$. Because such a sum for 0 is unique,

$$
a_{i 1} w_{i 1}+\cdots+a_{i n_{i}} w_{i n_{i}}=0 \quad \text { for } i=1, \ldots, r
$$

The independence of the bases $\left\{w_{i j}\right\}$ implies that all the $a$ 's are 0 . Thus, $B$ is linearly independent and is a basis of $V$.

10.8. Suppose $T: V \rightarrow V$ is linear and suppose $T=T_{1} \oplus T_{2}$ with respect to a $T$-invariant direct-sum decomposition $V=U \oplus W$. Show that

(a) $m(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$, where $m(t), m_{1}(t), m_{2}(t)$ are the minimum polynomials of $T, T_{1}, T_{2}$, respectively.

(b) $\Delta(t)=\Delta_{1}(t) \Delta_{2}(t)$, where $\Delta(t), \Delta_{1}(t), \Delta_{2}(t)$ are the characteristic polynomials of $T, T_{1}, T_{2}$, respectively.

(a) By Problem 10.6, each of $m_{1}(t)$ and $m_{2}(t)$ divides $m(t)$. Now suppose $f(t)$ is a multiple of both $m_{1}(t)$ and $m_{2}(t)$, then $f\left(T_{1}\right)(U)=0$ and $f\left(T_{2}\right)(W)=0$. Let $v \in V$, then $v=u+w$ with $u \in U$ and $w \in W$. Now

$$
f(T) v=f(T) u+f(T) w=f\left(T_{1}\right) u+f\left(T_{2}\right) w=0+0=0
$$

That is, $T$ is a zero of $f(t)$. Hence, $m(t)$ divides $f(t)$, and so $m(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$. (b) By Theorem $10.5, T$ has a matrix representation $M=\left[\begin{array}{ll}A & 0 \\ 0 & B\end{array}\right]$, where $A$ and $B$ are matrix representations\\
of $T_{1}$ and $T_{2}$, respectively. Then, as required,

$$
\Delta(t)=|t I-M|=\left|\begin{array}{cc}
t I-A & 0 \\
0 & t I-B
\end{array}\right|=|t I-A||t I-B|=\Delta_{1}(t) \Delta_{2}(t)
$$

10.9. Prove Theorem 10.7: Suppose $T: V \rightarrow V$ is linear, and suppose $f(t)=g(t) h(t)$ are polynomials such that $f(T)=\mathbf{0}$ and $g(t)$ and $h(t)$ are relatively prime. Then $V$ is the direct sum of the $T$-invariant subspaces $U$ and $W$ where $U=\operatorname{Ker} g(T)$ and $W=\operatorname{Ker} h(T)$.

Note first that $U$ and $W$ are $T$-invariant by Theorem 10.2. Now, because $g(t)$ and $h(t)$ are relatively prime, there exist polynomials $r(t)$ and $s(t)$ such that


\begin{equation*}
r(t) g(t)+s(t) h(t)=1 \tag{*}
\end{equation*}


Hence, for the operator $T, \quad r(T) g(T)+s(T) h(T)=I$

Let $v \in V$; then, by $\left(^{*}\right), \quad v=r(T) g(T) v+s(T) h(T) v$

But the first term in this sum belongs to $W=\operatorname{Ker} h(T)$, because

$$
h(T) r(T) g(T) v=r(T) g(T) h(T) v=r(T) f(T) v=r(T) \mathbf{0} v=0
$$

Similarly, the second term belongs to $U$. Hence, $V$ is the sum of $U$ and $W$.

To prove that $V=U \oplus W$, we must show that a sum $v=u+w$ with $u \in U, w \in W$, is uniquely determined by $v$. Applying the operator $r(T) g(T)$ to $v=u+w$ and using $g(T) u=0$, we obtain

$$
r(T) g(T) v=r(T) g(T) u+r(T) g(T) w=r(T) g(T) w
$$

Also, applying (*) to $w$ alone and using $h(T) w=0$, we obtain

$$
w=r(T) g(T) w+s(T) h(T) w=r(T) g(T) w
$$

Both of the above formulas give us $w=r(T) g(T) v$, and so $w$ is uniquely determined by $v$. Similarly $u$ is uniquely determined by $v$. Hence, $V=U \oplus W$, as required.

10.10. Prove Theorem 10.8: In Theorem 10.7 (Problem 10.9), if $f(t)$ is the minimal polynomial of $T$ (and $g(t)$ and $h(t)$ are monic), then $g(t)$ is the minimal polynomial of the restriction $T_{1}$ of $T$ to $U$ and $h(t)$ is the minimal polynomial of the restriction $T_{2}$ of $T$ to $W$.

Let $m_{1}(t)$ and $m_{2}(t)$ be the minimal polynomials of $T_{1}$ and $T_{2}$, respectively. Note that $g\left(T_{1}\right)=0$ and $h\left(T_{2}\right)=0$ because $U=\operatorname{Ker} g(T)$ and $W=\operatorname{Ker} h(T)$. Thus,


\begin{equation*}
m_{1}(t) \text { divides } g(t) \quad \text { and } \quad m_{2}(t) \text { divides } h(t) \tag{1}
\end{equation*}


By Problem 10.9, $f(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$. But $m_{1}(t)$ and $m_{2}(t)$ are relatively prime because $g(t)$ and $h(t)$ are relatively prime. Accordingly, $f(t)=m_{1}(t) m_{2}(t)$. We also have that $f(t)=g(t) h(t)$. These two equations together with (1) and the fact that all the polynomials are monic imply that $g(t)=m_{1}(t)$ and $h(t)=m_{2}(t)$, as required.

10.11. Prove the Primary Decomposition Theorem 10.6: Let $T: V \rightarrow V$ be a linear operator with minimal polynomial

$$
m(t)=f_{1}(t)^{n_{1}} f_{2}(t)^{n_{2}} \ldots f_{r}(t)^{n_{r}}
$$

where the $f_{i}(t)$ are distinct monic irreducible polynomials. Then $V$ is the direct sum of $T$ invariant subspaces $W_{1}, \ldots, W_{r}$ where $W_{i}$ is the kernel of $f_{i}(T)^{n_{i}}$. Moreover, $f_{i}(t)^{n_{i}}$ is the minimal polynomial of the restriction of $T$ to $W_{i}$.

The proof is by induction on $r$. The case $r=1$ is trivial. Suppose that the theorem has been proved for $r-1$. By Theorem 10.7, we can write $V$ as the direct sum of $T$-invariant subspaces $W_{1}$ and $V_{1}$, where $W_{1}$ is the kernel of $f_{1}(T)^{n_{1}}$ and where $V_{1}$ is the kernel of $f_{2}(T)^{n_{2}} \cdots f_{r}(T)^{n_{r}}$. By Theorem 10.8, the minimal polynomials of the restrictions of $T$ to $W_{1}$ and $V_{1}$ are $f_{1}(t)^{n_{1}}$ and $f_{2}(t)^{n_{2}} \cdots f_{r}(t)^{n_{r}}$, respectively.

Denote the restriction of $T$ to $V_{1}$ by $\hat{T}_{1}$. By the inductive hypothesis, $V_{1}$ is the direct sum of subspaces $W_{2}, \ldots, W_{r}$ such that $W_{i}$ is the kernel of $f_{i}\left(T_{1}\right)^{n_{i}}$ and such that $f_{i}(t)^{n_{i}}$ is the minimal polynomial for the restriction of $\hat{T}_{1}$ to $W_{i}$. But the kernel of $f_{i}(T)^{n_{i}}$, for $i=2, \ldots, r$ is necessarily contained in $V_{1}$, because $f_{i}(t)^{n_{i}}$ divides $f_{2}(t)^{n_{2}} \cdots f_{r}(t)^{n_{r}}$. Thus, the kernel of $f_{i}(T)^{n_{i}}$ is the same as the kernel of $f_{i}\left(T_{1}\right)^{n_{i}}$, which is $W_{i}$. Also, the restriction of $T$ to $W_{i}$ is the same as the restriction of $\hat{T}_{1}$ to $W_{i}$ (for $\left.i=2, \ldots, r\right)$; hence, $f_{i}(t)^{n_{i}}$ is also the minimal polynomial for the restriction of $T$ to $W_{i}$. Thus, $V=W_{1} \oplus W_{2} \oplus \cdots \oplus W_{r}$ is the desired decomposition of $T$.

10.12. Prove Theorem 10.9: A linear operator $T: V \rightarrow V$ has a diagonal matrix representation if and only if its minimal polynomal $m(t)$ is a product of distinct linear polynomials.

Suppose $m(t)$ is a product of distinct linear polynomials, say,

$$
m(t)=\left(t-\lambda_{1}\right)\left(t-\lambda_{2}\right) \cdots\left(t-\lambda_{r}\right)
$$

where the $\lambda_{i}$ are distinct scalars. By the Primary Decomposition Theorem, $V$ is the direct sum of subspaces $W_{1}, \ldots, W_{r}$, where $W_{i}=\operatorname{Ker}\left(T-\lambda_{i} I\right)$. Thus, if $v \in W_{i}$, then $\left(T-\lambda_{i} I\right)(v)=0$ or $T(v)=\lambda_{i} v$. In other words, every vector in $W_{i}$ is an eigenvector belonging to the eigenvalue $\lambda_{i}$. By Theorem 10.4, the union of bases for $W_{1}, \ldots, W_{r}$ is a basis of $V$. This basis consists of eigenvectors, and so $T$ is diagonalizable.

Conversely, suppose $T$ is diagonalizable (i.e., $V$ has a basis consisting of eigenvectors of $T$ ). Let $\lambda_{1}, \ldots, \lambda_{s}$ be the distinct eigenvalues of $T$. Then the operator

$$
f(T)=\left(T-\lambda_{1} I\right)\left(T-\lambda_{2} I\right) \cdots\left(T-\lambda_{s} I\right)
$$

maps each basis vector into 0 . Thus, $f(T)=0$, and hence, the minimal polynomial $m(t)$ of $T$ divides the polynomial

$$
f(t)=\left(t-\lambda_{1}\right)\left(t-\lambda_{2}\right) \cdots\left(t-\lambda_{s} I\right)
$$

Accordingly, $m(t)$ is a product of distinct linear polynomials.

\section*{Nilpotent Operators, Jordan Canonical Form}
10.13. Let $T: V$ be linear. Suppose, for $v \in V, T^{k}(v)=0$ but $T^{k-1}(v) \neq 0$. Prove

(a) The set $S=\left\{v, T(v), \ldots, T^{k-1}(v)\right\}$ is linearly independent.

(b) The subspace $W$ generated by $S$ is $T$-invariant.

(c) The restriction $\hat{T}$ of $T$ to $W$ is nilpotent of index $k$.

(d) Relative to the basis $\left\{T^{k-1}(v), \ldots, T(v), v\right\}$ of $W$, the matrix of $T$ is the $k$-square Jordan nilpotent block $N_{k}$ of index $k$ (see Example 10.5).

(a) Suppose


\begin{equation*}
a v+a_{1} T(v)+a_{2} T^{2}(v)+\cdots+a_{k-1} T^{k-1}(v)=0 \tag{*}
\end{equation*}


Applying $T^{k-1}$ to $\left(^{*}\right)$ and using $T^{k}(v)=0$, we obtain $a T^{k-1}(v)=0$; because $T^{k-1}(v) \neq 0, a=0$. Now applying $T^{k-2}$ to $\left(^{*}\right)$ and using $T^{k}(v)=0$ and $a=0$, we find $a_{1} T^{k-1}(v)=0$; hence, $a_{1}=0$. Next applying $T^{k-3}$ to $\left(^{*}\right)$ and using $T^{k}(v)=0$ and $a=a_{1}=0$, we obtain $a_{2} T^{k-1}(v)=0$; hence, $a_{2}=0$. Continuing this process, we find that all the $a^{\prime}$ s are 0 ; hence, $S$ is independent.

(b) Let $v \in W$. Then

$$
v=b v+b_{1} T(v)+b_{2} T^{2}(v)+\cdots+b_{k-1} T^{k-1}(v)
$$

Using $T^{k}(v)=0$, we have

$$
T(v)=b T(v)+b_{1} T^{2}(v)+\cdots+b_{k-2} T^{k-1}(v) \in W
$$

Thus, $W$ is $T$-invariant.

(c) By hypothesis, $T^{k}(v)=0$. Hence, for $i=0, \ldots, k-1$,

$$
\hat{T}^{k}\left(T^{i}(v)\right)=T^{k+i}(v)=0
$$

That is, applying $\hat{T}^{k}$ to each generator of $W$, we obtain 0 ; hence, $\hat{T}^{k}=\mathbf{0}$ and so $\hat{T}$ is nilpotent of index at most $k$. On the other hand, $\hat{T}^{k-1}(v)=T^{k-1}(v) \neq 0$; hence, $T$ is nilpotent of index exactly $k$.

(d) For the basis $\left\{T^{k-1}(v), T^{k-2}(v), \ldots, T(v), v\right\}$ of $W$,

$$
\begin{aligned}
& \hat{T}\left(T^{k-1}(v)\right)=T^{k}(v)=0 \\
& \hat{T}\left(T^{k-2}(v)\right)=T^{k-1}(v) \\
& \hat{T}\left(T^{k-3}(v)\right)=\quad T^{k-2}(v) \\
& \begin{array}{llll}
\hat{T}(T(v)) & = & T^{2}(v) & \\
\hat{T}(v) & = & T(v)
\end{array}
\end{aligned}
$$

Hence, as required, the matrix of $T$ in this basis is the $k$-square Jordan nilpotent block $N_{k}$.

10.14. Let $T: V \rightarrow V$ be linear. Let $U=\operatorname{Ker} T^{i}$ and $W=\operatorname{Ker} T^{i+1}$. Show that\\
(a) $U \subseteq W$,\\
(b) $T(W) \subseteq U$.

(a) Suppose $u \in U=\operatorname{Ker} T^{i}$. Then $T^{i}(u)=0 \quad$ and so $T^{i+1}(u)=T\left(T^{i}(u)\right)=T(0)=0$. Thus, $u \in \operatorname{Ker} T^{i+1}=W$. But this is true for every $u \in U$; hence, $U \subseteq W$.

(b) Similarly, if $w \in W=\operatorname{Ker} T^{i+1}$, then $T^{i+1}(w)=0$. Thus, $T^{i+1}(w)=T^{i}(T(w))=T^{i}(0)=0$ and so $T(W) \subseteq U$.

10.15. Let $T: V$ be linear. Let $X=\operatorname{Ker} T^{i-2}, Y=\operatorname{Ker} T^{i-1}, Z=\operatorname{Ker} T^{i}$. Therefore (Problem 10.14), $X \subseteq Y \subseteq Z$. Suppose

$$
\left\{u_{1}, \ldots, u_{r}\right\}, \quad\left\{u_{1}, \ldots, u_{r}, v_{1}, \ldots, v_{s}\right\}, \quad\left\{u_{1}, \ldots, u_{r}, v_{1}, \ldots, v_{s}, w_{1}, \ldots, w_{t}\right\}
$$

are bases of $X, Y, Z$, respectively. Show that

$$
S=\left\{u_{1}, \ldots, u_{r}, T\left(w_{1}\right), \ldots, T\left(w_{t}\right)\right\}
$$

is contained in $Y$ and is linearly independent.

By Problem 10.14, $T(Z) \subseteq Y$, and hence $S \subseteq Y$. Now suppose $S$ is linearly dependent. Then there exists a relation

$$
a_{1} u_{1}+\cdots+a_{r} u_{r}+b_{1} T\left(w_{1}\right)+\cdots+b_{t} T\left(w_{t}\right)=0
$$

where at least one coefficient is not zero. Furthermore, because $\left\{u_{i}\right\}$ is independent, at least one of the $b_{k}$ must be nonzero. Transposing, we find

Hence,

$$
b_{1} T\left(w_{1}\right)+\cdots+b_{t} T\left(w_{t}\right)=-a_{1} u_{1}-\cdots-a_{r} u_{r} \in X=\operatorname{Ker} T^{i-2}
$$

Thus,

$$
T^{i-2}\left(b_{1} T\left(w_{1}\right)+\cdots+b_{t} T\left(w_{t}\right)\right)=0
$$

Because $\left\{u_{i}, v_{j}\right\}$ generates $Y$, we obtain a relation among the $u_{i}, v_{i}, w_{k}$ where one of the coefficients (i.e., one of the $b_{k}$ ) is not zero. This contradicts the fact that $\left\{u_{i}, v_{j}, w_{k}\right\}$ is independent. Hence, $S$ must also be independent.

10.16. Prove Theorem 10.10: Let $T: V \rightarrow V$ be a nilpotent operator of index $k$. Then $T$ has a unique block diagonal matrix representation consisting of Jordan nilpotent blocks $N$. There is at least one $N$ of order $k$, and all other $N$ are of orders $\leq k$. The total number of $N$ of all orders is equal to the nullity of $T$.

Suppose $\operatorname{dim} V=n$. Let $W_{1}=\operatorname{Ker} T, W_{2}=\operatorname{Ker} T^{2}, \ldots, W_{k}=\operatorname{Ker} T^{k}$. Let us set $m_{i}=\operatorname{dim} W_{i}$, for $i=1, \ldots, k$. Because $T$ is of index $k, W_{k}=V$ and $W_{k-1} \neq V$ and so $m_{k-1}<m_{k}=n$. By Problem 10.14,

$$
W_{1} \subseteq W_{2} \subseteq \cdots \subseteq W_{k}=V
$$

Thus, by induction, we can choose a basis $\left\{u_{1}, \ldots, u_{n}\right\}$ of $V$ such that $\left\{u_{1}, \ldots, u_{m_{i}}\right\}$ is a basis of $W_{i}$.

We now choose a new basis for $V$ with respect to which $T$ has the desired form. It will be convenient to label the members of this new basis by pairs of indices. We begin by setting

$$
v(1, k)=u_{m_{k-1}+1}, \quad v(2, k)=u_{m_{k-1}+2}, \quad \ldots, \quad v\left(m_{k}-m_{k-1}, k\right)=u_{m_{k}}
$$

and setting

$$
v(1, k-1)=T v(1, k), \quad v(2, k-1)=T v(2, k), \quad \ldots, \quad v\left(m_{k}-m_{k-1}, k-1\right)=T v\left(m_{k}-m_{k-1}, k\right)
$$

By the preceding problem,

$$
S_{1}=\left\{u_{1} \ldots, u_{m_{k-2}}, v(1, k-1), \ldots, v\left(m_{k}-m_{k-1}, k-1\right)\right\}
$$

is a linearly independent subset of $W_{k-1}$. We extend $S_{1}$ to a basis of $W_{k-1}$ by adjoining new elements (if necessary), which we denote by

$$
v\left(m_{k}-m_{k-1}+1, k-1\right), \quad v\left(m_{k}-m_{k-1}+2, k-1\right), \quad \ldots, \quad v\left(m_{k-1}-m_{k-2}, k-1\right)
$$

Next we set

$$
\begin{array}{rr}
v(1, k-2)=T v(1, k-1), & v(2, k-2)=T v(2, k-1), \\
v\left(m_{k-1}-m_{k-2}, k-2\right)=T v\left(m_{k-1}-m_{k-2}, k-1\right)
\end{array}
$$

Again by the preceding problem,

$$
S_{2}=\left\{u_{1}, \ldots, u_{m_{k-s}}, v(1, k-2), \ldots, v\left(m_{k-1}-m_{k-2}, k-2\right)\right\}
$$

is a linearly independent subset of $W_{k-2}$, which we can extend to a basis of $W_{k-2}$ by adjoining elements

$$
v\left(m_{k-1}-m_{k-2}+1, k-2\right), \quad v\left(m_{k-1}-m_{k-2}+2, k-2\right), \quad \ldots, \quad v\left(m_{k-2}-m_{k-3}, k-2\right)
$$

Continuing in this manner, we get a new basis for $V$, which for convenient reference we arrange as follows:

$$
\begin{aligned}
& v(1, k) \quad \ldots, v\left(m_{k}-m_{k-1}, k\right) \\
& v(1, k-1), \quad \ldots, v\left(m_{k}-m_{k-1}, k-1\right) \quad \ldots, v\left(m_{k-1}-m_{k-2}, k-1\right) \\
& v(1,2), \quad \ldots, v\left(m_{k}-m_{k-1}, 2\right), \quad \ldots, v\left(m_{k-1}-m_{k-2}, 2\right), \quad \ldots, v\left(m_{2}-m_{1}, 2\right) \\
& v(1,1), \quad \ldots, v\left(m_{k}-m_{k-1}, 1\right), \quad \ldots, v\left(m_{k-1}-m_{k-2}, 1\right), \quad \ldots, v\left(m_{2}-m_{1}, 1\right), \quad \ldots, v\left(m_{1}, 1\right)
\end{aligned}
$$

The bottom row forms a basis of $W_{1}$, the bottom two rows form a basis of $W_{2}$, and so forth. But what is important for us is that $T$ maps each vector into the vector immediately below it in the table or into 0 if the vector is in the bottom row. That is,

$$
T v(i, j)= \begin{cases}v(i, j-1) & \text { for } j>1 \\ 0 & \text { for } j=1\end{cases}
$$

Now it is clear [see Problem 10.13(d)] that $T$ will have the desired form if the $v(i, j)$ are ordered lexicographically: beginning with $v(1,1)$ and moving up the first column to $v(1, k)$, then jumping to $v(2,1)$ and moving up the second column as far as possible.

Moreover, there will be exactly $m_{k}-m_{k-1}$ diagonal entries of order $k$. Also, there will be

$$
\begin{aligned}
& \left(m_{k-1}-m_{k-2}\right)-\left(m_{k}-m_{k-1}\right)=2 m_{k-1}-m_{k}-m_{k-2} \text { diagonal entries of order } k-1 \\
& 2 m_{2}-m_{1}-m_{3} \quad \text { diagonal entries of order } 2 \\
& 2 m_{1}-m_{2} \quad \text { diagonal entries of order } 1
\end{aligned}
$$

as can be read off directly from the table. In particular, because the numbers $m_{1}, \ldots, m_{k}$ are uniquely determined by $T$, the number of diagonal entries of each order is uniquely determined by $T$. Finally, the identity

$$
m_{1}=\left(m_{k}-m_{k-1}\right)+\left(2 m_{k-1}-m_{k}-m_{k-2}\right)+\cdots+\left(2 m_{2}-m_{1}-m_{3}\right)+\left(2 m_{1}-m_{2}\right)
$$

shows that the nullity $m_{1}$ of $T$ is the total number of diagonal entries of $T$.

10.17. Let $A=\left[\begin{array}{lllll}0 & 1 & 1 & 0 & 1 \\ 0 & 0 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0\end{array}\right]$ and $B=\left[\begin{array}{lllll}0 & 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0\end{array}\right]$. The reader can verify that $A$ and $B$ are both nilpotent of index 3 ; that is, $A^{3}=0$ but $A^{2} \neq 0$, and $B^{3}=0$ but $B^{2} \neq 0$. Find the nilpotent matrices $M_{A}$ and $M_{B}$ in canonical form that are similar to $A$ and $B$, respectively.

Because $A$ and $B$ are nilpotent of index $3, M_{A}$ and $M_{B}$ must each contain a Jordan nilpotent block of order 3 , and none greater then 3 . Note that $\operatorname{rank}(A)=2 \operatorname{and} \operatorname{rank}(B)=3$, so nullity $(A)=5-2=3$ and nullity $(B)=5-3=2$. Thus, $M_{A}$ must contain three diagonal blocks, which must be one of order 3 and two of order 1; and $M_{B}$ must contain two diagonal blocks, which must be one of order 3 and one of order 2 . Namely,

$$
M_{A}=\left[\begin{array}{lllll}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}\right] \quad \text { and } \quad M_{B}=\left[\begin{array}{ccccc}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0
\end{array}\right]
$$

10.18. Prove Theorem 10.11 on the Jordan canonical form for an operator $T$.

By the primary decomposition theorem, $T$ is decomposable into operators $T_{1}, \ldots, T_{r}$; that is, $T=T_{1} \oplus \cdots \oplus T_{r}$, where $\left(t-\lambda_{i}\right)^{m_{i}}$ is the minimal polynomial of $T_{i}$. Thus, in particular,

$$
\left(T_{1}-\lambda_{1} I\right)^{m_{1}}=\mathbf{0}, \ldots, \quad\left(T_{r}-\lambda_{r} I\right)^{m_{r}}=\mathbf{0}
$$

Set $N_{i}=T_{i}-\lambda_{i} I$. Then, for $i=1, \ldots, r$,

$$
T_{i}=N_{i}+\lambda_{i} I, \quad \text { where } \quad N_{i}^{m^{i}}=\mathbf{0}
$$

That is, $T_{i}$ is the sum of the scalar operator $\lambda_{i} I$ and a nilpotent operator $N_{i}$, which is of index $m_{i}$ because $\left(t-\lambda_{i}\right)_{i}^{m}$ is the minimal polynomial of $T_{i}$.

Now, by Theorem 10.10 on nilpotent operators, we can choose a basis so that $N_{i}$ is in canonical form. In this basis, $T_{i}=N_{i}+\lambda_{i} I$ is represented by a block diagonal matrix $M_{i}$ whose diagonal entries are the matrices $J_{i j}$. The direct sum $J$ of the matrices $M_{i}$ is in Jordan canonical form and, by Theorem 10.5, is a matrix representation of $T$.

Last, we must show that the blocks $J_{i j}$ satisfy the required properties. Property (i) follows from the fact that $N_{i}$ is of index $m_{i}$. Property (ii) is true because $T$ and $J$ have the same characteristic polynomial. Property (iii) is true because the nullity of $N_{i}=T_{i}-\lambda_{i} I$ is equal to the geometric multiplicity of the eigenvalue $\lambda_{i}$. Property (iv) follows from the fact that the $T_{i}$ and hence the $N_{i}$ are uniquely determined by $T$.

10.19. Determine all possible Jordan canonical forms $J$ for a linear operator $T: V \rightarrow V$ whose characteristic polynomial $\Delta(t)=(t-2)^{5}$ and whose minimal polynomial $m(t)=(t-2)^{2}$.

$J$ must be a $5 \times 5$ matrix, because $\Delta(t)$ has degree 5 , and all diagonal elements must be 2 , because 2 is the only eigenvalue. Moreover, because the exponent of $t-2$ in $m(t)$ is $2, J$ must have one Jordan block of order 2, and the others must be of order 2 or 1 . Thus, there are only two possibilities:

$$
J=\operatorname{diag}\left(\left[\begin{array}{ll}
2 & 1 \\
& 2
\end{array}\right],\left[\begin{array}{ll}
2 & 1 \\
& 2
\end{array}\right],[2]\right) \quad \text { or } \quad J=\operatorname{diag}\left(\left[\begin{array}{ll}
2 & 1 \\
& 2
\end{array}\right],[2],[2],[2]\right)
$$

10.20. Determine all possible Jordan canonical forms for a linear operator $T: V \rightarrow V$ whose characteristic polynomial $\Delta(t)=(t-2)^{3}(t-5)^{2}$. In each case, find the minimal polynomial $m(t)$.

Because $t-2$ has exponent 3 in $\Delta(t), 2$ must appear three times on the diagonal. Similarly, 5 must appear twice. Thus, there are six possibilities:\\
(a) $\operatorname{diag}\left(\left[\begin{array}{lll}2 & 1 & \\ & 2 & 1 \\ & & 2\end{array}\right],\left[\begin{array}{ll}5 & 1 \\ & 5\end{array}\right]\right)$,\\
(b) $\operatorname{diag}\left(\left[\begin{array}{lll}2 & 1 & \\ & 2 & 1 \\ & & 2\end{array}\right],[5],[5]\right)$,\\
(c) $\operatorname{diag}\left(\left[\begin{array}{ll}2 & 1 \\ & 2\end{array}\right],[2],\left[\begin{array}{ll}5 & 1 \\ & 5\end{array}\right]\right)$,\\
(d) $\operatorname{diag}\left(\left[\begin{array}{ll}2 & 1 \\ & 2\end{array}\right],[2],[5],[5]\right)$,\\
(e) $\operatorname{diag}\left([2],[2],[2],\left[\begin{array}{ll}5 & 1 \\ & 5\end{array}\right]\right)$,\\
(f) $\operatorname{diag}([2],[2],[2],[5], \quad[5])$

The exponent in the minimal polynomial $m(t)$ is equal to the size of the largest block. Thus,\\
(a) $m(t)=(t-2)^{3}(t-5)^{2}$,\\
(b) $m(t)=(t-2)^{3}(t-5)$,\\
(c) $m(t)=(t-2)^{2}(t-5)^{2}$,\\
(d) $m(t)=(t-2)^{2}(t-5)$,\\
(e) $m(t)=(t-2)(t-5)^{2}$\\
(f) $m(t)=(t-2)(t-5)$

\section*{Quotient Space and Triangular Form}
10.21. Let $W$ be a subspace of a vector space $V$. Show that the following are equivalent:\\
(i) $u \in v+W$,\\
(ii) $u-v \in W$,\\
(iii) $v \in u+W$.

Suppose $u \in v+W$. Then there exists $w_{0} \in W$ such that $u=v+w_{0}$. Hence, $u-v=w_{0} \in W$. Conversely, suppose $u-v \in W$. Then $u-v=w_{0}$ where $w_{0} \in W$. Hence, $u=v+w_{0} \in v+W$. Thus, (i) and (ii) are equivalent.

We also have $u-v \in W$ iff $-(u-v)=v-u \in W$ iff $v \in u+W$. Thus, (ii) and (iii) are also equivalent.

10.22. Prove the following: The cosets of $W$ in $V$ partition $V$ into mutually disjoint sets. That is,

(a) Any two cosets $u+W$ and $v+W$ are either identical or disjoint.

(b) Each $v \in V$ belongs to a coset; in fact, $v \in v+W$.

Furthermore, $u+W=v+W$ if and only if $u-v \in W$, and so $(v+w)+W=v+W$ for any $w \in W$.

Let $v \in V$. Because $0 \in W$, we have $v=v+0 \in v+W$, which proves (b).

Now suppose the cosets $u+W$ and $v+W$ are not disjoint; say, the vector $x$ belongs to both $u+W$ and $v+W$. Then $u-x \in W$ and $x-v \in W$. The proof of (a) is complete if we show that $u+W=v+W$. Let $u+w_{0}$ be any element in the coset $u+W$. Because $u-x, x-v$, $w_{0}$ belongs to $W$,

$$
\left(u+w_{0}\right)-v=(u-x)+(x-v)+w_{0} \in W
$$

Thus, $u+w_{0} \in v+W$, and hence the cost $u+W$ is contained in the coset $v+W$. Similarly, $v+W$ is contained in $u+W$, and so $u+W=v+W$.

The last statement follows from the fact that $u+W=v+W$ if and only if $u \in v+W$, and, by Problem 10.21, this is equivalent to $u-v \in W$.

10.23. Let $W$ be the solution space of the homogeneous equation $2 x+3 y+4 z=0$. Describe the cosets of $W$ in $\mathbf{R}^{3}$.

$W$ is a plane through the origin $O=(0,0,0)$, and the cosets of $W$ are the planes parallel to $W$. Equivalently, the cosets of $W$ are the solution sets of the family of equations

$$
2 x+3 y+4 z=k, \quad k \in \mathbf{R}
$$

In fact, the coset $v+W$, where $v=(a, b, c)$, is the solution set of the linear equation

$$
2 x+3 y+4 z=2 a+3 b+4 c \quad \text { or } \quad 2(x-a)+3(y-b)+4(z-c)=0
$$

10.24. Suppose $W$ is a subspace of a vector space $V$. Show that the operations in Theorem 10.15 are well defined; namely, show that if $u+W=u^{\prime}+W$ and $v+W=v^{\prime}+W$, then

(a) $(u+v)+W=\left(u^{\prime}+v^{\prime}\right)+W \quad$ and $\quad$ (b) $k u+W=k u^{\prime}+W \quad$ for any $k \in K$

(a) Because $u+W=u^{\prime}+W$ and $v+W=v^{\prime}+W$, both $u-u^{\prime}$ and $v-v^{\prime}$ belong to $W$. But then $(u+v)-\left(u^{\prime}+v^{\prime}\right)=\left(u-u^{\prime}\right)+\left(v-v^{\prime}\right) \in W$. Hence, $(u+v)+W=\left(u^{\prime}+v^{\prime}\right)+W$.

(b) Also, because $u-u^{\prime} \in W$ implies $k\left(u-u^{\prime}\right) \in W$, then $k u-k u^{\prime}=k\left(u-u^{\prime}\right) \in W$; accordingly, $k u+W=k u^{\prime}+W$.

10.25. Let $V$ be a vector space and $W$ a subspace of $V$. Show that the natural map $\eta: V \rightarrow V / W$, defined by $\eta(v)=v+W$, is linear.

For any $u, v \in V$ and any $k \in K$, we have

and

$$
n(u+v)=u+v+W=u+W+v+W=\eta(u)+\eta(v)
$$

Accordingly, $\eta$ is linear.

10.26. Let $W$ be a subspace of a vector space $V$. Suppose $\left\{w_{1}, \ldots, w_{r}\right\}$ is a basis of $W$ and the set of cosets $\left\{\bar{v}_{1}, \ldots, \bar{v}_{s}\right\}$, where $\bar{v}_{j}=v_{j}+W$, is a basis of the quotient space. Show that the set of vectors $B=\left\{v_{1}, \ldots, v_{s}, w_{1}, \ldots, w_{r}\right\}$ is a basis of $V$. Thus, $\operatorname{dim} V=\operatorname{dim} W+\operatorname{dim}(V / W)$.

Suppose $u \in V$. Because $\left\{\bar{v}_{j}\right\}$ is a basis of $V / W$,

$$
\bar{u}=u+W=a_{1} \bar{v}_{1}+a_{2} \bar{v}_{2}+\cdots+a_{s} \bar{v}_{s}
$$

Hence, $u=a_{1} v_{1}+\cdots+a_{s} v_{s}+w$, where $w \in W$. Since $\left\{w_{i}\right\}$ is a basis of $W$,

$$
u=a_{1} v_{1}+\cdots+a_{s} v_{s}+b_{1} w_{1}+\cdots+b_{r} w_{r}
$$

Accordingly, $B$ spans $V$.

We now show that $B$ is linearly independent. Suppose


\begin{gather*}
 \tag{1}\\
\text { Then }
\end{gather*} \begin{gather*}
c_{1} v_{1}+\cdots+c_{s} v_{s}+d_{1} w_{1}+\cdots+d_{r} w_{r}=0 \\
c_{1} \bar{v}_{1}+\cdots+c_{s} \bar{v}_{s}=\overline{0}=W
\end{gather*}


$$
c_{1} \bar{v}_{1}+\cdots+c_{s} \bar{v}_{s}=\overline{0}=W
$$

Because $\left\{\bar{v}_{j}\right\}$ is independent, the $c$ 's are all 0 . Substituting into (1), we find $d_{1} w_{1}+\cdots+d_{r} w_{r}=0$. Because $\left\{w_{i}\right\}$ is independent, the $d$ 's are all 0 . Thus, $B$ is linearly independent and therefore a basis of $V$.

10.27. Prove Theorem 10.16: Suppose $W$ is a subspace invariant under a linear operator $T: V \rightarrow V$. Then $T$ induces a linear operator $\bar{T}$ on $V / W$ defined by $\bar{T}(v+W)=T(v)+W$. Moreover, if $T$ is a zero of any polynomial, then so is $\bar{T}$. Thus, the minimal polynomial of $\bar{T}$ divides the minimal polynomial of $T$.

We first show that $\bar{T}$ is well defined; that is, if $u+W=v+W$, then $\bar{T}(u+W)=\bar{T}(v+W)$. If $u+W=v+W$, then $u-v \in W$, and, as $W$ is $T$-invariant, $T(u-v)=T(u)-T(v) \in W$. Accordingly,

$$
\bar{T}(u+W)=T(u)+W=T(v)+W=\bar{T}(v+W)
$$

as required.

We next show that $\bar{T}$ is linear. We have

$$
\begin{aligned}
\bar{T}((u+W)+(v+W)) & =\bar{T}(u+v+W)=T(u+v)+W=T(u)+T(v)+W \\
& =T(u)+W+T(v)+W=\bar{T}(u+W)+\bar{T}(v+W)
\end{aligned}
$$

Furthermore,

$$
\bar{T}(k(u+W))=\bar{T}(k u+W)=T(k u)+W=k T(u)+W=k(T(u)+W)=k \hat{T}(u+W)
$$

Thus, $\bar{T}$ is linear.

Now, for any coset $u+W$ in $V / W$,

$$
\overline{T^{2}}(u+W)=T^{2}(u)+W=T(T(u))+W=\bar{T}(T(u)+W)=\bar{T}(\bar{T}(u+W))=\bar{T}^{2}(u+W)
$$

Hence, $\overline{T^{2}}=\bar{T}^{2}$. Similarly, $\overline{T^{n}}=\bar{T}^{n}$ for any $n$. Thus, for any polynomial

$$
\begin{gathered}
f(t)=a_{n} t^{n}+\cdots+a_{0}=\sum a_{i} t^{i} \\
\overline{f(T)}(u+W)=f(T)(u)+W=\sum a_{i} T^{i}(u)+W=\sum a_{i}\left(T^{i}(u)+W\right) \\
=\sum a_{i} \overline{T^{i}}(u+W)=\sum a_{i} \bar{T}^{i}(u+W)=\left(\sum a_{i} \bar{T}^{i}\right)(u+W)=f(\bar{T})(u+W)
\end{gathered}
$$

and so $\overline{f(T)}=f(\bar{T})$. Accordingly, if $T$ is a root of $f(t)$ then $\overline{f(T)}=\overline{\mathbf{0}}=W=f(\bar{T})$; that is, $\bar{T}$ is also a root of $f(t)$. The theorem is proved.

10.28. Prove Theorem 10.1: Let $T: V \rightarrow V$ be a linear operator whose characteristic polynomial factors into linear polynomials. Then $V$ has a basis in which $T$ is represented by a triangular matrix.

The proof is by induction on the dimension of $V$. If $\operatorname{dim} V=1$, then every matrix representation of $T$ is a $1 \times 1$ matrix, which is triangular.

Now suppose $\operatorname{dim} V=n>1$ and that the theorem holds for spaces of dimension less than $n$. Because the characteristic polynomial of $T$ factors into linear polynomials, $T$ has at least one eigenvalue and so at least one nonzero eigenvector $v$, say $T(v)=a_{11} v$. Let $W$ be the one-dimensional subspace spanned by $v$. Set $\bar{V}=V / W$. Then (Problem 10.26) $\operatorname{dim} \bar{V}=\operatorname{dim} V-\operatorname{dim} W=n-1$. Note also that $W$ is invariant under $T$. By Theorem 10.16, $T$ induces a linear operator $\bar{T}$ on $\bar{V}$ whose minimal polynomial divides the minimal polynomial of $T$. Because the characteristic polynomial of $T$ is a product of linear polynomials, so is its minimal polynomial, and hence, so are the minimal and characteristic polynomials of $\bar{T}$. Thus, $\bar{V}$ and $\bar{T}$ satisfy the hypothesis of the theorem. Hence, by induction, there exists a basis $\left\{\bar{v}_{2}, \ldots, \bar{v}_{n}\right\}$ of $\bar{V}$ such that

$$
\begin{aligned}
& \bar{T}\left(\bar{v}_{2}\right)=a_{22} \bar{v}_{2} \\
& \bar{T}\left(\bar{v}_{3}\right)=a_{32} \bar{v}_{2}+a_{33} \bar{v}_{3} \\
& \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
& \bar{T}\left(\bar{v}_{n}\right)=a_{n 2} \bar{v}_{n}+a_{n 3} \bar{v}_{3}+\cdots+a_{n n}
\end{aligned}
$$

Now let $v_{2}, \ldots, v_{n}$ be elements of $V$ that belong to the cosets $v_{2}, \ldots, v_{n}$, respectively. Then $\left\{v, v_{2}, \ldots, v_{n}\right\}$ is a basis of $V$ (Problem 10.26). Because $\bar{T}\left(v_{2}\right)=a_{22} \bar{v}_{2}$, we have

$$
\bar{T}\left(\bar{v}_{2}\right)-a_{22} \bar{v}_{22}=0, \quad \text { and so } \quad T\left(v_{2}\right)-a_{22} v_{2} \in W
$$

But $W$ is spanned by $v$; hence, $T\left(v_{2}\right)-a_{22} v_{2}$ is a multiple of $v$, say,

$$
T\left(v_{2}\right)-a_{22} v_{2}=a_{21} v, \quad \text { and so } \quad T\left(v_{2}\right)=a_{21} v+a_{22} v_{2}
$$

Similarly, for $i=3, \ldots, n$

$$
T\left(v_{i}\right)-a_{i 2} v_{2}-a_{i 3} v_{3}-\cdots-a_{i i} v_{i} \in W, \quad \text { and so } \quad T\left(v_{i}\right)=a_{i 1} v+a_{i 2} v_{2}+\cdots+a_{i i} v_{i}
$$

Thus,

$$
\begin{aligned}
& T(v)=a_{11} v \\
& T\left(v_{2}\right)=a_{21} v+a_{22} v_{2} \\
& \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\
& T\left(v_{n}\right)=a_{n 1} v+a_{n 2} v_{2}+\cdots+a_{n n} v_{n}
\end{aligned}
$$

and hence the matrix of $T$ in this basis is triangular.

\section*{Cyclic Subspaces, Rational Canonical Form}
10.29. Prove Theorem 10.12: Let $Z(v, T)$ be a $T$-cyclic subspace, $T_{v}$ the restriction of $T$ to $Z(v, T)$, and $m_{v}(t)=t^{k}+a_{k-1} t^{k-1}+\cdots+a_{0}$ the $T$-annihilator of $v$. Then,

(i) The set $\left\{v, T(v), \ldots, T^{k-1}(v)\right\}$ is a basis of $Z(v, T)$; hence, $\operatorname{dim} Z(v, T)=k$.

(ii) The minimal polynomial of $T_{v}$ is $m_{v}(t)$.

(iii) The matrix of $T_{v}$ in the above basis is the companion matrix $C=C\left(m_{v}\right)$ of $m_{v}(t)$ [which has 1's below the diagonal, the negative of the coefficients $a_{0}, a_{1}, \ldots, a_{k-1}$ of $m_{v}(t)$ in the last column, and 0's elsewhere].

(i) By definition of $m_{v}(t), T^{k}(v)$ is the first vector in the sequence $v, T(v), T^{2}(v), \ldots$ that, is a linear combination of those vectors that precede it in the sequence; hence, the set $B=\left\{v, T(v), \ldots, T^{k-1}(v)\right\}$ is linearly independent. We now only have to show that $Z(v, T)=L(B)$, the linear span of $B$. By the above, $T^{k}(v) \in L(B)$. We prove by induction that $T^{n}(v) \in L(B)$ for every $n$. Suppose $n>k$ and $T^{n-1}(v) \in L(B)$ - that is, $\quad T^{n-1}(v) \quad$ is $\quad$ a linear combination of $\quad v, \ldots, T^{k-1}(v)$. Then $T^{n}(v)=T\left(T^{n-1}(v)\right)$ is a linear combination of $T(v), \ldots, T^{k}(v)$. But $T^{k}(v) \in L(B)$; hence, $T^{n}(v) \in L(B)$ for every $n$. Consequently, $f(T)(v) \in L(B)$ for any polynomial $f(t)$. Thus, $Z(v, T)=L(B)$, and so $B$ is a basis, as claimed.

(ii) Suppose $m(t)=t^{s}+b_{s-1} t^{s-1}+\cdots+b_{0}$ is the minimal polynomial of $T_{v}$. Then, because $v \in Z(v, T)$,

$$
0=m\left(T_{v}\right)(v)=m(T)(v)=T^{s}(v)+b_{s-1} T^{s-1}(v)+\cdots+b_{0} v
$$

Thus, $T^{s}(v)$ is a linear combination of $v, T(v), \ldots, T^{s-1}(v)$, and therefore $k \leq s$. However, $m_{v}(T)=\mathbf{0}$ and so $m_{v}\left(T_{v}\right)=\mathbf{0}$. Then $m(t)$ divides $m_{v}(t)$, and so $s \leq k$. Accordingly, $k=s$ and hence $m_{v}(t)=m(t)$.

(iii)

$$
\begin{array}{llc}
T_{v}(v) & = & T(v) \\
T_{v}(T(v)) & = & T^{2}(v) \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
T_{v}\left(T^{k-2}(v)\right) & = & T^{k-1}(v) \\
T_{v}\left(T^{k-1}(v)\right) & = & T^{k}(v)=-a_{0} v-a_{1} T(v)-a_{2} T^{2}(v)-\cdots-a_{k-1} T^{k-1}(v)
\end{array}
$$

By definition, the matrix of $T_{v}$ in this basis is the tranpose of the matrix of coefficients of the above system of equations; hence, it is $C$, as required.

10.30. Let $T: V \rightarrow V$ be linear. Let $W$ be a $T$-invariant subspace of $V$ and $\bar{T}$ the induced operator on $V / W$. Prove

(a) The T-annihilator of $v \in V$ divides the minimal polynomial of $T$.

(b) The $\bar{T}$-annihilator of $\bar{v} \in V / W$ divides the minimal polynomial of $T$.\\
(a) The $T$-annihilator of $v \in V$ is the minimal polynomial of the restriction of $T$ to $Z(v, T)$; therefore, by Problem 10.6, it divides the minimal polynomial of $T$.

(b) The $\bar{T}$-annihilator of $\bar{v} \in V / W$ divides the minimal polynomial of $\bar{T}$, which divides the minimal polynomial of $T$ by Theorem 10.16 .

Remark: In the case where the minimum polynomial of $T$ is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial, then the $T$-annihilator of $v \in V$ and the $\bar{T}$-annihilator of $\bar{v} \in V / W$ are of the form $f(t)^{m}$, where $m \leq n$.

10.31. Prove Lemma 10.13: Let $T: V \rightarrow V$ be a linear operator whose minimal polynomial is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial. Then $V$ is the direct sum of $T$-cyclic subspaces $Z_{i}=Z\left(v_{i}, T\right), i=1, \ldots, r$, with corresponding $T$-annihilators

$$
f(t)^{n_{1}}, f(t)^{n_{2}}, \ldots, f(t)^{n_{r}}, \quad n=n_{1} \geq n_{2} \geq \cdots \geq n_{r}
$$

Any other decomposition of $V$ into the direct sum of $T$-cyclic subspaces has the same number of components and the same set of $T$-annihilators.

The proof is by induction on the dimension of $V$. If $\operatorname{dim} V=1$, then $V$ is $T$-cyclic and the lemma holds. Now suppose $\operatorname{dim} V>1$ and that the lemma holds for those vector spaces of dimension less than that of $V$.

Because the minimal polynomial of $T$ is $f(t)^{n}$, there exists $v_{1} \in V$ such that $f(T)^{n-1}\left(v_{1}\right) \neq 0$; hence, the $T$-annihilator of $v_{1}$ is $f(t)^{n}$. Let $Z_{1}=Z\left(v_{1}, T\right)$ and recall that $Z_{1}$ is $T$-invariant. Let $\bar{V}=V / Z_{1}$ and let $\bar{T}$ be the linear operator on $\bar{V}$ induced by $T$. By Theorem 10.16, the minimal polynomial of $\bar{T}$ divides $f(t)^{n}$; hence, the hypothesis holds for $\bar{V}$ and $\bar{T}$. Consequently, by induction, $\bar{V}$ is the direct sum of $\bar{T}$-cyclic subspaces; say,

$$
\bar{V}=Z\left(\bar{v}_{2}, \bar{T}\right) \oplus \cdots \oplus Z\left(\bar{v}_{r}, \bar{T}\right)
$$

where the corresponding $\bar{T}$-annihilators are $f(t)^{n_{2}}, \ldots, f(t)^{n_{r}}, n \geq n_{2} \geq \cdots \geq n_{r}$.

We claim that there is a vector $v_{2}$ in the coset $\bar{v}_{2}$ whose $T$-annihilator is $f(t)^{n_{2}}$, the $\bar{T}$-annihilator of $\bar{v}_{2}$. Let $w$ be any vector in $\bar{v}_{2}$. Then $f(T)^{n_{2}}(w) \in Z_{1}$. Hence, there exists a polynomial $g(t)$ for which


\begin{equation*}
f(T)^{n_{2}}(w)=g(T)\left(v_{1}\right) \tag{1}
\end{equation*}


Because $f(t)^{n}$ is the minimal polynomial of $T$, we have, by (1),

$$
0=f(T)^{n}(w)=f(T)^{n-n_{2}} g(T)\left(v_{1}\right)
$$

But $f(t)^{n}$ is the $T$-annihilator of $v_{1}$; hence, $f(t)^{n}$ divides $f(t)^{n-n_{2}} g(t)$, and so $g(t)=f(t)^{n_{2}} h(t)$ for some polynomial $h(t)$. We set

$$
v_{2}=w-h(T)\left(v_{1}\right)
$$

Because $w-v_{2}=h(T)\left(v_{1}\right) \in Z_{1}, v_{2}$ also belongs to the coset $\bar{v}_{2}$. Thus, the $T$-annihilator of $v_{2}$ is a multiple of the $\bar{T}$-annihilator of $\bar{v}_{2}$. On the other hand, by (1),

$$
f(T)^{n_{2}}\left(v_{2}\right)=f(T)^{n_{s}}\left(w-h(T)\left(v_{1}\right)\right)=f(T)^{n_{2}}(w)-g(T)\left(v_{1}\right)=0
$$

Consequently, the $T$-annihilator of $v_{2}$ is $f(t)^{n_{2}}$, as claimed.

Similarly, there exist vectors $v_{3}, \ldots, v_{r} \in V$ such that $v_{i} \in \overline{v_{i}}$ and that the $T$-annihilator of $v_{i}$ is $f(t)^{n_{i}}$, the $\bar{T}$-annihilator of $\overline{v_{i}}$. We set

$$
Z_{2}=Z\left(v_{2}, T\right), \quad \ldots, \quad Z_{r}=Z\left(v_{r}, T\right)
$$

Let $d$ denote the degree of $f(t)$, so that $f(t)^{n_{i}}$ has degree $d n_{i}$. Then, because $f(t)^{n_{i}}$ is both the $T$-annihilator of $v_{i}$ and the $\bar{T}$-annihilator of $\bar{v}_{i}$, we know that

$$
\left\{v_{i}, T\left(v_{i}\right), \ldots, T^{d n_{i}-1}\left(v_{i}\right)\right\} \quad \text { and } \quad\left\{\bar{v}_{i} \cdot \bar{T}\left(\bar{v}_{i}\right), \ldots, \bar{T}^{d n_{i}-1}\left(\bar{v}_{i}\right)\right\}
$$

are bases for $Z\left(v_{i}, T\right)$ and $Z\left(\overline{v_{i}}, \bar{T}\right)$, respectively, for $i=2, \ldots, r$. But $\bar{V}=Z\left(\overline{v_{2}}, \bar{T}\right) \oplus \cdots \oplus Z\left(\overline{v_{r}}, \bar{T}\right)$; hence,

$$
\left\{\bar{v}_{2}, \ldots, \bar{T}^{d n_{2}-1}\left(\bar{v}_{2}\right), \ldots, \bar{v}_{r}, \ldots, \bar{T}^{d n_{r}-1}\left(\bar{v}_{r}\right)\right\}
$$

is a basis for $\bar{V}$. Therefore, by Problem 10.26 and the relation $\bar{T}^{i}(\bar{v})=\overline{T^{i}(v)}$ (see Problem 10.27),

$$
\left\{v_{1}, \ldots, T^{d n_{1}-1}\left(v_{1}\right), v_{2}, \ldots, T^{e n_{2}-1}\left(v_{2}\right), \ldots, v_{r}, \ldots, T^{d n_{r}-1}\left(v_{r}\right)\right\}
$$

is a basis for $V$. Thus, by Theorem 10.4, $V=Z\left(v_{1}, T\right) \oplus \cdots \oplus Z\left(v_{r}, T\right)$, as required.

It remains to show that the exponents $n_{1}, \ldots, n_{r}$ are uniquely determined by $T$. Because $d=$ degree of $f(t)$,

$$
\operatorname{dim} V=d\left(n_{1}+\cdots+n_{r}\right) \quad \text { and } \quad \operatorname{dim} Z_{i}=d n_{i}, \quad i=1, \ldots, r
$$

Also, if $s$ is any positive integer, then (Problem 10.59) $f(T)^{s}\left(Z_{i}\right)$ is a cyclic subspace generated by $f(T)^{s}\left(v_{i}\right)$, and it has dimension $d\left(n_{i}-s\right)$ if $n_{i}>s$ and dimension 0 if $n_{i} \leq s$.

Now any vector $v \in V$ can be written uniquely in the form $v=w_{1}+\cdots+w_{r}$, where $w_{i} \in Z_{i}$. Hence, any vector in $f(T)^{s}(V)$ can be written uniquely in the form

$$
f(T)^{s}(v)=f(T)^{s}\left(w_{1}\right)+\cdots+f(T)^{s}\left(w_{r}\right)
$$

where $f(T)^{s}\left(w_{i}\right) \in f(T)^{s}\left(Z_{i}\right)$. Let $t$ be the integer, dependent on $s$, for which

\begin{center}
\begin{tabular}{lc}
 & $n_{1}>s, \quad \cdots, \quad n_{t}>s, \quad n_{t+1} \geq s$ \\
Then & $f(T)^{s}(V)=f(T)^{s}\left(Z_{1}\right) \oplus \cdots \oplus f(T)^{s}\left(Z_{t}\right)$ \\
and so & $\operatorname{dim}\left[f(T)^{s}(V)\right]=d\left[\left(n_{1}-s\right)+\cdots+\left(n_{t}-s\right)\right]$ \\
\end{tabular}
\end{center}

The numbers on the left of (2) are uniquely determined by $T$. Set $s=n-1$, and (2) determines the number of $n_{i}$ equal to $n$. Next set $s=n-2$, and (2) determines the number of $n_{i}$ (if any) equal to $n-1$. We repeat the process until we set $s=0$ and determine the number of $n_{i}$ equal to 1 . Thus, the $n_{i}$ are uniquely determined by $T$ and $V$, and the lemma is proved.

10.32. Let $V$ be a seven-dimensional vector space over $\mathbf{R}$, and let $T: V \rightarrow V$ be a linear operator with minimal polynomial $m(t)=\left(t^{2}-2 t+5\right)(t-3)^{3}$. Find all possible rational canonical forms $M$ of $T$.

Because $\operatorname{dim} V=7$, there are only two possible characteristic polynomials, $\Delta_{1}(t)=\left(t^{2}-2 t+5\right)^{2}$ $(t-3)^{3}$ or $\Delta_{1}(t)=\left(t^{2}-2 t+5\right)(t-3)^{5}$. Moreover, the sum of the orders of the companion matrices must add up to 7. Also, one companion matrix must be $C\left(t^{2}-2 t+5\right)$ and one must be $C\left((t-3)^{3}\right)=$ $C\left(t^{3}-9 t^{2}+27 t-27\right)$. Thus, $M$ must be one of the following block diagonal matrices:

(a) $\operatorname{diag}\left(\left[\begin{array}{rr}0 & -5 \\ 1 & 2\end{array}\right],\left[\begin{array}{rr}0 & -5 \\ 1 & 2\end{array}\right],\left[\begin{array}{rrr}0 & 0 & 27 \\ 1 & 0 & -27 \\ 0 & 1 & 9\end{array}\right]\right)$,

(b) $\operatorname{diag}\left(\left[\begin{array}{rr}0 & -5 \\ 1 & 2\end{array}\right],\left[\begin{array}{rrr}0 & 0 & 27 \\ 1 & 0 & -27 \\ 0 & 1 & 9\end{array}\right],\left[\begin{array}{rr}0 & -9 \\ 1 & 6\end{array}\right]\right)$,

(c) $\operatorname{diag}\left(\left[\begin{array}{rr}0 & -5 \\ 1 & 2\end{array}\right],\left[\begin{array}{rrr}0 & 0 & 27 \\ 1 & 0 & -27 \\ 0 & 1 & 9\end{array}\right],[3],[3]\right)$

\section*{Projections}
10.33. Suppose $V=W_{1} \oplus \cdots \oplus W_{r}$. The projection of $V$ into its subspace $W_{k}$ is the mapping $E: V \rightarrow V$ defined by $E(v)=w_{k}$, where $v=w_{1}+\cdots+w_{r}, w_{i} \in W_{i}$. Show that (a) $E$ is linear, (b) $E^{2}=E$.

(a) Because the sum $v=w_{1}+\cdots+w_{r}, w_{i} \in W$ is uniquely determined by $v$, the mapping $E$ is well defined. Suppose, for $u \in V, u=w_{1}^{\prime}+\cdots+w_{r}^{\prime}$, $w_{i}^{\prime} \in W_{i}$. Then

$$
v+u=\left(w_{1}+w_{1}^{\prime}\right)+\cdots+\left(w_{r}+w_{r}^{\prime}\right) \quad \text { and } \quad k v=k w_{1}+\cdots+k w_{r}, \quad k w_{i}, w_{i}+w_{i}^{\prime} \in W_{i}
$$

are the unique sums corresponding to $v+u$ and $k v$. Hence,

$$
E(v+u)=w_{k}+w_{k}^{\prime}=E(v)+E(u) \quad \text { and } \quad E(k v)=k w_{k}+k E(v)
$$

and therefore $E$ is linear.\\
(b) We have that

$$
w_{k}=0+\cdots+0+w_{k}+0+\cdots+0
$$

is the unique sum corresponding to $w_{k} \in W_{k}$; hence, $E\left(w_{k}\right)=w_{k}$. Then, for any $v \in V$,

$$
E^{2}(v)=E(E(v))=E\left(w_{k}\right)=w_{k}=E(v)
$$

Thus, $E^{2}=E$, as required.

10.34. Suppose $E: V \rightarrow V$ is linear and $E^{2}=E$. Show that (a) $E(u)=u$ for any $u \in \operatorname{Im} E$ (i.e., the restriction of $E$ to its image is the identity mapping); (b) $V$ is the direct sum of the image and kernel of $E: V=\operatorname{Im} E \oplus \operatorname{Ker} E$; (c) $E$ is the projection of $V$ into $\operatorname{Im} E$, its image. Thus, by the preceding problem, a linear mapping $T: V \rightarrow V$ is a projection if and only if $T^{2}=T$; this characterization of a projection is frequently used as its definition.

(a) If $u \in \operatorname{Im} E$, then there exists $v \in V$ for which $E(v)=u$; hence, as required,

$$
E(u)=E(E(v))=E^{2}(v)=E(v)=u
$$

(b) Let $v \in V$. We can write $v$ in the form $v=E(v)+v-E(v)$. Now $E(v) \in \operatorname{Im} E$ and, because

$$
E(v-E(v))=E(v)-E^{2}(v)=E(v)-E(v)=0
$$

$v-E(v) \in \operatorname{Ker} E$. Accordingly, $V=\operatorname{Im} E+\operatorname{Ker} E$.

Now suppose $w \in \operatorname{Im} E \cap \operatorname{Ker} E$. By (i), $E(w)=w$ because $w \in \operatorname{Im} E$. On the other hand, $E(w)=0$ because $w \in \operatorname{Ker} E$. Thus, $w=0$, and so $\operatorname{Im} E \cap \operatorname{Ker} E=\{0\}$. These two conditions imply that $V$ is the direct sum of the image and kernel of $E$.

(c) Let $v \in V$ and suppose $v=u+w$, where $u \in \operatorname{Im} E$ and $w \in \operatorname{Ker} E$. Note that $E(u)=u$ by (i), and $E(w)=0$ because $w \in \operatorname{Ker} E$. Hence,

$$
E(v)=E(u+w)=E(u)+E(w)=u+0=u
$$

That is, $E$ is the projection of $V$ into its image.

10.35. Suppose $V=U \oplus W$ and suppose $T: V \rightarrow V$ is linear. Show that $U$ and $W$ are both $T$-invariant if and only if $T E=E T$, where $E$ is the projection of $V$ into $U$.

Observe that $E(v) \in U$ for every $v \in V$, and that (i) $E(v)=v$ iff $v \in U$, (ii) $E(v)=0$ iff $v \in W$.

Suppose $E T=T E$. Let $u \in U$. Because $E(u)=u$,

$$
T(u)=T(E(u))=(T E)(u)=(E T)(u)=E(T(u)) \in U
$$

Hence, $U$ is $T$-invariant. Now let $w \in W$. Because $E(w)=0$,

$$
E(T(w))=(E T)(w)=(T E)(w)=T(E(w))=T(0)=0, \quad \text { and so } \quad T(w) \in W
$$

Hence, $W$ is also $T$-invariant.

Conversely, suppose $U$ and $W$ are both $T$-invariant. Let $v \in V$ and suppose $v=u+w$, where $u \in T$ and $w \in W$. Then $T(u) \in U$ and $T(w) \in W$; hence, $E(T(u))=T(u)$ and $E(T(w))=0$. Thus,

and

$$
(E T)(v)=(E T)(u+w)=(E T)(u)+(E T)(w)=E(T(u))+E(T(w))=T(u)
$$

$$
(T E)(v)=(T E)(u+w)=T(E(u+w))=T(u)
$$

That is, $(E T)(v)=(T E)(v)$ for every $v \in V$; therefore, $E T=T E$, as required.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Invariant Subspaces}
10.36. Suppose $W$ is invariant under $T: V \rightarrow V$. Show that $W$ is invariant under $f(T)$ for any polynomial $f(t)$.

10.37. Show that every subspace of $V$ is invariant under $I$ and $\mathbf{0}$, the identity and zero operators.

10.38. Let $W$ be invariant under $T_{1}: V \rightarrow V$ and $T_{2}: V \rightarrow V$. Prove $W$ is also invariant under $T_{1}+T_{2}$ and $T_{1} T_{2}$.

10.39. Let $T: V \rightarrow V$ be linear. Prove that any eigenspace, $E_{\lambda}$ is $T$-invariant.

10.40. Let $V$ be a vector space of odd dimension (greater than 1 ) over the real field $\mathbf{R}$. Show that any linear operator on $V$ has an invariant subspace other than $V$ or $\{0\}$.

10.41. Determine the invariant subspace of $A=\left[\begin{array}{ll}2 & -4 \\ 5 & -2\end{array}\right]$ viewed as a linear operator on (a) $\mathbf{R}^{2}$, (b) $\mathbf{C}^{2}$.

10.42. Suppose $\operatorname{dim} V=n$. Show that $T: V \rightarrow V$ has a triangular matrix representation if and only if there exist $T$-invariant subspaces $W_{1} \subset W_{2} \subset \cdots \subset W_{n}=V$ for which $\operatorname{dim} W_{k}=k, k=1, \ldots, n$.

\section*{Invariant Direct Sums}
10.43. The subspaces $W_{1}, \ldots, W_{r}$ are said to be independent if $w_{1}+\cdots+w_{r}=0, w_{i} \in W_{i}$, implies that each $w_{i}=0$. Show that $\operatorname{span}\left(W_{i}\right)=W_{1} \oplus \cdots \oplus W_{r}$ if and only if the $W_{i}$ are independent. [Here $\operatorname{span}\left(W_{i}\right)$ denotes the linear span of the $W_{i}$.]

10.44. Show that $V=W_{1} \oplus \cdots \oplus W_{r}$ if and only if (i) $V=\operatorname{span}\left(W_{i}\right)$ and (ii) for $k=1,2, \ldots, r$, $W_{k} \cap \operatorname{span}\left(W_{1}, \ldots, W_{k-1}, W_{k+1}, \ldots, W_{r}\right)=\{0\}$.

10.45. Show that $\operatorname{span}\left(W_{i}\right)=W_{1} \oplus \cdots \oplus W_{r}$ if and only if $\operatorname{dim}\left[\operatorname{span}\left(W_{i}\right)\right]=\operatorname{dim} W_{1}+\cdots+\operatorname{dim} W_{r}$.

10.46. Suppose the characteristic polynomial of $T: V \rightarrow V$ is $\Delta(t)=f_{1}(t)^{n_{1}} f_{2}(t)^{n_{2}} \cdots f_{r}(t)^{n_{r}}$, where the $f_{i}(t)$ are distinct monic irreducible polynomials. Let $V=W_{1} \oplus \cdots \oplus W_{r}$ be the primary decomposition of $V$ into $T$ invariant subspaces. Show that $f_{i}(t)^{n_{i}}$ is the characteristic polynomial of the restriction of $T$ to $W_{i}$.

\section*{Nilpotent Operators}
10.47. Suppose $T_{1}$ and $T_{2}$ are nilpotent operators that commute (i.e., $T_{1} T_{2}=T_{2} T_{1}$ ). Show that $T_{1}+T_{2}$ and $T_{1} T_{2}$ are also nilpotent.

10.48. Suppose $A$ is a supertriangular matrix (i.e., all entries on and below the main diagonal are 0 ). Show that $A$ is nilpotent.

10.49. Let $V$ be the vector space of polynomials of degree $\leq n$. Show that the derivative operator on $V$ is nilpotent of index $n+1$.

10.50. Show that any Jordan nilpotent block matrix $N$ is similar to its transpose $N^{T}$ (the matrix with 1 's below the diagonal and 0's elsewhere).

10.51. Show that two nilpotent matrices of order 3 are similar if and only if they have the same index of nilpotency. Show by example that the statement is not true for nilpotent matrices of order 4.

\section*{Jordan Canonical Form}
10.52. Find all possible Jordan canonical forms for those matrices whose characteristic polynomial $\Delta(t)$ and minimal polynomial $m(t)$ are as follows:

(a) $\Delta(t)=(t-2)^{4}(t-3)^{2}, m(t)=(t-2)^{2}(t-3)^{2}$,

(b) $\Delta(t)=(t-7)^{5}, m(t)=(t-7)^{2}$, (c) $\Delta(t)=(t-2)^{7}, m(t)=(t-2)^{3}$

10.53. Show that every complex matrix is similar to its transpose. (Hint: Use its Jordan canonical form.)

10.54. Show that all $n \times n$ complex matrices $A$ for which $A^{n}=I$ but $A_{k} \neq I$ for $k<n$ are similar.

10.55. Suppose $A$ is a complex matrix with only real eigenvalues. Show that $A$ is similar to a matrix with only real entries.

\section*{Cyclic Subspaces}
10.56. Suppose $T: V \rightarrow V$ is linear. Prove that $Z(v, T)$ is the intersection of all $T$-invariant subspaces containing $v$.

10.57. Let $f(t)$ and $g(t)$ be the $T$-annihilators of $u$ and $v$, respectively. Show that if $f(t)$ and $g(t)$ are relatively prime, then $f(t) g(t)$ is the $T$-annihilator of $u+v$.

10.58. Prove that $Z(u, T)=Z(v, T)$ if and only if $g(T)(u)=v$ where $g(t)$ is relatively prime to the $T$-annihilator of $u$.

10.59. Let $W=Z(v, T)$, and suppose the $T$-annihilator of $v$ is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial of degree $d$. Show that $f(T)^{s}(W)$ is a cyclic subspace generated by $f(T)^{s}(v)$ and that it has dimension $d(n-s)$ if $n>s$ and dimension 0 if $n \leq s$.

\section*{Rational Canonical Form}
10.60. Find all possible rational forms for a $6 \times 6$ matrix over $\mathbf{R}$ with minimal polynomial:

(a) $m(t)=\left(t^{2}-2 t+3\right)(t+1)^{2}$, (b) $m(t)=(t-2)^{3}$.

10.61. Let $A$ be a $4 \times 4$ matrix with minimal polynomial $m(t)=\left(t^{2}+1\right)\left(t^{2}-3\right)$. Find the rational canonical form for $A$ if $A$ is a matrix over (a) the rational field $\mathbf{Q}$, (b) the real field $\mathbf{R}$, (c) the complex field $\mathbf{C}$.

10.62. Find the rational canonical form for the four-square Jordan block with $\lambda$ 's on the diagonal.

10.63. Prove that the characteristic polynomial of an operator $T: V \rightarrow V$ is a product of its elementary divisors.

10.64. Prove that two $3 \times 3$ matrices with the same minimal and characteristic polynomials are similar.

10.65. Let $C(f(t))$ denote the companion matrix to an arbitrary polynomial $f(t)$. Show that $f(t)$ is the characteristic polynomial of $C(f(t))$.

\section*{Projections}
10.66. Suppose $V=W_{1} \oplus \cdots \oplus W_{r}$. Let $E_{i}$ denote the projection of $V$ into $W_{i}$. Prove (i) $E_{i} E_{j}=0, i \neq j$; (ii) $I=E_{1}+\cdots+E_{r}$.

10.67. Let $E_{1}, \ldots, E_{r}$ be linear operators on $V$ such that

(i) $E_{i}^{2}=E_{i}$ (i.e., the $E_{i}$ are projections); (ii) $E_{i} E_{j}=0, i \neq j$; (iii) $I=E_{1}+\cdots+E_{r}$

Prove that $V=\operatorname{Im} E_{1} \oplus \cdots \oplus \operatorname{Im} E_{r}$.

10.68. Suppose $E: V \rightarrow V$ is a projection (i.e., $E^{2}=E$ ). Prove that $E$ has a matrix representation of the form $\left[\begin{array}{cc}I_{r} & 0 \\ 0 & 0\end{array}\right]$, where $r$ is the rank of $E$ and $I_{r}$ is the $r$-square identity matrix.

10.69. Prove that any two projections of the same rank are similar. (Hint: Use the result of Problem 10.68.)

10.70. Suppose $E: V \rightarrow V$ is a projection. Prove

(i) $I-E$ is a projection and $V=\operatorname{Im} E \oplus \operatorname{Im}(I-E)$, (ii) $I+E$ is invertible (if $1+1 \neq 0$ ).

\section*{Quotient Spaces}
10.71. Let $W$ be a subspace of $V$. Suppose the set of cosets $\left\{v_{1}+W, v_{2}+W, \ldots, v_{n}+W\right\}$ in $V / W$ is linearly independent. Show that the set of vectors $\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$ in $V$ is also linearly independent.

10.72. Let $W$ be a substance of $V$. Suppose the set of vectors $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ in $V$ is linearly independent, and that $L\left(u_{i}\right) \cap W=\{0\}$. Show that the set of cosets $\left\{u_{1}+W, \ldots, u_{n}+W\right\}$ in $V / W$ is also linearly independent.

10.73. Suppose $V=U \oplus W$ and that $\left\{u_{1}, \ldots, u_{n}\right\}$ is a basis of $U$. Show that $\left\{u_{1}+W, \ldots, u_{n}+W\right\}$ is a basis of the quotient spaces $V / W$. (Observe that no condition is placed on the dimensionality of $V$ or $W$.)

10.74. Let $W$ be the solution space of the linear equation

$$
a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=0, \quad a_{i} \in K
$$

and let $v=\left(b_{1}, b_{2}, \ldots, b_{n}\right) \in K^{n}$. Prove that the coset $v+W$ of $W$ in $K^{n}$ is the solution set of the linear equation

$$
a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}=b, \quad \text { where } \quad b=a_{1} b_{1}+\cdots+a_{n} b_{n}
$$

10.75. Let $V$ be the vector space of polynomials over $\mathbf{R}$ and let $W$ be the subspace of polynomials divisible by $t^{4}$ (i.e., of the form $a_{0} t^{4}+a_{1} t^{5}+\cdots+a_{n-4} t^{n}$ ). Show that the quotient space $V / W$ has dimension 4.

10.76. Let $U$ and $W$ be subspaces of $V$ such that $W \subset U \subset V$. Note that any coset $u+W$ of $W$ in $U$ may also be viewed as a coset of $W$ in $V$, because $u \in U$ implies $u \in V$; hence, $U / W$ is a subset of $V / W$. Prove that (i) $U / W$ is a subspace of $V / W$, (ii) $\operatorname{dim}(V / W)-\operatorname{dim}(U / W)=\operatorname{dim}(V / U)$.

10.77. Let $U$ and $W$ be subspaces of $V$. Show that the cosets of $U \cap W$ in $V$ can be obtained by intersecting each of the cosets of $U$ in $V$ by each of the cosets of $W$ in $V$ :

$$
V /(U \cap W)=\left\{(v+U) \cap\left(v^{\prime}+W\right): v, v^{\prime} \in V\right\}
$$

10.78. Let $T: V \rightarrow V^{\prime}$ be linear with kernel $W$ and image $U$. Show that the quotient space $V / W$ is isomorphic to $U$ under the mapping $\theta: V / W \rightarrow U$ defined by $\theta(v+W)=T(v)$. Furthermore, show that $T=i \circ \theta \circ \eta$, where $\eta: V \rightarrow V / W$ is the natural mapping of $V$ into $V / W$ (i.e., $\eta(v)=v+W$ ), and $i: U \hookrightarrow V^{\prime}$ is the inclusion mapping (i.e., $i(u)=u$ ). (See diagram.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-355}
\end{center}

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
10.41. (a) $\mathbf{R}^{2}$ and $\{0\}$,

(b) $\mathbf{C}^{2},\{0\}, W_{1}=\operatorname{span}(2,1-2 i), W_{2}=\operatorname{span}(2,1+2 i)$

10.52. (a) $\operatorname{diag}\left(\left[\begin{array}{ll}2 & 1 \\ & 2\end{array}\right],\left[\begin{array}{ll}2 & 1 \\ & 2\end{array}\right],\left[\begin{array}{ll}3 & 1 \\ & 3\end{array}\right]\right), \quad \operatorname{diag}\left(\left[\begin{array}{ll}2 & 1 \\ & 2\end{array}\right], \quad[2] . \quad\left[\begin{array}{ll}2\end{array}\right],\left[\begin{array}{ll}3 & 1 \\ & 3\end{array}\right]\right)$;

(b) $\operatorname{diag}\left(\left[\begin{array}{ll}7 & 1 \\ & 7\end{array}\right],\left[\begin{array}{ll}7 & 1 \\ & 7\end{array}\right],[7]\right), \quad \operatorname{diag}\left(\left[\begin{array}{ll}7 & 1 \\ & 7\end{array}\right], \quad[7], \quad[7],[7]\right)$;

(c) Let $M_{k}$ denote a Jordan block with $\lambda=2$ and order $k$. Then $\operatorname{diag}\left(M_{3}, M_{3}, M_{1}\right), \operatorname{diag}\left(M_{3}, M_{2}, M_{2}\right)$, $\operatorname{diag}\left(M_{3}, M_{2}, M_{1}, M_{1}\right), \operatorname{diag}\left(M_{3}, M_{1}, M_{1}, M_{1}, M_{1}\right)$

10.60. Let $A=\left[\begin{array}{rr}0 & -3 \\ 1 & 2\end{array}\right], \quad B=\left[\begin{array}{rr}0 & -1 \\ 1 & -2\end{array}\right], \quad C=\left[\begin{array}{rrr}0 & 0 & 8 \\ 1 & 0 & -12 \\ 0 & 1 & 6\end{array}\right], \quad D=\left[\begin{array}{rr}0 & -4 \\ 1 & 4\end{array}\right]$.\\
(a) $\operatorname{diag}(A, A, B), \operatorname{diag}(A, B, B), \operatorname{diag}(A, B,-1,-1)$;\\
(b) $\operatorname{diag}(C, C), \operatorname{diag}(C, D, 2), \operatorname{diag}(C, 2,2,2)$

10.61. Let $A=\left[\begin{array}{rr}0 & -1 \\ 1 & 0\end{array}\right], \quad B=\left[\begin{array}{ll}0 & 3 \\ 1 & 0\end{array}\right]$.\\
(a) $\operatorname{diag}(A, B)$,\\
(b) $\operatorname{diag}(A, \sqrt{3},-\sqrt{3})$,\\
(c) $\operatorname{diag}(i,-i, \sqrt{3},-\sqrt{3})$

10.62. Companion matrix with the last column $\left[-\lambda^{4}, 4 \lambda^{3},-6 \lambda^{2}, 4 \lambda\right]^{T}$

\section*{Linear Functionals and the Dual Space}
\subsection*{11.1 Introduction}
In this chapter, we study linear mappings from a vector space $V$ into its field $K$ of scalars. (Unless otherwise stated or implied, we view $K$ as a vector space over itself.) Naturally all the theorems and results for arbitrary mappings on $V$ hold for this special case. However, we treat these mappings separately because of their fundamental importance and because the special relationship of $V$ to $K$ gives rise to new notions and results that do not apply in the general case.

\subsection*{11.2 Linear Functionals and the Dual Space}
Let $V$ be a vector space over a field $K$. A mapping $\phi: V \rightarrow K$ is termed a linear functional (or linear form) if, for every $u, v \in V$ and every $a, b, \in K$,

$$
\phi(a u+b v)=a \phi(u)+b \phi(v)
$$

In other words, a linear functional on $V$ is a linear mapping from $V$ into $K$.

\section*{EXAMPLE 11.1}
(a) Let $\pi_{i}: K^{n} \rightarrow K$ be the $i$ th projection mapping; that is, $\pi_{i}\left(a_{1}, a_{2}, \ldots a_{n}\right)=a_{i}$. Then $\pi_{i}$ is linear and so it is a linear functional on $K^{n}$.

(b) Let $V$ be the vector space of polynomials in $t$ over $\mathbf{R}$. Let $\mathbf{J}: V \rightarrow \mathbf{R}$ be the integral operator defined by $\mathbf{J}(p(t))=\int_{0}^{1} p(t) d t$. Recall that $\mathbf{J}$ is linear; and hence, it is a linear functional on $V$.

(c) Let $V$ be the vector space of $n$-square matrices over $K$. Let $T: V \rightarrow K$ be the trace mapping

$$
T(A)=a_{11}+a_{22}+\cdots+a_{n n}, \quad \text { where } \quad A=\left[a_{i j}\right]
$$

That is, $T$ assigns to a matrix $A$ the sum of its diagonal elements. This map is linear (Problem 11.24), and so it is a linear functional on $V$.

By Theorem 5.10, the set of linear functionals on a vector space $V$ over a field $K$ is also a vector space over $K$, with addition and scalar multiplication defined by

$$
(\phi+\sigma)(v)=\phi(v)+\sigma(v) \quad \text { and } \quad(k \phi)(v)=k \phi(v)
$$

where $\phi$ and $\sigma$ are linear functionals on $V$ and $k \in K$. This space is called the dual space of $V$ and is denoted by $V^{*}$.

EXAMPLE 11.2 Let $V=K^{n}$, the vector space of $n$-tuples, which we write as column vectors. Then the dual space $V^{*}$ can be identified with the space of row vectors. In particular, any linear functional $\phi=\left(a_{1}, \ldots, a_{n}\right)$ in $V^{*}$ has the representation

$$
\phi\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\left[a_{1}, a_{2}, \ldots, a_{n}\right]\left[x_{2}, x_{2}, \ldots, x_{n}\right]^{T}=a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}
$$

Historically, the formal expression on the right was termed a linear form.

\subsection*{11.3 Dual Basis}
Suppose $V$ is a vector space of dimension $n$ over $K$. By Theorem 5.11, the dimension of the dual space $V^{*}$ is also $n$ (because $K$ is of dimension 1 over itself). In fact, each basis of $V$ determines a basis of $V^{*}$ as follows (see Problem 11.3 for the proof).

THEOREM 11.1: Suppose $\left\{v_{1}, \ldots, v_{n}\right\}$ is a basis of $V$ over $K$. Let $\phi_{1}, \ldots, \phi_{n} \in V^{*}$ be the linear functionals as defined by

$$
\phi_{i}\left(v_{j}\right)=\delta_{i j}= \begin{cases}1 & \text { if } i=j \\ 0 & \text { if } i \neq j\end{cases}
$$

Then $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ is a basis of $V^{*}$.

The above basis $\left\{\phi_{i}\right\}$ is termed the basis dual to $\left\{v_{i}\right\}$ or the dual basis. The above formula, which uses the Kronecker delta $\delta_{i j}$, is a short way of writing

$$
\begin{array}{ccccc}
\phi_{1}\left(v_{1}\right)=1, & \phi_{1}\left(v_{2}\right)=0, & \phi_{1}\left(v_{3}\right)=0, & \ldots, & \phi_{1}\left(v_{n}\right)=0 \\
\phi_{2}\left(v_{1}\right)=0, & \phi_{2}\left(v_{2}\right)=1, & \phi_{2}\left(v_{3}\right)=0, & \ldots, & \phi_{2}\left(v_{n}\right)=0 \\
\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \\
\phi_{n}\left(v_{1}\right)=0, & \phi_{n}\left(v_{2}\right)=0, & \ldots, \phi_{n}\left(v_{n-1}\right)=0, & \phi_{n}\left(v_{n}\right)=1
\end{array}
$$

By Theorem 5.2, these linear mappings $\phi_{i}$ are unique and well defined.

EXAMPLE 11.3 Consider the basis $\left\{v_{1}=(2,1), \quad v_{2}=(3,1)\right\}$ of $\mathbf{R}^{2}$. Find the dual basis $\left\{\phi_{1}, \phi_{2}\right\}$.

We seek linear functionals $\phi_{1}(x, y)=a x+b y$ and $\phi_{2}(x, y)=c x+d y$ such that

$$
\phi_{1}\left(v_{1}\right)=1, \quad \phi_{1}\left(v_{2}\right)=0, \quad \phi_{2}\left(v_{2}\right)=0, \quad \phi_{2}\left(v_{2}\right)=1
$$

These four conditions lead to the following two systems of linear equations:

$$
\left.\left.\begin{array}{l}
\phi_{1}\left(v_{1}\right)=\phi_{1}(2,1)=2 a+b=1 \\
\phi_{1}\left(v_{2}\right)=\phi_{1}(3,1)=3 a+b=0
\end{array}\right\} \quad \text { and } \quad \begin{array}{l}
\phi_{2}\left(v_{1}\right)=\phi_{2}(2,1)=2 c+d=0 \\
\phi_{2}\left(v_{2}\right)=\phi_{2}(3,1)=3 c+d=1
\end{array}\right\}
$$

The solutions yield $a=-1, b=3$ and $c=1, d=-2$. Hence, $\phi_{1}(x, y)=-x+3 y$ and $\phi_{2}(x, y)=x-2 y$ form the dual basis.

The next two theorems (proved in Problems 11.4 and 11.5, respectively) give relationships between bases and their duals.

THEOREM 11.2: Let $\left\{v_{1}, \ldots, v_{n}\right\}$ be a basis of $V$ and let $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ be the dual basis in $V^{*}$. Then

(i) For any vector $u \in V, u=\phi_{1}(u) v_{1}+\phi_{2}(u) v_{2}+\cdots+\phi_{n}(u) v_{n}$.

(ii) For any linear functional $\sigma \in V^{*}, \sigma=\sigma\left(v_{1}\right) \phi_{1}+\sigma\left(v_{2}\right) \phi_{2}+\cdots+\sigma\left(v_{n}\right) \phi_{n}$.

THEOREM 11.3: Let $\left\{v_{1}, \ldots, v_{n}\right\}$ and $\left\{w_{1}, \ldots, w_{n}\right\}$ be bases of $V$ and let $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ and $\left\{\sigma_{1}, \ldots, \sigma_{n}\right\}$ be the bases of $V^{*}$ dual to $\left\{v_{i}\right\}$ and $\left\{w_{i}\right\}$, respectively. Suppose $P$ is the change-of-basis matrix from $\left\{v_{i}\right\}$ to $\left\{w_{i}\right\}$. Then $\left(P^{-1}\right)^{T}$ is the change-of-basis matrix from $\left\{\phi_{i}\right\}$ to $\left\{\sigma_{i}\right\}$.

\subsection*{11.4 Second Dual Space}
We repeat: Every vector space $V$ has a dual space $V^{*}$, which consists of all the linear functionals on $V$. Thus, $V^{*}$ has a dual space $V^{* *}$, called the second dual of $V$, which consists of all the linear functionals on $V^{*}$.

We now show that each $v \in V$ determines a specific element $\hat{v} \in V^{* *}$. First, for any $\phi \in V^{*}$, we define

$$
\hat{v}(\phi)=\phi(v)
$$

It remains to be shown that this map $\hat{v}: V^{*} \rightarrow K$ is linear. For any scalars $a, b \in K$ and any linear functionals $\phi, \sigma \in V^{*}$, we have

$$
\hat{v}(a \phi+b \sigma)=(a \phi+b \sigma)(v)=a \phi(v)+b \sigma(v)=a \hat{v}(\phi)+b \hat{v}(\sigma)
$$

That is, $\hat{v}$ is linear and so $\hat{v} \in V^{* *}$. The following theorem (proved in Problem 12.7) holds.

THEOREM 11.4: If $V$ has finite dimensions, then the mapping $v \mapsto \hat{v}$ is an isomorphism of $V$ onto $V^{* *}$.

The above mapping $v \mapsto \hat{v}$ is called the natural mapping of $V$ into $V^{* *}$. We emphasize that this mapping is never onto $V^{* *}$ if $V$ is not finite-dimensional. However, it is always linear, and moreover, it is always one-to-one.

Now suppose $V$ does have finite dimension. By Theorem 11.4, the natural mapping determines an isomorphism between $V$ and $V^{* *}$. Unless otherwise stated, we will identify $V$ with $V^{* *}$ by this mapping. Accordingly, we will view $V$ as the space of linear functionals on $V^{*}$ and write $V=V^{* *}$. We remark that if $\left\{\phi_{i}\right\}$ is the basis of $V^{*}$ dual to a basis $\left\{v_{i}\right\}$ of $V$, then $\left\{v_{i}\right\}$ is the basis of $V^{* *}=V$ that is dual to $\left\{\phi_{i}\right\}$.

\subsection*{11.5 Annihilators}
Let $W$ be a subset (not necessarily a subspace) of a vector space $V$. A linear functional $\phi \in V^{*}$ is called an annihilator of $W$ if $\phi(w)=0$ for every $w \in W$ - that is, if $\phi(W)=\{0\}$. We show that the set of all such mappings, denoted by $W^{0}$ and called the annihilator of $W$, is a subspace of $V^{*}$. Clearly, $0 \in W^{0}$. Now suppose $\phi, \sigma \in W^{0}$. Then, for any scalars $a, b, \in K$ and for any $w \in W$,

$$
(a \phi+b \sigma)(w)=a \phi(w)+b \sigma(w)=a 0+b 0=0
$$

Thus, $a \phi+b \sigma \in W^{0}$, and so $W^{0}$ is a subspace of $V^{*}$.

In the case that $W$ is a subspace of $V$, we have the following relationship between $W$ and its annihilator $W^{0}$ (see Problem 11.11 for the proof).

THEOREM 11.5: $\quad$ Suppose $V$ has finite dimension and $W$ is a subspace of $V$. Then\\
(i) $\operatorname{dim} W+\operatorname{dim} W^{0}=\operatorname{dim} V$\\
and\\
(ii) $W^{00}=W$

Here $W^{00}=\left\{v \in V: \phi(v)=0\right.$ for every $\left.\phi \in W^{0}\right\}$ or, equivalently, $W^{00}=\left(W^{0}\right)^{0}$, where $W^{00}$ is viewed as a subspace of $V$ under the identification of $V$ and $V^{* *}$.

\subsection*{11.6 Transpose of a Linear Mapping}
Let $T: V \rightarrow U$ be an arbitrary linear mapping from a vector space $V$ into a vector space $U$. Now for any linear functional $\phi \in U^{*}$, the composition $\phi \circ T$ is a linear mapping from $V$ into $K$ :

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-358}
\end{center}

That is, $\phi \circ T \in V^{*}$. Thus, the correspondence

$$
\phi \mapsto \phi \circ T
$$

is a mapping from $U^{*}$ into $V^{*}$; we denote it by $T^{t}$ and call it the transpose of $T$. In other words, $T^{t}: U^{*} \rightarrow V^{*}$ is defined by

$$
T^{t}(\phi)=\phi \circ T
$$

Thus, $\left(T^{t}(\phi)\right)(v)=\phi(T(v))$ for every $v \in V$.

THEOREM 11.6: The transpose mapping $T^{t}$ defined above is linear.

Proof. For any scalars $a, b \in K$ and any linear functionals $\phi, \sigma \in U^{*}$,

$$
T^{t}(a \phi+b \sigma)=(a \phi+b \sigma) \circ T=a(\phi \circ T)+b(\sigma \circ T)=a T^{t}(\phi)+b T^{t}(\sigma)
$$

That is, $T^{t}$ is linear, as claimed.

We emphasize that if $T$ is a linear mapping from $V$ into $U$, then $T^{t}$ is a linear mapping from $U^{*}$ into $V^{*}$. The same "transpose" for the mapping $T^{t}$ no doubt derives from the following theorem (proved in Problem 11.16).

THEOREM 11.7: Let $T: V \rightarrow U$ be linear, and let $A$ be the matrix representation of $T$ relative to bases $\left\{v_{i}\right\}$ of $V$ and $\left\{u_{i}\right\}$ of $U$. Then the transpose matrix $A^{T}$ is the matrix representation of $T^{t}: U^{*} \rightarrow V^{*}$ relative to the bases dual to $\left\{u_{i}\right\}$ and $\left\{v_{i}\right\}$.

\section*{SOLVED PROBLEMS}
\section*{Dual Spaces and Dual Bases}
11.1. Find the basis $\left\{\phi_{1}, \phi_{2}, \phi_{3}\right\}$ that is dual to the following basis of $\mathbf{R}^{3}$ :

$$
\left\{v_{1}=(1,-1,3), \quad v_{2}=(0,1,-1), \quad v_{3}=(0,3,-2)\right\}
$$

The linear functionals may be expressed in the form

$$
\phi_{1}(x, y, z)=a_{1} x+a_{2} y+a_{3} z, \quad \phi_{2}(x, y, z)=b_{1} x+b_{2} y+b_{3} z, \quad \phi_{3}(x, y, z)=c_{1} x+c_{2} y+c_{3} z
$$

By definition of the dual basis, $\phi_{i}\left(v_{j}\right)=0$ for $i \neq j$, but $\phi_{i}\left(v_{j}\right)=1$ for $i=j$.

We find $\phi_{1}$ by setting $\phi_{1}\left(v_{1}\right)=1, \phi_{1}\left(v_{2}\right)=0, \phi_{1}\left(v_{3}\right)=0$. This yields

$$
\phi_{1}(1,-1,3)=a_{1}-a_{2}+3 a_{3}=1, \quad \phi_{1}(0,1,-1)=a_{2}-a_{3}=0, \quad \phi_{1}(0,3,-2)=3 a_{2}-2 a_{3}=0
$$

Solving the system of equations yields $a_{1}=1, a_{2}=0, a_{3}=0$. Thus, $\phi_{1}(x, y, z)=x$.

We find $\phi_{2}$ by setting $\phi_{2}\left(v_{1}\right)=0, \phi_{2}\left(v_{2}\right)=1, \phi_{2}\left(v_{3}\right)=0$. This yields

$$
\phi_{2}(1,-1,3)=b_{1}-b_{2}+3 b_{3}=0, \quad \phi_{2}(0,1,-1)=b_{2}-b_{3}=1, \quad \phi_{2}(0,3,-2)=3 b_{2}-2 b_{3}=0
$$

Solving the system of equations yields $b_{1}=7, b_{2}=-2, a_{3}=-3$. Thus, $\phi_{2}(x, y, z)=7 x-2 y-3 z$.

We find $\phi_{3}$ by setting $\phi_{3}\left(v_{1}\right)=0, \phi_{3}\left(v_{2}\right)=0, \phi_{3}\left(v_{3}\right)=1$. This yields

$$
\phi_{3}(1,-1,3)=c_{1}-c_{2}+3 c_{3}=0, \quad \phi_{3}(0,1,-1)=c_{2}-c_{3}=0, \quad \phi_{3}(0,3,-2)=3 c_{2}-2 c_{3}=1
$$

Solving the system of equations yields $c_{1}=-2, c_{2}=1, c_{3}=1$. Thus, $\phi_{3}(x, y, z)=-2 x+y+z$.

11.2. Let $V=\{a+b t: a, b \in \mathbf{R}\}$, the vector space of real polynomials of degree $\leq 1$. Find the basis $\left\{v_{1}, v_{2}\right\}$ of $V$ that is dual to the basis $\left\{\phi_{1}, \phi_{2}\right\}$ of $V^{*}$ defined by

$$
\phi_{1}(f(t))=\int_{0}^{1} f(t) d t \quad \text { and } \quad \phi_{2}(f(t))=\int_{0}^{2} f(t) d t
$$

Let $v_{1}=a+b t$ and $v_{2}=c+d t$. By definition of the dual basis,

$$
\phi_{1}\left(v_{1}\right)=1, \quad \phi_{1}\left(v_{2}\right)=0 \quad \text { and } \quad \phi_{2}\left(v_{1}\right)=0, \quad \phi_{i}\left(v_{j}\right)=1
$$

Thus,

$$
\left.\left.\begin{array}{l}
\phi_{1}\left(v_{1}\right)=\int_{0}^{1}(a+b t) d t=a+\frac{1}{2} b=1 \\
\phi_{2}\left(v_{1}\right)=\int_{0}^{2}(a+b t) d t=2 a+2 b=0
\end{array}\right\} \quad \text { and } \quad \begin{array}{c}
\phi_{1}\left(v_{2}\right)=\int_{0}^{1}(c+d t) d t=c+\frac{1}{2} d=0 \\
\phi_{2}\left(v_{2}\right)=\int_{0}^{2}(c+d t) d t=2 c+2 d=1
\end{array}\right\}
$$

Solving each system yields $a=2, \quad b=-2$ and $c=-\frac{1}{2}, \quad d=1$. Thus, $\left\{v_{1}=2-2 t, \quad v_{2}=-\frac{1}{2}+t\right\}$ is the basis of $V$ that is dual to $\left\{\phi_{1}, \phi_{2}\right\}$.

11.3. Prove Theorem 11.1: Suppose $\left\{v_{1}, \ldots, v_{n}\right\}$ is a basis of $V$ over $K$. Let $\phi_{1}, \ldots, \phi_{n} \in V^{*}$ be defined by $\phi_{i}\left(v_{j}\right)=0$ for $i \neq j$, but $\phi_{i}\left(v_{j}\right)=1$ for $i=j$. Then $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ is a basis of $V^{*}$.

We first show that $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ spans $V^{*}$. Let $\phi$ be an arbitrary element of $V^{*}$, and suppose

$$
\phi\left(v_{1}\right)=k_{1}, \quad \phi\left(v_{2}\right)=k_{2}, \quad \ldots, \quad \phi\left(v_{n}\right)=k_{n}
$$

Set $\sigma=k_{1} \phi_{1}+\cdots+k_{n} \phi_{n}$. Then

$$
\begin{aligned}
\sigma\left(v_{1}\right) & =\left(k_{1} \phi_{1}+\cdots+k_{n} \phi_{n}\right)\left(v_{1}\right)=k_{1} \phi_{1}\left(v_{1}\right)+k_{2} \phi_{2}\left(v_{1}\right)+\cdots+k_{n} \phi_{n}\left(v_{1}\right) \\
& =k_{1} \cdot 1+k_{2} \cdot 0+\cdots+k_{n} \cdot 0=k_{1}
\end{aligned}
$$

Similarly, for $i=2, \ldots, n$,

$$
\sigma\left(v_{i}\right)=\left(k_{1} \phi_{1}+\cdots+k_{n} \phi_{n}\right)\left(v_{i}\right)=k_{1} \phi_{1}\left(v_{i}\right)+\cdots+k_{i} \phi_{i}\left(v_{i}\right)+\cdots+k_{n} \phi_{n}\left(v_{i}\right)=k_{i}
$$

Thus, $\phi\left(v_{i}\right)=\sigma\left(v_{i}\right)$ for $i=1, \ldots, n$. Because $\phi$ and $\sigma$ agree on the basis vectors, $\phi=\sigma=k_{1} \phi_{1}+\cdots+k_{n} \phi_{n}$. Accordingly, $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ spans $V^{*}$.

It remains to be shown that $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ is linearly independent. Suppose

$$
a_{1} \phi_{1}+a_{2} \phi_{2}+\cdots+a_{n} \phi_{n}=\mathbf{0}
$$

Applying both sides to $v_{1}$, we obtain

$$
\begin{aligned}
0 & =\mathbf{0}\left(v_{1}\right)=\left(a_{1} \phi_{1}+\cdots+a_{n} \phi_{n}\right)\left(v_{1}\right)=a_{1} \phi_{1}\left(v_{1}\right)+a_{2} \phi_{2}\left(v_{1}\right)+\cdots+a_{n} \phi_{n}\left(v_{1}\right) \\
& =a_{1} \cdot 1+a_{2} \cdot 0+\cdots+a_{n} \cdot 0=a_{1}
\end{aligned}
$$

Similarly, for $i=2, \ldots, n$,

$$
0=\mathbf{0}\left(v_{i}\right)=\left(a_{1} \phi_{1}+\cdots+a_{n} \phi_{n}\right)\left(v_{i}\right)=a_{1} \phi_{1}\left(v_{i}\right)+\cdots+a_{i} \phi_{i}\left(v_{i}\right)+\cdots+a_{n} \phi_{n}\left(v_{i}\right)=a_{i}
$$

That is, $a_{1}=0, \ldots, a_{n}=0$. Hence, $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ is linearly independent, and so it is a basis of $V^{*}$.

11.4. Prove Theorem 11.2: Let $\left\{v_{1}, \ldots, v_{n}\right\}$ be a basis of $V$ and let $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ be the dual basis in $V^{*}$. For any $u \in V$ and any $\sigma \in V^{*}$, (i) $u=\sum_{i} \phi_{i}(u) v_{i}$. (ii) $\sigma=\sum_{i} \phi\left(v_{i}\right) \phi_{i}$.

Suppose


\begin{equation*}
u=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n} \tag{1}
\end{equation*}


Then

$$
\phi_{1}(u)=a_{1} \phi_{1}\left(v_{1}\right)+a_{2} \phi_{1}\left(v_{2}\right)+\cdots+a_{n} \phi_{1}\left(v_{n}\right)=a_{1} \cdot 1+a_{2} \cdot 0+\cdots+a_{n} \cdot 0=a_{1}
$$

Similarly, for $i=2, \ldots, n$,

$$
\phi_{i}(u)=a_{1} \phi_{i}\left(v_{1}\right)+\cdots+a_{i} \phi_{i}\left(v_{i}\right)+\cdots+a_{n} \phi_{i}\left(v_{n}\right)=a_{i}
$$

That is, $\phi_{1}(u)=a_{1}, \phi_{2}(u)=a_{2}, \ldots, \phi_{n}(u)=a_{n}$. Substituting these results into (1), we obtain (i).

Next we prove (ii). Applying the linear functional $\sigma$ to both sides of (i),

$$
\begin{aligned}
\sigma(u) & =\phi_{1}(u) \sigma\left(v_{1}\right)+\phi_{2}(u) \sigma\left(v_{2}\right)+\cdots+\phi_{n}(u) \sigma\left(v_{n}\right) \\
& =\sigma\left(v_{1}\right) \phi_{1}(u)+\sigma\left(v_{2}\right) \phi_{2}(u)+\cdots+\sigma\left(v_{n}\right) \phi_{n}(u) \\
& =\left(\sigma\left(v_{1}\right) \phi_{1}+\sigma\left(v_{2}\right) \phi_{2}+\cdots+\sigma\left(v_{n}\right) \phi_{n}\right)(u)
\end{aligned}
$$

Because the above holds for every $u \in V, \sigma=\sigma\left(v_{1}\right) \phi_{2}+\sigma\left(v_{2}\right) \phi_{2}+\cdots+\sigma\left(v_{n}\right) \phi_{n}$, as claimed.

11.5. Prove Theorem 11.3. Let $\left\{v_{i}\right\}$ and $\left\{w_{i}\right\}$ be bases of $V$ and let $\left\{\phi_{i}\right\}$ and $\left\{\sigma_{i}\right\}$ be the respective dual bases in $V^{*}$. Let $P$ be the change-of-basis matrix from $\left\{v_{i}\right\}$ to $\left\{w_{i}\right\}$. Then $\left(P^{-1}\right)^{T}$ is the change-of-basis matrix from $\left\{\phi_{i}\right\}$ to $\left\{\sigma_{i}\right\}$.

Suppose, for $i=1, \ldots, n$,

$$
w_{i}=a_{i 1} v_{1}+a_{i 2} v_{2}+\cdots+a_{i n} v_{n} \quad \text { and } \quad \sigma_{i}=b_{i 1} \phi_{1}+b_{i 2} \phi_{2}+\cdots+a_{i n} v_{n}
$$

Then $P=\left[a_{i j}\right]$ and $Q=\left[b_{i j}\right]$. We seek to prove that $Q=\left(P^{-1}\right)^{T}$.

Let $R_{i}$ denote the $i$ th row of $Q$ and let $C_{j}$ denote the $j$ th column of $P^{T}$. Then

$$
R_{i}=\left(b_{i 1}, b_{i 2}, \ldots, b_{i n}\right) \quad \text { and } \quad C_{j}=\left(a_{j 1}, a_{j 2}, \ldots, a_{j n}\right)^{T}
$$

By definition of the dual basis,

$$
\begin{aligned}
\sigma_{i}\left(w_{j}\right) & =\left(b_{i 1} \phi_{1}+b_{i 2} \phi_{2}+\cdots+b_{i n} \phi_{n}\right)\left(a_{j 1} v_{1}+a_{j 2} v_{2}+\cdots+a_{j n} v_{n}\right) \\
& =b_{i 1} a_{j 1}+b_{i 2} a_{j 2}+\cdots+b_{i n} a_{j n}=R_{i} C_{j}=\delta_{i j}
\end{aligned}
$$

where $\delta_{i j}$ is the Kronecker delta. Thus,

$$
Q P^{T}=\left[R_{i} C_{j}\right]=\left[\delta_{i j}\right]=I
$$

Therefore, $Q=\left(P^{T}\right)^{-1}=\left(P^{-1}\right)^{T}$, as claimed.

11.6. Suppose $v \in V, v \neq 0$, and $\operatorname{dim} V=n$. Show that there exists $\phi \in V^{*}$ such that $\phi(v) \neq 0$.

We extend $\{v\}$ to a basis $\left\{v, v_{2}, \ldots, v_{n}\right\}$ of $V$. By Theorem 5.2, there exists a unique linear mapping $\phi: V \rightarrow K$ such that $\phi(v)=1$ and $\phi\left(v_{i}\right)=0, i=2, \ldots, n$. Hence, $\phi$ has the desired property.

11.7. Prove Theorem 11.4: Suppose $\operatorname{dim} V=n$. Then the natural mapping $v \mapsto \hat{v}$ is an isomorphism of $V$ onto $V^{* *}$.

We first prove that the map $v \mapsto \hat{v}$ is linear-that is, for any vectors $v, w \in V$ and any scalars $a, b \in K$, $\widehat{a v+b w}=a \hat{v}+b \hat{w}$. For any linear functional $\phi \in V^{*}$,

$$
\widehat{a v+b w}(\phi)=\phi(a v+b w)=a \phi(v)+b \phi(w)=a \hat{v}(\phi)+b \hat{w}(\phi)=(a \hat{v}+b \hat{w})(\phi)
$$

Because $\widehat{a v+b w}(\phi)=(a \hat{v}+b \hat{w})(\phi)$ for every $\phi \in V^{*}$, we have $\widehat{a v+b w}=a \hat{v}+b \hat{w}$. Thus, the map $v \mapsto \hat{v}$ is linear.

Now suppose $v \in V, v \neq 0$. Then, by Problem 11.6, there exists $\phi \in V^{*}$ for which $\phi(v) \neq 0$. Hence, $\hat{v}(\phi)=\phi(v) \neq 0$, and thus $\hat{v} \neq 0$. Because $v \neq 0$ implies $\hat{v} \neq 0$, the map $v \mapsto \hat{v}$ is nonsingular and hence an isomorphism (Theorem 5.64).

Now $\operatorname{dim} V=\operatorname{dim} V^{*}=\operatorname{dim} V^{* *}$, because $V$ has finite dimension. Accordingly, the mapping $v \mapsto \hat{v}$ is an isomorphism of $V$ onto $V^{* *}$.

\section*{Annihilators}
11.8. Show that if $\phi \in V^{*}$ annihilates a subset $S$ of $V$, then $\phi$ annihilates the linear span $L(S)$ of $S$. Hence, $S^{0}=[\operatorname{span}(S)]^{0}$.

Suppose $v \in \operatorname{span}(S)$. Then there exists $w_{1}, \ldots, w_{r} \in S$ for which $v=a_{1} w_{1}+a_{2} w_{2}+\cdots+a_{r} w_{r}$.

$$
\phi(v)=a_{1} \phi\left(w_{1}\right)+a_{2} \phi\left(w_{2}\right)+\cdots+a_{r} \phi\left(w_{r}\right)=a_{1} 0+a_{2} 0+\cdots+a_{r} 0=0
$$

Because $v$ was an arbitrary element of $\operatorname{span}(S), \phi$ annihilates $\operatorname{span}(S)$, as claimed.

11.9. Find a basis of the annihilator $W^{0}$ of the subspace $W$ of $\mathbf{R}^{4}$ spanned by

$$
v_{1}=(1,2,-3,4) \quad \text { and } \quad v_{2}=(0,1,4,-1)
$$

By Problem 11.8, it suffices to find a basis of the set of linear functionals $\phi$ such that $\phi\left(v_{1}\right)=0$ and $\phi\left(v_{2}\right)=0$, where $\phi\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=a x_{1}+b x_{2}+c x_{3}+d x_{4}$. Thus,

$$
\phi(1,2,-3,4)=a+2 b-3 c+4 d=0 \quad \text { and } \quad \phi(0,1,4,-1)=b+4 c-d=0
$$

The system of two equations in the unknowns $a, b, c, d$ is in echelon form with free variables $c$ and $d$.

(1) Set $c=1, d=0$ to obtain the solution $a=11, b=-4, c=1, d=0$.

(2) Set $c=0, d=1$ to obtain the solution $a=6, b=-1, c=0, d=1$.

The linear functions $\phi_{1}\left(x_{i}\right)=11 x_{1}-4 x_{2}+x_{3}$ and $\phi_{2}\left(x_{i}\right)=6 x_{1}-x_{2}+x_{4}$ form a basis of $W^{0}$.

11.10. Show that (a) For any subset $S$ of $V, S \subseteq S^{00}$. (b) If $S_{1} \subseteq S_{2}$, then $S_{2}^{0} \subseteq S_{1}^{0}$.

(a) Let $v \in S$. Then for every linear functional $\phi \in S^{0}, \hat{v}(\phi)=\phi(v)=0$. Hence, $\hat{v} \in\left(S^{0}\right)^{0}$. Therefore, under the identification of $V$ and $V^{* *}, v \in S^{00}$. Accordingly, $S \subseteq S^{00}$.

(b) Let $\phi \in S_{2}^{0}$. Then $\phi(v)=0$ for every $v \in S_{2}$. But $S_{1} \subseteq S_{2}$; hence, $\phi$ annihilates every element of $S_{1}$ (i.e., $\phi \in S_{1}^{0}$ ). Therefore, $S_{2}^{0} \subseteq S_{1}^{0}$.

11.11. Prove Theorem 11.5: Suppose $V$ has finite dimension and $W$ is a subspace of $V$. Then

(i) $\operatorname{dim} W+\operatorname{dim} W^{0}=\operatorname{dim} V$, (ii) $W^{00}=W$.

(i) Suppose $\operatorname{dim} V=n$ and $\operatorname{dim} W=r \leq n$. We want to show that $\operatorname{dim} W^{0}=n-r$. We choose a basis $\left\{w_{1}, \ldots, w_{r}\right\}$ of $W$ and extend it to a basis of $V$, say $\left\{w_{1}, \ldots, w_{r}, v_{1}, \ldots, v_{n-r}\right\}$. Consider the dual basis

$$
\left\{\phi_{1}, \ldots, \phi_{r}, \sigma_{1}, \ldots, \sigma_{n-r}\right\}
$$

By definition of the dual basis, each of the above $\sigma$ 's annihilates each $w_{i}$; hence, $\sigma_{1}, \ldots, \sigma_{n-r} \in W^{0}$. We claim that $\left\{\sigma_{i}\right\}$ is a basis of $W^{0}$. Now $\left\{\sigma_{j}\right\}$ is part of a basis of $V^{*}$, and so it is linearly independent.

We next show that $\left\{\phi_{j}\right\}$ spans $W^{0}$. Let $\sigma \in W^{0}$. By Theorem 11.2,

$$
\begin{aligned}
\sigma & =\sigma\left(w_{1}\right) \phi_{1}+\cdots+\sigma\left(w_{r}\right) \phi_{r}+\sigma\left(v_{1}\right) \sigma_{1}+\cdots+\sigma\left(v_{n-r}\right) \sigma_{n-r} \\
& =0 \phi_{1}+\cdots+0 \phi_{r}+\sigma\left(v_{1}\right) \sigma_{1}+\cdots+\sigma\left(v_{n-r}\right) \sigma_{n-r} \\
& =\sigma\left(v_{1}\right) \sigma_{1}+\cdots+\sigma\left(v_{n-r}\right) \sigma_{n-r}
\end{aligned}
$$

Consequently, $\left\{\sigma_{1}, \ldots, \sigma_{n-r}\right\}$ spans $W^{0}$ and so it is a basis of $W^{0}$. Accordingly, as required $\operatorname{dim} W^{0}=n-r=\operatorname{dim} V-\operatorname{dim} W$.

(ii) Suppose $\operatorname{dim} V=n$ and $\operatorname{dim} W=r$. Then $\operatorname{dim} V^{*}=n$ and, by (i), $\operatorname{dim} W^{0}=n-r$. Thus, by (i), $\operatorname{dim} W^{00}=n-(n-r)=r$; therefore, $\operatorname{dim} W=\operatorname{dim} W^{00}$. By Problem 11.10, $W \subseteq W^{00}$. Accordingly, $W=W^{00}$.

11.12. Let $U$ and $W$ be subspaces of $V$. Prove that $(U+W)^{0}=U^{0} \cap W^{0}$.

Let $\phi \in(U+W)^{0}$. Then $\phi$ annihilates $U+W$, and so, in particular, $\phi$ annihilates $U$ and $W$. That is, $\phi \in U^{0}$ and $\phi \in W^{0}$; hence, $\phi \in U^{0} \cap W^{0}$. Thus, $(U+W)^{0} \subseteq U^{0} \cap W^{0}$.

On the other hand, suppose $\sigma \in U^{0} \cap W^{0}$. Then $\sigma$ annihilates $U$ and also $W$. If $v \in U+W$, then $v=u+w$, where $u \in U$ and $w \in W$. Hence, $\sigma(v)=\sigma(u)+\sigma(w)=0+0=0$. Thus, $\sigma$ annihilates $U+W$; that is, $\sigma \in(U+W)^{0}$. Accordingly, $U^{0}+W^{0} \subseteq(U+W)^{0}$.

The two inclusion relations together give us the desired equality.

Remark: Observe that no dimension argument is employed in the proof; hence, the result holds for spaces of finite or infinite dimension.

\section*{Transpose of a Linear Mapping}
11.13. Let $\phi$ be the linear functional on $\mathbf{R}^{2}$ defined by $\phi(x, y)=x-2 y$. For each of the following linear operators $T$ on $\mathbf{R}^{2}$, find $\left(T^{t}(\phi)\right)(x, y)$ :\\
(a) $T(x, y)=(x, 0)$,\\
(b) $T(x, y)=(y, x+y)$,\\
(c) $T(x, y)=(2 x-3 y, 5 x+2 y)$

By definition, $T^{t}(\phi)=\phi \circ T$; that is, $\left(T^{t}(\phi)\right)(v)=\phi(T(v))$ for every $v$. Hence,

(a) $\left(T^{t}(\phi)\right)(x, y)=\phi(T(x, y))=\phi(x, 0)=x$

(b) $\left(T^{t}(\phi)\right)(x, y)=\phi(T(x, y))=\phi(y, x+y)=y-2(x+y)=-2 x-y$

(c) $\left(T^{t}(\phi)\right)(x, y)=\phi(T(x, y))=\phi(2 x-3 y, 5 x+2 y)=(2 x-3 y)-2(5 x+2 y)=-8 x-7 y$

11.14. Let $T: V \rightarrow U$ be linear and let $T^{t}: U^{*} \rightarrow V^{*}$ be its transpose. Show that the kernel of $T^{t}$ is the annihilator of the image of $T$-that is, $\operatorname{Ker} T^{t}=(\operatorname{Im} T)^{0}$.

Suppose $\phi \in \operatorname{Ker} T^{t}$; that is, $T^{t}(\phi)=\phi \circ T=0$. If $u \in \operatorname{Im} T$, then $u=T(v)$ for some $v \in V$; hence,

$$
\phi(u)=\phi(T(v))=(\phi \circ T)(v)=\mathbf{0}(v)=0
$$

We have that $\phi(u)=0$ for every $u \in \operatorname{Im} T$; hence, $\phi \in(\operatorname{Im} T)^{0}$. Thus, $\operatorname{Ker} T^{t} \subseteq(\operatorname{Im} T)^{0}$.

On the other hand, suppose $\sigma \in(\operatorname{Im} T)^{0}$; that is, $\sigma(\operatorname{Im} T)=\{0\}$. Then, for every $v \in V$,

$$
\left(T^{t}(\sigma)\right)(v)=(\sigma \circ T)(v)=\sigma(T(v))=0=\mathbf{0}(v)
$$

We have $\left(T^{t}(\sigma)\right)(v)=\mathbf{0}(v)$ for every $v \in V$; hence, $T^{t}(\sigma)=0$. Thus, $\sigma \in \operatorname{Ker} T^{t}$, and so $(\operatorname{Im} T)^{0} \subseteq \operatorname{Ker} T^{t}$.

The two inclusion relations together give us the required equality.

11.15. Suppose $V$ and $U$ have finite dimension and $T: V \rightarrow U$ is linear. Prove $\operatorname{rank}(T)=\operatorname{rank}\left(T^{t}\right)$.

Suppose $\operatorname{dim} V=n$ and $\operatorname{dim} U=m$, and suppose $\operatorname{rank}(T)=r$. By Theorem 11.5,

$$
\operatorname{dim}(\operatorname{Im} T)^{0}=\operatorname{dim} u-\operatorname{dim}(\operatorname{Im} T)=m-\operatorname{rank}(T)=m-r
$$

By Problem 11.14, $\operatorname{Ker} T^{t}=(\operatorname{Im} T)^{0}$. Hence, nullity $\left(T^{t}\right)=m-r$. It then follows that, as claimed,

$$
\operatorname{rank}\left(T^{t}\right)=\operatorname{dim} U^{*}-\operatorname{nullity}\left(T^{t}\right)=m-(m-r)=r=\operatorname{rank}(T)
$$

11.16. Prove Theorem 11.7: Let $T: V \rightarrow U$ be linear and let $A$ be the matrix representation of $T$ in the bases $\left\{v_{j}\right\}$ of $V$ and $\left\{u_{i}\right\}$ of $U$. Then the transpose matrix $A^{T}$ is the matrix representation of $T^{t}: U^{*} \rightarrow V^{*}$ in the bases dual to $\left\{u_{i}\right\}$ and $\left\{v_{j}\right\}$.

Suppose, for $j=1, \ldots, m$,


\begin{equation*}
T\left(v_{j}\right)=a_{j 1} u_{1}+a_{j 2} u_{2}+\cdots+a_{j n} u_{n} \tag{1}
\end{equation*}


We want to prove that, for $i=1, \ldots, n$,


\begin{equation*}
T^{t}\left(\sigma_{i}\right)=a_{1 i} \phi_{1}+a_{2 i} \phi_{2}+\cdots+a_{m i} \phi_{m} \tag{2}
\end{equation*}


where $\left\{\sigma_{i}\right\}$ and $\left\{\phi_{j}\right\}$ are the bases dual to $\left\{u_{i}\right\}$ and $\left\{v_{j}\right\}$, respectively.

Let $v \in V$ and suppose $v=k_{1} v_{1}+k_{2} v_{2}+\cdots+k_{m} v_{m}$. Then, by (1),

$$
\begin{aligned}
T(v) & =k_{1} T\left(v_{1}\right)+k_{2} T\left(v_{2}\right)+\cdots+k_{m} T\left(v_{m}\right) \\
& =k_{1}\left(a_{11} u_{1}+\cdots+a_{1 n} u_{n}\right)+k_{2}\left(a_{21} u_{1}+\cdots+a_{2 n} u_{n}\right)+\cdots+k_{m}\left(a_{m 1} u_{1}+\cdots+a_{m n} u_{n}\right) \\
& =\left(k_{1} a_{11}+k_{2} a_{21}+\cdots+k_{m} a_{m 1}\right) u_{1}+\cdots+\left(k_{1} a_{1 n}+k_{2} a_{2 n}+\cdots+k_{m} a_{m n}\right) u_{n} \\
& =\sum_{i=1}^{n}\left(k_{1} a_{1 i}+k_{2} a_{2 i}+\cdots+k_{m} a_{m i}\right) u_{i}
\end{aligned}
$$

Hence, for $j=1, \ldots, n$.


\begin{align*}
\left(T^{t}\left(\sigma_{j}\right)(v)\right) & =\sigma_{j}(T(v))=\sigma_{j}\left(\sum_{i=1}^{n}\left(k_{1} a_{1 i}+k_{2} a_{2 i}+\cdots+k_{m} a_{m i}\right) u_{i}\right) \\
& =k_{1} a_{1 j}+k_{2} a_{2 j}+\cdots+k_{m} a_{m j} \tag{3}
\end{align*}


On the other hand, for $j=1, \ldots, n$,


\begin{align*}
\left(a_{1 j} \phi_{1}+a_{2 j} \phi_{2}+\cdots+a_{m j} \phi_{m}\right)(v) & =\left(a_{1 j} \phi_{1}+a_{2 j} \phi_{2}+\cdots+a_{m j} \phi_{m}\right)\left(k_{1} v_{1}+k_{2} v_{2}+\cdots+k_{m} v_{m}\right) \\
& =k_{1} a_{1 j}+k_{2} a_{2 j}+\cdots+k_{m} a_{m j} \tag{4}
\end{align*}


Because $v \in V$ was arbitrary, (3) and (4) imply that

$$
T^{t}\left(\sigma_{j}\right)=a_{1 j} \phi_{1}+a_{2 j} \phi_{2}+\cdots+a_{m j} \phi_{m}, \quad j=1, \ldots, n
$$

which is (2). Thus, the theorem is proved.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Dual Spaces and Dual Bases}
11.17. Find (a) $\phi+\sigma$, (b) $3 \phi$, (c) $2 \phi-5 \sigma$, where $\phi: \mathbf{R}^{3} \rightarrow \mathbf{R}$ and $\sigma: \mathbf{R}^{3} \rightarrow \mathbf{R}$ are defined by

$$
\phi(x, y, z)=2 x-3 y+z \quad \text { and } \quad \sigma(x, y, z)=4 x-2 y+3 z
$$

11.18. Find the dual basis of each of the following bases of $\mathbf{R}^{3}:(a)\{(1,0,0),(0,1,0),(0,0,1)\}$, (b) $\{(1,-2,3),(1,-1,1),(2,-4,7)\}$.

11.19. Let $V$ be the vector space of polynomials over $\mathbf{R}$ of degree $\leq 2$. Let $\phi_{1}, \phi_{2}, \phi_{3}$ be the linear functionals on $V$ defined by

$$
\phi_{1}(f(t))=\int_{0}^{1} f(t) d t, \quad \phi_{2}(f(t))=f^{\prime}(1), \quad \phi_{3}(f(t))=f(0)
$$

Here $f(t)=a+b t+c t^{2} \in V$ and $f^{\prime}(t)$ denotes the derivative of $f(t)$. Find the basis $\left\{f_{1}(t), f_{2}(t), f_{3}(t)\right\}$ of $V$ that is dual to $\left\{\phi_{1}, \phi_{2}, \phi_{3}\right\}$.

11.20. Suppose $u, v \in V$ and that $\phi(u)=0$ implies $\phi(v)=0$ for all $\phi \in V^{*}$. Show that $v=k u$ for some scalar $k$.

11.21. Suppose $\phi, \sigma \in V^{*}$ and that $\phi(v)=0$ implies $\sigma(v)=0$ for all $v \in V$. Show that $\sigma=k \phi$ for some scalar $k$.

11.22. Let $V$ be the vector space of polynomials over $K$. For $a \in K$, define $\phi_{a}: V \rightarrow K$ by $\phi_{a}(f(t))=f(a)$. Show that (a) $\phi_{a}$ is linear; (b) if $a \neq b$, then $\phi_{a} \neq \phi_{b}$.

11.23. Let $V$ be the vector space of polynomials of degree $\leq 2$. Let $a, b, c \in K$ be distinct scalars. Let $\phi_{a}, \phi_{b}, \phi_{c}$ be the linear functionals defined by $\phi_{a}(f(t))=f(a), \phi_{b}(f(t))=f(b), \phi_{c}(f(t))=f(c)$. Show that $\left\{\phi_{a}, \phi_{b}, \phi_{c}\right\}$ is linearly independent, and find the basis $\left\{f_{1}(t), f_{2}(t), f_{3}(t)\right\}$ of $V$ that is its dual.

11.24. Let $V$ be the vector space of square matrices of order $n$. Let $T: V \rightarrow K$ be the trace mapping; that is, $T(A)=a_{11}+a_{22}+\cdots+a_{n n}$, where $A=\left(a_{i j}\right)$. Show that $T$ is linear.

11.25. Let $W$ be a subspace of $V$. For any linear functional $\phi$ on $W$, show that there is a linear functional $\sigma$ on $V$ such that $\sigma(w)=\phi(w)$ for any $w \in W$; that is, $\phi$ is the restriction of $\sigma$ to $W$.

11.26. Let $\left\{e_{1}, \ldots, e_{n}\right\}$ be the usual basis of $K^{n}$. Show that the dual basis is $\left\{\pi_{1}, \ldots, \pi_{n}\right\}$ where $\pi_{i}$ is the $i$ th projection mapping; that is, $\pi_{i}\left(a_{1}, \ldots, a_{n}\right)=a_{i}$.

11.27. Let $V$ be a vector space over $\mathbf{R}$. Let $\phi_{1}, \phi_{2} \in V^{*}$ and suppose $\sigma: V \rightarrow \mathbf{R}$, defined by $\sigma(v)=\phi_{1}(v) \phi_{2}(v)$, also belongs to $V^{*}$. Show that either $\phi_{1}=\mathbf{0}$ or $\phi_{2}=\mathbf{0}$.

\section*{Annihilators}
11.28. Let $W$ be the subspace of $\mathbf{R}^{4}$ spanned by $(1,2,-3,4),(1,3,-2,6),(1,4,-1,8)$. Find a basis of the annihilator of $W$.

11.29. Let $W$ be the subspace of $\mathbf{R}^{3}$ spanned by $(1,1,0)$ and $(0,1,1)$. Find a basis of the annihilator of $W$.

11.30. Show that, for any subset $S$ of $V, \operatorname{span}(S)=S^{00}$, where $\operatorname{span}(S)$ is the linear span of $S$.

11.31. Let $U$ and $W$ be subspaces of a vector space $V$ of finite dimension. Prove that $(U \cap W)^{0}=U^{0}+W^{0}$.

11.32. Suppose $V=U \oplus W$. Prove that $V^{0}=U^{0} \oplus W^{0}$.

\section*{Transpose of a Linear Mapping}
11.33. Let $\phi$ be the linear functional on $\mathbf{R}^{2}$ defined by $\phi(x, y)=3 x-2 y$. For each of the following linear mappings $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}$, find $\left(T^{t}(\phi)\right)(x, y, z)$ :\\
(a) $T(x, y, z)=(x+y, y+z)$,\\
(b) $T(x, y, z)=(x+y+z, 2 x-y)$

11.34. Suppose $T_{1}: U \rightarrow V$ and $T_{2}: V \rightarrow W$ are linear. Prove that $\left(T_{2} \circ T_{1}\right)^{t}=T_{1}^{t} \circ T_{2}^{t}$.

11.35. Suppose $T: V \rightarrow U$ is linear and $V$ has finite dimension. Prove that $\operatorname{Im} T^{t}=(\operatorname{Ker} T)^{0}$.

11.36. Suppose $T: V \rightarrow U$ is linear and $u \in U$. Prove that $u \in \operatorname{Im} T$ or there exists $\phi \in V^{*}$ such that $T^{t}(\phi)=0$ and $\phi(u)=1$.

11.37. Let $V$ be of finite dimension. Show that the mapping $T \mapsto T^{t}$ is an isomorphism from $\operatorname{Hom}(V, V)$ onto $\operatorname{Hom}\left(V^{*}, V^{*}\right)$. (Here $T$ is any linear operator on $V$.)

\section*{Miscellaneous Problems}
11.38. Let $V$ be a vector space over $\mathbf{R}$. The line segment $\overline{u v}$ joining points $u, v \in V$ is defined by $\overline{u v}=\{t u+(1-t) v: 0 \leq t \leq 1\}$. A subset $S$ of $V$ is convex if $u, v \in S$ implies $\overline{u v} \subseteq S$. Let $\phi \in V^{*}$. Define

$$
W^{+}=\{v \in V: \phi(v)>0\}, \quad W=\{v \in V: \phi(v)=0\}, \quad W^{-}=\{v \in V: \phi(v)<0\}
$$

Prove that $W^{+}, W$, and $W^{-}$are convex.

11.39. Let $V$ be a vector space of finite dimension. A hyperplane $H$ of $V$ may be defined as the kernel of a nonzero linear functional $\phi$ on $V$. Show that every subspace of $V$ is the intersection of a finite number of hyperplanes.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
11.17. (a) $6 x-5 y+4 z$,\\
(b) $6 x-9 y+3 z$,\\
(c) $-16 x+4 y-13 z$

11.18. (a) $\phi_{1}=x, \phi_{2}=y, \phi_{3}=z$;

(b) $\phi_{1}=-3 x-5 y-2 z, \phi_{2}=2 x+y, \phi_{3}=x+2 y+z$

11.19. $f_{1}(t)=3 t-\frac{3}{2} t^{2}, \quad f_{2}(t)=-\frac{1}{2} t+\frac{3}{4} t^{2}, \quad f_{3}(t)=1-3 t+\frac{3}{2} t^{2}$

11.22. (b) Let $f(t)=t$. Then $\phi_{a}(f(t))=a \neq b=\phi_{b}(f(t))$; and therefore, $\phi_{a} \neq \phi_{b}$

11.23. $\left\{f_{1}(t)=\frac{t^{2}-(b+c) t+b c}{(a-b)(a-c)}, f_{2}(t)=\frac{t^{2}-(a+c) t+a c}{(b-a)(b-c)}, f_{3}(t)=\frac{t^{2}-(a+b) t+a b}{(c-a)(c-b)}\right\}$

11.28. $\left\{\phi_{1}(x, y, z, t)=5 x-y+z, \quad \phi_{2}(x, y, z, t)=2 y-t\right\}$

11.29. $\{\phi(x, y, z)=x-y+z\}$

11.33. (a) $\left(T^{t}(\phi)\right)(x, y, z)=3 x+y-2 z$,

(b) $\left(T^{t}(\phi)\right)(x, y, z)=-x+5 y+3 z$

\section*{CHAPTER 12}
\section*{Bilinear, Quadratic, and Hermitian Forms}
\subsection*{12.1 Introduction}
This chapter generalizes the notions of linear mappings and linear functionals. Specifically, we introduce the notion of a bilinear form. These bilinear maps also give rise to quadratic and Hermitian forms. Although quadratic forms were discussed previously, this chapter is treated independently of the previous results.

Although the field $K$ is arbitrary, we will later specialize to the cases $K=\mathbf{R}$ and $K=\mathbf{C}$. Furthermore, we may sometimes need to divide by 2 . In such cases, we must assume that $1+1 \neq 0$, which is true when $K=\mathbf{R}$ or $K=\mathbf{C}$.

\subsection*{12.2 Bilinear Forms}
Let $V$ be a vector space of finite dimension over a field $K$. A bilinear form on $V$ is a mapping $f: V \times V \rightarrow K$ such that, for all $a, b \in K$ and all $u_{i}, v_{i} \in V$ :

(i) $f\left(a u_{1}+b u_{2}, v\right)=a f\left(u_{1}, v\right)+b f\left(u_{2}, v\right)$,

(ii) $f\left(u, a v_{1}+b v_{2}\right)=a f\left(u, v_{1}\right)+b f\left(u, v_{2}\right)$

We express condition (i) by saying $f$ is linear in the first variable, and condition (ii) by saying $f$ is linear in the second variable.

\section*{EXAMPLE 12.1}
(a) Let $f$ be the dot product on $\mathbf{R}^{n}$; that is, for $u=\left(a_{i}\right)$ and $v=\left(b_{i}\right)$,

$$
f(u, v)=u \cdot v=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}
$$

Then $f$ is a bilinear form on $\mathbf{R}^{n}$. (In fact, any inner product on a real vector space $V$ is a bilinear form on $V$.)

(b) Let $\phi$ and $\sigma$ be arbitrarily linear functionals on $V$. Let $f: V \times V \rightarrow K$ be defined by $f(u, v)=\phi(u) \sigma(v)$. Then $f$ is a bilinear form, because $\phi$ and $\sigma$ are each linear.

(c) Let $A=\left[a_{i j}\right]$ be any $n \times n$ matrix over a field $K$. Then $A$ may be identified with the following bilinear form $F$ on $K^{n}$, where $X=\left[x_{i}\right]$ and $Y=\left[y_{i}\right]$ are column vectors of variables:

$$
f(X, Y)=X^{T} A Y=\sum_{i, j} a_{i j} x_{i} y_{i}=a_{11} x_{1} y_{1}+a_{12} x_{1} y_{2}+\cdots+a_{n n} x_{n} y_{n}
$$

The above formal expression in the variables $x_{i}, y_{i}$ is termed the bilinear polynomial corresponding to the matrix $A$. Equation (12.1) shows that, in a certain sense, every bilinear form is of this type.

\section*{Space of Bilinear Forms}
Let $B(V)$ denote the set of all bilinear forms on $V$. A vector space structure is placed on $B(V)$, where for any $f, g \in B(V)$ and any $k \in K$, we define $f+g$ and $k f$ as follows:

$$
(f+g)(u, v)=f(u, v)+g(u, v) \quad \text { and } \quad(k f)(u, v)=k f(u, v)
$$

The following theorem (proved in Problem 12.4) applies.

THEOREM 12.1: $\quad$ Let $V$ be a vector space of dimension $n$ over $K$. Let $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ be any basis of the dual space $V^{*}$. Then $\left\{f_{i j}: i, j=1, \ldots, n\right\}$ is a basis of $B(V)$, where $f_{i j}$ is defined by $f_{i j}(u, v)=\phi_{i}(u) \phi_{j}(v)$. Thus, in particular, $\operatorname{dim} B(V)=n^{2}$.

\subsection*{12.3 Bilinear Forms and Matrices}
Let $f$ be a bilinear form on $V$ and let $S=\left\{u_{1}, \ldots, u_{n}\right\}$ be a basis of $V$. Suppose $u, v \in V$ and

$$
u=a_{1} u_{1}+\cdots+a_{n} u_{n} \quad \text { and } \quad v=b_{1} u_{1}+\cdots+b_{n} u_{n}
$$

Then

$$
f(u, v)=f\left(a_{1} u_{1}+\cdots+a_{n} u_{n}, \quad b_{1} u_{1}+\cdots+b_{n} u_{n}\right)=\sum_{i, j} a_{i} b_{j} f\left(u_{i}, u_{j}\right)
$$

Thus, $f$ is completely determined by the $n^{2}$ values $f\left(u_{i}, u_{j}\right)$.

The matrix $A=\left[a_{i j}\right]$ where $a_{i j}=f\left(u_{i}, u_{j}\right)$ is called the matrix representation of $f$ relative to the basis $S$ or, simply, the "matrix of $f$ in $S$." It "represents" $f$ in the sense that, for all $u, v \in V$,


\begin{equation*}
f(u, v)=\sum_{i, j} a_{i} b_{j} f\left(u_{i}, u_{j}\right)=[u]_{S}^{T} A[v]_{S} \tag{12.1}
\end{equation*}


[As usual, $[u]_{S}$ denotes the coordinate (column) vector of $u$ in the basis $S$.]

\section*{Change of Basis, Congruent Matrices}
We now ask, how does a matrix representing a bilinear form transform when a new basis is selected? The answer is given in the following theorem (proved in Problem 12.5).

THEOREM 12.2: $\quad$ Let $P$ be a change-of-basis matrix from one basis $S$ to another basis $S^{\prime}$. If $A$ is the matrix representing a bilinear form $f$ in the original basis $S$, then $B=P^{T} A P$ is the matrix representing $f$ in the new basis $S^{\prime}$.

The above theorem motivates the following definition.

DEFINITION: A matrix $B$ is congruent to a matrix $A$, written $B \simeq A$, if there exists a nonsingular matrix $P$ such that $B=P^{T} A P$.

Thus, by Theorem 12.2, matrices representing the same bilinear form are congruent. We remark that congruent matrices have the same rank, because $P$ and $P^{T}$ are nonsingular; hence, the following definition is well defined.

DEFINITION: The rank of a bilinear form $f$ on $V$, written $\operatorname{rank}(f)$, is the $\operatorname{rank}$ of any matrix representation of $f$. We say $f$ is degenerate or nondegenerate according to whether $\operatorname{rank}(f)<\operatorname{dim} V$ or $\operatorname{rank}(f)=\operatorname{dim} V$.

\subsection*{12.4 Alternating Bilinear Forms}
Let $f$ be a bilinear form on $V$. Then $f$ is called

(i) alternating if $f(v, v)=0$ for every $v \in V$;

(ii) skew-symmetric if $f(u, v)=-f(v, u)$ for every $u, v \in V$.

Now suppose (i) is true. Then (ii) is true, because, for any $u, v, \in V$,

$$
0=f(u+v, u+v)=f(u, u)+f(u, v)+f(v, u)+f(v, v)=f(u, v)+f(v, u)
$$

On the other hand, suppose (ii) is true and also $1+1 \neq 0$. Then (i) is true, because, for every $v \in V$, we have $f(v, v)=-f(v, v)$. In other words, alternating and skew-symmetric are equivalent when $1+1 \neq 0$.

The main structure theorem of alternating bilinear forms (proved in Problem 12.23) is as follows.

THEOREM 12.3: $\quad$ Let $f$ be an alternating bilinear form on $V$. Then there exists a basis of $V$ in which $f$ is represented by a block diagonal matrix $M$ of the form

$$
M=\operatorname{diag}\left(\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right],\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right], \ldots,\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right],[0],[0], \ldots c c \mid c\right.
$$

Moreover, the number of nonzero blocks is uniquely determined by $f$ [because it is equal to $\left.\frac{1}{2} \operatorname{rank}(f)\right]$.

In particular, the above theorem shows that any alternating bilinear form must have even rank.

\subsection*{12.5 Symmetric Bilinear Forms, Quadratic Forms}
This section investigates the important notions of symmetric bilinear forms and quadratic forms and their representation by means of symmetric matrices. The only restriction on the field $K$ is that $1+1 \neq 0$. In Section 12.6, we will restrict $K$ to be the real field $\mathbf{R}$, which yields important special results.

\section*{Symmetric Bilinear Forms}
Let $f$ be a bilinear form on $V$. Then $f$ is said to be symmetric if, for every $u, v \in V$,

$$
f(u, v)=f(v, u)
$$

One can easily show that $f$ is symmetric if and only if any matrix representation $A$ of $f$ is a symmetric matrix.

The main result for symmetric bilinear forms (proved in Problem 12.10) is as follows. (We emphasize that we are assuming that $1+1 \neq 0$.)

THEOREM 12.4: Let $f$ be a symmetric bilinear form on $V$. Then $V$ has a basis $\left\{v_{1}, \ldots, v_{n}\right\}$ in which $f$ is represented by a diagonal matrix - that is, where $f\left(v_{i}, v_{j}\right)=0$ for $i \neq j$.

THEOREM 12.4: (Alternative Form) Let $A$ be a symmetric matrix over $K$. Then $A$ is congruent to a diagonal matrix; that is, there exists a nonsingular matrix $P$ such that $P^{T} A P$ is diagonal.

\section*{Diagonalization Algorithm}
Recall that a nonsingular matrix $P$ is a product of elementary matrices. Accordingly, one way of obtaining the diagonal form $D=P^{T} A P$ is by a sequence of elementary row operations and the same sequence of elementary column operations. This same sequence of elementary row operations on the identity matrix $I$ will yield $P^{T}$. This algorithm is formalized below.

ALGORITHM 12.1: (Congruence Diagonalization of a Symmetric Matrix) The input is a symmetric matrix $A=\left[a_{i j}\right]$ of order $n$.

Step 1. Form the $n \times 2 n$ (block) matrix $M=\left[A_{1}, I\right]$, where $A_{1}=A$ is the left half of $M$ and the identity matrix $I$ is the right half of $M$.

Step 2. Examine the entry $a_{11}$. There are three cases.

Case I: $\quad a_{11} \neq 0$. (Use $a_{11}$ as a pivot to put 0 's below $a_{11}$ in $M$ and to the right of $a_{11}$ in $A_{1}$.) For $i=2, \ldots, n$ :

(a) Apply the row operation "Replace $R_{i}$ by $-a_{i 1} R_{1}+a_{11} R_{i}$."

(b) Apply the corresponding column operation "Replace $C_{i}$ by $-a_{i 1} C_{1}+a_{11} C_{i}$."

These operations reduce the matrix $M$ to the form

\[
M \sim\left[\begin{array}{cccc}
a_{11} & 0 & * & *  \tag{*}\\
0 & A_{1} & * & *
\end{array}\right]
\]

Case II: $\quad a_{11}=0$ but $a_{k k} \neq 0$, for some $k>1$.

(a) Apply the row operation "Interchange $R_{1}$ and $R_{k}$."

(b) Apply the corresponding column operation "Interchange $C_{1}$ and $C_{k}$."

(These operations bring $a_{k k}$ into the first diagonal position, which reduces the matrix to Case I.)

Case III: All diagonal entries $a_{i i}=0$ but some $a_{i j} \neq 0$.

(a) Apply the row operation "Replace $R_{i}$ by $R_{j}+R_{i}$."

(b) Apply the corresponding column operation "Replace $C_{i}$ by $C_{j}+C_{i}$."

(These operations bring $2 a_{i j}$ into the $i$ th diagonal position, which reduces the matrix to Case II.)

Thus, $M$ is finally reduced to the form (*), where $A_{2}$ is a symmetric matrix of order less than $A$.

Step 3. Repeat Step 2 with each new matrix $A_{k}$ (by neglecting the first row and column of the preceding matrix) until $A$ is diagonalized. Then $M$ is transformed into the form $M^{\prime}=[D, Q]$, where $D$ is diagonal.

Step 4. Set $P=Q^{T}$. Then $D=P^{T} A P$.

Remark 1: We emphasize that in Step 2, the row operations will change both sides of $M$, but the column operations will only change the left half of $M$.

Remark 2: The condition $1+1 \neq 0$ is used in Case III, where we assume that $2 a_{i j} \neq 0$ when $a_{i j} \neq 0$.

The justification for the above algorithm appears in Problem 12.9.

EXAMPLE 12.2 Let $A=\left[\begin{array}{rrr}1 & 2 & -3 \\ 2 & 5 & -4 \\ -3 & -4 & 8\end{array}\right]$. Apply Algorithm 9.1 to find a nonsingular matrix $P$ such that $D=P^{T} A P$ is diagonal.

First form the block matrix $M=[A, I]$; that is, let

$$
M=[A, I]=\left[\begin{array}{rrr:rrr}
1 & 2 & -3 & 1 & 0 & 0 \\
2 & 5 & -4 & 0 & 1 & 0 \\
-3 & -4 & 8 & 0 & 0 & 1
\end{array}\right]
$$

Apply the row operations "Replace $R_{2}$ by $-2 R_{1}+R_{2}$ " and "Replace $R_{3}$ by $3 R_{1}+R_{3}$ " to $M$, and then apply the corresponding column operations "Replace $C_{2}$ by $-2 C_{1}+C_{2}$ " and "Replace $C_{3}$ by $3 C_{1}+C_{3}$ " to obtain

$$
\left[\begin{array}{rrr:rrr}
1 & 2 & -3 & 1 & 0 & 0 \\
0 & 1 & 2 & -2 & 1 & 0 \\
0 & 2 & -1 & 3 & 0 & 1
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 2 & -2 & 1 & 0 \\
0 & 2 & -1 & 3 & 0 & 1
\end{array}\right]
$$

Next apply the row operation "Replace $R_{3}$ by $-2 R_{2}+R_{3}$ " and then the corresponding column operation "Replace $C_{3}$ by $-2 C_{2}+C_{3}$, to obtain

$$
\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 2 & -2 & 1 & 0 \\
0 & 0 & -5 & 7 & -2 & 1
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & -2 & 1 & 0 \\
0 & 0 & -5 & 7 & -2 & 1
\end{array}\right]
$$

Now $A$ has been diagonalized. Set

$$
P=\left[\begin{array}{rrr}
1 & -2 & 7 \\
0 & 1 & -2 \\
0 & 0 & 1
\end{array}\right] \quad \text { and then } \quad D=P^{-1} A P=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -5
\end{array}\right]
$$

We emphasize that $P$ is the transpose of the right half of the final matrix.

\section*{Quadratic Forms}
We begin with a definition.

DEFINITION A: A mapping $q: V \rightarrow K$ is a quadratic form if $q(v)=f(v, v)$ for some symmetric bilinear form $f$ on $V$.

If $1+1 \neq 0$ in $K$, then the bilinear form $f$ can be obtained from the quadratic form $q$ by the following polar form of $f$ :

$$
f(u, v)=\frac{1}{2}[q(u+v)-q(u)-q(v)]
$$

Now suppose $f$ is represented by a symmetric matrix $A=\left[a_{i j}\right]$, and $1+1 \neq 0$. Letting $X=\left[x_{i}\right]$ denote a column vector of variables, $q$ can be represented in the form

$$
q(X)=f(X, X)=X^{T} A X=\sum_{i, j} a_{i j} x_{i} x_{j}=\sum_{i} a_{i i} x_{i}^{2}+2 \sum_{i<j} a_{i j} x_{i} x_{j}
$$

The above formal expression in the variables $x_{i}$ is also called a quadratic form. Namely, we have the following second definition.

DEFINITION B: A quadratic form $q$ in variables $x_{1}, x_{2}, \ldots, x_{n}$ is a polynomial such that every term has degree two; that is,

$$
q\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\sum_{i} c_{i} x_{i}^{2}+\sum_{i<j} d_{i j} x_{i} x_{j}
$$

Using $1+1 \neq 0$, the quadratic form $q$ in Definition $\mathrm{B}$ determines a symmetric matrix $A=\left[a_{i j}\right]$ where $a_{i i}=c_{i}$ and $a_{i j}=a_{j i}=\frac{1}{2} d_{i j}$. Thus, Definitions A and B are essentially the same.

If the matrix representation $A$ of $q$ is diagonal, then $q$ has the diagonal representation

$$
q(X)=X^{T} A X=a_{11} x_{1}^{2}+a_{22} x_{2}^{2}+\cdots+a_{n n} x_{n}^{2}
$$

That is, the quadratic polynomial representing $q$ will contain no "cross product" terms. Moreover, by Theorem 12.4, every quadratic form has such a representation (when $1+1 \neq 0$ ).

\subsection*{12.6 Real Symmetric Bilinear Forms, Law of Inertia}
This section treats symmetric bilinear forms and quadratic forms on vector spaces $V$ over the real field $\mathbf{R}$. The special nature of $\mathbf{R}$ permits an independent theory. The main result (proved in Problem 12.14) is as follows.

THEOREM 12.5: $\quad$ Let $f$ be a symmetric form on $V$ over $\mathbf{R}$. Then there exists a basis of $V$ in which $f$ is represented by a diagonal matrix. Every other diagonal matrix representation of $f$ has the same number $\mathbf{p}$ of positive entries and the same number $\mathbf{n}$ of negative entries.

The above result is sometimes called the Law of Inertia or Sylvester's Theorem. The rank and signature of the symmetric bilinear form $f$ are denoted and defined by

$$
\operatorname{rank}(f)=\mathbf{p}+\mathbf{n} \quad \text { and } \quad \operatorname{sig}(f)=\mathbf{p}-\mathbf{n}
$$

These are uniquely defined by Theorem 12.5 .

A real symmetric bilinear form $f$ is said to be

(i) positive definite if $q(v)=f(v, v)>0$ for every $v \neq 0$,

(ii) nonnegative semidefinite if $q(v)=f(v, v) \geq 0$ for every $v$.

EXAMPLE 12.3 Let $f$ be the dot product on $\mathbf{R}^{n}$. Recall that $f$ is a symmetric bilinear form on $\mathbf{R}^{n}$. We note that $f$ is also positive definite. That is, for any $u=\left(a_{i}\right) \neq 0$ in $\mathbf{R}^{n}$,

$$
f(u, u)=a_{1}^{2}+a_{2}^{2}+\cdots+a_{n}^{2}>0
$$

Section 12.5 and Chapter 13 tell us how to diagonalize a real quadratic form $q$ or, equivalently, a real symmetric matrix $A$ by means of an orthogonal transition matrix $P$. If $P$ is merely nonsingular, then $q$ can be represented in diagonal form with only 1 's and -1 's as nonzero coefficients. Namely, we have the following corollary.

COROLLARY 12.6: Any real quadratic form $q$ has a unique representation in the form

$$
q\left(x_{1}, x_{2}, \ldots, x_{n}\right)=x_{1}^{2}+\cdots+x_{\mathbf{p}}^{2}-x_{\mathbf{p}+1}^{2}-\cdots-x_{r}^{2}
$$

where $r=\boldsymbol{p}+\boldsymbol{n}$ is the rank of the form.

COROLLARY 12.6: (Alternative Form) Any real symmetric matrix $A$ is congruent to the unique diagonal matrix

$$
D=\operatorname{diag}\left(I_{\mathbf{p}},-I_{\mathrm{n}}, 0\right)
$$

where $r=\boldsymbol{p}+\boldsymbol{n}$ is the rank of $A$.

\subsection*{12.7 Hermitian Forms}
Let $V$ be a vector space of finite dimension over the complex field $\mathbf{C}$. A Hermitian form on $V$ is a mapping $f: V \times V \rightarrow \mathbf{C}$ such that, for all $a, b \in C$ and all $u_{i}, v \in V$,

(i) $f\left(a u_{1}+b u_{2}, v\right)=a f\left(u_{1}, v\right)+b f\left(u_{2}, v\right)$,

(ii) $f(u, v)=\overline{f(v, u)}$.

(As usual, $\bar{k}$ denotes the complex conjugate of $k \in \mathbf{C}$.)

Using (i) and (ii), we get

$$
\begin{aligned}
f\left(u, a v_{1}+b v_{2}\right) & =\overline{f\left(a v_{1}+b v_{2}, u\right)}=\overline{a f\left(v_{1}, u\right)+b f\left(v_{2}, u\right)} \\
& =\overline{\hat{a} f\left(v_{1}, u\right)}+\overline{b f\left(v_{2}, u\right)}=\bar{a} f\left(u, v_{1}\right)+\bar{b} f\left(u, v_{2}\right)
\end{aligned}
$$

That is,

(iii) $f\left(u, a v_{1}+b v_{2}\right)=\bar{a} f\left(u, v_{1}\right)+\bar{b} f\left(u, v_{2}\right)$.

As before, we express condition (i) by saying $f$ is linear in the first variable. On the other hand, we express condition (iii) by saying $f$ is "conjugate linear" in the second variable. Moreover, condition (ii) tells us that $f(v, v)=\overline{f(v, v)}$, and hence, $f(v, v)$ is real for every $v \in V$.

The results of Sections 12.5 and 12.6 for symmetric forms have their analogues for Hermitian forms. Thus, the mapping $q: V \rightarrow \mathbf{R}$, defined by $q(v)=f(v, v)$, is called the Hermitian quadratic form or complex quadratic form associated with the Hermitian form $f$. We can obtain $f$ from $q$ by the polar form

$$
f(u, v)=\frac{1}{4}[q(u+v)-q(u-v)]+\frac{1}{4}[q(u+i v)-q(u-i v)]
$$

Now suppose $S=\left\{u_{1}, \ldots, u_{n}\right\}$ is a basis of $V$. The matrix $H=\left[h_{i j}\right]$ where $h_{i j}=f\left(u_{i}, u_{j}\right)$ is called the matrix representation of $f$ in the basis $S$. By (ii), $f\left(u_{i}, u_{j}\right)=\overline{f\left(u_{j}, u_{i}\right)}$; hence, $H$ is Hermitian and, in particular, the diagonal entries of $H$ are real. Thus, any diagonal representation of $f$ contains only real entries.

The next theorem (to be proved in Problem 12.47) is the complex analog of Theorem 12.5 on real symmetric bilinear forms.

THEOREM 12.7: $\quad$ Let $f$ be a Hermitian form on $V$ over $\mathbf{C}$. Then there exists a basis of $V$ in which $f$ is represented by a diagonal matrix. Every other diagonal matrix representation of $f$ has the same number $\mathbf{p}$ of positive entries and the same number $\mathbf{n}$ of negative entries.

Again the rank and signature of the Hermitian form $f$ are denoted and defined by

$$
\operatorname{rank}(f)=\mathbf{p}+\mathbf{n} \quad \text { and } \quad \operatorname{sig}(f)=\mathbf{p}-\mathbf{n}
$$

These are uniquely defined by Theorem 12.7 .

Analogously, a Hermitian form $f$ is said to be

(i) positive definite if $q(v)=f(v, v)>0$ for every $v \neq 0$,

(ii) nonnegative semidefinite if $q(v)=f(v, v) \geq 0$ for every $v$.

EXAMPLE 12.4 Let $f$ be the dot product on $\mathbf{C}^{n}$; that is, for any $u=\left(z_{i}\right)$ and $v=\left(w_{i}\right)$ in $\mathbf{C}^{n}$,

$$
f(u, v)=u \cdot v=z_{1} \bar{w}_{1}+z_{2} \bar{w}_{2}+\cdots+z_{n} \bar{w}_{n}
$$

Then $f$ is a Hermitian form on $\mathbf{C}^{n}$. Moreover, $f$ is also positive definite, because, for any $u=\left(z_{i}\right) \neq 0$ in $\mathbf{C}^{n}$,

$$
f(u, u)=z_{1} \bar{z}_{1}+z_{2} \bar{z}_{2}+\cdots+z_{n} \bar{z}_{n}=\left|z_{1}\right|^{2}+\left|z_{2}\right|^{2}+\cdots+\left|z_{n}\right|^{2}>0
$$

\section*{SOLVED PROBLEMS}
\section*{Bilinear Forms}
12.1. Let $u=\left(x_{1}, x_{2}, x_{3}\right)$ and $v=\left(y_{1}, y_{2}, y_{3}\right)$. Express $f$ in matrix notation, where

$$
f(u, v)=3 x_{1} y_{1}-2 x_{1} y_{3}+5 x_{2} y_{1}+7 x_{2} y_{2}-8 x_{2} y_{3}+4 x_{3} y_{2}-6 x_{3} y_{3}
$$

Let $A=\left[a_{i j}\right]$, where $a_{i j}$ is the coefficient of $x_{i} y_{j}$. Then

$$
f(u, v)=X^{T} A Y=\left[x_{1}, x_{2}, x_{3}\right]\left[\begin{array}{lll}
3 & 0 & -2 \\
5 & 7 & -8 \\
0 & 4 & -6
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2} \\
y_{3}
\end{array}\right]
$$

12.2. Let $A$ be an $n \times n$ matrix over $K$. Show that the mapping $f$ defined by $f(X, Y)=X^{T} A Y$ is a bilinear form on $K^{n}$.

For any $a, b \in K$ and any $X_{i}, Y_{i} \in K^{n}$,

$$
\begin{aligned}
f\left(a X_{1}+b X_{2}, Y\right) & =\left(a X_{1}+b X_{2}\right)^{T} A Y=\left(a X_{1}^{T}+b X_{2}^{T}\right) A Y \\
& =a X_{1}^{T} A Y+b X_{2}^{T} A Y=a f\left(X_{1}, Y\right)+b f\left(X_{2}, Y\right)
\end{aligned}
$$

Hence, $f$ is linear in the first variable. Also,

$$
f\left(X, a Y_{1}+b Y_{2}\right)=X^{T} A\left(a Y_{1}+b Y_{2}\right)=a X^{T} A Y_{1}+b X^{T} A Y_{2}=a f\left(X, Y_{1}\right)+b f\left(X, Y_{2}\right)
$$

Hence, $f$ is linear in the second variable, and so $f$ is a bilinear form on $K^{n}$.

12.3. Let $f$ be the bilinear form on $\mathbf{R}^{2}$ defined by

$$
f\left[\left(x_{1}, x_{2}\right), \quad\left(y_{1}, y_{2}\right)\right]=2 x_{1} y_{1}-3 x_{1} y_{2}+4 x_{2} y_{2}
$$

(a) Find the matrix $A$ of $f$ in the basis $\left\{u_{1}=(1,0), u_{2}=(1,1)\right\}$.

(b) Find the matrix $B$ of $f$ in the basis $\left\{v_{1}=(2,1), v_{2}=(1,-1)\right\}$.

(c) Find the change-of-basis matrix $P$ from the basis $\left\{u_{i}\right\}$ to the basis $\left\{v_{i}\right\}$, and verify that $B=P^{T} A P$.

(a) Set $A=\left[a_{i j}\right]$, where $a_{i j}=f\left(u_{i}, u_{j}\right)$. This yields

$$
\begin{aligned}
& a_{11}=f[(1,0), \quad(1,0)]=2-0-0=2, \quad a_{21}=f[(1,1), \quad(1,0)]=2-0+0=2 \\
& a_{12}=f[(1,0), \quad(1,1)]=2-3-0=-1, \quad a_{22}=f[(1,1), \quad(1,1)]=2-3+4=3
\end{aligned}
$$

Thus, $A=\left[\begin{array}{rr}2 & -1 \\ 2 & 3\end{array}\right]$ is the matrix of $f$ in the basis $\left\{u_{1}, u_{2}\right\}$.

(b) Set $B=\left[b_{i j}\right]$, where $b_{i j}=f\left(v_{i}, v_{j}\right)$. This yields

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-373}
\end{center}

Thus, $B=\left[\begin{array}{rr}6 & 6 \\ -3 & 9\end{array}\right]$ is the matrix of $f$ in the basis $\left\{v_{1}, v_{2}\right\}$.

(c) Writing $v_{1}$ and $v_{2}$ in terms of the $u_{i}$ yields $v_{1}=u_{1}+u_{2}$ and $v_{2}=2 u_{1}-u_{2}$. Then

$$
P=\left[\begin{array}{rr}
1 & 2 \\
1 & -1
\end{array}\right], \quad P^{T}=\left[\begin{array}{rr}
1 & 1 \\
2 & -1
\end{array}\right]
$$

and

$$
P^{T} A P=\left[\begin{array}{rr}
1 & 1 \\
2 & -1
\end{array}\right]\left[\begin{array}{rr}
2 & -1 \\
2 & 3
\end{array}\right]\left[\begin{array}{rr}
1 & 2 \\
1 & -1
\end{array}\right]=\left[\begin{array}{rr}
6 & 6 \\
-3 & 9
\end{array}\right]=B
$$

12.4. Prove Theorem 12.1: Let $V$ be an $n$-dimensional vector space over $K$. Let $\left\{\phi_{1}, \ldots, \phi_{n}\right\}$ be any basis of the dual space $V^{*}$. Then $\left\{f_{i j}: i, j=1, \ldots, n\right\}$ is a basis of $B(V)$, where $f_{i j}$ is defined by $f_{i j}(u, v)=\phi_{i}(u) \phi_{j}(v)$. Thus, $\operatorname{dim} B(V)=n^{2}$.

Let $\left\{u_{1}, \ldots, u_{n}\right\}$ be the basis of $V$ dual to $\left\{\phi_{i}\right\}$. We first show that $\left\{f_{i j}\right\}$ spans $B(V)$. Let $f \in B(V)$ and suppose $f\left(u_{i}, u_{j}\right)=a_{i j}$. We claim that $f=\sum_{i, j} a_{i j} f_{i j}$. It suffices to show that

$$
f\left(u_{s}, u_{t}\right)=\left(\sum a_{i j} f_{i j}\right)\left(u_{s}, u_{t}\right) \quad \text { for } \quad s, t=1, \ldots, n
$$

We have

$$
\left(\sum a_{i j} f_{i j}\right)\left(u_{s}, u_{t}\right)=\sum a_{i j} f_{i j}\left(u_{s}, u_{t}\right)=\sum a_{i j} \phi_{i}\left(u_{s}\right) \phi_{j}\left(u_{t}\right)=\sum a_{i j} \delta_{i s} \delta_{j t}=a_{s t}=f\left(u_{s}, u_{t}\right)
$$

as required. Hence, $\left\{f_{i j}\right\}$ spans $B(V)$. Next, suppose $\sum a_{i j} f_{i j}=\mathbf{0}$. Then for $s, t=1, \ldots, n$,

$$
0=\mathbf{0}\left(u_{s}, u_{t}\right)=\left(\sum a_{i j} f_{i j}\right)\left(u_{s}, u_{t}\right)=a_{r s}
$$

The last step follows as above. Thus, $\left\{f_{i j}\right\}$ is independent, and hence is a basis of $B(V)$.

12.5. Prove Theorem 12.2. Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\prime}$. Let $A$ be the matrix representing a bilinear form in the basis $S$. Then $B=P^{T} A P$ is the matrix representing $f$ in the basis $S^{\prime}$.

Let $u, v \in V$. Because $P$ is the change-of-basis matrix from $S$ to $S^{\prime}$, we have $P[u]_{S^{\prime}}=[u]_{S}$ and also $P[v]_{S^{\prime}}=[v]_{S}$; hence, $[u]_{S}^{T}=[u]_{S^{\prime}}^{T} P^{T}$. Thus,

$$
f(u, v)=[u]_{S}^{T} A[v]_{S}=[u]_{S^{\prime}}^{T} P^{T} A P[v]_{S^{\prime}}
$$

Because $u$ and $v$ are arbitrary elements of $V, P^{T} A P$ is the matrix of $f$ in the basis $S^{\prime}$.

\section*{Symmetric Bilinear Forms, Quadratic Forms}
12.6. Find the symmetric matrix that corresponds to each of the following quadratic forms:

(a) $q(x, y, z)=3 x^{2}+4 x y-y^{2}+8 x z-6 y z+z^{2}$,

(b) $q^{\prime}(x, y, z)=3 x^{2}+x z-2 y z$, (c) $q^{\prime \prime}(x, y, z)=2 x^{2}-5 y^{2}-7 z^{2}$

The symmetric matrix $A=\left[a_{i j}\right]$ that represents $q\left(x_{1}, \ldots, x_{n}\right)$ has the diagonal entry $a_{i i}$ equal to the coefficient of the square term $x_{i}^{2}$ and the nondiagonal entries $a_{i j}$ and $a_{j i}$ each equal to half of the coefficient of the cross-product term $x_{i} x_{j}$. Thus,

(a) $A=\left[\begin{array}{rrr}3 & 2 & 4 \\ 2 & -1 & -3 \\ 4 & -3 & 1\end{array}\right]$, (b) $A^{\prime}=\left[\begin{array}{rrr}3 & 0 & \frac{1}{2} \\ 0 & 0 & -1 \\ \frac{1}{2} & -1 & 0\end{array}\right]$, (c) $A^{\prime \prime}=\left[\begin{array}{rrr}2 & 0 & 0 \\ 0 & -5 & 0 \\ 0 & 0 & -7\end{array}\right]$

The third matrix $A^{\prime \prime}$ is diagonal, because the quadratic form $q^{\prime \prime}$ is diagonal; that is, $q^{\prime \prime}$ has no cross-product terms.

12.7. Find the quadratic form $q(X)$ that corresponds to each of the following symmetric matrices:

(a)

$A=\left[\begin{array}{rr}5 & -3 \\ -3 & 8\end{array}\right]$

(b) $B=\left[\begin{array}{rrr}4 & -5 & 7 \\ -5 & -6 & 8 \\ 7 & 8 & -9\end{array}\right]$,

$C=\left[\begin{array}{rrrr}2 & 4 & -1 & 5 \\ 4 & -7 & -6 & 8 \\ -1 & -6 & 3 & 9 \\ 5 & 8 & 9 & 1\end{array}\right]$

The quadratic form $q(X)$ that corresponds to a symmetric matrix $M$ is defined by $q(X)=X^{T} M X$, where $X=\left[x_{i}\right]$ is the column vector of unknowns.

(a) Compute as follows:

$$
\begin{aligned}
q(x, y) & =X^{T} A X=[x, y]\left[\begin{array}{rr}
5 & -3 \\
-3 & 8
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{ll}
5 x-3 y, & -3 x+8 y
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right] \\
& =5 x^{2}-3 x y-3 x y+8 y^{2}=5 x^{2}-6 x y+8 y^{2}
\end{aligned}
$$

As expected, the coefficient 5 of the square term $x^{2}$ and the coefficient 8 of the square term $y^{2}$ are the diagonal elements of A, and the coefficient -6 of the cross-product term $x y$ is the sum of the nondiagonal elements -3 and -3 of $A$ (or twice the nondiagonal element -3 , because $A$ is symmetric).

(b) Because $B$ is a three-square matrix, there are three unknowns, say $x, y, z$ or $x_{1}, x_{2}, x_{3}$. Then

or

$$
\begin{gathered}
q(x, y, z)=4 x^{2}-10 x y-6 y^{2}+14 x z+16 y z-9 z^{2} \\
q\left(x_{1}, x_{2}, x_{3}\right)=4 x_{1}^{2}-10 x_{1} x_{2}-6 x_{2}^{2}+14 x_{1} x_{3}+16 x_{2} x_{3}-9 x_{3}^{2}
\end{gathered}
$$

Here we use the fact that the coefficients of the square terms $x_{1}^{2}, x_{2}^{2}, x_{3}^{2}\left(\right.$ or $\left.x^{2}, y^{2}, z^{2}\right)$ are the respective diagonal elements $4,-6,-9$ of $B$, and the coefficient of the cross-product term $x_{i} x_{j}$ is the sum of the nondiagonal elements $b_{i j}$ and $b_{j i}$ (or twice $b_{i j}$, because $b_{i j}=b_{j i}$ ).

(c) Because $C$ is a four-square matrix, there are four unknowns. Hence,

$$
\begin{aligned}
q\left(x_{1}, x_{2}, x_{3}, x_{4}\right)= & 2 x_{1}^{2}-7 x_{2}^{2}+3 x_{3}^{2}+x_{4}^{2}+8 x_{1} x_{2}-2 x_{1} x_{3} \\
& +10 x_{1} x_{4}-12 x_{2} x_{3}+16 x_{2} x_{4}+18 x_{3} x_{4}
\end{aligned}
$$

12.8. Let $A=\left[\begin{array}{rrr}1 & -3 & 2 \\ -3 & 7 & -5 \\ 2 & -5 & 8\end{array}\right]$. Apply Algorithm 12.1 to find a nonsingular matrix $P$ such that $D=P^{T} A P$ is diagonal, and find $\operatorname{sig}(A)$, the signature of $A$.

First form the block matrix $M=[A, I]$ :

$$
M=[A, I]=\left[\begin{array}{rrr:rrr}
1 & -3 & 2 & 1 & 0 & 0 \\
-3 & 7 & -5 & 0 & 1 & 0 \\
2 & -5 & 8 & 0 & 0 & 1
\end{array}\right]
$$

Using $a_{11}=1$ as a pivot, apply the row operations "Replace $R_{2}$ by $3 R_{1}+R_{2}$ " and "Replace $R_{3}$ by $-2 R_{1}+R_{3}$ " to $M$ and then apply the corresponding column operations "Replace $C_{2}$ by $3 C_{1}+C_{2}$ " and "Replace $C_{3}$ by $-2 C_{1}+C_{3}$ " to $A$ to obtain

$$
\left[\begin{array}{rrr:rrr}
1 & -3 & 2 & 1 & 0 & 0 \\
0 & -2 & 1 & 3 & 1 & 0 \\
0 & 1 & 4 & -2 & 0 & 1
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{rrr:rrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & -2 & 1 & 3 & 1 & 0 \\
0 & 1 & 4 & -2 & 0 & 1
\end{array}\right]
$$

Next apply the row operation "Replace $R_{3}$ by $R_{2}+2 R_{3}$ " and then the corresponding column operation "Replace $C_{3}$ by $C_{2}+2 C_{3}$ " to obtain

$$
\left[\begin{array}{rrrrrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & -2 & 1 & 3 & 1 & 0 \\
0 & 0 & 9 & -1 & 1 & 2
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{rrrrrr}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & -2 & 0 & 3 & 1 & 0 \\
0 & 0 & 18 & -1 & 1 & 2
\end{array}\right]
$$

Now $A$ has been diagonalized and the transpose of $P$ is in the right half of $M$. Thus, set

$$
P=\left[\begin{array}{rrr}
1 & 3 & -1 \\
0 & 1 & 1 \\
0 & 0 & 2
\end{array}\right] \quad \text { and then } \quad D=P^{T} A P=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & 18
\end{array}\right]
$$

Note $D$ has $\mathbf{p}=2$ positive and $\mathbf{n}=1$ negative diagonal elements. Thus, the signature of $A$ is $\operatorname{sig}(A)=\mathbf{p}-\mathbf{n}=2-1=1$.

12.9. Justify Algorithm 12.1, which diagonalizes (under congruence) a symmetric matrix $A$.

Consider the block matrix $M=[A, I]$. The algorithm applies a sequence of elementary row operations and the corresponding column operations to the left side of $M$, which is the matrix $A$. This is equivalent to premultiplying $A$ by a sequence of elementary matrices, say, $E_{1}, E_{2}, \ldots, E_{r}$, and postmultiplying $A$ by the transposes of the $E_{i}$. Thus, when the algorithm ends, the diagonal matrix $D$ on the left side of $M$ is equal to

$$
D=E_{r} \cdots E_{2} E_{1} A E_{1}^{T} E_{2}^{T} \cdots E_{r}^{T}=Q A Q^{T}, \quad \text { where } \quad Q=E_{r} \cdots E_{2} E_{1}
$$

On the other hand, the algorithm only applies the elementary row operations to the identity matrix $I$ on the right side of $M$. Thus, when the algorithm ends, the matrix on the right side of $M$ is equal to

$$
E_{r} \cdots E_{2} E_{1} I=E_{r} \cdots E_{2} E_{1}=Q
$$

Setting $P=Q^{T}$, we get $D=P^{T} A P$, which is a diagonalization of $A$ under congruence.

12.10. Prove Theorem 12.4: Let $f$ be a symmetric bilinear form on $V$ over $K$ (where $1+1 \neq 0$ ). Then $V$ has a basis in which $f$ is represented by a diagonal matrix.

Algorithm 12.1 shows that every symmetric matrix over $K$ is congruent to a diagonal matrix. This is equivalent to the statement that $f$ has a diagonal representation.

12.11. Let $q$ be the quadratic form associated with the symmetric bilinear form $f$. Verify the polar identity $f(u, v)=\frac{1}{2}[q(u+v)-q(u)-q(v)]$. (Assume that $1+1 \neq 0$.)

We have

$$
\begin{aligned}
q(u+v)-q(u)-q(v) & =f(u+v, u+v)-f(u, u)-f(v, v) \\
& =f(u, u)+f(u, v)+f(v, u)+f(v, v)-f(u, u)-f(v, v)=2 f(u, v)
\end{aligned}
$$

If $1+1 \neq 0$, we can divide by 2 to obtain the required identity.

12.12. Consider the quadratic form $q(x, y)=3 x^{2}+2 x y-y^{2}$ and the linear substitution

$$
x=s-3 t, \quad y=2 s+t
$$

(a) Rewrite $q(x, y)$ in matrix notation, and find the matrix $A$ representing $q(x, y)$.

(b) Rewrite the linear substitution using matrix notation, and find the matrix $P$ corresponding to the substitution.

(c) Find $q(s, t)$ using direct substitution.

(d) Find $q(s, t)$ using matrix notation.

(a) Here $q(x, y)=[x, y]\left[\begin{array}{rr}3 & 1 \\ 1 & -1\end{array}\right]\left[\begin{array}{l}x \\ y\end{array}\right]$. Thus, $A=\left[\begin{array}{rr}3 & 1 \\ 1 & -1\end{array}\right]$; and $q(X)=X^{T} A X$, where $X=[x, y]^{T}$.

(b) Here $\left[\begin{array}{l}x \\ y\end{array}\right]=\left[\begin{array}{rr}1 & -3 \\ 2 & 1\end{array}\right]\left[\begin{array}{l}s \\ t\end{array}\right]$. Thus, $P=\left[\begin{array}{rr}1 & -3 \\ 2 & 1\end{array}\right]$; and $X=\left[\begin{array}{l}x \\ y\end{array}\right], Y=\left[\begin{array}{l}s \\ t\end{array}\right]$ and $X=P Y$.

(c) Substitute for $x$ and $y$ in $q$ to obtain

$$
\begin{aligned}
q(s, t) & =3(s-3 t)^{2}+2(s-3 t)(2 s+t)-(2 s+t)^{2} \\
& =3\left(s^{2}-6 s t+9 t^{2}\right)+2\left(2 s^{2}-5 s t-3 t^{2}\right)-\left(4 s^{2}+4 s t+t^{2}\right)=3 s^{2}-32 s t+20 t^{2}
\end{aligned}
$$

(d) Here $q(X)=X^{T} A X$ and $X=P Y$. Thus, $X^{T}=Y^{T} P^{T}$. Therefore,

$$
\begin{aligned}
q(s, t) & =q(Y)=Y^{T} P^{T} A P Y=[s, t]\left[\begin{array}{rr}
1 & 2 \\
-3 & 1
\end{array}\right]\left[\begin{array}{rr}
3 & 1 \\
1 & -1
\end{array}\right]\left[\begin{array}{rr}
1 & -3 \\
2 & 1
\end{array}\right]\left[\begin{array}{l}
s \\
t
\end{array}\right] \\
& =[s, t]\left[\begin{array}{rr}
3 & -16 \\
-16 & 20
\end{array}\right]\left[\begin{array}{l}
s \\
t
\end{array}\right]=3 s^{2}-32 s t+20 t^{2}
\end{aligned}
$$

[As expected, the results in parts (c) and (d) are equal.]

12.13. Consider any diagonal matrix $A=\operatorname{diag}\left(a_{1}, \ldots, a_{n}\right)$ over $K$. Show that for any nonzero scalars $k_{1}, \ldots, k_{n} \in K, A$ is congruent to a diagonal matrix $D$ with diagonal entries $a_{1} k_{1}^{2}, \ldots, a_{n} k_{n}^{2}$. Furthermore, show that

(a) If $K=\mathbf{C}$, then we can choose $D$ so that its diagonal entries are only l's and 0 's.

(b) If $K=\mathbf{R}$, then we can choose $D$ so that its diagonal entries are only 1 's, -1 's, and 0 's.

Let $P=\operatorname{diag}\left(k_{1}, \ldots, k_{n}\right)$. Then, as required,

$$
D=P^{T} A P=\operatorname{diag}\left(k_{i}\right) \operatorname{diag}\left(a_{i}\right) \operatorname{diag}\left(k_{i}\right)=\operatorname{diag}\left(a_{1} k_{1}^{2}, \ldots, a_{n} k_{n}^{2}\right)
$$

(a) Let $P=\operatorname{diag}\left(b_{i}\right)$, where $b_{i}=\left\{\begin{array}{cll}1 / \sqrt{a_{i}} & \text { if } a_{i} \neq 0 \\ 1 & \text { if } & a_{i}=0\end{array}\right.$

Then $P^{T} A P$ has the required form.

(b) Let $P=\operatorname{diag}\left(b_{i}\right)$, where $b_{i}=\left\{\begin{array}{cl}1 / \sqrt{\left|a_{i}\right|} & \text { if } a_{i} \neq 0 \\ 1 & \text { if } a_{i}=0\end{array}\right.$

Then $P^{T} A P$ has the required form.

Remark: We emphasize that (b) is no longer true if "congruence" is replaced by "Hermitian congruence."

12.14. Prove Theorem 12.5: Let $f$ be a symmetric bilinear form on $V$ over $\mathbf{R}$. Then there exists a basis of $V$ in which $f$ is represented by a diagonal matrix. Every other diagonal matrix representation of $f$ has the same number $\mathbf{p}$ of positive entries and the same number $\mathbf{n}$ of negative entries.

By Theorem 12.4, there is a basis $\left\{u_{1}, \ldots, u_{n}\right\}$ of $V$ in which $f$ is represented by a diagonal matrix with, say, $\mathbf{p}$ positive and $\mathbf{n}$ negative entries. Now suppose $\left\{w_{1}, \ldots, w_{n}\right\}$ is another basis of $V$, in which $f$ is represented by a diagonal matrix with $\mathbf{p}^{\prime}$ positive and $\mathbf{n}^{\prime}$ negative entries. We can assume without loss of generality that the positive entries in each matrix appear first. Because $\operatorname{rank}(f)=\mathbf{p}+\mathbf{n}=\mathbf{p}^{\prime}+\mathbf{n}^{\prime}$, it suffices to prove that $\mathbf{p}=\mathbf{p}^{\prime}$.

Let $U$ be the linear span of $u_{1}, \ldots, u_{\mathbf{p}}$ and let $W$ be the linear span of $w_{\mathbf{p}^{\prime}+1}, \ldots, w_{n}$. Then $f(v, v)>0$ for every nonzero $v \in U$, and $f(v, v) \leq 0$ for every nonzero $v \in W$. Hence, $U \cap W=\{0\}$. Note that $\operatorname{dim} U=\mathbf{p}$ and $\operatorname{dim} W=n-\mathbf{p}^{\prime}$. Thus,

$$
\operatorname{dim}(U+W)=\operatorname{dim} U+\operatorname{dim} W-\operatorname{dim}(U \cap W)=\mathbf{p}+\left(n-\mathbf{p}^{\prime}\right)-0=\mathbf{p}-\mathbf{p}^{\prime}+n
$$

But $\operatorname{dim}(U+W) \leq \operatorname{dim} V=n$; hence, $\mathbf{p}-\mathbf{p}^{\prime}+n \leq n$ or $\mathbf{p} \leq \mathbf{p}^{\prime}$. Similarly, $\mathbf{p}^{\prime} \leq \mathbf{p}$ and therefore $\mathbf{p}=\mathbf{p}^{\prime}$, as required.

Remark: The above theorem and proof depend only on the concept of positivity. Thus, the theorem is true for any subfield $K$ of the real field $\mathbf{R}$ such as the rational field $\mathbf{Q}$.

\section*{Positive Definite Real Quadratic Forms}
12.15. Prove that the following definitions of a positive definite quadratic form $q$ are equivalent:

(a) The diagonal entries are all positive in any diagonal representation of $q$.

(b) $q(Y)>0$, for any nonzero vector $Y$ in $\mathbf{R}^{n}$.

Suppose $q(Y)=a_{1} y_{1}^{2}+a_{2} y_{2}^{2}+\cdots+a_{n} y_{n}^{2}$. If all the coefficients are positive, then clearly $q(Y)>0$ whenever $Y \neq 0$. Thus, (a) implies (b). Conversely, suppose (a) is not true; that is, suppose some diagonal entry $a_{k} \leq 0$. Let $e_{k}=(0, \ldots, 1, \ldots 0)$ be the vector whose entries are all 0 except 1 in the $k$ th position. Then $q\left(e_{k}\right)=a_{k}$ is not positive, and so (b) is not true. That is, (b) implies (a). Accordingly, (a) and (b) are equivalent.

12.16. Determine whether each of the following quadratic forms $q$ is positive definite:

(a) $q(x, y, z)=x^{2}+2 y^{2}-4 x z-4 y z+7 z^{2}$

(b) $q(x, y, z)=x^{2}+y^{2}+2 x z+4 y z+3 z^{2}$

Diagonalize (under congruence) the symmetric matrix $A$ corresponding to $q$.

(a) Apply the operations "Replace $R_{3}$ by $2 R_{1}+R_{3}$ " and "Replace $C_{3}$ by $2 C_{1}+C_{3}$," and then "Replace $R_{3}$ by $R_{2}+R_{3}$ " and "Replace $C_{3}$ by $C_{2}+C_{3}$." These yield

$$
A=\left[\begin{array}{rrr}
1 & 0 & -2 \\
0 & 2 & -2 \\
-2 & -2 & 7
\end{array}\right] \simeq\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 2 & -2 \\
0 & -2 & 3
\end{array}\right] \simeq\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

The diagonal representation of $q$ only contains positive entries, $1,2,1$, on the diagonal. Thus, $q$ is positive definite.

(b) We have

$$
A=\left[\begin{array}{lll}
1 & 0 & 1 \\
0 & 1 & 2 \\
1 & 2 & 3
\end{array}\right] \simeq\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 2 \\
0 & 2 & 2
\end{array}\right] \simeq\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -2
\end{array}\right]
$$

There is a negative entry -2 on the diagonal representation of $q$. Thus, $q$ is not positive definite.

12.17. Show that $q(x, y)=a x^{2}+b x y+c y^{2}$ is positive definite if and only if $a>0$ and the discriminant $D=b^{2}-4 a c<0$.

Suppose $v=(x, y) \neq 0$. Then either $x \neq 0$ or $y \neq 0$; say, $y \neq 0$. Let $t=x / y$. Then

$$
q(v)=y^{2}\left[a(x / y)^{2}+b(x / y)+c\right]=y^{2}\left(a t^{2}+b t+c\right)
$$

However, the following are equivalent:

(i) $s=a t^{2}+b t+c$ is positive for every value of $t$.

(ii) $s=a t^{2}+b t+c$ lies above the $t$-axis.

(iii) $a>0$ and $D=b^{2}-4 a c<0$.

Thus, $q$ is positive definite if and only if $a>0$ and $D<0$. [Remark: $D<0$ is the same as $\operatorname{det}(A)>0$, where $A$ is the symmetric matrix corresponding to $q$.]

12.18. Determine whether or not each of the following quadratic forms $q$ is positive definite:

(a) $q(x, y)=x^{2}-4 x y+7 y^{2}$, (b) $q(x, y)=x^{2}+8 x y+5 y^{2}$, (c) $q(x, y)=3 x^{2}+2 x y+y^{2}$

Compute the discriminant $D=b^{2}-4 a c$, and then use Problem 12.17.

(a) $D=16-28=-12$. Because $a=1>0$ and $D<0, q$ is positive definite.

(b) $D=64-20=44$. Because $D>0, q$ is not positive definite.

(c) $D=4-12=-8$. Because $a=3>0$ and $D<0, q$ is positive definite.

\section*{Hermitian Forms}
12.19. Determine whether the following matrices are Hermitian:

(a) $\left[\begin{array}{ccc}2 & 2+3 i & 4-5 i \\ 2-3 i & 5 & 6+2 i \\ 4+5 i & 6-2 i & -7\end{array}\right]$, (b) $\left[\begin{array}{ccc}3 & 2-i & 4+i \\ 2-i & 6 & i \\ 4+i & i & 7\end{array}\right]$, (c) $\left[\begin{array}{rrr}4 & -3 & 5 \\ -3 & 2 & 1 \\ 5 & 1 & -6\end{array}\right]$

A complex matrix $A=\left[a_{i j}\right]$ is Hermitian if $A^{*}=A$-that is, if $a_{i j}=\bar{a}_{j i}$.

(a) Yes, because it is equal to its conjugate transpose.

(b) No, even though it is symmetric.

(c) Yes. In fact, a real matrix is Hermitian if and only if it is symmetric.

12.20. Let $A$ be a Hermitian matrix. Show that $f$ is a Hermitian form on $\mathbf{C}^{n}$ where $f$ is defined by $f(X, Y)=X^{T} A \bar{Y}$.

For all $a, b \in \mathbf{C}$ and all $X_{1}, X_{2}, Y \in \mathbf{C}^{n}$,

$$
\begin{aligned}
f\left(a X_{1}+b X_{2}, \quad Y\right) & =\left(a X_{1}+b X_{2}\right)^{T} A \bar{Y}=\left(a X_{1}^{T}+b X_{2}^{T}\right) A \bar{Y} \\
& =a X_{1}^{T} A \bar{Y}+b X_{2}^{T} A \bar{Y}=a f\left(X_{1}, Y\right)+b f\left(X_{2}, Y\right)
\end{aligned}
$$

Hence, $f$ is linear in the first variable. Also,

$$
\overline{f(X, Y)}=\overline{X^{T} A \bar{Y}}=\overline{\left(X^{T} A \bar{Y}\right)^{T}}=\overline{\bar{Y}^{T} A^{T} X}=Y^{T} A^{*} \bar{X}=Y^{T} A \bar{X}=f(Y, X)
$$

Hence, $f$ is a Hermitian form on $\mathbf{C}^{n}$.

Remark: We use the fact that $X^{T} A \bar{Y}$ is a scalar and so it is equal to its transpose.

12.21. Let $f$ be a Hermitian form on $V$. Let $H$ be the matrix of $f$ in a basis $S=\left\{u_{i}\right\}$ of $V$. Prove the following:

(a) $f(u, v)=[u]_{S}^{T} H \overline{[v]_{S}}$ for all $u, v \in V$.

(b) If $P$ is the change-of-basis matrix from $S$ to a new basis $S^{\prime}$ of $V$, then $B=P^{T} H \bar{P}$ (or $B=Q^{*} H Q$, where $\left.Q=\bar{P}\right)$ is the matrix of $f$ in the new basis $S^{\prime}$.

Note that (b) is the complex analog of Theorem 12.2.

(a) Let $u, v \in V$ and suppose $u=a_{1} u_{1}+\cdots+a_{n} u_{n}$ and $v=b_{1} u_{1}+\cdots+b_{n} u_{n}$. Then, as required,

$$
\begin{aligned}
f(u, v) & =f\left(a_{1} u_{1}+\cdots+a_{n} u_{n}, \quad b_{1} u_{1}+\cdots+b_{n} u_{n}\right) \\
& =\sum_{i, j} a_{i} \bar{b}_{j} f\left(u_{i}, v_{j}\right)=\left[a_{1}, \ldots, a_{n}\right] H\left[\bar{b}_{1}, \ldots, \bar{b}_{n}\right]^{T}=[u]_{S}^{T} H \overline{[v]}_{S}
\end{aligned}
$$

(b) Because $P$ is the change-of-basis matrix from $S$ to $S^{\prime}$, we have $P[u]_{S^{\prime}}=[u]_{S}$ and $P[v]_{S^{\prime}}=[v]_{S}$; hence, $[u]_{S}^{T}=[u]_{S^{\prime}}^{T} P^{T}$ and $\overline{[v]_{S}}=\bar{P} \overline{[v]_{S^{\prime}}}$. Thus, by (a),

$$
f(u, v)=[u]_{S}^{T} H \overline{[v]_{S}}=[u]_{S^{\prime}}^{T} P^{T} H \bar{P} \overline{[v]_{S^{\prime}}}
$$

But $u$ and $v$ are arbitrary elements of $V$; hence, $P^{T} H \bar{P}$ is the matrix of $f$ in the basis $S^{\prime}$.

12.22. Let $H=\left[\begin{array}{ccc}1 & 1+i & 2 i \\ 1-i & 4 & 2-3 i \\ -2 i & 2+3 i & 7\end{array}\right]$, a Hermitian matrix.

Find a nonsingular matrix $P$ such that $D=P^{T} H \bar{P}$ is diagonal. Also, find the signature of $H$.

Use the modified Algorithm 12.1 that applies the same row operations but the corresponding conjugate column operations. Thus, first form the block matrix $M=[H, I]$ :

$$
M=\left[\begin{array}{cccccc}
1 & 1+i & 2 i & 1 & 0 & 0 \\
1-i & 4 & 2-3 i & 0 & 1 & 0 \\
-2 i & 2+3 i & 7 & 0 & 0 & 1
\end{array}\right]
$$

Apply the row operations "Replace $R_{2}$ by $(-1+i) R_{1}+R_{2}$ " and "Replace $R_{3}$ by $2 i R_{1}+R_{3}$ " and then the corresponding conjugate column operations "Replace $C_{2}$ by $(-1-i) C_{1}+C_{2}$ "' and "Replace $C_{3}$ by $-2 i C_{1}+C_{3}$, to obtain

$$
\left[\begin{array}{cccccc}
1 & 1+i & 2 i & 1 & 0 & 0 \\
0 & 2 & -5 i & -1+i & 1 & 0 \\
0 & 5 i & 3 & 2 i & 0 & 1
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{cccccc}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 2 & -5 i & -1+i & 1 & 0 \\
0 & 5 i & 3 & 2 i & 0 & 1
\end{array}\right]
$$

Next apply the row operation "Replace $R_{3}$ by $-5 i R_{2}+2 R_{3}$ " and the corresponding conjugate column operation "Replace $C_{3}$ by $5 i C_{2}+2 C_{3}$ " to obtain

$$
\left[\begin{array}{cccccc}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 2 & -5 i & -1+i & 1 & 0 \\
0 & 0 & -19 & 5+9 i & -5 i & 2
\end{array}\right] \quad \text { and then } \quad\left[\begin{array}{cccccc}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 2 & 0 & -1+i & 1 & 0 \\
0 & 0 & -38 & 5+9 i & -5 i & 2
\end{array}\right]
$$

Now $H$ has been diagonalized, and the transpose of the right half of $M$ is $P$. Thus, set

$$
P=\left[\begin{array}{ccc}
1 & -1+i & 5+9 i \\
0 & 1 & -5 i \\
0 & 0 & 2
\end{array}\right], \quad \text { and then } \quad D=P^{T} H \bar{P}=\left[\begin{array}{rrr}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & -38
\end{array}\right]
$$

Note $D$ has $\mathbf{p}=2$ positive elements and $\mathbf{n}=1$ negative elements. Thus, the signature of $H$ is $\operatorname{sig}(H)=2-1=1$.

\section*{Miscellaneous Problems}
12.23. Prove Theorem 12.3: Let $f$ be an alternating form on $V$. Then there exists a basis of $V$ in which $f$ is represented by a block diagonal matrix $M$ with blocks of the form $\left[\begin{array}{rr}0 & 1 \\ -1 & 0\end{array}\right]$ or 0 . The number of nonzero blocks is uniquely determined by $f$ [because it is equal to $\frac{1}{2} \operatorname{rank}(f)$ ].

If $f=0$, then the theorem is obviously true. Also, if $\operatorname{dim} V=1$, then $f\left(k_{1} u, k_{2} u\right)=k_{1} k_{2} f(u, u)=0$ and so $f=0$. Accordingly, we can assume that $\operatorname{dim} V>1$ and $f \neq 0$.

Because $f \neq 0$, there exist (nonzero) $u_{1}, u_{2} \in V$ such that $f\left(u_{1}, u_{2}\right) \neq 0$. In fact, multiplying $u_{1}$ by an appropriate factor, we can assume that $f\left(u_{1}, u_{2}\right)=1$ and so $f\left(u_{2}, u_{1}\right)=-1$. Now $u_{1}$ and $u_{2}$ are linearly independent; because if, say, $u_{2}=k u_{1}$, then $f\left(u_{1}, u_{2}\right)=f\left(u_{1}, k u_{1}\right)=k f\left(u_{1}, u_{1}\right)=0$. Let $U=\operatorname{span}\left(u_{1}, u_{2}\right)$; then,\\
(i) The matrix representation of the restriction of $f$ to $U$ in the basis $\left\{u_{1}, u_{2}\right\}$ is $\left[\begin{array}{rr}0 & 1 \\ -1 & 0\end{array}\right]$,

(ii) If $u \in U$, say $u=a u_{1}+b u_{2}$, then

$$
f\left(u, u_{1}\right)=f\left(a u_{1}+b u_{2}, u_{1}\right)=-b \quad \text { and } \quad f\left(u, u_{2}\right)=f\left(a u_{1}+b u_{2}, u_{2}\right)=a
$$

Let $W$ consists of those vectors $w \in V$ such that $f\left(w, u_{1}\right)=0$ and $f\left(w, u_{2}\right)=0$. Equivalently,

$$
W=\{w \in V: f(w, u)=0 \text { for every } u \in U\}
$$

We claim that $V=U \oplus W$. It is clear that $U \cap W=\{0\}$, and so it remains to show that $V=U+W$. Let $v \in V$. Set


\begin{equation*}
u=f\left(v, u_{2}\right) u_{1}-f\left(v, u_{1}\right) u_{2} \quad \text { and } \quad w=v-u \tag{1}
\end{equation*}


Because $u$ is a linear combination of $u_{1}$ and $u_{2}, u \in U$.

We show next that $w \in W$. By (1) and (ii), $f\left(u, u_{1}\right)=f\left(v, u_{1}\right)$; hence,

$$
f\left(w, u_{1}\right)=f\left(v-u, u_{1}\right)=f\left(v, u_{1}\right)-f\left(u, u_{1}\right)=0
$$

Similarly, $f\left(u, u_{2}\right)=f\left(v, u_{2}\right)$ and so

$$
f\left(w, u_{2}\right)+f\left(v-u, u_{2}\right)=f\left(v, u_{2}\right)-f\left(u, u_{2}\right)=0
$$

Then $w \in W$ and so, by (1), $v=u+w$, where $u \in W$. This shows that $V=U+W$; therefore, $V=U \oplus W$.

Now the restriction of $f$ to $W$ is an alternating bilinear form on $W$. By induction, there exists a basis $u_{3}, \ldots, u_{n}$ of $W$ in which the matrix representing $f$ restricted to $W$ has the desired form. Accordingly, $u_{1}, u_{2}, u_{3}, \ldots, u_{n}$ is a basis of $V$ in which the matrix representing $f$ has the desired form.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Bilinear Forms}
12.24. Let $u=\left(x_{1}, x_{2}\right)$ and $v=\left(y_{1}, y_{2}\right)$. Determine which of the following are bilinear forms on $\mathbf{R}^{2}$ :\\
(a) $f(u, v)=2 x_{1} y_{2}-3 x_{2} y_{1}$,\\
(c) $f(u, v)=3 x_{2} y_{2}$,\\
(e) $f(u, v)=1$,\\
(b) $f(u, v)=x_{1}+y_{2}$,\\
(d) $f(u, v)=x_{1} x_{2}+y_{1} y_{2}$,\\
(f) $f(u, v)=0$

12.25. Let $f$ be the bilinear form on $\mathbf{R}^{2}$ defined by

$$
f\left[\left(x_{1}, x_{2}\right), \quad\left(y_{1}, y_{2}\right)\right]=3 x_{1} y_{1}-2 x_{1} y_{2}+4 x_{2} y_{1}-x_{2} y_{2}
$$

(a) Find the matrix $A$ of $f$ in the basis $\left\{u_{1}=(1,1), u_{2}=(1,2)\right\}$.

(b) Find the matrix $B$ of $f$ in the basis $\left\{v_{1}=(1,-1), v_{2}=(3,1)\right\}$.

(c) Find the change-of-basis matrix $P$ from $\left\{u_{i}\right\}$ to $\left\{v_{i}\right\}$, and verify that $B=P^{T} A P$.

12.26. Let $V$ be the vector space of two-square matrices over $\mathbf{R}$. Let $M=\left[\begin{array}{ll}1 & 2 \\ 3 & 5\end{array}\right]$, and let $f(A, B)=\operatorname{tr}\left(A^{T} M B\right)$, where $A, B \in V$ and "tr" denotes trace. (a) Show that $f$ is a bilinear form on $V$. (b) Find the matrix of $f$ in the basis

$$
\left\{\left[\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right],\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right]\right\}
$$

12.27. Let $B(V)$ be the set of bilinear forms on $V$ over $K$. Prove the following:

(a) If $f, g \in B(V)$, then $f+g$, $k g \in B(V)$ for any $k \in K$.

(b) If $\phi$ and $\sigma$ are linear functions on $V$, then $f(u, v)=\phi(u) \sigma(v)$ belongs to $B(V)$.

12.28. Let $[f]$ denote the matrix representation of a bilinear form $f$ on $V$ relative to a basis $\left\{u_{i}\right\}$. Show that the mapping $f \mapsto[f]$ is an isomorphism of $B(V)$ onto the vector space $V$ of $n$-square matrices.

12.29. Let $f$ be a bilinear form on $V$. For any subset $S$ of $V$, let

$$
S^{\perp}=\{v \in V: f(u, v)=0 \text { for every } u \in S\} \text { and } S^{\top}=\{v \in V: f(v, u)=0 \text { for every } u \in S\}
$$

Show that: (a) $S^{\top}$ and $S^{\top}$ are subspaces of $V$; (b) $S_{1} \subseteq S_{2}$ implies $S_{2}^{\perp} \subseteq S_{1}^{\perp}$ and $S_{2}^{\top} \subseteq S_{1}^{\top}$; (c) $\{0\}^{\perp}=\{0\}^{\top}=V$.

12.30. Suppose $f$ is a bilinear form on $V$. Prove that: $\operatorname{rank}(f)=\operatorname{dim} V-\operatorname{dim} V^{\perp}=\operatorname{dim} V-\operatorname{dim} V^{\top}$, and hence, $\operatorname{dim} V^{\perp}=\operatorname{dim} V^{\top}$.

12.31. Let $f$ be a bilinear form on $V$. For each $u \in V$, let $\hat{u}: V \rightarrow K$ and $\tilde{u}: V \rightarrow K$ be defined by $\hat{u}(x)=f(x, u)$ and $\tilde{u}(x)=f(u, x)$. Prove the following:

(a) $\hat{u}$ and $\tilde{u}$ are each linear; i.e., $\hat{u}, \tilde{u} \in V^{*}$,

(b) $u \mapsto \hat{u}$ and $u \mapsto \tilde{u}$ are each linear mappings from $V$ into $V^{*}$,

(c) $\operatorname{rank}(f)=\operatorname{rank}(u \mapsto \hat{u})=\operatorname{rank}(u \mapsto \tilde{u})$.

12.32. Show that congruence of matrices (denoted by $\simeq$ ) is an equivalence relation; that is,

(i) $A \simeq A$; (ii) If $A \simeq B$, then $B \simeq A$; (iii) If $A \simeq B$ and $B \simeq C$, then $A \simeq C$.

\section*{Symmetric Bilinear Forms, Quadratic Forms}
12.33. Find the symmetric matrix $A$ belonging to each of the following quadratic forms:\\
(a) $q(x, y, z)-2 x^{2}-8 x y+y^{2}-16 x z+14 y z+5 z^{2}$,\\
(c) $q(x, y, z)=x y+y^{2}+4 x z+z^{2}$\\
(b) $q(x, y, z)=x^{2}-x z+y^{2}$,\\
(d) $q(x, y, z)=x y+y z$

12.34. For each of the following symmetric matrices $A$, find a nonsingular matrix $P$ such that $D=P^{T} A P$ is diagonal:

(a) $A=\left[\begin{array}{lll}1 & 0 & 2 \\ 0 & 3 & 6 \\ 2 & 6 & 7\end{array}\right]$, (b) $A=\left[\begin{array}{rrr}1 & -2 & 1 \\ -2 & 5 & 3 \\ 1 & 3 & -2\end{array}\right]$, (c) $A=\left[\begin{array}{rrrr}1 & -1 & 0 & 2 \\ -1 & 2 & 1 & 0 \\ 0 & 1 & 1 & 2 \\ 2 & 0 & 2 & -1\end{array}\right]$

12.35. Let $q(x, y)=2 x^{2}-6 x y-3 y^{2}$ and $x=s+2 t, y=3 s-t$.

(a) Rewrite $q(x, y)$ in matrix notation, and find the matrix $A$ representing the quadratic form.

(b) Rewrite the linear substitution using matrix notation, and find the matrix $P$ corresponding to the substitution.

(c) Find $q(s, t)$ using (i) direct substitution, (ii) matrix notation.

12.36. For each of the following quadratic forms $q(x, y, z)$, find a nonsingular linear substitution expressing the variables $x, y, z$ in terms of variables $r, s, t$ such that $q(r, s, t)$ is diagonal:

(a) $q(x, y, z)=x^{2}+6 x y+8 y^{2}-4 x z+2 y z-9 z^{2}$,

(b) $q(x, y, z)=2 x^{2}-3 y^{2}+8 x z+12 y z+25 z^{2}$,

(c) $q(x, y, z)=x^{2}+2 x y+3 y^{2}+4 x z+8 y z+6 z^{2}$.

In each case, find the rank and signature.

12.37. Give an example of a quadratic form $q(x, y)$ such that $q(u)=0$ and $q(v)=0$ but $q(u+v) \neq 0$.

12.38. Let $S(V)$ denote all symmetric bilinear forms on $V$. Show that

(a) $S(V)$ is a subspace of $B(V)$; (b) If $\operatorname{dim} V=n$, then $\operatorname{dim} S(V)=\frac{1}{2} n(n+1)$.

12.39. Consider a real quadratic polynomial $q\left(x_{1}, \ldots, x_{n}\right)=\sum_{i, j=1}^{n} a_{i j} x_{i} x_{j}$, where $a_{i j}=a_{j i}$.\\
(a) If $a_{11} \neq 0$, show that the substitution

$$
x_{1}=y_{1}-\frac{1}{a_{11}}\left(a_{12} y_{2}+\cdots+a_{1 n} y_{n}\right), \quad x_{2}=y_{2}, \quad \ldots, \quad x_{n}=y_{n}
$$

yields the equation $q\left(x_{1}, \ldots, x_{n}\right)=a_{11} y_{1}^{2}+q^{\prime}\left(y_{2}, \ldots, y_{n}\right)$, where $q^{\prime}$ is also a quadratic polynomial.

(b) If $a_{11}=0$ but, say, $a_{12} \neq 0$, show that the substitution

$$
x_{1}=y_{1}+y_{2}, \quad x_{2}=y_{1}-y_{2}, \quad x_{3}=y_{3}, \quad \ldots, \quad x_{n}=y_{n}
$$

yields the equation $q\left(x_{1}, \ldots, x_{n}\right)=\sum b_{i j} y_{i} y_{j}$, where $b_{11} \neq 0$, which reduces this case to case (a).

Remark: This method of diagonalizing $q$ is known as completing the square.

\section*{Positive Definite Quadratic Forms}
12.40. Determine whether or not each of the following quadratic forms is positive definite:\\
(a) $q(x, y)=4 x^{2}+5 x y+7 y^{2}$,\\
(c) $q(x, y, z)=x^{2}+4 x y+5 y^{2}+6 x z+2 y z+4 z^{2}$\\
(b) $q(x, y)=2 x^{2}-3 x y-y^{2}$,\\
(d) $q(x, y, z)=x^{2}+2 x y+2 y^{2}+4 x z+6 y z+7 z^{2}$

12.41. Find those values of $k$ such that the given quadratic form is positive definite:\\
(a) $q(x, y)=2 x^{2}-5 x y+k y^{2}$,\\
(b) $q(x, y)=3 x^{2}-k x y+12 y^{2}$\\
(c) $q(x, y, z)=x^{2}+2 x y+2 y^{2}+2 x z+6 y z+k z^{2}$

12.42. Suppose $A$ is a real symmetric positive definite matrix. Show that $A=P^{T} P$ for some nonsingular matrix $P$.

\section*{Hermitian Forms}
12.43. Modify Algorithm 12.1 so that, for a given Hermitian matrix $H$, it finds a nonsingular matrix $P$ for which $D=P^{T} A \bar{P}$ is diagonal.

12.44. For each Hermitian matrix $H$, find a nonsingular matrix $P$ such that $D=P^{T} H \bar{P}$ is diagonal:\\
(a) $H=\left[\begin{array}{rr}1 & i \\ -i & 2\end{array}\right]$,\\
(b) $H=\left[\begin{array}{cc}1 & 2+3 i \\ 2-3 i & -1\end{array}\right]$,\\
(c) $H=\left[\begin{array}{ccc}1 & i & 2+i \\ -i & 2 & 1-i \\ 2-i & 1+i & 2\end{array}\right]$

Find the rank and signature in each case.

12.45. Let $A$ be a complex nonsingular matrix. Show that $H=A^{*} A$ is Hermitian and positive definite.

12.46. We say that $B$ is Hermitian congruent to $A$ if there exists a nonsingular matrix $P$ such that $B=P^{T} A \bar{P}$ or, equivalently, if there exists a nonsingular matrix $Q$ such that $B=Q^{*} A Q$. Show that Hermitian congruence is an equivalence relation. (Note: If $P=\bar{Q}$, then $P^{T} A \bar{P}=Q^{*} A Q$.)

12.47. Prove Theorem 12.7: Let $f$ be a Hermitian form on $V$. Then there is a basis $S$ of $V$ in which $f$ is represented by a diagonal matrix, and every such diagonal representation has the same number $\mathbf{p}$ of positive entries and the same number $\mathbf{n}$ of negative entries.

\section*{Miscellaneous Problems}
12.48. Let $e$ denote an elementary row operation, and let $f^{*}$ denote the corresponding conjugate column operation (where each scalar $k$ in $e$ is replaced by $\bar{k}$ in $f^{*}$ ). Show that the elementary matrix corresponding to $f^{*}$ is the conjugate transpose of the elementary matrix corresponding to $e$.

12.49. Let $V$ and $W$ be vector spaces over $K$. A mapping $f: V \times W \rightarrow K$ is called a bilinear form on $V$ and $W$ if

(i) $f\left(a v_{1}+b v_{2}, w\right)=a f\left(v_{1}, w\right)+b f\left(v_{2}, w\right)$,

(ii) $f\left(v, a w_{1}+b w_{2}\right)=a f\left(v, w_{1}\right)+b f\left(v, w_{2}\right)$

for every $a, b \in K, v_{i} \in V, w_{j} \in W$. Prove the following:\\
(a) The set $B(V, W)$ of bilinear forms on $V$ and $W$ is a subspace of the vector space of functions from $V \times W$ into $K$.

(b) If $\left\{\phi_{1}, \ldots, \phi_{m}\right\}$ is a basis of $V^{*}$ and $\left\{\sigma_{1}, \ldots, \sigma_{n}\right\}$ is a basis of $W^{*}$, then $\left\{f_{i j}: i=1, \ldots, m, j=1, \ldots, n\right\}$ is a basis of $B(V, W)$, where $f_{i j}$ is defined by $f_{i j}(v, w)=\phi_{i}(v) \sigma_{j}(w)$. Thus, $\operatorname{dim} B(V, W)=\operatorname{dim} V \operatorname{dim} W$.

[Note that if $V=W$, then we obtain the space $B(V)$ investigated in this chapter.]

12.50. Let $V$ be a vector space over $K$. A mapping $f: \overbrace{V \times V \times \ldots \times V}^{m \text { times }} \rightarrow K$ is called a multilinear (or m-linear) form on $V$ if $f$ is linear in each variable; that is, for $i=1, \ldots, m$,

$$
f(\ldots, \widehat{a u+b v}, \ldots)=a f(\ldots, \hat{u}, \ldots)+b f(\ldots, \hat{v}, \ldots)
$$

where $\widehat{. .}$ denotes the $i$ th element, and other elements are held fixed. An $m$-linear form $f$ is said to be alternating if $f\left(v_{1}, \ldots v_{m}\right)=0$ whenever $v_{i}=v_{j}$ for $i \neq j$. Prove the following:

(a) The set $B_{m}(V)$ of $m$-linear forms on $V$ is a subspace of the vector space of functions from $V \times V \times \cdots \times V$ into $K$.

(b) The set $A_{m}(V)$ of alternating $m$-linear forms on $V$ is a subspace of $B_{m}(V)$.

Remark 1: If $m=2$, then we obtain the space $B(V)$ investigated in this chapter.

Remark 2: If $V=K^{m}$, then the determinant function is an alternating $m$-linear form on $V$.

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $M=\left[R_{1} ; \quad R_{2} ; \quad \ldots\right]$ denotes a matrix $M$ with rows $R_{1}, R_{2}, \ldots$\\
12.24. (a) yes,\\
(b) no,\\
(c) yes,\\
(d) no,\\
(e) no,\\
(f) yes\\
12.25. (a) $A=[4,1 ; 7,3]$,\\
(b) $B=[0,-4 ; 20,32]$,\\
(c) $P=[3,5 ;-2,-2]$\\
12.26. (b) $[1,0,2,0 ; 0,1,0,2 ; 3,0,5,0 ; 0,3,0,5]$\\
12.33. (a) $[2,-4,-8 ;-4,1,7 ;-8,7,5]$,\\
(c) $\left[0, \frac{1}{2}, 2 ; \frac{1}{2}, 1,0 ; 2,0,1\right]$,\\
(d) $\quad\left[0, \frac{1}{2}, 0 ; \frac{1}{2}, 0,1 ; \frac{1}{2}, 0, \frac{1}{2} ; 0, \frac{1}{2}, 0\right]$\\
12.34. (a) $P=[1,0,-2 ; 0,1,-2 ; 0,0,1], D=\operatorname{diag}(1,3,-9)$;\\
(b) $P=[1,2,-11 ; 0,1,-5 ; 0,0,1], D=\operatorname{diag}(1,1,-28)$;\\
(c) $P=[1,1,-1,-4 ; 0,1,-1,-2 ; 0,0,1,0 ; 0,0,0,1], D=\operatorname{diag}(1,1,0,-9)$

12.35. $A=[2,-3 ;-3,-3], P=[1,2 ; 3,-1], q(s, t)=-43 s^{2}-4 s t+17 t^{2}$

12.36. (a) $x=r-3 s-19 t, \quad y=s+7 t, \quad z=t ; \quad q(r, s, t)=r^{2}-s^{2}+36 t^{2}$;

(b) $x=r-2 t, \quad y=s+2 t, \quad z=t ; \quad q(r, s, t)=2 r^{2}-3 s^{2}+29 t^{2}$;

(c) $x=r-s-t, y=s-t, z=t ; \quad q(r, s, t)=r^{2}-2 s^{2}$

12.37. $q(x, y)=x^{2}-y^{2}, u=(1,1), v=(1,-1)$\\
12.40. (a) yes,\\
(b) no,\\
(c) no,\\
(d) yes

12.41. (a) $k>\frac{25}{8}$,

(b) $-12<k<12$,

(c) $k>5$

12.44. (a) $P=[1, i ; \quad 0,1], D=I, s=2$;

(b) $P=[1,-2+3 i ; 0,1], D=\operatorname{diag}(1,-14), s=0$;

(c) $P=[1, i,-3+i ; 0,1, i ; 0,0,1], D=\operatorname{diag}(1,1,-4), s=1$

\section*{Linear Operators on Inner Product Spaces}
\subsection*{13.1 Introduction}
This chapter investigates the space $A(V)$ of linear operators $T$ on an inner product space $V$. (See Chapter 7.) Thus, the base field $K$ is either the real numbers $\mathbf{R}$ or the complex numbers $\mathbf{C}$. In fact, different terminologies will be used for the real case and the complex case. We also use the fact that the inner products on real Euclidean space $\mathbf{R}^{n}$ and complex Euclidean space $\mathbf{C}^{n}$ may be defined, respectively, by

$$
\langle u, v\rangle=u^{T} v \quad \text { and } \quad\langle u, v\rangle=u^{T} \bar{v}
$$

where $u$ and $v$ are column vectors.

The reader should review the material in Chapter 7 and be very familiar with the notions of norm (length), orthogonality, and orthonormal bases. We also note that Chapter 7 mainly dealt with real inner product spaces, whereas here we assume that $V$ is a complex inner product space unless otherwise stated or implied.

Lastly, we note that in Chapter 2, we used $A^{H}$ to denote the conjugate transpose of a complex matrix $A$; that is, $A^{H}=\overline{A^{T}}$. This notation is not standard. Many texts, expecially advanced texts, use $A^{*}$ to denote such a matrix; we will use that notation in this chapter. That is, now $A^{*}=\overline{A^{T}}$.

\subsection*{13.2 Adjoint Operators}
We begin with the following basic definition.

DEFINITION: $\quad$ A linear operator $T$ on an inner product space $V$ is said to have an adjoint operator $T^{*}$ on $V$ if $\langle T(u), v\rangle=\left\langle u, T^{*}(v)\right\rangle$ for every $u, v \in V$.

The following example shows that the adjoint operator has a simple description within the context of matrix mappings.

\section*{EXAMPLE 13.1}
(a) Let $A$ be a real $n$-square matrix viewed as a linear operator on $\mathbf{R}^{n}$. Then, for every $u, v \in \mathbf{R}_{n}$,

$$
\langle A u, v\rangle=(A u)^{T} v=u^{T} A^{T} v=\left\langle u, A^{T} v\right\rangle
$$

Thus, the transpose $A^{T}$ of $A$ is the adjoint of $A$.

(b) Let $B$ be a complex $n$-square matrix viewed as a linear operator on $\mathbf{C}^{n}$. Then for every $u, v, \in \mathbf{C}^{n}$,

$$
\langle B u, v\rangle=(B u)^{T} \bar{v}=u^{T} B^{T} \bar{v}=u^{T} \overline{B^{*}} \bar{v}=\left\langle u, B^{*} v\right\rangle
$$

Thus, the conjugate transpose $B^{*}$ of $B$ is the adjoint of $B$.

Remark: $B^{*}$ may mean either the adjoint of $B$ as a linear operator or the conjugate transpose of $B$ as a matrix. By Example 13.1(b), the ambiguity makes no difference, because they denote the same object.

The following theorem (proved in Problem 13.4) is the main result in this section.

THEOREM 13.1: $\quad$ Let $T$ be a linear operator on a finite-dimensional inner product space $V$ over $K$. Then

(i) There exists a unique linear operator $T^{*}$ on $V$ such that $\langle T(u), v\rangle=\left\langle u, T^{*}(v)\right\rangle$ for every $u, v \in V$. (That is, $T$ has an adjoint $T^{*}$.)

(ii) If $A$ is the matrix representation $T$ with respect to any orthonormal basis $S=\left\{u_{i}\right\}$ of $V$, then the matrix representation of $T^{*}$ in the basis $S$ is the conjugate transpose $A^{*}$ of $A$ (or the transpose $A^{T}$ of $A$ when $K$ is real).

We emphasize that no such simple relationship exists between the matrices representing $T$ and $T^{*}$ if the basis is not orthonormal. Thus, we see one useful property of orthonormal bases. We also emphasize that this theorem is not valid if $V$ has infinite dimension (Problem 13.31).

The following theorem (proved in Problem 13.5) summarizes some of the properties of the adjoint.

THEOREM 13.2: $\quad$ Let $T, T_{1}, T_{2}$ be linear operators on $V$ and let $k \in K$. Then\\
(i) $\left(T_{1}+T_{2}\right) *=T_{1}^{*}+T_{2}^{*}$,\\
(iii) $\left(T_{1} T_{2}\right)^{*}=T_{2}^{*} T_{1}^{*}$,\\
(ii) $(k T)^{*}=\bar{k} T^{*}$,\\
(iv) $\left(T^{*}\right)^{*}=T$.

Observe the similarity between the above theorem and Theorem 2.3 on properties of the transpose operation on matrices.

\section*{Linear Functionals and Inner Product Spaces}
Recall (Chapter 11) that a linear functional $\phi$ on a vector space $V$ is a linear mapping $\phi: V \rightarrow K$. This subsection contains an important result (Theorem 13.3) that is used in the proof of the above basic Theorem 13.1.

Let $V$ be an inner product space. Each $u \in V$ determines a mapping $\hat{u}: V \rightarrow K$ defined by

$$
\hat{u}(v)=\langle v, u\rangle
$$

Now, for any $a, b \in K$ and any $v_{1}, v_{2} \in V$,

$$
\hat{u}\left(a v_{1}+b v_{2}\right)=\left\langle a v_{1}+b v_{2}, \quad u\right\rangle=a\left\langle v_{1}, u\right\rangle+b\left\langle v_{2}, u\right\rangle=a \hat{u}\left(v_{1}\right)+b \hat{u}\left(v_{2}\right)
$$

That is, $\hat{u}$ is a linear functional on $V$. The converse is also true for spaces of finite dimension and it is contained in the following important theorem (proved in Problem 13.3).

THEOREM 13.3: Let $\phi$ be a linear functional on a finite-dimensional inner product space $V$. Then there exists a unique vector $u \in V$ such that $\phi(v)=\langle v, u\rangle$ for every $v \in V$.

We remark that the above theorem is not valid for spaces of infinite dimension (Problem 13.24).

\subsection*{13.3 Analogy Between $A(V)$ and C, Special Linear Operators}
Let $A(V)$ denote the algebra of all linear operators on a finite-dimensional inner product space $V$. The adjoint mapping $T \mapsto T^{*}$ on $A(V)$ is quite analogous to the conjugation mapping $z \mapsto \bar{z}$ on the complex field $\mathbf{C}$. To illustrate this analogy we identify in Table 13-1 certain classes of operators $T \in A(V)$ whose behavior under the adjoint map imitates the behavior under conjugation of familiar classes of complex numbers.

The analogy between these operators $T$ and complex numbers $z$ is reflected in the next theorem.

Table 13-1

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\begin{tabular}{l}
Class of complex \\
numbers \\
\end{tabular} & \begin{tabular}{l}
Behavior under \\
conjugation \\
\end{tabular} & Class of operators in $A(V)$ & \begin{tabular}{l}
Behavior under the \\
adjoint map \\
\end{tabular} \\
\hline
Unit circle $(|z|=1)$ & $\bar{z}=1 / z$ & \begin{tabular}{l}
Orthogonal operators (real case) \\
Unitary operators (complex case) \\
\end{tabular} & $T^{*}=T^{-1}$ \\
\hline
Real axis & $\bar{z}=z$ & \begin{tabular}{l}
Self-adjoint operators \\
Also called: \\
symmetric (real case) \\
Hermitian (complex case) \\
\end{tabular} & $T^{*}=T$ \\
\hline
Imaginary axis & $\bar{z}=-z$ & \begin{tabular}{l}
Skew-adjoint operators \\
Also called: \\
skew-symmetric (real case) \\
skew-Hermitian (complex case) \\
\end{tabular} & $T^{*}=-T$ \\
\hline
\begin{tabular}{l}
Positive real axis \\
$(0, \infty)$ \\
\end{tabular} & $z=\bar{w} w, w \neq 0$ & Positive definite operators & \begin{tabular}{l}
$T=S^{*} S$ \\
with $S$ nonsingular \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

THEOREM 13.4: Let $\lambda$ be an eigenvalue of a linear operator $T$ on $V$.

(i) If $T^{*}=T^{-1}$ (i.e., $T$ is orthogonal or unitary), then $|\lambda|=1$.

(ii) If $T^{*}=T$ (i.e., $T$ is self-adjoint), then $\lambda$ is real.

(iii) If $T^{*}=-T$ (i.e., $T$ is skew-adjoint), then $\lambda$ is pure imaginary.

(iv) If $T=S^{*} S$ with $S$ nonsingular (i.e., $T$ is positive definite), then $\lambda$ is real and positive.

Proof. In each case let $v$ be a nonzero eigenvector of $T$ belonging to $\lambda$; that is, $T(v)=\lambda v$ with $v \neq 0$. Hence, $\langle v, v\rangle$ is positive.

Proof of (i). We show that $\lambda \bar{\lambda}\langle v, v\rangle=\langle v, v\rangle$ :

$$
\lambda \bar{\lambda}\langle v, v\rangle=\langle\lambda v, \lambda v\rangle=\langle T(v), T(v)\rangle=\left\langle v, T^{*} T(v)\right\rangle=\langle v, I(v)\rangle=\langle v, v\rangle
$$

But $\langle v, v\rangle \neq 0$; hence, $\lambda \bar{\lambda}=1$ and so $|\lambda|=1$.

Proof of (ii). We show that $\lambda\langle v, v\rangle=\bar{\lambda}\langle v, v\rangle$ :

$$
\lambda\langle v, v\rangle=\langle\lambda v, v\rangle=\langle T(v), v\rangle=\left\langle v, T^{*}(v)\right\rangle=\langle v, T(v)\rangle=\langle v, \lambda v\rangle=\bar{\lambda}\langle v, v\rangle
$$

But $\langle v, v\rangle \neq 0$; hence, $\lambda=\bar{\lambda}$ and so $\lambda$ is real.

Proof of (iii). We show that $\lambda\langle v, v\rangle=-\bar{\lambda}\langle v, v\rangle$ :

$$
\lambda\langle v, v\rangle=\langle\lambda v, v\rangle=\langle T(v), v\rangle=\left\langle v, T^{*}(v)\right\rangle=\langle v,-T(v)\rangle=\langle v,-\lambda v\rangle=-\bar{\lambda}\langle v, v\rangle
$$

But $\langle v, v\rangle \neq 0$; hence, $\lambda=-\bar{\lambda}$ or $\bar{\lambda}=-\lambda$, and so $\lambda$ is pure imaginary.

Proof of (iv). Note first that $S(v) \neq 0$ because $S$ is nonsingular; hence, $\langle S(v), S(v)\rangle$ is positive. We show that $\lambda\langle v, v\rangle=\langle S(v), S(v)\rangle$ :

$$
\lambda\langle v, v\rangle=\langle\lambda v, v\rangle=\langle T(v), v\rangle=\left\langle S^{*} S(v), v\right\rangle=\langle S(v), S(v)\rangle
$$

But $\langle v, v\rangle$ and $\langle S(v), S(v)\rangle$ are positive; hence, $\lambda$ is positive.

Remark: Each of the above operators $T$ commutes with its adjoint; that is, $T T^{*}=T^{*} T$. Such operators are called normal operators.

\subsection*{13.4 Self-Adjoint Operators}
Let $T$ be a self-adjoint operator on an inner product space $V$; that is, suppose

$$
T^{*}=T
$$

(If $T$ is defined by a matrix $A$, then $A$ is symmetric or Hermitian according as $A$ is real or complex.) By Theorem 13.4, the eigenvalues of $T$ are real. The following is another important property of $T$.

THEOREM 13.5: Let $T$ be a self-adjoint operator on $V$. Suppose $u$ and $v$ are eigenvectors of $T$ belonging to distinct eigenvalues. Then $u$ and $v$ are orthogonal; that is, $\langle u, v\rangle=0$.

Proof. Suppose $T(u)=\lambda_{1} u$ and $T(v)=\lambda_{2} v$, where $\lambda_{1} \neq \lambda_{2}$. We show that $\lambda_{1}\langle u, v\rangle=\lambda_{2}\langle u, v\rangle$ :

$$
\begin{aligned}
\lambda_{1}\langle u, v\rangle & =\left\langle\lambda_{1} u, v\right\rangle=\langle T(u), v\rangle=\left\langle u, T^{*}(v)\right\rangle=\langle u, T(v)\rangle \\
& =\left\langle u, \lambda_{2} v\right\rangle=\bar{\lambda}_{2}\langle u, v\rangle=\lambda_{2}\langle u, v\rangle
\end{aligned}
$$

(The fourth equality uses the fact that $T^{*}=T$, and the last equality uses the fact that the eigenvalue $\lambda_{2}$ is real.) Because $\lambda_{1} \neq \lambda_{2}$, we get $\langle u, v\rangle=0$. Thus, the theorem is proved.

\subsection*{13.5 Orthogonal and Unitary Operators}
Let $U$ be a linear operator on a finite-dimensional inner product space $V$. Suppose

$$
U^{*}=U^{-1} \quad \text { or equivalently } \quad U U^{*}=U^{*} U=I
$$

Recall that $U$ is said to be orthogonal or unitary according as the underlying field is real or complex. The next theorem (proved in Problem 13.10) gives alternative characterizations of these operators.

THEOREM 13.6: The following conditions on an operator $U$ are equivalent:

(i) $U^{*}=U^{-1}$; that is, $U U^{*}=U^{*} U=I$. [U is unitary (orthogonal).]

(ii) $U$ preserves inner products; that is, for every $v, w \in V$, $\langle U(v), U(w)\rangle=\langle v, w\rangle$.

(iii) $U$ preserves lengths; that is, for every $v \in V,\|U(v)\|=\|v\|$.

\section*{EXAMPLE 13.2}
(a) Let $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be the linear operator that rotates each vector $v$ about the $z$-axis by a fixed angle $\theta$ as shown in Fig. 10-1 (Section 10.3). That is, $T$ is defined by

$$
T(x, y, z)=(x \cos \theta-y \sin \theta, \quad x \sin \theta+y \cos \theta, \quad z)
$$

We note that lengths (distances from the origin) are preserved under $T$. Thus, $T$ is an orthogonal operator.

(b) Let $V$ be $l_{2}$-space (Hilbert space), defined in Section 7.3. Let $T: V \rightarrow V$ be the linear operator defined by

$$
T\left(a_{1}, a_{2}, a_{3}, \ldots\right)=\left(0, a_{1}, a_{2}, a_{3}, \ldots\right)
$$

Clearly, $T$ preserves inner products and lengths. However, $T$ is not surjective, because, for example, $(1,0,0, \ldots)$ does not belong to the image of $T$; hence, $T$ is not invertible. Thus, we see that Theorem 13.6 is not valid for spaces of infinite dimension.

An isomorphism from one inner product space into another is a bijective mapping that preserves the three basic operations of an inner product space: vector addition, scalar multiplication, and inner\\
products. Thus, the above mappings (orthogonal and unitary) may also be characterized as the isomorphisms of $V$ into itself. Note that such a mapping $U$ also preserves distances, because

$$
\|U(v)-U(w)\|=\|U(v-w)\|=\|v-w\|
$$

Hence, $U$ is called an isometry.

\subsection*{13.6 Orthogonal and Unitary Matrices}
Let $U$ be a linear operator on an inner product space $V$. By Theorem 13.1, we obtain the following results.

THEOREM 13.7A: A complex matrix $A$ represents a unitary operator $U$ (relative to an orthonormal basis) if and only if $A^{*}=A^{-1}$.

THEOREM 13.7B: A real matrix $A$ represents an orthogonal operator $U$ (relative to an orthonormal basis) if and only if $A^{T}=A^{-1}$.

The above theorems motivate the following definitions (which appeared in Sections 2.10 and 2.11).

DEFINITION: A complex matrix $A$ for which $A^{*}=A^{-1}$ is called a unitary matrix.

DEFINITION: $\quad$ A real matrix $A$ for which $A^{T}=A^{-1}$ is called an orthogonal matrix.

We repeat Theorem 2.6, which characterizes the above matrices.

THEOREM 13.8: The following conditions on a matrix $A$ are equivalent:

(i) $A$ is unitary (orthogonal).

(ii) The rows of $A$ form an orthonormal set.

(iii) The columns of $A$ form an orthonormal set.

\subsection*{13.7 Change of Orthonormal Basis}
Orthonormal bases play a special role in the theory of inner product spaces $V$. Thus, we are naturally interested in the properties of the change-of-basis matrix from one such basis to another. The following theorem (proved in Problem 13.12) holds.

THEOREM 13.9: Let $\left\{u_{1}, \ldots, u_{n}\right\}$ be an orthonormal basis of an inner product space $V$. Then the change-of-basis matrix from $\left\{u_{i}\right\}$ into another orthonormal basis is unitary (orthogonal). Conversely, if $P=\left[a_{i j}\right]$ is a unitary (orthogonal) matrix, then the following is an orthonormal basis:

$$
\left\{u_{i}^{\prime}=a_{1 i} u_{1}+a_{2 i} u_{2}+\cdots+a_{n i} u_{n}: i=1, \ldots, n\right\}
$$

Recall that matrices $A$ and $B$ representing the same linear operator $T$ are similar; that is, $B=P^{-1} A P$, where $P$ is the (nonsingular) change-of-basis matrix. On the other hand, if $V$ is an inner product space, we are usually interested in the case when $P$ is unitary (or orthogonal) as suggested by Theorem 13.9. (Recall that $P$ is unitary if the conjugate tranpose $P^{*}=P^{-1}$, and $P$ is orthogonal if the transpose $P^{T}=P^{-1}$.) This leads to the following definition.

DEFINITION: $\quad$ Complex matrices $A$ and $B$ are unitarily equivalent if there exists a unitary matrix $P$ for which $B=P^{*} A P$. Analogously, real matrices $A$ and $B$ are orthogonally equivalent if there exists an orthogonal matrix $P$ for which $B=P^{T} A P$.

Note that orthogonally equivalent matrices are necessarily congruent.

\subsection*{13.8 Positive Definite and Positive Operators}
Let $P$ be a linear operator on an inner product space $V$. Then

(i) $P$ is said to be positive definite if $P=S^{*} S$ for some nonsingular operators $S$.

(ii) $P$ is said to be positive (or nonnegative or semidefinite) if $P=S^{*} S$ for some operator $S$.

The following theorems give alternative characterizations of these operators.

THEOREM 13.10A: The following conditions on an operator $P$ are equivalent:

(i) $P=T^{2}$ for some nonsingular self-adjoint operator $T$.

(ii) $P$ is positive definite.

(iii) $P$ is self-adjoint and $\langle P(u), u\rangle>0$ for every $u \neq 0$ in $V$.

The corresponding theorem for positive operators (proved in Problem 13.21) follows.

THEOREM 13.10B: The following conditions on an operator $P$ are equivalent:

(i) $P=T^{2}$ for some self-adjoint operator $T$.

(ii) $P$ is positive; that is, $P=S^{*} S$.

(iii) $P$ is self-adjoint and $\langle P(u), u\rangle \geq 0$ for every $u \in V$.

\subsection*{13.9 Diagonalization and Canonical Forms in Inner Product Spaces}
Let $T$ be a linear operator on a finite-dimensional inner product space $V$ over $K$. Representing $T$ by a diagonal matrix depends upon the eigenvectors and eigenvalues of $T$, and hence, upon the roots of the characteristic polynomial $\Delta(t)$ of $T$. Now $\Delta(t)$ always factors into linear polynomials over the complex field $\mathbf{C}$ but may not have any linear polynomials over the real field $\mathbf{R}$. Thus, the situation for real inner product spaces (sometimes called Euclidean spaces) is inherently different than the situation for complex inner product spaces (sometimes called unitary spaces). Thus, we treat them separately.

\section*{Real Inner Product Spaces, Symmetric and Orthogonal Operators}
The following theorem (proved in Problem 13.14) holds.

THEOREM 13.11: Let $T$ be a symmetric (self-adjoint) operator on a real finite-dimensional product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$; that is, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.

We give the corresponding statement for matrices.

THEOREM 13.11: (Alternative Form) Let $A$ be a real symmetric matrix. Then there exists an orthogonal matrix $P$ such that $B=P^{-1} A P=P^{T} A P$ is diagonal.

We can choose the columns of the above matrix $P$ to be normalized orthogonal eigenvectors of $A$; then the diagonal entries of $B$ are the corresponding eigenvalues.

On the other hand, an orthogonal operator $T$ need not be symmetric, and so it may not be represented by a diagonal matrix relative to an orthonormal matrix. However, such a matrix $T$ does have a simple canonical representation, as described in the following theorem (proved in Problem 13.16).

THEOREM 13.12: $\quad$ Let $T$ be an orthogonal operator on a real inner product space $V$. Then there exists an orthonormal basis of $V$ in which $T$ is represented by a block diagonal matrix $M$ of the form

$$
M=\operatorname{diag}\left(I_{s}, \quad-I_{t}, \quad\left[\begin{array}{rr}
\cos \theta_{1} & -\sin \theta_{1} \\
\sin \theta_{1} & \cos \theta_{1}
\end{array}\right], \ldots,\left[\begin{array}{rr}
\cos \theta_{r} & -\sin \theta_{r} \\
\sin \theta_{r} & \cos \theta_{r}
\end{array}\right]\right)
$$

The reader may recognize that each of the $2 \times 2$ diagonal blocks represents a rotation in the corresponding two-dimensional subspace, and each diagonal entry -1 represents a reflection in the corresponding one-dimensional subspace.

\section*{Complex Inner Product Spaces, Normal and Triangular Operators}
A linear operator $T$ is said to be normal if it commutes with its adjoint- that is, if $T T^{*}=T^{*} T$. We note that normal operators include both self-adjoint and unitary operators.

Analogously, a complex matrix $A$ is said to be normal if it commutes with its conjugate transposethat is, if $A A^{*}=A^{*} A$.

EXAMPLE 13.3 Let $A=\left[\begin{array}{cc}1 & 1 \\ i & 3+2 i\end{array}\right]$. Then $A^{*}=\left[\begin{array}{cc}1 & -i \\ 1 & 3-2 i\end{array}\right]$.

Also $A A^{*}=\left[\begin{array}{cc}2 & 3-3 i \\ 3+3 i & 14\end{array}\right]=A^{*} A$. Thus, $A$ is normal.

The following theorem (proved in Problem 13.19) holds.

THEOREM 13.13: $\quad$ Let $T$ be a normal operator on a complex finite-dimensional inner product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$; that is, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.

We give the corresponding statement for matrices.

THEOREM 13.13: (Alternative Form) Let $A$ be a normal matrix. Then there exists a unitary matrix $P$ such that $B=P^{-1} A P=P^{*} A P$ is diagonal.

The following theorem (proved in Problem 13.20) shows that even nonnormal operators on unitary spaces have a relatively simple form.

THEOREM 13.14: $\quad$ Let $T$ be an arbitrary operator on a complex finite-dimensional inner product space $V$. Then $T$ can be represented by a triangular matrix relative to an orthonormal basis of $V$.

THEOREM 13.14: (Alternative Form) Let $A$ be an arbitrary complex matrix. Then there exists a unitary matrix $P$ such that $B=P^{-1} A P=P^{*} A P$ is triangular.

\subsection*{13.10 Spectral Theorem}
The Spectral Theorem is a reformulation of the diagonalization Theorems 13.11 and 13.13.

THEOREM 13.15: (Spectral Theorem) Let $T$ be a normal (symmetric) operator on a complex (real) finite-dimensional inner product space $V$. Then there exists linear operators $E_{1}, \ldots, E_{r}$ on $V$ and scalars $\lambda_{1}, \ldots, \lambda_{r}$ such that\\
(i) $T=\lambda_{1} E_{1}+\lambda_{2} E_{2}+\cdots+\lambda_{r} E_{r}$\\
(iii) $E_{1}^{2}=E_{1}, E_{2}^{2}=E_{2}, \ldots, E_{r}^{2}=E_{r}$,\\
(ii) $E_{1}+E_{2}+\cdots+E_{r}=I$,\\
(iv) $E_{i} E_{j}=0$ for $i \neq j$.

The above linear operators $E_{1}, \ldots, E_{r}$ are projections in the sense that $E_{i}^{2}=E_{i}$. Moreover, they are said to be orthogonal projections because they have the additional property that $E_{i} E_{j}=0$ for $i \neq j$.

The following example shows the relationship between a diagonal matrix representation and the corresponding orthogonal projections.

EXAMPLE 13.4 Consider the following diagonal matrices $A, E_{1}, E_{2}, E_{3}$ :

$$
A=\left[\begin{array}{llll}
2 & & & \\
& 3 & & \\
& & 3 & \\
& & & 5
\end{array}\right], \quad E_{1}=\left[\begin{array}{llll}
1 & & & \\
& 0 & & \\
& & 0 & \\
& & & 0
\end{array}\right], \quad E_{2}=\left[\begin{array}{llll}
0 & & & \\
& 1 & & \\
& & 1 & \\
& & & 0
\end{array}\right], \quad E_{3}=\left[\begin{array}{llll}
0 & & & \\
& 0 & & \\
& & 0 & \\
& & & 1
\end{array}\right]
$$

The reader can verify that

(i) $A=2 E_{1}+3 E_{2}+5 E_{3}$, (ii) $E_{1}+E_{2}+E_{3}=I$, (iii) $E_{i}^{2}=E_{i}$, (iv) $E_{i} E_{j}=0$ for $i \neq j$.

\section*{SOLVED PROBLEMS}
\section*{Adjoints}
13.1. Find the adjoint of $F: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ defined by

$$
F(x, y, z)=(3 x+4 y-5 z, \quad 2 x-6 y+7 z, \quad 5 x-9 y+z)
$$

First find the matrix $A$ that represents $F$ in the usual basis of $\mathbf{R}^{3}$ - that is, the matrix $A$ whose rows are the coefficients of $x, y, z$ - and then form the transpose $A^{T}$ of $A$. This yields

$$
A=\left[\begin{array}{rrr}
3 & 4 & -5 \\
2 & -6 & 7 \\
5 & -9 & 1
\end{array}\right] \quad \text { and then } \quad A^{T}=\left[\begin{array}{rrr}
3 & 2 & 5 \\
4 & -6 & -9 \\
-5 & 7 & 1
\end{array}\right]
$$

The adjoint $F^{*}$ is represented by the transpose of $A$; hence,

$$
F^{*}(x, y, z)=(3 x+2 y+5 z, \quad 4 x-6 y-9 z, \quad-5 x+7 y+z)
$$

13.2. Find the adjoint of $G: \mathbf{C}^{3} \rightarrow \mathbf{C}^{3}$ defined by

$$
G(x, y, z)=[2 x+(1-i) y, \quad(3+2 i) x-4 i z, \quad 2 i x+(4-3 i) y-3 z]
$$

First find the matrix $B$ that represents $G$ in the usual basis of $\mathbf{C}^{3}$, and then form the conjugate transpose $B^{*}$ of $B$. This yields

$$
B=\left[\begin{array}{ccc}
2 & 1-i & 0 \\
3+2 i & 0 & -4 i \\
2 i & 4-3 i & -3
\end{array}\right] \quad \text { and then } \quad B^{*}=\left[\begin{array}{ccc}
2 & 3-2 i & -2 i \\
1+i & 0 & 4+3 i \\
0 & 4 i & -3
\end{array}\right]
$$

Then $G^{*}(x, y, z)=[2 x+(3-2 i) y-2 i z, \quad(1+i) x+(4+3 i) z, 4 i y-3 z]$.

13.3. Prove Theorem 13.3: Let $\phi$ be a linear functional on an $n$-dimensional inner product space $V$. Then there exists a unique vector $u \in V$ such that $\phi(v)=\langle v, u\rangle$ for every $v \in V$.

Let $\left\{w_{1}, \ldots, w_{n}\right\}$ be an orthonormal basis of $V$. Set

$$
u=\overline{\phi\left(w_{1}\right)} w_{1}+\overline{\phi\left(w_{2}\right)} w_{2}+\cdots+\overline{\phi\left(w_{n}\right)} w_{n}
$$

Let $\hat{u}$ be the linear functional on $V$ defined by $\hat{u}(v)=\langle v, u\rangle$ for every $v \in V$. Then, for $i=1, \ldots, n$,

$$
\hat{u}\left(w_{i}\right)=\left\langle w_{i}, u\right\rangle=\left\langle w_{i}, \overline{\phi\left(w_{1}\right)} w_{1}+\cdots+\overline{\phi\left(w_{n}\right)} w_{n}\right\rangle=\phi\left(w_{i}\right)
$$

Because $\hat{u}$ and $\phi$ agree on each basis vector, $\hat{u}=\phi$.

Now suppose $u^{\prime}$ is another vector in $V$ for which $\phi(v)=\left\langle v, u^{\prime}\right\rangle$ for every $v \in V$. Then $\langle v, u\rangle=\left\langle v, u^{\prime}\right\rangle$ or $\left\langle v, u-u^{\prime}\right\rangle=0$. In particular, this is true for $v=u-u^{\prime}$, and so $\left\langle u-u^{\prime}, u-u^{\prime}\right\rangle=0$. This yields $u-u^{\prime}=0$ and $u=u^{\prime}$. Thus, such a vector $u$ is unique, as claimed.

13.4. Prove Theorem 13.1: Let $T$ be a linear operator on an $n$-dimensional inner product space $V$. Then

(a) There exists a unique linear operator $T^{*}$ on $V$ such that

$$
\langle T(u), v\rangle=\left\langle u, T^{*}(v)\right\rangle \quad \text { for all } u, v \in V .
$$

(b) Let $A$ be the matrix that represents $T$ relative to an orthonormal basis $S=\left\{u_{i}\right\}$. Then the conjugate transpose $A^{*}$ of $A$ represents $T^{*}$ in the basis $S$.

(a) We first define the mapping $T^{*}$. Let $v$ be an arbitrary but fixed element of $V$. The map $u \mapsto\langle T(u), v\rangle$ is a linear functional on $V$. Hence, by Theorem 13.3, there exists a unique element $v^{\prime} \in V$ such that $\langle T(u), v\rangle=\left\langle u, v^{\prime}\right\rangle$ for every $u \in V$. We define $T^{*}: V \rightarrow V$ by $T^{*}(v)=v^{\prime}$. Then $\langle T(u), v\rangle=\left\langle u, T^{*}(v)\right\rangle$ for every $u, v \in V$.

We next show that $T^{*}$ is linear. For any $u, v_{i} \in V$, and any $a, b \in K$,

$$
\begin{aligned}
\left\langle u, \quad T^{*}\left(a v_{1}+b v_{2}\right)\right\rangle & =\left\langle T(u), \quad a v_{1}+b v_{2}\right\rangle=\bar{a}\left\langle T(u), v_{1}\right\rangle+\bar{b}\left\langle T(u), v_{2}\right\rangle \\
& =\bar{a}\left\langle u, T^{*}\left(v_{1}\right)\right\rangle+\bar{b}\left\langle u, T^{*}\left(v_{2}\right)\right\rangle=\left\langle u, a T^{*}\left(v_{1}\right)+b T^{*}\left(v_{2}\right)\right\rangle
\end{aligned}
$$

But this is true for every $u \in V$; hence, $T^{*}\left(a v_{1}+b v_{2}\right)=a T^{*}\left(v_{1}\right)+b T^{*}\left(v_{2}\right)$. Thus, $T^{*}$ is linear.

(b) The matrices $A=\left[a_{i j}\right]$ and $B=\left[b_{i j}\right]$ that represent $T$ and $T^{*}$, respectively, relative to the orthonormal basis $S$ are given by $a_{i j}=\left\langle T\left(u_{j}\right), u_{i}\right\rangle$ and $b_{i j}=\left\langle T^{*}\left(u_{j}\right), u_{i}\right\rangle$ (Problem 13.67). Hence,

$$
b_{i j}=\left\langle T^{*}\left(u_{j}\right), u_{i}\right\rangle=\overline{\left\langle u_{i}, T^{*}\left(u_{j}\right)\right\rangle}=\overline{\left\langle T\left(u_{i}\right), u_{j}\right\rangle}=\overline{a_{j i}}
$$

Thus, $B=A^{*}$, as claimed.

13.5. Prove Theorem 13.2:

(i) $\left(T_{1}+T_{2}\right)^{*}=T_{1}^{*}+T_{2}^{*}$,

(iii) $\left(T_{1} T_{2}\right)^{*}=T_{2}^{*} T_{1}^{*}$,

(ii) $(k T)^{*}=\bar{k} T^{*}$,

(iv) $\left(T^{*}\right)^{*}=T$.

(i) For any $u, v \in V$,

$$
\begin{aligned}
\left\langle\left(T_{1}+T_{2}\right)(u), v\right\rangle & =\left\langle T_{1}(u)+T_{2}(u), \quad v\right\rangle=\left\langle T_{1}(u), v\right\rangle+\left\langle T_{2}(u), v\right\rangle \\
& =\left\langle u, T_{1}^{*}(v)\right\rangle+\left\langle u, T_{2}^{*}(v)\right\rangle=\left\langle u, \quad T_{1}^{*}(v)+T_{2}^{*}(v)\right\rangle \\
& =\left\langle u, \quad\left(T_{1}^{*}+T_{2}^{*}\right)(v)\right\rangle
\end{aligned}
$$

The uniqueness of the adjoint implies $\left(T_{1}+T_{2}\right)^{*}=T_{1}^{*}+T_{2}^{*}$.

(ii) For any $u, v \in V$,

$$
\langle(k T)(u), \quad v\rangle=\langle k T(u), v\rangle=k\langle T(u), v\rangle=k\left\langle u, T^{*}(v)\right\rangle=\left\langle u, \bar{k} T^{*}(v)\right\rangle=\left\langle u,\left(\bar{k} T^{*}\right)(v)\right\rangle
$$

The uniqueness of the adjoint implies $(k T)^{*}=\bar{k} T^{*}$.

(iii) For any $u, v \in V$,

$$
\begin{aligned}
\left\langle\left(T_{1} T_{2}\right)(u), v\right\rangle & =\left\langle T_{1}\left(T_{2}(u)\right), v\right\rangle=\left\langle T_{2}(u), T_{1}^{*}(v)\right\rangle \\
& =\left\langle u, T_{2}^{*}\left(T_{1}^{*}(v)\right)\right\rangle=\left\langle u,\left(T_{2}^{*} T_{1}^{*}\right)(v)\right\rangle
\end{aligned}
$$

The uniqueness of the adjoint implies $\left(T_{1} T_{2}\right)^{*}=T_{2}^{*} T_{1}^{*}$.

(iv) For any $u, v \in V$,

$$
\left\langle T^{*}(u), v\right\rangle=\overline{\left\langle v, T^{*}(u)\right\rangle}=\overline{\langle T(v), u\rangle}=\langle u, T(v)\rangle
$$

The uniqueness of the adjoint implies $\left(T^{*}\right)^{*}=T$.

13.6. Show that (a) $I^{*}=I$, and (b) $\mathbf{0}^{*}=\mathbf{0}$.

(a) For every $u, v \in V,\langle I(u), v\rangle=\langle u, v\rangle=\langle u, I(v)\rangle$; hence, $I^{*}=I$.

(b) For every $u, v \in V,\langle\mathbf{0}(u), v\rangle=\langle 0, v\rangle=0=\langle u, 0\rangle=\langle u, \mathbf{0}(v)\rangle$; hence, $\mathbf{0}^{*}=\mathbf{0}$.

13.7. Suppose $T$ is invertible. Show that $\left(T^{-1}\right)^{*}=\left(T^{*}\right)^{-1}$.

$$
I=I^{*}=\left(T T^{-1}\right)^{*}=\left(T^{-1}\right)^{*} T^{*} \text {; hence, }\left(T^{-1}\right)^{*}=\left(T^{*}\right)^{-1} \text {. }
$$

13.8. Let $T$ be a linear operator on $V$, and let $W$ be a $T$-invariant subspace of $V$. Show that $W^{\perp}$ is invariant under $T^{*}$.

Let $u \in W^{\perp}$. If $w \in W$, then $T(w) \in W$ and so $\left\langle w, T^{*}(u)\right\rangle=\langle T(w), u\rangle=0$. Thus, $T^{*}(u) \in W^{\perp}$ because it is orthogonal to every $w \in W$. Hence, $W^{\perp}$ is invariant under $T^{*}$.

13.9. Let $T$ be a linear operator on $V$. Show that each of the following conditions implies $T=\mathbf{0}$ :

(i) $\langle T(u), v\rangle=0$ for every $u, v \in V$.

(ii) $\quad V$ is a complex space, and $\langle T(u), u\rangle=0$ for every $u \in V$.

(iii) $T$ is self-adjoint and $\langle T(u), u\rangle=0$ for every $u \in V$.

Give an example of an operator $T$ on a real space $V$ for which $\langle T(u), u\rangle=0$ for every $u \in V$ but $T \neq \mathbf{0}$. [Thus, (ii) need not hold for a real space $V$.]

(i) Set $v=T(u)$. Then $\langle T(u), T(u)\rangle=0$, and hence, $T(u)=0$, for every $u \in V$. Accordingly, $T=\mathbf{0}$.

(ii) By hypothesis, $\langle T(v+w), v+w\rangle=0$ for any $v, w \in V$. Expanding and setting $\langle T(v), v\rangle=0$ and $\langle T(w), w\rangle=0$, we find


\begin{equation*}
\langle T(v), w\rangle+\langle T(w), v\rangle=0 \tag{1}
\end{equation*}


Note $w$ is arbitrary in (1). Substituting $i w$ for $w$, and using $\langle T(v), i w\rangle=\bar{i}\langle T(v), w\rangle=-i\langle T(v), w\rangle$ and $\langle T(i w), v\rangle=\langle i T(w), v\rangle=i\langle T(w), v\rangle$, we find

$$
-i\langle T(v), w\rangle+i\langle T(w), v\rangle=0
$$

Dividing through by $i$ and adding to (1), we obtain $\langle T(w), v\rangle=0$ for any $v, w, \in V$. By (i), $T=\mathbf{0}$.

(iii) By (ii), the result holds for the complex case; hence we need only consider the real case. Expanding $\langle T(v+w), v+w\rangle=0$, we again obtain (1). Because $T$ is self-adjoint and as it is a real space, we have $\langle T(w), v\rangle=\langle w, T(v)\rangle=\langle T(v), w\rangle$. Substituting this into (1), we obtain $\langle T(v), w\rangle=0$ for any $v, w \in V$. By (i), $T=\mathbf{0}$.

For an example, consider the linear operator $T$ on $\mathbf{R}^{2}$ defined by $T(x, y)=(y,-x)$. Then $\langle T(u), u\rangle=0$ for every $u \in V$, but $T \neq \mathbf{0}$.

\section*{Orthogonal and Unitary Operators and Matrices}
13.10. Prove Theorem 13.6: The following conditions on an operator $U$ are equivalent:

(i) $U^{*}=U^{-1}$; that is, $U$ is unitary. (ii) $\langle U(v), U(w)\rangle=\langle u, w\rangle$. (iii) $\|U(v)\|=\|v\|$. Suppose (i) holds. Then, for every $v, w, \in V$,

$$
\langle U(v), U(w)\rangle=\left\langle v, U^{*} U(w)\right\rangle=\langle v, I(w)\rangle=\langle v, w\rangle
$$

Thus, (i) implies (ii). Now if (ii) holds, then

$$
\|U(v)\|=\sqrt{\langle U(v), U(v)\rangle}=\sqrt{\langle v, v\rangle}=\|v\|
$$

Hence, (ii) implies (iii). It remains to show that (iii) implies (i).

Suppose (iii) holds. Then for every $v \in V$,

$$
\left\langle U^{*} U(v)\right\rangle=\langle U(v), U(v)\rangle=\langle v, v\rangle=\langle I(v), v\rangle
$$

Hence, $\left\langle\left(U^{*} U-I\right)(v), \quad v\right\rangle=0$ for every $v \in V$. But $U^{*} U-I$ is self-adjoint (Prove!); then, by Problem 13.9, we have $U^{*} U-I=0$ and so $U^{*} U=I$. Thus, $U^{*}=U^{-1}$, as claimed.

13.11. Let $U$ be a unitary (orthogonal) operator on $V$, and let $W$ be a subspace invariant under $U$. Show that $W^{\perp}$ is also invariant under $U$.

Because $U$ is nonsingular, $U(W)=W$; that is, for any $w \in W$, there exists $w^{\prime} \in W$ such that $U\left(w^{\prime}\right)=w$. Now let $v \in W^{\perp}$. Then, for any $w \in W$,

$$
\langle U(v), w\rangle=\left\langle U(v), U\left(w^{\prime}\right)\right\rangle=\left\langle v, w^{\prime}\right\rangle=0
$$

Thus, $U(v)$ belongs to $W^{\perp}$. Therefore, $W^{\perp}$ is invariant under $U$.

13.12. Prove Theorem 13.9: The change-of-basis matrix from an orthonormal basis $\left\{u_{1}, \ldots, u_{n}\right\}$ into another orthonormal basis is unitary (orthogonal). Conversely, if $P=\left[a_{i j}\right]$ is a unitary (orthogonal) matrix, then the vectors $u_{i^{\prime}}=\sum_{j} a_{j i} u_{j}$ form an orthonormal basis.

Suppose $\left\{v_{i}\right\}$ is another orthonormal basis and suppose


\begin{equation*}
v_{i}=b_{i 1} u_{1}+b_{i 2} u_{2}+\cdots+b_{i n} u_{n}, \quad i=1, \ldots, n \tag{1}
\end{equation*}


Because $\left\{v_{i}\right\}$ is orthonormal,


\begin{equation*}
\delta_{i j}=\left\langle v_{i}, v_{j}\right\rangle=b_{i 1} \overline{b_{j 1}}+b_{i 2} \overline{b_{j 2}}+\cdots+b_{i n} \overline{b_{j n}} \tag{2}
\end{equation*}


Let $B=\left[b_{i j}\right]$ be the matrix of coefficients in (1). (Then $B^{T}$ is the change-of-basis matrix from $\left\{u_{i}\right\}$ to $\left\{v_{i}\right\}$.) Then $B B^{*}=\left[c_{i j}\right]$, where $c_{i j}=b_{i 1} \overline{b_{j 1}}+b_{i 2} \overline{b_{j 2}}+\cdots+b_{i n} \overline{b_{j n}}$. By (2), $c_{i j}=\delta_{i j}$, and therefore $B B^{*}=I$. Accordingly, $B$, and hence, $B^{T}$, is unitary.

It remains to prove that $\left\{u_{i}^{\prime}\right\}$ is orthonormal. By Problem 13.67,

$$
\left\langle u_{i}^{\prime}, u_{j}^{\prime}\right\rangle=a_{1 i} \overline{a_{1 j}}+a_{2 i} \overline{a_{2 j}}+\cdots+a_{n i} \overline{a_{n j}}=\left\langle C_{i}, C_{j}\right\rangle
$$

where $C_{i}$ denotes the $i$ th column of the unitary (orthogonal) matrix $P=\left[a_{i j}\right]$. Because $P$ is unitary (orthogonal), its columns are orthonormal; hence, $\left\langle u_{i}^{\prime}, u_{j}^{\prime}\right\rangle=\left\langle C_{i}, C_{j}\right\rangle=\delta_{i j}$. Thus, $\left\{u_{i}^{\prime}\right\}$ is an orthonormal basis.

\section*{Symmetric Operators and Canonical Forms in Euclidean Spaces}
13.13. Let $T$ be a symmetric operator. Show that (a) The characteristic polynomial $\Delta(t)$ of $T$ is a product of linear polynomials (over $\mathbf{R}$ ); (b) $T$ has a nonzero eigenvector.

(a) Let $A$ be a matrix representing $T$ relative to an orthonormal basis of $V$; then $A=A^{T}$. Let $\Delta(t)$ be the characteristic polynomial of $A$. Viewing $A$ as a complex self-adjoint operator, $A$ has only real eigenvalues by Theorem 13.4. Thus,

$$
\Delta(t)=\left(t-\lambda_{1}\right)\left(t-\lambda_{2}\right) \cdots\left(t-\lambda_{n}\right)
$$

where the $\lambda_{i}$ are all real. In other words, $\Delta(t)$ is a product of linear polynomials over $\mathbf{R}$.

(b) By (a), $T$ has at least one (real) eigenvalue. Hence, $T$ has a nonzero eigenvector.

13.14. Prove Theorem 13.11: Let $T$ be a symmetric operator on a real $n$-dimensional inner product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$. (Hence, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.)

The proof is by induction on the dimension of $V$. If $\operatorname{dim} V=1$, the theorem trivially holds. Now suppose $\operatorname{dim} V=n>1$. By Problem 13.13, there exists a nonzero eigenvector $v_{1}$ of $T$. Let $W$ be the space spanned by $v_{1}$, and let $u_{1}$ be a unit vector in $W$, e.g., let $u_{1}=v_{1} /\left\|v_{1}\right\|$.

Because $v_{1}$ is an eigenvector of $T$, the subspace $W$ of $V$ is invariant under $T$. By Problem 13.8, $W^{\perp}$ is invariant under $T^{*}=T$. Thus, the restriction $\hat{T}$ of $T$ to $W^{\perp}$ is a symmetric operator. By Theorem 7.4, $V=W \oplus W^{\perp}$. Hence, $\operatorname{dim} W^{\perp}=n-1$, because $\operatorname{dim} W=1$. By induction, there exists an orthonormal basis $\left\{u_{2}, \ldots, u_{n}\right\}$ of $W^{\perp}$ consisting of eigenvectors of $\hat{T}$ and hence of $T$. But $\left\langle u_{1}, u_{i}\right\rangle=0$ for $i=2, \ldots, n$ because $u_{i} \in W^{\perp}$. Accordingly $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is an orthonormal set and consists of eigenvectors of $T$. Thus, the theorem is proved.

13.15. Let $q(x, y)=3 x^{2}-6 x y+11 y^{2}$. Find an orthonormal change of coordinates (linear substitution) that diagonalizes the quadratic form $q$.

Find the symmetric matrix $A$ representing $q$ and its characteristic polynomial $\Delta(t)$. We have

$$
A=\left[\begin{array}{rr}
3 & -3 \\
-3 & 11
\end{array}\right] \quad \text { and } \quad \Delta(t)=t^{2}-\operatorname{tr}(A) t+|A|=t^{2}-14 t+24=(t-2)(t-12)
$$

The eigenvalues are $\lambda=2$ and $\lambda=12$. Hence, a diagonal form of $q$ is

$$
q(s, t)=2 s^{2}+12 t^{2}
$$

(where we use $s$ and $t$ as new variables). The corresponding orthogonal change of coordinates is obtained by finding an orthogonal set of eigenvectors of $A$.

Subtract $\lambda=2$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{rr}
1 & -3 \\
-3 & 9
\end{array}\right] \quad \text { corresponding to } \quad \begin{array}{r}
x-3 y=0 \\
-3 x+9 y=0
\end{array} \quad \text { or } \quad x-3 y=0
$$

A nonzero solution is $u_{1}=(3,1)$. Next subtract $\lambda=12$ down the diagonal of $A$ to obtain the matrix

$$
M=\left[\begin{array}{ll}
-9 & -3 \\
-3 & -1
\end{array}\right] \quad \text { corresponding to } \quad \begin{array}{r}
-9 x-3 y=0 \\
-3 x-y=0
\end{array} \quad \text { or } \quad-3 x-y=0
$$

A nonzero solution is $u_{2}=(-1,3)$. Normalize $u_{1}$ and $u_{2}$ to obtain the orthonormal basis

$$
\hat{u}_{1}=(3 / \sqrt{10}, 1 / \sqrt{10}), \quad \hat{u}_{2}=(-1 / \sqrt{10}, 3 / \sqrt{10})
$$

Now let $P$ be the matrix whose columns are $\hat{u}_{1}$ and $\hat{u}_{2}$. Then

$$
P=\left[\begin{array}{rr}
3 / \sqrt{10} & -1 / \sqrt{10} \\
1 / \sqrt{10} & 3 / \sqrt{10}
\end{array}\right] \quad \text { and } \quad D=P^{-1} A P=P^{T} A P=\left[\begin{array}{rr}
2 & 0 \\
0 & 12
\end{array}\right]
$$

Thus, the required orthogonal change of coordinates is

$$
\left[\begin{array}{l}
x \\
y
\end{array}\right]=P\left[\begin{array}{l}
s \\
t
\end{array}\right] \quad \text { or } \quad x=\frac{3 s-t}{\sqrt{10}}, \quad y=\frac{s+3 t}{\sqrt{10}}
$$

One can also express $s$ and $t$ in terms of $x$ and $y$ by using $P^{-1}=P^{T}$; that is,

$$
s=\frac{3 x+y}{\sqrt{10}}, \quad t=\frac{-x+3 y}{\sqrt{10}}
$$

13.16. Prove Theorem 13.12: Let $T$ be an orthogonal operator on a real inner product space $V$. Then there exists an orthonormal basis of $V$ in which $T$ is represented by a block diagonal matrix $M$ of the form

$$
M=\operatorname{diag}\left(1, \ldots, 1,-1, \ldots,-1,\left[\begin{array}{rr}
\cos \theta_{1} & -\sin \theta_{1} \\
\sin \theta_{1} & \cos \theta_{1}
\end{array}\right], \ldots,\left[\begin{array}{rr}
\cos \theta_{r} & -\sin \theta_{r} \\
\sin \theta_{r} & \cos \theta_{r}
\end{array}\right]\right)
$$

Let $S=T+T^{-1}=T+T^{*}$. Then $S^{*}=\left(T+T^{*}\right)^{*}=T^{*}+T=S$. Thus, $S$ is a symmetric operator on $V$. By Theorem 13.11, there exists an orthonormal basis of $V$ consisting of eigenvectors of $S$. If $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $S$, then $V$ can be decomposed into the direct sum $V=V_{1} \oplus V_{2} \oplus \cdots \oplus V_{m}$ where the $V_{i}$ consists of the eigenvectors of $S$ belonging to $\lambda_{i}$. We claim that each $V_{i}$ is invariant under $T$. For suppose $v \in V$; then $S(v)=\lambda_{i} v$ and

$$
S(T(v))=\left(T+T^{-1}\right) T(v)=T\left(T+T^{-1}\right)(v)=T S(v)=T\left(\lambda_{i} v\right)=\lambda_{i} T(v)
$$

That is, $T(v) \in V_{i}$. Hence, $V_{i}$ is invariant under $T$. Because the $V_{i}$ are orthogonal to each other, we can restrict our investigation to the way that $T$ acts on each individual $V_{i}$.

On a given $V_{i}$, we have $\left(T+T^{-1}\right) v=S(v)=\lambda_{i} v$. Multiplying by $T$, we get


\begin{equation*}
\left(T^{2}-\lambda_{i} T+I\right)(v)=0 \tag{1}
\end{equation*}


We consider the cases $\lambda_{i}= \pm 2$ and $\lambda_{i} \neq \pm 2$ separately. If $\lambda_{i}= \pm 2$, then $(T \pm I)^{2}(v)=0$, which leads to $(T \pm I)(v)=0$ or $T(v)= \pm v$. Thus, $T$ restricted to this $V_{i}$ is either $I$ or $-I$.

If $\lambda_{i} \neq \pm 2$, then $T$ has no eigenvectors in $V_{i}$, because, by Theorem 13.4, the only eigenvalues of $T$ are 1 or -1 . Accordingly, for $v \neq 0$, the vectors $v$ and $T(v)$ are linearly independent. Let $W$ be the subspace spanned by $v$ and $T(v)$. Then $W$ is invariant under $T$, because using (1) we get

$$
T(T(v))=T^{2}(v)=\lambda_{i} T(v)-v \in W
$$

By Theorem 7.4, $V_{i}=W \oplus W^{\perp}$. Furthermore, by Problem 13.8, $W^{\perp}$ is also invariant under $T$. Thus, we can decompose $V_{i}$ into the direct sum of two-dimensional subspaces $W_{j}$ where the $W_{j}$ are orthogonal to each other and each $W_{j}$ is invariant under $T$. Thus, we can restrict our investigation to the way in which $T$ acts on each individual $W_{j}$.

Because $T^{2}-\lambda_{i} T+I=0$, the characteristic polynomial $\Delta(t)$ of $T$ acting on $W_{j}$ is $\Delta(t)=t^{2}-\lambda_{i} t+1$. Thus, the determinant of $T$ is 1 , the constant term in $\Delta(t)$. By Theorem 2.7, the matrix $A$ representing $T$ acting on $W_{j}$ relative to any orthogonal basis of $W_{j}$ must be of the form

$$
\left[\begin{array}{rr}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right]
$$

The union of the bases of the $W_{j}$ gives an orthonormal basis of $V_{i}$, and the union of the bases of the $V_{i}$ gives an orthonormal basis of $V$ in which the matrix representing $T$ is of the desired form.

\section*{Normal Operators and Canonical Forms in Unitary Spaces}
13.17. Determine which of the following matrices is normal:

(a) $A=\left[\begin{array}{ll}1 & i \\ 0 & 1\end{array}\right]$, (b) $B=\left[\begin{array}{cc}1 & i \\ 1 & 2+i\end{array}\right]$

(a) $A A^{*}=\left[\begin{array}{ll}1 & i \\ 0 & 1\end{array}\right]\left[\begin{array}{rr}1 & 0 \\ -i & 1\end{array}\right]=\left[\begin{array}{rr}2 & i \\ -i & 1\end{array}\right], \quad A^{*} A=\left[\begin{array}{rr}1 & 0 \\ -i & 1\end{array}\right]\left[\begin{array}{ll}1 & i \\ 0 & 1\end{array}\right]=\left[\begin{array}{rr}1 & i \\ -i & 2\end{array}\right]$

Because $A A^{*} \neq A^{*} A$, the matrix $A$ is not normal.

(b) $B B^{*}\left[\begin{array}{cc}1 & i \\ 1 & 2+i\end{array}\right]\left[\begin{array}{cc}1 & 1 \\ -i & 2-i\end{array}\right]=\left[\begin{array}{cc}2 & 2+2 i \\ 2-2 i & 6\end{array}\right]=\left[\begin{array}{cc}1 & 1 \\ -i & 2-i\end{array}\right]\left[\begin{array}{cc}1 & i \\ 1 & 2+i\end{array}\right]=B^{*} B$

Because $B B^{*}=B^{*} B$, the matrix $B$ is normal.

13.18. Let $T$ be a normal operator. Prove the following:

(a) $T(v)=0$ if and only if $T^{*}(v)=0$. (b) $T-\lambda I$ is normal.

(c) If $T(v)=\lambda v$, then $T^{*}(v)=\bar{\lambda} v$; hence, any eigenvector of $T$ is also an eigenvector of $T^{*}$.

(d) If $T(v)=\lambda_{1} v$ and $T(w)=\lambda_{2} w$ where $\lambda_{1} \neq \lambda_{2}$, then $\langle v, w\rangle=0$; that is, eigenvectors of $T$ belonging to distinct eigenvalues are orthogonal.

(a) We show that $\langle T(v), T(v)\rangle=\left\langle T^{*}(v), T^{*}(v)\right\rangle$ :

$$
\langle T(v), T(v)\rangle=\left\langle v, T^{*} T(v)\right\rangle=\left\langle v, T T^{*}(v)\right\rangle=\left\langle T^{*}(v), T^{*}(v)\right\rangle
$$

Hence, by $\left[I_{3}\right]$ in the definition of the inner product in Section 7.2, $T(v)=0$ if and only if $T^{*}(v)=0$.

(b) We show that $T-\lambda I$ commutes with its adjoint:

$$
\begin{aligned}
(T-\lambda I)(T-\lambda I)^{*} & =(T-\lambda I)\left(T^{*}-\bar{\lambda} I\right)=T T^{*}-\lambda T^{*}-\bar{\lambda} T+\lambda \bar{\lambda} I \\
& =T^{*} T-\bar{\lambda} T-\lambda T^{*}+\bar{\lambda} \lambda I=\left(T^{*}-\bar{\lambda} I\right)(T-\lambda I) \\
& =(T-\lambda I)^{*}(T-\lambda I)
\end{aligned}
$$

Thus, $T-\lambda I$ is normal.\\
(c) If $T(v)=\lambda v$, then $(T-\lambda I)(v)=0$. Now $T-\lambda I$ is normal by (b); therefore, by (a), $(T-\lambda I)^{*}(v)=0$. That is, $\left(T^{*}-\lambda I\right)(v)=0$; hence, $T^{*}(v)=\bar{\lambda} v$.

(d) We show that $\lambda_{1}\langle v, w\rangle=\lambda_{2}\langle v, w\rangle$ :

$$
\lambda_{1}\langle v, w\rangle=\left\langle\lambda_{1} v, w\right\rangle=\langle T(v), w\rangle=\left\langle v, T^{*}(w)\right\rangle=\left\langle v, \bar{\lambda}_{2} w\right\rangle=\lambda_{2}\langle v, w\rangle
$$

But $\lambda_{1} \neq \lambda_{2}$; hence, $\langle v, w\rangle=0$.

13.19. Prove Theorem 13.13: Let $T$ be a normal operator on a complex finite-dimensional inner product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$. (Thus, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.)

The proof is by induction on the dimension of $V$. If $\operatorname{dim} V=1$, then the theorem trivially holds. Now suppose $\operatorname{dim} V=n>1$. Because $V$ is a complex vector space, $T$ has at least one eigenvalue and hence a nonzero eigenvector $v$. Let $W$ be the subspace of $V$ spanned by $v$, and let $u_{1}$ be a unit vector in $W$.

Because $v$ is an eigenvector of $T$, the subspace $W$ is invariant under $T$. However, $v$ is also an eigenvector of $T^{*}$ by Problem 13.18; hence, $W$ is also invariant under $T^{*}$. By Problem 13.8, $W^{\perp}$ is invariant under $T^{* *}=T$. The remainder of the proof is identical with the latter part of the proof of Theorem 13.11 (Problem 13.14).

13.20. Prove Theorem 13.14: Let $T$ be any operator on a complex finite-dimensional inner product space $V$. Then $T$ can be represented by a triangular matrix relative to an orthonormal basis of $V$.

The proof is by induction on the dimension of $V$. If $\operatorname{dim} V=1$, then the theorem trivially holds. Now suppose $\operatorname{dim} V=n>1$. Because $V$ is a complex vector space, $T$ has at least one eigenvalue and hence at least one nonzero eigenvector $v$. Let $W$ be the subspace of $V$ spanned by $v$, and let $u_{1}$ be a unit vector in $W$. Then $u_{1}$ is an eigenvector of $T$ and, say, $T\left(u_{1}\right)=a_{11} u_{1}$.

By Theorem 7.4, $V=W \oplus W^{\perp}$. Let $E$ denote the orthogonal projection $V$ into $W^{\perp}$. Clearly $W^{\perp}$ is invariant under the operator $E T$. By induction, there exists an orthonormal basis $\left\{u_{2}, \ldots, u_{n}\right\}$ of $W^{\perp}$ such that, for $i=2, \ldots, n$,

$$
E T\left(u_{i}\right)=a_{i 2} u_{2}+{ }_{i 3} u_{3}+\cdots+a_{i i} u_{i}
$$

(Note that $\left\{u_{1}, u_{2}, \ldots, u_{n}\right\}$ is an orthonormal basis of $V$.) But $E$ is the orthogonal projection of $V$ onto $W^{\perp}$; hence, we must have

$$
T\left(u_{i}\right)=a_{i 1} u_{1}+a_{i 2} u_{2}+\cdots+a_{i i} u_{i}
$$

for $i=2, \ldots, n$. This with $T\left(u_{1}\right)=a_{11} u_{1}$ gives us the desired result.

\section*{Miscellaneous Problems}
13.21. Prove Theorem 13.10B: The following are equivalent:

(i) $P=T^{2}$ for some self-adjoint operator $T$.

(ii) $P=S^{*} S$ for some operator $S$; that is, $P$ is positive.

(iii) $P$ is self-adjoint and $\langle P(u), u\rangle \geq 0$ for every $u \in V$.

Suppose (i) holds; that is, $P=T^{2}$ where $T=T^{*}$. Then $P=T T=T^{*} T$, and so (i) implies (ii). Now suppose (ii) holds. Then $P^{*}=\left(S^{*} S\right)^{*}=S^{*} S^{* *}=S^{*} S=P$, and so $P$ is self-adjoint. Furthermore,

$$
\langle P(u), u\rangle=\left\langle S^{*} S(u), u\right\rangle=\langle S(u), S(u)\rangle \geq 0
$$

Thus, (ii) implies (iii), and so it remains to prove that (iii) implies (i).

Now suppose (iii) holds. Because $P$ is self-adjoint, there exists an orthonormal basis $\left\{u_{1}, \ldots, u_{n}\right\}$ of $V$ consisting of eigenvectors of $P$; say, $P\left(u_{i}\right)=\lambda_{i} u_{i}$. By Theorem 13.4, the $\lambda_{i}$ are real. Using (iii), we show that the $\lambda_{i}$ are nonnegative. We have, for each $i$,

$$
0 \leq\left\langle P\left(u_{i}\right), u_{i}\right\rangle=\left\langle\lambda_{i} u_{i}, u_{i}\right\rangle=\lambda_{i}\left\langle u_{i}, u_{i}\right\rangle
$$

Thus, $\left\langle u_{i}, u_{i}\right\rangle \geq 0$ forces $\lambda_{i} \geq 0$, as claimed. Accordingly, $\sqrt{\lambda_{i}}$ is a real number. Let $T$ be the linear operator defined by

$$
T\left(u_{i}\right)=\sqrt{\lambda_{i}} u_{i} \quad \text { for } i=1, \ldots, n
$$

Because $T$ is represented by a real diagonal matrix relative to the orthonormal basis $\left\{u_{i}\right\}, T$ is self-adjoint. Moreover, for each $i$,

$$
T^{2}\left(u_{i}\right)=T\left(\sqrt{\lambda_{i}} u_{i}\right)=\sqrt{\lambda_{i}} T\left(i_{i}\right)=\sqrt{\lambda_{i}} \sqrt{\lambda_{i}} u_{i}=\lambda_{i} u_{i}=P\left(u_{i}\right)
$$

Because $T^{2}$ and $P$ agree on a basis of $V, P=T^{2}$. Thus, the theorem is proved.

Remark: The above operator $T$ is the unique positive operator such that $P=T^{2}$; it is called the positive square root of $P$.

13.22. Show that any operator $T$ is the sum of a self-adjoint operator and a skew-adjoint operator.

Set $S=\frac{1}{2}\left(T+T^{*}\right)$ and $U=\frac{1}{2}\left(T-T^{*}\right)$. Then $T=S+U$, where

and

$$
\begin{aligned}
S^{*} & =\left[\frac{1}{2}\left(T+T^{*}\right)\right]^{*}=\frac{1}{2}\left(T^{*}+T^{* *}\right)=\frac{1}{2}\left(T^{*}+T\right)=S \\
U^{*} & =\left[\frac{1}{2}\left(T-T^{*}\right)\right]^{*}=\frac{1}{2}\left(T^{*}-T\right)=-\frac{1}{2}\left(T-T^{*}\right)=-U
\end{aligned}
$$

that is, $S$ is self-adjoint and $U$ is skew-adjoint.

13.23. Prove: Let $T$ be an arbitrary linear operator on a finite-dimensional inner product space $V$. Then $T$ is a product of a unitary (orthogonal) operator $U$ and a unique positive operator $P$; that is, $T=U P$. Furthermore, if $T$ is invertible, then $U$ is also uniquely determined.

By Theorem 13.10, $T^{*} T$ is a positive operator; hence, there exists a (unique) positive operator $P$ such that $P^{2}=T^{*} T$ (Problem 13.43). Observe that


\begin{equation*}
\|P(v)\|^{2}=\langle P(v), P(v)\rangle=\left\langle P^{2}(v), v\right\rangle=\left\langle T^{*} T(v), v\right\rangle=\langle T(v), T(v)\rangle=\|T(v)\|^{2} \tag{1}
\end{equation*}


We now consider separately the cases when $T$ is invertible and noninvertible.

If $T$ is invertible, then we set $\hat{U}=P T^{-1}$. We show that $\hat{U}$ is unitary:

$$
\hat{U}^{*}=\left(P T^{-1}\right)^{*}=T^{-1 *} P^{*}=\left(T^{*}\right)^{-1} P \quad \text { and } \quad \hat{U} * \hat{U}=\left(T^{*}\right)^{-1} P P T^{-1}=\left(T^{*}\right)^{-1} T^{*} T T^{-1}=I
$$

Thus, $\hat{U}$ is unitary. We next set $U=\hat{U}^{-1}$. Then $U$ is also unitary, and $T=U P$ as required.

To prove uniqueness, we assume $T=U_{0} P_{0}$, where $U_{0}$ is unitary and $P_{0}$ is positive. Then

$$
T^{*} T=P_{0}^{*} U_{0}^{*} U_{0} P_{0}=P_{0} I P_{0}=P_{0}^{2}
$$

But the positive square root of $T^{*} T$ is unique (Problem 13.43); hence, $P_{0}=P$. (Note that the invertibility of $T$ is not used to prove the uniqueness of $P$.) Now if $T$ is invertible, then $P$ is also invertible by (1). Multiplying $U_{0} P=U P$ on the right by $P^{-1}$ yields $U_{0}=U$. Thus, $U$ is also unique when $T$ is invertible.

Now suppose $T$ is not invertible. Let $W$ be the image of $P$; that is, $W=\operatorname{Im} P$. We define $U_{1}: W \rightarrow V$ by


\begin{equation*}
U_{1}(w)=T(v), \quad \text { where } \quad P(v)=w \tag{2}
\end{equation*}


We must show that $U_{1}$ is well defined; that is, that $P(v)=P\left(v^{\prime}\right)$ implies $T(v)=T\left(v^{\prime}\right)$. This follows from the fact that $P\left(v-v^{\prime}\right)=0$ is equivalent to $\left\|P\left(v-v^{\prime}\right)\right\|=0$, which forces $\left\|T\left(v-v^{\prime}\right)\right\|=0$ by (1). Thus, $U_{1}$ is well defined. We next define $U_{2}: W \rightarrow V$. Note that, by (1), $P$ and $T$ have the same kernels. Hence, the images of $P$ and $T$ have the same dimension; that is, $\operatorname{dim}(\operatorname{Im} P)=\operatorname{dim} W=\operatorname{dim}(\operatorname{Im} T)$. Consequently, $W^{\perp}$ and $(\operatorname{Im} T)^{\perp}$ also have the same dimension. We let $U_{2}$ be any isomorphism between $W^{\perp}$ and $(\operatorname{Im} T)^{\perp}$.

We next set $U=U_{1} \oplus U_{2}$. [Here $U$ is defined as follows: If $v \in V$ and $v=w+w^{\prime}$, where $w \in W$, $w^{\prime} \in W^{\perp}$, then $U(v)=U_{1}(w)+U_{2}\left(w^{\prime}\right)$.] Now $U$ is linear (Problem 13.69), and, if $v \in V$ and $P(v)=w$, then, by (2),

$$
T(v)=U_{1}(w)=U(w)=U P(v)
$$

Thus, $T=U P$, as required.

It remains to show that $U$ is unitary. Now every vector $x \in V$ can be written in the form $x=P(v)+w^{\prime}$, where $w^{\prime} \in W^{\perp}$. Then $U(x)=U P(v)+U_{2}\left(w^{\prime}\right)=T(v)+U_{2}\left(w^{\prime}\right)$, where $\left\langle T(v), U_{2}\left(w^{\prime}\right)\right\rangle=0$ by definition\\
of $U_{2}$. Also, $\langle T(v), T(v)\rangle=\langle P(v), P(v)\rangle$ by (1). Thus,

$$
\begin{aligned}
\langle U(x), U(x)\rangle & =\left\langle T(v)+U_{2}\left(w^{\prime}\right), \quad T(v)+U_{2}\left(w^{\prime}\right)\right\rangle=\langle T(v), T(v)\rangle+\left\langle U_{2}\left(w^{\prime}\right), U_{2}\left(w^{\prime}\right)\right\rangle \\
& =\langle P(v), P(v)\rangle+\left\langle w^{\prime}, w^{\prime}\right\rangle=\left\langle P(v)+w^{\prime}, \quad P(v)+w^{\prime}\right)=\langle x, x\rangle
\end{aligned}
$$

[We also used the fact that $\left\langle P(v), w^{\prime}\right\rangle=0$.] Thus, $U$ is unitary, and the theorem is proved.

13.24. Let $V$ be the vector space of polynomials over $\mathbf{R}$ with inner product defined by

$$
\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t
$$

Give an example of a linear functional $\phi$ on $V$ for which Theorem 13.3 does not hold - that is, for which there is no polynomial $h(t)$ such that $\phi(f)=\langle f, h\rangle$ for every $f \in V$.

Let $\phi: V \rightarrow \mathbf{R}$ be defined by $\phi(f)=f(0)$; that is, $\phi$ evaluates $f(t)$ at 0 , and hence maps $f(t)$ into its constant term. Suppose a polynomial $h(t)$ exists for which


\begin{equation*}
\phi(f)=f(0)=\int_{0}^{1} f(t) h(t) d t \tag{1}
\end{equation*}


for every polynomial $f(t)$. Observe that $\phi$ maps the polynomial $t f(t)$ into 0 ; hence, by (1),


\begin{equation*}
\int_{0}^{1} t f(t) h(t) d t=0 \tag{2}
\end{equation*}


for every polynomial $f(t)$. In particular (2) must hold for $f(t)=t h(t)$; that is,

$$
\int_{0}^{1} t^{2} h^{2}(t) d t=0
$$

This integral forces $h(t)$ to be the zero polynomial; hence, $\phi(f)=\langle f, h\rangle=\langle f, \mathbf{0}\rangle=0$ for every polynomial $f(t)$. This contradicts the fact that $\phi$ is not the zero functional; hence, the polynomial $h(t)$ does not exist.

\section*{SUPPLEMENTARY PROBLEMS}
\section*{Adjoint Operators}
13.25. Find the adjoint of:\\
(a) $A=\left[\begin{array}{ll}5-2 i & 3+7 i \\ 4-6 i & 8+3 i\end{array}\right]$\\
(b) $B=\left[\begin{array}{rr}3 & 5 i \\ i & -2 i\end{array}\right]$,\\
(c) $C=\left[\begin{array}{ll}1 & 1 \\ 2 & 3\end{array}\right]$

13.26. Let $T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{3}$ be defined by $T(x, y, z)=(x+2 y, 3 x-4 z, y)$. Find $T^{*}(x, y, z)$.

13.27. Let $T: \mathbf{C}^{3} \rightarrow \mathbf{C}^{3}$ be defined by $T(x, y, z)=[i x+(2+3 i) y, 3 x+(3-i) z, \quad(2-5 i) y+i z]$. Find $T^{*}(x, y, z)$.

13.28. For each linear function $\phi$ on $V$, find $u \in V$ such that $\phi(v)=\langle v, u\rangle$ for every $v \in V$ :

(a) $\phi: \mathbf{R}^{3} \rightarrow \mathbf{R}$ defined by $\phi(x, y, z)=x+2 y-3 z$.

(b) $\phi: \mathbf{C}^{3} \rightarrow \mathbf{C}$ defined by $\phi(x, y, z)=i x+(2+3 i) y+(1-2 i) z$.

13.29. Suppose $V$ has finite dimension. Prove that the image of $T^{*}$ is the orthogonal complement of the kernel of $T$; that is, $\operatorname{Im} T^{*}=(\operatorname{Ker} T)^{\perp}$. Hence, $\operatorname{rank}(T)=\operatorname{rank}\left(T^{*}\right)$.

13.30. Show that $T^{*} T=0$ implies $T=0$.

13.31. Let $V$ be the vector space of polynomials over $\mathbf{R}$ with inner product defined by $\langle f, g\rangle=\int_{0}^{1} f(t) g(t) d t$. Let $\mathbf{D}$ be the derivative operator on $V$; that is, $\mathbf{D}(f)=d f / d t$. Show that there is no operator $\mathbf{D}^{*}$ on $V$ such that $\langle\mathbf{D}(f), g\rangle=\left\langle f, \mathbf{D}^{*}(g)\right\rangle$ for every $f, g \in V$. That is, $\mathbf{D}$ has no adjoint.

\section*{Unitary and Orthogonal Operators and Matrices}
13.32. Find a unitary (orthogonal) matrix whose first row is\\
(a) $(2 / \sqrt{13}, 3 / \sqrt{13})$,\\
(b) a multiple of $(1,1-i)$,\\
(c) a multiple of $(1,-i, 1-i)$.

13.33. Prove that the products and inverses of orthogonal matrices are orthogonal. (Thus, the orthogonal matrices form a group under multiplication, called the orthogonal group.)

13.34. Prove that the products and inverses of unitary matrices are unitary. (Thus, the unitary matrices form a group under multiplication, called the unitary group.)

13.35. Show that if an orthogonal (unitary) matrix is triangular, then it is diagonal.

13.36. Recall that the complex matrices $A$ and $B$ are unitarily equivalent if there exists a unitary matrix $P$ such that $B=P^{*} A P$. Show that this relation is an equivalence relation.

13.37. Recall that the real matrices $A$ and $B$ are orthogonally equivalent if there exists an orthogonal matrix $P$ such that $B=P^{T} A P$. Show that this relation is an equivalence relation.

13.38. Let $W$ be a subspace of $V$. For any $v \in V$, let $v=w+w^{\prime}$, where $w \in W, w^{\prime} \in W^{\perp}$. (Such a sum is unique because $V=W \oplus W^{\perp}$.) Let $T: V \rightarrow V$ be defined by $T(v)=w-w^{\prime}$. Show that $T$ is self-adjoint unitary operator on $V$.

13.39. Let $V$ be an inner product space, and suppose $U: V \rightarrow V$ (not assumed linear) is surjective (onto) and preserves inner products; that is, $\langle U(v), U(w)\rangle=\langle u, w\rangle$ for every $v, w \in V$. Prove that $U$ is linear and hence unitary.

\section*{Positive and Positive Definite Operators}
13.40. Show that the sum of two positive (positive definite) operators is positive (positive definite).

13.41. Let $T$ be a linear operator on $V$ and let $f: V \times V \rightarrow K$ be defined by $f(u, v)=\langle T(u), v\rangle$. Show that $f$ is an inner product on $V$ if and only if $T$ is positive definite.

13.42. Suppose $E$ is an orthogonal projection onto some subspace $W$ of $V$. Prove that $k I+E$ is positive (positive definite) if $k \geq 0(k>0)$.

13.43. Consider the operator $T$ defined by $T\left(u_{i}\right)=\sqrt{\lambda_{i}} u_{i}, i=1, \ldots, n$, in the proof of Theorem 13.10A. Show that $T$ is positive and that it is the only positive operator for which $T^{2}=P$.

13.44. Suppose $P$ is both positive and unitary. Prove that $P=I$.

13.45. Determine which of the following matrices are positive (positive definite):\\
(i) $\left[\begin{array}{ll}1 & 1 \\ 1 & 1\end{array}\right]$,\\
(ii) $\left[\begin{array}{rr}0 & i \\ -i & 0\end{array}\right]$,\\
(iii) $\left[\begin{array}{rr}0 & 1 \\ -1 & 0\end{array}\right]$,\\
(iv) $\left[\begin{array}{ll}1 & 1 \\ 0 & 1\end{array}\right]$,\\
(v) $\left[\begin{array}{ll}2 & 1 \\ 1 & 2\end{array}\right]$\\
(vi) $\left[\begin{array}{ll}1 & 2 \\ 2 & 1\end{array}\right]$

13.46. Prove that a $2 \times 2$ complex matrix $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ is positive if and only if (i) $A=A^{*}$, and (ii) $a, d$ and $|A|=a d-b c$ are nonnegative real numbers.

13.47. Prove that a diagonal matrix $A$ is positive (positive definite) if and only if every diagonal entry is a nonnegative (positive) real number.

\section*{Self-adjoint and Symmetric Matrices}
13.48. For any operator $T$, show that $T+T^{*}$ is self-adjoint and $T-T^{*}$ is skew-adjoint.

13.49. Suppose $T$ is self-adjoint. Show that $T^{2}(v)=0$ implies $T(v)=0$. Using this to prove that $T^{n}(v)=0$ also implies that $T(v)=0$ for $n>0$.

13.50. Let $V$ be a complex inner product space. Suppose $\langle T(v), v\rangle$ is real for every $v \in V$. Show that $T$ is selfadjoint.

13.51. Suppose $T_{1}$ and $T_{2}$ are self-adjoint. Show that $T_{1} T_{2}$ is self-adjoint if and only if $T_{1}$ and $T_{1}$ commute; that is, $T_{1} T_{2}=T_{2} T_{1}$.

13.52. For each of the following symmetric matrices $A$, find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $P^{T} A P$ is diagonal:\\
(a) $A=\left[\begin{array}{rr}1 & 2 \\ 2 & -2\end{array}\right]$,\\
(b) $A=\left[\begin{array}{rr}5 & 4 \\ 4 & -1\end{array}\right]$\\
(c) $A=\left[\begin{array}{rr}7 & 3 \\ 3 & -1\end{array}\right]$

13.53. Find an orthogonal change of coordinates $X=P X^{\prime}$ that diagonalizes each of the following quadratic forms and find the corresponding diagonal quadratic form $q\left(x^{\prime}\right)$ :\\
(a) $q(x, y)=2 x^{2}-6 x y+10 y^{2}$,\\
(b) $q(x, y)=x^{2}+8 x y-5 y^{2}$\\
(c) $q(x, y, z)=2 x^{2}-4 x y+5 y^{2}+2 x z-4 y z+2 z^{2}$

\section*{Normal Operators and Matrices}
13.54. Let $A=\left[\begin{array}{ll}2 & i \\ i & 2\end{array}\right]$. Verify that $A$ is normal. Find a unitary matrix $P$ such that $P^{*} A P$ is diagonal. Find $P^{*} A P$.

13.55. Show that a triangular matrix is normal if and only if it is diagonal.

13.56. Prove that if $T$ is normal on $V$, then $\|T(v)\|=\left\|T^{*}(v)\right\|$ for every $v \in V$. Prove that the converse holds in complex inner product spaces.

13.57. Show that self-adjoint, skew-adjoint, and unitary (orthogonal) operators are normal.

13.58. Suppose $T$ is normal. Prove that

(a) $T$ is self-adjoint if and only if its eigenvalues are real.

(b) $T$ is unitary if and only if its eigenvalues have absolute value 1 .

(c) $T$ is positive if and only if its eigenvalues are nonnegative real numbers.

13.59. Show that if $T$ is normal, then $T$ and $T^{*}$ have the same kernel and the same image.

13.60. Suppose $T_{1}$ and $T_{2}$ are normal and commute. Show that $T_{1}+T_{2}$ and $T_{1} T_{2}$ are also normal.

13.61. Suppose $T_{1}$ is normal and commutes with $T_{2}$. Show that $T_{1}$ also commutes with $T_{2}^{*}$.

13.62. Prove the following: Let $T_{1}$ and $T_{2}$ be normal operators on a complex finite-dimensional vector space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of both $T_{1}$ and $T_{2}$. (That is, $T_{1}$ and $T_{2}$ can be simultaneously diagonalized.)

\section*{Isomorphism Problems for Inner Product Spaces}
13.63. Let $S=\left\{u_{1}, \ldots, u_{n}\right\}$ be an orthonormal basis of an inner product space $V$ over $K$. Show that the mapping $v \mapsto[v]_{S}$ is an (inner product space) isomorphism between $V$ and $K^{n}$. (Here $[v]_{S}$ denotes the coordinate vector of $v$ in the basis $S$.)

13.64. Show that inner product spaces $V$ and $W$ over $K$ are isomorphic if and only if $V$ and $W$ have the same dimension.

13.65. Suppose $\left\{u_{1}, \ldots, u_{n}\right\}$ and $\left\{u_{1}^{\prime}, \ldots, u_{n}^{\prime}\right\}$ are orthonormal bases of $V$ and $W$, respectively. Let $T: V \rightarrow W$ be the linear map defined by $T\left(u_{i}\right)=u_{i}^{\prime}$ for each $i$. Show that $T$ is an isomorphism.

13.66. Let $V$ be an inner product space. Recall that each $u \in V$ determines a linear functional $\hat{u}$ in the dual space $V^{*}$ by the definition $\hat{u}(v)=\langle v, u\rangle$ for every $v \in V$. (See the text immediately preceding Theorem 13.3.) Show that the map $u \mapsto \hat{u}$ is linear and nonsingular, and hence an isomorphism from $V$ onto $V^{*}$.

\section*{Miscellaneous Problems}
13.67. Suppose $\left\{u_{1}, \ldots, u_{n}\right\}$ is an orthonormal basis of $V$. Prove

(a) $\left\langle a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}, b_{1} u_{1}+b_{2} u_{2}+\cdots+b_{n} u_{n}\right\rangle=a_{1} \bar{b}_{1}+a_{2} \bar{b}_{2}+\ldots \bar{a}_{n} \bar{b}_{n}$

(b) Let $A=\left[a_{i j}\right]$ be the matrix representing $T: V \rightarrow V$ in the basis $\left\{u_{i}\right\}$. Then $a_{i j}=\left\langle T\left(u_{i}\right), u_{j}\right\rangle$.

13.68. Show that there exists an orthonormal basis $\left\{u_{1}, \ldots, u_{n}\right\}$ of $V$ consisting of eigenvectors of $T$ if and only if there exist orthogonal projections $E_{1}, \ldots, E_{r}$ and scalars $\lambda_{1}, \ldots, \lambda_{r}$ such that\\
(i) $T=\lambda_{1} E_{1}+\cdots+\lambda_{r} E_{r}$,\\
$E_{1}+\cdots+E_{r}=I$, (iii)\\
$E_{i} E_{j}=0 \quad$ for $i \neq j$

13.69. Suppose $V=U \oplus W$ and suppose $T_{1}: U \rightarrow V$ and $T_{2}: W \rightarrow V$ are linear. Show that $T=T_{1} \oplus T_{2}$ is also linear. Here $T$ is defined as follows: If $v \in V$ and $v=u+w$ where $u \in U, w \in W$, then

$$
T(v)=T_{1}(u)+T_{2}(w)
$$

\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS}
Notation: $\left[R_{1} ; R_{2} ; \ldots ; R_{n}\right]$ denotes a matrix with rows $R_{1}, R_{2}, \ldots, R_{n}$.\\
13.25. (a) $[5+2 i, 4+6 i ; 3-7 i, \quad 8-3 i]$\\
(b) $[3,-i ;-5 i, 2 i]$\\
(c) $[1,2 ; 1,3]$

13.26. $T^{*}(x, y, z)=(x+3 y, 2 x+z, \quad-4 y)$

13.27. $T^{*}(x, y, z)=[-i x+3 y, \quad(2-3 i) x+(2+5 i) z, \quad(3+i) y-i z]$

13.28. (a) $u=(1,2,-3), \quad$ (b) $u=(-i, 2-3 i, 1+2 i)$

13.32. (a) $(1 / \sqrt{13})[2,3 ; 3,-2]$, (b) $(1 / \sqrt{3})[1,1-i ; 1+i,-1]$,

(c) $\frac{1}{2}[1,-i, 1-i ; \quad \sqrt{2} i,-\sqrt{2}, 0 ; \quad 1,-i,-1+i]$

13.45. Only (i) and (v) are positive. Only (v) is positive definite.

13.52. (a and b) $P=(1 / \sqrt{5})[2,-1 ; 1,2], \quad$ (c) $P=(1 / \sqrt{10})[3,-1 ; 1,3]$\\
(a) $D=[2,0 ; 0,-3]$,\\
(b) $D=[7,0 ; 0,-3]$,\\
(c) $D=[8,0 ; 0,-2]$

13.53. (a) $x=\left(3 x^{\prime}-y^{\prime}\right) / \sqrt{10}, \quad y=\left(x^{\prime}+3 y^{\prime}\right) / \sqrt{10} ; \quad$ (b) $\quad x=\left(2 x^{\prime}-y^{\prime}\right) / \sqrt{5}, y=\left(x^{\prime}+2 y^{\prime}\right) / \sqrt{5}$;

(c) $x=x^{\prime} / \sqrt{3}+y^{\prime} / \sqrt{2}+z^{\prime} / \sqrt{6}, \quad y=x^{\prime} / \sqrt{3}-2 z^{\prime} / \sqrt{6}, \quad z=x^{\prime} / \sqrt{3}-y^{\prime} / \sqrt{2}+z^{\prime} / \sqrt{6}$;

(a) $q\left(x^{\prime}\right)=\operatorname{diag}(1,11) ; \quad$ (b) $q\left(x^{\prime}\right)=\operatorname{diag}(3,-7) ; \quad$ (c) $q\left(x^{\prime}\right)=\operatorname{diag}(1,17)$

13.54. (a) $P=(1 / \sqrt{2})[1,-1 ; 1,1], P^{*} A P=\operatorname{diag}(2+i, 2-i)$

\section*{APPENDIX A}
\section*{Multilinear Products}
\section*{A. 1 Introduction}
The material in this appendix is much more abstract than that which has previously appeared. Accordingly, many of the proofs will be omitted. Also, we motivate the material with the following observation.

Let $S$ be a basis of a vector space $V$. Theorem 5.2 may be restated as follows.

THEOREM 5.2: $\quad$ Let $g: S \rightarrow V$ be the inclusion map of the basis $S$ into $V$. Then, for any vector space $U$ and any mapping $f: S \rightarrow U$, there exists a unique linear mapping $f^{*}: V \rightarrow U$ such that $f=f^{*} \cdot g$.

Another way to state the fact that $f=f^{*} \cdot g$ is that the diagram in Fig. A-1(a) commutes.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-403(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-403(2)}
\end{center}

(b)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-403}
\end{center}

(c)

Figure A-1

\section*{A. 2 Bilinear Mapping and Tensor Products}
Let $U, V, W$ be vector spaces over a field $\mathbf{K}$. Consider a map

$$
f: V \times W \rightarrow U
$$

Then $f$ is said to be bilinear if, for each $v \in V$, the $\operatorname{map} f_{v}: W \rightarrow U$ defined by $f_{v}(w)=f(v, w)$ is linear; and, for each $w \in W$, the map $f_{w}: V \rightarrow U$ defined by $f_{w}(v)=f(v, w)$ is linear.

That is, $f$ is linear in each of its two variables. Note that $f$ is similar to a bilinear form except that the values of the map $f$ are in a vector space $U$ rather than the field $\mathbf{K}$.

DEFINITION A.1: $\quad$ Let $V$ and $W$ be vector spaces over the same field $\mathbf{K}$. The tensor product of $V$ and $W$ is a vector space $T$ over $\mathbf{K}$ together with a bilinear map $g: V \times W \rightarrow T$, denoted by $g(v, w)=v \otimes w$, with the following property: $(*)$ For any vector space $U$ over $\mathbf{K}$ and any bilinear map $f: V \times W \rightarrow U$ there exists a unique linear $\operatorname{map} f^{*}: T \rightarrow U$ such that $f^{*} \cdot g=f$.

The tensor product $(T, g)$ [or simply $T$ when $g$ is understood] of $V$ and $W$ is denoted by $V \otimes W$, and the element $v \otimes w$ is called the tensor of $v$ and $w$.

Another way to state condition $\left({ }^{*}\right)$ is that the diagram in Fig. A-1(b) commutes. The fact that such a unique linear map $f^{*}$ exists is called the "Universal Mapping Principle" (UMP). As illustrated in Fig. A-1(b), condition (*) also says that any bilinear map $f: V \times W \rightarrow U$ "factors through" the tensor product $T=V \otimes W$. The uniqueness in $(*)$ implies that the image of $g$ spans $T$; that is, $\operatorname{span}(\{v \otimes w\})=T$.

THEOREM A.1: (Uniqueness of Tensor Products) Let $(T, g)$ and $\left(T^{\prime}, g^{\prime}\right)$ be tensor products of $V$ and $W$. Then there exists a unique isomorphism $h: T \rightarrow T^{\prime}$ such that $h g=g^{\prime}$.

Proof. Because $T$ is a tensor product, and $g^{\prime}: V \otimes W \rightarrow T^{\prime}$ is bilinear, there exists a unique linear map $h: T \rightarrow T^{\prime}$ such that $h g=g^{\prime}$. Similarly, because $T^{\prime}$ is a tensor product, and $g: V \otimes W \rightarrow T^{\prime}$ is bilinear, there exists a unique linear map $h^{\prime}: T^{\prime} \rightarrow T$ such that $h^{\prime} g^{\prime}=g$. Using $h g=g^{\prime}$, we get $h^{\prime} h g=g$. Also, because $T$ is a tensor product, and $g: V \otimes W \rightarrow T$ is bilinear, there exists a unique linear map $h^{*}: T \rightarrow T$ such that $h^{*} g=g$. But $1_{\mathrm{T}} g=g$. Thus, $h^{\prime} h=h^{*}=1_{\mathrm{T}}$. Similarly, $h h^{\prime}=1_{\mathrm{T}^{\prime}}$. Therefore, $h$ is an isomorphism from $T$ to $T^{\prime}$.

THEOREM A.2: (Existence of Tensor Product) The tensor product $T=V \otimes W$ of vector spaces $V$ and $W$ over $\mathbf{K}$ exists. Let $\left\{v_{1}, \ldots, v_{m}\right\}$ be a basis of $V$ and let $\left\{w_{1}, \ldots, w_{n}\right\}$ be a basis of $W$. Then the $m n$ vectors

$$
v_{i} \otimes w_{i} \quad(i=1, \ldots, m ; j=1, \ldots, n)
$$

form a basis of $T$. Thus, $\operatorname{dim} T=m n=(\operatorname{dim} V)(\operatorname{dim} W)$.

Outline of Proof. Suppose $\left\{v_{1}, \ldots, v_{m}\right\}$ is a basis of $V$, and suppose $\left\{w_{1}, \ldots, w_{n}\right\}$ is a basis of $W$. Consider the $m n$ symbols $\left\{t_{i j} \mid i=i, \ldots, m, j=1, \ldots, n\right\}$. Let $T$ be the vector space generated by the $t_{i j}$. That is, $T$ consists of all linear combinations of the $t_{i j}$ with coefficients in K. [See Problem 4.137.]

Let $v \in V$ and $w \in W$. Say

$$
v=a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{m} v_{m} \quad \text { and } \quad w=b_{1} w_{1}+b_{2} w_{2}+\cdots+b_{m} w_{m}
$$

Let $g: V \times W \rightarrow T$ be defined by

$$
g(v, w)=\sum_{i} \sum_{j} a_{i} b_{j} t_{i j}
$$

Then $g$ is bilinear. [Proof left to reader.]

Now let $f: V \times W \rightarrow U$ be bilinear. Because the $t_{i j}$ form a basis of $T$, Theorem 5.2 (stated above) tells us that there exists a unique linear map $f^{*}: T \rightarrow U$ such that $f^{*}\left(t_{i j}\right)=f\left(v_{i}, w_{j}\right)$. Then, for $v=\sum_{i} a_{i} v_{i}$ and $w=\sum_{j} b_{j} w_{j}$, we have

$$
f(v, w)=f\left(\sum_{i} a_{i} v_{i}, \sum_{j} b_{j} w_{j}\right)=\sum_{i} \sum_{j} a_{i} b_{j} f\left(v_{i}, w_{j}\right)=\sum_{i} \sum_{j} a_{i} b_{j} t_{i j}=f^{*}(g(v, w)) .
$$

Therefore, $f=f^{*} g$ where $f^{*}$ is the required map in Definition A.1. Thus, $T$ is a tensor product.

Let $\left\{v_{1}^{\prime}, \ldots, v_{m}^{\prime}\right\}$ be any basis of $V$ and $\left\{w_{1}^{\prime}, \ldots, w_{m}^{\prime}\right\}$ be any basis of $W$.

Let $v \in V$ and $w \in W$ and say

$$
v=a_{1}^{\prime} v_{1}^{\prime}+\cdots+a_{m}^{\prime} v_{m}^{\prime} \quad \text { and } \quad w=b_{1}^{\prime} w_{1}^{\prime}+\cdots+b_{m}^{\prime} w_{m}^{\prime}
$$

Then

$$
v \otimes w=g(v, w)=\sum_{i} \sum_{j} a_{i}^{\prime} b_{i}^{\prime} g\left(v_{i}^{\prime}, w_{i}^{\prime}\right)=\sum_{i} \sum_{j} a_{i}^{\prime} b_{j}^{\prime}\left(v_{i}^{\prime} \otimes w_{j}^{\prime}\right)
$$

Thus, the elements $v_{i}^{\prime} \otimes w_{j}^{\prime}$ span $T$. There are $m n$ such elements. They cannot be linearly dependent because $\left\{t_{i j}\right\}$ is a basis of $T$, and hence, $\operatorname{dim} T=m n$. Thus, the $v_{i}^{\prime} \otimes w_{j}^{\prime}$ form a basis of $T$.

Next we give two concrete examples of tensor products.

EXAMPLE A. 1 Let $V$ be the vector space of polynomials $\mathbf{P}_{r-1}(x)$ and let $W$ be the vector space of polynomials $\mathbf{P}_{s-1}(y)$. Thus, the following from bases of $V$ and $W$, respectively,

$$
1, x, x^{2}, \ldots, x^{r-1} \quad \text { and } \quad 1, y, y^{2}, \ldots, y^{s-1}
$$

In particular, $\operatorname{dim} V=r$ and $\operatorname{dim} W=s$. Let $T$ be the vector space of polynomials in variables $x$ and $y$ with basis

$$
\left\{x^{i} y^{j}\right\} \text { where } i=0,1, \ldots, r-1 ; j=0,1, \ldots, s-1
$$

Then $T$ is the tensor product $V \otimes W$ under the mapping

$$
x^{i} \otimes y^{j}=x^{i} y^{i}
$$

For example, suppose $v=2-5 x+3 x^{3}$ and $w=7 y+4 y^{2}$. Then

$$
v \otimes w=14 y+8 y^{2}-35 x y-20 x y^{2}+21 x^{3} y+12 x^{3} y^{2}
$$

Note, $\operatorname{dim} T=r s=(\operatorname{dim} V)(\operatorname{dim} W)$.

\section*{EXAMPLE A. 2}
Let $V$ be the vector space of $m \times n$ matrices over a field $\mathbf{K}$ and let $W$ be the vector space of $p \times q$ matrices over K. Suppose $A=\left[a_{11}\right]$ belongs to $V$, and $B$ belongs to $W$. Let $T$ be the vector space of $m p \times n q$ matrices over $\mathbf{K}$. Then $T$ is the tensor product of $V$ and $W$ where $A \otimes B$ is the block matrix

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_de2bde501961f6000cc6g-405}
\end{center}

For example, suppose $A=\left[\begin{array}{ll}1 & 2 \\ 3 & 4\end{array}\right]$ and $B=\left[\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6\end{array}\right]$. Then

$$
A \otimes B=\left[\begin{array}{rrrrrr}
1 & 2 & 3 & 2 & 4 & 6 \\
4 & 5 & 6 & 8 & 10 & 12 \\
3 & 6 & 9 & 4 & 8 & 12 \\
12 & 15 & 18 & 16 & 20 & 24
\end{array}\right]
$$

\section*{Isomorphisms of Tensor Products}
First we note that tensoring is associative in a cannonical way. Namely,

THEOREM A.3: Let $U, V, W$ be vector spaces over a field $\mathbf{K}$. Then there exists a unique isomorphism

$$
(U \otimes V) \otimes W \rightarrow U \otimes(V \otimes W)
$$

such that, for every $u \in U, v \in V, w \in W$,

$$
(u \otimes v) \otimes w \mapsto u \otimes(v \otimes w)
$$

Accordingly, we may omit parenthesis when tensoring any number of factors. Specifically, given vectors spaces $V_{1}, V_{2}, \ldots, V_{m}$ over a field $\mathbf{K}$, we may unambiguously form their tensor product

$$
V_{1} \otimes V_{2} \otimes \ldots \otimes V_{m}
$$

and, for vectors $v_{j}$ in $V_{j}$, we may unambiguously form the tensor product

$$
v_{1} \otimes v_{2} \otimes \ldots \otimes v_{m}
$$

Moreover, given a vector space $V$ over $\mathbf{K}$, we may unambiguously define the following tensor product:

$$
\otimes^{r} V=V \otimes V \otimes \ldots \otimes V(r \text { factors })
$$

Also, there is a canonical isomorphism

$$
\left(\otimes^{r} V\right) \otimes\left(\otimes^{s} V\right) \rightarrow \otimes^{r+s} V
$$

Furthermore, viewing $\mathbf{K}$ as a vector space over itself, we have the canonical isomorphism

$$
\mathbf{K} \otimes V \rightarrow V
$$

where we define $a \otimes v=a v$.

\section*{A. 3 Alternating Multilinear Maps}
Let $f: V^{r} \rightarrow U$ where $V$ and $U$ are vector spaces over $\mathbf{K}$. [Recall $V^{r}=V \times V \times \ldots \times V, r$ factors.]

(1) The mapping $f$ is said to be multilinear or $r$-linear if $f\left(v_{1}, \ldots, v_{r}\right)$ is linear as a function of each $v_{j}$ when the other $v_{i}$ 's are held fixed. That is,

$$
\begin{gathered}
f\left(\ldots, v_{j}+v_{j}^{\prime}, \ldots\right)=f\left(\ldots, v_{j}, \ldots\right)+f\left(\ldots, v_{j}^{\prime}, \ldots\right) \\
f\left(\ldots, k v_{j}, \ldots\right)=k f\left(\ldots, v_{j}, \ldots\right)
\end{gathered}
$$

where only the $j$ th position changes.

(2) The mapping $f$ is said to be alternating if

$$
f\left(v_{1}, \ldots, v_{r}\right)=0 \text { whenever } v_{i}=v_{j} \text { with } i \neq j
$$

One can easily show (Prove!) that if $f$ is an alternating multilinear mapping on $V^{r}$, then

$$
f\left(\ldots, v_{i}, \ldots, v_{j}, \ldots\right)=-f\left(\ldots, v_{j}, \ldots, v_{i}, \ldots\right)
$$

That is, if two of the vectors are interchanged, then the associated value changes sign.

\section*{EXAMPLE A. 3 (Determinants)}
The determinant function $D: M \rightarrow K$ on the space $M$ of $n \times n$ matrices may be viewed as an $n$-variable function

$$
D(A)=D\left(R_{1}, R_{2}, \ldots, R_{n}\right)
$$

defined on the rows $R_{1}, R_{2}, \ldots, R_{n}$ of $A$. Recall (Chapter 8 ) that, in this context, $D$ is both $n$-linear and alternating.

We now need some additional notation. Let $K=\left[k_{1}, k_{2}, \ldots, k_{r}\right]$ denote an $r$-list ( $r$-tuple) of elements from $I_{n}=(1,2, \ldots, n)$. We will then use the following notation where the $v_{k}$ 's denote vectors and the $a_{i k}$ 's denote scalars:

$$
v_{K}=\left(v_{k_{1}}, v_{k_{2}}, \ldots, v_{k_{r}}\right) \text { and } a_{K}=a_{1 k_{1}} a_{2 k_{2}} \ldots a_{r k_{r}}
$$

Note $v_{K}$ is a list of $r$ vectors, and $a_{K}$ is a product of $r$ scalars.

Now suppose the elements in $K=\left[k_{1}, k_{2}, \ldots, k_{r}\right]$ are distinct. Then $K$ is a permutation $\sigma_{K}$ of an $r$-list $J=\left[i_{1}, i_{2}, \ldots, i_{r}\right]$ in standard form, that is, where $i_{1}<i_{2}<\ldots<i_{r}$. The number of such standard-form $r$-lists $J$ from $I_{n}$ is the binomial coefficient:

$$
\left(\begin{array}{l}
n \\
r
\end{array}\right)=\frac{n !}{r !(n-r) !}
$$

[Recall $\operatorname{sign}\left(\sigma_{K}\right)=(-1)^{m_{K}}$ where $m_{K}$ is the number of interchanges that transforms $K$ into $J$.]

Now suppose $A=\left[a_{i j}\right]$ is an $r \times n$ matrix. For a given ordered $r$-list $J$, we define

$$
D_{J}(A)=\left|\begin{array}{cccc}
a_{1 i_{1}} & a_{1 i_{2}} & \ldots & a_{1 i_{r}} \\
a_{2 i_{1}} & a_{2 i_{2}} & \ldots & a_{2 i_{r}} \\
\ldots \ldots & \ldots & \ldots & \ldots \\
a_{r i_{1}} & a_{r i_{2}} & \ldots & a_{r i_{r}}
\end{array}\right|
$$

That is, $D_{J}(A)$ is the determinant of the $r \times r$ submatrix of $A$ whose column subscripts belong to $J$.

Our main theorem below uses the following "shuffling" lemma.

LEMMA A. 4 Let $V$ and $U$ be vector spaces over $K$, and let $f: V^{r} \rightarrow U$ be an alternating $r$-linear mapping. Let $v_{1}, v_{2}, \ldots, v_{n}$ be vectors in $V$ and let $A=\left[a_{i j}\right]$ be an $r \times n$ matrix over $K$ where $r \leq n$. For $i=1,2, \ldots, r$, let

$$
u_{i}=a_{i 1} v_{i}+a_{i 2} v_{2}+\cdots+a_{i n} v_{n}
$$

Then

$$
f\left(u_{1}, \ldots, u_{r}\right)=\sum_{f} D_{J}(A) f\left(v_{i_{1}}, v_{i_{2}}, \ldots, v_{i_{r}}\right)
$$

where the sum is over all standard-form $r$-lists $J=\left\{i_{1}, i_{2}, \ldots, i_{r}\right\}$.

The proof is technical but straightforward. The linearity of $f$ gives us the sum

$$
f\left(u_{1}, \ldots, u_{r}\right)=\sum_{K} a_{K} f\left(v_{K}\right)
$$

where the sum is over all $r$-lists $K$ from $\{1, \ldots, n\}$. The alternating property of $f$ tells us that $f\left(v_{K}\right)=0$ when $K$ does not contain distinct integers. The proof now mainly uses the fact that as we interchange the $v_{j}$ 's to transform

$$
f\left(v_{K}\right)=f\left(v_{k_{1}}, v_{k_{2}}, \ldots, v_{k_{r}}\right) \quad \text { to } \quad f\left(v_{j}\right)=f\left(v_{i_{1}}, v_{i_{2}}, \ldots, v_{i_{r}}\right)
$$

so that $i_{1}<\cdots<i_{r}$, the associated sign of $a_{K}$, will change in the same way as the sign of the corresponding permutation $\sigma_{K}$ changes when it is transformed to the identity permutation using transpositions.

We illustrate the lemma below for $r=2$ and $n=3$.

EXAMPLE A. 4 Suppose $f: V^{2} \rightarrow U$ is an alternating multilinear function. Let $v_{1}, v_{2}, v_{3} \in V$ and let $u, w \in V$. Suppose

$$
u=a_{1} v_{1}+a_{2} v_{2}+a_{3} v_{3} \text { and } w=b_{1} v_{1}+b_{2} v_{2}+b_{3} v_{3}
$$

Consider

$$
f(u, w)=f\left(a_{1} v_{1}+a_{2} v_{2}+a_{3} v_{3}, b_{1} v_{1}+b_{2} v_{2}+b_{3} v_{3}\right)
$$

Using multilinearity, we get nine terms:

$$
\begin{aligned}
f(u, w)= & a_{1} b_{1} f\left(v_{1}, v_{r}\right)+a_{1} b_{2} f\left(v_{1}, v_{2}\right)+a_{1} b_{3} f\left(v_{1}, v_{3}\right) \\
& +a_{2} b_{1} f\left(v_{2}, v_{1}\right)+a_{2} b_{2} f\left(v_{2}, v_{2}\right)+a_{2} b_{3} f\left(v_{2}, v_{3}\right) \\
& +a_{3} b_{1} f\left(v_{3}, v_{1}\right)+a_{3} b_{2} f\left(v_{3}, v_{2}\right)+a_{3} b_{3} f\left(v_{3}, v_{3}\right)
\end{aligned}
$$

(Note that $J=[1,2], J^{\prime}=[1,3]$ and $J^{\prime \prime}=[2,3]$ are the three standard-form 2-lists of $I=[1,2,3]$.) The alternating property of $f$ tells us that each $f\left(v_{i}, v_{i}\right)=0$; hence, three of the above nine terms are equal to 0 . The alternating property also tells us that $f\left(v_{i}, v_{f}\right)=-f\left(v_{f}, v_{r}\right)$. Thus, three of the terms can be transformed so their subscripts form a standard-form 2-list by a single interchange. Finally we obtain

$$
\begin{aligned}
f(u, w) & =\left(a_{1} b_{2}-a_{2} b_{1}\right) f\left(v_{1}, v_{2}\right)+\left(a_{1} b_{3}-a_{3} b_{1}\right) f\left(v_{1}, v_{3}\right)+\left(a_{2} b_{3}-a_{3} b_{2}\right) f\left(v_{2}, v_{3}\right) \\
& =\left|\begin{array}{ll}
a_{1} & a_{2} \\
b_{1} & b_{2}
\end{array}\right| f\left(v_{1}, v_{2}\right)+\left|\begin{array}{ll}
a_{1} & a_{3} \\
b_{1} & b_{3}
\end{array}\right| f\left(v_{1}, v_{3}\right)+\left|\begin{array}{ll}
a_{2} & a_{3} \\
b_{2} & b_{3}
\end{array}\right| f\left(v_{2}, v_{3}\right)
\end{aligned}
$$

which is the content of Lemma A.4.

\section*{A. 4 Exterior Products}
The following definition applies.

DEFINITION A.2: $\quad$ Let $V$ be an $n$-dimensionmal vector space over a field $\mathbf{K}$, and let $r$ be an integer such that $1 \leq r \leq n$. The $r$-fold exterior product (or simply exterior product when $r$ is understood) is a vector space $\mathbf{E}$ over $\mathbf{K}$ together with an alternating $r$-linear mapping $g: V^{r} \rightarrow \mathbf{E}$, denoted by $g\left(v_{1}, \ldots, v_{r}\right)=v_{1} \wedge \ldots \wedge v_{r}$, with the following property: (*) For any vector space $U$ over $K$ and any alternating $r$-linear map $f: V^{r} \rightarrow U$ there exists a unique linear map $f^{*}: \mathbf{E} \rightarrow U$ such that $f^{*} \cdot g=f$.

The $r$-fold tensor product (E, $g$ ) (or simply $\mathbf{E}$ when $g$ is understood) of $V$ is denoted by $\wedge^{r} V$, and the element $v_{1} \wedge \cdots \wedge v_{r}$ is called the exterior product or wedge product of the $v_{i}$ 's.

Another way to state condition $\left(^{*}\right)$ is that the diagram in Fig. A-1(c) commutes. Again, the fact that such a unique linear map $f^{*}$ exists is called the "Universal Mapping Principle (UMP)". As illustrated in Fig. A-1(c), condition (\textit{) also says that any alternating $r$-linear map $f: V^{r} \rightarrow U$ "factors through" the exterior product $\mathbf{E}=\wedge^{r} V$. Again, the uniqueness in (}) implies that the image of $g$ spans $\mathbf{E}$; that is, $\operatorname{span}\left(v_{1} \wedge \cdots \wedge v_{r}\right)=\mathbf{E}$.

THEOREM A.5: (Uniqueness of Exterior Products) Let $(\mathbf{E}, g)$ and $\left(\mathbf{E}^{\prime}, g^{\prime}\right)$ be $r$-fold exterior products of $V$. Then there exists a unique isomorphism $h: \mathbf{E} \rightarrow \mathbf{E}^{\prime}$ such that $h g=g^{\prime}$.

The proof is the same as the proof of Theorem A.1, which uses the UMP.

THEOREM A.6: (Existence of Exterior Products) Let $V$ be an $n$-dimensional vector space over K. Then the exterior product $\mathbf{E}=\wedge^{r} V$ exists. If $r>n$, then $\mathbf{E}=\{0\}$. If $r \leq n$, then $\operatorname{dim} E=\left(\begin{array}{l}n \\ r\end{array}\right)$. Moreover, if $\left[v_{1}, \ldots, v_{n}\right]$ is a basis of $V$, then the vectors $v_{i_{1}} \wedge v_{i_{2}} \wedge \cdots \wedge v_{i_{r}}$

where $1 \leq i_{1}<i_{2}<\cdots<i_{r} \leq n$, form a basis of $E$.

We give a concrete example of an exterior product.

\section*{EXAMPLE A. 5 (Cross Product)}
Consider $V=\mathbf{R}^{3}$ with the usual basis (i, j, k). Let $E=\wedge^{2} V$. Note $\operatorname{dim} V=3$. Thus, $\operatorname{dim} E=3$ with basis $\mathbf{i} \wedge \mathbf{j}, \mathbf{i} \wedge \mathbf{k}, \mathbf{j} \wedge \mathbf{k}$. We identify $E$ with $\mathbf{R}^{3}$ under the correspondence

$$
\mathbf{i}=\mathbf{j} \wedge \mathbf{k}, \mathbf{j}=\mathbf{k} \wedge \mathbf{i}=-\mathbf{i} \wedge \mathbf{k}, \mathbf{k}=\mathbf{i} \wedge \mathbf{j}
$$

Let $u$ and $w$ be arbitrary vectors in $V=\mathbf{R}^{3}$, say

$$
u=\left(a_{1}, a_{2}, a_{3}\right)=a_{1} \mathbf{i}+a_{2} \mathbf{j}+a_{3} \mathbf{k} \text { and } w=\left(b_{1}, b_{2}, b_{3}\right)=b_{1} \mathbf{i}+b_{2} \mathbf{j}+b_{3} \mathbf{k}
$$

Then, as in Example A.3,

$$
u \wedge w=\left(a_{1} b_{2}-a_{2} b_{1}\right)(\mathbf{i} \wedge \mathbf{j})+\left(a_{1} b_{3}-a_{3} b_{1}\right)(\mathbf{i} \wedge \mathbf{k})+\left(a_{2} b_{3}-a_{3} b_{2}\right)(\mathbf{j} \wedge \mathbf{k})
$$

Using the above identification, we get

$$
\begin{aligned}
u \wedge w & =\left(a_{2} b_{3}-a_{3} b_{2}\right) \mathbf{i}-\left(a_{1} b_{3}-a_{3} b_{1}\right) \mathbf{j}+\left(a_{1} b_{2}-a_{2} b_{1}\right) \mathbf{k} \\
& =\left|\begin{array}{ll}
a_{2} & a_{3} \\
b_{2} & b_{3}
\end{array}\right| \mathbf{i}-\left|\begin{array}{cc}
a_{1} & a_{3} \\
b_{1} & b_{3}
\end{array}\right| \mathbf{j}+\left|\begin{array}{ll}
a_{1} & a_{2} \\
b_{1} & b_{2}
\end{array}\right| \mathbf{k}
\end{aligned}
$$

The reader may recognize that the above exterior product is precisely the well-known cross product in $\mathbf{R}^{3}$.

Our last theorem tells us that we are actually able to "multiply" exterior products, which allows us to form an "exterior algebra" that is illustrated below.

THEOREM A.7: Let $V$ be a vector space over K. Let $r$ and $s$ be positive integers. Then there is a unique bilinear mapping

$$
\wedge^{r} V \times \wedge^{s} V \rightarrow \wedge^{r+s} V
$$

such that, for any vectors $u_{i}, w_{j}$ in $V$,

$$
\left(u_{1} \wedge \cdots \wedge u_{r}\right) \times\left(w_{1} \wedge \cdots \wedge w_{s}\right) \mapsto u_{1} \wedge \cdots \wedge u_{r} \wedge w_{1} \wedge \cdots \wedge w_{s}
$$

\section*{EXAMPLE A. 6}
We form an exterior algebra $A$ over a field $\mathbf{K}$ using noncommuting variables $x, y, z$. Because it is an exterior algebra, our variables satisfy:

$$
x \wedge x=0, \quad y \wedge y=0, \quad z \wedge z=0, \quad \text { and } \quad y \wedge x=-x \wedge y, \quad z \wedge x=-x \wedge z, \quad z \wedge y=-y \wedge z
$$

Every element of $A$ is a linear combination of the eight elements

$1, \quad x, \quad y, \quad z, \quad x \wedge y, \quad x \wedge z, \quad y \wedge z, \quad x \wedge y \wedge z$

We multiply two "polynomials" in $A$ using the usual distributive law, but now we also use the above conditions. For example,

$$
[3+4 y-5 x \wedge y+6 x \wedge z] \wedge[5 x-2 y]=15 x-6 y-20 x \wedge y+12 x \wedge y \wedge z
$$

Observe we use the fact that

$$
[4 y] \wedge[5 x]=20 y \wedge x=-20 x \wedge y \quad \text { and } \quad[6 x \wedge z] \wedge[-2 y]=-12 x \wedge z \wedge y=12 x \wedge y \wedge z
$$

\section*{Algebraic Structures}
\section*{B. 1 Introduction}
We define here algebraic structures that occur in almost all branches of mathematics. In particular, we will define a field that appears in the definition of a vector space. We begin with the definition of a group, which is a relatively simple algebraic structure with only one operation and is used as a building block for many other algebraic systems.

\section*{B. 2 Groups}
Let $G$ be a nonempty set with a binary operation; that is, to each pair of elements $a, b \in G$ there is assigned an element $a b \in G$. Then $G$ is called a group if the following axioms hold:

$\left[G_{1}\right]$ For any $a, b, c \in G$, we have $(a b) c=a(b c)$ (the associative law).

$\left[G_{2}\right]$ There exists an element $e \in G$, called the identity element, such that $a e=e a=a$ for every $a \in G$.

$\left[G_{3}\right]$ For each $a \in G$ there exists an element $a^{-1} \in G$, called the inverse of $a$, such that $a a^{-1}=a^{-1} a=e$.

A group $G$ is said to be abelian (or: commutative) if the commutative law holds-that is, if $a b=b a$ for every $a, b \in G$.

When the binary operation is denoted by juxtaposition as above, the group $G$ is said to be written multiplicatively. Sometimes, when $G$ is abelian, the binary operation is denoted by + and $G$ is said to be written additively. In such a case, the identity element is denoted by 0 and is called the zero element; the inverse is denoted by $-a$ and it is called the negative of $a$.

If $A$ and $B$ are subsets of a group $G$, then we write

$$
A B=\{a b \mid a \in A, b \in B\} \quad \text { or } \quad A+B=\{a+b \mid a \in A, b \in B\}
$$

We also write $a$ for $\{a\}$.

A subset $H$ of a group $G$ is called a subgroup of $G$ if $H$ forms a group under the operation of $G$. If $H$ is a subgroup of $G$ and $a \in G$, then the set $H a$ is called a right coset of $H$ and the set $a H$ is called a left coset of $H$.

DEFINITION: A subgroup $H$ of $G$ is called a normal subgroup if $a^{-1} H a \subseteq H$ for every $a \in G$. Equivalently, $H$ is normal if $a H=H a$ for every $a \in G-$ that is, if the right and left cosets of $H$ coincide.

Note that every subgroup of an abelian group is normal.

THEOREM B.1: Let $H$ be a normal subgroup of $G$. Then the cosets of $H$ in $G$ form a group under coset multiplication. This group is called the quotient group and is denoted by $G / H$.

EXAMPLE B. 1 The set $\mathbf{Z}$ of integers forms an abelian group under addition. (We remark that the even integers form a subgroup of $\mathbf{Z}$ but the odd integers do not.) Let $H$ denote the set of multiples of 5 ; that is, $H=\{\ldots,-10,-5,0,5,10, \ldots\}$. Then $H$ is a subgroup (necessarily normal) of $\mathbf{Z}$. The cosets of $H$ in $\mathbf{Z}$ follow:

$$
\begin{aligned}
& \overline{0}=0+H=H=\{\ldots,-10,-5,0,5,10, \ldots\} \\
& \overline{1}=1+H=\{\ldots,-9,-4,1,6,11, \ldots\} \\
& \overline{2}=2+H=\{\ldots,-8,-3,2,7,12, \ldots\} \\
& \overline{3}=3+H=\{\ldots,-7,-2,3,8,13, \ldots\} \\
& \overline{4}=4+H=\{\ldots,-6,-1,4,9,14, \ldots\}
\end{aligned}
$$

For any other integer $n \in \mathbf{Z}, \bar{n}=n+H$ coincides with one of the above cosets. Thus, by the above theorem, $\mathbf{Z} / H=\{\overline{0}, \overline{1}, \overline{2}, \overline{3}, \overline{4}\}$ forms a group under coset addition; its addition table follows:

\begin{center}
\begin{tabular}{c|ccccc}
+ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ \\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ \\
$\overline{1}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$ \\
$\overline{2}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$ \\
$\overline{3}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ \\
$\overline{4}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ \\
\end{tabular}
\end{center}

This quotient group $\mathbf{Z} / H$ is referred to as the integers modulo 5 and is frequently denoted by $\mathbf{Z}_{5}$. Analogeusly, for any positive integer $n$, there exists the quotient group $\mathbf{Z}_{n}$ called the integers modulo $n$.

EXAMPLE B. 2 The permutations of $n$ symbols (see page 267) form a group under composition of mappings; it is called the symmetric group of degree $n$ and is denoted by $\mathbf{S}_{n}$. We investigate $\mathbf{S}_{3}$ here; its elements are

$$
\begin{array}{rll}
\epsilon=\left(\begin{array}{lll}
1 & 2 & 3 \\
1 & 2 & 3
\end{array}\right) & \sigma_{2}=\left(\begin{array}{lll}
1 & 2 & 3 \\
3 & 2 & 1
\end{array}\right) & \phi_{1}=\left(\begin{array}{lll}
1 & 2 & 3 \\
2 & 3 & 1
\end{array}\right) \\
\sigma_{1}=\left(\begin{array}{lll}
1 & 2 & 3 \\
1 & 3 & 2
\end{array}\right) & \sigma_{3}=\left(\begin{array}{lll}
1 & 2 & 3 \\
2 & 1 & 3
\end{array}\right) & \phi_{2}=\left(\begin{array}{lll}
1 & 2 & 3 \\
3 & 1 & 2
\end{array}\right)
\end{array}
$$

Here $\left(\begin{array}{lll}1 & 2 & 3 \\ i & j & k\end{array}\right)$ is the permutation that maps $1 \mapsto i, 2 \mapsto j, 3 \mapsto k$. The multiplication table of $\mathbf{S}_{3}$ is

\begin{center}
\begin{tabular}{l|llllll}
 & $\epsilon$ & $\sigma_{1}$ & $\sigma_{2}$ & $\sigma_{3}$ & $\phi_{1}$ & $\phi_{2}$ \\
\hline
$\epsilon$ & $\epsilon$ & $\sigma_{1}$ & $\sigma_{2}$ & $\sigma_{3}$ & $\phi_{1}$ & $\phi_{2}$ \\
$\sigma_{1}$ & $\sigma_{1}$ & $\epsilon$ & $\phi_{1}$ & $\phi_{2}$ & $\sigma_{2}$ & $\sigma_{3}$ \\
$\sigma_{2}$ & $\sigma_{2}$ & $\phi_{2}$ & $\epsilon$ & $\phi_{1}$ & $\phi_{3}$ & $\sigma_{1}$ \\
$\sigma_{3}$ & $\sigma_{3}$ & $\phi_{1}$ & $\phi_{2}$ & $\epsilon$ & $\sigma_{1}$ & $\sigma_{2}$ \\
$\phi_{1}$ & $\phi_{1}$ & $\sigma_{3}$ & $\sigma_{1}$ & $\sigma_{2}$ & $\phi_{2}$ & $\epsilon$ \\
$\phi_{2}$ & $\phi_{2}$ & $\sigma_{2}$ & $\sigma_{3}$ & $\sigma_{1}$ & $\epsilon$ & $\phi_{1}$ \\
\end{tabular}
\end{center}

(The element in the $a$ th row and $b$ th column is $a b$.) The set $H=\left\{\epsilon, \sigma_{1}\right\}$ is a subgroup of $\mathbf{S}_{3}$; its right and left cosets are

$$
\begin{array}{cc}
\text { Right Cosets } & \text { Left Cosets } \\
H=\left\{\epsilon, \sigma_{1}\right\} & H=\left\{\epsilon, \sigma_{1}\right\} \\
H_{\phi_{1}}=\left\{\phi_{1}, \sigma_{2}\right\} & \phi_{2} H=\left\{\phi_{1}, \sigma_{3}\right\} \\
H_{\phi_{2}}=\left\{\phi_{2}, \sigma_{3}\right\} & \phi_{2} H=\left\{\phi_{2}, \sigma_{2}\right\}
\end{array}
$$

Observe that the right cosets and the left cosets are distinct; hence, $H$ is not a normal subgroup of $\mathbf{S}_{3}$.

A mapping $f$ from a group $G$ into a group $G^{\prime}$ is called a homomorphism if $f(a b)=f(a) f(b)$. For every $a, b \in G$. (If $f$ is also bijective, i.e., one-to-one and onto, then $f$ is called an isomorphism and $G$ and $G^{\prime}$ are\\
said to be isomorphic.) If $f: G \rightarrow G^{\prime}$ is a homomorphism, then the kernel of $f$ is the set of elements of $G$ that map into the identity element $e^{\prime} \in G^{\prime}$ :

$$
\text { kernel of } f=\left\{a \in G \mid f(a)=e^{\prime}\right\}
$$

(As usual, $f(G)$ is called the image of the mapping $f: G \rightarrow G^{\prime}$.) The following theorem applies.

THEOREM B.2: Let $f: G \rightarrow G$ be a homomorphism with kernel $K$. Then $K$ is a normal subgroup of $G$, and the quotient group $G / K$ is isomorphic to the image of $f$.

EXAMPLE B. 3 Let $G$ be the group of real numbers under addition, and let $G^{\prime}$ be the group of positive real numbers under multiplication. The mapping $f: G \rightarrow G^{\prime}$ defined by $f(a)=2^{a}$ is a homomorphism because

$$
f(a+b)=2^{a+b}=2^{a} 2^{b}=f(a) f(b)
$$

In particular, $f$ is bijective, hence, $G$ and $G^{\prime}$ are isomorphic.

EXAMPLE B. 4 Let $G$ be the group of nonzero complex numbers under multiplication, and let $G^{\prime}$ be the group of nonzero real numbers under multiplication. The mapping $f: G \rightarrow G^{\prime}$ defined by $f(z)=|z|$ is a homomorphism because

$$
f\left(z_{1} z_{2}\right)=\left|z_{1} z_{2}\right|=\left|z_{1}\right|\left|z_{2}\right|=f\left(z_{1}\right) f\left(z_{2}\right)
$$

The kernel $K$ of $f$ consists of those complex numbers $z$ on the unit circle-that is, for which $|z|=1$. Thus, $G / K$ is isomorphic to the image of $f-$ that is, to the group of positive real numbers under multiplication.

\section*{B. 3 Rings, Integral Domains, and Fields}
Let $R$ be a nonempty set with two binary operations, an operation of addition (denoted by + ) and an operation of multiplication (denoted by juxtaposition). Then $R$ is called a ring if the following axioms are satisfied:

$\left[R_{1}\right]$ For any $a, b, c \in R$, we have $(a+b)+c=a+(b+c)$.

$\left[R_{2}\right]$ There exists an element $0 \in R$, called the zero element, such that $a+0=0+a=a$ for every $a \in R$.

$\left[R_{3}\right]$ For each $a \in R$ there exists an element $-a \in R$, called the negative of $a$, such that $a+(-a)=(-a)+a=0$.

$\left[R_{4}\right]$ For any $a, b \in R$, we have $a+b=b+a$.

$\left[R_{5}\right]$ For any $a, b, c \in R$, we have $(a b) c=a(b c)$.

$\left[R_{6}\right]$ For any $a, b, c \in R$, we have

(i) $a(b+c)=a b+a c$, and (ii) $(b+c) a=b a+c a$.

Observe that the axioms $\left[R_{1}\right]$ through $\left[R_{4}\right]$ may be summarized by saying that $R$ is an abelian group under addition.

Subtraction is defined in $R$ by $a-b \equiv a+(-b)$.

It can be shown (see Problem B.25) that $a \cdot 0=0 \cdot a=0$ for every $a \in R$.

$R$ is called a commutative ring if $a b=b a$ for every $a, b \in R$. We also say that $R$ is a ring with a unit element if there exists a nonzero element $1 \in R$ such that $a \cdot 1=1 \cdot a=a$ for every $a \in R$.

A nonempty subset $S$ of $R$ is called a subring of $R$ if $S$ forms a ring under the operations of $R$. We note that $S$ is a subring of $R$ if and only if $a, b \in S$ implies $a-b \in S$ and $a b \in S$.

A nonempty subset $I$ of $R$ is called a left ideal in $R$ if (i) $a-b \in I$ whenever $a, b \in I$, and (ii) $r a \in I$ whenever $r \in R, a \in I$. Note that a left ideal $I$ in $R$ is also a subring of $R$. Similarly, we can define a right ideal and a two-sided ideal. Clearly all ideals in commutative rings are two sided. The term ideal shall mean two-sided ideal uniess otherwise specified.

THEOREM B.3: Let $I$ be a (two-sided) ideal in a ring $R$. Then the cosets $\{a+I \mid a \in R\}$ form a ring under coset addition and coset multiplication. This ring is denoted by $R / I$ and is called the quotient ring.

Now let $R$ be a commutative ring with a unit element. For any $a \in R$, the set $(a)=\{r a \mid r \in R\}$ is an ideal; it is called the principal ideal generated by $a$. If every ideal in $R$ is a principal ideal, then $R$ is called a principal ideal ring.

DEFINITION: $\quad$ A commutative ring $R$ with a unit element is called an integral domain if $R$ has no zero divisors - that is, if $a b=0$ implies $a=0$ or $b=0$.

DEFINITION: $\quad$ A commutative ring $R$ with a unit element is called a field if every nonzero $a \in R$ has a multiplicative inverse; that is, there exists an element $a^{-1} \in R$ such that $a a^{-1}=a^{-1} a=1$.

A field is necessarily an integral domain; for if $a b=0$ and $a \neq 0$, then

$$
b=1 \cdot b=a^{-1} a b=a^{-1} \cdot 0=0
$$

We remark that a field may also be viewed as a commutative ring in which the nonzero elements form a group under multiplication.

EXAMPLE B. 5 The set $\mathbf{Z}$ of integers with the usual operations of addition and multiplication is the classical example of an integral domain with a unit element. Every ideal $I$ in $\mathbf{Z}$ is a principal ideal; that is, $I=(n)$ for some integer $n$. The quotient ring $\mathbf{Z}_{n}=\mathbf{Z} /(n)$ is called the ring of integers module $n$. If $n$ is prime, then $\mathbf{Z}_{n}$ is a field. On the other hand, if $n$ is not prime then $\mathbf{Z}_{n}$ has zero divisors. For example, in the ring $\mathbf{Z}_{6}, \overline{2} \overline{3}=\overline{0}$ and $\overline{2} \neq \overline{0}$ and $\overline{3} \neq \overline{0}$.

EXAMPLE B. 6 The rational numbers $\mathbf{Q}$ and the real numbers $\mathbf{R}$ each form a field with respect to the usual operations of addition and multiplication.

EXAMPLE B. 7 Let $\mathbf{C}$ denote the set of ordered pairs of real numbers with addition and multiplication defined by

$$
\begin{gathered}
(a, b)+(c, d)=(a+c, b+d) \\
(a, b) \cdot(c, d)=(a c-b d, a d+b c)
\end{gathered}
$$

Then $\mathbf{C}$ satisfies all the required properties of a field. In fact, $\mathbf{C}$ is just the field of complex numbers (see page 4).

EXAMPLE B. 8 The set $M$ of all $2 \times 2$ matrices with real entries forms a noncommutative ring with zero divisors under the operations of matrix addition and matrix multiplication.

EXAMPLE B. 9 Let $R$ be any ring. Then the set $R[x]$ of all polynomials over $R$ forms a ring with respect to the usual operations of addition and multiplication of polynomials. Moreover, if $R$ is an integral domain then $R[x]$ is also an integral domain.

Now let $D$ be an integral domain. We say that $b$ divides $a$ in $D$ if $a=b c$ for some $c \in D$. An element $u \in D$ is called a unit if $u$ divides 1 - that is, if $u$ has a multiplicative inverse. An element $b \in D$ is called an associate of $a \in D$ if $b=u a$ for some unit $u \in D$. A nonunit $p \in D$ is said to be irreducible if $p=a b$ implies $a$ or $b$ is a unit.

An integral domain $D$ is called a unique factorization domain if every nonunit $a \in D$ can be written uniquely (up to associates and order) as a product of irreducible elements.

EXAMPLE B. 10 The ring $\mathbf{Z}$ of integers is the classical example of a unique factorization domain. The units of $\mathbf{Z}$ are 1 and -1 . The only associates of $n \in \mathbf{Z}$ are $n$ and $-n$. The irreducible elements of $\mathbf{Z}$ are the prime numbers.

EXAMPLE B. 11 The set $D=\{a+b \sqrt{13} \mid a, b$ integers $\}$ is an integral domain. The units of $D$ are $\pm 1$, $18 \pm 5 \sqrt{13}$ and $-18 \pm 5 \sqrt{13}$. The elements $2,3-\sqrt{13}$ and $-3-\sqrt{13}$ are irreducible in $D$. Observe that $4=2 \cdot 2=(3-\sqrt{13})(-3-\sqrt{13})$. Thus, $D$ is not a unique factorization domain. (See Problem B.40.)

\section*{B. 4 Modules}
Let $M$ be an additive abelian group and let $R$ be a ring with a unit element. Then $M$ is said to be a (left) $R$ module if there exists a mapping $R \times M \rightarrow M$ that satisfies the following axioms:

$$
\begin{aligned}
& {\left[M_{1}\right] r\left(m_{1}+m_{2}\right)=r m_{1}+r m_{2}} \\
& {\left[M_{2}\right](r+s) m=r m+s m} \\
& {\left[M_{3}\right](r s) m=r(s m)} \\
& {\left[M_{4}\right] 1 \cdot m=m}
\end{aligned}
$$

for any $r, s \in R$ and any $m_{i} \in M$.

We emphasize that an $R$-module is a generalization of a vector space where we allow the scalars to come from a ring rather than a field.

EXAMPLE B. 12 Let $G$ be any additive abelian group. We make $G$ into a module over the $\operatorname{ring} \mathbf{Z}$ of integers by defining

$$
n g=\overbrace{g+g+\cdots+g}^{n \text { times }}, \quad 0 g=0, \quad(-n) g=-n g
$$

where $n$ is any positive integer.

EXAMPLE B. 13 Let $R$ be a ring and let $I$ be an ideal in $R$. Then $I$ may be viewed as a module over $R$.

EXAMPLE B. 14 Let $V$ be a vector space over a field $K$ and let $T: V \rightarrow V$ be a linear mapping. We make $V$ into a module over the ring $K[x]$ of polynomials over $K$ by defining $f(x) v=f(T)(v)$. The reader should check that a scalar multiplication has been defined.

Let $M$ be a module over $R$. An additive subgroup $N$ of $M$ is called a submodule of $M$ if $u \in N$ and $k \in R$ imply $k u \in N$. (Note that $N$ is then a module over $R$.)

Let $M$ and $M^{\prime}$ be $R$-modules. A mapping $T: M \rightarrow M^{\prime}$ is called a homomorphism (or: $R$-homomorphism or $R$-linear) if\\
(i) $T(u+v)=T(u)+T(v)$ and\\
(ii) $T(k u)=k T(u)$

for every $u, v \in M$ and every $k \in R$.

\section*{PROBLEMS}
\section*{Groups}
B.1. Determine whether each of the following systems forms a group $G$ :

(i) $G=$ set of integers, operation subtraction;

(ii) $G=\{1,-1\}$, operation multiplication;

(iii) $G=$ set of nonzero rational numbers, operation division;

(iv) $G=$ set of nonsingular $n \times n$ matrices, operation matrix multiplication;

(v) $G=\{a+b i: a, b \in \mathbf{Z}\}$, operation addition.

B.2. Show that in a group $G$ :

(i) the identity element of $G$ is unique;

(ii) each $a \in G$ has a unique inverse $a^{-1} \in G$;

(iii) $\left(a^{-1}\right)^{-1}=a$, and $(a b)^{-1}=b^{-1} a^{-1}$;

(iv) $a b=a c$ implies $b=c$, and $b a=c a$ implies $b=c$.

B.3. In a group $G$, the powers of $a \in G$ are defined by

$$
a^{0}=e, \quad a^{n}=a a^{n-1}, \quad a^{-n}=\left(a^{n}\right)^{-1}, \quad \text { where } n \in N
$$

Show that the following formulas hold for any integers $r, s, t \in \mathbf{Z}$ : (i) $a^{r} a^{s}=a^{r+s}$, (ii) $\left(a^{r}\right)^{s}=a^{r s}$, (iii) $\left(a^{r+s}\right)^{t}=a^{r s+s t}$.

B.4. Show that if $G$ is an abelian group, then $(a b)^{n}=a^{n} b^{n}$ for any $a, b \in G$ and any integer $n \in \mathbf{Z}$.

B.5. Suppose $G$ is a group such that $(a b)^{2}=a^{2} b^{2}$ for every $a, b \in G$. Show that $G$ is abelian.

B.6. Suppose $H$ is a subset of a group $G$. Show that $H$ is a subgroup of $G$ if and only if (i) $H$ is nonempty, and (ii) $a, b \in H$ implies $a b^{-1} \in H$.

B.7. Prove that the intersection of any number of subgroups of $G$ is also a subgroup of $G$.

B.8. Show that the set of all powers of $a \in G$ is a subgroup of $G$; it is called the cyclic group generated by $a$.

B.9. A group $G$ is said to be cyclic if $G$ is generated by some $a \in G$; that is, $G=\left(a^{n}: n \in Z\right)$. Show that every subgroup of a cyclic group is cyclic.

B.10. Suppose $G$ is a cyclic subgroup. Show that $G$ is isomorphic to the set $\mathbf{Z}$ of integers under addition or to the set $\mathbf{Z}_{n}$ (of the integers module $n$ ) under addition.

B.11. Let $H$ be a subgroup of $G$. Show that the right (left) cosets of $H$ partition $G$ into mutually disjoint subsets.

B.12. The order of a group $G$, denoted by $|G|$, is the number of elements of $G$. Prove Lagrange's theorem: If $H$ is a subgroup of a finite group $G$, then $|H|$ divides $|G|$.

B.13. Suppose $|G|=p$ where $p$ is prime. Show that $G$ is cyclic.

B.14. Suppose $H$ and $N$ are subgroups of $G$ with $N$ normal. Show that (i) $H N$ is a subgroup of $G$ and (ii) $H \cap N$ is a normal subgroup of $G$.

B.15. Let $H$ be a subgroup of $G$ with only two right (left) cosets. Show that $H$ is a normal subgroup of $G$.

B.16. Prove Theorem B.1: Let $H$ be a normal subgroup of $G$. Then the cosets of $H$ in $G$ form a group $G / H$ under coset multiplication.

B.17. Suppose $G$ is an abelian group. Show that any factor group $G / H$ is also abelian.

B.18. Let $f: G \rightarrow G^{\prime}$ be a group homomorphism. Show that

(i) $f(e)=e^{\prime}$ where $e$ and $e^{\prime}$ are the identity elements of $G$ and $G^{\prime}$, respectively;

(ii) $f\left(a^{-1}\right)=f(a)^{-1}$ for any $a \in G$.

B.19. Prove Theorem B.2: Let $f: G \rightarrow G^{\prime}$ be a group homomorphism with kernel $K$. Then $K$ is a normal subgroup of $G$, and the quotient group $G / K$ is isomorphic to the image of $f$.

B.20. Let $G$ be the multiplicative group of complex numbers $z$ such that $|z|=1$, and let $\mathbf{R}$ be the additive group of real numbers. Prove that $G$ is isomorphic to $\mathbf{R} / \mathbf{Z}$.

B.21. For a fixed $g \in G$, let $\hat{g}: G \rightarrow G$ be defined by $\hat{g}(a)=g^{-1} a g$. Show that $G$ is an isomorphism of $G$ onto $G$.

B.22. Let $G$ be the multiplicative group of $n \times n$ nonsingular matrices over $\mathbf{R}$. Show that the mapping $A \mapsto|A|$ is a homomorphism of $G$ into the multiplicative group of nonzero real numbers.

B.23. Let $G$ be an abelian group. For a fixed $n \in \mathbf{Z}$, show that the map $a \mapsto a^{n}$ is a homomorphism of $G$ into $G$.

B.24. Suppose $H$ and $N$ are subgroups of $G$ with $N$ normal. Prove that $H \cap N$ is normal in $H$ and $H /(H \cap N)$ is isomorphic to $H N / N$.

\section*{Rings}
B.25. Show that in a ring $R$ :

(i) $a \cdot 0=0 \cdot a=0$, (ii) $a(-b)=(-a) b=-a b$, (iii) $(-a)(-b)=a b$.

B.26. Show that in a ring $R$ with a unit element: (i) $(-1) a=-a$, (ii) $(-1)(-1)=1$.

B.27. Let $R$ be a ring. Suppose $a^{2}=a$ for every $a \in R$. Prove that $R$ is a commutative ring. (Such a ring is called a Boolean ring.)

B.28. Let $R$ be a ring with a unit element. We make $R$ into another $\operatorname{ring} \hat{R}$ by defining $a \oplus b=a+b+1$ and $a \cdot b=a b+a+b$. (i) Verify that $\hat{R}$ is a ring. (ii) Determine the 0 -element and 1 -element of $\hat{R}$.

B.29. Let $G$ be any (additive) abelian group. Define a multiplication in $G$ by $a \cdot b=0$. Show that this makes $G$ into a ring.

B.30. Prove Theorem B.3: Let $I$ be a (two-sided) ideal in a ring $R$. Then the cosets $(a+I \mid a \in R)$ form a ring under coset addition and coset multiplication.

B.31. Let $I_{1}$ and $I_{2}$ be ideals in $R$. Prove that $I_{1}+I_{2}$ and $I_{1} \cap I_{2}$ are also ideals in $R$.

B.32. Let $R$ and $R^{\prime}$ be rings. A mapping $f: R \rightarrow R^{\prime}$ is called a homomorphism (or: ring homomorphism) if (i) $f(a+b)=f(a)+f(b)$ and (ii) $f(a b)=f(a) f(b)$, for every $a, b \in R$. Prove that if $f: R \rightarrow R^{\prime}$ is a homomorphism, then the set $K=\{r \in R \mid f(r)=0\}$ is an ideal in $R$. (The set $K$ is called the kernel of $f$.)

\section*{Integral Domains and Fields}
B.33. Prove that in an integral domain $D$, if $a b=a c, a \neq 0$, then $b=c$.

B.34. Prove that $F=\{a+b \sqrt{2} \mid a, b$ rational $\}$ is a field.

B.35. Prove that $D=\{a+b \sqrt{2} \mid a, b$ integers $\}$ is an integral domain but not a field.

B.36. Prove that a finite integral domain $D$ is a field.

B.37. Show that the only ideals in a field $K$ are $\{0\}$ and $K$.

B.38. A complex number $a+b i$ where $a, b$ are integers is called a Gaussian integer. Show that the set $G$ of Gaussian integers is an integral domain. Also show that the units in $G$ are $\pm 1$ and $\pm i$.

B.39. Let $D$ be an integral domain and let $I$ be an ideal in $D$. Prove that the factor ring $D / I$ is an integral domain if and only if $I$ is a prime ideal. (An ideal $I$ is prime if $a b \in I$ implies $a \in I$ or $b \in I$.)

B.40. Consider the integral domain $D=\{a+b \sqrt{13} \mid a, b$ integers $\}$ (see Example B.11). If $\alpha=a+b \sqrt{13}$, we define $N(\alpha)=a^{2}-13 b^{2}$. Prove: (i) $N(\alpha \beta)=N(\alpha) N(\beta)$; (ii) $\alpha$ is a unit if and only if $N(\alpha)= \pm 1$; (iii) the units of $D$ are $\pm 1,18 \pm 5 \sqrt{13}$ and $-18 \pm 5 \sqrt{13}$; (iv) the numbers $2,3-\sqrt{13}$ and $-3-\sqrt{13}$ are irreducible.

\section*{Modules}
B.41. Let $M$ be an $R$-module and let $A$ and $B$ be submodules of $M$. Show that $A+B$ and $A \cap B$ are also submodules of $M$.

B.42. Let $M$ be an $R$-module with submodule $N$. Show that the cosets $\{u+N: u \in M\}$ form an $R$-module under coset addition and scalar multiplication defined by $r(u+N)=r u+N$. (This module is denoted by $M / N$ and is called the quotient module.)

B.43. Let $M$ and $M^{\prime}$ be $R$-modules and let $f: M \rightarrow M^{\prime}$ be an $R$-homomorphism. Show that the set $K=\{u \in M: f(u)=0\}$ is a submodule of $f$. (The set $K$ is called the kernel of $f$.)

B.44. Let $M$ be an $R$-module and let $E(M)$ denote the set of all $R$-homomorphism of $M$ into itself. Define the appropriate operations of addition and multiplication in $E(M)$ so that $E(M)$ becomes a ring.

\section*{APPENDIX C}
\section*{Polynomials over a Field}
\section*{C. 1 Introduction}
We will investigate polynomials over a field $K$ and show that they have many properties that are analogous to properties of the integers. These results play an important role in obtaining canonical forms for a linear operator $T$ on a vector space $V$ over $K$.

\section*{C. 2 Ring of Polynomials}
Let $K$ be a field. Formally, a polynomial of $f$ over $K$ is an infinite sequence of elements from $K$ in which all except a finite number of them are 0 :

$$
f=\left(\ldots, 0, a_{n}, \ldots, a_{1}, a_{0}\right)
$$

(We write the sequence so that it extends to the left instead of to the right.) The entry $a_{k}$ is called the $k$ th coefficient of $f$. If $n$ is the largest integer for which $a_{n} \neq 0$, then we say that the degree of $f$ is $n$, written

$\operatorname{deg} f=n$

We also call $a_{n}$ the leading coefficient of $f$, and if $a_{n}=1$ we call $f$ a monic polynomial. On the other hand, if every coefficient of $f$ is 0 then $f$ is called the zero polynomial, written $f=0$. The degree of the zero polynomial is not defined.

Now if $g$ is another polynomial over $K$, say

$$
g=\left(\ldots, 0, b_{m}, \ldots, b_{1}, b_{0}\right)
$$

then the $\operatorname{sum} f+g$ is the polynomial obtained by adding corresponding coefficients. That is, if $m \leq n$, then

$$
f+g=\left(\ldots, 0, a_{n}, \ldots, a_{m}+b_{m}, \ldots, a_{1}+b_{1}, a_{0}+b_{0}\right)
$$

Furthermore, the product $f g$ is the polynomial

$$
f g=\left(\ldots, 0, a_{n} b_{m}, \ldots, a_{1} b_{0}+a_{0} b_{1}, a_{0} b_{0}\right)
$$

that is, the $k$ th coefficient $c_{k}$ of $f g$ is

$$
c_{k}=\sum_{t=0}^{k} a_{1} b_{k-1}=a_{0} b_{k}+a_{1} b_{k-1}+\cdots+a_{k} b_{0}
$$

The following theorem applies.

THEOREM C.1: $\quad$ The set $P$ of polynomials over a field $K$ under the above operations of addition and multiplication forms a commutative ring with a unit element and with no zero divisors-an integral domain. If $f$ and $g$ are nonzero polynomials in $P$, then $\operatorname{deg}(f g)=(\operatorname{deg} f)(\operatorname{deg} g)$.

\section*{Notation}
We identify the scalar $a_{0} \in K$ with the polynomial

$$
a_{0}=\left(\ldots, 0, a_{0}\right)
$$

We also choose a symbol, say $t$, to denote the polynomial

$$
t=(\ldots, 0,1,0)
$$

We call the symbol $t$ an indeterminant. Multiplying $t$ with itself, we obtain

$$
t^{2}=(\ldots, 0,1,0,0), \quad t^{3}=(\ldots, 0,1,0,0,0), \ldots
$$

Thus, the above polynomial $f$ can be written uniquely in the usual form

$$
f=a_{n} t^{n}+\cdots+a_{s} t+a_{0}
$$

When the symbol $t$ is selected as the indeterminant, the ring of polynomials over $K$ is denoted by

$K[t]$

and a polynomial $f$ is frequently denoted by $f(t)$.

We also view the field $K$ as a subset of $K[t]$ under the above identification. This is possible because the operations of addition and multiplication of elements of $K$ are preserved under this identification:

$$
\begin{gathered}
\left(\ldots, 0, a_{0}\right)+\left(\ldots, 0, b_{0}\right)=\left(\ldots, 0, a_{0}+b_{0}\right) \\
\left(\ldots, 0, a_{0}\right) \cdot\left(\ldots, 0, b_{0}\right)=\left(\ldots, 0, a_{0} b_{0}\right)
\end{gathered}
$$

We remark that the nonzero elements of $K$ are the units of the ring $K[t]$.

We also remark that every nonzero polynomial is an associate of a unique monic polynomial. Hence, if $d$ and $d^{\prime}$ are monic polynomials for which $d$ divides $d^{\prime}$ and $d^{\prime}$ divides $d$, then $d=d^{\prime}$. (A polynomial $g$ divides a polynomial $f$ if there is a polynomial $h$ such that $f=h g$.)

\section*{C. 3 Divisibility}
The following theorem formalizes the process known as "long division."

THEOREM C. 2 (Division Algorithm): Let $f$ and $g$ be polynomials over a field $K$ with $g \neq 0$. Then there exist polynomials $q$ and $r$ such that

$$
f=q g+r
$$

where either $r=0$ or $\operatorname{deg} r<\operatorname{deg} g$.

Proof: If $f=0$ or if $\operatorname{deg} f<\operatorname{deg} g$, then we have the required representation

$$
f=0 g+f
$$

Now suppose $\operatorname{deg} f \geq \operatorname{deg} g$, say

$$
f=a_{n} t^{n}+\cdots+a_{1} t+a_{0} \quad \text { and } \quad g=b_{m} t^{m}+\cdots+b_{1} t+b_{0}
$$

where $a_{n}, b_{m} \neq 0$ and $n \geq m$. We form the polynomial


\begin{equation*}
f_{1}=f-\frac{a_{n}}{b_{m}} t^{n-m} g \tag{1}
\end{equation*}


Then $\operatorname{deg} f_{1}<\operatorname{deg} f$. By induction, there exist polynomials $q_{1}$ and $r$ such that

$$
f_{1}=q_{1} g+r
$$

where either $r=0$ or $\operatorname{deg} r<\operatorname{deg} g$. Substituting this into (1) and solving for $f$,

$$
f=\left(q_{1}+\frac{a_{n}}{b_{m}} t^{n-m}\right) g+r
$$

which is the desired representation.

THEOREM C.3: The ring $K[t]$ of polynomials over a field $K$ is a principal ideal ring. If $I$ is an ideal in $K[t]$, then there exists a unique monic polynomial $d$ that generates $I$, such that $d$ divides every polynomial $f \in I$.

Proof. Let $d$ be a polynomial of lowest degree in $I$. Because we can multiply $d$ by a nonzero scalar and still remain in $I$, we can assume without loss in generality that $d$ is a monic polynomial. Now suppose $f \in I$. By Theorem C. 2 there exist polynomials $q$ and $r$ such that

$$
f=q d+r \text { where either } r=0 \text { or } \operatorname{deg} r<\operatorname{deg} d
$$

Now $f, d \in I$ implies $q d \in I$, and hence, $r=f-q d \in I$. But $d$ is a polynomial of lowest degree in $I$. Accordingly, $r=0$ and $f=q d$; that is, $d$ divides $f$. It remains to show that $d$ is unique. If $d^{\prime}$ is another monic polynomial that generates $I$, then $d$ divides $d^{\prime}$ and $d^{\prime}$ divides $d$. This implies that $d=d^{\prime}$, because $d$ and $d^{\prime}$ are monic. Thus, the theorem is proved.

THEOREM C.4: Let $f$ and $g$ be nonzero polynomials in $K[t]$. Then there exists a unique monic polynomial $d$ such that

(i) $d$ divides $f$ and $g$; and (ii) $d^{\prime}$ divides $f$ and $g$, then $d^{\prime}$ divides $d$.

DEFINITION: The above polynomial $d$ is called the greatest common divisor of $f$ and $g$. If $d=1$, then $f$ and $g$ are said to be relatively prime.

Proof of Theorem C.4. The set $I=\{m f+n g \mid m, n \in K[t]\}$ is an ideal. Let $d$ be the monic polynomial that generates $I$. Note $f, g \in I$; hence, $d$ divides $f$ and $g$. Now suppose $d^{\prime}$ divides $f$ and $g$. Let $J$ be the ideal generated by $d^{\prime}$. Then $f, g \in J$, and hence, $I \subset J$. Accordingly, $d \in J$ and so $d^{\prime}$ divides $d$ as claimed. It remains to show that $d$ is unique. If $d_{1}$ is another (monic) greatest common divisor of $f$ and $g$, then $d$ divides $d_{1}$ and $d_{1}$ divides $d$. This implies that $d=d_{1}$ because $d$ and $d_{1}$ are monic. Thus, the theorem is proved.

COROLLARY C.5: Let $d$ be the greatest common divisor of the polynomials $f$ and $g$. Then there exist polynomials $m$ and $n$ such that $d=m f+n g$. In particular, if $f$ and $g$ are relatively prime, then there exist polynomials $m$ and $n$ such that $m f+n g=1$.

The corollary follows directly from the fact that $d$ generates the ideal

$$
I=\{m f+n g \mid m, n \in K[t]\}
$$

\section*{C. 4 Factorization}
A polynomial $p \in K[t]$ of positive degree is said to be irreducible if $p=f g$ implies $f$ or $g$ is a scalar.

LEMMA C.6: Suppose $p \in K[t]$ is irreducible. If $p$ divides the product $f g$ of polynomials $f, g \in K[t]$, then $p$ divides $f$ or $p$ divides $g$. More generally, if $p$ divides the product of $n$ polynomials $f_{1} f_{2} \ldots f_{n}$, then $p$ divides one of them.

Proof. Suppose $p$ divides $f g$ but not $f$. Because $p$ is irreducible, the polynomials $f$ and $p$ must then be relatively prime. Thus, there exist polynomials $m, n \in K[t]$ such that $m f+n p=1$. Multiplying this


\end{document}