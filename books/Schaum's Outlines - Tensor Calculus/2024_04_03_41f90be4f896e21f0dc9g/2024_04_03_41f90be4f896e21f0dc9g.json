[[{"all": ["\\section*{Chapter 1}", "\\section*{The Einstein Summation Convention}\n\\subsection*{1.1 INTRODUCTION}\nA study of tensor calculus requires a certain amount of background material that may seem unimportant in itself, but without which one could not proceed very far. Included in that prerequisite material is the topic of the present chapter, the summation convention. As the reader proceeds to later chapters he or she will see that it is this convention which makes the results of tensor analysis surveyable.\n\n\\subsection*{1.2 REPEATED INDICES IN SUMS}\nA certain notation introduced by Einstein in his development of the Theory of Relativity streamlines many common algebraic expressions. Instead of using the traditional sigma for sums, the strategy is to allow the repeated subscript to become itself the designation for the summation. Thus,\n\n$$\na_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}+\\cdots+a_{n} x_{n} \\equiv \\sum_{i=1}^{n} a_{i} x_{i}\n$$\n\nbecomes just $a_{i} x_{i}$, where $1 \\leqq i \\leqq n$ is adopted as the universal range for summation.\n\nEXAMPLE 1.1 The expression $a_{i j} x_{k}$ does not indicate summation, but both $a_{i i} x_{k}$ and $a_{i j} x_{j}$ do so over the respective ranges $1 \\leqq i \\leqq n$ and $1 \\leqq j \\leqq n$. If $n=4$, then\n\n$$\n\\begin{aligned}\n& a_{\\mathrm{ii}} x_{k} \\equiv a_{11} x_{k}+a_{22} x_{k}+a_{33} x_{k}+a_{44} x_{k} \\\\\n& a_{i j} x_{j} \\equiv a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}+a_{i 4} x_{4}\n\\end{aligned}\n$$\n\n\\section*{Free and Dummy Indices}\nIn Example 1.1, the expression $a_{i j} x_{j}$ involves two sorts of indices. The index of summation, $j$, which ranges over the integers $1,2,3, \\ldots, n$, cannot be preempted. But at the same time, it is clear that the use of the particular character $j$ is inessential; e.g., the expressions $a_{i r} x_{r}$ and $a_{i v} x_{v}$ represent exactly the same sum as $a_{i j} x_{j}$ does. For this reason, $j$ is called a dummy index. The index $i$, which may take on any particular value $1,2,3, \\ldots, n$ independently, is called a free index. Note that, although we call the index $i$ \"free\" in the expression $a_{i j} x_{j}$, that \"freedom\" is limited in the sense that generally, unless $i=k$,\n\n$$\na_{i \\mathrm{j}} x_{\\mathrm{j}} \\neq a_{k \\mathrm{j}} x_{\\mathrm{j}}\n$$\n\nEXAMPLE 1.2 If $n=3$, write down explicitly the equations represented by the expression $y_{i}=a_{i r} x_{r}$.\n\nHolding $i$ fixed and summing over $r=1,2,3$ yields\n\n$$\ny_{i}=a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}\n$$\n\nNext, setting the free index $i=1,2,3$ leads to three separate equations:\n\n$$\n\\begin{aligned}\n& y_{1}=a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3} \\\\\n& y_{2}=a_{21} x_{1}+a_{22} x_{2}+a_{23} x_{3} \\\\\n& y_{3}=a_{31} x_{1}+a_{32} x_{2}+a_{33} x_{3}\n\\end{aligned}\n$$\n\n\\section*{Einstein Summation Convention}\nAny expression involving a twice-repeated index (occurring twice as a subscript, twice as a superscript, or once as a subscript and once as a superscript) shall automatically stand for its sum\\\\\nover the values $1,2,3, \\ldots, n$ of the repeated index. Unless explicitly stated otherwise, the single exception to this rule is the character $n$, which represents the range of all summations.\n\nRemark 1: Any free index in an expression shall have the same range as summation indices, unless stated otherwise.\n\nRemark 2: No index may occur more than twice in any given expression.\n\nEXAMPLE 1.3 (a) According to Remark 2, an expression like $a_{i i} x_{i}$ is without meaning. (b) The meaningless expression $a_{i}^{i} x_{i} x_{i}$ might be presumed to represent $a_{j}^{i}\\left(x_{i}\\right)^{2}$, which is meaningful. (c) An expression of the form $a_{i}\\left(x_{i}+y_{i}\\right)$ is considered well-defined, for it is obtained by composition of the meaningful expressions $a_{i} z_{i}$ and $x_{i}+y_{i}=z_{i}$. In other words, the index $i$ is regarded as occuring once in the term $\\left(x_{i}+y_{i}\\right)$.\n\n\\subsection*{1.3 DOUBLE SUMS}\nAn expression can involve more than one summation index. For example, $a_{i j} x_{i} y_{j}$ indicates a summation taking place on both $i$ and $j$ simultaneously. If an expression has two summation (dummy) indices, there will be a total of $n^{2}$ terms in the sum; if there are three indices, there will be $n^{3}$ terms; and so on. The expansion of $a_{i j} x_{i} y_{j}$ can be arrived at logically by first summing over $i$, then over $j$ :\n\n$$\n\\begin{array}{cc}\na_{i j} x_{i} y_{j}=a_{1 j} x_{1} y_{j}+a_{2 j} x_{2} y_{j}+a_{3 j} x_{3} y_{j}+\\cdots+a_{n j} x_{n} y_{j} & \\text { [summed over } i] \\\\\n=\\left(a_{11} x_{1} y_{1}+a_{12} x_{1} y_{2}+\\cdots+a_{1 n} x_{1} y_{n}\\right) & \\text { [summed over } j] \\\\\n+\\left(a_{21} x_{2} y_{1}+a_{22} x_{2} y_{2}+\\cdots+a_{2 n} x_{2} y_{n}\\right) & \\\\\n+\\left(a_{31} x_{3} y_{1}+a_{32} x_{3} y_{2}+\\cdots+a_{3 n} x_{3} y_{n}\\right) & \\\\\n\\left.\\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots+a_{n n} x_{n} y_{n}\\right) &\n\\end{array}\n$$\n\nThe result is the same if one sums over $j$ first, and then over $i$.\n\nEXAMPLE 1.4 If $n=2$, the expression $y_{i}=c_{i}^{r} a_{r s} x_{s}$ stands for the two equations:\n\n$$\n\\begin{aligned}\n& y_{1}=c_{1}^{1} a_{11} x_{1}+c_{1}^{2} a_{21} x_{1}+c_{1}^{1} a_{12} x_{2}+c_{1}^{2} a_{22} x_{2} \\\\\n& y_{2}=c_{2}^{1} a_{11} x_{1}+c_{2}^{2} a_{21} x_{1}+c_{2}^{1} a_{12} x_{2}+c_{2}^{2} a_{22} x_{2}\n\\end{aligned}\n$$\n\n\\subsection*{1.4 SUBSTITUTIONS}\nSuppose it is required to substitute $y_{i}=a_{i j} x_{j}$ in the equation $Q=b_{i j} y_{i} x_{j}$. Disregard of Remark 2 above would lead to an absurd expression like $Q=b_{i j} a_{i j} x_{j} x_{j}$. The correct procedure is first to identify any dummy indices in the expression to be substituted that coincide with indices occurring in the main expression. Changing these dummy indices to characters not found in the main expression, one may then carry out the substitution in the usual fashion.\n\nSTEP $1 Q=b_{i j} y_{i} x_{j}, \\quad y_{i}=a_{i j} x_{j} \\quad$ [dummy index $j$ is duplicated]\n\nSTEP $2 y_{i}=a_{i r} x_{r} \\quad$ [change dummy index from $j$ to $r$ ]\n\nSTEP $3 Q=b_{i j}\\left(a_{i r} x_{r}\\right) x_{j}=a_{i r} b_{i j} x_{r} x_{j} \\quad$ [substitute and rearrange]\n\nEXAMPLE 1.5 If $y_{i}=a_{i j} x_{j}$, express the quadratic form $Q=g_{i j} y_{i} y_{j}$ in terms of the $x$-variables.\n\nFirst write: $y_{i}=a_{i r} x_{r}, y_{j}=a_{j s} x_{s}$. Then, by substitution,\n\n$$\nQ=g_{i j}\\left(a_{i r} x_{r}\\right)\\left(a_{j s} x_{s}\\right)=g_{i j} a_{i r} a_{j s} x_{r} x_{s}\n$$\n\nor $Q=h_{r s} x_{r} x_{s}$, where $h_{r s} \\equiv g_{i j} a_{i r} a_{j s}$.\n\n\\subsection*{1.5 KRONECKER DELTA AND ALGEBRAIC MANIPULATIONS}\nA much used symbol in tensor calculus has the effect of annihilating the \"off-diagonal\" terms in a double summation.\n\n\\section*{Kronecker Delta}\n\\[\n\\delta_{i j} \\equiv \\delta_{j}^{i} \\equiv \\delta^{i j} \\equiv \\begin{cases}1 & i=j  \\tag{1.1}\\\\ 0 & i \\neq j\\end{cases}\n\\]\n\nClearly, $\\delta_{i j}=\\delta_{j i}$ for all $i, j$.\n\nEXAMPLE 1.6 If $n=3$,\n\n$$\n\\begin{aligned}\n\\delta_{i j} x_{i} x_{j} & =1 x_{1} x_{1}+0 x_{1} x_{2}+0 x_{1} x_{3}+0 x_{2} x_{1}+1 x_{2} x_{2}+0 x_{2} x_{3}+0 x_{3} x_{1}+0 x_{3} x_{2}+1 x_{3} x_{3} \\\\\n& =\\left(x_{1}\\right)^{2}+\\left(x_{2}\\right)^{2}+\\left(x_{3}\\right)^{2}=x_{i} x_{i}\n\\end{aligned}\n$$\n\nIn general, $\\delta_{i j} x_{i} x_{j}=x_{i} x_{i}$ and $\\delta_{j}^{r} a_{i r} x_{i}=a_{i j} x_{i}$.\n\nEXAMPLE 1.7 Suppose that $T^{i}=g_{r}^{i} a_{r s} y_{s}$ and $y_{i}=b_{i r} x_{r}$. If further, $a_{i r} b_{r j}=\\delta_{i j}$, find $T^{i}$ in terms of the $x_{r}$.\n\nFirst write $y_{s}=b_{s t} x_{t}$. Then, by substitution,\n\n$$\nT^{i}=g_{r}^{i} a_{r s} b_{s t} x_{t}=g_{r}^{i} \\delta_{r t} x_{t}=g_{r}^{i} x_{r}\n$$\n\n\\section*{Algebra and the Summation Convention}\nCertain routine manipulations in tensor calculus can be easily justified by properties of ordinary sums. However, some care is warranted. For instance, the identity (1.2) below not only involves the distributive law for real numbers, $a(x+y) \\equiv a x+a y$, but also requires a rearrangement of terms utilizing the associative and commutative laws. At least a mental verification of such operations must be made, if false results are to be avoided.\n\nEXAMPLE 1.8 The following nonidentities should be carefully noted:\n\n$$\n\\begin{gathered}\na_{i j}\\left(x_{i}+y_{j}\\right) \\not \\equiv a_{i j} x_{i}+a_{i j} y_{j} \\\\\na_{i j} x_{i} y_{j} \\neq a_{i j} y_{i} x_{j} \\\\\n\\left(a_{i j}+a_{j i}\\right) x_{i} y_{j} \\neq 12 a_{i j} x_{i} y_{j}\n\\end{gathered}\n$$\n\nListed below are several valid identities; they, and others like them, will be used repeatedly from now on.\n\n\n\\begin{gather*}\na_{i j}\\left(x_{j}+y_{j}\\right) \\equiv a_{i j} x_{j}+a_{i j} y_{j}  \\tag{1:2}\\\\\na_{i j} x_{i} y_{j} \\equiv a_{i j} y_{j} x_{i}  \\tag{1.3}\\\\\na_{i j} x_{i} x_{j} \\equiv a_{j i} x_{i} x_{j}  \\tag{1.4}\\\\\n\\left(a_{i j}+a_{j i}\\right) x_{i} x_{j} \\equiv 2 a_{i j} x_{i} x_{j}  \\tag{1.5}\\\\\n\\left(a_{i j}-a_{j i}\\right) x_{i} x_{j} \\equiv 0 \\tag{1.6}\n\\end{gather*}\n\n\n\\section*{Solved Problems}\n\\section*{REPEATED INDICES}\n1.1 Use the summation convention to write the following, and assign the value of $n$ in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{41} b_{14}$\n\n(b) $a_{11} b_{11}+a_{12} b_{12}+a_{13} b_{13}+a_{14} b_{14}+a_{15} b_{15}+a_{16} b_{16}$\n\n(c) $c_{11}^{i}+c_{22}^{i}+c_{33}^{i}+c_{44}^{i}+c_{55}^{i}+c_{66}^{i}+c_{77}^{i}+c_{88}^{i} \\quad(1 \\leqq i \\leqq 8)$\n\n(a) $a_{i 1} b_{1 i} \\quad(n=4) ;(b) a_{1 i} b_{1 i} \\quad(n=6) ;(c) c_{j j}^{i} \\quad(n=8)$.\n\n1.2 Use the summation convention to write each of the following systems, state which indices are free and which are dummy indices, and fix the value of $n$ :\n\n$$\n\\begin{array}{lc}\nc_{11} x_{1}+c_{12} x_{2}+c_{13} x_{3}=2 & \\text { (b) } a_{j}^{1} x_{1}+a_{j}^{2} x_{2}+a_{j}^{3} x_{3}+a_{j}^{4} x_{4}=b_{j} \\\\\nc_{21} x_{1}+c_{22} x_{2}+c_{23} x_{3}=-3 & (j=1,2) \\\\\nc_{31} x_{1}+c_{32} x_{2}+c_{33} x_{3}=5 &\n\\end{array}\n$$\n\n(a) Set $d_{1}=2, d_{2}=-3$, and $d_{3}=5$. Then one can write the system as $c_{i j} x_{j}=d_{i} \\quad(n=3)$. The free index is $i$ and the dummy index is $j$.\n\n(b) Here, the range of the free index does not match that of the dummy index $(n=4)$, and this fact must be indicated:\n\n$$\na_{j}^{i} x_{i}=b_{j} \\quad(j=1,2)\n$$\n\nThe free index is $j$ and the dummy index is $i$.\n\n1.3 Write out explicitly the summations\n\n$$\nc_{i}\\left(x_{i}+y_{i}\\right) \\quad c_{j} x_{j}+c_{k} y_{k}\n$$\n\nwhere $n=4$ for both, and compare the results.\n\n$$\n\\begin{aligned}\nc_{i}\\left(x_{i}+y_{i}\\right) & =c_{1}\\left(x_{1}+y_{1}\\right)+c_{2}\\left(x_{2}+y_{2}\\right)+c_{3}\\left(x_{3}+y_{3}\\right)+c_{4}\\left(x_{4}+y_{4}\\right) \\\\\n& =c_{1} x_{1}+c_{1} y_{1}+c_{2} x_{2}+c_{2} y_{2}+c_{3} x_{3}+c_{3} y_{3}+c_{4} x_{4}+c_{4} y_{4} \\\\\nc_{j} x_{j}+c_{k} y_{k} & =c_{1} x_{1}+c_{2} x_{2}+c_{3} x_{3}+c_{4} x_{4}+c_{1} y_{1}+c_{2} y_{2}+c_{3} y_{3}+c_{4} y_{4}\n\\end{aligned}\n$$\n\nThe two summations are identical except for the order in which the terms occur, constituting a special case of (1.2).\n\n\\section*{DOUBLE SUMS}\n1.4 If $n=3$, expand $Q=a^{i j} x_{i} x_{j}$.\n\n$$\n\\begin{aligned}\nQ & =a^{1 j} x_{1} x_{j}+a^{2 j} x_{2} x_{j}+a^{3 j} x_{3} x_{j} \\\\\n& =a^{11} x_{1} x_{1}+a^{12} x_{1} x_{2}+a^{13} x_{1} x_{3}+a^{21} x_{2} x_{1}+a^{22} x_{2} x_{2}+a^{23} x_{2} x_{3}+a^{31} x_{3} x_{1}+a^{32} x_{3} x_{2}+a^{33} x_{3} x_{3}\n\\end{aligned}\n$$\n\n1.5 Use the summation convention to write the following, and state the value of $n$ necessary in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{12} b_{21}+a_{22} b_{22}+a_{32} b_{23}+a_{13} b_{31}+a_{23} b_{32}+a_{33} b_{33}$\n\n?\n\n(b) \" $g_{11}^{1}+g_{12}^{1}+\\dot{g}_{21}^{1}+g_{22}^{1}+g_{11}^{2}+g_{12}^{2}+g_{21}^{2}+g_{22}^{2}$\\\\\n(a) $a_{i 1} b_{1 i}+a_{i 2} b_{2 i}+a_{i 3} b_{3 i} \\equiv a_{i j} b_{j i} \\quad(n=3)$.\n\n(b) Set $c_{i}=1$ for each $i \\quad(n=2)$. Then the expression may be written\n\n$$\n\\begin{aligned}\ng_{11}^{i} c_{i}+g_{12}^{i} c_{i}+g_{21}^{i} c_{i}+g_{22}^{i} c_{i} & =\\left(g_{11}^{i}+g_{12}^{i}+g_{21}^{i}+g_{22}^{i}\\right) c_{i} \\\\\n& =\\left(g_{j k}^{i} c_{j} c_{k}\\right) c_{i}=g_{j k}^{i} c_{i} c_{j} c_{k}\n\\end{aligned}\n$$\n\n1.6 If $n=2$, write out explicitly the triple summation $c_{r s t} x^{r} y^{s} z^{t}$.\n\nAny expansion technique that yields all $2^{3}=8$ terms will do. In this case we shall interpret the triplet rst as a three-digit integer, and list the terms in increasing order of that integer:\n\n$$\n\\begin{aligned}\n& c_{r s t} x^{r} y^{s} z^{t}=c_{111} x^{1} y^{1} z^{1}+c_{112} x^{1} y^{1} z^{2}+c_{121} x^{1} y^{2} z^{1}+c_{122} x^{1} y^{2} z^{2} \\\\\n& +c_{211} x^{2} y^{1} z^{1}+c_{212} x^{2} y^{1} z^{2}+c_{221} x^{2} y^{2} z^{1}+c_{222} x^{2} y^{2} z^{2}\n\\end{aligned}\n$$\n\n1.7 Show that $a_{i j} x_{i} x_{j}=0$ if $a_{i j} \\equiv i-j$.\n\nBecause, for all $i$ and $j, a_{i j}=-a_{j i}$ and $x_{i} x_{j}=x_{i} x_{i}$, the \"off-diagonal\" terms $a_{i j} x_{i} x_{j} \\quad(i<j$; no sum) and $a_{j i} x_{j} x_{i} \\quad\\left(j>i\\right.$; no sum) cancel in pairs, while the \"diagonal\" terms $a_{i i}\\left(x_{i}\\right)^{2}$ are zero to begin with. Thus the sum is zero.\n\nThe result also follows at once from (1.5).\n\n1.8 If the $a_{i j}$ are constants, calculate the partial derivative\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nReverting to $\\Sigma$-notation, we have:\n\n$$\n\\begin{aligned}\n\\sum_{i, j} a_{i j} x_{i} x_{j} & =\\sum_{\\substack{i \\neq k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i \\neq k \\\\\nj=k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj=k}} a_{i j} x_{i} x_{j} \\\\\n& =C+\\left(\\sum_{j \\neq k} a_{k j} x_{j}\\right) x_{k}+\\left(\\sum_{i \\neq k} a_{i k} x_{i}\\right) x_{k}+a_{k k}\\left(x_{k}\\right)^{2}\n\\end{aligned}\n$$\n\nwhere $C$ is independent of $x_{k}$. Differentiating with respect to $x_{k}$,\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(\\sum_{i, j} a_{i j} x_{i} x_{j}\\right) & =0+\\sum_{j \\neq k} a_{k j} x_{j}+\\sum_{i \\neq k} a_{i k} x_{i}+2 a_{k k} x_{k} \\\\\n& =\\sum_{j} a_{k j} x_{j}+\\sum_{i} a_{i k} x_{i}\n\\end{aligned}\n$$\n\nor, going back to the Einstein summation convention,\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)=a_{k i} x_{i}+a_{i k} x_{i}=\\left(a_{i k}+a_{k i}\\right) x_{i}\n$$\n\n\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.9 Express $b^{i j} y_{i} y_{j}$ in terms of $x$-variables, if $y_{i}=c_{i j} x_{j}$ and $b^{i j} c_{i k}=\\delta_{k}^{j}$.\n\n$$\nb^{i j} y_{i} y_{j}=b^{i j}\\left(c_{i r} x_{r}\\right)\\left(c_{j s} x_{s}\\right)=\\left(b^{i j} c_{i r}\\right) x_{r} c_{j s} x_{s}=\\delta^{i}{ }_{r} x_{r} c_{j s} x_{s}=x_{j} c_{j s} x_{s}=c_{i j} x_{i} x_{j}\n$$\n\n1.10 Rework Problem 1.8 by use of the product rule for differentiation and the fact that\n\n$$\n\\frac{\\partial x_{p}}{\\partial x_{q}}=\\delta_{p q}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right) & =a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i} x_{j}\\right)=a_{i j}\\left(x_{j} \\frac{\\partial x_{i}}{\\partial x_{k}}+x_{i} \\frac{\\partial x_{j}}{\\partial x_{k}}\\right) \\\\\n& =a_{i j}\\left(x_{j} \\delta_{i k}+x_{i} \\delta_{j k}\\right)=a_{k j} x_{j}+a_{i k} x_{i} \\\\\n& =\\left(a_{i k}+a_{k i}\\right) x_{i}\n\\end{aligned}\n$$\n\n1.11 If $a_{i j}=a_{j i}$ are constants, calculate\n\n$$\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nUsing Problem 1.8, we have\n\n$$\n\\begin{aligned}\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right) & =\\frac{\\partial}{\\partial x_{k}}\\left[\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\\right]=\\frac{\\partial}{\\partial x_{k}}\\left[\\left(a_{l j}+a_{j l}\\right) x_{j}\\right] \\\\\n& =\\frac{\\partial}{\\partial x_{k}}\\left(2 a_{i l} x_{i}\\right)=2 a_{i l} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i}\\right)=2 a_{i l} \\delta_{k i}=2 a_{k l}\n\\end{aligned}\n$$\n\n1.12 Consider a system of linear equations of the form $y^{i}=a^{i j} x_{j}$ and suppose that $\\left(b_{i j}\\right)$ is a matrix of numbers such that for all $i$ and $j, b_{i r} a^{r j}=\\delta_{i}^{j}$ [that is, the matrix $\\left(b_{i j}\\right)$ is the inverse of the matrix $\\left(a^{i j}\\right)$ ]. Solve the system for $x_{i}$ in terms of the $y^{j}$.\n\nMultiply both sides of the $i$ th equation by $b_{k i}$ and sum over $i$ :\n\n$$\nb_{k i} y^{i}=b_{k i} a^{i j} x_{j}=\\delta_{k}^{j} x_{j}=x_{k}\n$$\n\nor $x_{i}=b_{i j} y^{j}$.\n\n1.13 Show that, generally, $a_{i j k}\\left(x_{i}+y_{j}\\right) z_{k} \\neq a_{i j k} x_{i} z_{k}+a_{i j k} y_{j} z_{k}$.\n\nSimply observe that on the left side there are no free indices, but on the right, $j$ is free for the first term and $i$ is free for the second.\n\n1.14 Show that $c_{i j}\\left(x_{i}+y_{i}\\right) z_{j} \\equiv c_{i j} x_{i} z_{j}+c_{i j} y_{i} z_{j}$.\n\nLet us prove (1.2); the desired identity will then follow upon setting $a_{i j} \\equiv c_{j i}$.\n\n$$\n\\begin{aligned}\na_{i j} x_{j}+a_{i j} y_{j} & \\equiv \\sum_{j} a_{i j} x_{j}+\\sum_{j} a_{i j} y_{j}=\\sum_{j}\\left(a_{i j} x_{j}+a_{i j} y_{j}\\right) \\\\\n& =\\sum_{j} a_{i j}\\left(x_{j}+y_{j}\\right) \\equiv a_{i j}\\left(x_{j}+y_{j}\\right)\n\\end{aligned}\n$$\n\n\\section*{Supplementary Problems}\n1.15 Write out the expression $a_{i} b_{i}(n=6)$ in full.\n\n1.16 Write out the expression $R_{j k i}^{i} \\quad(n=4)$ in full. Which are free and which are dummy indices? How many summations are there?\n\n1.17 Evaluate $\\delta_{j}^{i} x_{i}$ ( $n$ arbitrary).\n\n1.18 For $n$ arbitrary, evaluate (a) $\\delta_{i i}$, (b) $\\delta_{i j} \\delta_{i j}$, (c) $\\delta_{i j} \\delta_{k}^{j} c_{i k}$.\n\n1.19 Use the summation convention to indicate $a_{13} b_{13}+a_{23} b_{23}+a_{33} b_{33}$, and state the value of $n$.\n\n1.20 Use the summation convention to indicate\n\n$$\na_{11}\\left(x_{1}\\right)^{2}+a_{12} x_{1} x_{2}+a_{13} x_{1} x_{3}+a_{21} x_{2} x_{1}+a_{22}\\left(x_{2}\\right)^{2}+a_{23} x_{2} x_{3}+a_{31} x_{3} x_{1}+a_{32} x_{3} x_{2}+a_{33}\\left(x_{3}\\right)^{2}\n$$\n\n1.21 Use the summation convention and free subscripts to indicate the following linear system, stating the value of $n$ :\n\n$$\n\\begin{aligned}\n& y_{1}=c_{11} x_{1}+c_{12} x_{2} \\\\\n& y_{2}=c_{21} x_{1}+c_{22} x_{2}\n\\end{aligned}\n$$\n\n1.22 Find the following partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}\\right) \\quad(k=1,2,3)\n$$\n\n1.23 Use the Kronecker delta to calculate the partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)\n$$\n\n1.24 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left[a_{i j} x_{i}\\left(x_{j}\\right)^{2}\\right]\n$$\n\nwhere the $a_{i j}$ are constants such that $a_{i j}=a_{j i}$.\n\n1.25 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j k} x_{i} x_{j} x_{k}\\right)\n$$\n\nwhere the $a_{i j k}$ are constants.\n\n1.26 Solve Problem 1.11 without the symmetry condition on $a_{i j}$.\n\n1.27 Evaluate: (a) $b_{j}^{i} y_{i}$ if $y_{i}=T_{i}^{j j}$, (b) $a_{i j} y_{j}$ if $y_{i}=b_{i j} x_{j}$, (c) $a_{i j k} y_{i} y_{j} y_{k}$ if $y_{i}=b_{i j} x_{j}$.\n\n1.28 If $\\varepsilon_{i}=1$ for all $i$, prove that\n\n(a) $\\left(a_{1}+a_{2}+\\cdots+a_{n}\\right)^{2} \\equiv \\varepsilon_{i} \\varepsilon_{j} a_{i} a_{j}$\n\n(b) $a_{i}\\left(1+x_{i}\\right) \\equiv a_{i} \\varepsilon_{i}+a_{i} x_{i}$\n\n(c) $a_{i j}\\left(x_{i}+x_{j}\\right) \\equiv 2 a_{i j} \\varepsilon_{i} x_{j}$ if $a_{i j}=a_{j i}$\n\n"], "lesson": "\\section*{Chapter 1}\n\\section*{The Einstein Summation Convention}\n\\subsection*{1.1 INTRODUCTION}\nA study of tensor calculus requires a certain amount of background material that may seem unimportant in itself, but without which one could not proceed very far. Included in that prerequisite material is the topic of the present chapter, the summation convention. As the reader proceeds to later chapters he or she will see that it is this convention which makes the results of tensor analysis surveyable.\n\n\\subsection*{1.2 REPEATED INDICES IN SUMS}\nA certain notation introduced by Einstein in his development of the Theory of Relativity streamlines many common algebraic expressions. Instead of using the traditional sigma for sums, the strategy is to allow the repeated subscript to become itself the designation for the summation. Thus,\n\n$$\na_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}+\\cdots+a_{n} x_{n} \\equiv \\sum_{i=1}^{n} a_{i} x_{i}\n$$\n\nbecomes just $a_{i} x_{i}$, where $1 \\leqq i \\leqq n$ is adopted as the universal range for summation.\n\nEXAMPLE 1.1 The expression $a_{i j} x_{k}$ does not indicate summation, but both $a_{i i} x_{k}$ and $a_{i j} x_{j}$ do so over the respective ranges $1 \\leqq i \\leqq n$ and $1 \\leqq j \\leqq n$. If $n=4$, then\n\n$$\n\\begin{aligned}\n& a_{\\mathrm{ii}} x_{k} \\equiv a_{11} x_{k}+a_{22} x_{k}+a_{33} x_{k}+a_{44} x_{k} \\\\\n& a_{i j} x_{j} \\equiv a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}+a_{i 4} x_{4}\n\\end{aligned}\n$$\n\n\\section*{Free and Dummy Indices}\nIn Example 1.1, the expression $a_{i j} x_{j}$ involves two sorts of indices. The index of summation, $j$, which ranges over the integers $1,2,3, \\ldots, n$, cannot be preempted. But at the same time, it is clear that the use of the particular character $j$ is inessential; e.g., the expressions $a_{i r} x_{r}$ and $a_{i v} x_{v}$ represent exactly the same sum as $a_{i j} x_{j}$ does. For this reason, $j$ is called a dummy index. The index $i$, which may take on any particular value $1,2,3, \\ldots, n$ independently, is called a free index. Note that, although we call the index $i$ \"free\" in the expression $a_{i j} x_{j}$, that \"freedom\" is limited in the sense that generally, unless $i=k$,\n\n$$\na_{i \\mathrm{j}} x_{\\mathrm{j}} \\neq a_{k \\mathrm{j}} x_{\\mathrm{j}}\n$$\n\nEXAMPLE 1.2 If $n=3$, write down explicitly the equations represented by the expression $y_{i}=a_{i r} x_{r}$.\n\nHolding $i$ fixed and summing over $r=1,2,3$ yields\n\n$$\ny_{i}=a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}\n$$\n\nNext, setting the free index $i=1,2,3$ leads to three separate equations:\n\n$$\n\\begin{aligned}\n& y_{1}=a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3} \\\\\n& y_{2}=a_{21} x_{1}+a_{22} x_{2}+a_{23} x_{3} \\\\\n& y_{3}=a_{31} x_{1}+a_{32} x_{2}+a_{33} x_{3}\n\\end{aligned}\n$$\n\n\\section*{Einstein Summation Convention}\nAny expression involving a twice-repeated index (occurring twice as a subscript, twice as a superscript, or once as a subscript and once as a superscript) shall automatically stand for its sum\\\\\nover the values $1,2,3, \\ldots, n$ of the repeated index. Unless explicitly stated otherwise, the single exception to this rule is the character $n$, which represents the range of all summations.\n\nRemark 1: Any free index in an expression shall have the same range as summation indices, unless stated otherwise.\n\nRemark 2: No index may occur more than twice in any given expression.\n\nEXAMPLE 1.3 (a) According to Remark 2, an expression like $a_{i i} x_{i}$ is without meaning. (b) The meaningless expression $a_{i}^{i} x_{i} x_{i}$ might be presumed to represent $a_{j}^{i}\\left(x_{i}\\right)^{2}$, which is meaningful. (c) An expression of the form $a_{i}\\left(x_{i}+y_{i}\\right)$ is considered well-defined, for it is obtained by composition of the meaningful expressions $a_{i} z_{i}$ and $x_{i}+y_{i}=z_{i}$. In other words, the index $i$ is regarded as occuring once in the term $\\left(x_{i}+y_{i}\\right)$.\n\n\\subsection*{1.3 DOUBLE SUMS}\nAn expression can involve more than one summation index. For example, $a_{i j} x_{i} y_{j}$ indicates a summation taking place on both $i$ and $j$ simultaneously. If an expression has two summation (dummy) indices, there will be a total of $n^{2}$ terms in the sum; if there are three indices, there will be $n^{3}$ terms; and so on. The expansion of $a_{i j} x_{i} y_{j}$ can be arrived at logically by first summing over $i$, then over $j$ :\n\n$$\n\\begin{array}{cc}\na_{i j} x_{i} y_{j}=a_{1 j} x_{1} y_{j}+a_{2 j} x_{2} y_{j}+a_{3 j} x_{3} y_{j}+\\cdots+a_{n j} x_{n} y_{j} & \\text { [summed over } i] \\\\\n=\\left(a_{11} x_{1} y_{1}+a_{12} x_{1} y_{2}+\\cdots+a_{1 n} x_{1} y_{n}\\right) & \\text { [summed over } j] \\\\\n+\\left(a_{21} x_{2} y_{1}+a_{22} x_{2} y_{2}+\\cdots+a_{2 n} x_{2} y_{n}\\right) & \\\\\n+\\left(a_{31} x_{3} y_{1}+a_{32} x_{3} y_{2}+\\cdots+a_{3 n} x_{3} y_{n}\\right) & \\\\\n\\left.\\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots+a_{n n} x_{n} y_{n}\\right) &\n\\end{array}\n$$\n\nThe result is the same if one sums over $j$ first, and then over $i$.\n\nEXAMPLE 1.4 If $n=2$, the expression $y_{i}=c_{i}^{r} a_{r s} x_{s}$ stands for the two equations:\n\n$$\n\\begin{aligned}\n& y_{1}=c_{1}^{1} a_{11} x_{1}+c_{1}^{2} a_{21} x_{1}+c_{1}^{1} a_{12} x_{2}+c_{1}^{2} a_{22} x_{2} \\\\\n& y_{2}=c_{2}^{1} a_{11} x_{1}+c_{2}^{2} a_{21} x_{1}+c_{2}^{1} a_{12} x_{2}+c_{2}^{2} a_{22} x_{2}\n\\end{aligned}\n$$\n\n\\subsection*{1.4 SUBSTITUTIONS}\nSuppose it is required to substitute $y_{i}=a_{i j} x_{j}$ in the equation $Q=b_{i j} y_{i} x_{j}$. Disregard of Remark 2 above would lead to an absurd expression like $Q=b_{i j} a_{i j} x_{j} x_{j}$. The correct procedure is first to identify any dummy indices in the expression to be substituted that coincide with indices occurring in the main expression. Changing these dummy indices to characters not found in the main expression, one may then carry out the substitution in the usual fashion.\n\nSTEP $1 Q=b_{i j} y_{i} x_{j}, \\quad y_{i}=a_{i j} x_{j} \\quad$ [dummy index $j$ is duplicated]\n\nSTEP $2 y_{i}=a_{i r} x_{r} \\quad$ [change dummy index from $j$ to $r$ ]\n\nSTEP $3 Q=b_{i j}\\left(a_{i r} x_{r}\\right) x_{j}=a_{i r} b_{i j} x_{r} x_{j} \\quad$ [substitute and rearrange]\n\nEXAMPLE 1.5 If $y_{i}=a_{i j} x_{j}$, express the quadratic form $Q=g_{i j} y_{i} y_{j}$ in terms of the $x$-variables.\n\nFirst write: $y_{i}=a_{i r} x_{r}, y_{j}=a_{j s} x_{s}$. Then, by substitution,\n\n$$\nQ=g_{i j}\\left(a_{i r} x_{r}\\right)\\left(a_{j s} x_{s}\\right)=g_{i j} a_{i r} a_{j s} x_{r} x_{s}\n$$\n\nor $Q=h_{r s} x_{r} x_{s}$, where $h_{r s} \\equiv g_{i j} a_{i r} a_{j s}$.\n\n\\subsection*{1.5 KRONECKER DELTA AND ALGEBRAIC MANIPULATIONS}\nA much used symbol in tensor calculus has the effect of annihilating the \"off-diagonal\" terms in a double summation.\n\n\\section*{Kronecker Delta}\n\\[\n\\delta_{i j} \\equiv \\delta_{j}^{i} \\equiv \\delta^{i j} \\equiv \\begin{cases}1 & i=j  \\tag{1.1}\\\\ 0 & i \\neq j\\end{cases}\n\\]\n\nClearly, $\\delta_{i j}=\\delta_{j i}$ for all $i, j$.\n\nEXAMPLE 1.6 If $n=3$,\n\n$$\n\\begin{aligned}\n\\delta_{i j} x_{i} x_{j} & =1 x_{1} x_{1}+0 x_{1} x_{2}+0 x_{1} x_{3}+0 x_{2} x_{1}+1 x_{2} x_{2}+0 x_{2} x_{3}+0 x_{3} x_{1}+0 x_{3} x_{2}+1 x_{3} x_{3} \\\\\n& =\\left(x_{1}\\right)^{2}+\\left(x_{2}\\right)^{2}+\\left(x_{3}\\right)^{2}=x_{i} x_{i}\n\\end{aligned}\n$$\n\nIn general, $\\delta_{i j} x_{i} x_{j}=x_{i} x_{i}$ and $\\delta_{j}^{r} a_{i r} x_{i}=a_{i j} x_{i}$.\n\nEXAMPLE 1.7 Suppose that $T^{i}=g_{r}^{i} a_{r s} y_{s}$ and $y_{i}=b_{i r} x_{r}$. If further, $a_{i r} b_{r j}=\\delta_{i j}$, find $T^{i}$ in terms of the $x_{r}$.\n\nFirst write $y_{s}=b_{s t} x_{t}$. Then, by substitution,\n\n$$\nT^{i}=g_{r}^{i} a_{r s} b_{s t} x_{t}=g_{r}^{i} \\delta_{r t} x_{t}=g_{r}^{i} x_{r}\n$$\n\n\\section*{Algebra and the Summation Convention}\nCertain routine manipulations in tensor calculus can be easily justified by properties of ordinary sums. However, some care is warranted. For instance, the identity (1.2) below not only involves the distributive law for real numbers, $a(x+y) \\equiv a x+a y$, but also requires a rearrangement of terms utilizing the associative and commutative laws. At least a mental verification of such operations must be made, if false results are to be avoided.\n\nEXAMPLE 1.8 The following nonidentities should be carefully noted:\n\n$$\n\\begin{gathered}\na_{i j}\\left(x_{i}+y_{j}\\right) \\not \\equiv a_{i j} x_{i}+a_{i j} y_{j} \\\\\na_{i j} x_{i} y_{j} \\neq a_{i j} y_{i} x_{j} \\\\\n\\left(a_{i j}+a_{j i}\\right) x_{i} y_{j} \\neq 12 a_{i j} x_{i} y_{j}\n\\end{gathered}\n$$\n\nListed below are several valid identities; they, and others like them, will be used repeatedly from now on.\n\n\n\\begin{gather*}\na_{i j}\\left(x_{j}+y_{j}\\right) \\equiv a_{i j} x_{j}+a_{i j} y_{j}  \\tag{1:2}\\\\\na_{i j} x_{i} y_{j} \\equiv a_{i j} y_{j} x_{i}  \\tag{1.3}\\\\\na_{i j} x_{i} x_{j} \\equiv a_{j i} x_{i} x_{j}  \\tag{1.4}\\\\\n\\left(a_{i j}+a_{j i}\\right) x_{i} x_{j} \\equiv 2 a_{i j} x_{i} x_{j}  \\tag{1.5}\\\\\n\\left(a_{i j}-a_{j i}\\right) x_{i} x_{j} \\equiv 0 \\tag{1.6}\n\\end{gather*}\n\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{REPEATED INDICES}\n1.1 Use the summation convention to write the following, and assign the value of $n$ in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{41} b_{14}$\n\n(b) $a_{11} b_{11}+a_{12} b_{12}+a_{13} b_{13}+a_{14} b_{14}+a_{15} b_{15}+a_{16} b_{16}$\n\n(c) $c_{11}^{i}+c_{22}^{i}+c_{33}^{i}+c_{44}^{i}+c_{55}^{i}+c_{66}^{i}+c_{77}^{i}+c_{88}^{i} \\quad(1 \\leqq i \\leqq 8)$\n\n(a) $a_{i 1} b_{1 i} \\quad(n=4) ;(b) a_{1 i} b_{1 i} \\quad(n=6) ;(c) c_{j j}^{i} \\quad(n=8)$.\n\n1.2 Use the summation convention to write each of the following systems, state which indices are free and which are dummy indices, and fix the value of $n$ :\n\n$$\n\\begin{array}{lc}\nc_{11} x_{1}+c_{12} x_{2}+c_{13} x_{3}=2 & \\text { (b) } a_{j}^{1} x_{1}+a_{j}^{2} x_{2}+a_{j}^{3} x_{3}+a_{j}^{4} x_{4}=b_{j} \\\\\nc_{21} x_{1}+c_{22} x_{2}+c_{23} x_{3}=-3 & (j=1,2) \\\\\nc_{31} x_{1}+c_{32} x_{2}+c_{33} x_{3}=5 &\n\\end{array}\n$$\n\n(a) Set $d_{1}=2, d_{2}=-3$, and $d_{3}=5$. Then one can write the system as $c_{i j} x_{j}=d_{i} \\quad(n=3)$. The free index is $i$ and the dummy index is $j$.\n\n(b) Here, the range of the free index does not match that of the dummy index $(n=4)$, and this fact must be indicated:\n\n$$\na_{j}^{i} x_{i}=b_{j} \\quad(j=1,2)\n$$\n\nThe free index is $j$ and the dummy index is $i$.\n\n1.3 Write out explicitly the summations\n\n$$\nc_{i}\\left(x_{i}+y_{i}\\right) \\quad c_{j} x_{j}+c_{k} y_{k}\n$$\n\nwhere $n=4$ for both, and compare the results.\n\n$$\n\\begin{aligned}\nc_{i}\\left(x_{i}+y_{i}\\right) & =c_{1}\\left(x_{1}+y_{1}\\right)+c_{2}\\left(x_{2}+y_{2}\\right)+c_{3}\\left(x_{3}+y_{3}\\right)+c_{4}\\left(x_{4}+y_{4}\\right) \\\\\n& =c_{1} x_{1}+c_{1} y_{1}+c_{2} x_{2}+c_{2} y_{2}+c_{3} x_{3}+c_{3} y_{3}+c_{4} x_{4}+c_{4} y_{4} \\\\\nc_{j} x_{j}+c_{k} y_{k} & =c_{1} x_{1}+c_{2} x_{2}+c_{3} x_{3}+c_{4} x_{4}+c_{1} y_{1}+c_{2} y_{2}+c_{3} y_{3}+c_{4} y_{4}\n\\end{aligned}\n$$\n\nThe two summations are identical except for the order in which the terms occur, constituting a special case of (1.2).\n\n\\section*{DOUBLE SUMS}\n1.4 If $n=3$, expand $Q=a^{i j} x_{i} x_{j}$.\n\n$$\n\\begin{aligned}\nQ & =a^{1 j} x_{1} x_{j}+a^{2 j} x_{2} x_{j}+a^{3 j} x_{3} x_{j} \\\\\n& =a^{11} x_{1} x_{1}+a^{12} x_{1} x_{2}+a^{13} x_{1} x_{3}+a^{21} x_{2} x_{1}+a^{22} x_{2} x_{2}+a^{23} x_{2} x_{3}+a^{31} x_{3} x_{1}+a^{32} x_{3} x_{2}+a^{33} x_{3} x_{3}\n\\end{aligned}\n$$\n\n1.5 Use the summation convention to write the following, and state the value of $n$ necessary in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{12} b_{21}+a_{22} b_{22}+a_{32} b_{23}+a_{13} b_{31}+a_{23} b_{32}+a_{33} b_{33}$\n\n?\n\n(b) \" $g_{11}^{1}+g_{12}^{1}+\\dot{g}_{21}^{1}+g_{22}^{1}+g_{11}^{2}+g_{12}^{2}+g_{21}^{2}+g_{22}^{2}$\\\\\n(a) $a_{i 1} b_{1 i}+a_{i 2} b_{2 i}+a_{i 3} b_{3 i} \\equiv a_{i j} b_{j i} \\quad(n=3)$.\n\n(b) Set $c_{i}=1$ for each $i \\quad(n=2)$. Then the expression may be written\n\n$$\n\\begin{aligned}\ng_{11}^{i} c_{i}+g_{12}^{i} c_{i}+g_{21}^{i} c_{i}+g_{22}^{i} c_{i} & =\\left(g_{11}^{i}+g_{12}^{i}+g_{21}^{i}+g_{22}^{i}\\right) c_{i} \\\\\n& =\\left(g_{j k}^{i} c_{j} c_{k}\\right) c_{i}=g_{j k}^{i} c_{i} c_{j} c_{k}\n\\end{aligned}\n$$\n\n1.6 If $n=2$, write out explicitly the triple summation $c_{r s t} x^{r} y^{s} z^{t}$.\n\nAny expansion technique that yields all $2^{3}=8$ terms will do. In this case we shall interpret the triplet rst as a three-digit integer, and list the terms in increasing order of that integer:\n\n$$\n\\begin{aligned}\n& c_{r s t} x^{r} y^{s} z^{t}=c_{111} x^{1} y^{1} z^{1}+c_{112} x^{1} y^{1} z^{2}+c_{121} x^{1} y^{2} z^{1}+c_{122} x^{1} y^{2} z^{2} \\\\\n& +c_{211} x^{2} y^{1} z^{1}+c_{212} x^{2} y^{1} z^{2}+c_{221} x^{2} y^{2} z^{1}+c_{222} x^{2} y^{2} z^{2}\n\\end{aligned}\n$$\n\n1.7 Show that $a_{i j} x_{i} x_{j}=0$ if $a_{i j} \\equiv i-j$.\n\nBecause, for all $i$ and $j, a_{i j}=-a_{j i}$ and $x_{i} x_{j}=x_{i} x_{i}$, the \"off-diagonal\" terms $a_{i j} x_{i} x_{j} \\quad(i<j$; no sum) and $a_{j i} x_{j} x_{i} \\quad\\left(j>i\\right.$; no sum) cancel in pairs, while the \"diagonal\" terms $a_{i i}\\left(x_{i}\\right)^{2}$ are zero to begin with. Thus the sum is zero.\n\nThe result also follows at once from (1.5).\n\n1.8 If the $a_{i j}$ are constants, calculate the partial derivative\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nReverting to $\\Sigma$-notation, we have:\n\n$$\n\\begin{aligned}\n\\sum_{i, j} a_{i j} x_{i} x_{j} & =\\sum_{\\substack{i \\neq k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i \\neq k \\\\\nj=k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj=k}} a_{i j} x_{i} x_{j} \\\\\n& =C+\\left(\\sum_{j \\neq k} a_{k j} x_{j}\\right) x_{k}+\\left(\\sum_{i \\neq k} a_{i k} x_{i}\\right) x_{k}+a_{k k}\\left(x_{k}\\right)^{2}\n\\end{aligned}\n$$\n\nwhere $C$ is independent of $x_{k}$. Differentiating with respect to $x_{k}$,\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(\\sum_{i, j} a_{i j} x_{i} x_{j}\\right) & =0+\\sum_{j \\neq k} a_{k j} x_{j}+\\sum_{i \\neq k} a_{i k} x_{i}+2 a_{k k} x_{k} \\\\\n& =\\sum_{j} a_{k j} x_{j}+\\sum_{i} a_{i k} x_{i}\n\\end{aligned}\n$$\n\nor, going back to the Einstein summation convention,\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)=a_{k i} x_{i}+a_{i k} x_{i}=\\left(a_{i k}+a_{k i}\\right) x_{i}\n$$\n\n\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.9 Express $b^{i j} y_{i} y_{j}$ in terms of $x$-variables, if $y_{i}=c_{i j} x_{j}$ and $b^{i j} c_{i k}=\\delta_{k}^{j}$.\n\n$$\nb^{i j} y_{i} y_{j}=b^{i j}\\left(c_{i r} x_{r}\\right)\\left(c_{j s} x_{s}\\right)=\\left(b^{i j} c_{i r}\\right) x_{r} c_{j s} x_{s}=\\delta^{i}{ }_{r} x_{r} c_{j s} x_{s}=x_{j} c_{j s} x_{s}=c_{i j} x_{i} x_{j}\n$$\n\n1.10 Rework Problem 1.8 by use of the product rule for differentiation and the fact that\n\n$$\n\\frac{\\partial x_{p}}{\\partial x_{q}}=\\delta_{p q}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right) & =a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i} x_{j}\\right)=a_{i j}\\left(x_{j} \\frac{\\partial x_{i}}{\\partial x_{k}}+x_{i} \\frac{\\partial x_{j}}{\\partial x_{k}}\\right) \\\\\n& =a_{i j}\\left(x_{j} \\delta_{i k}+x_{i} \\delta_{j k}\\right)=a_{k j} x_{j}+a_{i k} x_{i} \\\\\n& =\\left(a_{i k}+a_{k i}\\right) x_{i}\n\\end{aligned}\n$$\n\n1.11 If $a_{i j}=a_{j i}$ are constants, calculate\n\n$$\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nUsing Problem 1.8, we have\n\n$$\n\\begin{aligned}\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right) & =\\frac{\\partial}{\\partial x_{k}}\\left[\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\\right]=\\frac{\\partial}{\\partial x_{k}}\\left[\\left(a_{l j}+a_{j l}\\right) x_{j}\\right] \\\\\n& =\\frac{\\partial}{\\partial x_{k}}\\left(2 a_{i l} x_{i}\\right)=2 a_{i l} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i}\\right)=2 a_{i l} \\delta_{k i}=2 a_{k l}\n\\end{aligned}\n$$\n\n1.12 Consider a system of linear equations of the form $y^{i}=a^{i j} x_{j}$ and suppose that $\\left(b_{i j}\\right)$ is a matrix of numbers such that for all $i$ and $j, b_{i r} a^{r j}=\\delta_{i}^{j}$ [that is, the matrix $\\left(b_{i j}\\right)$ is the inverse of the matrix $\\left(a^{i j}\\right)$ ]. Solve the system for $x_{i}$ in terms of the $y^{j}$.\n\nMultiply both sides of the $i$ th equation by $b_{k i}$ and sum over $i$ :\n\n$$\nb_{k i} y^{i}=b_{k i} a^{i j} x_{j}=\\delta_{k}^{j} x_{j}=x_{k}\n$$\n\nor $x_{i}=b_{i j} y^{j}$.\n\n1.13 Show that, generally, $a_{i j k}\\left(x_{i}+y_{j}\\right) z_{k} \\neq a_{i j k} x_{i} z_{k}+a_{i j k} y_{j} z_{k}$.\n\nSimply observe that on the left side there are no free indices, but on the right, $j$ is free for the first term and $i$ is free for the second.\n\n1.14 Show that $c_{i j}\\left(x_{i}+y_{i}\\right) z_{j} \\equiv c_{i j} x_{i} z_{j}+c_{i j} y_{i} z_{j}$.\n\nLet us prove (1.2); the desired identity will then follow upon setting $a_{i j} \\equiv c_{j i}$.\n\n$$\n\\begin{aligned}\na_{i j} x_{j}+a_{i j} y_{j} & \\equiv \\sum_{j} a_{i j} x_{j}+\\sum_{j} a_{i j} y_{j}=\\sum_{j}\\left(a_{i j} x_{j}+a_{i j} y_{j}\\right) \\\\\n& =\\sum_{j} a_{i j}\\left(x_{j}+y_{j}\\right) \\equiv a_{i j}\\left(x_{j}+y_{j}\\right)\n\\end{aligned}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n1.15 Write out the expression $a_{i} b_{i}(n=6)$ in full.\n\n1.16 Write out the expression $R_{j k i}^{i} \\quad(n=4)$ in full. Which are free and which are dummy indices? How many summations are there?\n\n1.17 Evaluate $\\delta_{j}^{i} x_{i}$ ( $n$ arbitrary).\n\n1.18 For $n$ arbitrary, evaluate (a) $\\delta_{i i}$, (b) $\\delta_{i j} \\delta_{i j}$, (c) $\\delta_{i j} \\delta_{k}^{j} c_{i k}$.\n\n1.19 Use the summation convention to indicate $a_{13} b_{13}+a_{23} b_{23}+a_{33} b_{33}$, and state the value of $n$.\n\n1.20 Use the summation convention to indicate\n\n$$\na_{11}\\left(x_{1}\\right)^{2}+a_{12} x_{1} x_{2}+a_{13} x_{1} x_{3}+a_{21} x_{2} x_{1}+a_{22}\\left(x_{2}\\right)^{2}+a_{23} x_{2} x_{3}+a_{31} x_{3} x_{1}+a_{32} x_{3} x_{2}+a_{33}\\left(x_{3}\\right)^{2}\n$$\n\n1.21 Use the summation convention and free subscripts to indicate the following linear system, stating the value of $n$ :\n\n$$\n\\begin{aligned}\n& y_{1}=c_{11} x_{1}+c_{12} x_{2} \\\\\n& y_{2}=c_{21} x_{1}+c_{22} x_{2}\n\\end{aligned}\n$$\n\n1.22 Find the following partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}\\right) \\quad(k=1,2,3)\n$$\n\n1.23 Use the Kronecker delta to calculate the partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)\n$$\n\n1.24 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left[a_{i j} x_{i}\\left(x_{j}\\right)^{2}\\right]\n$$\n\nwhere the $a_{i j}$ are constants such that $a_{i j}=a_{j i}$.\n\n1.25 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j k} x_{i} x_{j} x_{k}\\right)\n$$\n\nwhere the $a_{i j k}$ are constants.\n\n1.26 Solve Problem 1.11 without the symmetry condition on $a_{i j}$.\n\n1.27 Evaluate: (a) $b_{j}^{i} y_{i}$ if $y_{i}=T_{i}^{j j}$, (b) $a_{i j} y_{j}$ if $y_{i}=b_{i j} x_{j}$, (c) $a_{i j k} y_{i} y_{j} y_{k}$ if $y_{i}=b_{i j} x_{j}$.\n\n1.28 If $\\varepsilon_{i}=1$ for all $i$, prove that\n\n(a) $\\left(a_{1}+a_{2}+\\cdots+a_{n}\\right)^{2} \\equiv \\varepsilon_{i} \\varepsilon_{j} a_{i} a_{j}$\n\n(b) $a_{i}\\left(1+x_{i}\\right) \\equiv a_{i} \\varepsilon_{i}+a_{i} x_{i}$\n\n(c) $a_{i j}\\left(x_{i}+x_{j}\\right) \\equiv 2 a_{i j} \\varepsilon_{i} x_{j}$ if $a_{i j}=a_{j i}$\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 2}", "\\section*{Basic Linear Algebra for Tensors}\n\\subsection*{2.1 INTRODUCTION}\nFamiliarity with the topics in this chapter will result in a much greater appreciation for the geometric aspects of tensor calculus. The main purpose is to reformulate the expressions of linear algebra and matrix theory using the summation convention.\n\n\\subsection*{2.2 TENSOR NOTATION FOR MATRICES, VECTORS, AND DETERMINANTS}\nIn the ordinary matrix notation $\\left(a_{i j}\\right)$, the first subscript, $i$, tells what row the number $a_{i j}$ lies in, and the second, $j$, designates the column. A fuller notation is $\\left[a_{i j}\\right]_{m n}$, which exhibits the number of rows, $m$, and the number of columns, $n$. This notation may be extended as follows.\n\n\\section*{Upper-Index Matrix Notation}\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-017}\n\\end{center}\n\nNote that, for mixed indices (one upper, one lower), it is the upper index that designates the row, and the lower index, the column. In the case of pure superscripts, the scheme is identical to the familiar one for subscripts.\n\n\\section*{EXAMPLE 2.1}\n$$\n\\begin{array}{r}\n{\\left[c_{j}^{i}\\right]_{23} \\equiv\\left[\\begin{array}{lll}\nc_{1}^{1} & c_{2}^{1} & c_{3}^{1} \\\\\nc_{1}^{2} & c_{2}^{2} & c_{3}^{2}\n\\end{array}\\right] .\\left[d_{i}^{j}\\right]_{23} \\equiv\\left[\\begin{array}{lll}\nd_{1}^{1} & d_{2}^{1} & d_{3}^{1} \\\\\nd_{1}^{2} & d_{2}^{2} & d_{3}^{2}\n\\end{array}\\right] \\equiv\\left[d_{j}^{i}\\right]_{23}} \\\\\n{\\left[x_{s}^{r}\\right]_{14} \\equiv\\left[\\begin{array}{llll}\nx_{1}^{1} & x_{2}^{1} & x_{3}^{1} & x_{4}^{1}\n\\end{array}\\right] \\quad\\left[y^{p q}\\right]_{42} \\equiv\\left[\\begin{array}{ll}\ny^{11} & y^{12} \\\\\ny^{21} & y^{22} \\\\\ny^{31} & y^{32} \\\\\ny^{41} & y^{42}\n\\end{array}\\right]}\n\\end{array}\n$$\n\n\\section*{Vectors}\nA real $n$-dimensional vector is any column matrix $\\mathbf{v}=\\left[x_{i j}\\right]_{n 1}$ with real components $x_{i} \\equiv x_{i 1}$; one usually writes simply $\\mathbf{v}=\\left(x_{i}\\right)$. The collection of all real $n$-dimensional vectors is the $n$-dimensional real vector space denoted $\\mathbf{R}^{n}$.\n\nVector sums are determined by coordinatewise addition, as are matrix sums: if $A \\equiv\\left[a_{i j}\\right]_{m n}$ and $B \\equiv\\left[b_{i j}\\right]_{m n}$, then\n\n$$\nA+B \\equiv\\left[a_{i j}+b_{i j}\\right]_{m n}\n$$\n\nScalar multiplication of a vector or matrix is defined by\n\n$$\n\\lambda\\left[a_{i j}\\right]_{m n} \\equiv\\left[\\lambda a_{i j}\\right]_{m n}\n$$\n\n\\section*{Basic Formulas}\nThe essential formulas involving matrices, vectors, and determinants are now given in terms of the summation convention.\n\nMatrix multiplication. If $A \\equiv\\left[a_{i j}\\right]_{m n}$ and $B \\equiv\\left[b_{i j}\\right]_{n k}$, then\n\n\n\\begin{equation*}\nA B=\\left[a_{i r} b_{r j}\\right]_{m k} \\tag{2.1a}\n\\end{equation*}\n\n\nAnalogously, for mixed or upper indices,\n\n\n\\begin{equation*}\nA B \\equiv\\left[a_{j}^{i}\\right]_{m n}\\left[b_{j}^{i}\\right]_{n k}=\\left[a_{r}^{i} b_{j}^{r}\\right]_{m k} \\quad A B \\equiv\\left[a^{i j}\\right]_{m n}\\left[b^{i j}\\right]_{n k}=\\left[a^{i r} b^{r j}\\right]_{m k} \\tag{2.1b}\n\\end{equation*}\n\n\nwherein $i$ and $j$ are not summed on.\n\nIdentity matrix. In terms of the Kronecker deltas, the identity matrix of order $n$ is\n\n$$\nI=\\left[\\delta_{i j}\\right]_{n n} \\equiv\\left[\\delta_{j}^{i}\\right]_{n n} \\equiv\\left[\\delta^{i j}\\right]_{n n}\n$$\n\nwhich has the property $I A=A I=A$ for any square matrix $A$ of order $n$.\n\nInverse of a square matrix. A square matrix $A \\equiv\\left[a_{i j}\\right]_{n n}$ is invertible if there exists a (unique) matrix $B \\equiv\\left[b_{i j}\\right]_{n n}$, called the inverse of $A$, such that $A B=B A=I$. In terms of components, the criterion reads:\n\n\n\\begin{equation*}\na_{i r} b_{r j}=b_{i r} a_{r j}=\\delta_{i j} \\tag{2.2a}\n\\end{equation*}\n\n\nor, for mixed or upper indices,\n\n\n\\begin{equation*}\na_{r}^{i} b_{j}^{r}=b_{r}^{i} a_{j}^{r}=\\delta_{j}^{i} \\quad a^{i r} b^{r j}=b^{i r} a^{r j}=\\delta^{i j} \\tag{2.2b}\n\\end{equation*}\n\n\nTranspose of a matrix. Transposition of an arbitrary matrix is defined by $A^{T} \\equiv\\left[a_{i j}\\right]_{m n}^{T}=\\left[a_{i j}^{\\prime}\\right]_{n m}$, where $a_{i j}^{\\prime}=a_{j i}$ for all $i$, $j$. If $A^{T}=A$ (that is, $a_{i j}=a_{j i}$ for all $i, j$ ), then $A$ is called symmetric; if $A^{T}=-A$ (that is, $a_{i j}=-a_{j i}$ for all $i, j$ ), then $A$ is called antisymmetric or skew-symmetric.\n\nOrthogonal matrix. A matrix $A$ is orthogonal if $A^{T}=A^{-1}$ (or if $A^{T} A=A A^{T}=I$ ).\n\nPermutation symbol. The symbol $e_{i j k \\ldots w}$ (with $n$ subscripts) has the value zero if any pair of subscripts are identical, and equals $(-1)^{p}$ otherwise, where $p$ is the number of subscript transpositions (interchanges of consecutive subscripts) required to bring $(i j k \\ldots w)$ to the natural order $(123 \\ldots n)$.\n\nDeterminant of a square matrix. If $A \\equiv\\left[a_{i j}\\right]_{n n}$ is any square matrix, define the scalar\n\n\n\\begin{equation*}\n\\operatorname{det} A \\equiv e_{i_{1} i_{2} i_{3} \\cdots i_{n}} a_{1 i_{1}} a_{2 i_{2}} a_{3 i_{3}} \\cdots a_{n i_{n}} \\tag{2.3}\n\\end{equation*}\n\n\nOther notations are $|A|,\\left|a_{i j}\\right|$, and $\\operatorname{det}\\left(a_{i j}\\right)$. The chief properties of determinants are\n\n\n\\begin{equation*}\n|A B|=|A||B| \\quad\\left|A^{T}\\right|=|A| \\tag{2.4}\n\\end{equation*}\n\n\nLaplace expansion of a determinant. For each $i$ and $j$, let $M_{i j}$ be the determinant of the square matrix of order $n-1$ obtained from $A \\equiv\\left[a_{i j}\\right]_{n n}$ by deleting the $i$ th row and $j$ th column; $M_{i j}$ is called the minor of $a_{i j}$ in $|A|$. Define the cofactor of $a_{i j}$ to be the scalar\n\n\n\\begin{equation*}\nA_{i j}=(-1)^{k} M_{i j} \\quad \\text { where } \\quad k=i+j \\tag{2.5}\n\\end{equation*}\n\n\nThen the Laplace expansions of $|A|$ are given by\n\n$$\n\\begin{aligned}\n& |A|=a_{1 j} A_{1 j}=a_{2 j} A_{2 j}=\\cdots=a_{n j} A_{n j} \\quad \\text { [row expansions] } \\\\\n& |A|=a_{i 1} A_{i 1}=a_{i 2} A_{i 2}=\\cdots=a_{i n} A_{\\text {in }} \\quad \\text { [column expansions] }\n\\end{aligned}\n$$\n\nScalar product of vectors. If $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, then\n\n\n\\begin{equation*}\n\\mathbf{u v} \\equiv \\mathbf{u} \\cdot \\mathbf{v} \\equiv \\mathbf{u}^{T} \\mathbf{v}=x_{i} y_{i} \\tag{2.7}\n\\end{equation*}\n\n\nIf $\\mathbf{u}=\\mathbf{v}$, the notation $\\mathbf{u u} \\equiv \\mathbf{u}^{2} \\equiv \\mathbf{v}^{2}$ will often be used. Vectors $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal if $\\mathbf{u v}=0$.\n\nNorm (length) of a vector. If $\\mathbf{u}=\\left(x_{i}\\right)$, then\n\n\n\\begin{equation*}\n\\|\\mathbf{u}\\| \\equiv \\sqrt{\\mathbf{u}^{2}}=\\sqrt{x_{i} x_{i}} \\tag{2.8}\n\\end{equation*}\n\n\nAngle between two vectors. The angle $\\theta$ between two nonzero vectors, $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, is defined by\n\n\n\\begin{equation*}\n\\cos \\theta \\equiv \\frac{\\mathbf{u v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}=\\frac{x_{i} y_{i}}{\\sqrt{x_{j} x_{j}} \\sqrt{y_{k} y_{k}}} \\quad(0 \\leqq \\theta \\leqq \\pi) \\tag{2.9}\n\\end{equation*}\n\n\nIt follows that $\\theta=\\pi / 2$ if $\\mathbf{u}$ and $\\mathbf{v}$ are nonzero orthogonal vectors.\n\nVector product in $\\mathbf{R}^{3}$. If $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, and if the standard basis vectors are designated\n\n$$\n\\mathbf{i} \\equiv\\left(\\delta_{i 1}\\right) \\quad \\mathbf{j} \\equiv\\left(\\delta_{i 2}\\right) \\quad \\mathbf{k} \\equiv\\left(\\delta_{i 3}\\right)\n$$\n\nthen\n\n\\[\n\\mathbf{u} \\times \\mathbf{v} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k}  \\tag{2.10a}\\\\\nx_{1} & x_{2} & x_{3} \\\\\ny_{1} & y_{2} & y_{3}\n\\end{array}\\right|=\\left|\\begin{array}{ll}\nx_{2} & x_{3} \\\\\ny_{2} & y_{3}\n\\end{array}\\right| \\mathbf{i}-\\left|\\begin{array}{ll}\nx_{1} & x_{3} \\\\\ny_{1} & y_{3}\n\\end{array}\\right| \\mathbf{j}+\\left|\\begin{array}{ll}\nx_{1} & x_{2} \\\\\ny_{1} & y_{2}\n\\end{array}\\right| \\mathbf{k}\n\\]\n\nExpressing the second-order determinants by means of $(2.3)$, one can rewrite $(2.10 a)$ in terms of components only:\n\n\n\\begin{equation*}\n\\mathbf{u} \\times \\mathbf{v}=\\left(e_{i j k} x_{j} y_{k}\\right) \\tag{2.10b}\n\\end{equation*}\n\n\n\\subsection*{2.3 INVERTING A MATRIX}\nThere are a number of algorithms for computing the inverse of $A \\equiv\\left[a_{i j}\\right]_{n n}$, where $|A| \\neq 0$ (a necessary and sufficient condition for $A$ to be invertible). When $n$ is large, the method of elementary row operations is efficient. For small $n$, it is practical to apply the explicit formula\n\n\n\\begin{equation*}\nA^{-1}=\\frac{1}{|A|}\\left[A_{i j}\\right]_{n n}^{T} \\tag{2.11a}\n\\end{equation*}\n\n\nThus, for $n=2$,\n\n\\[\n\\left[\\begin{array}{ll}\na_{11} & a_{12}  \\tag{2.11b}\\\\\na_{21} & a_{22}\n\\end{array}\\right]^{-1}=\\frac{1}{|A|}\\left[\\begin{array}{rr}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{array}\\right]\n\\]\n\nin which $|A|=a_{11} a_{22}-a_{12} a_{21}$; and, for $n=3$,\n\nin which\n\n\\[\n\\left[\\begin{array}{lll}\na_{11} & a_{12} & a_{13}  \\tag{2.11c}\\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right]^{-1}=\\frac{1}{|A|}\\left[\\begin{array}{lll}\nA_{11} & A_{21} & A_{31} \\\\\nA_{12} & A_{22} & A_{32} \\\\\nA_{13} & A_{23} & A_{33}\n\\end{array}\\right]\n\\]\n\n$$\nA_{11}=a_{22} a_{33}^{\\dot{b}}-a_{23} a_{32} \\quad A_{21}=-\\left(a_{12} a_{33}-a_{13} a_{32}\\right) \\quad \\ldots\n$$\n\n\\subsection*{2.4 MATRIX EXPRESSIONS FOR LINEAR SYSTEMS AND QUADRATIC FORMS}\nBecause of the product rule for matrices and the rule for matrix equality, one can write a system of equations such as\n\n$$\n\\begin{array}{r}\n3 x-4 y=2 \\\\\n-5 x+8 y=7\n\\end{array}\n$$\n\nin the matrix form\n\n$$\n\\left[\\begin{array}{rr}\n3 & -4 \\\\\n-5 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{l}\n2 \\\\\n7\n\\end{array}\\right]\n$$\n\nIn general, any $m \\times n$ system of equations\n\n\n\\begin{equation*}\na_{i j} x_{j}=b_{i} \\quad(1 \\leqq i \\leqq m) \\tag{2.12a}\n\\end{equation*}\n\n\ncan be written in the matrix form\n\n\n\\begin{equation*}\nA \\mathbf{x}=\\mathbf{b} \\tag{2.12b}\n\\end{equation*}\n\n\nwhere $A \\equiv\\left[a_{i j}\\right]_{m n}, \\mathbf{x} \\equiv\\left(x_{i}\\right)$, and $\\mathbf{b} \\equiv\\left(b_{i}\\right)$. One advantage in doing this is that, if $m=n$ and $A$ is invertible, the solution of the system can proceed entirely by matrices: $\\mathbf{x}=A^{-1} \\mathbf{b}$.\n\nAnother useful fact for work with tensors is that a quadratic form $Q$ (a homogeneous second-degree polynomial) in the $n$ variables $x_{1}, x_{2}, \\ldots, x_{n}$ also has a strictly matrix representation:\n\n\n\\begin{equation*}\nQ=a_{i j} x_{i} x_{j}=\\mathbf{x}^{T} A \\mathbf{x} \\tag{2.13}\n\\end{equation*}\n\n\nwhere the row matrix $\\mathbf{x}^{T}$ is the transpose of the column matrix $\\mathbf{x}=\\left(x_{i}\\right)$ and where $A \\equiv\\left[a_{i j}\\right]_{n n}$.\n\n\\section*{EXAMPLE 2.2}\n$$\n\\left[\\begin{array}{lll}\nx_{1} & x_{2} & x_{3}\n\\end{array}\\right]\\left[\\begin{array}{lll}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\nx_{1} & x_{2} & x_{3}\n\\end{array}\\right]\\left[\\begin{array}{l}\na_{1 j} x_{j} \\\\\na_{2 j} x_{j} \\\\\na_{3 j} x_{j}\n\\end{array}\\right]=\\left[x_{i}\\left(a_{i j} x_{j}\\right)\\right]=a_{i j} x_{i} x_{j}\n$$\n\nThe matrix $A$ that produces a given quadratic form is not unique. In fact, the matrix $B=\\frac{1}{2}\\left(A+A^{T}\\right)$ may always be substituted for $A$ in (2.13); i.e., the matrix of a quadratic form may always be assumed symmetric.\n\nEXAMPLE 2.3 Write the quadratic equation\n\n$$\n3 x^{2}+y^{2}-2 z^{2}-5 x y-6 y z=10\n$$\n\nusing a symmetric matrix.\n\nThe quadratic form (2.13) is given in terms of the nonsymmetric matrix\n\n$$\nA=\\left[\\begin{array}{rrr}\n3 & -5 & 0 \\\\\n0 & 1 & -6 \\\\\n0 & 0 & -2\n\\end{array}\\right]\n$$\n\nThe symmetric equivalent is obtained by replacing each off-diagonal element by one-half the sum of that element and its mirror image in the main diagonal. Hence, the desired representation is\n\n$$\n\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n3 & -5 / 2 & 0 \\\\\n-5 / 2 & 1 & -3 \\\\\n0 & -3 & -2\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=10\n$$\n\n\\subsection*{2.5 LINEAR TRANSFORMATIONS}\nOf utmost importance for the study of tensor calculus is a basic knowledge of transformation theory and changes in coordinate systems. A set of linear equations like\n\n\n\\begin{align*}\n& \\bar{x}=5 x-2 y \\\\\n& \\bar{y}=3 x+2 y \\tag{I}\n\\end{align*}\n\n\ndefines a linear transformation (or linear mapping) from each point $(x, y)$ to its corresponding image $(\\bar{x}, \\bar{y})$. In matrix form, a, linear transformation may be written $\\overline{\\mathbf{x}}=A \\mathbf{x}$; if, as in $(I)$, the mapping is one-one, then $|A| \\neq 0$.\n\nThere is always an alias-alibi aspect of such transformations: When $(\\bar{x}, \\bar{y})$ is regarded as defining new coordinates (a new name) for $(x, y)$, one is dealing with the alias aspect; when $(\\bar{x}, \\bar{y})$ is regarded as a new position (place) for $(x, y)$, the alibi aspect emerges. In tensor calculus, one is generally more interested in the alias aspect: the two coordinate systems related by $\\overline{\\mathbf{x}}=A \\mathbf{x}$ are referred to as the unbarred and the barred systems.\n\nEXAMPLE 2.4 In order to find the image of the point $(0,-1)$ under $(I)$, merely set $x=0$ and $y=-1$; the result is\n\n$$\n\\bar{x}=5(0)-2(-1)=2 \\quad \\bar{y}=3(0)+2(-1)=-2\n$$\n\nHence, $\\overline{(0,-1)}=(2,-2)$. Similarly, we find that $\\overline{(2,1)}=(8,8)$.\n\nIf we regard $(\\bar{x}, \\bar{y})$ merely as a different coordinate system, we would say that two fixed points, $P$ and $Q$, have the respective coordinates $(0,-1)$ and $(2,1)$ in the unbarred system, and $(2,-2)$ and $(8,8)$ in the barred system.\n\n\\section*{Distance in a Barred Coordinate System}\nWhat is the expression for the (invariant) distance between two points in terms of differing aliases? Let $\\overline{\\mathbf{x}}=A \\mathbf{x}(|A| \\neq 0)$ define an invertible linear transformation between unbarred and barred coordinates. It is shown in Problem 2.20 that the desired distance formula is\n\n\n\\begin{equation*}\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} G(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})}=\\sqrt{g_{i j} \\Delta \\bar{x}_{i} \\Delta \\bar{x}_{j}} \\tag{2.14}\n\\end{equation*}\n\n\nwhere $\\left[g_{i j}\\right]_{n n} \\equiv G=\\left(A A^{T}\\right)^{-1}$ and $\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}=\\left(\\Delta \\bar{x}_{i}\\right)$. If $A$ is orthogonal (a rotation of the axes), then $g_{i j}=\\delta_{i j}$, and (2.14) reduces to the ordinary form\n\n$$\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\|\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}\\|=\\sqrt{\\Delta \\bar{x}_{i} \\Delta \\bar{x}_{i}}\n$$\n\n[cf. (2.8)].\n\nEXAMPLE 2.5 Calculate the distance between points $P$ and $Q$ of Example 2.4 in terms of their barred coordinates. Verify that the same distance is found in the unbarred coordinate system.\n\nFirst calculate the matrix $G=\\left(A A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T} A^{-1}$ (see Problem 2.13):\n\nand $\\quad G=\\frac{1}{16}\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]^{T} \\cdot \\frac{1}{16}\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]=\\frac{1}{256}\\left[\\begin{array}{rr}2 & -3 \\\\ 2 & 5\\end{array}\\right]\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]=\\frac{1}{256}\\left[\\begin{array}{rr}13 & -11 \\\\ -11 & 29\\end{array}\\right]$\n\nHence $g_{11}=13 / 256, g_{12}=g_{21}=-11 / 256$, and $g_{22}=29 / 256$. Now, with $\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}=\\left[\\begin{array}{ll}2-8 & -2-8\\end{array}\\right]^{T}=\\left[\\begin{array}{ll}-6 & -10\\end{array}\\right]^{T}$, (2.14) gives:\n\n$$\n\\begin{aligned}\nd^{2} & =g_{i j} \\Delta \\bar{x}_{i} \\Delta \\bar{x}_{j} \\\\\n& =\\frac{13}{256}(-6)^{2}+2 \\cdot \\frac{-11}{256}(-6)(-10)+\\frac{29}{256}(-10)^{2} \\\\\n& =\\frac{13(36)-22(60)+29(100)}{256}=8\n\\end{aligned}\n$$\n\nIn the unbarred system, the distance between $P(0,-1)$ and $Q(2,1)$ is given, in agreement, by the Pythagorean theorem:\n\n$$\nd^{2}=(0-2)^{2}+(-1-1)^{2}=8\n$$\n\n\\subsection*{2.6 GENERAL COORDINATE TRANSFORMATIONS}\nA general mapping or transformation $T$ of $\\mathbf{R}^{n}$ may be indicated in functional (vector) or in component form:\n\n$$\n\\overline{\\mathbf{x}}=T(\\mathbf{x}) \\quad \\text { or } \\quad \\bar{x}_{i}=T_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\n$$\n\nIn the alibi description, any point $\\mathbf{x}$ in the domain of $T$ (possibly the whole of $\\mathbf{R}^{n}$ ) has as its image the point $T(\\mathbf{x})$ in the range of $T$. Considered as a coordinate transformation (the alias description), $T$ sets up, for each point $P$ in its domain, a correspondence between $\\left(x_{i}\\right)$ and $\\left(\\bar{x}_{i}\\right)$, the coordinates of $P$ in two different systems. As explained below, $T$ may be interpreted as a coordinate transformation only if a certain condition is fulfilled.\n\n\\section*{Bijections, Curvilinear Coordinates}\nA map $T$ is called a bijection or a one-one mapping if it maps each pair of distinct points $\\mathbf{x} \\neq \\mathbf{y}$ in its domain into distinct points $T(\\mathbf{x}) \\neq T(\\mathbf{y})$ in its range. Whenever $T$ is bijective, we call the image $\\overline{\\mathbf{x}}=T(\\mathbf{x})$ a set of admissible coordinates for $\\mathbf{x}$, and the aggregate of all such coordinates (alibi: the range of $T$ ), a coordinate system.\n\nCertain coordinate systems are named after the characteristics of the mapping $T$. For example, if $T$ is linear, the $\\left(\\bar{x}_{i}\\right)$-system is called affine; and if $T$ is a rigid motion, $\\left(\\bar{x}_{i}\\right)$ is called rectangular or cartesian. [It is presumed in making this statement that the original coordinate system $\\left(x_{i}\\right)$ is the familiar cartesian coordinate system of analytic geometry, or its natural extension to vectors in $\\mathbf{R}^{n}$.] Nonaffine coordinate systems are generally called curvilinear coordinates; these include polar coordinates in two dimensions, and cylindrical and spherical coordinates in three dimensions.\n\n\\subsection*{2.7 THE CHAIN RULE FOR PARTIAL DERIVATIVES}\nIn working with curvilinear coordinates, one needs the Jacobian matrix (Chapter 3) and, therefore, the chain rule of multivariate calculus. The summation convention makes possible a compact statement of this rule: If $w=f\\left(x_{1}, x_{2}, x_{3}, \\ldots, x_{n}\\right)$ and $x_{i}=x_{i}\\left(u_{1}, u_{2}, \\ldots, u_{m}\\right) \\quad(i=$ $1,2, \\ldots, n)$, where all functions involved have continuous partial derivatives, then\n\n\n\\begin{equation*}\n\\frac{\\partial w}{\\partial u_{j}}=\\frac{\\partial f}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\partial u_{j}} \\quad(1 \\leqq j \\leqq m) \\tag{2.15}\n\\end{equation*}\n\n\n\\section*{Solved Problems}\n\\section*{TENSOR NOTATION}\n2.1 Display explicitly the matrices $(a)\\left[b_{i}^{j}\\right]_{42},(b)\\left[b_{j}^{i}\\right]_{24},(c)\\left[\\delta^{i j}\\right]_{33}$.\n\n$$\n\\begin{gathered}\n\\text { (a) }\\left[b_{i}^{j}\\right]_{42}=\\left[\\begin{array}{ll}\nb_{1}^{1} & b_{2}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} \\\\\nb_{1}^{3} & b_{2}^{3} \\\\\nb_{1}^{4} & b_{2}^{4}\n\\end{array}\\right] \\quad \\text { (b) }\\left[b_{j}^{i}\\right]_{24}=\\left[\\begin{array}{llll}\nb_{1}^{1} & b_{2}^{1} & b_{3}^{1} & b_{4}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} & b_{3}^{2} & b_{4}^{2}\n\\end{array}\\right] \\\\\n\\text { (c) }\\left[\\delta^{i j}\\right]_{33}=\\left[\\begin{array}{lll}\n\\delta^{11} & \\delta^{12} & \\delta^{13} \\\\\n\\delta^{21} & \\delta^{22} & \\delta^{23} \\\\\n\\delta^{31} & \\delta^{32} & \\delta^{33}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{gathered}\n$$\n\nFrom (a) and (b) it is evident that merely interchanging the indices $i$ and $j$ in a matrix $A \\equiv\\left[a_{i j}\\right]_{m n}$ does not necessarily yield the transpose, $A^{T}$.\n\n\\subsection*{2.2 Given}\n$$\nA=\\left[\\begin{array}{ccc}\na & -a & -a \\\\\n2 b & b & -b \\\\\n4 c & 2 c & -2 c\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{rrr}\n2 & 4 & -6 \\\\\n-1 & -2 & 3 \\\\\n3 & 6 & -9\n\\end{array}\\right]\n$$\n\nverify that $A B \\neq B A$.\n\n$$\nA B=\\left[\\begin{array}{ccc}\n2 a+a-3 a & 4 a+2 a-6 a & -6 a-3 a+9 a \\\\\n4 b-b-3 b & 8 b-2 b-6 b & -12 b+3 b+9 b \\\\\n8 c-2 c-6 c & 16 c-4 c-12 c & -24 c+6 c+18 c\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\equiv \\mathrm{O}\n$$\n\nbut $\\quad B A=\\left[\\begin{array}{ccc}2 a+8 b-24 c & -2 a+4 b-12 c & -2 a-4 b+12 c \\\\ -a-4 b+12 c & a-2 b+6 c & a+2 b-6 c \\\\ 3 a+12 b-36 c & -3 a+6 b-18 c & -3 a-6 b+18 c\\end{array}\\right] \\neq \\mathrm{O}$\n\nThus, the commutative law $(A B=B A)$ fails for matrices. Further, $A B=\\mathrm{O}$ does not imply that $A=\\mathrm{O}$ or $B=$ O.\n\n2.3 Prove by use of tensor notation and the product rule for matrices that $(A B)^{T}=B^{T} A^{T}$, for any two conformable matrices $A$ and $B$.\n\nLet $A \\equiv\\left[a_{i j}\\right]_{m n}, B \\equiv\\left[b_{i j}\\right]_{n k}, A B \\equiv\\left[c_{i j}\\right]_{m k}$, and, for all $i$ and $j$,\n\n$$\na_{i j}^{\\prime}=a_{j i} \\quad b_{i j}^{\\prime}=b_{j i} \\quad c_{i j}^{\\prime}=c_{j i}\n$$\n\nHence, $A^{T}=\\left[a_{i j}^{\\prime}\\right]_{n m}, B^{T}=\\left[b_{i j}^{\\prime}\\right]_{k n}$, and $(A B)^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. We must show that $B^{T} A^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. By definition of matrix product, $B^{T} A^{T}=\\left[b_{i r}^{\\prime}{ }^{\\prime} a_{r j}^{\\prime}\\right]_{k m}$, and since\n\n$$\nb_{i r}^{\\prime} a_{r j}^{\\prime}=b_{r i} a_{j r}=a_{j r} b_{r i}=c_{j i}=c_{i j}^{\\prime}\n$$\n\nthe desired result follows.\n\n2.4 Show that any matrix of the form $A=B^{T} B$ is symmetric.\n\nBy Problem 2.3 and the involutory nature of the transpose operation,\n\n$$\nA^{T}=\\left(B^{T} B\\right)^{T}=B^{T}\\left(B^{T}\\right)^{T}=B^{T} B=A\n$$\n\n2.5 From the definition, (2.3), of a determinant of order 3, derive the Laplace expansion by cofactors of the first row.\n\nIn the case $n=3,(2.3)$ becomes\n\n$$\n\\left|\\begin{array}{lll}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right| \\equiv\\left|a_{i j}\\right|=e_{i j k} a_{1 i} a_{2 j} a_{3 k}\n$$\n\nSince $e_{i j k}=0$ if any two subscripts coincide, we write only terms for which (ijk) is a permutation of (123):\n\n$$\n\\begin{aligned}\n\\left|a_{i j}\\right| & =e_{123} a_{11} a_{22} a_{33}+e_{132} a_{11} a_{23} a_{32}+e_{213} a_{12} a_{21} a_{33} \\\\\n& \\quad+e_{231} a_{12} a_{23} a_{31}+e_{312} a_{13} a_{21} a_{32}+e_{321} a_{13} a_{22} a_{31} \\\\\n& =a_{11} a_{22} a_{33}-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31} \\\\\n& =a_{11}\\left(a_{22} a_{33}-a_{23} a_{32}\\right)-a_{12}\\left(a_{21} a_{33}-a_{23} a_{31}\\right)+a_{13}\\left(a_{21} a_{32}-a_{22} a_{31}\\right)\n\\end{aligned}\n$$\n\nBut, for $n=2,(2.3)$ gives\n\n$$\n\\left|\\begin{array}{ll}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{array}\\right| \\equiv+A_{11}=e_{12} a_{22} a_{33}+e_{2}: a_{23} a_{32}=a_{22} a_{33}-a_{23} a_{32}\n$$\n\nand the analogous expansions of $-A_{12}$ and $+A_{13}$. Hence,\n\n$$\n\\left|a_{i j}\\right|=a_{11} A_{11}+a_{12} A_{12}+a_{13} A_{13}=a_{1 j} A_{1 j}\n$$\n\nas in $(2.6)$.\n\n\\subsection*{2.6 Evaluate:}\n$$\n\\begin{aligned}\n& \\text { (a) }\\left|\\begin{array}{cc}\nb & -2 a \\\\\n-2 c & b\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| \\\\\n& \\left|\\begin{array}{cc}\nb \\cdot & -2 a \\\\\n-2 c & b\n\\end{array}\\right|=b \\cdot b-(-2 a)(-2 c)=b^{2}-4 a c\n\\end{aligned}\n$$\n\n(b) Because of the zeros in the second column, it is simplest to expand by that column:\n\n$$\n\\begin{aligned}\n\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| & =-(-2)\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|+0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n15 & 30\n\\end{array}\\right|-0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n-10 & 10\n\\end{array}\\right| \\\\\n& =2\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|=2(10)(15)\\left|\\begin{array}{rr}\n-1 & 1 \\\\\n1 & 2\n\\end{array}\\right|=300(-2-1)=-900\n\\end{aligned}\n$$\n\n2.7 Calculate the angle between the following two vectors in $\\mathbf{R}^{5}$ :\n\n$$\n\\mathbf{x}=(1,0,-2,-1,0) \\quad \\text { and } \\quad \\mathbf{y}=(0,0,2,2,0)\n$$\n\nWe have:\n\n$$\n\\begin{aligned}\n& \\mathbf{x y}=(1)(0)+(0)(0)+(-2)(2)+(-1)(2)+(0)(0)=-6 \\\\\n& \\mathbf{x}^{2}=1^{2}+0^{2}+(-2)^{2}+(-1)^{2}+0^{2}=6 \\\\\n& \\mathbf{y}^{2}=0^{2}+0^{2}+2^{2}+2^{2}+0^{2}=8\n\\end{aligned}\n$$\n\nand (2.9) gives\n\n$$\n\\cos \\theta=\\frac{-6}{\\sqrt{6} \\cdot \\sqrt{8}}=-\\frac{\\sqrt{3}}{2} \\quad \\text { or } \\quad \\theta=\\frac{5 \\pi}{6}\n$$\n\n2.8 Find three linearly independent vectors in $\\mathbf{R}^{4}$ which are orthogonal to the vector $(3,4,1,-2)$.\n\nIt is useful to choose vectors having as many zero components as possible. The components $(0,1,0,2)$ clearly work, and $(1,0,-3,0)$ also. Finally, $(0,0,2,1)$ is orthogonal to the given vector, and seems not to be dependent on the first two chosen. To check independence, suppose scalars $x, y$, and $z$ exist such that\n\n$$\nx\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n0 \\\\\n2\n\\end{array}\\right]+y\\left[\\begin{array}{r}\n1 \\\\\n0 \\\\\n-3 \\\\\n0\n\\end{array}\\right]+z\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2 \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right] \\quad \\text { or } \\quad \\begin{aligned}\n& x(0)+y(1)+z(0)=0 \\\\\n& x(1)+y(0)+z(0)=0 \\\\\n& x(0)+y(-3)+z(2)=0 \\\\\n& x(2)+y(0)+z(1)=0\n\\end{aligned}\n$$\n\nThis system has the sole solution $x=y=z=0$, and the vectors are independent.\n\n2.9 Prove that the vector product in $R^{3}$ is anticommutative: $\\mathbf{x} \\times \\mathbf{y}=-\\mathbf{y} \\times \\mathbf{x}$.\n\nBy $(2.10 b)$,\n\n$$\n\\mathbf{x} \\times \\mathbf{y}=\\left(e_{i j k} x_{j} y_{k}\\right) \\quad \\text { and } \\quad \\mathbf{y} \\times \\mathbf{x}=\\left(e_{i j k} y_{j} x_{k}\\right)\n$$\n\nBut $e_{i k j}=-e_{i j k}$, so that\n\n$$\ne_{i j k} y_{j} x_{k}=e_{i k j} y_{k} x_{j}=-e_{i j k} y_{k} x_{j}=-e_{i j k} x_{j} y_{k}\n$$\n\n\\section*{INVERTING A MATRIX}\n2.10 Establish the generalized Laplace expansion theorem: $a_{r j} A_{s j}=|A| \\delta_{r s}$.\n\nConsider the matrix\n\n$$\nA^{*}=\\left[\\begin{array}{cccc}\na_{11} & a_{12} & \\ldots & a_{1 n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\cdots & \\ldots & \\cdots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{n 1} & a_{n 2} & \\ldots & a_{n n}\n\\end{array}\\right] \\text { row } \\boldsymbol{r}\n$$\n\nwhich is obtained from matrix $A$ by replacing its $s$ th row by its $r$ th row $(r \\neq s)$. By (2.6), applied to row $r$ of $A^{*}$,\n\n$$\n\\operatorname{det} A^{*}=a_{r j} A_{r j}^{*} \\quad(\\text { not summed on } r)\n$$\n\nNow, because rows $r$ and $s$ are identical, we have for all $j$,\n\n$$\nA_{r j}^{*}=(-1)^{p} A_{s j}^{*}=(-1)^{p} A_{s j} \\quad(p \\equiv r-s)\n$$\n\nTherefore, $\\operatorname{det} A^{*}=(-1)^{p} a_{r j} A_{s j}$. But it is easy to see (Problem 2.31) that, with two rows the same, $\\operatorname{det} A^{*}=0$. We have thus proved that\n\n$$\na_{r j} A_{s j}=0 \\quad(r \\neq s)\n$$\n\nand this, together with (2.6) for the case $r=s$, yields the theorem.\n\n2.11 Given a matrix $A \\equiv\\left[a_{i j}\\right]_{n n}$, with $|A| \\neq 0$, use Problem 2.10 to show that\n\n$$\nA B=I \\quad \\text { where } \\quad B=\\frac{1}{|A|}\\left[A_{i j}\\right]_{n n}^{T}\n$$\n\nSince the $(i, j)$-element of $B$ is $A_{j i} /|A|$,\n\n$$\nA B=\\left[a_{i k}\\left(A_{j k} /|A|\\right)\\right]_{n n}=\\frac{1}{|A|}\\left[|A| \\delta_{i j}\\right]_{n n}=\\frac{|A|}{|A|}\\left[\\delta_{i j}\\right]_{n n}=I\n$$\n\n[It follows from basic facts of linear algebra that also $B A=I$; therefore, $A^{-1}=B$, which establishes (2.11a).]\n\n2.12 Invert the matrix\n\n$$\nA=\\left[\\begin{array}{rrr}\n-2 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n2 & -2 & 3\n\\end{array}\\right]\n$$\n\nrow:\n\nUse (2.11c). To evaluate $|A|$, add twice the third column to the first and then expand by the first\n\n$$\n|A|=\\left|\\begin{array}{rrr}\n0 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n8 & -2 & 3\n\\end{array}\\right|=1 \\cdot\\left|\\begin{array}{rr}\n3 & 1 \\\\\n8 & -2\n\\end{array}\\right|=-6-8=-14\n$$\n\nThen, computing cofactors as we go,\n\n$$\nA^{-1}=\\frac{1}{-14}\\left[\\begin{array}{rrr}\n3 & -2 & -1 \\\\\n-9 & -8 & 3 \\\\\n-8 & -4 & -2\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n-3 / 14 & 1 / 7 & 1 / 14 \\\\\n9 / 14 & 4 / 7 & -3 / 14 \\\\\n4 / 7 & 2 / 7 & 1 / 7\n\\end{array}\\right]\n$$\n\n2.13 Let $A$ and $B$ be invertible matrices of the same order. Prove that $(a)\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$ (i.e., the operations of transposition and inversion commute); (b) $(A B)^{-1}=B^{-1} A^{-1}$.\n\n(a) Transpose the equations $A A^{-1}=A^{-1} A=I$, recalling Problem 2.3, to obtain\n\n$$\n\\left(A^{-1}\\right)^{T} A^{T}=A^{T}\\left(A^{-1}\\right)^{T}=I^{T}=I\n$$\n\nwhich show that $A^{T}$ is invertible, with inverse $\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$.\n\n(b) By the associative law for matrix multiplication,\n\n$$\n(A B)\\left(B^{-1} A^{-1}\\right)=A\\left(B B^{-1}\\right) A^{-1}=A I A^{-1}=A A^{-1}=I\n$$\n\nand, similarly,\n\n$$\n\\left(B^{-1} A^{-1}\\right)(A B)=I\n$$\n\nHence, $(A B)^{-1}=B^{-1} A^{-1}$.\n\n\\section*{LINEAR SYSTEMS; QUADRATIC FORMS}\n2.14 Write the following system of equations in matrix form, then solve by using the inverse matrix:\n\n$$\n\\begin{aligned}\n3 x-4 y & =-18 \\\\\n-5 x+8 y & =34\n\\end{aligned}\n$$\n\nThe matrix form of the system is\n\n\\[\n\\left[\\begin{array}{rr}\n3 & -4  \\tag{1}\\\\\n-5 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]\n\\]\n\nThe inverse of the $2 \\times 2$ coefficient matrix is:\n\n$$\n\\left[\\begin{array}{rr}\n3 & -4 \\\\\n-5 & 8\n\\end{array}\\right]^{-1}=\\frac{1}{24-(+20)}\\left[\\begin{array}{ll}\n8 & 4 \\\\\n5 & 3\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\n$$\n\nPremultiplying (1) by this matrix gives\n\n$$\nI\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-2 \\\\\n3\n\\end{array}\\right]\n$$\n\nor $x=-2, y=3$.\n\n2.15 If $\\left[b^{i j}\\right]=\\left[a_{i j}\\right]^{-1}$, solve the $n \\times n$ system\n\n\n\\begin{equation*}\ny_{i}=a_{i j} x_{j} \\tag{1}\n\\end{equation*}\n\n\nfor the $x_{j}$ in terms of the $y_{i}$.\n\nMultiply both sides of (1) by $b^{k i}$ and sum on $i$ :\n\n$$\nb^{k i} y_{i}=b^{k i} a_{i j} x_{j}=\\delta_{j}^{k} x_{j}=x_{k}\n$$\n\nTherefore, $x_{j}=b^{j i} y_{i}$.\n\n2.16 Write the quadratic form in $\\mathbf{R}^{4}$\n\n$$\nQ=7 x_{1}^{2}-4 x_{1} x_{3}+3 x_{1} x_{4}-x_{2}^{2}+10 x_{2} x_{4}+x_{3}^{2}-6 x_{3} x_{4}+3 x_{4}^{2}\n$$\n\nin the matrix form $\\mathbf{x}^{T} A \\mathbf{x}$ with $A$ symmetric.\n\n$$\nQ=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -4 & 3 \\\\\n0 & -1 & 0 & 10 \\\\\n0 & 0 & 1 & -6 \\\\\n0 & 0 & 0 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -2 & 3 / 2 \\\\\n0 & -1 & 0 & 5 \\\\\n-2 & 0 & 1 & -3 \\\\\n3 / 2 & 5 & -3 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]\n$$\n\n\\section*{LINEAR TRANSFORMATIONS}\n2.17 Show that under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$, the quadric hypersurface $c_{i j} x_{i} x_{j}=1$ transforms to $\\bar{c}_{i j} \\bar{x}_{i} \\bar{x}_{j}=1$, where\n\n$$\n\\bar{c}_{i j}=c_{r s} b_{r i} b_{s j} \\quad \\text { with } \\quad\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}\n$$\n\nThis will be worked using matrices, from which the component form can be easily deduced. The hypersurface has the equation $\\mathbf{x}^{T} C \\mathbf{x}=1$ in unbarred coordinates, and $\\overline{\\mathbf{x}}=A \\mathbf{x}$ defines a barred coordinate system. Substituting $\\mathbf{x}=B \\overline{\\mathbf{x}} \\quad\\left(B=A^{-1}\\right)$ into the equation of the quadric, we have\n\n$$\n(B \\overline{\\mathbf{x}})^{T} C(B \\overline{\\mathbf{x}})=1 \\quad \\text { or } \\quad \\overline{\\mathbf{x}}^{T} B^{T} C B \\overline{\\mathbf{x}}=1\n$$\n\nThus, in the barred coordinate system, the equation of the quadric is $\\overline{\\mathbf{x}}^{T} \\bar{C} \\overline{\\mathbf{x}}=1$, where $\\bar{C}=B^{T} C B$.\n\n\\section*{DISTANCE IN A BARRED COORDINATE SYSTEM}\n2.18 Calculate the coefficients $g_{i j}$ in the distance formula (2.14) for the barred coordinate system in $\\mathbf{R}^{2}$ defined by $\\bar{x}_{i}=a_{i j} x_{j}$, where $a_{11}=a_{22}=1, a_{12}=0$, and $a_{21}=2$.\n\nWe have merely to calculate $G=\\left(A A^{T}\\right)^{-1}$, where $A=\\left(a_{i j}\\right)$ :\n\n$$\nA A^{T}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & 2 \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n1 \\cdot 1+0 & 1 \\cdot 2+0 \\\\\n2 \\cdot 1+0 & 2 \\cdot 2+1 \\cdot 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]\n$$\n\nBy $(2.11 b)$,\n\n$$\n\\left(A A^{T}\\right)^{-1}=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]^{-1}=\\frac{1}{5-4}\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]\n$$\n\nThus, $g_{11}=5, g_{12}=g_{21}=-2, g_{22}=1$.\n\n2.19 Test the distance formula obtained in Problem 2.18 by finding the distance between the aliases of $\\left(x_{i}\\right)=(1,-3)$ and $\\left(y_{i}\\right)=(0,-2)$, which points are a distance $\\sqrt{2}$ apart.\n\nThe coordinates for the given points in the barred system are found to be\n\n$$\n\\overline{\\mathbf{x}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n1 \\\\\n-3\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n-1\n\\end{array}\\right] \\quad \\overline{\\mathbf{y}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]=\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]\n$$\n\nor $\\left(\\bar{x}_{i}\\right)=(1,-1)$ and $\\left(\\bar{y}_{i}\\right)=(0,-2)$. Using the $g_{i j}$ calculated in Problem 2.18,\n\n$$\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\sqrt{5(1-0)^{2}-2 \\cdot 2(1-0)(-1+2)+1(-1+2)^{2}}=\\sqrt{2}\n$$\n\n2.20 Prove formula (2.14).\n\nIn unbarred coordinates, the distance formula has the matrix form\n\n$$\nd(\\mathbf{x}, \\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|=\\sqrt{(\\mathbf{x}-\\mathbf{y})^{T}(\\mathbf{x}-\\mathbf{y})}\n$$\n\nNow, $\\overline{\\mathbf{x}}=A \\mathbf{x}$ or $\\mathbf{x}=B \\overline{\\mathbf{x}}$, where $B=A^{-1}$; so we have by substitution,\n\n$$\n\\begin{aligned}\nd(\\mathbf{x}, \\mathbf{y}) & =\\sqrt{(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})^{T}(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})}=\\sqrt{(B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}))^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} B^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})}=\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} G(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})\n\\end{aligned}\n$$\n\nwhere $G \\equiv B^{T} B=\\left(A^{-1}\\right)^{T} A^{-1}=\\left(A^{T}\\right)^{-1} A^{-1}=\\left(A A^{T}\\right)^{-1}$, the last two equalities following from Problem 2.13 .\n\n\\section*{RECTANGULAR COORDINATES}\n2.21 Suppose that $\\left(x^{i}\\right)=(x, y, z)$ and $\\left(\\bar{x}^{i}\\right)=(\\bar{x}, \\bar{y}, \\bar{z})$ (the use of superscripts here anticipates future notation) denote two rectangular coordinate systems at $O$ and that the direction angles of the $\\bar{x}^{i}$-axis relative to the $x$-, $y$-, and $z$-axes are $\\left(\\alpha_{i}, \\beta_{i}, \\gamma_{i}\\right), i=1,2,3$. Show that the correspondence between the coordinate systems is given by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, where $\\mathbf{x}=(x, y, z)$, $\\overline{\\mathbf{x}}=(\\bar{x}, \\bar{y}, \\bar{z})$, and where the matrix\n\n$$\nA=\\left[\\begin{array}{lll}\n\\cos \\alpha_{1} & \\cos \\beta_{1} & \\cos \\gamma_{1} \\\\\n\\cos \\alpha_{2} & \\cos \\beta_{2} & \\cos \\gamma_{2} \\\\\n\\cos \\alpha_{3} & \\cos \\beta_{3} & \\cos \\gamma_{3}\n\\end{array}\\right]\n$$\n\nis orthogonal.\n\nLet the unit vectors along the $\\bar{x}$-, $\\bar{y}$, and $\\bar{z}$-axes be $\\overline{\\mathbf{i}}=\\overrightarrow{O P}, \\overline{\\mathbf{j}}=\\overrightarrow{O Q}$, and $\\overline{\\mathbf{k}}=\\overrightarrow{O R}$, respectively (see Fig. 2-1). If $\\overline{\\mathbf{x}}$ is the position vector of any point $W(x, y, z)$, then\n\n$$\n\\overline{\\mathbf{x}}=\\bar{x} \\overline{\\mathbf{i}}+\\bar{y} \\overline{\\mathbf{j}}+\\bar{z} \\overline{\\mathbf{k}}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-028}\n\\end{center}\n\nFig. 2-1\n\nWe know that the $(x, y, z)$-coordinates of $P$ are $\\left(\\cos \\alpha_{1}, \\cos \\beta_{1}, \\cos \\gamma_{1}\\right)$. Similar statements hold for the coordinates of $Q$ and $R$, respectively. Hence:\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{i}} & =\\left(\\cos \\alpha_{1}\\right) \\mathbf{i}+\\left(\\cos \\beta_{1}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{1}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{j}} & =\\left(\\cos \\alpha_{2}\\right) \\mathbf{i}+\\left(\\cos \\beta_{2}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{2}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{k}} & =\\left(\\cos \\alpha_{3}\\right) \\mathbf{i}+\\left(\\cos \\beta_{3}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nSubstituting these into the expression for $\\overline{\\mathbf{x}}$ and collecting coefficients of $\\mathbf{i}, \\mathbf{j}$, and $\\mathbf{k}$ :\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{x}}=\\left(\\bar{x} \\cos \\alpha_{1}\\right. & \\left.+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\\right) \\mathbf{i} \\\\\n& +\\left(\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3}\\right) \\mathbf{j} \\\\\n& +\\left(\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nHence, the $x$-coordinate of $W$ is the coefficient of $\\mathbf{i}$, or\n\nSimilarly,\n\n$$\nx=\\bar{x} \\cos \\alpha_{1}+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\n$$\n\n$$\n\\begin{aligned}\n& y=\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3} \\\\\n& z=\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\n\\end{aligned}\n$$\n\nIn terms of the matrix $A$ defined above, we can write these three equations in the matrix form\n\n\n\\begin{equation*}\n\\mathbf{x}=A^{T} \\overline{\\mathbf{x}} \\tag{1}\n\\end{equation*}\n\n\nNow, the $(i, j)$-element of the matrix $A A^{T}$ is\n\n$$\n\\cos \\alpha_{i} \\cos \\alpha_{j}+\\cos \\beta_{i} \\cos \\beta_{j}+\\cos \\gamma_{i} \\cos \\gamma_{j}\n$$\n\nfor $i, j=1,2,3$. Note that the diagonal elements,\n\n$$\n\\left(\\cos \\alpha_{i}\\right)^{2}+\\left(\\cos \\beta_{i}\\right)^{2}+\\left(\\cos \\gamma_{i}\\right)^{2} \\quad(i=1,2,3)\n$$\n\nare the three quantities $\\overrightarrow{O P} \\cdot \\overrightarrow{O P}, \\overrightarrow{O Q} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O R} \\cdot \\overrightarrow{O R}$; i.e., they are unity. If $i \\neq j$, then the corresponding element of $A A^{T}$ is either $\\overrightarrow{O P} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O P} \\cdot \\overrightarrow{O R}$, or $\\overrightarrow{O Q} \\cdot \\overrightarrow{O R}$, and is therefore zero (since these vectors are mutually orthogonal). Hence, $A A^{T}=I$ (and also $A^{T} A=I$ ), and, from (1),\n\n$$\nA \\mathbf{x}=A A^{T} \\overrightarrow{\\mathbf{x}}=\\overline{\\mathbf{x}}\n$$\n\n\\section*{CURVILINEAR COORDINATES}\n2.22 A curvilinear coordinate system $(\\bar{x}, \\bar{y})$ is defined in terms of rectangular coordinates $(x, y)$ by\n\n\n\\begin{align*}\n& \\bar{x}=x^{2}-x y  \\tag{1}\\\\\n& \\bar{y}=x y\n\\end{align*}\n\n\nShow that in the barred coordinate system the equation of the line $y=x-1$ is $\\bar{y}=\\bar{x}^{2}-\\bar{x}$. [In the alibi interpretation, (1) deforms the straight line into a parabola.]\n\nIt helps initially to parameterize the equation of the line as $x=t, y=t-1$. Substitution of $x=t$, $y=t-1$ in the change-of-coordinates formula gives the parametric equations of the line in the barred coordinate system:\n\n\n\\begin{align*}\n& \\bar{x}=t^{2}-t(t-1)=t \\\\\n& \\bar{y}=t(t-1)=t^{2}-t \\tag{2}\n\\end{align*}\n\n\nNow $t$ may be eliminated from (2) to give $\\bar{y}=\\bar{x}^{2}-\\bar{x}$.\n\n\\section*{CHAIN RULE}\n2.23 Suppose that under a change of coordinates, $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)(1 \\leqq i \\leqq n)$, the realvalued vector functions $\\left(\\bar{T}_{i}\\right)$ and $\\left(T_{i}\\right)$ are related by the formula\n\n\n\\begin{equation*}\n\\bar{T}_{i}=T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}} \\tag{1}\n\\end{equation*}\n\n\nFind the transformation rule for the partial derivatives of $\\left(T_{i}\\right)$-that is, express the $\\partial \\bar{T}_{i} / \\partial \\bar{x}_{j}$ in terms of the $\\partial T_{r} / \\partial x_{s}$-given that all second-order partial derivatives are zero.\n\nBegin by taking the partial derivative with respect to $\\bar{x}_{j}$ of both sides of (1), using the product rule:\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}=\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}+T_{r} \\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{\\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}\n$$\n\nBy assumption, the second term on the right is zero; and, by the chain rule,\n\n$$\n\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}}\n$$\n\nConsequently, the desired transformation rule is\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\n$$\n\n\\section*{Supplementary Problems}\n2.24 Display the matrices $(a)\\left[u^{i j}\\right]_{35},(b)\\left[u^{j i}\\right]_{35},(c)\\left[u^{i j}\\right]_{53},(d)\\left[\\delta_{j}^{i}\\right]_{36}$.\n\n2.25 Carry out the following matrix multiplications:\n\n$$\n\\text { (a) }\\left[\\begin{array}{rrr}\n3 & -1 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 2 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n2\n\\end{array}\\right] \\quad \\text { (b) }\\left[\\begin{array}{rr}\n3 & -1 \\\\\n2 & 0\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n1 & 1 & -1 \\\\\n2 & 1 & 1\n\\end{array}\\right]\n$$\n\n2.26 Prove by the product rule and by use of the summation convention the associative law for matrices:\n\n$$\n(A B) C=A(B C)\n$$\n\nwhere $A \\equiv\\left(a_{i j}\\right), B \\equiv\\left(b_{i j}\\right)$, and $C \\equiv\\left(c_{i j}\\right)$ are arbitrary matrices, but compatible for multiplication.\n\n2.27 Prove: (a) if $A$ and $B$ are symmetric matrices and if $A B=B A=C$, then $C$ is symmetric; (b) if $A$ and $B$ are skew-symmetric and if $A B=-B A=C$, then $C$ is skew-symmetric.\n\n2.28 Prove that the product of two orthogonal matrices is orthogonal.\n\n2.29 Evaluate the determinants\n\n$$\n\\text { (a) }\\left|\\begin{array}{rr}\n3 & -2 \\\\\n1 & 5\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n2 & 1 & -1 \\\\\n3 & 0 & 1 \\\\\n1 & -1 & 2\n\\end{array}\\right| \\quad \\text { (c) }\\left|\\begin{array}{rrrrr}\n-1 & 1 & -1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n-1 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 0 & 0 & 0\n\\end{array}\\right|\n$$\n\n2.30 In the Laplace expansion of the fourth-order determinant $\\left|a_{i j}\\right|$, the six-term summation $e_{2 i j k} a_{12} a_{2 i} a_{3 j} a_{4 k}$ appears. (a) Write out this sum explicitly, then (b) represent it as a third-order determinant.\n\n2.31 Prove that if a matrix has two rows the same, its determinant is zero. (Hint: First show that interchanging any two subscripts reverses the sign of the permutation symbol.)\n\n2.32 Calculate the inverse of\n\n$$\n\\text { (a) }\\left[\\begin{array}{ll}\n3 & 1 \\\\\n5 & 2\n\\end{array}\\right] \\quad(b)\\left[\\begin{array}{rrr}\n0 & 1 & 2 \\\\\n1 & -1 & 0 \\\\\n2 & 1 & -1\n\\end{array}\\right]\n$$\n\n2.33 (a) Verify the following formulas for the permutation symbols $e_{i j}$ and $e_{i j k}$ (for distinct values of the indices only):\n\n$$\ne_{i j}=\\frac{j-i}{|j-i|} \\quad e_{i j k}=\\frac{(j-i)(k-i)(k-j)}{|j-i||k-i||k-j|}\n$$\n\n(b) Prove the general formula:\n\n$$\ne_{i_{1} i_{2} \\cdots i_{n}}=\\frac{\\left(i_{2}-i_{1}\\right)\\left(i_{3}-i_{1}\\right) \\cdots\\left(i_{n}-i_{1}\\right)\\left(i_{3}-i_{2}\\right) \\cdots\\left(i_{n}-i_{2}\\right) \\cdots\\left(i_{n}-i_{n-1}\\right)}{\\left|i_{2}-i_{1}\\right|\\left|i_{3}-i_{1}\\right| \\cdots\\left|i_{n}-i_{1}\\right|\\left|i_{3}-i_{2}\\right| \\cdots\\left|i_{n}-i_{2}\\right| \\cdots\\left|i_{n}-i_{n-1}\\right|} \\equiv \\prod_{p>q} \\frac{i_{p}-i_{q}}{\\left|i_{p}-i_{q}\\right|}\n$$\n\n2.34 Calculate the angle between the $\\mathbf{R}^{6}$-vectors $\\mathbf{x}=(3,-1,0,1,2,-3)$ and $\\mathbf{y}=(-2,1,0,1,0,0)$.\n\n2.35 Find two linearly independent vectors in $\\mathbf{R}^{3}$ which are orthogonal to the vector $(3,-2,1)$.\n\n2.36 Solve for $x$ and $y$ by use of matrices:\n\n$$\n\\begin{aligned}\n& 3 x-4 y=-23 \\\\\n& 5 x+3 y=10\n\\end{aligned}\n$$\n\n2.37 Write out the quadratic form in $\\mathbf{R}^{3}$ represented by $Q=\\mathbf{x}^{T} A \\mathbf{x}$, where\n\n$$\nA=\\left[\\begin{array}{rrr}\n1 & 4 & 3 \\\\\n4 & 2 & 0 \\\\\n3 & 0 & -1\n\\end{array}\\right]\n$$\n\n2.38 Represent with a symmetric matrix $A$ the quadratic form in $\\mathbf{R}^{4}$\n\n$$\nQ=-3 x_{1}^{2}-x_{2}^{2}+x_{3}^{2}-x_{1} x_{2}-x_{1} x_{3}+6 x_{1} x_{4}\n$$\n\n2.39 Given the hyperplane $c_{r} x_{r}=1$, how do the coefficients $c_{i}$ transform under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$ ?\n\n2.40 Calculate the $g_{i j}$ for the distance formula (2.14) in a barred coordinate system defined by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, with\n\n$$\nA=\\left[\\begin{array}{rr}\n1 & -2 \\\\\n2 & 3\n\\end{array}\\right]\n$$\n\n2.41 Test the distance formula of Problem 2.40 on the pair of points whose unbarred coordinates are $(2,-1)$ and $(2,-4)$.\n\n2.42 (a) Show that for independent functions $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$,\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=\\delta_{j}^{i} \\tag{1}\n\\end{equation*}\n\n\n(b) Take the partial derivative with respect to $x_{k}$ of (1) to establish the formula\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\bar{x}_{i}}{\\partial x_{k} \\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=-\\frac{\\partial^{2} x_{r}}{\\partial \\bar{x}_{s} \\partial \\bar{x}_{j}} \\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial \\bar{x}_{s}}{\\partial x_{k}} \\tag{2}\n\\end{equation*}\n\n\n"], "lesson": "\\section*{Chapter 2}\n\\section*{Basic Linear Algebra for Tensors}\n\\subsection*{2.1 INTRODUCTION}\nFamiliarity with the topics in this chapter will result in a much greater appreciation for the geometric aspects of tensor calculus. The main purpose is to reformulate the expressions of linear algebra and matrix theory using the summation convention.\n\n\\subsection*{2.2 TENSOR NOTATION FOR MATRICES, VECTORS, AND DETERMINANTS}\nIn the ordinary matrix notation $\\left(a_{i j}\\right)$, the first subscript, $i$, tells what row the number $a_{i j}$ lies in, and the second, $j$, designates the column. A fuller notation is $\\left[a_{i j}\\right]_{m n}$, which exhibits the number of rows, $m$, and the number of columns, $n$. This notation may be extended as follows.\n\n\\section*{Upper-Index Matrix Notation}\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-017}\n\\end{center}\n\nNote that, for mixed indices (one upper, one lower), it is the upper index that designates the row, and the lower index, the column. In the case of pure superscripts, the scheme is identical to the familiar one for subscripts.\n\n\\section*{EXAMPLE 2.1}\n$$\n\\begin{array}{r}\n{\\left[c_{j}^{i}\\right]_{23} \\equiv\\left[\\begin{array}{lll}\nc_{1}^{1} & c_{2}^{1} & c_{3}^{1} \\\\\nc_{1}^{2} & c_{2}^{2} & c_{3}^{2}\n\\end{array}\\right] .\\left[d_{i}^{j}\\right]_{23} \\equiv\\left[\\begin{array}{lll}\nd_{1}^{1} & d_{2}^{1} & d_{3}^{1} \\\\\nd_{1}^{2} & d_{2}^{2} & d_{3}^{2}\n\\end{array}\\right] \\equiv\\left[d_{j}^{i}\\right]_{23}} \\\\\n{\\left[x_{s}^{r}\\right]_{14} \\equiv\\left[\\begin{array}{llll}\nx_{1}^{1} & x_{2}^{1} & x_{3}^{1} & x_{4}^{1}\n\\end{array}\\right] \\quad\\left[y^{p q}\\right]_{42} \\equiv\\left[\\begin{array}{ll}\ny^{11} & y^{12} \\\\\ny^{21} & y^{22} \\\\\ny^{31} & y^{32} \\\\\ny^{41} & y^{42}\n\\end{array}\\right]}\n\\end{array}\n$$\n\n\\section*{Vectors}\nA real $n$-dimensional vector is any column matrix $\\mathbf{v}=\\left[x_{i j}\\right]_{n 1}$ with real components $x_{i} \\equiv x_{i 1}$; one usually writes simply $\\mathbf{v}=\\left(x_{i}\\right)$. The collection of all real $n$-dimensional vectors is the $n$-dimensional real vector space denoted $\\mathbf{R}^{n}$.\n\nVector sums are determined by coordinatewise addition, as are matrix sums: if $A \\equiv\\left[a_{i j}\\right]_{m n}$ and $B \\equiv\\left[b_{i j}\\right]_{m n}$, then\n\n$$\nA+B \\equiv\\left[a_{i j}+b_{i j}\\right]_{m n}\n$$\n\nScalar multiplication of a vector or matrix is defined by\n\n$$\n\\lambda\\left[a_{i j}\\right]_{m n} \\equiv\\left[\\lambda a_{i j}\\right]_{m n}\n$$\n\n\\section*{Basic Formulas}\nThe essential formulas involving matrices, vectors, and determinants are now given in terms of the summation convention.\n\nMatrix multiplication. If $A \\equiv\\left[a_{i j}\\right]_{m n}$ and $B \\equiv\\left[b_{i j}\\right]_{n k}$, then\n\n\n\\begin{equation*}\nA B=\\left[a_{i r} b_{r j}\\right]_{m k} \\tag{2.1a}\n\\end{equation*}\n\n\nAnalogously, for mixed or upper indices,\n\n\n\\begin{equation*}\nA B \\equiv\\left[a_{j}^{i}\\right]_{m n}\\left[b_{j}^{i}\\right]_{n k}=\\left[a_{r}^{i} b_{j}^{r}\\right]_{m k} \\quad A B \\equiv\\left[a^{i j}\\right]_{m n}\\left[b^{i j}\\right]_{n k}=\\left[a^{i r} b^{r j}\\right]_{m k} \\tag{2.1b}\n\\end{equation*}\n\n\nwherein $i$ and $j$ are not summed on.\n\nIdentity matrix. In terms of the Kronecker deltas, the identity matrix of order $n$ is\n\n$$\nI=\\left[\\delta_{i j}\\right]_{n n} \\equiv\\left[\\delta_{j}^{i}\\right]_{n n} \\equiv\\left[\\delta^{i j}\\right]_{n n}\n$$\n\nwhich has the property $I A=A I=A$ for any square matrix $A$ of order $n$.\n\nInverse of a square matrix. A square matrix $A \\equiv\\left[a_{i j}\\right]_{n n}$ is invertible if there exists a (unique) matrix $B \\equiv\\left[b_{i j}\\right]_{n n}$, called the inverse of $A$, such that $A B=B A=I$. In terms of components, the criterion reads:\n\n\n\\begin{equation*}\na_{i r} b_{r j}=b_{i r} a_{r j}=\\delta_{i j} \\tag{2.2a}\n\\end{equation*}\n\n\nor, for mixed or upper indices,\n\n\n\\begin{equation*}\na_{r}^{i} b_{j}^{r}=b_{r}^{i} a_{j}^{r}=\\delta_{j}^{i} \\quad a^{i r} b^{r j}=b^{i r} a^{r j}=\\delta^{i j} \\tag{2.2b}\n\\end{equation*}\n\n\nTranspose of a matrix. Transposition of an arbitrary matrix is defined by $A^{T} \\equiv\\left[a_{i j}\\right]_{m n}^{T}=\\left[a_{i j}^{\\prime}\\right]_{n m}$, where $a_{i j}^{\\prime}=a_{j i}$ for all $i$, $j$. If $A^{T}=A$ (that is, $a_{i j}=a_{j i}$ for all $i, j$ ), then $A$ is called symmetric; if $A^{T}=-A$ (that is, $a_{i j}=-a_{j i}$ for all $i, j$ ), then $A$ is called antisymmetric or skew-symmetric.\n\nOrthogonal matrix. A matrix $A$ is orthogonal if $A^{T}=A^{-1}$ (or if $A^{T} A=A A^{T}=I$ ).\n\nPermutation symbol. The symbol $e_{i j k \\ldots w}$ (with $n$ subscripts) has the value zero if any pair of subscripts are identical, and equals $(-1)^{p}$ otherwise, where $p$ is the number of subscript transpositions (interchanges of consecutive subscripts) required to bring $(i j k \\ldots w)$ to the natural order $(123 \\ldots n)$.\n\nDeterminant of a square matrix. If $A \\equiv\\left[a_{i j}\\right]_{n n}$ is any square matrix, define the scalar\n\n\n\\begin{equation*}\n\\operatorname{det} A \\equiv e_{i_{1} i_{2} i_{3} \\cdots i_{n}} a_{1 i_{1}} a_{2 i_{2}} a_{3 i_{3}} \\cdots a_{n i_{n}} \\tag{2.3}\n\\end{equation*}\n\n\nOther notations are $|A|,\\left|a_{i j}\\right|$, and $\\operatorname{det}\\left(a_{i j}\\right)$. The chief properties of determinants are\n\n\n\\begin{equation*}\n|A B|=|A||B| \\quad\\left|A^{T}\\right|=|A| \\tag{2.4}\n\\end{equation*}\n\n\nLaplace expansion of a determinant. For each $i$ and $j$, let $M_{i j}$ be the determinant of the square matrix of order $n-1$ obtained from $A \\equiv\\left[a_{i j}\\right]_{n n}$ by deleting the $i$ th row and $j$ th column; $M_{i j}$ is called the minor of $a_{i j}$ in $|A|$. Define the cofactor of $a_{i j}$ to be the scalar\n\n\n\\begin{equation*}\nA_{i j}=(-1)^{k} M_{i j} \\quad \\text { where } \\quad k=i+j \\tag{2.5}\n\\end{equation*}\n\n\nThen the Laplace expansions of $|A|$ are given by\n\n$$\n\\begin{aligned}\n& |A|=a_{1 j} A_{1 j}=a_{2 j} A_{2 j}=\\cdots=a_{n j} A_{n j} \\quad \\text { [row expansions] } \\\\\n& |A|=a_{i 1} A_{i 1}=a_{i 2} A_{i 2}=\\cdots=a_{i n} A_{\\text {in }} \\quad \\text { [column expansions] }\n\\end{aligned}\n$$\n\nScalar product of vectors. If $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, then\n\n\n\\begin{equation*}\n\\mathbf{u v} \\equiv \\mathbf{u} \\cdot \\mathbf{v} \\equiv \\mathbf{u}^{T} \\mathbf{v}=x_{i} y_{i} \\tag{2.7}\n\\end{equation*}\n\n\nIf $\\mathbf{u}=\\mathbf{v}$, the notation $\\mathbf{u u} \\equiv \\mathbf{u}^{2} \\equiv \\mathbf{v}^{2}$ will often be used. Vectors $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal if $\\mathbf{u v}=0$.\n\nNorm (length) of a vector. If $\\mathbf{u}=\\left(x_{i}\\right)$, then\n\n\n\\begin{equation*}\n\\|\\mathbf{u}\\| \\equiv \\sqrt{\\mathbf{u}^{2}}=\\sqrt{x_{i} x_{i}} \\tag{2.8}\n\\end{equation*}\n\n\nAngle between two vectors. The angle $\\theta$ between two nonzero vectors, $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, is defined by\n\n\n\\begin{equation*}\n\\cos \\theta \\equiv \\frac{\\mathbf{u v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}=\\frac{x_{i} y_{i}}{\\sqrt{x_{j} x_{j}} \\sqrt{y_{k} y_{k}}} \\quad(0 \\leqq \\theta \\leqq \\pi) \\tag{2.9}\n\\end{equation*}\n\n\nIt follows that $\\theta=\\pi / 2$ if $\\mathbf{u}$ and $\\mathbf{v}$ are nonzero orthogonal vectors.\n\nVector product in $\\mathbf{R}^{3}$. If $\\mathbf{u}=\\left(x_{i}\\right)$ and $\\mathbf{v}=\\left(y_{i}\\right)$, and if the standard basis vectors are designated\n\n$$\n\\mathbf{i} \\equiv\\left(\\delta_{i 1}\\right) \\quad \\mathbf{j} \\equiv\\left(\\delta_{i 2}\\right) \\quad \\mathbf{k} \\equiv\\left(\\delta_{i 3}\\right)\n$$\n\nthen\n\n\\[\n\\mathbf{u} \\times \\mathbf{v} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k}  \\tag{2.10a}\\\\\nx_{1} & x_{2} & x_{3} \\\\\ny_{1} & y_{2} & y_{3}\n\\end{array}\\right|=\\left|\\begin{array}{ll}\nx_{2} & x_{3} \\\\\ny_{2} & y_{3}\n\\end{array}\\right| \\mathbf{i}-\\left|\\begin{array}{ll}\nx_{1} & x_{3} \\\\\ny_{1} & y_{3}\n\\end{array}\\right| \\mathbf{j}+\\left|\\begin{array}{ll}\nx_{1} & x_{2} \\\\\ny_{1} & y_{2}\n\\end{array}\\right| \\mathbf{k}\n\\]\n\nExpressing the second-order determinants by means of $(2.3)$, one can rewrite $(2.10 a)$ in terms of components only:\n\n\n\\begin{equation*}\n\\mathbf{u} \\times \\mathbf{v}=\\left(e_{i j k} x_{j} y_{k}\\right) \\tag{2.10b}\n\\end{equation*}\n\n\n\\subsection*{2.3 INVERTING A MATRIX}\nThere are a number of algorithms for computing the inverse of $A \\equiv\\left[a_{i j}\\right]_{n n}$, where $|A| \\neq 0$ (a necessary and sufficient condition for $A$ to be invertible). When $n$ is large, the method of elementary row operations is efficient. For small $n$, it is practical to apply the explicit formula\n\n\n\\begin{equation*}\nA^{-1}=\\frac{1}{|A|}\\left[A_{i j}\\right]_{n n}^{T} \\tag{2.11a}\n\\end{equation*}\n\n\nThus, for $n=2$,\n\n\\[\n\\left[\\begin{array}{ll}\na_{11} & a_{12}  \\tag{2.11b}\\\\\na_{21} & a_{22}\n\\end{array}\\right]^{-1}=\\frac{1}{|A|}\\left[\\begin{array}{rr}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{array}\\right]\n\\]\n\nin which $|A|=a_{11} a_{22}-a_{12} a_{21}$; and, for $n=3$,\n\nin which\n\n\\[\n\\left[\\begin{array}{lll}\na_{11} & a_{12} & a_{13}  \\tag{2.11c}\\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right]^{-1}=\\frac{1}{|A|}\\left[\\begin{array}{lll}\nA_{11} & A_{21} & A_{31} \\\\\nA_{12} & A_{22} & A_{32} \\\\\nA_{13} & A_{23} & A_{33}\n\\end{array}\\right]\n\\]\n\n$$\nA_{11}=a_{22} a_{33}^{\\dot{b}}-a_{23} a_{32} \\quad A_{21}=-\\left(a_{12} a_{33}-a_{13} a_{32}\\right) \\quad \\ldots\n$$\n\n\\subsection*{2.4 MATRIX EXPRESSIONS FOR LINEAR SYSTEMS AND QUADRATIC FORMS}\nBecause of the product rule for matrices and the rule for matrix equality, one can write a system of equations such as\n\n$$\n\\begin{array}{r}\n3 x-4 y=2 \\\\\n-5 x+8 y=7\n\\end{array}\n$$\n\nin the matrix form\n\n$$\n\\left[\\begin{array}{rr}\n3 & -4 \\\\\n-5 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{l}\n2 \\\\\n7\n\\end{array}\\right]\n$$\n\nIn general, any $m \\times n$ system of equations\n\n\n\\begin{equation*}\na_{i j} x_{j}=b_{i} \\quad(1 \\leqq i \\leqq m) \\tag{2.12a}\n\\end{equation*}\n\n\ncan be written in the matrix form\n\n\n\\begin{equation*}\nA \\mathbf{x}=\\mathbf{b} \\tag{2.12b}\n\\end{equation*}\n\n\nwhere $A \\equiv\\left[a_{i j}\\right]_{m n}, \\mathbf{x} \\equiv\\left(x_{i}\\right)$, and $\\mathbf{b} \\equiv\\left(b_{i}\\right)$. One advantage in doing this is that, if $m=n$ and $A$ is invertible, the solution of the system can proceed entirely by matrices: $\\mathbf{x}=A^{-1} \\mathbf{b}$.\n\nAnother useful fact for work with tensors is that a quadratic form $Q$ (a homogeneous second-degree polynomial) in the $n$ variables $x_{1}, x_{2}, \\ldots, x_{n}$ also has a strictly matrix representation:\n\n\n\\begin{equation*}\nQ=a_{i j} x_{i} x_{j}=\\mathbf{x}^{T} A \\mathbf{x} \\tag{2.13}\n\\end{equation*}\n\n\nwhere the row matrix $\\mathbf{x}^{T}$ is the transpose of the column matrix $\\mathbf{x}=\\left(x_{i}\\right)$ and where $A \\equiv\\left[a_{i j}\\right]_{n n}$.\n\n\\section*{EXAMPLE 2.2}\n$$\n\\left[\\begin{array}{lll}\nx_{1} & x_{2} & x_{3}\n\\end{array}\\right]\\left[\\begin{array}{lll}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\nx_{1} & x_{2} & x_{3}\n\\end{array}\\right]\\left[\\begin{array}{l}\na_{1 j} x_{j} \\\\\na_{2 j} x_{j} \\\\\na_{3 j} x_{j}\n\\end{array}\\right]=\\left[x_{i}\\left(a_{i j} x_{j}\\right)\\right]=a_{i j} x_{i} x_{j}\n$$\n\nThe matrix $A$ that produces a given quadratic form is not unique. In fact, the matrix $B=\\frac{1}{2}\\left(A+A^{T}\\right)$ may always be substituted for $A$ in (2.13); i.e., the matrix of a quadratic form may always be assumed symmetric.\n\nEXAMPLE 2.3 Write the quadratic equation\n\n$$\n3 x^{2}+y^{2}-2 z^{2}-5 x y-6 y z=10\n$$\n\nusing a symmetric matrix.\n\nThe quadratic form (2.13) is given in terms of the nonsymmetric matrix\n\n$$\nA=\\left[\\begin{array}{rrr}\n3 & -5 & 0 \\\\\n0 & 1 & -6 \\\\\n0 & 0 & -2\n\\end{array}\\right]\n$$\n\nThe symmetric equivalent is obtained by replacing each off-diagonal element by one-half the sum of that element and its mirror image in the main diagonal. Hence, the desired representation is\n\n$$\n\\left[\\begin{array}{lll}\nx & y & z\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n3 & -5 / 2 & 0 \\\\\n-5 / 2 & 1 & -3 \\\\\n0 & -3 & -2\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right]=10\n$$\n\n\\subsection*{2.5 LINEAR TRANSFORMATIONS}\nOf utmost importance for the study of tensor calculus is a basic knowledge of transformation theory and changes in coordinate systems. A set of linear equations like\n\n\n\\begin{align*}\n& \\bar{x}=5 x-2 y \\\\\n& \\bar{y}=3 x+2 y \\tag{I}\n\\end{align*}\n\n\ndefines a linear transformation (or linear mapping) from each point $(x, y)$ to its corresponding image $(\\bar{x}, \\bar{y})$. In matrix form, a, linear transformation may be written $\\overline{\\mathbf{x}}=A \\mathbf{x}$; if, as in $(I)$, the mapping is one-one, then $|A| \\neq 0$.\n\nThere is always an alias-alibi aspect of such transformations: When $(\\bar{x}, \\bar{y})$ is regarded as defining new coordinates (a new name) for $(x, y)$, one is dealing with the alias aspect; when $(\\bar{x}, \\bar{y})$ is regarded as a new position (place) for $(x, y)$, the alibi aspect emerges. In tensor calculus, one is generally more interested in the alias aspect: the two coordinate systems related by $\\overline{\\mathbf{x}}=A \\mathbf{x}$ are referred to as the unbarred and the barred systems.\n\nEXAMPLE 2.4 In order to find the image of the point $(0,-1)$ under $(I)$, merely set $x=0$ and $y=-1$; the result is\n\n$$\n\\bar{x}=5(0)-2(-1)=2 \\quad \\bar{y}=3(0)+2(-1)=-2\n$$\n\nHence, $\\overline{(0,-1)}=(2,-2)$. Similarly, we find that $\\overline{(2,1)}=(8,8)$.\n\nIf we regard $(\\bar{x}, \\bar{y})$ merely as a different coordinate system, we would say that two fixed points, $P$ and $Q$, have the respective coordinates $(0,-1)$ and $(2,1)$ in the unbarred system, and $(2,-2)$ and $(8,8)$ in the barred system.\n\n\\section*{Distance in a Barred Coordinate System}\nWhat is the expression for the (invariant) distance between two points in terms of differing aliases? Let $\\overline{\\mathbf{x}}=A \\mathbf{x}(|A| \\neq 0)$ define an invertible linear transformation between unbarred and barred coordinates. It is shown in Problem 2.20 that the desired distance formula is\n\n\n\\begin{equation*}\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} G(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})}=\\sqrt{g_{i j} \\Delta \\bar{x}_{i} \\Delta \\bar{x}_{j}} \\tag{2.14}\n\\end{equation*}\n\n\nwhere $\\left[g_{i j}\\right]_{n n} \\equiv G=\\left(A A^{T}\\right)^{-1}$ and $\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}=\\left(\\Delta \\bar{x}_{i}\\right)$. If $A$ is orthogonal (a rotation of the axes), then $g_{i j}=\\delta_{i j}$, and (2.14) reduces to the ordinary form\n\n$$\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\|\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}\\|=\\sqrt{\\Delta \\bar{x}_{i} \\Delta \\bar{x}_{i}}\n$$\n\n[cf. (2.8)].\n\nEXAMPLE 2.5 Calculate the distance between points $P$ and $Q$ of Example 2.4 in terms of their barred coordinates. Verify that the same distance is found in the unbarred coordinate system.\n\nFirst calculate the matrix $G=\\left(A A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T} A^{-1}$ (see Problem 2.13):\n\nand $\\quad G=\\frac{1}{16}\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]^{T} \\cdot \\frac{1}{16}\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]=\\frac{1}{256}\\left[\\begin{array}{rr}2 & -3 \\\\ 2 & 5\\end{array}\\right]\\left[\\begin{array}{rr}2 & 2 \\\\ -3 & 5\\end{array}\\right]=\\frac{1}{256}\\left[\\begin{array}{rr}13 & -11 \\\\ -11 & 29\\end{array}\\right]$\n\nHence $g_{11}=13 / 256, g_{12}=g_{21}=-11 / 256$, and $g_{22}=29 / 256$. Now, with $\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}=\\left[\\begin{array}{ll}2-8 & -2-8\\end{array}\\right]^{T}=\\left[\\begin{array}{ll}-6 & -10\\end{array}\\right]^{T}$, (2.14) gives:\n\n$$\n\\begin{aligned}\nd^{2} & =g_{i j} \\Delta \\bar{x}_{i} \\Delta \\bar{x}_{j} \\\\\n& =\\frac{13}{256}(-6)^{2}+2 \\cdot \\frac{-11}{256}(-6)(-10)+\\frac{29}{256}(-10)^{2} \\\\\n& =\\frac{13(36)-22(60)+29(100)}{256}=8\n\\end{aligned}\n$$\n\nIn the unbarred system, the distance between $P(0,-1)$ and $Q(2,1)$ is given, in agreement, by the Pythagorean theorem:\n\n$$\nd^{2}=(0-2)^{2}+(-1-1)^{2}=8\n$$\n\n\\subsection*{2.6 GENERAL COORDINATE TRANSFORMATIONS}\nA general mapping or transformation $T$ of $\\mathbf{R}^{n}$ may be indicated in functional (vector) or in component form:\n\n$$\n\\overline{\\mathbf{x}}=T(\\mathbf{x}) \\quad \\text { or } \\quad \\bar{x}_{i}=T_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\n$$\n\nIn the alibi description, any point $\\mathbf{x}$ in the domain of $T$ (possibly the whole of $\\mathbf{R}^{n}$ ) has as its image the point $T(\\mathbf{x})$ in the range of $T$. Considered as a coordinate transformation (the alias description), $T$ sets up, for each point $P$ in its domain, a correspondence between $\\left(x_{i}\\right)$ and $\\left(\\bar{x}_{i}\\right)$, the coordinates of $P$ in two different systems. As explained below, $T$ may be interpreted as a coordinate transformation only if a certain condition is fulfilled.\n\n\\section*{Bijections, Curvilinear Coordinates}\nA map $T$ is called a bijection or a one-one mapping if it maps each pair of distinct points $\\mathbf{x} \\neq \\mathbf{y}$ in its domain into distinct points $T(\\mathbf{x}) \\neq T(\\mathbf{y})$ in its range. Whenever $T$ is bijective, we call the image $\\overline{\\mathbf{x}}=T(\\mathbf{x})$ a set of admissible coordinates for $\\mathbf{x}$, and the aggregate of all such coordinates (alibi: the range of $T$ ), a coordinate system.\n\nCertain coordinate systems are named after the characteristics of the mapping $T$. For example, if $T$ is linear, the $\\left(\\bar{x}_{i}\\right)$-system is called affine; and if $T$ is a rigid motion, $\\left(\\bar{x}_{i}\\right)$ is called rectangular or cartesian. [It is presumed in making this statement that the original coordinate system $\\left(x_{i}\\right)$ is the familiar cartesian coordinate system of analytic geometry, or its natural extension to vectors in $\\mathbf{R}^{n}$.] Nonaffine coordinate systems are generally called curvilinear coordinates; these include polar coordinates in two dimensions, and cylindrical and spherical coordinates in three dimensions.\n\n\\subsection*{2.7 THE CHAIN RULE FOR PARTIAL DERIVATIVES}\nIn working with curvilinear coordinates, one needs the Jacobian matrix (Chapter 3) and, therefore, the chain rule of multivariate calculus. The summation convention makes possible a compact statement of this rule: If $w=f\\left(x_{1}, x_{2}, x_{3}, \\ldots, x_{n}\\right)$ and $x_{i}=x_{i}\\left(u_{1}, u_{2}, \\ldots, u_{m}\\right) \\quad(i=$ $1,2, \\ldots, n)$, where all functions involved have continuous partial derivatives, then\n\n\n\\begin{equation*}\n\\frac{\\partial w}{\\partial u_{j}}=\\frac{\\partial f}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\partial u_{j}} \\quad(1 \\leqq j \\leqq m) \\tag{2.15}\n\\end{equation*}\n\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{TENSOR NOTATION}\n2.1 Display explicitly the matrices $(a)\\left[b_{i}^{j}\\right]_{42},(b)\\left[b_{j}^{i}\\right]_{24},(c)\\left[\\delta^{i j}\\right]_{33}$.\n\n$$\n\\begin{gathered}\n\\text { (a) }\\left[b_{i}^{j}\\right]_{42}=\\left[\\begin{array}{ll}\nb_{1}^{1} & b_{2}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} \\\\\nb_{1}^{3} & b_{2}^{3} \\\\\nb_{1}^{4} & b_{2}^{4}\n\\end{array}\\right] \\quad \\text { (b) }\\left[b_{j}^{i}\\right]_{24}=\\left[\\begin{array}{llll}\nb_{1}^{1} & b_{2}^{1} & b_{3}^{1} & b_{4}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} & b_{3}^{2} & b_{4}^{2}\n\\end{array}\\right] \\\\\n\\text { (c) }\\left[\\delta^{i j}\\right]_{33}=\\left[\\begin{array}{lll}\n\\delta^{11} & \\delta^{12} & \\delta^{13} \\\\\n\\delta^{21} & \\delta^{22} & \\delta^{23} \\\\\n\\delta^{31} & \\delta^{32} & \\delta^{33}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{gathered}\n$$\n\nFrom (a) and (b) it is evident that merely interchanging the indices $i$ and $j$ in a matrix $A \\equiv\\left[a_{i j}\\right]_{m n}$ does not necessarily yield the transpose, $A^{T}$.\n\n\\subsection*{2.2 Given}\n$$\nA=\\left[\\begin{array}{ccc}\na & -a & -a \\\\\n2 b & b & -b \\\\\n4 c & 2 c & -2 c\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{rrr}\n2 & 4 & -6 \\\\\n-1 & -2 & 3 \\\\\n3 & 6 & -9\n\\end{array}\\right]\n$$\n\nverify that $A B \\neq B A$.\n\n$$\nA B=\\left[\\begin{array}{ccc}\n2 a+a-3 a & 4 a+2 a-6 a & -6 a-3 a+9 a \\\\\n4 b-b-3 b & 8 b-2 b-6 b & -12 b+3 b+9 b \\\\\n8 c-2 c-6 c & 16 c-4 c-12 c & -24 c+6 c+18 c\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\equiv \\mathrm{O}\n$$\n\nbut $\\quad B A=\\left[\\begin{array}{ccc}2 a+8 b-24 c & -2 a+4 b-12 c & -2 a-4 b+12 c \\\\ -a-4 b+12 c & a-2 b+6 c & a+2 b-6 c \\\\ 3 a+12 b-36 c & -3 a+6 b-18 c & -3 a-6 b+18 c\\end{array}\\right] \\neq \\mathrm{O}$\n\nThus, the commutative law $(A B=B A)$ fails for matrices. Further, $A B=\\mathrm{O}$ does not imply that $A=\\mathrm{O}$ or $B=$ O.\n\n2.3 Prove by use of tensor notation and the product rule for matrices that $(A B)^{T}=B^{T} A^{T}$, for any two conformable matrices $A$ and $B$.\n\nLet $A \\equiv\\left[a_{i j}\\right]_{m n}, B \\equiv\\left[b_{i j}\\right]_{n k}, A B \\equiv\\left[c_{i j}\\right]_{m k}$, and, for all $i$ and $j$,\n\n$$\na_{i j}^{\\prime}=a_{j i} \\quad b_{i j}^{\\prime}=b_{j i} \\quad c_{i j}^{\\prime}=c_{j i}\n$$\n\nHence, $A^{T}=\\left[a_{i j}^{\\prime}\\right]_{n m}, B^{T}=\\left[b_{i j}^{\\prime}\\right]_{k n}$, and $(A B)^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. We must show that $B^{T} A^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. By definition of matrix product, $B^{T} A^{T}=\\left[b_{i r}^{\\prime}{ }^{\\prime} a_{r j}^{\\prime}\\right]_{k m}$, and since\n\n$$\nb_{i r}^{\\prime} a_{r j}^{\\prime}=b_{r i} a_{j r}=a_{j r} b_{r i}=c_{j i}=c_{i j}^{\\prime}\n$$\n\nthe desired result follows.\n\n2.4 Show that any matrix of the form $A=B^{T} B$ is symmetric.\n\nBy Problem 2.3 and the involutory nature of the transpose operation,\n\n$$\nA^{T}=\\left(B^{T} B\\right)^{T}=B^{T}\\left(B^{T}\\right)^{T}=B^{T} B=A\n$$\n\n2.5 From the definition, (2.3), of a determinant of order 3, derive the Laplace expansion by cofactors of the first row.\n\nIn the case $n=3,(2.3)$ becomes\n\n$$\n\\left|\\begin{array}{lll}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right| \\equiv\\left|a_{i j}\\right|=e_{i j k} a_{1 i} a_{2 j} a_{3 k}\n$$\n\nSince $e_{i j k}=0$ if any two subscripts coincide, we write only terms for which (ijk) is a permutation of (123):\n\n$$\n\\begin{aligned}\n\\left|a_{i j}\\right| & =e_{123} a_{11} a_{22} a_{33}+e_{132} a_{11} a_{23} a_{32}+e_{213} a_{12} a_{21} a_{33} \\\\\n& \\quad+e_{231} a_{12} a_{23} a_{31}+e_{312} a_{13} a_{21} a_{32}+e_{321} a_{13} a_{22} a_{31} \\\\\n& =a_{11} a_{22} a_{33}-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31} \\\\\n& =a_{11}\\left(a_{22} a_{33}-a_{23} a_{32}\\right)-a_{12}\\left(a_{21} a_{33}-a_{23} a_{31}\\right)+a_{13}\\left(a_{21} a_{32}-a_{22} a_{31}\\right)\n\\end{aligned}\n$$\n\nBut, for $n=2,(2.3)$ gives\n\n$$\n\\left|\\begin{array}{ll}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{array}\\right| \\equiv+A_{11}=e_{12} a_{22} a_{33}+e_{2}: a_{23} a_{32}=a_{22} a_{33}-a_{23} a_{32}\n$$\n\nand the analogous expansions of $-A_{12}$ and $+A_{13}$. Hence,\n\n$$\n\\left|a_{i j}\\right|=a_{11} A_{11}+a_{12} A_{12}+a_{13} A_{13}=a_{1 j} A_{1 j}\n$$\n\nas in $(2.6)$.\n\n\\subsection*{2.6 Evaluate:}\n$$\n\\begin{aligned}\n& \\text { (a) }\\left|\\begin{array}{cc}\nb & -2 a \\\\\n-2 c & b\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| \\\\\n& \\left|\\begin{array}{cc}\nb \\cdot & -2 a \\\\\n-2 c & b\n\\end{array}\\right|=b \\cdot b-(-2 a)(-2 c)=b^{2}-4 a c\n\\end{aligned}\n$$\n\n(b) Because of the zeros in the second column, it is simplest to expand by that column:\n\n$$\n\\begin{aligned}\n\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| & =-(-2)\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|+0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n15 & 30\n\\end{array}\\right|-0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n-10 & 10\n\\end{array}\\right| \\\\\n& =2\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|=2(10)(15)\\left|\\begin{array}{rr}\n-1 & 1 \\\\\n1 & 2\n\\end{array}\\right|=300(-2-1)=-900\n\\end{aligned}\n$$\n\n2.7 Calculate the angle between the following two vectors in $\\mathbf{R}^{5}$ :\n\n$$\n\\mathbf{x}=(1,0,-2,-1,0) \\quad \\text { and } \\quad \\mathbf{y}=(0,0,2,2,0)\n$$\n\nWe have:\n\n$$\n\\begin{aligned}\n& \\mathbf{x y}=(1)(0)+(0)(0)+(-2)(2)+(-1)(2)+(0)(0)=-6 \\\\\n& \\mathbf{x}^{2}=1^{2}+0^{2}+(-2)^{2}+(-1)^{2}+0^{2}=6 \\\\\n& \\mathbf{y}^{2}=0^{2}+0^{2}+2^{2}+2^{2}+0^{2}=8\n\\end{aligned}\n$$\n\nand (2.9) gives\n\n$$\n\\cos \\theta=\\frac{-6}{\\sqrt{6} \\cdot \\sqrt{8}}=-\\frac{\\sqrt{3}}{2} \\quad \\text { or } \\quad \\theta=\\frac{5 \\pi}{6}\n$$\n\n2.8 Find three linearly independent vectors in $\\mathbf{R}^{4}$ which are orthogonal to the vector $(3,4,1,-2)$.\n\nIt is useful to choose vectors having as many zero components as possible. The components $(0,1,0,2)$ clearly work, and $(1,0,-3,0)$ also. Finally, $(0,0,2,1)$ is orthogonal to the given vector, and seems not to be dependent on the first two chosen. To check independence, suppose scalars $x, y$, and $z$ exist such that\n\n$$\nx\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n0 \\\\\n2\n\\end{array}\\right]+y\\left[\\begin{array}{r}\n1 \\\\\n0 \\\\\n-3 \\\\\n0\n\\end{array}\\right]+z\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2 \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right] \\quad \\text { or } \\quad \\begin{aligned}\n& x(0)+y(1)+z(0)=0 \\\\\n& x(1)+y(0)+z(0)=0 \\\\\n& x(0)+y(-3)+z(2)=0 \\\\\n& x(2)+y(0)+z(1)=0\n\\end{aligned}\n$$\n\nThis system has the sole solution $x=y=z=0$, and the vectors are independent.\n\n2.9 Prove that the vector product in $R^{3}$ is anticommutative: $\\mathbf{x} \\times \\mathbf{y}=-\\mathbf{y} \\times \\mathbf{x}$.\n\nBy $(2.10 b)$,\n\n$$\n\\mathbf{x} \\times \\mathbf{y}=\\left(e_{i j k} x_{j} y_{k}\\right) \\quad \\text { and } \\quad \\mathbf{y} \\times \\mathbf{x}=\\left(e_{i j k} y_{j} x_{k}\\right)\n$$\n\nBut $e_{i k j}=-e_{i j k}$, so that\n\n$$\ne_{i j k} y_{j} x_{k}=e_{i k j} y_{k} x_{j}=-e_{i j k} y_{k} x_{j}=-e_{i j k} x_{j} y_{k}\n$$\n\n\\section*{INVERTING A MATRIX}\n2.10 Establish the generalized Laplace expansion theorem: $a_{r j} A_{s j}=|A| \\delta_{r s}$.\n\nConsider the matrix\n\n$$\nA^{*}=\\left[\\begin{array}{cccc}\na_{11} & a_{12} & \\ldots & a_{1 n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\cdots & \\ldots & \\cdots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{n 1} & a_{n 2} & \\ldots & a_{n n}\n\\end{array}\\right] \\text { row } \\boldsymbol{r}\n$$\n\nwhich is obtained from matrix $A$ by replacing its $s$ th row by its $r$ th row $(r \\neq s)$. By (2.6), applied to row $r$ of $A^{*}$,\n\n$$\n\\operatorname{det} A^{*}=a_{r j} A_{r j}^{*} \\quad(\\text { not summed on } r)\n$$\n\nNow, because rows $r$ and $s$ are identical, we have for all $j$,\n\n$$\nA_{r j}^{*}=(-1)^{p} A_{s j}^{*}=(-1)^{p} A_{s j} \\quad(p \\equiv r-s)\n$$\n\nTherefore, $\\operatorname{det} A^{*}=(-1)^{p} a_{r j} A_{s j}$. But it is easy to see (Problem 2.31) that, with two rows the same, $\\operatorname{det} A^{*}=0$. We have thus proved that\n\n$$\na_{r j} A_{s j}=0 \\quad(r \\neq s)\n$$\n\nand this, together with (2.6) for the case $r=s$, yields the theorem.\n\n2.11 Given a matrix $A \\equiv\\left[a_{i j}\\right]_{n n}$, with $|A| \\neq 0$, use Problem 2.10 to show that\n\n$$\nA B=I \\quad \\text { where } \\quad B=\\frac{1}{|A|}\\left[A_{i j}\\right]_{n n}^{T}\n$$\n\nSince the $(i, j)$-element of $B$ is $A_{j i} /|A|$,\n\n$$\nA B=\\left[a_{i k}\\left(A_{j k} /|A|\\right)\\right]_{n n}=\\frac{1}{|A|}\\left[|A| \\delta_{i j}\\right]_{n n}=\\frac{|A|}{|A|}\\left[\\delta_{i j}\\right]_{n n}=I\n$$\n\n[It follows from basic facts of linear algebra that also $B A=I$; therefore, $A^{-1}=B$, which establishes (2.11a).]\n\n2.12 Invert the matrix\n\n$$\nA=\\left[\\begin{array}{rrr}\n-2 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n2 & -2 & 3\n\\end{array}\\right]\n$$\n\nrow:\n\nUse (2.11c). To evaluate $|A|$, add twice the third column to the first and then expand by the first\n\n$$\n|A|=\\left|\\begin{array}{rrr}\n0 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n8 & -2 & 3\n\\end{array}\\right|=1 \\cdot\\left|\\begin{array}{rr}\n3 & 1 \\\\\n8 & -2\n\\end{array}\\right|=-6-8=-14\n$$\n\nThen, computing cofactors as we go,\n\n$$\nA^{-1}=\\frac{1}{-14}\\left[\\begin{array}{rrr}\n3 & -2 & -1 \\\\\n-9 & -8 & 3 \\\\\n-8 & -4 & -2\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n-3 / 14 & 1 / 7 & 1 / 14 \\\\\n9 / 14 & 4 / 7 & -3 / 14 \\\\\n4 / 7 & 2 / 7 & 1 / 7\n\\end{array}\\right]\n$$\n\n2.13 Let $A$ and $B$ be invertible matrices of the same order. Prove that $(a)\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$ (i.e., the operations of transposition and inversion commute); (b) $(A B)^{-1}=B^{-1} A^{-1}$.\n\n(a) Transpose the equations $A A^{-1}=A^{-1} A=I$, recalling Problem 2.3, to obtain\n\n$$\n\\left(A^{-1}\\right)^{T} A^{T}=A^{T}\\left(A^{-1}\\right)^{T}=I^{T}=I\n$$\n\nwhich show that $A^{T}$ is invertible, with inverse $\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$.\n\n(b) By the associative law for matrix multiplication,\n\n$$\n(A B)\\left(B^{-1} A^{-1}\\right)=A\\left(B B^{-1}\\right) A^{-1}=A I A^{-1}=A A^{-1}=I\n$$\n\nand, similarly,\n\n$$\n\\left(B^{-1} A^{-1}\\right)(A B)=I\n$$\n\nHence, $(A B)^{-1}=B^{-1} A^{-1}$.\n\n\\section*{LINEAR SYSTEMS; QUADRATIC FORMS}\n2.14 Write the following system of equations in matrix form, then solve by using the inverse matrix:\n\n$$\n\\begin{aligned}\n3 x-4 y & =-18 \\\\\n-5 x+8 y & =34\n\\end{aligned}\n$$\n\nThe matrix form of the system is\n\n\\[\n\\left[\\begin{array}{rr}\n3 & -4  \\tag{1}\\\\\n-5 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]\n\\]\n\nThe inverse of the $2 \\times 2$ coefficient matrix is:\n\n$$\n\\left[\\begin{array}{rr}\n3 & -4 \\\\\n-5 & 8\n\\end{array}\\right]^{-1}=\\frac{1}{24-(+20)}\\left[\\begin{array}{ll}\n8 & 4 \\\\\n5 & 3\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\n$$\n\nPremultiplying (1) by this matrix gives\n\n$$\nI\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-2 \\\\\n3\n\\end{array}\\right]\n$$\n\nor $x=-2, y=3$.\n\n2.15 If $\\left[b^{i j}\\right]=\\left[a_{i j}\\right]^{-1}$, solve the $n \\times n$ system\n\n\n\\begin{equation*}\ny_{i}=a_{i j} x_{j} \\tag{1}\n\\end{equation*}\n\n\nfor the $x_{j}$ in terms of the $y_{i}$.\n\nMultiply both sides of (1) by $b^{k i}$ and sum on $i$ :\n\n$$\nb^{k i} y_{i}=b^{k i} a_{i j} x_{j}=\\delta_{j}^{k} x_{j}=x_{k}\n$$\n\nTherefore, $x_{j}=b^{j i} y_{i}$.\n\n2.16 Write the quadratic form in $\\mathbf{R}^{4}$\n\n$$\nQ=7 x_{1}^{2}-4 x_{1} x_{3}+3 x_{1} x_{4}-x_{2}^{2}+10 x_{2} x_{4}+x_{3}^{2}-6 x_{3} x_{4}+3 x_{4}^{2}\n$$\n\nin the matrix form $\\mathbf{x}^{T} A \\mathbf{x}$ with $A$ symmetric.\n\n$$\nQ=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -4 & 3 \\\\\n0 & -1 & 0 & 10 \\\\\n0 & 0 & 1 & -6 \\\\\n0 & 0 & 0 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -2 & 3 / 2 \\\\\n0 & -1 & 0 & 5 \\\\\n-2 & 0 & 1 & -3 \\\\\n3 / 2 & 5 & -3 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]\n$$\n\n\\section*{LINEAR TRANSFORMATIONS}\n2.17 Show that under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$, the quadric hypersurface $c_{i j} x_{i} x_{j}=1$ transforms to $\\bar{c}_{i j} \\bar{x}_{i} \\bar{x}_{j}=1$, where\n\n$$\n\\bar{c}_{i j}=c_{r s} b_{r i} b_{s j} \\quad \\text { with } \\quad\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}\n$$\n\nThis will be worked using matrices, from which the component form can be easily deduced. The hypersurface has the equation $\\mathbf{x}^{T} C \\mathbf{x}=1$ in unbarred coordinates, and $\\overline{\\mathbf{x}}=A \\mathbf{x}$ defines a barred coordinate system. Substituting $\\mathbf{x}=B \\overline{\\mathbf{x}} \\quad\\left(B=A^{-1}\\right)$ into the equation of the quadric, we have\n\n$$\n(B \\overline{\\mathbf{x}})^{T} C(B \\overline{\\mathbf{x}})=1 \\quad \\text { or } \\quad \\overline{\\mathbf{x}}^{T} B^{T} C B \\overline{\\mathbf{x}}=1\n$$\n\nThus, in the barred coordinate system, the equation of the quadric is $\\overline{\\mathbf{x}}^{T} \\bar{C} \\overline{\\mathbf{x}}=1$, where $\\bar{C}=B^{T} C B$.\n\n\\section*{DISTANCE IN A BARRED COORDINATE SYSTEM}\n2.18 Calculate the coefficients $g_{i j}$ in the distance formula (2.14) for the barred coordinate system in $\\mathbf{R}^{2}$ defined by $\\bar{x}_{i}=a_{i j} x_{j}$, where $a_{11}=a_{22}=1, a_{12}=0$, and $a_{21}=2$.\n\nWe have merely to calculate $G=\\left(A A^{T}\\right)^{-1}$, where $A=\\left(a_{i j}\\right)$ :\n\n$$\nA A^{T}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & 2 \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n1 \\cdot 1+0 & 1 \\cdot 2+0 \\\\\n2 \\cdot 1+0 & 2 \\cdot 2+1 \\cdot 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]\n$$\n\nBy $(2.11 b)$,\n\n$$\n\\left(A A^{T}\\right)^{-1}=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]^{-1}=\\frac{1}{5-4}\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]\n$$\n\nThus, $g_{11}=5, g_{12}=g_{21}=-2, g_{22}=1$.\n\n2.19 Test the distance formula obtained in Problem 2.18 by finding the distance between the aliases of $\\left(x_{i}\\right)=(1,-3)$ and $\\left(y_{i}\\right)=(0,-2)$, which points are a distance $\\sqrt{2}$ apart.\n\nThe coordinates for the given points in the barred system are found to be\n\n$$\n\\overline{\\mathbf{x}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n1 \\\\\n-3\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n-1\n\\end{array}\\right] \\quad \\overline{\\mathbf{y}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]=\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]\n$$\n\nor $\\left(\\bar{x}_{i}\\right)=(1,-1)$ and $\\left(\\bar{y}_{i}\\right)=(0,-2)$. Using the $g_{i j}$ calculated in Problem 2.18,\n\n$$\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\sqrt{5(1-0)^{2}-2 \\cdot 2(1-0)(-1+2)+1(-1+2)^{2}}=\\sqrt{2}\n$$\n\n2.20 Prove formula (2.14).\n\nIn unbarred coordinates, the distance formula has the matrix form\n\n$$\nd(\\mathbf{x}, \\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|=\\sqrt{(\\mathbf{x}-\\mathbf{y})^{T}(\\mathbf{x}-\\mathbf{y})}\n$$\n\nNow, $\\overline{\\mathbf{x}}=A \\mathbf{x}$ or $\\mathbf{x}=B \\overline{\\mathbf{x}}$, where $B=A^{-1}$; so we have by substitution,\n\n$$\n\\begin{aligned}\nd(\\mathbf{x}, \\mathbf{y}) & =\\sqrt{(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})^{T}(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})}=\\sqrt{(B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}))^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} B^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})}=\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} G(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})\n\\end{aligned}\n$$\n\nwhere $G \\equiv B^{T} B=\\left(A^{-1}\\right)^{T} A^{-1}=\\left(A^{T}\\right)^{-1} A^{-1}=\\left(A A^{T}\\right)^{-1}$, the last two equalities following from Problem 2.13 .\n\n\\section*{RECTANGULAR COORDINATES}\n2.21 Suppose that $\\left(x^{i}\\right)=(x, y, z)$ and $\\left(\\bar{x}^{i}\\right)=(\\bar{x}, \\bar{y}, \\bar{z})$ (the use of superscripts here anticipates future notation) denote two rectangular coordinate systems at $O$ and that the direction angles of the $\\bar{x}^{i}$-axis relative to the $x$-, $y$-, and $z$-axes are $\\left(\\alpha_{i}, \\beta_{i}, \\gamma_{i}\\right), i=1,2,3$. Show that the correspondence between the coordinate systems is given by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, where $\\mathbf{x}=(x, y, z)$, $\\overline{\\mathbf{x}}=(\\bar{x}, \\bar{y}, \\bar{z})$, and where the matrix\n\n$$\nA=\\left[\\begin{array}{lll}\n\\cos \\alpha_{1} & \\cos \\beta_{1} & \\cos \\gamma_{1} \\\\\n\\cos \\alpha_{2} & \\cos \\beta_{2} & \\cos \\gamma_{2} \\\\\n\\cos \\alpha_{3} & \\cos \\beta_{3} & \\cos \\gamma_{3}\n\\end{array}\\right]\n$$\n\nis orthogonal.\n\nLet the unit vectors along the $\\bar{x}$-, $\\bar{y}$, and $\\bar{z}$-axes be $\\overline{\\mathbf{i}}=\\overrightarrow{O P}, \\overline{\\mathbf{j}}=\\overrightarrow{O Q}$, and $\\overline{\\mathbf{k}}=\\overrightarrow{O R}$, respectively (see Fig. 2-1). If $\\overline{\\mathbf{x}}$ is the position vector of any point $W(x, y, z)$, then\n\n$$\n\\overline{\\mathbf{x}}=\\bar{x} \\overline{\\mathbf{i}}+\\bar{y} \\overline{\\mathbf{j}}+\\bar{z} \\overline{\\mathbf{k}}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-028}\n\\end{center}\n\nFig. 2-1\n\nWe know that the $(x, y, z)$-coordinates of $P$ are $\\left(\\cos \\alpha_{1}, \\cos \\beta_{1}, \\cos \\gamma_{1}\\right)$. Similar statements hold for the coordinates of $Q$ and $R$, respectively. Hence:\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{i}} & =\\left(\\cos \\alpha_{1}\\right) \\mathbf{i}+\\left(\\cos \\beta_{1}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{1}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{j}} & =\\left(\\cos \\alpha_{2}\\right) \\mathbf{i}+\\left(\\cos \\beta_{2}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{2}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{k}} & =\\left(\\cos \\alpha_{3}\\right) \\mathbf{i}+\\left(\\cos \\beta_{3}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nSubstituting these into the expression for $\\overline{\\mathbf{x}}$ and collecting coefficients of $\\mathbf{i}, \\mathbf{j}$, and $\\mathbf{k}$ :\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{x}}=\\left(\\bar{x} \\cos \\alpha_{1}\\right. & \\left.+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\\right) \\mathbf{i} \\\\\n& +\\left(\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3}\\right) \\mathbf{j} \\\\\n& +\\left(\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nHence, the $x$-coordinate of $W$ is the coefficient of $\\mathbf{i}$, or\n\nSimilarly,\n\n$$\nx=\\bar{x} \\cos \\alpha_{1}+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\n$$\n\n$$\n\\begin{aligned}\n& y=\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3} \\\\\n& z=\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\n\\end{aligned}\n$$\n\nIn terms of the matrix $A$ defined above, we can write these three equations in the matrix form\n\n\n\\begin{equation*}\n\\mathbf{x}=A^{T} \\overline{\\mathbf{x}} \\tag{1}\n\\end{equation*}\n\n\nNow, the $(i, j)$-element of the matrix $A A^{T}$ is\n\n$$\n\\cos \\alpha_{i} \\cos \\alpha_{j}+\\cos \\beta_{i} \\cos \\beta_{j}+\\cos \\gamma_{i} \\cos \\gamma_{j}\n$$\n\nfor $i, j=1,2,3$. Note that the diagonal elements,\n\n$$\n\\left(\\cos \\alpha_{i}\\right)^{2}+\\left(\\cos \\beta_{i}\\right)^{2}+\\left(\\cos \\gamma_{i}\\right)^{2} \\quad(i=1,2,3)\n$$\n\nare the three quantities $\\overrightarrow{O P} \\cdot \\overrightarrow{O P}, \\overrightarrow{O Q} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O R} \\cdot \\overrightarrow{O R}$; i.e., they are unity. If $i \\neq j$, then the corresponding element of $A A^{T}$ is either $\\overrightarrow{O P} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O P} \\cdot \\overrightarrow{O R}$, or $\\overrightarrow{O Q} \\cdot \\overrightarrow{O R}$, and is therefore zero (since these vectors are mutually orthogonal). Hence, $A A^{T}=I$ (and also $A^{T} A=I$ ), and, from (1),\n\n$$\nA \\mathbf{x}=A A^{T} \\overrightarrow{\\mathbf{x}}=\\overline{\\mathbf{x}}\n$$\n\n\\section*{CURVILINEAR COORDINATES}\n2.22 A curvilinear coordinate system $(\\bar{x}, \\bar{y})$ is defined in terms of rectangular coordinates $(x, y)$ by\n\n\n\\begin{align*}\n& \\bar{x}=x^{2}-x y  \\tag{1}\\\\\n& \\bar{y}=x y\n\\end{align*}\n\n\nShow that in the barred coordinate system the equation of the line $y=x-1$ is $\\bar{y}=\\bar{x}^{2}-\\bar{x}$. [In the alibi interpretation, (1) deforms the straight line into a parabola.]\n\nIt helps initially to parameterize the equation of the line as $x=t, y=t-1$. Substitution of $x=t$, $y=t-1$ in the change-of-coordinates formula gives the parametric equations of the line in the barred coordinate system:\n\n\n\\begin{align*}\n& \\bar{x}=t^{2}-t(t-1)=t \\\\\n& \\bar{y}=t(t-1)=t^{2}-t \\tag{2}\n\\end{align*}\n\n\nNow $t$ may be eliminated from (2) to give $\\bar{y}=\\bar{x}^{2}-\\bar{x}$.\n\n\\section*{CHAIN RULE}\n2.23 Suppose that under a change of coordinates, $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)(1 \\leqq i \\leqq n)$, the realvalued vector functions $\\left(\\bar{T}_{i}\\right)$ and $\\left(T_{i}\\right)$ are related by the formula\n\n\n\\begin{equation*}\n\\bar{T}_{i}=T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}} \\tag{1}\n\\end{equation*}\n\n\nFind the transformation rule for the partial derivatives of $\\left(T_{i}\\right)$-that is, express the $\\partial \\bar{T}_{i} / \\partial \\bar{x}_{j}$ in terms of the $\\partial T_{r} / \\partial x_{s}$-given that all second-order partial derivatives are zero.\n\nBegin by taking the partial derivative with respect to $\\bar{x}_{j}$ of both sides of (1), using the product rule:\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}=\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}+T_{r} \\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{\\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}\n$$\n\nBy assumption, the second term on the right is zero; and, by the chain rule,\n\n$$\n\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}}\n$$\n\nConsequently, the desired transformation rule is\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n2.24 Display the matrices $(a)\\left[u^{i j}\\right]_{35},(b)\\left[u^{j i}\\right]_{35},(c)\\left[u^{i j}\\right]_{53},(d)\\left[\\delta_{j}^{i}\\right]_{36}$.\n\n2.25 Carry out the following matrix multiplications:\n\n$$\n\\text { (a) }\\left[\\begin{array}{rrr}\n3 & -1 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 2 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n2\n\\end{array}\\right] \\quad \\text { (b) }\\left[\\begin{array}{rr}\n3 & -1 \\\\\n2 & 0\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n1 & 1 & -1 \\\\\n2 & 1 & 1\n\\end{array}\\right]\n$$\n\n2.26 Prove by the product rule and by use of the summation convention the associative law for matrices:\n\n$$\n(A B) C=A(B C)\n$$\n\nwhere $A \\equiv\\left(a_{i j}\\right), B \\equiv\\left(b_{i j}\\right)$, and $C \\equiv\\left(c_{i j}\\right)$ are arbitrary matrices, but compatible for multiplication.\n\n2.27 Prove: (a) if $A$ and $B$ are symmetric matrices and if $A B=B A=C$, then $C$ is symmetric; (b) if $A$ and $B$ are skew-symmetric and if $A B=-B A=C$, then $C$ is skew-symmetric.\n\n2.28 Prove that the product of two orthogonal matrices is orthogonal.\n\n2.29 Evaluate the determinants\n\n$$\n\\text { (a) }\\left|\\begin{array}{rr}\n3 & -2 \\\\\n1 & 5\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n2 & 1 & -1 \\\\\n3 & 0 & 1 \\\\\n1 & -1 & 2\n\\end{array}\\right| \\quad \\text { (c) }\\left|\\begin{array}{rrrrr}\n-1 & 1 & -1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n-1 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 0 & 0 & 0\n\\end{array}\\right|\n$$\n\n2.30 In the Laplace expansion of the fourth-order determinant $\\left|a_{i j}\\right|$, the six-term summation $e_{2 i j k} a_{12} a_{2 i} a_{3 j} a_{4 k}$ appears. (a) Write out this sum explicitly, then (b) represent it as a third-order determinant.\n\n2.31 Prove that if a matrix has two rows the same, its determinant is zero. (Hint: First show that interchanging any two subscripts reverses the sign of the permutation symbol.)\n\n2.32 Calculate the inverse of\n\n$$\n\\text { (a) }\\left[\\begin{array}{ll}\n3 & 1 \\\\\n5 & 2\n\\end{array}\\right] \\quad(b)\\left[\\begin{array}{rrr}\n0 & 1 & 2 \\\\\n1 & -1 & 0 \\\\\n2 & 1 & -1\n\\end{array}\\right]\n$$\n\n2.33 (a) Verify the following formulas for the permutation symbols $e_{i j}$ and $e_{i j k}$ (for distinct values of the indices only):\n\n$$\ne_{i j}=\\frac{j-i}{|j-i|} \\quad e_{i j k}=\\frac{(j-i)(k-i)(k-j)}{|j-i||k-i||k-j|}\n$$\n\n(b) Prove the general formula:\n\n$$\ne_{i_{1} i_{2} \\cdots i_{n}}=\\frac{\\left(i_{2}-i_{1}\\right)\\left(i_{3}-i_{1}\\right) \\cdots\\left(i_{n}-i_{1}\\right)\\left(i_{3}-i_{2}\\right) \\cdots\\left(i_{n}-i_{2}\\right) \\cdots\\left(i_{n}-i_{n-1}\\right)}{\\left|i_{2}-i_{1}\\right|\\left|i_{3}-i_{1}\\right| \\cdots\\left|i_{n}-i_{1}\\right|\\left|i_{3}-i_{2}\\right| \\cdots\\left|i_{n}-i_{2}\\right| \\cdots\\left|i_{n}-i_{n-1}\\right|} \\equiv \\prod_{p>q} \\frac{i_{p}-i_{q}}{\\left|i_{p}-i_{q}\\right|}\n$$\n\n2.34 Calculate the angle between the $\\mathbf{R}^{6}$-vectors $\\mathbf{x}=(3,-1,0,1,2,-3)$ and $\\mathbf{y}=(-2,1,0,1,0,0)$.\n\n2.35 Find two linearly independent vectors in $\\mathbf{R}^{3}$ which are orthogonal to the vector $(3,-2,1)$.\n\n2.36 Solve for $x$ and $y$ by use of matrices:\n\n$$\n\\begin{aligned}\n& 3 x-4 y=-23 \\\\\n& 5 x+3 y=10\n\\end{aligned}\n$$\n\n2.37 Write out the quadratic form in $\\mathbf{R}^{3}$ represented by $Q=\\mathbf{x}^{T} A \\mathbf{x}$, where\n\n$$\nA=\\left[\\begin{array}{rrr}\n1 & 4 & 3 \\\\\n4 & 2 & 0 \\\\\n3 & 0 & -1\n\\end{array}\\right]\n$$\n\n2.38 Represent with a symmetric matrix $A$ the quadratic form in $\\mathbf{R}^{4}$\n\n$$\nQ=-3 x_{1}^{2}-x_{2}^{2}+x_{3}^{2}-x_{1} x_{2}-x_{1} x_{3}+6 x_{1} x_{4}\n$$\n\n2.39 Given the hyperplane $c_{r} x_{r}=1$, how do the coefficients $c_{i}$ transform under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$ ?\n\n2.40 Calculate the $g_{i j}$ for the distance formula (2.14) in a barred coordinate system defined by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, with\n\n$$\nA=\\left[\\begin{array}{rr}\n1 & -2 \\\\\n2 & 3\n\\end{array}\\right]\n$$\n\n2.41 Test the distance formula of Problem 2.40 on the pair of points whose unbarred coordinates are $(2,-1)$ and $(2,-4)$.\n\n2.42 (a) Show that for independent functions $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$,\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=\\delta_{j}^{i} \\tag{1}\n\\end{equation*}\n\n\n(b) Take the partial derivative with respect to $x_{k}$ of (1) to establish the formula\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\bar{x}_{i}}{\\partial x_{k} \\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=-\\frac{\\partial^{2} x_{r}}{\\partial \\bar{x}_{s} \\partial \\bar{x}_{j}} \\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial \\bar{x}_{s}}{\\partial x_{k}} \\tag{2}\n\\end{equation*}\n\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 3}", "\\section*{General Tensors}\n\\subsection*{3.1 COORDINATE TRANSFORMATIONS}\nAt this point the notation for coordinates will be changed to that usual in tensor calculus.\n\n\\section*{Superscripts for Vector Components}\nThe coordinates of a point (vector) in $\\mathbf{R}^{n}$ will henceforth be denoted as $\\left(x^{1}, x^{2}, x^{3}, \\ldots, x^{n}\\right)$. Thus, the familiar subscripts are now replaced by superscripts, and the upper position is no longer reserved for exponents alone. It will be clear by context whether a character represents a vector component or the power of a scalar.\n\nEXAMPLE 3.1 If a power of some vector component is to be indicated, obviously parentheses are necessary; thus, $\\left(x^{3}\\right)^{2}$ and $\\left(x^{n-1}\\right)^{5}$ represent, respectively, the square of the third component, and the $(n-1)$ st component raised to the fifth power, of the vector $\\mathbf{x}$. If $u$ is introduced as a real number, then $u^{2}$ and $u^{3}$ are powers of $u$ and not vector components. If $(c)^{k}$ appears without explanation, the parentheses indicate the use of the superscript $k$ as an exponent and not as the index of a vector component.\n\n\\section*{Rectangular Coordinates}\nCoordinates in $\\mathbf{R}^{n}$ are called rectangular (also rectangular cartesian or cartesian) if they are patterned after the usual orthogonal coordinate systems of two- and three-dimensional analytic geometry. A general definition that is workable in this setting is, in effect, an assertion of the converse of the Pythagorean theorem.\n\nDefinition 1: A coordinate system $\\left(x^{i}\\right)$ is rectangular if the distance between two arbitrary points $P\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ and $Q\\left(y^{1}, y^{2}, \\ldots, y^{n}\\right)$ is given by\n\n$$\nP Q=\\sqrt{\\left(x^{1}-y^{1}\\right)^{2}+\\left(x^{2}-y^{2}\\right)^{2}+\\cdots+\\left(x^{n}-y^{n}\\right)^{2}} \\equiv \\sqrt{\\delta_{i j} \\Delta x^{i} \\Delta x^{j}}\n$$\n\nwhere $\\Delta x^{i} \\equiv x^{i}-y^{i}$.\n\nUnder orthogonal coordinate changes, which are isometric, the above formula for distance is invariant (cf. Section 2.5). Hence, all coordinate systems $\\left(\\bar{x}^{i}\\right)$ defined by $\\bar{x}^{i}=a_{r}^{i} x^{r}$, where $\\left(a_{j}^{i}\\right)$ is such that $a_{i}^{r} a_{j}^{r}=\\delta_{i j}$, are rectangular. It can be shown that these are the only rectangular coordinate systems whose origin coincides with that of the $\\left(x^{i}\\right)$-system.\n\n\\section*{Curvilinear Coordinates}\nSuppose that in some region of $\\mathbf{R}^{n}$ two coordinate systems are defined, and that these two systems are connected by equations of the form\n\n\n\\begin{equation*}\n\\mathscr{T}: \\bar{x}^{i}=\\bar{x}^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\quad(1 \\leqq i \\leqq n) \\tag{3.1}\n\\end{equation*}\n\n\nwhere, for each $i$, the function, or scalar field, $x^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ maps the given region in $\\mathbf{R}^{n}$ to the reals and has continuous second-partial derivatives at every point in the region (is class $C^{2}$ ). The transformation $\\mathscr{T}$, if bijective, is called a coordinate transformation, as in Section 2.6. If $\\left(x^{i}\\right)$ are ordinary rectangular coordinates, the $\\left(\\bar{x}^{i}\\right)$ are called curvilinear coordinates unless $\\mathscr{T}$ is linear, in which case $\\left(\\bar{x}^{i}\\right)$ are called affine coordinates.\n\nFor convenience, the three most common curvilinear-coordinate systems are presented below. In each case, a \"reverse\" notation is employed: the two- or three-dimensional curvilinear system $\\left(x^{i}\\right)$ is defined by the mapping $\\mathscr{T}$ that takes it into a rectangular system $\\left(\\bar{x}^{i}\\right)$ of the same dimension.\n\nPolar coordinates (Fig. 3-1). Let $\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)=(x, y)$ and $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$, under the restriction $r>0$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033}\n\\end{center}\n\nFig. 3-1\n\nThen,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\cos x^{2}  \\tag{3.2}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\\\\nx^{2}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{-1}\\right)\n\\end{array}\\right.\\right.\n\\]\n\n(The inverse given here is, in the equation for $x^{2}$, valid only in the first and fourth quadrants of the $\\bar{x}_{1} \\bar{x}_{2}$-plane; other solutions must be used over the other two quadrants. Likewise for the $\\theta$-coordinate in the cylindrical and spherical systems.)\n\nCylindrical coordinates (Fig. 3-2). If $\\left(\\bar{x}^{1}, \\bar{x}^{2}, \\bar{x}^{3}\\right)=(x, y, z)$ and $\\left(x^{1}, x^{2}, x^{3}\\right)=(r, \\theta, z)$, where $r>0$,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\cos x^{2}  \\tag{3.3}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\\\\nx^{2}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{-1}\\right) \\\\\nx^{3}=\\bar{x}^{3}\n\\end{array}\\right.\\right.\n\\]\n\nSpherical coordinates (Fig. 3-3). If $\\left(\\bar{x}_{1}, \\bar{x}_{2}, \\bar{x}_{3}\\right)=(x, y, z)$ and $\\left(x^{1}, x^{2}, x^{3}\\right)=(\\rho, \\varphi, \\theta)$, where $\\rho>0$ and $0 \\leqq \\varphi \\leqq \\pi$,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\sin x^{2} \\cos x^{3}  \\tag{3.4}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2} \\sin x^{3} \\\\\n\\bar{x}^{3}=x^{1} \\cos x^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}+\\left(\\bar{x}^{3}\\right)^{2}} \\\\\nx^{2}=\\cos ^{-1}\\left(\\bar{x}^{3} / \\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}+\\left(\\bar{x}^{3}\\right)^{2}}\\right) \\\\\nx^{3}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{1}\\right)\n\\end{array}\\right.\\right.\n\\]\n\n(Caution: In an older but still common notation for spherical coordinates, $\\theta$ denotes the polar angle and $\\varphi$ the equatorial angle.)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033(1)}\n\\end{center}\n\nFig. 3-2\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-034}\n\\end{center}\n\nFig. 3-3\n\n\\section*{The Jacobian}\n matrix,The $n^{2}$ first-order partial derivatives $\\partial \\bar{x}^{i} / \\partial x^{j}$ arising from (3.1) are normally arranged in an $n \\times n$\n\n\\[\nJ=\\left[\\begin{array}{cccc}\n\\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{n}}  \\tag{3.5}\\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{n}} \\\\\n\\cdots \\cdots & \\cdots & \\cdots \\\\\n\\frac{\\partial \\bar{x}^{n}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{n}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{n}}{\\partial x^{n}}\n\\end{array}\\right]\n\\]\n\nMatrix $J$ is the Jacobian matrix, and its determinant $\\mathscr{F} \\equiv \\operatorname{det} J$ is the Jacobian, of the transformation $\\mathscr{T}$.\n\nEXAMPLE 3.2 In $\\mathbf{R}^{2}$ let a curvilinear coordinate system $\\left(\\bar{x}^{i}\\right)$ be defined from rectangular coordinates $\\left(x^{i}\\right)$ by the equations\n\n$$\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} x^{2} \\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array}\\right.\n$$\n\nSince $\\partial \\bar{x}^{1} / \\partial x^{1}=x^{2}, \\partial \\bar{x}^{1} / \\partial x^{2}=x^{1}, \\partial \\bar{x}^{2} / \\partial x^{1}=0$, and $\\partial \\bar{x}^{2} / \\partial x^{2}=2 x^{2}$, the Jacobian of $\\mathscr{T}$ is\n\n$$\n\\mathscr{J}=\\left|\\begin{array}{cc}\nx^{2} & x^{1} \\\\\n0 & 2 x^{2}\n\\end{array}\\right|=2\\left(x^{2}\\right)^{2}\n$$\n\nA well-known theorem from analysis states that $\\mathscr{T}$ is locally bijective on an open set $\\mathscr{U}$ in $\\mathbf{R}^{n}$ if and only if $\\mathscr{F} \\neq 0$ at each point of $U$. When $\\mathscr{F} \\neq 0$ in $U$ and $\\mathscr{T}$ is class $C^{2}$ in $\\mathscr{U}$, then (3.1) is termed an admissible change of coordinates for $\\mathcal{U}$.\n\nEXAMPLE 3.3 The curvilinear coordinates of Example 3.2 are admissible for the regions $x^{2}>0$ and $x^{2}<0$ (both open sets in the plane). See Problem 3.1.\n\nIn an admissible change of coordinates, the inverse transformation $\\mathscr{T}^{-1}$ (the local existence of which is guaranteed by the theorem mentioned above) is also class $C^{2}$, on $\\bar{U}$, the image of $\\mathscr{U}$ under $\\mathscr{T}$. Moreover, if $\\mathscr{T}^{-1}$ has the form\n\n\n\\begin{equation*}\n\\mathscr{T}^{-1}: x^{i}=x^{i}\\left(\\bar{x}^{-1}, \\bar{x}^{2}, \\ldots, \\bar{x}^{n}\\right) \\quad(1 \\leqq i \\leqq n) \\tag{3.6}\n\\end{equation*}\n\n\non $\\bar{U}$, the Jacobian matrix $\\bar{J}$ of $\\mathscr{T}^{-1}$ is the inverse of $J$. Thus, $J \\bar{J}=\\bar{J} J=I$, or\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{j}}=\\delta_{j}^{i} \\tag{3.7}\n\\end{equation*}\n\n\n[cf. Problem 2.42(a)]. It also follows that $\\overline{\\mathscr{F}}=1 / \\overline{\\mathscr{g}}$.\n\n\\section*{General Coordinate Systems}\nIn later developments it will be necessary to adopt coordinate systems that are not tied to rectangular coordinates in any way [via (3:1)] and to define distance in terms of an arc-length formula for arbitrary curves, with points represented abstractly by $n$-tuples $\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$. Each such distance functional or metric will be invariant under admissible changes of coordinates, and admissible coordinate systems will exist for each separate metric. Under such metrics, $\\mathbf{R}^{n}$ will generally become non-Euclidean; e.g., the angle sum of a triangle will not invariably equal $\\pi$.\n\nAlthough the curvilinear coordinate systems presented above are explicitly associated with the Euclidean metric (since they are connected via (3.1) with rectangular coordinates and Euclidean space), those same systems could be formally adopted in a non-Euclidean space if some purpose were served by doing so. The point to be made is that the space metric and the coordinate system used to describe that metric are completely independent of each other, except in the single instance of rectangular coordinates, whose very definition (see Definition 1) involves the Euclidean metric.\n\n\\section*{Usefulness of Coordinate Changes}\nA primary concern in studying tensor analysis is the manner in which a change of coordinates affects the way geometrical objects or physical laws are described. For example, in rectangular coordinates the equation of a circle of radius $a$ centered at the origin is quadratic,\n\n$$\n\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}=a^{2}\n$$\n\nbut in polar coordinates, (3.2), that same circle has the simple linear equation $x^{1}=a$. The reader is no doubt familiar with the sometimes dramatic change that takes place in a differential equation under a change of variables, which is nothing but a change of coordinates. This idea of changing the description of phenomena by changing coordinate systems lies at the heart of not only what a tensor means, but how it is used in practice.\n\n\\subsection*{3.2 FIRST-ORDER TENSORS}\nConsider a vector field $\\mathbf{V}=\\left(V^{i}\\right)$ defined on some subset $\\mathscr{S}$ of $\\mathbf{R}^{n}$ [that is, for each $i$, the component $V^{i}=V^{i}(\\mathbf{x})$ is a scalar field (real-valued function) as $\\mathbf{x}$ varies over $\\mathscr{Y}$ ]. In each admissible coordinate system of a region $U$ containing $\\mathscr{S}$, let the $n$ components $V^{1}, V^{2}, \\ldots, V^{n}$ of $\\mathbf{V}$ be expressible as $n$ real-valued functions; say, as\n\n$$\nT^{1}, \\quad T^{2}, \\ldots, \\quad T^{n} \\quad \\text { in the }\\left(x^{i}\\right) \\text {-system }\n$$\n\nand\n\n$$\n\\bar{T}^{1}, \\quad \\bar{T}^{2}, \\ldots, \\quad \\bar{T}^{n} \\quad \\text { in the }\\left(\\bar{x}^{i}\\right) \\text {-system }\n$$\n\nwhere $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ are related by (3.1) and (3.6).\n\nDefinition 2: The vector field $\\mathbf{V}$ is a contravariant tensor of order one (or contravariant vector) provided its components $\\left(T^{i}\\right)$ and $\\left(\\bar{T}^{i}\\right)$ relative to the respective coordinate systems $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { contravariant vector } \\quad \\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad(1 \\leqq i \\leqq n) \\tag{3.8}\n\\end{equation*}\n\n\nEXAMPLE 3.4 Let $\\mathscr{C}$ be a curve given parametrically in the $\\left(x^{i}\\right)$-system by\n\n$$\nx^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)\n$$\n\nThe tangent vector field $\\mathbf{T}=\\left(T^{i}\\right)$ is defined by the usual differentiation formula\n\n$$\nT^{i}=\\frac{d x^{i}}{d t}\n$$\n\nUnder a change of coordinates (3.1), the same curve is given in the $\\left(\\bar{x}^{i}\\right)$-system by\n\n$$\n\\bar{x}^{i}=\\bar{x}^{i}(t) \\equiv \\bar{x}^{i}\\left(x^{1}(t), x^{2}(t), \\ldots, x^{n}(t)\\right) \\quad(a \\leqq t \\leqq b)\n$$\n\nand the tangent vector for $\\mathscr{C}$ in the $\\left(\\bar{x}^{i}\\right)$-system has components\n\n$$\n\\bar{T}^{i}=\\frac{d \\bar{x}^{i}}{d t}\n$$\n\nBut, by the chain rule,\n\n$$\n\\frac{d \\bar{x}^{i}}{d t}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{d x^{r}}{d t} \\quad \\text { or } \\quad \\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nproving that $\\mathbf{T}$ is a contravariant vector. (Note that because $\\mathbf{T}$ is defined only on the curve $\\mathscr{C}$, we have $\\mathscr{S}=\\mathscr{C}$ for this particular vector field.) We conclude in general that under a change of coordinates, the tangent vector of $a$ smooth curve transforms as a contravariant tensor of order one.\n\nRemark 1: In some treatments of the subject, tensors are defined to possess certain weights, with (3.8) replaced by\n\n\n\\begin{equation*}\n\\text { weighted contravariant vector } \\quad \\bar{T}^{i}=w T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad(1 \\leqq i \\leqq n) \\tag{3.9}\n\\end{equation*}\n\n\nfor some real-valued function $w$ (the \"weight of $\\mathbf{T}$ \").\n\nIn framing the next definition we (arbitrarily) shift to a subscript notation for the components of the vector field.\n\nDefinition 3: The vector field $\\mathbf{V}$ is a covariant tensor of order one (or covariant vector) provided its components $\\left(T_{i}\\right)$ and $\\left(\\bar{T}_{i}\\right)$ relative to an arbitrary pair of coordinate systems $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$, respectively, obey the law of transformation\n\n\n\\begin{equation*}\n\\text { covariant vector } \\quad \\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\quad(1 \\leqq i \\leqq n) \\tag{3.10}\n\\end{equation*}\n\n\nEXAMPLE 3.5 Let $F(\\mathbf{x})$ denote a differentiable scalar field defined in a coordinate system $\\left(x^{i}\\right)$ of $\\mathbf{R}^{n}$. The gradient of $F$ is defined as the vector field\n\n$$\n\\nabla F \\equiv\\left(\\frac{\\partial F}{\\partial x^{1}}, \\frac{\\partial F}{\\partial x^{2}}, \\ldots, \\frac{\\partial F}{\\partial x^{n}}\\right)\n$$\n\nIn a barred coordinate system, the gradient is given by $\\overline{\\nabla F}=\\left(\\partial \\bar{F} / \\partial \\bar{x}^{i}\\right)$, where $\\bar{F}(\\overline{\\mathbf{x}}) \\equiv F \\circ \\mathbf{x}(\\overline{\\mathbf{x}})$. The chain rule for partial derivatives, together with the functional relations (3.6), gives\n\n$$\n\\frac{\\partial \\bar{F}}{\\partial \\bar{x}^{i}}=\\frac{\\partial F}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nwhich is just (3.10) for $T_{i}=\\partial F / \\partial x^{i}, \\bar{T}_{i}=\\partial \\bar{F} / \\partial \\bar{x}^{i}$. Thus, the gradient of an arbitrary differentiable function is $a$ covariant vector.\n\nRemark 2: Tangent vectors and gradient vectors are really two different kinds of vectors. Tensor calculus is vitally concerned with the distinction between contravariance and covariance, and consistently employs upper indices to indicate the one and lower indices to indicate the other.\n\nRemark 3: From this point on, we shall frequently refer to first-order tensors, contravariant or covariant as the case may be, simply as \"vectors\"; they are, of course, actually vector fields, defined on $\\mathbf{R}^{n}$. This usage will coexist with our earlier employment of \"vectors\" to denote real $n$-tuples; i.e., elements of $\\mathbf{R}^{n}$. There is no conflict here insofar as the $n$-tuples make up the vector field corresponding to the identity mapping $V^{i}(\\mathbf{x})=$ $x^{i} \\quad(i=1,2, \\ldots, n)$. But the vector $\\left(x^{i}\\right)$ does not enjoy the transformation property of a tensor; so, to emphasize that fact, we shall sometimes refer to it as a position vector.\n\n\\subsection*{3.3 INVARIANTS}\nObjects, functions, equations, or formulas that are independent of the coordinate system used to express them have intrinsic value and are of fundamental significance; they are called invariants. Roughly speaking, the product of a contravariant vector and a covariant vector always is an invariant. The following is a more precise statement of this fact.\n\nTheorem 3.1: Let $S^{i}$ and $T_{i}$ be the components of a contravariant and covariant vector, respectively. If the inner product $E \\equiv S^{r} T_{r}$ is defined in each coordinate system, then $E$ is an invariant.\n\nEXAMPLE 3.6 In Examples 3.4 and 3.5 it was established that the tangent vector, $\\left(S^{i}\\right)=\\left(d x^{i} / d t\\right)$, to a curve $\\mathscr{C}$ and the gradient of a function, $\\left(T_{i}\\right)=\\left(\\partial F / \\partial x^{i}\\right)$, are contravariant and covariant vectors, respectively. Let us verify Theorem 3.1 for these two vectors. Define\n\n$$\nE=S^{r} T_{r} \\equiv \\frac{\\partial F}{\\partial x^{r}} \\frac{d x^{r}}{d t}\n$$\n\nNow, by the chain rule,\n\n$$\nE=\\frac{d F}{d t}\n$$\n\nso the assertion of Theorem 3.1 is that the value of\n\n$$\n\\frac{d}{d t}\\left[F \\circ\\left(x^{i}(t)\\right)\\right] \\equiv \\frac{d}{d t}[\\hat{F}(t)]\n$$\n\nis independent of the particular coordinate system $\\left(x^{i}\\right)$ used to specify the curve. To visualize this, the reader should study Fig. 3-4, which shows how the composition $\\hat{F}=F^{\\circ}\\left(x^{i}(t)\\right)$ works out in $\\mathbf{R}^{3}$. It is apparent here that the map $\\hat{F}$ entirely bypasses the coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$. Thus, $\\hat{F}$ - and with it, $d \\hat{F} / d t-$ is an invariant with respect to coordinate changes.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-037}\n\\end{center}\n\nFig. 3-4\n\n\\subsection*{3.4 HIGHER-ORDER TENSORS}\nTensors of arbitrary order may be defined. Although most work does not involve tensors of order greater than 4 , the general definition will be included here for completeness. We begin with the three types of second-order tensors.\n\n\\section*{Second-Order Tensors}\nLet $\\mathbf{V}=\\left(V^{i j}\\right)$ denote a matrix field; that is, $\\left(V^{i j}\\right)$ is an $n \\times n$ matrix of scalar fields $V^{i j}(\\mathbf{x})$, all defined over the same region $\\mathscr{U}=\\{\\mathbf{x}\\}$ in $\\mathbf{R}^{n}$. As before, it will be assumed that $\\mathbf{V}$ has a representation $\\left(T^{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}^{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$, where $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ are admissible coordinates related by (3.1) and (3.6).\n\nDefinition 4: The matrix field $\\mathbf{V}$ is a contravariant tensor of order two if its components $\\left(T^{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}^{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { contravariant tensor } \\quad \\bar{T}^{i j}=T^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.11}\n\\end{equation*}\n\n\nAgain going over to subscript notation for the components of the matrix field, we state\n\nDefinition 5: The matrix field $\\mathbf{V}$ is a covariant tensor of order two if its components $\\left(T_{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { covariant tensor } \\quad \\bar{T}_{i j}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.12}\n\\end{equation*}\n\n\nTheorem 3.2: Suppose that $\\left(T_{i j}\\right)$ is a covariant tensor of order two. If the matrix $\\left[T_{i j}\\right]_{n n}$ is invertible on $\\mathscr{U}$, with inverse matrix $\\left[T^{i j}\\right]_{n n}$, then $\\left(T^{i j}\\right)$ is a contravariant tensor of order two.\n\nDefinition 6: The matrix field $\\mathbf{V}$ is a mixed tensor of order two, contravariant of order one and covariant of order one, if its components $\\left(T_{j}^{i}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{j}^{i}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { mixed tensor } \\quad \\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.13}\n\\end{equation*}\n\n\n\\section*{Tensors of Arbitrary Order}\nVector and matrix fields are inadequate for higher-order tensors. It is necessary to introduce a generalized vector field $\\mathbf{V}$, which is an ordered array of $n^{m}(m=p+q)$ scalar fields, $\\left(V_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$, defined over a region $\\mathcal{U}$ in $\\mathbf{R}^{n}$; let $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right.$ ) denote the set of component-functions in various coordinate systems which are defined on $U$.\n\nDefinition 7: The generalized vector field $\\mathbf{V}$ is a tensor of order $m=p+q$, contravariant of order $p$ and covariant of order $q$, if its components $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\ngeneral tensor $\\bar{T}_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}=T_{s_{1} s_{2} \\ldots s_{q}}^{r_{1} r_{2} \\ldots r_{p}} \\frac{\\partial \\bar{x}^{i_{1}}}{\\partial x^{r_{1}}} \\frac{\\partial \\bar{x}^{i_{2}}}{\\partial x^{r_{2}}} \\cdots \\frac{\\partial \\bar{x}^{i_{p}}}{\\partial \\bar{x}_{p}^{r_{p}}} \\frac{\\partial x^{s_{1}}}{\\partial \\bar{x}^{j_{1}}} \\frac{\\partial x^{s_{2}}}{\\partial \\bar{x}^{j_{2}}} \\cdots \\frac{\\partial x^{s_{q}}}{\\partial \\bar{x}^{j_{q}}}$\n\nwith the obvious range for free indices.\n\n\\subsection*{3.5 THE STRESS TENSOR}\nIt was the concept of stress in mechanics that originally led to the invention of tensors (tenseur, that which exerts tension, stress). Suppose that the unit cube is in equilibrium under forces applied to three of its faces [Fig. 3-5(a)]. Since each face has unit area, each force vector represents the force\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039(1)}\n\\end{center}\n\n(b)\n\nFig. 3-5\n\nper unit area, or stress. Those forces are represented in the component form in Fig. 3-5(b). Using the standard basis $\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{2}$, we have\n\n\\[\n\\begin{array}{ll}\n\\mathbf{v}_{1}=\\sigma^{1 s} \\mathbf{e}_{s} & (\\text { stress on face } 1) \\\\\n\\mathbf{v}_{2}=\\sigma^{2 s} \\mathbf{e}_{s} & (\\text { stress on face } 2)  \\tag{3.15}\\\\\n\\mathbf{v}_{3}=\\sigma^{3 s} \\mathbf{e}_{s} & (\\text { stress on face } 3)\n\\end{array}\n\\]\n\n\\section*{Stress on a Cube Section}\nThe question arises: What stress $\\mathbf{F}$ is transmitted to a planar cross section of the cube that has unit normal $\\mathbf{n}$ ? To answer this, refer to Fig. 3-6, which shows the tetrahedron formed by the cross section and the coordinate planes. Let $A$ be the cross-sectional area. By the assumed equilibrium of the cube, the stresses on the $x^{1} x^{2}-, x^{1} x^{3}$, and $x^{2} x^{3}$-bases of the tetrahedron are $-\\mathbf{v}_{3},-\\mathbf{v}_{2}$, and $-\\mathbf{v}_{1}$, respectively, as shown componentwise in Fig. 3-6. Hence, the forces on these same bases are $B_{1}\\left(-\\mathbf{v}_{3}\\right), B_{2}\\left(-\\mathbf{v}_{2}\\right)$, and $B_{3}\\left(-\\mathbf{v}_{1}\\right)$, respectively. For the tetrahedron itself to be in equilibrium, the resultant force on it must vanish:\n\n$$\nA \\mathbf{F}+B_{1}\\left(-\\mathbf{v}_{3}\\right)+B_{2}\\left(-\\mathbf{v}_{2}\\right)+B_{3}\\left(-\\mathbf{v}_{1}\\right)=0\n$$\n\nor, solving for $\\mathbf{F}$,\n\n\n\\begin{equation*}\n\\mathbf{F}=\\frac{B_{3}}{A} \\mathbf{v}_{1}+\\frac{B_{2}}{A} \\mathbf{v}_{2}+\\frac{B_{1}}{A} \\mathbf{v}_{3} \\tag{3.16}\n\\end{equation*}\n\n\nBut $B_{3}$ is the projection of $A$ in the $x^{2} x^{3}$-plane: $B_{3}=A \\mathbf{n e}_{1}$ or $B_{3} / A=\\mathbf{n e}_{1}$. Similarly, $B_{2} / A=\\mathbf{n e}_{2}$ and $B_{1} / A=\\mathbf{n e}_{3}$. Substituting these expressions and the expressions (3.15) into (3.16), we find that\n\n\n\\begin{equation*}\n\\mathbf{F}=\\sigma^{r s}\\left(\\mathbf{n} \\mathbf{e}_{r}\\right) \\mathbf{e}_{s} \\tag{3.17}\n\\end{equation*}\n\n\n\\section*{Contravariance of Stress}\n\\section*{Under Change of Coordinates}\nAn interesting formula results from (3.17) when we change the basis of $\\mathbf{R}^{3}$ by a transformation of the form $\\mathbf{e}_{i}=a_{i}^{j} \\mathbf{f}_{j}$ (with $\\left|a_{i}^{j}\\right| \\neq 0$ ). In terms of coordinates,\n\n$$\nx^{i} \\mathbf{e}_{i}=x^{i}\\left(a_{i}^{j} \\mathbf{f}_{j}\\right)=\\left(a_{i}^{j} x^{i}\\right) \\mathbf{f}_{j} \\equiv \\bar{x}^{j} \\mathbf{f}_{j}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-040}\n\\end{center}\n\nFig. 3-6\n\nThat is, we have a new coordinate system $\\left(\\vec{x}^{i}\\right)$ that is related to $\\left(x^{i}\\right)$ via\n\n\n\\begin{equation*}\n\\bar{x}^{j}=a_{i}^{j} x^{i} \\tag{3.18}\n\\end{equation*}\n\n\nNote that here we have\n\n$$\n\\frac{\\partial \\bar{x}^{j}}{\\partial x^{i}}=a_{i}^{j}\n$$\n\nSubstituting $\\mathbf{e}_{r}=a_{r}^{i} \\mathbf{f}_{i}$ into (3.17) yields the stress components $\\left(\\bar{\\sigma}^{i j}\\right)$ in the new coordinate system, as follows:\n\nwith\n\n\n\\begin{gather*}\n\\mathbf{F}=\\sigma^{r s}\\left[\\mathbf{n}\\left(a_{r}^{i} \\mathbf{f}_{i}\\right)\\right]\\left(a_{s}^{j} \\mathbf{f}_{j}\\right)=\\sigma^{r s} a_{r}^{i} a_{s}^{j}\\left(\\mathbf{n f}_{i}\\right) \\mathbf{f}_{j} \\equiv \\bar{\\sigma}^{i j}\\left(\\mathbf{n f}_{i}\\right) \\mathbf{f}_{j} \\\\\n\\bar{\\sigma}^{i j}=\\sigma^{r s} a_{r}^{i} a_{s}^{j}=\\sigma^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\tag{3.19}\n\\end{gather*}\n\n\nA comparison of (3.19) with the transformation law (3.11) leads to the conclusion that the stress components $\\sigma^{i j}$ define a second-order contravariant tensor, at least for linear coordinate changes.\n\n\\subsection*{3.6 CARTESIAN TENSORS}\nTensors corresponding to admissible linear coordinate changes, $\\mathscr{T}: \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$, are called affine tensors. If ( $a_{j}^{i}$ ) is orthogonal (and $\\mathscr{T}$ is distance-preserving), the corresponding tensors are cartesian tensors. Now, an object that is a tensor with respect to all one-one linear transformations is necessarily a tensor with respect to all orthogonal linear transformations, but the converse is not true. Hence, affine tensors are special cartesian tensors. Likewise, affine invariants are particular cartesian invariants.\n\n\\section*{Affine Tensors}\nA transformation of the form $\\mathscr{T}: \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$ takes a rectangular coordinate system $\\left(x^{i}\\right)$ into a system $\\left(\\bar{x}^{i}\\right)$ having oblique axes; thus affine tensors are defined on the class of all such\\\\\noblique coordinate systems. Since the Jacobian matrices of $\\mathscr{T}$ and $\\mathscr{T}^{-1}$ are\n\n\n\\begin{equation*}\nJ=\\left[\\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right]_{n n}=\\left[a_{j}^{i}\\right]_{n n} \\quad \\text { and } \\quad J^{-1}=\\left[\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right]_{n n} \\equiv\\left[b_{j}^{i}\\right]_{n n} \\tag{3.20}\n\\end{equation*}\n\n\nthe transformation laws for affine tensors are:\n\n\\[\n\\begin{array}{rll}\n\\text { contravariant } & \\bar{T}^{i}=a_{r}^{i} T^{r}, \\quad \\bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, \\quad \\bar{T}^{i j k}=a_{r}^{i} a_{s}^{j} a_{t}^{k} T^{r s t}, \\ldots \\\\\n\\text { covariant } & \\bar{T}_{i}=b_{i}^{r} T_{r}, \\quad \\bar{T}_{i j}=b_{i}^{r} b_{j}^{s} T_{r s}, \\quad \\bar{T}_{i j k}=b_{i}^{r} b_{j}^{s} b_{k}^{t} T_{r s t}, \\quad \\ldots  \\tag{3.21}\\\\\n\\text { mixed } & \\bar{T}_{j}^{i}=a_{r}^{i} b_{j}^{s} T_{s}^{r}, \\quad \\bar{T}_{j k}^{i}=a_{r}^{i} b_{j}^{s} b_{k}^{t} T_{s t}^{r}, \\ldots\n\\end{array}\n\\]\n\nUnder the less stringent conditions (3.21), more objects can qualify as tensors than before; for instance, an ordinary position vector $\\mathbf{x}=\\left(x^{i}\\right)$ becomes an (affine) tensor (see Problem 3.9), and the partial derivatives of a tensor define an (affine) tensor (as implied by Problem 2.23).\n\n\\section*{Cartesian Tensors}\nWhen the above linear transformation $\\mathscr{T}$ is restricted to be orthogonal, then $J^{-1}=J^{T}$, or\n\n$$\nb_{j}^{i}=a_{i}^{j} \\quad(1 \\leqq i, j \\leqq n)\n$$\n\nso that the transformation laws for cartesian tensors are, from (3.21),\n\n$$\n\\begin{array}{rlll}\n\\text { contravariant } & \\bar{T}^{i}=a_{r}^{i} T^{r}, & \\bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, & \\ldots \\\\\n\\text { covariant } & \\bar{T}_{i}=a_{r}^{i} T_{r}, & \\bar{T}_{i j}=a_{r}^{i} a_{s}^{j} T_{r s}, & \\ldots \\\\\n\\text { mixed } & & \\bar{T}_{j}^{i}=a_{r}^{i} a_{s}^{j} T_{s}^{r}, & \\ldots\n\\end{array}\n$$\n\nA striking feature of these forms is that contravariant and covariant behaviors do not distinguish themselves. Consequently, all cartesian tensors are notated the same way-with subscripts:\n\n\n\\begin{align*}\n\\begin{array}{r}\n\\text { allowable } \\\\\n\\text { coordinate changes }\n\\end{array} & \\bar{x}_{i}=a_{i j} x_{j} \\quad \\text { or } \\quad x_{i}=a_{j i} \\bar{x}_{j} \\\\\n\\begin{array}{r}\n\\text { cartesian } \\\\\n\\text { tensor laws }\n\\end{array} & \\bar{T}_{i}=a_{i r} T_{r}, \\quad \\bar{T}_{i j}=a_{i r} a_{j s} T_{r s}, \\ldots \\tag{3.22}\n\\end{align*}\n\n\nBecause an orthogonal transformation takes one rectangular coordinate system into another (having the same origin), cartesian tensors appertain to the rectangular (cartesian) coordinate systems. There are, of course, even more cartesian tensors than affine tensors.\n\nNote that $J J^{T}=I$ implies $\\mathscr{J}^{2}=1$, or $\\mathscr{J}= \\pm 1$. Objects that obey the tensor laws (3.22) when the allowable coordinate changes are such that\n\n$$\n\\mathscr{J}=\\left|a_{i j}\\right|=+1\n$$\n\nare called direct cartesian tensors.\n\n\\section*{Solved Problems}\n\\section*{CHANGE OF COORDINATES}\n3.1 For the transformation of Example 3.2, (a) obtain the equations for $\\mathscr{T}^{-1} ;(b)$ compute $\\bar{J}$ from (a), and compare with $J^{-1}$.\n\n(a) Solving $\\bar{x}^{1}=x^{1} x^{2}, \\bar{x}^{2}=\\left(x^{2}\\right)^{2}$ for $x^{1}$ and $x^{2}$, we find that\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} x^{2}  \\tag{1}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis a one-one mapping between the regions $x^{2}>0$ and $\\bar{x}^{2}>0$, and that\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{\\prime} x^{2}  \\tag{2}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=-\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=-\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis one-one between $x^{2}<0$ and $\\bar{x}^{2}>0$. Note that the two regions of the $x^{1} x^{2}$-plane are separated by the line on which the Jacobian of $\\mathscr{T}$ vanishes.\n\n(b) From Example 3.2,\n\n$$\nJ=\\left[\\begin{array}{cc}\nx^{2} & x^{1} \\\\\n0 & 2 x^{2}\n\\end{array}\\right] \\quad \\text { and so } \\quad J^{-1}=\\frac{1}{2\\left(x^{2}\\right)^{2}}\\left[\\begin{array}{cc}\n2 x^{2} & -x^{1} \\\\\n0 & x^{2}\n\\end{array}\\right]\n$$\n\nvalid in both regions $x^{2}>0$ and $x^{2}<0$. Now, on $\\bar{x}^{2}>0$, differentiation of the inverse transformation (1), followed by a change back to unbarred coordinates, yields\n\n$$\n\\bar{J}=\\left[\\begin{array}{ll}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(\\bar{x}^{2}\\right)^{-1 / 2} & -\\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & \\frac{1}{2}\\left(\\bar{x}^{2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & \\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\n$$\n\nIt is seen that on $x^{2}>0, \\bar{J}=J^{-1}$.\n\nSimilarly, from (2), with $x^{2}<0$,\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n-\\left(\\bar{x}^{2}\\right)^{-1 / 2} & \\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & -\\frac{1}{2}\\left(\\bar{x}^{-2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n+\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & +\\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=J^{-1}\n$$\n\n3.2 For polar coordinates as defined by (3.2), (a) calculate the Jacobian matrix of $\\mathscr{T}$ and infer the region over which $\\mathscr{T}$ is bijective; $(b)$ calculate the Jacobian matrix of $\\mathscr{T}^{-1}$ for the region\n\n$$\n\\{(r, \\theta) \\mid r>0,-\\pi / 2<\\theta<\\pi / 2\\}\n$$\n\ni.e., the right half-plane, and verify that it is the inverse of the matrix of $(a)$.\n\n(a)\n\n$$\nJ=\\left[\\begin{array}{cc}\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\cos x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\cos x^{2}\\right) \\\\\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\sin x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\sin x^{2}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\cos x^{2} & -x^{1} \\sin x^{2} \\\\\n\\sin x^{2} & x^{1} \\cos x^{2}\n\\end{array}\\right]\n$$\n\nwhence $\\mathscr{J}=x^{1} \\equiv r$. Therefore, $\\mathscr{T}$ is bijective on the open set $r>0$, which is the entire plane punctured at the origin.\n\n(b) For $\\mathscr{T}^{-1}$ we have, over the right half-plane,\n\n$$\n\\begin{gathered}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}}=\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\quad \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}}=\\frac{1}{1+\\left(\\bar{x}^{2} / \\bar{x}^{1}\\right)^{2}}\\left[-\\frac{\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}}\\right]=\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\quad \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{gathered}\n$$\n\nand so\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} & \\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} & \\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]\n$$\n\nNow compute $J^{-1}$ :\n\n$$\nJ^{-1}=\\frac{1}{x^{1}}\\left[\\begin{array}{cc}\nx^{1} \\cos x^{2} & x^{1} \\sin x^{2} \\\\\n-\\sin x^{2} & \\cos x^{2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]=\\bar{J}\n$$\n\n\\section*{CONTRAVARIANT VECTORS}\n3.3 If $\\mathbf{V}=\\left(T^{i}\\right)$ is a contravariant vector, show that the partial derivatives $T_{j}^{i} \\equiv \\partial T^{i} / \\partial x^{j}$, defined in each coordinate system, transform according to the rule\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{r} \\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nDifferentiate both sides of\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $\\bar{x}^{j}$, using the product rule:\n\n\n\\begin{equation*}\n\\bar{T}_{j}^{i} \\equiv \\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{j}}=\\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy the chain rule for partial derivatives, $(2.15)$,\n\n$$\n\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\equiv T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { and } \\quad \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left[\\frac{\\partial}{\\partial x^{s}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\right] \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nSubstituting these expressions into (1) yields the desired formula.\n\n3.4 Suppose that $\\left(T^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{2}$ and that $\\left(T^{i}\\right)=\\left(x^{2}, x^{1}\\right)$ in the $\\left(x^{i}\\right)$-system.\n\nCalculate $\\left(\\bar{T}^{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system, under the change of coordinates\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{2}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\nBy definition of contravariance,\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}=T^{1} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{1}}+T^{2} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{2}}\n$$\n\nNote that the top row of the Jacobian matrix $J$ is needed for the case $i=1$, and the bottom row is needed for $i=2$.\n\n$$\nJ=\\left[\\begin{array}{ll}\n\\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} \\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 2 x^{2} \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\n$$\n\nThus,\n\n$$\n\\bar{T}^{1}=T^{1}(0)+T^{2}\\left(2 x^{2}\\right)=2 x^{1} x^{2} \\quad \\bar{T}^{2}=T^{1}\\left(x^{2}\\right)+T^{2}\\left(x^{1}\\right)=\\left(x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\n$$\n\nwhich, in terms of barred coordinates, are\n\n$$\n\\bar{T}^{1}=2 \\bar{x}^{2} \\quad \\bar{T}^{2}=\\bar{x}^{1}+\\frac{\\left(\\bar{x}^{2}\\right)^{2}}{\\bar{x}^{1}}\n$$\n\n3.5 Show that a contravariant vector can be constructed the components of which take on a given set of values $(a, b, c, \\ldots)$ in some particular coordinate system. (The prescribed values may be point functions.)\n\nLet $(a, b, c, \\ldots) \\equiv\\left(a^{i}\\right)$ be the given values to be assigned in the coordinate system $\\left(x^{i}\\right)$. Set $V^{i}=a^{i}$ for the values in $\\left(x^{i}\\right)$, and for any other admissible coordinate system $\\left(\\bar{x}^{i}\\right)$, set $\\bar{V}^{i}=a^{r}\\left(\\partial \\bar{x}^{i} / \\partial x^{r}\\right)$. To show that $\\left(V^{i}\\right)$ is a contravariant tensor, let $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ be any two admissible coordinate systems. Then, $y^{i}=f^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ and $\\bar{y}^{i}=g^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$, and, by definition, the values of $\\left(V^{i}\\right)$ in $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ are, respectively, $T^{i}=a^{r}\\left(\\partial y^{i} / \\partial x^{r}\\right)$ and $\\bar{T}^{i}=a^{r}\\left(\\partial \\bar{y}^{i} / \\partial x^{r}\\right)$. But, by the chain rule,\n\n$$\n\\bar{T}^{i}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial x^{r}}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\frac{\\partial y^{s}}{\\partial x^{r}}=T^{s} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\quad \\text { QED }\n$$\n\n\\section*{COVARIANT VECTORS}\n3.6 Calculate $\\left(\\bar{T}_{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system if $\\mathbf{V}=\\left(T_{i}\\right) \\equiv\\left(x^{2}, x^{1}+2 x^{2}\\right)$ is a covariant vector under the coordinate transformation of Problem 3.4.\n\nTo avoid radicals, compute $J^{-1}$ in terms of $\\left(x^{i}\\right)$ :\n\n$$\nJ^{-1}=\\left[\\begin{array}{cc}\n\\frac{-x^{1}}{2\\left(x^{2}\\right)^{2}} & \\frac{1}{x^{2}} \\\\\n\\frac{1}{2 x^{2}} & 0\n\\end{array}\\right]\n$$\n\nBy covariance,\n\n$$\n\\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}=T_{1} \\frac{\\partial x^{1}}{\\partial \\bar{x}^{i}}+T_{2} \\frac{\\partial x^{2}}{\\partial \\bar{x}^{i}} \\quad(i=1,2)\n$$\n\nFor $i=1$, read off the partials from the first column of $J^{-1}$ :\n\n$$\n\\bar{T}_{1}=T_{1}\\left(-x^{1} / 2\\left(x^{2}\\right)^{2}\\right)+T_{2}\\left(1 / 2 x^{2}\\right)=-x^{1} / 2 x^{2}+x^{1} / 2 x^{2}+1=1\n$$\n\nSimilarly, for $i=2$, use the second column of $J^{-1}$ :\n\n$$\n\\bar{T}_{2}=T_{1}\\left(1 / x^{2}\\right)+T_{2}(0)=x^{2}\\left(1 / x^{2}\\right)=1\n$$\n\nHence, $\\left(\\bar{T}_{i}\\right)=(1,1)$ at all points in the $\\left(\\bar{x}^{i}\\right)$-system $\\left(\\bar{x}^{1}=0\\right.$ excluded $)$.\n\n3.7 Use the fact that $\\nabla f$ is a covariant vector (Example 3.5) to bring the partial differential equation\n\n\n\\begin{equation*}\nx \\frac{\\partial f}{\\partial x}=y \\frac{\\partial f}{\\partial y} \\tag{1}\n\\end{equation*}\n\n\ninto simpler form by the change of variables $\\bar{x}=x y, \\bar{y}=(y)^{2}$; then solve.\n\nWrite $\\nabla f=(\\partial f / \\partial x, \\partial f / \\partial y) \\equiv\\left(T_{i}\\right),\\left(x^{1}, x^{2}\\right)=(x, y),\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)=(\\bar{x}, \\bar{y})$, and\n\n$$\n\\bar{T}_{i} \\equiv \\frac{\\partial \\bar{f}}{\\partial \\bar{x}^{i}}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nAgain calculating $J$ first, then its inverse, we have\n\nso that\n\n$$\n\\left(\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right) \\equiv J^{-1}=\\left[\\begin{array}{cc}\ny & x \\\\\n0 & 2 y\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{cc}\n\\frac{1}{y} & \\frac{-x}{2(y)^{2}} \\\\\n0 & \\frac{1}{2 y}\n\\end{array}\\right]\n$$\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{x}} \\equiv \\bar{T}_{1}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{1}}=T_{1} \\cdot \\frac{1}{y}+T_{2} \\cdot 0=\\frac{1}{y} \\frac{\\partial f}{\\partial x} \\\\\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{y}} \\equiv \\bar{T}_{2}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{2}}=T_{1} \\cdot \\frac{-x}{2(y)^{2}}+T_{2} \\cdot \\frac{1}{2 y}=-\\frac{x}{2(y)^{2}} \\frac{\\partial f}{\\partial x}+\\frac{1}{2 y} \\frac{\\partial f}{\\partial y}\n\\end{aligned}\n$$\n\nBut, by (1),\n\n$$\n\\frac{\\partial \\bar{f}}{\\partial \\bar{y}}=\\frac{1}{2(y)^{2}}\\left(-x \\frac{\\partial f}{\\partial x}+y \\frac{\\partial f}{\\partial y}\\right)=0\n$$\n\nwhich implies that $\\bar{f}=F(\\bar{x})$, a function of $\\bar{x}$ alone; therefore, $f=F(x y)$ is the general solution to (1).\n\n\\section*{INVARIANTS}\n\\subsection*{3.8 Prove Theorem 3.1.}\nWe must show that if $\\left(S^{i}\\right)$ and $\\left(T_{i}\\right)$ are tensors of the indicated types and order, then the quantity $E \\equiv S^{i} T_{i}$ is invariant with respect to coordinate changes; that is, $\\bar{E}=E$, where $\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}$. But observe that\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}_{i}=T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}\n$$\n\nso that, in view of $(3.7)$,\n\n$$\n\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\delta_{r}^{s}=S^{r} T_{r}=E\n$$\n\n3.9 Show that under linear coordinate changes of $\\mathbf{R}^{n}, \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$, the equation of a hyperplane $A_{i} x^{i}=1$ is invariant provided the normal vector $\\left(A_{i}\\right)$ is covariant.\n\nIn view of Theorem 3.1, it suffices to show that $\\left(T^{i}\\right)=\\left(x^{i}\\right)$ is a contravariant affine tensor. But this is immediate:\n\n$$\n\\bar{T}^{i} \\equiv \\bar{x}^{i}=a_{j}^{i} x^{j} \\equiv a_{j}^{i} T^{j}\n$$\n\nwhich is the transformation law (3.21).\n\n\\section*{SECOND-ORDER CONTRAVARIANT TENSORS}\n3.10 Suppose that the components of a contravariant tensor $T$ of order 2 in a coordinate system $\\left(x^{i}\\right)$ of $\\mathbf{R}^{2}$ are $T^{11}=1, T^{12}=1, T^{21}=-1$, and $T^{22}=2$. (a) Find the components $\\bar{T}^{j j}$ of $\\mathbf{T}$ in the $\\left(\\bar{x}^{i}\\right)$-system, connected to the $\\left(x^{i}\\right)$-system via\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{1}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\n(b) Compute the values of the $\\bar{T}^{i j}$ at the point which corresponds to $x^{1}=1, x^{2}=-2$.\n\nFor economy of effort, the problem will be worked using matrices.\n\n(a) Writing\n\n$$\nJ_{j}^{i} \\equiv J_{i}^{\\prime j} \\equiv \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\n$$\n\nwe have from $(2.1 b)$,\n\n$$\n\\bar{T}^{i j}=T^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}}=J_{r}^{i} T^{r s} J_{j}^{s}\n$$\n\nThat is,\n\n$$\n\\begin{aligned}\n\\bar{T} & =J T J^{T} \\\\\n& =\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\n2 x^{1} & x^{2} \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n(b) At the point $(1,-2)$,\n\n$$\n\\begin{array}{ll}\n\\bar{T}^{11}=4(1)^{2}=4 & \\bar{T}^{12}=2(1)(-2)+2(1)^{2}=-2 \\\\\n\\bar{T}^{21}=2(1)(-2)-2(1)^{2}=-6 & \\bar{T}^{22}=2(1)^{2}+(-2)^{2}=6\n\\end{array}\n$$\n\n3.11 Show that if $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ are contravariant vectors on $\\mathbf{R}^{n}$, the matrix $\\left[U^{i j}\\right] \\equiv\\left[S^{i} T^{j}\\right]_{n n}$, defined in this manner for all coordinate systems, represents a contravariant tensor of order 2.\n\nMultiply\n\nto obtain\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}^{j}=T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\n$$\n\\bar{U}^{i j}=\\bar{S}^{i} \\bar{T}^{j}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}=U^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the tensor law. (The notion of the \"outer product\" of two tensors will be further developed in Chapter 4.)\n\n\\section*{SECOND-ORDER COVARIANT TENSORS}\n3.12 Show that if $T_{i}$ are the components of covariant vector $\\mathbf{T}$, then $S_{i j} \\equiv T_{i} T_{j}-T_{j} T_{i}$ are the components of a skew-symmetric covariant tensor $\\mathbf{S}$.\n\nThe skew-symmetry is obvious. From the transformation law for $\\mathbf{T}$,\n\nor\n\n$$\n\\begin{gathered}\n\\bar{T}_{i} \\bar{T}_{j}-\\bar{T}_{j} \\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\cdot T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\\\\n=T_{r} T_{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\left(T_{r} T_{s}-T_{s} T_{r}\\right) \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\\\\n\\bar{S}_{i j}=S_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n\\end{gathered}\n$$\n\nwhich establishes the covariant tensor character of $\\mathbf{S}$.\n\n3.13 If a symmetric array $\\left(T_{i j}\\right)$ transforms according to\n\n$$\n\\bar{T}_{i j}=T_{r t} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}}\n$$\n\nshow that it defines a second-order covariant tensor.\n\n$$\n\\begin{aligned}\n\\bar{T}_{i j} & =T_{r t}\\left(\\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{s}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{r t} \\delta_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\\\\n& =T_{s t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{t s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n\\end{aligned}\n$$\n\n3.14 Let $\\mathbf{U}=\\left(U_{i j}\\right)$ be a covariant tensor of order 2 . Under the same coordinate change as in Problem 3.10, (a) calculate the components $\\bar{U}_{i j}$, if $U_{11}=x^{2}, U_{12}=U_{21}=0, U_{22}=x^{1} ;(b)$ verify that the quantity $T^{i j} U_{i j}=E$ is an invariant, where the $T^{i j}$ and $\\bar{T}^{i j}$ are obtained from Problem 3.10.\n\n(a) In terms of the inverse Jacobian matrix, the covariant transformation law is\n\n$$\n\\bar{U}_{i j}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} U_{r s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\bar{J}_{i}^{r} U_{r s} \\bar{J}_{j}^{s}=\\bar{J}_{r}^{\\prime i} U_{r s} \\bar{J}_{j}^{s} \\quad \\text { or } \\quad \\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nSubstituting\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\quad U=\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\n$$\n\nwe find\n\n$$\n\\bar{U}=\\left[\\begin{array}{cc}\n\\frac{1}{2 x^{1}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n0 & \\frac{1}{x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]\n$$\n\nfrom which the $\\bar{U}_{i j}$ may be read off.\n\n(b) Continuing in the matrix approach, we note that $E$ is the trace (sum of diagonal elements) of the matrix $T U^{T}$.\n\n$$\n\\begin{aligned}\n& T U^{T}=\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{rr}\nx^{2} & x^{1} \\\\\n-x^{2} & 2 x^{1}\n\\end{array}\\right] \\\\\n& E=x^{2}+2 x^{1}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\bar{T} \\bar{U}^{T}=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{cc}\n0 & 2 x^{1} \\\\\n-\\frac{3 x^{2}}{2} & x^{2}+2 x^{1}\n\\end{array}\\right] \\\\\n& \\bar{E}=x^{2}+2 x^{1}=E\n\\end{aligned}\n$$\n\n3.15 Prove Theorem 3.2.\n\nObserve first of all that if a covariant matrix (second-order tensor) $U$ has inverse $V$ in unbarred coordinates, then $\\bar{U}$ has inverse $\\bar{V}$ in barred coordinates; i.e., $(\\bar{U})^{-1}=\\overline{U^{-1}}$. Now, by Problem 3.14(a),\n\n$$\n\\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nInverting both sides of this matrix equation, applying Problem 2.13 , and recalling that $J \\bar{J}=I$, we obtain\n\n$$\n\\overline{U^{-1}}=\\bar{J}^{-1} U^{-1}\\left(\\bar{J}^{T}\\right)^{-1}=J U^{-1} J^{T}\n$$\n\nwhich is the contravariant law for $U^{-1}$ [see Problem 3.10(a)].\n\n\\section*{MIXED TENSORS}\n3.16 Compute the formulas for the tensor components $\\left(\\bar{T}_{j}^{i}\\right)$ in polar coordinates in terms of $\\left(T_{j}^{i}\\right)$ in rectangular coordinates, if the tensor is symmetric in rectangular coordinates. (In contrast to Section 3.1, it is now the curvilinear coordinates that are barred.)\n\nThe general formula calls for the calculations\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad\\left(T_{j}^{i}=T_{i}^{j}\\right)\n$$\n\nUsing $(2.1 b)$, this may be written in matrix form as\n\n\n\\begin{equation*}\n\\bar{T}=J T J^{-1}=\\bar{J}^{-1} T \\bar{J} \\tag{1}\n\\end{equation*}\n\n\nwhere $T=\\left[T_{j}^{i}\\right]_{22}$ and where\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right]\n$$\n\nis the Jacobian matrix of the transformation from $(r, \\theta)$ to $(x, y)$. Thus,\n\n$$\n\\begin{aligned}\n& \\bar{T}=\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} & T_{2}^{1} \\\\\nT_{2}^{1} & T_{2}^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos \\theta+T_{2}^{1} \\sin \\theta & -r T_{1}^{1} \\sin \\theta+r T_{2}^{1} \\cos \\theta \\\\\nT_{2}^{1} \\cos \\theta+T_{2}^{2} \\sin \\theta & -r T_{2}^{1} \\sin \\theta+r T_{2}^{2} \\cos \\theta\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nThe final matrix multiplication can be carried out routinely, simplifying by means of trigonometric identities:\n\n$$\n\\bar{T}=\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos ^{2} \\theta+T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\sin ^{2} \\theta & -\\frac{r}{2} T_{1}^{1} \\sin 2 \\theta+r T_{2}^{1} \\cos 2 \\theta+\\frac{r}{2} T_{2}^{2} \\sin 2 \\theta \\\\\n-T_{1}^{1} \\frac{\\sin 2 \\theta}{2 r}+T_{2}^{1} \\frac{\\cos 2 \\theta}{r}+T_{2}^{2} \\frac{\\sin 2 \\theta}{2 r} & T_{1}^{1} \\sin ^{2} \\theta-T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\cos ^{2} \\theta\n\\end{array}\\right]\n$$\n\nObserve that $\\bar{T}$ does not share the symmetry of $T: \\quad \\bar{T}_{1}^{2}=r^{-2} \\bar{T}_{2}^{1}$.\n\n3.17 Prove that the determinant of a mixed tensor of order two is invariant.\n\nBy (1) of Problem 3.16, we have-whether or not $T$ is symmetric-\n\n$$\n|\\bar{T}|=\\left|J T J^{-1}\\right|=|J||T|\\left|J^{-1}\\right|=\\mathscr{J}|T| \\mathscr{J}^{-1}=|T|\n$$\n\n\\section*{GENERAL TENSORS}\n3.18 Display the transformation law for a third-order tensor that is contravariant of order two and covariant of order one.\n\nTake $p=2$ and $q=1$ in Definition 7 and, to avoid unnecessary subscripts, write $i, j, k, r, s, t$ in place of $i_{1}, i_{2}, j_{1}, r_{1}, r_{2}, s_{1}$. Then (3.14) gives\n\n$$\n\\bar{T}_{k}^{i j}=T_{t}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\n3.19 Let $\\mathbf{T}=\\left(T_{k l m}^{i j}\\right)$ denote a tensor of the order and type indicated by the indices. Prove that $\\mathbf{S}=\\left(T_{k}\\right) \\equiv\\left(T_{k i j}^{i j}\\right)$ is a covariant vector.\n\nThe transformation law (3.14) for $\\mathbf{T}$ is\n\n$$\n\\bar{T}_{k l m}^{i j}=T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{l}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{m}}\n$$\n\nSet $l=i, m=j$ and sum:\n\n$$\n\\begin{aligned}\n\\bar{T}_{k} \\equiv \\bar{T}_{k i j}^{i j} & =T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}=T_{t u v}^{r s}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}}\\right)\\left(\\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}\\right) \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{t u v}^{r s} \\delta_{r}^{u} \\delta_{s}^{v} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{t r s}^{r s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\equiv T_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\n\\section*{CARTESIAN TENSORS}\n3.20 Show that the permutation symbol $\\left(e_{i j}\\right)$ defines a direct cartesian tensor over $\\mathbf{R}^{2}$. Assume that $e_{i j}$ is defined the same way for all rectangular coordinate systems.\n\nIf the coordinate change is $\\bar{x}_{i}=a_{i j} x_{j}$, where $\\left(a_{i j}\\right)^{T}\\left(a_{k l}\\right)=\\left(\\delta_{p q}\\right)$ and\n\n$$\n\\left|a_{i j}\\right|=a_{11} a_{22}-a_{12} a_{21}=1\n$$\n\nwe must establish the cartesian tensor law (3.22):\n\n$$\n\\bar{e}_{i j}=e_{r s} a_{i r} a_{j s} \\quad(n=2)\n$$\n\nWe examine separately the four possible cases:\n\n$$\n\\begin{aligned}\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{1 r} a_{1 s}=a_{11} a_{12}-a_{12} a_{11}=0=\\bar{e}_{11} \\\\\n\\boldsymbol{i}=\\mathbf{1}, \\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{1 r} a_{2 s}=a_{11} a_{22}-a_{12} a_{21}=1=\\bar{e}_{12} \\\\\n\\boldsymbol{i}=\\mathbf{2}, \\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{2 r} a_{1 s}=a_{21} a_{12}-a_{22} a_{11}=-1=\\bar{e}_{21} \\\\\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{2 r} a_{2 s}=a_{21} a_{22}-a_{22} a_{21}=0=\\bar{e}_{22}\n\\end{aligned}\n$$\n\n3.21 Prove that (a) the coefficients $c_{i j}$ of the quadratic form $c_{i j} x^{i} x^{j}=1$ transform as an affine tensor and $(b)$ the trace $c_{i i}$ of $\\left(c_{i j}\\right)$ is a cartesian invariant.\n\n(a) If $\\bar{x}^{i}=a_{j}^{i} x^{j}$ and $x^{i}=b_{j}^{i} \\bar{x}^{j}$, where $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{-1}$, the quadratic form goes over into\n\n$$\n1=c_{i j}\\left(b_{r}^{i} \\bar{x}^{r}\\right)\\left(b_{s}^{j} \\bar{x}^{s}\\right) \\equiv \\bar{c}_{r s} \\bar{x}^{r} \\bar{x}^{s}\n$$\n\nwith $\\bar{c}_{r s}=b_{r}^{i} b_{s}^{j} c_{i j}$. But this formula is just (3.21) for a covariant affine tensor of order two.\n\n(b) Assuming an orthogonal transformation, $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{T}$, we have\n\n$$\n\\bar{c}_{r s}=b_{r}^{i} a_{j}^{s} c_{i j}\n$$\n\nHence, $\\bar{c}_{r r}=\\left(b_{r}^{i} a_{j}^{r}\\right) c_{i j}=\\delta_{j}^{i} c_{i j}=c_{i i}$.\n\n3.22 Establish the identity between the permutation symbol and the Kronecker delta:\n\n\n\\begin{equation*}\ne_{r i j} e_{r k l} \\equiv \\delta_{i k} \\delta_{j l}-\\delta_{i l} \\delta_{j k} \\tag{3.23}\n\\end{equation*}\n\n\nThe identity implies $n=3$, so that there are potentially $3^{4}=81$ separate cases to consider. However, this number can be quickly reduced to only 4 cases by the following reasoning: If either $i=j$ or $k=l$, then both sides vanish. For example, if $i=j$, then on the left $e_{r i j}=0$, and on the right,\n\n$$\n\\delta_{i k} \\delta_{j t}-\\delta_{j l} \\delta_{i k}=0\n$$\n\nHence, we need only consider the cases in which both $i \\neq j$ and $k \\neq l$. Upon writing out the sum on the left, two of the terms drop out, since $i \\neq j$ :\n\n$$\ne_{1 i j} e_{1 k l}+e_{2 i j} e_{2 k l}+e_{3 i j} e_{3 k l}=e_{1^{\\prime} 2^{\\prime} 3^{\\prime}} e_{1^{\\prime} k l} \\quad\\left(i=2^{\\prime}, j=3^{\\prime}\\right)\n$$\n\nwhere $\\left(1^{\\prime} 2^{\\prime} 3^{\\prime}\\right)$ denotes some permutation of (123). Thus, there are left only two cases, each with two subcases.\n\nCase 1: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l} \\neq 0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Here, either $k=2^{\\prime}$ and $l=3^{\\prime}$ or $k=3^{\\prime}$ and $l=2^{\\prime}$. If the former, then the left member of (3.23) is +1 , while the right member equals\n\n$$\n\\delta_{2^{\\prime} 2^{\\prime}} \\delta_{3^{\\prime} 3^{\\prime}}-\\delta_{2^{\\prime} 3^{\\prime}} \\delta_{3^{\\prime} 2^{\\prime}}=1-0=1\n$$\n\nIf the latter, then both members equal -1 , as can be easily verified.\n\nCase 2: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l}=0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Since $k \\neq l$, either $k=1^{\\prime}$ or $l=1^{\\prime}$. If $k=1^{\\prime}$, then the right member of (3.23) equals\n\n$$\n\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} l}-\\delta_{2^{\\prime} l^{\\prime}} \\delta_{3^{\\prime} 1^{\\prime}}=0-0=0\n$$\n\nIf $l=1^{\\prime}$, we have $\\delta_{2^{\\prime} k} \\delta_{3^{\\prime} 1^{\\prime}}-\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} k}=0-0=0$.\n\nThis completes the examination of all cases, and the identity is established.\n\n\\section*{Supplementary Problems}\n3.23 Suppose that the following transformation connects the $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ coordinate systems:\n\n$$\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\\\\n\\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n\\end{array}\\right.\n$$\n\n(a) Calculate the Jacobian matrix $J$ and the Jacobian $\\mathscr{J}$. Show that $\\mathscr{J} \\neq 0$ over all of $\\mathbf{R}^{2}$. (b) Give equations for $\\mathscr{T}^{-1}$. (c) Calculate the Jacobian matrix $\\bar{J}$ of $\\mathscr{T}^{-1}$ and compare with $J^{-1}$.\n\n3.24 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector, and if the components $S_{i j} \\equiv T_{i} T_{j}+T_{j} T_{i}$ are defined in each coordinate system, then $\\left(S_{i j}\\right)$ is a symmetric covariant tensor. (Compare Problem 3.12.)\n\n3.25 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector and, in each coordinate system, we define\n\n$$\n\\frac{\\partial T_{i}}{\\partial x^{j}}-\\frac{\\partial T_{j}}{\\partial x^{i}}=T_{i j}\n$$\n\nthen $\\left(T_{i j}\\right)$ is a skew-symmetric covariant tensor of the second order. [Hint: Model the proof on Problem 3.3.]\n\n3.26 Convert the partial differential equation\n\n$$\ny \\frac{\\partial f}{\\partial x}=x \\frac{\\partial f}{\\partial y}\n$$\n\nto polar form (making use of the fact that $\\nabla f$ is a covariant vector), and solve for $f(x, y)$.\n\n3.27 Show that the quadratic form $Q=g_{i j} x^{i} x^{j}$ is an affine invariant provided $\\left(g_{i j}\\right)$ is a covariant affine tensor. [Converse of Problem 3.21(a).]\n\n3.28 Prove that the partial derivatives of a contravariant vector $\\left(T^{i}\\right)$ define a mixed affine tensor of order two. [Hint: Compare Problem 2.23.]\n\n3.29 Prove that the Kronecker delta $\\left(\\delta_{j}^{i}\\right)$, uniformly defined in all coordinate systems, is a mixed tensor of order two.\n\n3.30 Show that the permutation symbol $\\left(e_{i j}\\right)$ of order two, uniformly defined in all coordinate systems, is not-Problem 3.20 notwithstanding-covariant under arbitrary coordinate changes. [Hint: Use $x^{1}=$ $\\bar{x}^{1} \\bar{x}^{2}, x^{2}=\\bar{x}^{2}$, at the point $\\left.\\left(\\bar{x}^{i}\\right)=(1,2).\\right]$\n\n3.31 By use of (3.23), establish the familiar identity for the vector product of three vectors,\n\n$$\n\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}\n$$\n\nor, in coordinate form,\n\n$$\ne_{i j k} u_{j}\\left(e_{k r s} v_{r} w_{s}\\right)=\\left(u_{j} w_{j}\\right) v_{i}-\\left(u_{j} v_{j}\\right) w_{i}\n$$\n\n3.32 (a) Show that if $\\left(T_{j}^{i}\\right)$ is a mixed tensor, then $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ is not generally a tensor. (b) Show that a mixed tensor of order two, symmetric in a given coordinate system, will transform as a symmetric tensor if the Jacobian matrix is orthogonal.\n\n3.33 Prove: (a) If $\\left(T_{j}^{i}\\right)$ is a mixed tensor of order two, $T_{i}^{i}$ is an invariant; $(b)$ if $\\left(S_{j k}^{i}\\right)$ and $\\left(T^{i}\\right)$ are tensors of the type and order indicated, $S_{j r}^{r} T^{j}$ is an invariant.\n\n3.34 If $\\mathbf{T} \\equiv\\left(T_{m l}^{i j k}\\right)$ is a tensor, contravariant of order 3 and covariant of order 2 , show that $\\mathbf{S} \\equiv\\left(T_{k j}^{i j k}\\right)$ is a contravariant vector.\n\n3.35 Show that the derivative, $d \\mathbf{T} / d t$, of the tangent vector $\\mathbf{T} \\equiv\\left(T^{i}\\right)=\\left(d x^{i} / d t\\right)$ to a curve $x^{i}=x^{i}(t)$ is a contravariant affine tensor. Is it a cartesian tensor?\n\n3.36 (a) Use the theory of tensors to prove that the scalar product $\\mathbf{u v} \\equiv u_{i} v_{i}$ of two vectors $\\mathbf{u}=\\left(u_{i}\\right)$ and $\\mathbf{v}=\\left(v_{i}\\right)$ is a cartesian invariant. (b) Is uv an affine invariant?\n\n"], "lesson": "\\section*{Chapter 3}\n\\section*{General Tensors}\n\\subsection*{3.1 COORDINATE TRANSFORMATIONS}\nAt this point the notation for coordinates will be changed to that usual in tensor calculus.\n\n\\section*{Superscripts for Vector Components}\nThe coordinates of a point (vector) in $\\mathbf{R}^{n}$ will henceforth be denoted as $\\left(x^{1}, x^{2}, x^{3}, \\ldots, x^{n}\\right)$. Thus, the familiar subscripts are now replaced by superscripts, and the upper position is no longer reserved for exponents alone. It will be clear by context whether a character represents a vector component or the power of a scalar.\n\nEXAMPLE 3.1 If a power of some vector component is to be indicated, obviously parentheses are necessary; thus, $\\left(x^{3}\\right)^{2}$ and $\\left(x^{n-1}\\right)^{5}$ represent, respectively, the square of the third component, and the $(n-1)$ st component raised to the fifth power, of the vector $\\mathbf{x}$. If $u$ is introduced as a real number, then $u^{2}$ and $u^{3}$ are powers of $u$ and not vector components. If $(c)^{k}$ appears without explanation, the parentheses indicate the use of the superscript $k$ as an exponent and not as the index of a vector component.\n\n\\section*{Rectangular Coordinates}\nCoordinates in $\\mathbf{R}^{n}$ are called rectangular (also rectangular cartesian or cartesian) if they are patterned after the usual orthogonal coordinate systems of two- and three-dimensional analytic geometry. A general definition that is workable in this setting is, in effect, an assertion of the converse of the Pythagorean theorem.\n\nDefinition 1: A coordinate system $\\left(x^{i}\\right)$ is rectangular if the distance between two arbitrary points $P\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ and $Q\\left(y^{1}, y^{2}, \\ldots, y^{n}\\right)$ is given by\n\n$$\nP Q=\\sqrt{\\left(x^{1}-y^{1}\\right)^{2}+\\left(x^{2}-y^{2}\\right)^{2}+\\cdots+\\left(x^{n}-y^{n}\\right)^{2}} \\equiv \\sqrt{\\delta_{i j} \\Delta x^{i} \\Delta x^{j}}\n$$\n\nwhere $\\Delta x^{i} \\equiv x^{i}-y^{i}$.\n\nUnder orthogonal coordinate changes, which are isometric, the above formula for distance is invariant (cf. Section 2.5). Hence, all coordinate systems $\\left(\\bar{x}^{i}\\right)$ defined by $\\bar{x}^{i}=a_{r}^{i} x^{r}$, where $\\left(a_{j}^{i}\\right)$ is such that $a_{i}^{r} a_{j}^{r}=\\delta_{i j}$, are rectangular. It can be shown that these are the only rectangular coordinate systems whose origin coincides with that of the $\\left(x^{i}\\right)$-system.\n\n\\section*{Curvilinear Coordinates}\nSuppose that in some region of $\\mathbf{R}^{n}$ two coordinate systems are defined, and that these two systems are connected by equations of the form\n\n\n\\begin{equation*}\n\\mathscr{T}: \\bar{x}^{i}=\\bar{x}^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\quad(1 \\leqq i \\leqq n) \\tag{3.1}\n\\end{equation*}\n\n\nwhere, for each $i$, the function, or scalar field, $x^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ maps the given region in $\\mathbf{R}^{n}$ to the reals and has continuous second-partial derivatives at every point in the region (is class $C^{2}$ ). The transformation $\\mathscr{T}$, if bijective, is called a coordinate transformation, as in Section 2.6. If $\\left(x^{i}\\right)$ are ordinary rectangular coordinates, the $\\left(\\bar{x}^{i}\\right)$ are called curvilinear coordinates unless $\\mathscr{T}$ is linear, in which case $\\left(\\bar{x}^{i}\\right)$ are called affine coordinates.\n\nFor convenience, the three most common curvilinear-coordinate systems are presented below. In each case, a \"reverse\" notation is employed: the two- or three-dimensional curvilinear system $\\left(x^{i}\\right)$ is defined by the mapping $\\mathscr{T}$ that takes it into a rectangular system $\\left(\\bar{x}^{i}\\right)$ of the same dimension.\n\nPolar coordinates (Fig. 3-1). Let $\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)=(x, y)$ and $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$, under the restriction $r>0$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033}\n\\end{center}\n\nFig. 3-1\n\nThen,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\cos x^{2}  \\tag{3.2}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\\\\nx^{2}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{-1}\\right)\n\\end{array}\\right.\\right.\n\\]\n\n(The inverse given here is, in the equation for $x^{2}$, valid only in the first and fourth quadrants of the $\\bar{x}_{1} \\bar{x}_{2}$-plane; other solutions must be used over the other two quadrants. Likewise for the $\\theta$-coordinate in the cylindrical and spherical systems.)\n\nCylindrical coordinates (Fig. 3-2). If $\\left(\\bar{x}^{1}, \\bar{x}^{2}, \\bar{x}^{3}\\right)=(x, y, z)$ and $\\left(x^{1}, x^{2}, x^{3}\\right)=(r, \\theta, z)$, where $r>0$,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\cos x^{2}  \\tag{3.3}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\\\\nx^{2}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{-1}\\right) \\\\\nx^{3}=\\bar{x}^{3}\n\\end{array}\\right.\\right.\n\\]\n\nSpherical coordinates (Fig. 3-3). If $\\left(\\bar{x}_{1}, \\bar{x}_{2}, \\bar{x}_{3}\\right)=(x, y, z)$ and $\\left(x^{1}, x^{2}, x^{3}\\right)=(\\rho, \\varphi, \\theta)$, where $\\rho>0$ and $0 \\leqq \\varphi \\leqq \\pi$,\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} \\sin x^{2} \\cos x^{3}  \\tag{3.4}\\\\\n\\bar{x}^{2}=x^{1} \\sin x^{2} \\sin x^{3} \\\\\n\\bar{x}^{3}=x^{1} \\cos x^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}+\\left(\\bar{x}^{3}\\right)^{2}} \\\\\nx^{2}=\\cos ^{-1}\\left(\\bar{x}^{3} / \\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}+\\left(\\bar{x}^{3}\\right)^{2}}\\right) \\\\\nx^{3}=\\tan ^{-1}\\left(\\bar{x}^{2} / \\bar{x}^{1}\\right)\n\\end{array}\\right.\\right.\n\\]\n\n(Caution: In an older but still common notation for spherical coordinates, $\\theta$ denotes the polar angle and $\\varphi$ the equatorial angle.)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033(1)}\n\\end{center}\n\nFig. 3-2\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-034}\n\\end{center}\n\nFig. 3-3\n\n\\section*{The Jacobian}\n matrix,The $n^{2}$ first-order partial derivatives $\\partial \\bar{x}^{i} / \\partial x^{j}$ arising from (3.1) are normally arranged in an $n \\times n$\n\n\\[\nJ=\\left[\\begin{array}{cccc}\n\\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{n}}  \\tag{3.5}\\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{n}} \\\\\n\\cdots \\cdots & \\cdots & \\cdots \\\\\n\\frac{\\partial \\bar{x}^{n}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{n}}{\\partial x^{2}} & \\cdots & \\frac{\\partial \\bar{x}^{n}}{\\partial x^{n}}\n\\end{array}\\right]\n\\]\n\nMatrix $J$ is the Jacobian matrix, and its determinant $\\mathscr{F} \\equiv \\operatorname{det} J$ is the Jacobian, of the transformation $\\mathscr{T}$.\n\nEXAMPLE 3.2 In $\\mathbf{R}^{2}$ let a curvilinear coordinate system $\\left(\\bar{x}^{i}\\right)$ be defined from rectangular coordinates $\\left(x^{i}\\right)$ by the equations\n\n$$\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} x^{2} \\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array}\\right.\n$$\n\nSince $\\partial \\bar{x}^{1} / \\partial x^{1}=x^{2}, \\partial \\bar{x}^{1} / \\partial x^{2}=x^{1}, \\partial \\bar{x}^{2} / \\partial x^{1}=0$, and $\\partial \\bar{x}^{2} / \\partial x^{2}=2 x^{2}$, the Jacobian of $\\mathscr{T}$ is\n\n$$\n\\mathscr{J}=\\left|\\begin{array}{cc}\nx^{2} & x^{1} \\\\\n0 & 2 x^{2}\n\\end{array}\\right|=2\\left(x^{2}\\right)^{2}\n$$\n\nA well-known theorem from analysis states that $\\mathscr{T}$ is locally bijective on an open set $\\mathscr{U}$ in $\\mathbf{R}^{n}$ if and only if $\\mathscr{F} \\neq 0$ at each point of $U$. When $\\mathscr{F} \\neq 0$ in $U$ and $\\mathscr{T}$ is class $C^{2}$ in $\\mathscr{U}$, then (3.1) is termed an admissible change of coordinates for $\\mathcal{U}$.\n\nEXAMPLE 3.3 The curvilinear coordinates of Example 3.2 are admissible for the regions $x^{2}>0$ and $x^{2}<0$ (both open sets in the plane). See Problem 3.1.\n\nIn an admissible change of coordinates, the inverse transformation $\\mathscr{T}^{-1}$ (the local existence of which is guaranteed by the theorem mentioned above) is also class $C^{2}$, on $\\bar{U}$, the image of $\\mathscr{U}$ under $\\mathscr{T}$. Moreover, if $\\mathscr{T}^{-1}$ has the form\n\n\n\\begin{equation*}\n\\mathscr{T}^{-1}: x^{i}=x^{i}\\left(\\bar{x}^{-1}, \\bar{x}^{2}, \\ldots, \\bar{x}^{n}\\right) \\quad(1 \\leqq i \\leqq n) \\tag{3.6}\n\\end{equation*}\n\n\non $\\bar{U}$, the Jacobian matrix $\\bar{J}$ of $\\mathscr{T}^{-1}$ is the inverse of $J$. Thus, $J \\bar{J}=\\bar{J} J=I$, or\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{j}}=\\delta_{j}^{i} \\tag{3.7}\n\\end{equation*}\n\n\n[cf. Problem 2.42(a)]. It also follows that $\\overline{\\mathscr{F}}=1 / \\overline{\\mathscr{g}}$.\n\n\\section*{General Coordinate Systems}\nIn later developments it will be necessary to adopt coordinate systems that are not tied to rectangular coordinates in any way [via (3:1)] and to define distance in terms of an arc-length formula for arbitrary curves, with points represented abstractly by $n$-tuples $\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$. Each such distance functional or metric will be invariant under admissible changes of coordinates, and admissible coordinate systems will exist for each separate metric. Under such metrics, $\\mathbf{R}^{n}$ will generally become non-Euclidean; e.g., the angle sum of a triangle will not invariably equal $\\pi$.\n\nAlthough the curvilinear coordinate systems presented above are explicitly associated with the Euclidean metric (since they are connected via (3.1) with rectangular coordinates and Euclidean space), those same systems could be formally adopted in a non-Euclidean space if some purpose were served by doing so. The point to be made is that the space metric and the coordinate system used to describe that metric are completely independent of each other, except in the single instance of rectangular coordinates, whose very definition (see Definition 1) involves the Euclidean metric.\n\n\\section*{Usefulness of Coordinate Changes}\nA primary concern in studying tensor analysis is the manner in which a change of coordinates affects the way geometrical objects or physical laws are described. For example, in rectangular coordinates the equation of a circle of radius $a$ centered at the origin is quadratic,\n\n$$\n\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}=a^{2}\n$$\n\nbut in polar coordinates, (3.2), that same circle has the simple linear equation $x^{1}=a$. The reader is no doubt familiar with the sometimes dramatic change that takes place in a differential equation under a change of variables, which is nothing but a change of coordinates. This idea of changing the description of phenomena by changing coordinate systems lies at the heart of not only what a tensor means, but how it is used in practice.\n\n\\subsection*{3.2 FIRST-ORDER TENSORS}\nConsider a vector field $\\mathbf{V}=\\left(V^{i}\\right)$ defined on some subset $\\mathscr{S}$ of $\\mathbf{R}^{n}$ [that is, for each $i$, the component $V^{i}=V^{i}(\\mathbf{x})$ is a scalar field (real-valued function) as $\\mathbf{x}$ varies over $\\mathscr{Y}$ ]. In each admissible coordinate system of a region $U$ containing $\\mathscr{S}$, let the $n$ components $V^{1}, V^{2}, \\ldots, V^{n}$ of $\\mathbf{V}$ be expressible as $n$ real-valued functions; say, as\n\n$$\nT^{1}, \\quad T^{2}, \\ldots, \\quad T^{n} \\quad \\text { in the }\\left(x^{i}\\right) \\text {-system }\n$$\n\nand\n\n$$\n\\bar{T}^{1}, \\quad \\bar{T}^{2}, \\ldots, \\quad \\bar{T}^{n} \\quad \\text { in the }\\left(\\bar{x}^{i}\\right) \\text {-system }\n$$\n\nwhere $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ are related by (3.1) and (3.6).\n\nDefinition 2: The vector field $\\mathbf{V}$ is a contravariant tensor of order one (or contravariant vector) provided its components $\\left(T^{i}\\right)$ and $\\left(\\bar{T}^{i}\\right)$ relative to the respective coordinate systems $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { contravariant vector } \\quad \\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad(1 \\leqq i \\leqq n) \\tag{3.8}\n\\end{equation*}\n\n\nEXAMPLE 3.4 Let $\\mathscr{C}$ be a curve given parametrically in the $\\left(x^{i}\\right)$-system by\n\n$$\nx^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)\n$$\n\nThe tangent vector field $\\mathbf{T}=\\left(T^{i}\\right)$ is defined by the usual differentiation formula\n\n$$\nT^{i}=\\frac{d x^{i}}{d t}\n$$\n\nUnder a change of coordinates (3.1), the same curve is given in the $\\left(\\bar{x}^{i}\\right)$-system by\n\n$$\n\\bar{x}^{i}=\\bar{x}^{i}(t) \\equiv \\bar{x}^{i}\\left(x^{1}(t), x^{2}(t), \\ldots, x^{n}(t)\\right) \\quad(a \\leqq t \\leqq b)\n$$\n\nand the tangent vector for $\\mathscr{C}$ in the $\\left(\\bar{x}^{i}\\right)$-system has components\n\n$$\n\\bar{T}^{i}=\\frac{d \\bar{x}^{i}}{d t}\n$$\n\nBut, by the chain rule,\n\n$$\n\\frac{d \\bar{x}^{i}}{d t}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{d x^{r}}{d t} \\quad \\text { or } \\quad \\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nproving that $\\mathbf{T}$ is a contravariant vector. (Note that because $\\mathbf{T}$ is defined only on the curve $\\mathscr{C}$, we have $\\mathscr{S}=\\mathscr{C}$ for this particular vector field.) We conclude in general that under a change of coordinates, the tangent vector of $a$ smooth curve transforms as a contravariant tensor of order one.\n\nRemark 1: In some treatments of the subject, tensors are defined to possess certain weights, with (3.8) replaced by\n\n\n\\begin{equation*}\n\\text { weighted contravariant vector } \\quad \\bar{T}^{i}=w T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad(1 \\leqq i \\leqq n) \\tag{3.9}\n\\end{equation*}\n\n\nfor some real-valued function $w$ (the \"weight of $\\mathbf{T}$ \").\n\nIn framing the next definition we (arbitrarily) shift to a subscript notation for the components of the vector field.\n\nDefinition 3: The vector field $\\mathbf{V}$ is a covariant tensor of order one (or covariant vector) provided its components $\\left(T_{i}\\right)$ and $\\left(\\bar{T}_{i}\\right)$ relative to an arbitrary pair of coordinate systems $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$, respectively, obey the law of transformation\n\n\n\\begin{equation*}\n\\text { covariant vector } \\quad \\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\quad(1 \\leqq i \\leqq n) \\tag{3.10}\n\\end{equation*}\n\n\nEXAMPLE 3.5 Let $F(\\mathbf{x})$ denote a differentiable scalar field defined in a coordinate system $\\left(x^{i}\\right)$ of $\\mathbf{R}^{n}$. The gradient of $F$ is defined as the vector field\n\n$$\n\\nabla F \\equiv\\left(\\frac{\\partial F}{\\partial x^{1}}, \\frac{\\partial F}{\\partial x^{2}}, \\ldots, \\frac{\\partial F}{\\partial x^{n}}\\right)\n$$\n\nIn a barred coordinate system, the gradient is given by $\\overline{\\nabla F}=\\left(\\partial \\bar{F} / \\partial \\bar{x}^{i}\\right)$, where $\\bar{F}(\\overline{\\mathbf{x}}) \\equiv F \\circ \\mathbf{x}(\\overline{\\mathbf{x}})$. The chain rule for partial derivatives, together with the functional relations (3.6), gives\n\n$$\n\\frac{\\partial \\bar{F}}{\\partial \\bar{x}^{i}}=\\frac{\\partial F}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nwhich is just (3.10) for $T_{i}=\\partial F / \\partial x^{i}, \\bar{T}_{i}=\\partial \\bar{F} / \\partial \\bar{x}^{i}$. Thus, the gradient of an arbitrary differentiable function is $a$ covariant vector.\n\nRemark 2: Tangent vectors and gradient vectors are really two different kinds of vectors. Tensor calculus is vitally concerned with the distinction between contravariance and covariance, and consistently employs upper indices to indicate the one and lower indices to indicate the other.\n\nRemark 3: From this point on, we shall frequently refer to first-order tensors, contravariant or covariant as the case may be, simply as \"vectors\"; they are, of course, actually vector fields, defined on $\\mathbf{R}^{n}$. This usage will coexist with our earlier employment of \"vectors\" to denote real $n$-tuples; i.e., elements of $\\mathbf{R}^{n}$. There is no conflict here insofar as the $n$-tuples make up the vector field corresponding to the identity mapping $V^{i}(\\mathbf{x})=$ $x^{i} \\quad(i=1,2, \\ldots, n)$. But the vector $\\left(x^{i}\\right)$ does not enjoy the transformation property of a tensor; so, to emphasize that fact, we shall sometimes refer to it as a position vector.\n\n\\subsection*{3.3 INVARIANTS}\nObjects, functions, equations, or formulas that are independent of the coordinate system used to express them have intrinsic value and are of fundamental significance; they are called invariants. Roughly speaking, the product of a contravariant vector and a covariant vector always is an invariant. The following is a more precise statement of this fact.\n\nTheorem 3.1: Let $S^{i}$ and $T_{i}$ be the components of a contravariant and covariant vector, respectively. If the inner product $E \\equiv S^{r} T_{r}$ is defined in each coordinate system, then $E$ is an invariant.\n\nEXAMPLE 3.6 In Examples 3.4 and 3.5 it was established that the tangent vector, $\\left(S^{i}\\right)=\\left(d x^{i} / d t\\right)$, to a curve $\\mathscr{C}$ and the gradient of a function, $\\left(T_{i}\\right)=\\left(\\partial F / \\partial x^{i}\\right)$, are contravariant and covariant vectors, respectively. Let us verify Theorem 3.1 for these two vectors. Define\n\n$$\nE=S^{r} T_{r} \\equiv \\frac{\\partial F}{\\partial x^{r}} \\frac{d x^{r}}{d t}\n$$\n\nNow, by the chain rule,\n\n$$\nE=\\frac{d F}{d t}\n$$\n\nso the assertion of Theorem 3.1 is that the value of\n\n$$\n\\frac{d}{d t}\\left[F \\circ\\left(x^{i}(t)\\right)\\right] \\equiv \\frac{d}{d t}[\\hat{F}(t)]\n$$\n\nis independent of the particular coordinate system $\\left(x^{i}\\right)$ used to specify the curve. To visualize this, the reader should study Fig. 3-4, which shows how the composition $\\hat{F}=F^{\\circ}\\left(x^{i}(t)\\right)$ works out in $\\mathbf{R}^{3}$. It is apparent here that the map $\\hat{F}$ entirely bypasses the coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$. Thus, $\\hat{F}$ - and with it, $d \\hat{F} / d t-$ is an invariant with respect to coordinate changes.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-037}\n\\end{center}\n\nFig. 3-4\n\n\\subsection*{3.4 HIGHER-ORDER TENSORS}\nTensors of arbitrary order may be defined. Although most work does not involve tensors of order greater than 4 , the general definition will be included here for completeness. We begin with the three types of second-order tensors.\n\n\\section*{Second-Order Tensors}\nLet $\\mathbf{V}=\\left(V^{i j}\\right)$ denote a matrix field; that is, $\\left(V^{i j}\\right)$ is an $n \\times n$ matrix of scalar fields $V^{i j}(\\mathbf{x})$, all defined over the same region $\\mathscr{U}=\\{\\mathbf{x}\\}$ in $\\mathbf{R}^{n}$. As before, it will be assumed that $\\mathbf{V}$ has a representation $\\left(T^{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}^{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$, where $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ are admissible coordinates related by (3.1) and (3.6).\n\nDefinition 4: The matrix field $\\mathbf{V}$ is a contravariant tensor of order two if its components $\\left(T^{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}^{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { contravariant tensor } \\quad \\bar{T}^{i j}=T^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.11}\n\\end{equation*}\n\n\nAgain going over to subscript notation for the components of the matrix field, we state\n\nDefinition 5: The matrix field $\\mathbf{V}$ is a covariant tensor of order two if its components $\\left(T_{i j}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{i j}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { covariant tensor } \\quad \\bar{T}_{i j}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.12}\n\\end{equation*}\n\n\nTheorem 3.2: Suppose that $\\left(T_{i j}\\right)$ is a covariant tensor of order two. If the matrix $\\left[T_{i j}\\right]_{n n}$ is invertible on $\\mathscr{U}$, with inverse matrix $\\left[T^{i j}\\right]_{n n}$, then $\\left(T^{i j}\\right)$ is a contravariant tensor of order two.\n\nDefinition 6: The matrix field $\\mathbf{V}$ is a mixed tensor of order two, contravariant of order one and covariant of order one, if its components $\\left(T_{j}^{i}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{j}^{i}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\n\n\\begin{equation*}\n\\text { mixed tensor } \\quad \\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad(1 \\leqq i, j \\leqq n) \\tag{3.13}\n\\end{equation*}\n\n\n\\section*{Tensors of Arbitrary Order}\nVector and matrix fields are inadequate for higher-order tensors. It is necessary to introduce a generalized vector field $\\mathbf{V}$, which is an ordered array of $n^{m}(m=p+q)$ scalar fields, $\\left(V_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$, defined over a region $\\mathcal{U}$ in $\\mathbf{R}^{n}$; let $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right.$ ) denote the set of component-functions in various coordinate systems which are defined on $U$.\n\nDefinition 7: The generalized vector field $\\mathbf{V}$ is a tensor of order $m=p+q$, contravariant of order $p$ and covariant of order $q$, if its components $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ in $\\left(x^{i}\\right)$ and $\\left(\\bar{T}_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ in $\\left(\\bar{x}^{i}\\right)$ obey the law of transformation\n\ngeneral tensor $\\bar{T}_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}=T_{s_{1} s_{2} \\ldots s_{q}}^{r_{1} r_{2} \\ldots r_{p}} \\frac{\\partial \\bar{x}^{i_{1}}}{\\partial x^{r_{1}}} \\frac{\\partial \\bar{x}^{i_{2}}}{\\partial x^{r_{2}}} \\cdots \\frac{\\partial \\bar{x}^{i_{p}}}{\\partial \\bar{x}_{p}^{r_{p}}} \\frac{\\partial x^{s_{1}}}{\\partial \\bar{x}^{j_{1}}} \\frac{\\partial x^{s_{2}}}{\\partial \\bar{x}^{j_{2}}} \\cdots \\frac{\\partial x^{s_{q}}}{\\partial \\bar{x}^{j_{q}}}$\n\nwith the obvious range for free indices.\n\n\\subsection*{3.5 THE STRESS TENSOR}\nIt was the concept of stress in mechanics that originally led to the invention of tensors (tenseur, that which exerts tension, stress). Suppose that the unit cube is in equilibrium under forces applied to three of its faces [Fig. 3-5(a)]. Since each face has unit area, each force vector represents the force\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039(1)}\n\\end{center}\n\n(b)\n\nFig. 3-5\n\nper unit area, or stress. Those forces are represented in the component form in Fig. 3-5(b). Using the standard basis $\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{2}$, we have\n\n\\[\n\\begin{array}{ll}\n\\mathbf{v}_{1}=\\sigma^{1 s} \\mathbf{e}_{s} & (\\text { stress on face } 1) \\\\\n\\mathbf{v}_{2}=\\sigma^{2 s} \\mathbf{e}_{s} & (\\text { stress on face } 2)  \\tag{3.15}\\\\\n\\mathbf{v}_{3}=\\sigma^{3 s} \\mathbf{e}_{s} & (\\text { stress on face } 3)\n\\end{array}\n\\]\n\n\\section*{Stress on a Cube Section}\nThe question arises: What stress $\\mathbf{F}$ is transmitted to a planar cross section of the cube that has unit normal $\\mathbf{n}$ ? To answer this, refer to Fig. 3-6, which shows the tetrahedron formed by the cross section and the coordinate planes. Let $A$ be the cross-sectional area. By the assumed equilibrium of the cube, the stresses on the $x^{1} x^{2}-, x^{1} x^{3}$, and $x^{2} x^{3}$-bases of the tetrahedron are $-\\mathbf{v}_{3},-\\mathbf{v}_{2}$, and $-\\mathbf{v}_{1}$, respectively, as shown componentwise in Fig. 3-6. Hence, the forces on these same bases are $B_{1}\\left(-\\mathbf{v}_{3}\\right), B_{2}\\left(-\\mathbf{v}_{2}\\right)$, and $B_{3}\\left(-\\mathbf{v}_{1}\\right)$, respectively. For the tetrahedron itself to be in equilibrium, the resultant force on it must vanish:\n\n$$\nA \\mathbf{F}+B_{1}\\left(-\\mathbf{v}_{3}\\right)+B_{2}\\left(-\\mathbf{v}_{2}\\right)+B_{3}\\left(-\\mathbf{v}_{1}\\right)=0\n$$\n\nor, solving for $\\mathbf{F}$,\n\n\n\\begin{equation*}\n\\mathbf{F}=\\frac{B_{3}}{A} \\mathbf{v}_{1}+\\frac{B_{2}}{A} \\mathbf{v}_{2}+\\frac{B_{1}}{A} \\mathbf{v}_{3} \\tag{3.16}\n\\end{equation*}\n\n\nBut $B_{3}$ is the projection of $A$ in the $x^{2} x^{3}$-plane: $B_{3}=A \\mathbf{n e}_{1}$ or $B_{3} / A=\\mathbf{n e}_{1}$. Similarly, $B_{2} / A=\\mathbf{n e}_{2}$ and $B_{1} / A=\\mathbf{n e}_{3}$. Substituting these expressions and the expressions (3.15) into (3.16), we find that\n\n\n\\begin{equation*}\n\\mathbf{F}=\\sigma^{r s}\\left(\\mathbf{n} \\mathbf{e}_{r}\\right) \\mathbf{e}_{s} \\tag{3.17}\n\\end{equation*}\n\n\n\\section*{Contravariance of Stress}\n\\section*{Under Change of Coordinates}\nAn interesting formula results from (3.17) when we change the basis of $\\mathbf{R}^{3}$ by a transformation of the form $\\mathbf{e}_{i}=a_{i}^{j} \\mathbf{f}_{j}$ (with $\\left|a_{i}^{j}\\right| \\neq 0$ ). In terms of coordinates,\n\n$$\nx^{i} \\mathbf{e}_{i}=x^{i}\\left(a_{i}^{j} \\mathbf{f}_{j}\\right)=\\left(a_{i}^{j} x^{i}\\right) \\mathbf{f}_{j} \\equiv \\bar{x}^{j} \\mathbf{f}_{j}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-040}\n\\end{center}\n\nFig. 3-6\n\nThat is, we have a new coordinate system $\\left(\\vec{x}^{i}\\right)$ that is related to $\\left(x^{i}\\right)$ via\n\n\n\\begin{equation*}\n\\bar{x}^{j}=a_{i}^{j} x^{i} \\tag{3.18}\n\\end{equation*}\n\n\nNote that here we have\n\n$$\n\\frac{\\partial \\bar{x}^{j}}{\\partial x^{i}}=a_{i}^{j}\n$$\n\nSubstituting $\\mathbf{e}_{r}=a_{r}^{i} \\mathbf{f}_{i}$ into (3.17) yields the stress components $\\left(\\bar{\\sigma}^{i j}\\right)$ in the new coordinate system, as follows:\n\nwith\n\n\n\\begin{gather*}\n\\mathbf{F}=\\sigma^{r s}\\left[\\mathbf{n}\\left(a_{r}^{i} \\mathbf{f}_{i}\\right)\\right]\\left(a_{s}^{j} \\mathbf{f}_{j}\\right)=\\sigma^{r s} a_{r}^{i} a_{s}^{j}\\left(\\mathbf{n f}_{i}\\right) \\mathbf{f}_{j} \\equiv \\bar{\\sigma}^{i j}\\left(\\mathbf{n f}_{i}\\right) \\mathbf{f}_{j} \\\\\n\\bar{\\sigma}^{i j}=\\sigma^{r s} a_{r}^{i} a_{s}^{j}=\\sigma^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\tag{3.19}\n\\end{gather*}\n\n\nA comparison of (3.19) with the transformation law (3.11) leads to the conclusion that the stress components $\\sigma^{i j}$ define a second-order contravariant tensor, at least for linear coordinate changes.\n\n\\subsection*{3.6 CARTESIAN TENSORS}\nTensors corresponding to admissible linear coordinate changes, $\\mathscr{T}: \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$, are called affine tensors. If ( $a_{j}^{i}$ ) is orthogonal (and $\\mathscr{T}$ is distance-preserving), the corresponding tensors are cartesian tensors. Now, an object that is a tensor with respect to all one-one linear transformations is necessarily a tensor with respect to all orthogonal linear transformations, but the converse is not true. Hence, affine tensors are special cartesian tensors. Likewise, affine invariants are particular cartesian invariants.\n\n\\section*{Affine Tensors}\nA transformation of the form $\\mathscr{T}: \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$ takes a rectangular coordinate system $\\left(x^{i}\\right)$ into a system $\\left(\\bar{x}^{i}\\right)$ having oblique axes; thus affine tensors are defined on the class of all such\\\\\noblique coordinate systems. Since the Jacobian matrices of $\\mathscr{T}$ and $\\mathscr{T}^{-1}$ are\n\n\n\\begin{equation*}\nJ=\\left[\\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right]_{n n}=\\left[a_{j}^{i}\\right]_{n n} \\quad \\text { and } \\quad J^{-1}=\\left[\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right]_{n n} \\equiv\\left[b_{j}^{i}\\right]_{n n} \\tag{3.20}\n\\end{equation*}\n\n\nthe transformation laws for affine tensors are:\n\n\\[\n\\begin{array}{rll}\n\\text { contravariant } & \\bar{T}^{i}=a_{r}^{i} T^{r}, \\quad \\bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, \\quad \\bar{T}^{i j k}=a_{r}^{i} a_{s}^{j} a_{t}^{k} T^{r s t}, \\ldots \\\\\n\\text { covariant } & \\bar{T}_{i}=b_{i}^{r} T_{r}, \\quad \\bar{T}_{i j}=b_{i}^{r} b_{j}^{s} T_{r s}, \\quad \\bar{T}_{i j k}=b_{i}^{r} b_{j}^{s} b_{k}^{t} T_{r s t}, \\quad \\ldots  \\tag{3.21}\\\\\n\\text { mixed } & \\bar{T}_{j}^{i}=a_{r}^{i} b_{j}^{s} T_{s}^{r}, \\quad \\bar{T}_{j k}^{i}=a_{r}^{i} b_{j}^{s} b_{k}^{t} T_{s t}^{r}, \\ldots\n\\end{array}\n\\]\n\nUnder the less stringent conditions (3.21), more objects can qualify as tensors than before; for instance, an ordinary position vector $\\mathbf{x}=\\left(x^{i}\\right)$ becomes an (affine) tensor (see Problem 3.9), and the partial derivatives of a tensor define an (affine) tensor (as implied by Problem 2.23).\n\n\\section*{Cartesian Tensors}\nWhen the above linear transformation $\\mathscr{T}$ is restricted to be orthogonal, then $J^{-1}=J^{T}$, or\n\n$$\nb_{j}^{i}=a_{i}^{j} \\quad(1 \\leqq i, j \\leqq n)\n$$\n\nso that the transformation laws for cartesian tensors are, from (3.21),\n\n$$\n\\begin{array}{rlll}\n\\text { contravariant } & \\bar{T}^{i}=a_{r}^{i} T^{r}, & \\bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, & \\ldots \\\\\n\\text { covariant } & \\bar{T}_{i}=a_{r}^{i} T_{r}, & \\bar{T}_{i j}=a_{r}^{i} a_{s}^{j} T_{r s}, & \\ldots \\\\\n\\text { mixed } & & \\bar{T}_{j}^{i}=a_{r}^{i} a_{s}^{j} T_{s}^{r}, & \\ldots\n\\end{array}\n$$\n\nA striking feature of these forms is that contravariant and covariant behaviors do not distinguish themselves. Consequently, all cartesian tensors are notated the same way-with subscripts:\n\n\n\\begin{align*}\n\\begin{array}{r}\n\\text { allowable } \\\\\n\\text { coordinate changes }\n\\end{array} & \\bar{x}_{i}=a_{i j} x_{j} \\quad \\text { or } \\quad x_{i}=a_{j i} \\bar{x}_{j} \\\\\n\\begin{array}{r}\n\\text { cartesian } \\\\\n\\text { tensor laws }\n\\end{array} & \\bar{T}_{i}=a_{i r} T_{r}, \\quad \\bar{T}_{i j}=a_{i r} a_{j s} T_{r s}, \\ldots \\tag{3.22}\n\\end{align*}\n\n\nBecause an orthogonal transformation takes one rectangular coordinate system into another (having the same origin), cartesian tensors appertain to the rectangular (cartesian) coordinate systems. There are, of course, even more cartesian tensors than affine tensors.\n\nNote that $J J^{T}=I$ implies $\\mathscr{J}^{2}=1$, or $\\mathscr{J}= \\pm 1$. Objects that obey the tensor laws (3.22) when the allowable coordinate changes are such that\n\n$$\n\\mathscr{J}=\\left|a_{i j}\\right|=+1\n$$\n\nare called direct cartesian tensors.\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{CHANGE OF COORDINATES}\n3.1 For the transformation of Example 3.2, (a) obtain the equations for $\\mathscr{T}^{-1} ;(b)$ compute $\\bar{J}$ from (a), and compare with $J^{-1}$.\n\n(a) Solving $\\bar{x}^{1}=x^{1} x^{2}, \\bar{x}^{2}=\\left(x^{2}\\right)^{2}$ for $x^{1}$ and $x^{2}$, we find that\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} x^{2}  \\tag{1}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis a one-one mapping between the regions $x^{2}>0$ and $\\bar{x}^{2}>0$, and that\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{\\prime} x^{2}  \\tag{2}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=-\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=-\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis one-one between $x^{2}<0$ and $\\bar{x}^{2}>0$. Note that the two regions of the $x^{1} x^{2}$-plane are separated by the line on which the Jacobian of $\\mathscr{T}$ vanishes.\n\n(b) From Example 3.2,\n\n$$\nJ=\\left[\\begin{array}{cc}\nx^{2} & x^{1} \\\\\n0 & 2 x^{2}\n\\end{array}\\right] \\quad \\text { and so } \\quad J^{-1}=\\frac{1}{2\\left(x^{2}\\right)^{2}}\\left[\\begin{array}{cc}\n2 x^{2} & -x^{1} \\\\\n0 & x^{2}\n\\end{array}\\right]\n$$\n\nvalid in both regions $x^{2}>0$ and $x^{2}<0$. Now, on $\\bar{x}^{2}>0$, differentiation of the inverse transformation (1), followed by a change back to unbarred coordinates, yields\n\n$$\n\\bar{J}=\\left[\\begin{array}{ll}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(\\bar{x}^{2}\\right)^{-1 / 2} & -\\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & \\frac{1}{2}\\left(\\bar{x}^{2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & \\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\n$$\n\nIt is seen that on $x^{2}>0, \\bar{J}=J^{-1}$.\n\nSimilarly, from (2), with $x^{2}<0$,\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n-\\left(\\bar{x}^{2}\\right)^{-1 / 2} & \\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & -\\frac{1}{2}\\left(\\bar{x}^{-2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n+\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & +\\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=J^{-1}\n$$\n\n3.2 For polar coordinates as defined by (3.2), (a) calculate the Jacobian matrix of $\\mathscr{T}$ and infer the region over which $\\mathscr{T}$ is bijective; $(b)$ calculate the Jacobian matrix of $\\mathscr{T}^{-1}$ for the region\n\n$$\n\\{(r, \\theta) \\mid r>0,-\\pi / 2<\\theta<\\pi / 2\\}\n$$\n\ni.e., the right half-plane, and verify that it is the inverse of the matrix of $(a)$.\n\n(a)\n\n$$\nJ=\\left[\\begin{array}{cc}\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\cos x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\cos x^{2}\\right) \\\\\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\sin x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\sin x^{2}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\cos x^{2} & -x^{1} \\sin x^{2} \\\\\n\\sin x^{2} & x^{1} \\cos x^{2}\n\\end{array}\\right]\n$$\n\nwhence $\\mathscr{J}=x^{1} \\equiv r$. Therefore, $\\mathscr{T}$ is bijective on the open set $r>0$, which is the entire plane punctured at the origin.\n\n(b) For $\\mathscr{T}^{-1}$ we have, over the right half-plane,\n\n$$\n\\begin{gathered}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}}=\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\quad \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}}=\\frac{1}{1+\\left(\\bar{x}^{2} / \\bar{x}^{1}\\right)^{2}}\\left[-\\frac{\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}}\\right]=\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\quad \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{gathered}\n$$\n\nand so\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} & \\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} & \\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]\n$$\n\nNow compute $J^{-1}$ :\n\n$$\nJ^{-1}=\\frac{1}{x^{1}}\\left[\\begin{array}{cc}\nx^{1} \\cos x^{2} & x^{1} \\sin x^{2} \\\\\n-\\sin x^{2} & \\cos x^{2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]=\\bar{J}\n$$\n\n\\section*{CONTRAVARIANT VECTORS}\n3.3 If $\\mathbf{V}=\\left(T^{i}\\right)$ is a contravariant vector, show that the partial derivatives $T_{j}^{i} \\equiv \\partial T^{i} / \\partial x^{j}$, defined in each coordinate system, transform according to the rule\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{r} \\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nDifferentiate both sides of\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $\\bar{x}^{j}$, using the product rule:\n\n\n\\begin{equation*}\n\\bar{T}_{j}^{i} \\equiv \\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{j}}=\\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy the chain rule for partial derivatives, $(2.15)$,\n\n$$\n\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\equiv T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { and } \\quad \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left[\\frac{\\partial}{\\partial x^{s}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\right] \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nSubstituting these expressions into (1) yields the desired formula.\n\n3.4 Suppose that $\\left(T^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{2}$ and that $\\left(T^{i}\\right)=\\left(x^{2}, x^{1}\\right)$ in the $\\left(x^{i}\\right)$-system.\n\nCalculate $\\left(\\bar{T}^{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system, under the change of coordinates\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{2}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\nBy definition of contravariance,\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}=T^{1} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{1}}+T^{2} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{2}}\n$$\n\nNote that the top row of the Jacobian matrix $J$ is needed for the case $i=1$, and the bottom row is needed for $i=2$.\n\n$$\nJ=\\left[\\begin{array}{ll}\n\\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} \\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 2 x^{2} \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\n$$\n\nThus,\n\n$$\n\\bar{T}^{1}=T^{1}(0)+T^{2}\\left(2 x^{2}\\right)=2 x^{1} x^{2} \\quad \\bar{T}^{2}=T^{1}\\left(x^{2}\\right)+T^{2}\\left(x^{1}\\right)=\\left(x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\n$$\n\nwhich, in terms of barred coordinates, are\n\n$$\n\\bar{T}^{1}=2 \\bar{x}^{2} \\quad \\bar{T}^{2}=\\bar{x}^{1}+\\frac{\\left(\\bar{x}^{2}\\right)^{2}}{\\bar{x}^{1}}\n$$\n\n3.5 Show that a contravariant vector can be constructed the components of which take on a given set of values $(a, b, c, \\ldots)$ in some particular coordinate system. (The prescribed values may be point functions.)\n\nLet $(a, b, c, \\ldots) \\equiv\\left(a^{i}\\right)$ be the given values to be assigned in the coordinate system $\\left(x^{i}\\right)$. Set $V^{i}=a^{i}$ for the values in $\\left(x^{i}\\right)$, and for any other admissible coordinate system $\\left(\\bar{x}^{i}\\right)$, set $\\bar{V}^{i}=a^{r}\\left(\\partial \\bar{x}^{i} / \\partial x^{r}\\right)$. To show that $\\left(V^{i}\\right)$ is a contravariant tensor, let $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ be any two admissible coordinate systems. Then, $y^{i}=f^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ and $\\bar{y}^{i}=g^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$, and, by definition, the values of $\\left(V^{i}\\right)$ in $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ are, respectively, $T^{i}=a^{r}\\left(\\partial y^{i} / \\partial x^{r}\\right)$ and $\\bar{T}^{i}=a^{r}\\left(\\partial \\bar{y}^{i} / \\partial x^{r}\\right)$. But, by the chain rule,\n\n$$\n\\bar{T}^{i}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial x^{r}}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\frac{\\partial y^{s}}{\\partial x^{r}}=T^{s} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\quad \\text { QED }\n$$\n\n\\section*{COVARIANT VECTORS}\n3.6 Calculate $\\left(\\bar{T}_{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system if $\\mathbf{V}=\\left(T_{i}\\right) \\equiv\\left(x^{2}, x^{1}+2 x^{2}\\right)$ is a covariant vector under the coordinate transformation of Problem 3.4.\n\nTo avoid radicals, compute $J^{-1}$ in terms of $\\left(x^{i}\\right)$ :\n\n$$\nJ^{-1}=\\left[\\begin{array}{cc}\n\\frac{-x^{1}}{2\\left(x^{2}\\right)^{2}} & \\frac{1}{x^{2}} \\\\\n\\frac{1}{2 x^{2}} & 0\n\\end{array}\\right]\n$$\n\nBy covariance,\n\n$$\n\\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}=T_{1} \\frac{\\partial x^{1}}{\\partial \\bar{x}^{i}}+T_{2} \\frac{\\partial x^{2}}{\\partial \\bar{x}^{i}} \\quad(i=1,2)\n$$\n\nFor $i=1$, read off the partials from the first column of $J^{-1}$ :\n\n$$\n\\bar{T}_{1}=T_{1}\\left(-x^{1} / 2\\left(x^{2}\\right)^{2}\\right)+T_{2}\\left(1 / 2 x^{2}\\right)=-x^{1} / 2 x^{2}+x^{1} / 2 x^{2}+1=1\n$$\n\nSimilarly, for $i=2$, use the second column of $J^{-1}$ :\n\n$$\n\\bar{T}_{2}=T_{1}\\left(1 / x^{2}\\right)+T_{2}(0)=x^{2}\\left(1 / x^{2}\\right)=1\n$$\n\nHence, $\\left(\\bar{T}_{i}\\right)=(1,1)$ at all points in the $\\left(\\bar{x}^{i}\\right)$-system $\\left(\\bar{x}^{1}=0\\right.$ excluded $)$.\n\n3.7 Use the fact that $\\nabla f$ is a covariant vector (Example 3.5) to bring the partial differential equation\n\n\n\\begin{equation*}\nx \\frac{\\partial f}{\\partial x}=y \\frac{\\partial f}{\\partial y} \\tag{1}\n\\end{equation*}\n\n\ninto simpler form by the change of variables $\\bar{x}=x y, \\bar{y}=(y)^{2}$; then solve.\n\nWrite $\\nabla f=(\\partial f / \\partial x, \\partial f / \\partial y) \\equiv\\left(T_{i}\\right),\\left(x^{1}, x^{2}\\right)=(x, y),\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)=(\\bar{x}, \\bar{y})$, and\n\n$$\n\\bar{T}_{i} \\equiv \\frac{\\partial \\bar{f}}{\\partial \\bar{x}^{i}}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nAgain calculating $J$ first, then its inverse, we have\n\nso that\n\n$$\n\\left(\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right) \\equiv J^{-1}=\\left[\\begin{array}{cc}\ny & x \\\\\n0 & 2 y\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{cc}\n\\frac{1}{y} & \\frac{-x}{2(y)^{2}} \\\\\n0 & \\frac{1}{2 y}\n\\end{array}\\right]\n$$\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{x}} \\equiv \\bar{T}_{1}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{1}}=T_{1} \\cdot \\frac{1}{y}+T_{2} \\cdot 0=\\frac{1}{y} \\frac{\\partial f}{\\partial x} \\\\\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{y}} \\equiv \\bar{T}_{2}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{2}}=T_{1} \\cdot \\frac{-x}{2(y)^{2}}+T_{2} \\cdot \\frac{1}{2 y}=-\\frac{x}{2(y)^{2}} \\frac{\\partial f}{\\partial x}+\\frac{1}{2 y} \\frac{\\partial f}{\\partial y}\n\\end{aligned}\n$$\n\nBut, by (1),\n\n$$\n\\frac{\\partial \\bar{f}}{\\partial \\bar{y}}=\\frac{1}{2(y)^{2}}\\left(-x \\frac{\\partial f}{\\partial x}+y \\frac{\\partial f}{\\partial y}\\right)=0\n$$\n\nwhich implies that $\\bar{f}=F(\\bar{x})$, a function of $\\bar{x}$ alone; therefore, $f=F(x y)$ is the general solution to (1).\n\n\\section*{INVARIANTS}\n\\subsection*{3.8 Prove Theorem 3.1.}\nWe must show that if $\\left(S^{i}\\right)$ and $\\left(T_{i}\\right)$ are tensors of the indicated types and order, then the quantity $E \\equiv S^{i} T_{i}$ is invariant with respect to coordinate changes; that is, $\\bar{E}=E$, where $\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}$. But observe that\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}_{i}=T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}\n$$\n\nso that, in view of $(3.7)$,\n\n$$\n\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\delta_{r}^{s}=S^{r} T_{r}=E\n$$\n\n3.9 Show that under linear coordinate changes of $\\mathbf{R}^{n}, \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$, the equation of a hyperplane $A_{i} x^{i}=1$ is invariant provided the normal vector $\\left(A_{i}\\right)$ is covariant.\n\nIn view of Theorem 3.1, it suffices to show that $\\left(T^{i}\\right)=\\left(x^{i}\\right)$ is a contravariant affine tensor. But this is immediate:\n\n$$\n\\bar{T}^{i} \\equiv \\bar{x}^{i}=a_{j}^{i} x^{j} \\equiv a_{j}^{i} T^{j}\n$$\n\nwhich is the transformation law (3.21).\n\n\\section*{SECOND-ORDER CONTRAVARIANT TENSORS}\n3.10 Suppose that the components of a contravariant tensor $T$ of order 2 in a coordinate system $\\left(x^{i}\\right)$ of $\\mathbf{R}^{2}$ are $T^{11}=1, T^{12}=1, T^{21}=-1$, and $T^{22}=2$. (a) Find the components $\\bar{T}^{j j}$ of $\\mathbf{T}$ in the $\\left(\\bar{x}^{i}\\right)$-system, connected to the $\\left(x^{i}\\right)$-system via\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{1}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\n(b) Compute the values of the $\\bar{T}^{i j}$ at the point which corresponds to $x^{1}=1, x^{2}=-2$.\n\nFor economy of effort, the problem will be worked using matrices.\n\n(a) Writing\n\n$$\nJ_{j}^{i} \\equiv J_{i}^{\\prime j} \\equiv \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\n$$\n\nwe have from $(2.1 b)$,\n\n$$\n\\bar{T}^{i j}=T^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}}=J_{r}^{i} T^{r s} J_{j}^{s}\n$$\n\nThat is,\n\n$$\n\\begin{aligned}\n\\bar{T} & =J T J^{T} \\\\\n& =\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\n2 x^{1} & x^{2} \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n(b) At the point $(1,-2)$,\n\n$$\n\\begin{array}{ll}\n\\bar{T}^{11}=4(1)^{2}=4 & \\bar{T}^{12}=2(1)(-2)+2(1)^{2}=-2 \\\\\n\\bar{T}^{21}=2(1)(-2)-2(1)^{2}=-6 & \\bar{T}^{22}=2(1)^{2}+(-2)^{2}=6\n\\end{array}\n$$\n\n3.11 Show that if $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ are contravariant vectors on $\\mathbf{R}^{n}$, the matrix $\\left[U^{i j}\\right] \\equiv\\left[S^{i} T^{j}\\right]_{n n}$, defined in this manner for all coordinate systems, represents a contravariant tensor of order 2.\n\nMultiply\n\nto obtain\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}^{j}=T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\n$$\n\\bar{U}^{i j}=\\bar{S}^{i} \\bar{T}^{j}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}=U^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the tensor law. (The notion of the \"outer product\" of two tensors will be further developed in Chapter 4.)\n\n\\section*{SECOND-ORDER COVARIANT TENSORS}\n3.12 Show that if $T_{i}$ are the components of covariant vector $\\mathbf{T}$, then $S_{i j} \\equiv T_{i} T_{j}-T_{j} T_{i}$ are the components of a skew-symmetric covariant tensor $\\mathbf{S}$.\n\nThe skew-symmetry is obvious. From the transformation law for $\\mathbf{T}$,\n\nor\n\n$$\n\\begin{gathered}\n\\bar{T}_{i} \\bar{T}_{j}-\\bar{T}_{j} \\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\cdot T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\\\\n=T_{r} T_{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\left(T_{r} T_{s}-T_{s} T_{r}\\right) \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\\\\n\\bar{S}_{i j}=S_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n\\end{gathered}\n$$\n\nwhich establishes the covariant tensor character of $\\mathbf{S}$.\n\n3.13 If a symmetric array $\\left(T_{i j}\\right)$ transforms according to\n\n$$\n\\bar{T}_{i j}=T_{r t} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}}\n$$\n\nshow that it defines a second-order covariant tensor.\n\n$$\n\\begin{aligned}\n\\bar{T}_{i j} & =T_{r t}\\left(\\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{s}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{r t} \\delta_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\\\\n& =T_{s t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{t s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n\\end{aligned}\n$$\n\n3.14 Let $\\mathbf{U}=\\left(U_{i j}\\right)$ be a covariant tensor of order 2 . Under the same coordinate change as in Problem 3.10, (a) calculate the components $\\bar{U}_{i j}$, if $U_{11}=x^{2}, U_{12}=U_{21}=0, U_{22}=x^{1} ;(b)$ verify that the quantity $T^{i j} U_{i j}=E$ is an invariant, where the $T^{i j}$ and $\\bar{T}^{i j}$ are obtained from Problem 3.10.\n\n(a) In terms of the inverse Jacobian matrix, the covariant transformation law is\n\n$$\n\\bar{U}_{i j}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} U_{r s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\bar{J}_{i}^{r} U_{r s} \\bar{J}_{j}^{s}=\\bar{J}_{r}^{\\prime i} U_{r s} \\bar{J}_{j}^{s} \\quad \\text { or } \\quad \\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nSubstituting\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\quad U=\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\n$$\n\nwe find\n\n$$\n\\bar{U}=\\left[\\begin{array}{cc}\n\\frac{1}{2 x^{1}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n0 & \\frac{1}{x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]\n$$\n\nfrom which the $\\bar{U}_{i j}$ may be read off.\n\n(b) Continuing in the matrix approach, we note that $E$ is the trace (sum of diagonal elements) of the matrix $T U^{T}$.\n\n$$\n\\begin{aligned}\n& T U^{T}=\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{rr}\nx^{2} & x^{1} \\\\\n-x^{2} & 2 x^{1}\n\\end{array}\\right] \\\\\n& E=x^{2}+2 x^{1}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\bar{T} \\bar{U}^{T}=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{cc}\n0 & 2 x^{1} \\\\\n-\\frac{3 x^{2}}{2} & x^{2}+2 x^{1}\n\\end{array}\\right] \\\\\n& \\bar{E}=x^{2}+2 x^{1}=E\n\\end{aligned}\n$$\n\n3.15 Prove Theorem 3.2.\n\nObserve first of all that if a covariant matrix (second-order tensor) $U$ has inverse $V$ in unbarred coordinates, then $\\bar{U}$ has inverse $\\bar{V}$ in barred coordinates; i.e., $(\\bar{U})^{-1}=\\overline{U^{-1}}$. Now, by Problem 3.14(a),\n\n$$\n\\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nInverting both sides of this matrix equation, applying Problem 2.13 , and recalling that $J \\bar{J}=I$, we obtain\n\n$$\n\\overline{U^{-1}}=\\bar{J}^{-1} U^{-1}\\left(\\bar{J}^{T}\\right)^{-1}=J U^{-1} J^{T}\n$$\n\nwhich is the contravariant law for $U^{-1}$ [see Problem 3.10(a)].\n\n\\section*{MIXED TENSORS}\n3.16 Compute the formulas for the tensor components $\\left(\\bar{T}_{j}^{i}\\right)$ in polar coordinates in terms of $\\left(T_{j}^{i}\\right)$ in rectangular coordinates, if the tensor is symmetric in rectangular coordinates. (In contrast to Section 3.1, it is now the curvilinear coordinates that are barred.)\n\nThe general formula calls for the calculations\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad\\left(T_{j}^{i}=T_{i}^{j}\\right)\n$$\n\nUsing $(2.1 b)$, this may be written in matrix form as\n\n\n\\begin{equation*}\n\\bar{T}=J T J^{-1}=\\bar{J}^{-1} T \\bar{J} \\tag{1}\n\\end{equation*}\n\n\nwhere $T=\\left[T_{j}^{i}\\right]_{22}$ and where\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right]\n$$\n\nis the Jacobian matrix of the transformation from $(r, \\theta)$ to $(x, y)$. Thus,\n\n$$\n\\begin{aligned}\n& \\bar{T}=\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} & T_{2}^{1} \\\\\nT_{2}^{1} & T_{2}^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos \\theta+T_{2}^{1} \\sin \\theta & -r T_{1}^{1} \\sin \\theta+r T_{2}^{1} \\cos \\theta \\\\\nT_{2}^{1} \\cos \\theta+T_{2}^{2} \\sin \\theta & -r T_{2}^{1} \\sin \\theta+r T_{2}^{2} \\cos \\theta\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nThe final matrix multiplication can be carried out routinely, simplifying by means of trigonometric identities:\n\n$$\n\\bar{T}=\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos ^{2} \\theta+T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\sin ^{2} \\theta & -\\frac{r}{2} T_{1}^{1} \\sin 2 \\theta+r T_{2}^{1} \\cos 2 \\theta+\\frac{r}{2} T_{2}^{2} \\sin 2 \\theta \\\\\n-T_{1}^{1} \\frac{\\sin 2 \\theta}{2 r}+T_{2}^{1} \\frac{\\cos 2 \\theta}{r}+T_{2}^{2} \\frac{\\sin 2 \\theta}{2 r} & T_{1}^{1} \\sin ^{2} \\theta-T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\cos ^{2} \\theta\n\\end{array}\\right]\n$$\n\nObserve that $\\bar{T}$ does not share the symmetry of $T: \\quad \\bar{T}_{1}^{2}=r^{-2} \\bar{T}_{2}^{1}$.\n\n3.17 Prove that the determinant of a mixed tensor of order two is invariant.\n\nBy (1) of Problem 3.16, we have-whether or not $T$ is symmetric-\n\n$$\n|\\bar{T}|=\\left|J T J^{-1}\\right|=|J||T|\\left|J^{-1}\\right|=\\mathscr{J}|T| \\mathscr{J}^{-1}=|T|\n$$\n\n\\section*{GENERAL TENSORS}\n3.18 Display the transformation law for a third-order tensor that is contravariant of order two and covariant of order one.\n\nTake $p=2$ and $q=1$ in Definition 7 and, to avoid unnecessary subscripts, write $i, j, k, r, s, t$ in place of $i_{1}, i_{2}, j_{1}, r_{1}, r_{2}, s_{1}$. Then (3.14) gives\n\n$$\n\\bar{T}_{k}^{i j}=T_{t}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\n3.19 Let $\\mathbf{T}=\\left(T_{k l m}^{i j}\\right)$ denote a tensor of the order and type indicated by the indices. Prove that $\\mathbf{S}=\\left(T_{k}\\right) \\equiv\\left(T_{k i j}^{i j}\\right)$ is a covariant vector.\n\nThe transformation law (3.14) for $\\mathbf{T}$ is\n\n$$\n\\bar{T}_{k l m}^{i j}=T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{l}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{m}}\n$$\n\nSet $l=i, m=j$ and sum:\n\n$$\n\\begin{aligned}\n\\bar{T}_{k} \\equiv \\bar{T}_{k i j}^{i j} & =T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}=T_{t u v}^{r s}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}}\\right)\\left(\\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}\\right) \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{t u v}^{r s} \\delta_{r}^{u} \\delta_{s}^{v} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{t r s}^{r s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\equiv T_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\n\\section*{CARTESIAN TENSORS}\n3.20 Show that the permutation symbol $\\left(e_{i j}\\right)$ defines a direct cartesian tensor over $\\mathbf{R}^{2}$. Assume that $e_{i j}$ is defined the same way for all rectangular coordinate systems.\n\nIf the coordinate change is $\\bar{x}_{i}=a_{i j} x_{j}$, where $\\left(a_{i j}\\right)^{T}\\left(a_{k l}\\right)=\\left(\\delta_{p q}\\right)$ and\n\n$$\n\\left|a_{i j}\\right|=a_{11} a_{22}-a_{12} a_{21}=1\n$$\n\nwe must establish the cartesian tensor law (3.22):\n\n$$\n\\bar{e}_{i j}=e_{r s} a_{i r} a_{j s} \\quad(n=2)\n$$\n\nWe examine separately the four possible cases:\n\n$$\n\\begin{aligned}\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{1 r} a_{1 s}=a_{11} a_{12}-a_{12} a_{11}=0=\\bar{e}_{11} \\\\\n\\boldsymbol{i}=\\mathbf{1}, \\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{1 r} a_{2 s}=a_{11} a_{22}-a_{12} a_{21}=1=\\bar{e}_{12} \\\\\n\\boldsymbol{i}=\\mathbf{2}, \\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{2 r} a_{1 s}=a_{21} a_{12}-a_{22} a_{11}=-1=\\bar{e}_{21} \\\\\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{2 r} a_{2 s}=a_{21} a_{22}-a_{22} a_{21}=0=\\bar{e}_{22}\n\\end{aligned}\n$$\n\n3.21 Prove that (a) the coefficients $c_{i j}$ of the quadratic form $c_{i j} x^{i} x^{j}=1$ transform as an affine tensor and $(b)$ the trace $c_{i i}$ of $\\left(c_{i j}\\right)$ is a cartesian invariant.\n\n(a) If $\\bar{x}^{i}=a_{j}^{i} x^{j}$ and $x^{i}=b_{j}^{i} \\bar{x}^{j}$, where $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{-1}$, the quadratic form goes over into\n\n$$\n1=c_{i j}\\left(b_{r}^{i} \\bar{x}^{r}\\right)\\left(b_{s}^{j} \\bar{x}^{s}\\right) \\equiv \\bar{c}_{r s} \\bar{x}^{r} \\bar{x}^{s}\n$$\n\nwith $\\bar{c}_{r s}=b_{r}^{i} b_{s}^{j} c_{i j}$. But this formula is just (3.21) for a covariant affine tensor of order two.\n\n(b) Assuming an orthogonal transformation, $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{T}$, we have\n\n$$\n\\bar{c}_{r s}=b_{r}^{i} a_{j}^{s} c_{i j}\n$$\n\nHence, $\\bar{c}_{r r}=\\left(b_{r}^{i} a_{j}^{r}\\right) c_{i j}=\\delta_{j}^{i} c_{i j}=c_{i i}$.\n\n3.22 Establish the identity between the permutation symbol and the Kronecker delta:\n\n\n\\begin{equation*}\ne_{r i j} e_{r k l} \\equiv \\delta_{i k} \\delta_{j l}-\\delta_{i l} \\delta_{j k} \\tag{3.23}\n\\end{equation*}\n\n\nThe identity implies $n=3$, so that there are potentially $3^{4}=81$ separate cases to consider. However, this number can be quickly reduced to only 4 cases by the following reasoning: If either $i=j$ or $k=l$, then both sides vanish. For example, if $i=j$, then on the left $e_{r i j}=0$, and on the right,\n\n$$\n\\delta_{i k} \\delta_{j t}-\\delta_{j l} \\delta_{i k}=0\n$$\n\nHence, we need only consider the cases in which both $i \\neq j$ and $k \\neq l$. Upon writing out the sum on the left, two of the terms drop out, since $i \\neq j$ :\n\n$$\ne_{1 i j} e_{1 k l}+e_{2 i j} e_{2 k l}+e_{3 i j} e_{3 k l}=e_{1^{\\prime} 2^{\\prime} 3^{\\prime}} e_{1^{\\prime} k l} \\quad\\left(i=2^{\\prime}, j=3^{\\prime}\\right)\n$$\n\nwhere $\\left(1^{\\prime} 2^{\\prime} 3^{\\prime}\\right)$ denotes some permutation of (123). Thus, there are left only two cases, each with two subcases.\n\nCase 1: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l} \\neq 0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Here, either $k=2^{\\prime}$ and $l=3^{\\prime}$ or $k=3^{\\prime}$ and $l=2^{\\prime}$. If the former, then the left member of (3.23) is +1 , while the right member equals\n\n$$\n\\delta_{2^{\\prime} 2^{\\prime}} \\delta_{3^{\\prime} 3^{\\prime}}-\\delta_{2^{\\prime} 3^{\\prime}} \\delta_{3^{\\prime} 2^{\\prime}}=1-0=1\n$$\n\nIf the latter, then both members equal -1 , as can be easily verified.\n\nCase 2: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l}=0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Since $k \\neq l$, either $k=1^{\\prime}$ or $l=1^{\\prime}$. If $k=1^{\\prime}$, then the right member of (3.23) equals\n\n$$\n\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} l}-\\delta_{2^{\\prime} l^{\\prime}} \\delta_{3^{\\prime} 1^{\\prime}}=0-0=0\n$$\n\nIf $l=1^{\\prime}$, we have $\\delta_{2^{\\prime} k} \\delta_{3^{\\prime} 1^{\\prime}}-\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} k}=0-0=0$.\n\nThis completes the examination of all cases, and the identity is established.\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n3.23 Suppose that the following transformation connects the $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ coordinate systems:\n\n$$\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\\\\n\\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n\\end{array}\\right.\n$$\n\n(a) Calculate the Jacobian matrix $J$ and the Jacobian $\\mathscr{J}$. Show that $\\mathscr{J} \\neq 0$ over all of $\\mathbf{R}^{2}$. (b) Give equations for $\\mathscr{T}^{-1}$. (c) Calculate the Jacobian matrix $\\bar{J}$ of $\\mathscr{T}^{-1}$ and compare with $J^{-1}$.\n\n3.24 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector, and if the components $S_{i j} \\equiv T_{i} T_{j}+T_{j} T_{i}$ are defined in each coordinate system, then $\\left(S_{i j}\\right)$ is a symmetric covariant tensor. (Compare Problem 3.12.)\n\n3.25 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector and, in each coordinate system, we define\n\n$$\n\\frac{\\partial T_{i}}{\\partial x^{j}}-\\frac{\\partial T_{j}}{\\partial x^{i}}=T_{i j}\n$$\n\nthen $\\left(T_{i j}\\right)$ is a skew-symmetric covariant tensor of the second order. [Hint: Model the proof on Problem 3.3.]\n\n3.26 Convert the partial differential equation\n\n$$\ny \\frac{\\partial f}{\\partial x}=x \\frac{\\partial f}{\\partial y}\n$$\n\nto polar form (making use of the fact that $\\nabla f$ is a covariant vector), and solve for $f(x, y)$.\n\n3.27 Show that the quadratic form $Q=g_{i j} x^{i} x^{j}$ is an affine invariant provided $\\left(g_{i j}\\right)$ is a covariant affine tensor. [Converse of Problem 3.21(a).]\n\n3.28 Prove that the partial derivatives of a contravariant vector $\\left(T^{i}\\right)$ define a mixed affine tensor of order two. [Hint: Compare Problem 2.23.]\n\n3.29 Prove that the Kronecker delta $\\left(\\delta_{j}^{i}\\right)$, uniformly defined in all coordinate systems, is a mixed tensor of order two.\n\n3.30 Show that the permutation symbol $\\left(e_{i j}\\right)$ of order two, uniformly defined in all coordinate systems, is not-Problem 3.20 notwithstanding-covariant under arbitrary coordinate changes. [Hint: Use $x^{1}=$ $\\bar{x}^{1} \\bar{x}^{2}, x^{2}=\\bar{x}^{2}$, at the point $\\left.\\left(\\bar{x}^{i}\\right)=(1,2).\\right]$\n\n3.31 By use of (3.23), establish the familiar identity for the vector product of three vectors,\n\n$$\n\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}\n$$\n\nor, in coordinate form,\n\n$$\ne_{i j k} u_{j}\\left(e_{k r s} v_{r} w_{s}\\right)=\\left(u_{j} w_{j}\\right) v_{i}-\\left(u_{j} v_{j}\\right) w_{i}\n$$\n\n3.32 (a) Show that if $\\left(T_{j}^{i}\\right)$ is a mixed tensor, then $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ is not generally a tensor. (b) Show that a mixed tensor of order two, symmetric in a given coordinate system, will transform as a symmetric tensor if the Jacobian matrix is orthogonal.\n\n3.33 Prove: (a) If $\\left(T_{j}^{i}\\right)$ is a mixed tensor of order two, $T_{i}^{i}$ is an invariant; $(b)$ if $\\left(S_{j k}^{i}\\right)$ and $\\left(T^{i}\\right)$ are tensors of the type and order indicated, $S_{j r}^{r} T^{j}$ is an invariant.\n\n3.34 If $\\mathbf{T} \\equiv\\left(T_{m l}^{i j k}\\right)$ is a tensor, contravariant of order 3 and covariant of order 2 , show that $\\mathbf{S} \\equiv\\left(T_{k j}^{i j k}\\right)$ is a contravariant vector.\n\n3.35 Show that the derivative, $d \\mathbf{T} / d t$, of the tangent vector $\\mathbf{T} \\equiv\\left(T^{i}\\right)=\\left(d x^{i} / d t\\right)$ to a curve $x^{i}=x^{i}(t)$ is a contravariant affine tensor. Is it a cartesian tensor?\n\n3.36 (a) Use the theory of tensors to prove that the scalar product $\\mathbf{u v} \\equiv u_{i} v_{i}$ of two vectors $\\mathbf{u}=\\left(u_{i}\\right)$ and $\\mathbf{v}=\\left(v_{i}\\right)$ is a cartesian invariant. (b) Is uv an affine invariant?\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 4}", "\\section*{Tensor Operations; Tests for Tensor Character}\n\\subsection*{4.1 FUNDAMENTAL OPERATIONS}\nFrom two given tensors,\n\n\n\\begin{equation*}\n\\mathbf{S}=\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right) \\quad \\mathbf{T}=\\left(T_{l_{1} l_{2} \\ldots l_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.1}\n\\end{equation*}\n\n\ncertain operations, to be described, will produce a third tensor.\n\n\\section*{Sums, Linear Combinations}\nLet $p=r$ and $q=s$ in (4.1). Since the transformation law (3.14) is linear in the tensor components, it is clear that\n\n\n\\begin{equation*}\n\\mathbf{S}+\\mathbf{T} \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{s}}^{i_{1} i_{2} \\ldots i_{r}}+T_{j_{1} j_{2} \\ldots j_{s}}^{i_{1} i_{2} \\ldots i_{r}}\\right) \\tag{4.2a}\n\\end{equation*}\n\n\nis a tensor of the same type and order as the original two tensors. More generally, if $\\mathbf{T}_{1}, \\mathbf{T}_{2}, \\ldots, \\mathbf{T}_{\\mu}$ are tensors of the same type and order and if $\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{\\mu}$ are invariant scalars, then\n\n\n\\begin{equation*}\n\\lambda_{1} \\mathbf{T}_{1}+\\lambda_{2} \\mathbf{T}_{2}+\\cdots+\\lambda_{\\mu} \\mathbf{T}_{\\mu} \\tag{4.2b}\n\\end{equation*}\n\n\nis a tensor of that same type and order.\n\n\\section*{Outer Product}\nThe outer product of the tensors $\\mathbf{S}$ and $\\mathbf{T}$ of (4.1) is the tensor\n\n\n\\begin{equation*}\n[\\mathbf{S T}] \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} \\cdot T_{l_{1} l_{2} \\ldots i_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.3}\n\\end{equation*}\n\n\nwhich is of order $m=p+q+r+s$ (the sum of the orders of $\\mathbf{S}$ and $\\mathbf{T}$ ), contravariant of order $p+r$ and covariant of order $q+s$. Note that $[\\mathbf{S T}]=[\\mathbf{T S}]$.\n\nEXAMPLE 4.1 Given two tensors, $\\mathbf{S}=\\left(S_{j}^{i}\\right)$ and $\\mathbf{T}=\\left(T_{k}\\right)$, the outer product $[\\mathbf{S T}]=\\left(S_{j}^{i} T_{k}\\right) \\equiv\\left(P_{j k}^{i}\\right)$ is a tensor because\n\n$$\n\\bar{P}_{j k}^{i} \\equiv \\bar{S}_{j}^{i} \\bar{T}_{k}=\\left(S_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)\\left(T_{u} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{k}}\\right)=P_{s u}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{k}}\n$$\n\n\\section*{Inner Product}\nTo take the inner product of two tensors, one equates an upper (contravariant) index of one tensor to a lower (covariant) index of the other, and sums products of components over the repeated index. In effect, the contravariant and covariant behaviors cancel out, which lowers the total order of the two tensors.\n\nTo state this more formally, set $i_{\\alpha}=u=l_{\\beta}$ in (4.1). Then the inner product corresponding to this pair of indices is\n\n\n\\begin{equation*}\n\\mathbf{S T} \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} \\ldots i_{p}} T_{l_{1} \\ldots u \\ldots l_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.4}\n\\end{equation*}\n\n\nIt is seen that there will exist $p s+r q$ inner products ST and TS; in general, all of these will be distinct. Each will be a tensor of order\n\n$$\nm=p+q+r+s-2\n$$\n\nEXAMPLE 4.2 From the tensors $\\mathbf{S}=\\left(S^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{k l m}\\right)$, form the inner product $\\mathbf{U}=\\left(U_{k m}^{j}\\right) \\equiv\\left(S^{u j} T_{k u m}\\right)$. We have:\n\n$$\n\\begin{aligned}\n\\bar{U}_{k m}^{j} & =\\left(S^{p r} \\frac{\\partial \\bar{x}^{u}}{\\partial x^{p}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}}\\right)\\left(T_{s q t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{u}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}\\right) \\\\\n& =S^{r} T_{s q t}\\left(\\frac{\\partial \\bar{x}^{u}}{\\partial x^{p}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{u}}\\right) \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}=S^{p r} T_{s q t} \\delta_{p}^{q} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}} \\\\\n& =S^{p r} T_{s p t} \\frac{\\partial \\bar{x}^{j}}{\\partial \\bar{x}^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}} \\equiv U_{s t}^{r} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}\n\\end{aligned}\n$$\n\nwhich verifies that $\\mathbf{U}$ is a tensor of order 3 , contravariant of order 1 and covariant of order 2 .\n\nEXAMPLE 4.3 With $\\left(T_{i j}\\right)$ and $\\left(T^{i j}\\right)$ as in Theorem 3.2,\n\n$$\nT^{i u} T_{u j}=\\delta_{j}^{i}\n$$\n\nAs an inner product, the left side defines a second-order tensor that is contravariant of order one and covariant of order one. This constitutes a new proof (cf. Problem 3.29) of the tensor nature of the Kronecker delta.\n\nIn the special case when $\\mathbf{S}$ is a contravariant vector and $\\mathbf{T}$ is a covariant vector, the inner product ST is of the form $S^{i} T_{i}$, which is an invariant (Theorem 3.1). Because the tensor ST is of order\n\n$$\nm=p+q+r+s-2=1+0+0+1-2=0\n$$\n\nan invariant is regarded as a tensor of order zero.\n\n\\section*{Contraction}\nAnother order-reducing operation, like the inner product but applying to single tensors, is that of contracting a tensor on a pair of indices. In tensor $\\mathbf{S}$ of (4.1) set $i_{\\alpha}=u=j_{\\beta}$ and sum on $u$; the resulting tensor (Problem 4.7),\n\n\n\\begin{equation*}\n\\mathbf{S}^{\\prime}=\\left(S_{j_{1} \\ldots u \\ldots i_{p}}^{i_{1} \\ldots u \\ldots i_{p}}\\right) \\tag{4.5}\n\\end{equation*}\n\n\nis called a contraction of $\\mathbf{S}$, with contraction indices $i_{\\alpha}$ and $j_{\\beta}$. $\\mathbf{S}^{\\prime}$ is contravariant of order $p-1$ and covariant of order $q-1$.\n\n\\section*{Combined Operations}\nIt is clear that one may form new tensors from old in a variety of ways by performing a sequence of the tensor operations discussed above. For example, one might form the outer product of two tensors, then take an inner product of this with a third tensor; or contract on one or more pairs of indices, either before or after taking a product. It is noteworthy that an inner product of two tensors may be characterized as a contraction of their outer product: $\\mathbf{S T}=[\\mathbf{S T}]^{\\prime}$. See Fig. 4-1.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-053}\n\\end{center}\n\nFig. 4-1\n\n\\subsection*{4.2 TESTS FOR TENSOR CHARACTER}\nIt is useful to have an alternative method for verifying tensor character that does not directly appeal to the tensor transformation laws. Roughly stated, the principle is this: If it can be shown that the inner product $\\mathbf{T V}$ is a tensor for all vectors $\\mathbf{V}$, then $\\mathbf{T}$ is a tensor. This idea is often referred to as the Quotient Rule for tensors; the official Quotient Theorem is our Theorem 4.2 below.\n\nThe following statements are useful criteria or \"tests\" for tensor character; they may all be derived as special cases of the Quotient Theorem.\n\n(1) If $T_{i} V^{i} \\equiv E$ is invariant for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i}\\right)$ is a covariant vector (tensor of order 1).\n\n(2) If $T_{i j} V^{i} \\equiv U_{j}$ are components of a covariant vector for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\n(3) If $T_{i j} U^{i} V^{j} \\equiv E$ is invariant for all contravariant vectors $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\n(4) If $\\left(T_{i j}\\right)$ is symmetric and $T_{i j} V^{i} V^{j} \\equiv E$ is invariant for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\nEXAMPLE 4.4 Establish criterion (1).\n\nSince $E$ is invariant, $\\bar{E}=E$, or $\\bar{T}_{i} \\bar{V}^{i}=T_{i} V^{i}$. Substitute in this equation the transformation law for $\\left(V^{i}\\right)$ and change the dummy index on the right:\n\n$$\n\\bar{T}_{i}\\left(V^{j} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right)=T_{j} V^{j} \\quad \\text { or } \\quad\\left(T_{j}-\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right) V^{j}=0\n$$\n\nThe latter equation must hold when $\\left(V^{i}\\right)$ is any of the contravariant vectors represented in $\\left(x^{i}\\right)$ by $\\left(\\delta_{1}^{i}\\right),\\left(\\delta_{2}^{i}\\right), \\ldots,\\left(\\delta_{n}^{i}\\right)$; their existence is guaranteed by Problem 3.5. Thus, for the $k$ th of these vectors $(1 \\leqq k \\leqq n)$,\n\n$$\n\\left(T_{k}-\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{k}}\\right) \\cdot 1=0 \\quad \\text { or } \\quad T_{k}=\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{k}}\n$$\n\nwhich is the law of transformation-from $\\left(\\bar{x}^{i}\\right)$ to $\\left(x^{i}\\right)$-of a covariant vector.\n\nThe method of Example 4.4 may be easily extended to establish the following result, which in turn implies the Quotient Theorem.\n\nLemma 4.1: If $T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}} \\equiv E$ is an invariant for arbitrary covariant vectors $\\left(U_{i_{\\alpha}}^{(\\alpha)}\\right) \\equiv \\mathbf{U}^{(\\alpha)} \\quad(\\alpha=1,2, \\ldots, p)$ and arbitrary contravariant vectors $\\left(V_{(\\beta)}^{j_{\\beta}}\\right) \\equiv \\mathbf{V}_{(\\beta)} \\quad(\\beta=1,2, \\ldots, q)$, then $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor of the type indicated by its indices.\n\nTheorem 4.2 (Quotient Theorem): If $T_{j_{1} j_{2} \\ldots j_{q} k}^{i_{1} i_{2} \\ldots i_{p}} V^{k} \\equiv S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}$ are components of a tensor for an arbitrary contravariant vector $\\left(V^{k}\\right)$, then $\\left(T_{j_{1} j_{2} \\ldots j_{q} j_{q+1}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor of the type and order indicated.\n\n\\subsection*{4.3 TENSOR EQUATIONS}\nMuch of the importance of tensors in mathematical physics and engineering resides in the fact that if a tensor equation or identity is true in one coordinate system, then it is true in all coordinate systems.\n\nEXAMPLE 4.5 Suppose that in some special coordinate system, $\\left(x^{i}\\right)$, the covariant tensor $\\mathbf{T}=\\left(T_{i j}\\right)$ vanishes. The components of $\\mathbf{T}$ in any other coordinate system, $\\left(\\bar{x}^{i}\\right)$, are given by\n\n$$\n\\bar{T}_{i j}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=0+0+\\cdots+0=0\n$$\n\nTherefore, $\\mathbf{T}=\\mathbf{0}$ in every coordinate system.\n\nEXAMPLE 4.6 Consider a putative equation\n\n\n\\begin{equation*}\nR_{i j k} U^{k}=A W_{i}^{k l} M_{j k} I_{l} \\tag{1}\n\\end{equation*}\n\n\nconnecting six entities that may or may not be tensors. If it can be shown that (i) $\\mathbf{T}=\\left(T_{i j}\\right) \\equiv\\left(R_{i j k} U^{k}-\\right.$ $A W_{i}^{k l} M_{i k} U_{l}$ ) is a tensor, and (ii) a special coordinate system exists in which all $T_{i j}$ are zero, then (1) is valid in every coordinate system.\n\nEXAMPLE 4.7 A second-order covariant tensor, or a second-order contravariant tensor, that is known to be symmetric in one coordinate system must be symmetric in every coordinate system. (This statement does not extend to a second-order mixed tensor; see Problem 3.16.)\n\nAnother application of the principle yields (Problem 4.15) a useful fact in tensor analysis, often taken for granted:\n\nTheorem 4.3: If $\\left(T_{i j}\\right)$ is a covariant tensor of order two whose determinant vanishes in one particular coordinate system, then its determinant vanishes in all coordinate systems.\n\nCorollary 4.4: A covariant tensor of order two that is invertible in one coordinate system is invertible in all coordinate systems.\n\n\\section*{Solved Problems}\n\\section*{TENSOR SUMS}\n4.1 Show that if $\\lambda$ and $\\mu$ are invariants and $S^{i}$ and $T^{i}$ are components of contravariant vectors, the vector defined in all coordinate systems by $\\left(\\lambda S^{i}+\\mu T^{i}\\right)$ is a contravariant vector.\n\nSince $\\bar{\\lambda}=\\lambda$ and $\\bar{\\mu}=\\mu$,\n\nas desired.\n\n$$\n\\bar{\\lambda} \\bar{S}^{i}+\\bar{\\mu} \\bar{T}^{i}=\\lambda\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)+\\mu\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left(\\lambda S^{r}+\\mu T^{r}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\n4.2 Prove that (a) the array defined in each coordinate system by $\\left(T_{i j}-T_{j i}\\right)$, where $\\left(T_{i j}\\right)$ is a given covariant tensor, is a covariant tensor; $(b)$ the array defined in each coordinate system by $\\left(T_{j}^{i}-T_{i}^{j}\\right)$, where $\\left(T_{j}^{i}\\right)$ is a given mixed tensor, is not generally a tensor, but is a cartesian tensor.\n\n(a) By (4.2b), the array is a tensor if and only if $\\left(T_{i j}^{*}\\right) \\equiv\\left(T_{j i}\\right)$ is a covariant tensor. But the transformation law for $\\left(T_{i j}\\right)$ gives\n\n$$\n\\bar{T}_{j i}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{T}_{i j}^{*}=T_{s r}^{*} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich shows that $\\left(T_{i j}^{*}\\right)$ is indeed a covariant tensor.\n\n(b) We give a second proof [recall Problem 3.32(a)], based on (4.2b). The question is whether $\\left(U_{j}^{i}\\right) \\equiv\\left(T_{i}^{j}\\right)$ is a tensor. From the transformation law for $\\left(T_{j}^{i}\\right)$,\n\n$$\n\\bar{T}_{i}^{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{U}_{j}^{i}=U_{r}^{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}}\n$$\n\nThus, $\\left(U_{i}^{i}\\right)$ does not obey a tensor law, unless, for all $p, q$,\n\n$$\n\\frac{\\partial \\bar{x}^{p}}{\\partial x^{q}}=\\frac{\\partial x^{q}}{\\partial \\bar{x}^{p}} \\quad \\text { or } \\quad J=\\left(J^{-1}\\right)^{T}\n$$\n\ni.e., unless the Jacobian matrix is orthogonal-as it is for orthogonal linear transformations (cartesian tensors).\n\n\\section*{OUTER PRODUCT}\n4.3 Show that the outer product of two contravariant vectors is a contravariant tensor of order two.\n\nWith $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ as the given vectors,\n\n$$\n\\bar{S}^{i} \\bar{T}^{j}=\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\\right)=S^{r} T^{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the correct transformation law for the outer product to be a contravariant tensor of order two.\n\n\\section*{INNER PRODUCT}\n4.4 Prove that the inner product $\\left(T^{r} U_{i r}\\right)$ is a tensor if $\\left(T^{i}\\right)$ and $\\left(U_{i j}\\right)$ are tensors of the types indicated.\n\nWith $V_{j} \\equiv T^{i} U_{j i}$,\n\n$$\n\\bar{V}_{j}=\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(U_{s t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}\\right)=\\left(T^{r} U_{s t} \\delta_{r}^{t}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich is the desired transformation law.\n\n4.5 Prove that if $\\mathbf{g}=\\left(g_{i j}\\right)$ is a covariant tensor of order two, and $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ are contravariant vectors, then the double inner product $\\mathbf{g U V}=g_{i j} U^{i} V^{j}$ is an invariant.\n\nThe transformation laws are\n\n$$\n\\bar{g}_{i j}=g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\bar{U}^{i}=U^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{i}} \\quad \\bar{V}^{j}=V^{u} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}\n$$\n\nMultiply, and sum over $i$ and $j$ :\n\n$$\n\\overline{\\mathbf{g}} \\overline{\\mathbf{U}} \\overline{\\mathbf{V}}=\\bar{g}_{i j} \\bar{U}^{i} \\bar{V}^{j}=g_{r s} U^{t} V^{u} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}=g_{r s} U^{t} V^{u} \\delta_{t}^{r} \\delta_{u}^{s}=g_{r s} U^{r} V^{s}=\\mathbf{g} \\mathbf{U} \\mathbf{V}\n$$\n\n\\section*{CONTRACTION}\n4.6 Assuming that contraction of a tensor yields a tensor, how many tensors may be created by repeated contraction of the tensor $\\mathbf{T}=\\left(T_{k l}^{i j}\\right)$ ?\n\nSingle contraction produces the four mixed tensors\n\n$$\n\\left(T_{u l}^{u j}\\right) \\quad\\left(T_{k u}^{u j}\\right) \\quad\\left(T_{u l}^{i u}\\right) \\quad\\left(T_{k u}^{i u}\\right)\n$$\n\nand double contraction produces the two zero-order tensors (invariants) $T_{u v}^{u v}$ and $T_{v u}^{u v}$. Thus there are six tensors, in general all distinct.\n\n4.7 Show that any contraction of the tensor $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$ results in a covariant vector.\n\nWe may contract on either $i=j$ or $i=k$. For $\\left(S_{k}\\right) \\equiv\\left(T_{i k}^{i}\\right)$, we have the transformation law\n\n$$\n\\bar{S}_{k} \\equiv \\bar{T}_{i k}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{s t}^{r} \\delta_{r}^{s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{r t}^{r} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=S_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\nand, for $\\left(U_{j}\\right) \\equiv\\left(T_{j i}^{i}\\right)$,\n\n$$\n\\bar{U}_{j} \\equiv \\bar{T}_{j i}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{s t}^{r} \\delta_{r}^{t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s r}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nIn either case, the transformation law is that of a covariant vector.\n\n\\section*{COMBINED OPERATIONS}\n4.8 Suppose that $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ are tensors from which a contravariant vector $\\mathbf{V}=\\left(V^{i}\\right)$ is to be constructed using a combination of outer/inner products and contractions. (a) Show that there are six possibilities for $\\mathbf{V}$, which can all be distinct. (b) Verify that each possible $\\mathbf{V}$ is obtainable as a contraction of an inner product ST.\n\n(a) Writing $[\\mathbf{S T}] \\equiv \\mathbf{U}=\\left(U_{l m}^{i j k}\\right)$, we obtain the contravariant vectors as the double contractions of $\\mathbf{U}$ :\n\n$$\n\\begin{array}{llllll}\n\\left(U_{u v}^{u v k}\\right) & \\left(U_{v u}^{u v k}\\right) & \\left(U_{u v}^{u j v}\\right) & \\left(U_{v u}^{u j v}\\right) & \\left(U_{u v}^{i u v}\\right) & \\left(U_{v u}^{i u v}\\right)\n\\end{array}\n$$\n\n(b) The vector $\\left(U_{u v}^{u v k}\\right) \\equiv\\left(S_{u}^{u v} T_{v}^{k}\\right)$ may be obtained by first taking the inner product $\\left(S_{l}^{i v} T_{v}^{k}\\right)$ and then contracting on $i=u=l$. Likewise for the other five vectors of $(a)$.\n\n\\section*{TESTS FOR TENSOR CHARACTER}\n4.9 Prove criterion (2) of Section 4.2 without invoking the Quotient Theorem.\n\nWe are to verify that $\\left(T_{i j}\\right)$ is a covariant tensor of order two if it is given that for every contravariant vector $\\left(V^{i}\\right), T_{i j} V^{i} \\equiv U_{j}$ are components of a covariant vector. Start out with the transformation law for $\\left(U_{j}\\right)$ [from $\\left(x^{i}\\right)$ to $\\left.\\left(\\bar{x}^{i}\\right)\\right]$ :\n\n$$\n\\bar{U}_{j}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad \\bar{T}_{i j} \\bar{V}^{i}=T_{i s} V^{i} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nNow substitute the transformation law for $\\bar{V}^{i}$ [from $\\left(\\bar{x}^{i}\\right)$ to $\\left.\\left(x^{i}\\right)\\right]$ :\n\n$$\n\\bar{T}_{i j} \\bar{V}^{i}=T_{i s}\\left(\\overline{\\bar{V}}^{p} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{p}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nReplace the dummy index $i$ by $p$ on the left and by $r$ on the right:\n\n$$\n\\bar{T}_{p j} \\bar{V}^{p}=T_{r s} \\bar{V}^{p} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad\\left(\\bar{T}_{p j}-T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right) \\bar{V}^{p}=0\n$$\n\nThe proof is concluded as in Example 4.4.\n\n4.10 Prove criterion (3) of Section 4.2.\n\nHere we must show that $\\left(T_{i j}\\right)$ is a covariant tensor, assuming that $T_{i j} U^{i} V^{j}$ is invariant. Using criterion (1), we conclude that $\\left(T_{i j} U^{i}\\right)$ is a covariant vector. Using criterion (2), it follows that since $\\left(U^{i}\\right)$ is arbitrary, $\\left(T_{i j}\\right)$ is a covariant tensor of order two, the desired conclusion.\n\n4.11 Prove criterion (4) of Section 4.2.\n\nWe wish to show that if $\\left(T_{i j}\\right)$ is a symmetric array such that $T_{i j} V^{i} V^{j}$ is an invariant for every contravariant vector $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a (symmetric) covariant tensor of order two.\n\nLet $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ denote arbitrary contravariant vectors and let $\\left(W^{i}\\right) \\equiv\\left(U^{i}+V^{i}\\right)$, a contravariant vector by $(4.2 a)$. Then,\n\n$$\n\\begin{aligned}\nT_{i j} W^{i} W^{j} & \\equiv T_{i j}\\left(U^{i}+V^{i}\\right)\\left(U^{j}+V^{j}\\right) \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} U^{j}+T_{i j} U^{i} V^{j}+T_{i j} V^{i} V^{j} \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} V^{j}+2 T_{i j} U^{i} V^{j}\n\\end{aligned}\n$$\n\nwhere the symmetry of $\\left(T_{i j}\\right)$ has been used in the last step. Now, by hypothesis, the left-hand side and the first two terms of the right-hand side of the above identity are invariants. Therefore, $T_{i j} U^{i} V^{j}$ must be an invariant, and the desired conclusion follows from criterion (3).\n\n4.12 Use Lemma 4.1 to write a proof of the Quotient Theorem, Theorem 4.2.\n\nIn the notation of the theorem and lemma, $S_{j_{1} j_{2} \\cdots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} \\cdot U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}}$ is a tensor of order zero, or an invariant, for arbitrary $\\mathbf{U}^{(\\alpha)}$ and $\\mathbf{V}_{(\\beta)}$; that is,\n\n$$\nT_{j_{1} j_{2} \\cdots j_{q} k}^{i_{1} i_{2} \\cdots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}} V^{k}\n$$\n\nis an invariant, with $\\left(V^{k}\\right)$ also arbitrary. It then follows from Lemma 4.1 (with $q$ replaced by $q+1$ ) that $\\left(T_{j_{1} j_{2} \\ldots j_{q^{k}}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor, contravariant of order $p$ and covariant of order $q+1$.\n\nFrom the above method of proof, it is clear that the Quotient Theorem is equally valid when the \"divisor\" is an arbitrary covariant vector. This form of the theorem will be used in Problem 4.13.\n\n4.13 Use the Quotient Theorem to prove Theorem 3.2.\n\nIf $\\mathbf{U}=\\left(U^{i}\\right)$ is a contravariant vector, the inner product\n\n$$\n\\mathbf{V}=\\mathbf{T} \\mathbf{U} \\equiv\\left(T_{i j} U^{j}\\right)\n$$\n\nis a covariant vector. Moreover, because $\\left[T_{i j}\\right]_{n n}$ has an inverse, it follows that as $\\mathbf{U}$ runs through all contravariant vectors, $\\mathbf{V}$ runs through all covariant vectors. Thus,\n\n$$\n\\mathbf{U}=\\mathbf{T}^{-1} \\mathbf{V} \\equiv\\left(T^{i l} V_{j}\\right)\n$$\n\nis a tensor for an arbitrary $\\left(V_{i}\\right)$, making $\\left(T^{i j}\\right)$ a contravariant tensor of order two.\n\n\\section*{TENSOR EQUATIONS}\n4.14 Prove that if $\\left(T_{j k l}^{i}\\right)$ is a tensor such that, in the $\\left(x^{i}\\right)$-system, $T_{j k l}^{i}=3 T_{l j k}^{i}$, then $T_{j k l}^{i}=3 T_{l j k}^{i}$ in all coordinate systems.\n\nWe must prove that $\\bar{T}_{j k l}^{i}=3 \\bar{T}_{l j k}^{i}$ in $\\left(\\bar{x}^{i}\\right)$. But\n\n$$\n\\begin{aligned}\n\\bar{T}_{j k l}^{i}-3 \\bar{T}_{l j k}^{i} & =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{t r s}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\\\\n& =\\left(T_{r s t}^{p}-3 T_{t r s}^{p}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{l}}=0\n\\end{aligned}\n$$\n\nas desired.\n\n\\subsection*{4.15 Prove Theorem 4.3.}\nBy Problem 3.14(a), the covariant transformation law has the matrix expression\n\n$$\n\\bar{T}=\\bar{J}^{T} T \\bar{J} \\quad \\text { whence } \\quad|\\bar{T}|=\\overline{\\mathscr{J}}^{2}|T|\n$$\n\nThus, $|T|=0$ implies $|\\bar{T}|=0$.\n\n4.16 Prove that if a mixed tensor $\\left(T_{j}^{i}\\right)$ can be expressed as the outer product of contravariant and covariant vectors $\\left(U^{i}\\right)$ and $\\left(V_{j}\\right)$ in one coordinate system, then $\\left(T_{j}^{i}\\right)$ is the outer product of those vectors in general.\n\nWe must prove that $\\bar{T}_{j}^{i}=\\bar{U}^{i} \\bar{V}_{j}$ for any admissible coordinate system $\\left(\\bar{x}^{i}\\right)$. But, by hypothesis,\n\n$$\n\\bar{T}_{j}^{i}-\\bar{U}^{i} \\bar{V}_{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-\\left(U^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)=\\left(T_{s}^{r}-U^{r} V_{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=0\n$$\n\n\\section*{Supplementary Problems}\n4.17 If $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ are contravariant vectors, verify that $\\left(2 U^{i}+3 V^{i}\\right)$ is also a contravariant vector.\n\n4.18 Verify that the outer product of a contravariant vector and a covariant vector is a mixed tensor of order two.\n\n4.19 How many potentially different mixed tensors of order two can be defined by taking the outer product of $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$, then contracting twice?\n\n4.20 Show that if $T_{k l}^{i j}$ are tensor components, $T_{i j}^{i j}$ is an invariant.\n\n4.21 Prove that if $T_{j k l}^{i} U^{j} \\equiv S_{k l}^{i}$ are components of a tensor for any contravariant vector $\\left(U^{j}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Apply the Quotient Theorem to $\\left(M_{k l j}^{i}\\right) \\equiv\\left(T_{j k l}^{i}\\right)$. More generally, the Quotient Theorem is valid for all choices of the inner product.]\n\n4.22 Prove that if $T_{j k l}^{i} S^{k l} \\equiv U_{j}^{i}$ are tensor components for arbitrary contravariant tensors $\\left(S^{k l}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Follow Problem 4.9.]\n\n4.23 Prove that if $T_{j k l}^{i} U^{k} U^{l} \\equiv V_{j}^{i}$ are components of a tensor for an arbitrary contravariant vector $\\left(U^{i}\\right)$, and if $\\left(T_{j k l}^{i}\\right)$ is symmetric in the last two lower indices in all coordinate systems, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the type indicated.\n\n4.24 Show that Theorem 4.3 and Corollary 4.4 are equivalent.\n\n4.25 Prove the assertion of Example 4.7.\n\n4.26 Prove that if an invariant $E$ can be expressed as the inner product of vectors $\\left(U_{i}\\right)$ and $\\left(V^{i}\\right)$ in one coordinate system, then $E$ has that representation in any coordinate system.\n\n"], "lesson": "\\section*{Chapter 4}\n\\section*{Tensor Operations; Tests for Tensor Character}\n\\subsection*{4.1 FUNDAMENTAL OPERATIONS}\nFrom two given tensors,\n\n\n\\begin{equation*}\n\\mathbf{S}=\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right) \\quad \\mathbf{T}=\\left(T_{l_{1} l_{2} \\ldots l_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.1}\n\\end{equation*}\n\n\ncertain operations, to be described, will produce a third tensor.\n\n\\section*{Sums, Linear Combinations}\nLet $p=r$ and $q=s$ in (4.1). Since the transformation law (3.14) is linear in the tensor components, it is clear that\n\n\n\\begin{equation*}\n\\mathbf{S}+\\mathbf{T} \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{s}}^{i_{1} i_{2} \\ldots i_{r}}+T_{j_{1} j_{2} \\ldots j_{s}}^{i_{1} i_{2} \\ldots i_{r}}\\right) \\tag{4.2a}\n\\end{equation*}\n\n\nis a tensor of the same type and order as the original two tensors. More generally, if $\\mathbf{T}_{1}, \\mathbf{T}_{2}, \\ldots, \\mathbf{T}_{\\mu}$ are tensors of the same type and order and if $\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{\\mu}$ are invariant scalars, then\n\n\n\\begin{equation*}\n\\lambda_{1} \\mathbf{T}_{1}+\\lambda_{2} \\mathbf{T}_{2}+\\cdots+\\lambda_{\\mu} \\mathbf{T}_{\\mu} \\tag{4.2b}\n\\end{equation*}\n\n\nis a tensor of that same type and order.\n\n\\section*{Outer Product}\nThe outer product of the tensors $\\mathbf{S}$ and $\\mathbf{T}$ of (4.1) is the tensor\n\n\n\\begin{equation*}\n[\\mathbf{S T}] \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} \\cdot T_{l_{1} l_{2} \\ldots i_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.3}\n\\end{equation*}\n\n\nwhich is of order $m=p+q+r+s$ (the sum of the orders of $\\mathbf{S}$ and $\\mathbf{T}$ ), contravariant of order $p+r$ and covariant of order $q+s$. Note that $[\\mathbf{S T}]=[\\mathbf{T S}]$.\n\nEXAMPLE 4.1 Given two tensors, $\\mathbf{S}=\\left(S_{j}^{i}\\right)$ and $\\mathbf{T}=\\left(T_{k}\\right)$, the outer product $[\\mathbf{S T}]=\\left(S_{j}^{i} T_{k}\\right) \\equiv\\left(P_{j k}^{i}\\right)$ is a tensor because\n\n$$\n\\bar{P}_{j k}^{i} \\equiv \\bar{S}_{j}^{i} \\bar{T}_{k}=\\left(S_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)\\left(T_{u} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{k}}\\right)=P_{s u}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{k}}\n$$\n\n\\section*{Inner Product}\nTo take the inner product of two tensors, one equates an upper (contravariant) index of one tensor to a lower (covariant) index of the other, and sums products of components over the repeated index. In effect, the contravariant and covariant behaviors cancel out, which lowers the total order of the two tensors.\n\nTo state this more formally, set $i_{\\alpha}=u=l_{\\beta}$ in (4.1). Then the inner product corresponding to this pair of indices is\n\n\n\\begin{equation*}\n\\mathbf{S T} \\equiv\\left(S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} \\ldots i_{p}} T_{l_{1} \\ldots u \\ldots l_{s}}^{k_{1} k_{2} \\ldots k_{r}}\\right) \\tag{4.4}\n\\end{equation*}\n\n\nIt is seen that there will exist $p s+r q$ inner products ST and TS; in general, all of these will be distinct. Each will be a tensor of order\n\n$$\nm=p+q+r+s-2\n$$\n\nEXAMPLE 4.2 From the tensors $\\mathbf{S}=\\left(S^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{k l m}\\right)$, form the inner product $\\mathbf{U}=\\left(U_{k m}^{j}\\right) \\equiv\\left(S^{u j} T_{k u m}\\right)$. We have:\n\n$$\n\\begin{aligned}\n\\bar{U}_{k m}^{j} & =\\left(S^{p r} \\frac{\\partial \\bar{x}^{u}}{\\partial x^{p}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}}\\right)\\left(T_{s q t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{u}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}\\right) \\\\\n& =S^{r} T_{s q t}\\left(\\frac{\\partial \\bar{x}^{u}}{\\partial x^{p}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{u}}\\right) \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}=S^{p r} T_{s q t} \\delta_{p}^{q} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}} \\\\\n& =S^{p r} T_{s p t} \\frac{\\partial \\bar{x}^{j}}{\\partial \\bar{x}^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}} \\equiv U_{s t}^{r} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{m}}\n\\end{aligned}\n$$\n\nwhich verifies that $\\mathbf{U}$ is a tensor of order 3 , contravariant of order 1 and covariant of order 2 .\n\nEXAMPLE 4.3 With $\\left(T_{i j}\\right)$ and $\\left(T^{i j}\\right)$ as in Theorem 3.2,\n\n$$\nT^{i u} T_{u j}=\\delta_{j}^{i}\n$$\n\nAs an inner product, the left side defines a second-order tensor that is contravariant of order one and covariant of order one. This constitutes a new proof (cf. Problem 3.29) of the tensor nature of the Kronecker delta.\n\nIn the special case when $\\mathbf{S}$ is a contravariant vector and $\\mathbf{T}$ is a covariant vector, the inner product ST is of the form $S^{i} T_{i}$, which is an invariant (Theorem 3.1). Because the tensor ST is of order\n\n$$\nm=p+q+r+s-2=1+0+0+1-2=0\n$$\n\nan invariant is regarded as a tensor of order zero.\n\n\\section*{Contraction}\nAnother order-reducing operation, like the inner product but applying to single tensors, is that of contracting a tensor on a pair of indices. In tensor $\\mathbf{S}$ of (4.1) set $i_{\\alpha}=u=j_{\\beta}$ and sum on $u$; the resulting tensor (Problem 4.7),\n\n\n\\begin{equation*}\n\\mathbf{S}^{\\prime}=\\left(S_{j_{1} \\ldots u \\ldots i_{p}}^{i_{1} \\ldots u \\ldots i_{p}}\\right) \\tag{4.5}\n\\end{equation*}\n\n\nis called a contraction of $\\mathbf{S}$, with contraction indices $i_{\\alpha}$ and $j_{\\beta}$. $\\mathbf{S}^{\\prime}$ is contravariant of order $p-1$ and covariant of order $q-1$.\n\n\\section*{Combined Operations}\nIt is clear that one may form new tensors from old in a variety of ways by performing a sequence of the tensor operations discussed above. For example, one might form the outer product of two tensors, then take an inner product of this with a third tensor; or contract on one or more pairs of indices, either before or after taking a product. It is noteworthy that an inner product of two tensors may be characterized as a contraction of their outer product: $\\mathbf{S T}=[\\mathbf{S T}]^{\\prime}$. See Fig. 4-1.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-053}\n\\end{center}\n\nFig. 4-1\n\n\\subsection*{4.2 TESTS FOR TENSOR CHARACTER}\nIt is useful to have an alternative method for verifying tensor character that does not directly appeal to the tensor transformation laws. Roughly stated, the principle is this: If it can be shown that the inner product $\\mathbf{T V}$ is a tensor for all vectors $\\mathbf{V}$, then $\\mathbf{T}$ is a tensor. This idea is often referred to as the Quotient Rule for tensors; the official Quotient Theorem is our Theorem 4.2 below.\n\nThe following statements are useful criteria or \"tests\" for tensor character; they may all be derived as special cases of the Quotient Theorem.\n\n(1) If $T_{i} V^{i} \\equiv E$ is invariant for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i}\\right)$ is a covariant vector (tensor of order 1).\n\n(2) If $T_{i j} V^{i} \\equiv U_{j}$ are components of a covariant vector for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\n(3) If $T_{i j} U^{i} V^{j} \\equiv E$ is invariant for all contravariant vectors $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\n(4) If $\\left(T_{i j}\\right)$ is symmetric and $T_{i j} V^{i} V^{j} \\equiv E$ is invariant for all contravariant vectors $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a covariant tensor of order 2.\n\nEXAMPLE 4.4 Establish criterion (1).\n\nSince $E$ is invariant, $\\bar{E}=E$, or $\\bar{T}_{i} \\bar{V}^{i}=T_{i} V^{i}$. Substitute in this equation the transformation law for $\\left(V^{i}\\right)$ and change the dummy index on the right:\n\n$$\n\\bar{T}_{i}\\left(V^{j} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right)=T_{j} V^{j} \\quad \\text { or } \\quad\\left(T_{j}-\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\\right) V^{j}=0\n$$\n\nThe latter equation must hold when $\\left(V^{i}\\right)$ is any of the contravariant vectors represented in $\\left(x^{i}\\right)$ by $\\left(\\delta_{1}^{i}\\right),\\left(\\delta_{2}^{i}\\right), \\ldots,\\left(\\delta_{n}^{i}\\right)$; their existence is guaranteed by Problem 3.5. Thus, for the $k$ th of these vectors $(1 \\leqq k \\leqq n)$,\n\n$$\n\\left(T_{k}-\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{k}}\\right) \\cdot 1=0 \\quad \\text { or } \\quad T_{k}=\\bar{T}_{i} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{k}}\n$$\n\nwhich is the law of transformation-from $\\left(\\bar{x}^{i}\\right)$ to $\\left(x^{i}\\right)$-of a covariant vector.\n\nThe method of Example 4.4 may be easily extended to establish the following result, which in turn implies the Quotient Theorem.\n\nLemma 4.1: If $T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}} \\equiv E$ is an invariant for arbitrary covariant vectors $\\left(U_{i_{\\alpha}}^{(\\alpha)}\\right) \\equiv \\mathbf{U}^{(\\alpha)} \\quad(\\alpha=1,2, \\ldots, p)$ and arbitrary contravariant vectors $\\left(V_{(\\beta)}^{j_{\\beta}}\\right) \\equiv \\mathbf{V}_{(\\beta)} \\quad(\\beta=1,2, \\ldots, q)$, then $\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor of the type indicated by its indices.\n\nTheorem 4.2 (Quotient Theorem): If $T_{j_{1} j_{2} \\ldots j_{q} k}^{i_{1} i_{2} \\ldots i_{p}} V^{k} \\equiv S_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}$ are components of a tensor for an arbitrary contravariant vector $\\left(V^{k}\\right)$, then $\\left(T_{j_{1} j_{2} \\ldots j_{q} j_{q+1}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor of the type and order indicated.\n\n\\subsection*{4.3 TENSOR EQUATIONS}\nMuch of the importance of tensors in mathematical physics and engineering resides in the fact that if a tensor equation or identity is true in one coordinate system, then it is true in all coordinate systems.\n\nEXAMPLE 4.5 Suppose that in some special coordinate system, $\\left(x^{i}\\right)$, the covariant tensor $\\mathbf{T}=\\left(T_{i j}\\right)$ vanishes. The components of $\\mathbf{T}$ in any other coordinate system, $\\left(\\bar{x}^{i}\\right)$, are given by\n\n$$\n\\bar{T}_{i j}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=0+0+\\cdots+0=0\n$$\n\nTherefore, $\\mathbf{T}=\\mathbf{0}$ in every coordinate system.\n\nEXAMPLE 4.6 Consider a putative equation\n\n\n\\begin{equation*}\nR_{i j k} U^{k}=A W_{i}^{k l} M_{j k} I_{l} \\tag{1}\n\\end{equation*}\n\n\nconnecting six entities that may or may not be tensors. If it can be shown that (i) $\\mathbf{T}=\\left(T_{i j}\\right) \\equiv\\left(R_{i j k} U^{k}-\\right.$ $A W_{i}^{k l} M_{i k} U_{l}$ ) is a tensor, and (ii) a special coordinate system exists in which all $T_{i j}$ are zero, then (1) is valid in every coordinate system.\n\nEXAMPLE 4.7 A second-order covariant tensor, or a second-order contravariant tensor, that is known to be symmetric in one coordinate system must be symmetric in every coordinate system. (This statement does not extend to a second-order mixed tensor; see Problem 3.16.)\n\nAnother application of the principle yields (Problem 4.15) a useful fact in tensor analysis, often taken for granted:\n\nTheorem 4.3: If $\\left(T_{i j}\\right)$ is a covariant tensor of order two whose determinant vanishes in one particular coordinate system, then its determinant vanishes in all coordinate systems.\n\nCorollary 4.4: A covariant tensor of order two that is invertible in one coordinate system is invertible in all coordinate systems.\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{TENSOR SUMS}\n4.1 Show that if $\\lambda$ and $\\mu$ are invariants and $S^{i}$ and $T^{i}$ are components of contravariant vectors, the vector defined in all coordinate systems by $\\left(\\lambda S^{i}+\\mu T^{i}\\right)$ is a contravariant vector.\n\nSince $\\bar{\\lambda}=\\lambda$ and $\\bar{\\mu}=\\mu$,\n\nas desired.\n\n$$\n\\bar{\\lambda} \\bar{S}^{i}+\\bar{\\mu} \\bar{T}^{i}=\\lambda\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)+\\mu\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left(\\lambda S^{r}+\\mu T^{r}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\n4.2 Prove that (a) the array defined in each coordinate system by $\\left(T_{i j}-T_{j i}\\right)$, where $\\left(T_{i j}\\right)$ is a given covariant tensor, is a covariant tensor; $(b)$ the array defined in each coordinate system by $\\left(T_{j}^{i}-T_{i}^{j}\\right)$, where $\\left(T_{j}^{i}\\right)$ is a given mixed tensor, is not generally a tensor, but is a cartesian tensor.\n\n(a) By (4.2b), the array is a tensor if and only if $\\left(T_{i j}^{*}\\right) \\equiv\\left(T_{j i}\\right)$ is a covariant tensor. But the transformation law for $\\left(T_{i j}\\right)$ gives\n\n$$\n\\bar{T}_{j i}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{T}_{i j}^{*}=T_{s r}^{*} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich shows that $\\left(T_{i j}^{*}\\right)$ is indeed a covariant tensor.\n\n(b) We give a second proof [recall Problem 3.32(a)], based on (4.2b). The question is whether $\\left(U_{j}^{i}\\right) \\equiv\\left(T_{i}^{j}\\right)$ is a tensor. From the transformation law for $\\left(T_{j}^{i}\\right)$,\n\n$$\n\\bar{T}_{i}^{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{U}_{j}^{i}=U_{r}^{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}}\n$$\n\nThus, $\\left(U_{i}^{i}\\right)$ does not obey a tensor law, unless, for all $p, q$,\n\n$$\n\\frac{\\partial \\bar{x}^{p}}{\\partial x^{q}}=\\frac{\\partial x^{q}}{\\partial \\bar{x}^{p}} \\quad \\text { or } \\quad J=\\left(J^{-1}\\right)^{T}\n$$\n\ni.e., unless the Jacobian matrix is orthogonal-as it is for orthogonal linear transformations (cartesian tensors).\n\n\\section*{OUTER PRODUCT}\n4.3 Show that the outer product of two contravariant vectors is a contravariant tensor of order two.\n\nWith $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ as the given vectors,\n\n$$\n\\bar{S}^{i} \\bar{T}^{j}=\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\\right)=S^{r} T^{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the correct transformation law for the outer product to be a contravariant tensor of order two.\n\n\\section*{INNER PRODUCT}\n4.4 Prove that the inner product $\\left(T^{r} U_{i r}\\right)$ is a tensor if $\\left(T^{i}\\right)$ and $\\left(U_{i j}\\right)$ are tensors of the types indicated.\n\nWith $V_{j} \\equiv T^{i} U_{j i}$,\n\n$$\n\\bar{V}_{j}=\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(U_{s t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}\\right)=\\left(T^{r} U_{s t} \\delta_{r}^{t}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich is the desired transformation law.\n\n4.5 Prove that if $\\mathbf{g}=\\left(g_{i j}\\right)$ is a covariant tensor of order two, and $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ are contravariant vectors, then the double inner product $\\mathbf{g U V}=g_{i j} U^{i} V^{j}$ is an invariant.\n\nThe transformation laws are\n\n$$\n\\bar{g}_{i j}=g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\bar{U}^{i}=U^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{i}} \\quad \\bar{V}^{j}=V^{u} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}\n$$\n\nMultiply, and sum over $i$ and $j$ :\n\n$$\n\\overline{\\mathbf{g}} \\overline{\\mathbf{U}} \\overline{\\mathbf{V}}=\\bar{g}_{i j} \\bar{U}^{i} \\bar{V}^{j}=g_{r s} U^{t} V^{u} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}=g_{r s} U^{t} V^{u} \\delta_{t}^{r} \\delta_{u}^{s}=g_{r s} U^{r} V^{s}=\\mathbf{g} \\mathbf{U} \\mathbf{V}\n$$\n\n\\section*{CONTRACTION}\n4.6 Assuming that contraction of a tensor yields a tensor, how many tensors may be created by repeated contraction of the tensor $\\mathbf{T}=\\left(T_{k l}^{i j}\\right)$ ?\n\nSingle contraction produces the four mixed tensors\n\n$$\n\\left(T_{u l}^{u j}\\right) \\quad\\left(T_{k u}^{u j}\\right) \\quad\\left(T_{u l}^{i u}\\right) \\quad\\left(T_{k u}^{i u}\\right)\n$$\n\nand double contraction produces the two zero-order tensors (invariants) $T_{u v}^{u v}$ and $T_{v u}^{u v}$. Thus there are six tensors, in general all distinct.\n\n4.7 Show that any contraction of the tensor $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$ results in a covariant vector.\n\nWe may contract on either $i=j$ or $i=k$. For $\\left(S_{k}\\right) \\equiv\\left(T_{i k}^{i}\\right)$, we have the transformation law\n\n$$\n\\bar{S}_{k} \\equiv \\bar{T}_{i k}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{s t}^{r} \\delta_{r}^{s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{r t}^{r} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=S_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\nand, for $\\left(U_{j}\\right) \\equiv\\left(T_{j i}^{i}\\right)$,\n\n$$\n\\bar{U}_{j} \\equiv \\bar{T}_{j i}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{s t}^{r} \\delta_{r}^{t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s r}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nIn either case, the transformation law is that of a covariant vector.\n\n\\section*{COMBINED OPERATIONS}\n4.8 Suppose that $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ are tensors from which a contravariant vector $\\mathbf{V}=\\left(V^{i}\\right)$ is to be constructed using a combination of outer/inner products and contractions. (a) Show that there are six possibilities for $\\mathbf{V}$, which can all be distinct. (b) Verify that each possible $\\mathbf{V}$ is obtainable as a contraction of an inner product ST.\n\n(a) Writing $[\\mathbf{S T}] \\equiv \\mathbf{U}=\\left(U_{l m}^{i j k}\\right)$, we obtain the contravariant vectors as the double contractions of $\\mathbf{U}$ :\n\n$$\n\\begin{array}{llllll}\n\\left(U_{u v}^{u v k}\\right) & \\left(U_{v u}^{u v k}\\right) & \\left(U_{u v}^{u j v}\\right) & \\left(U_{v u}^{u j v}\\right) & \\left(U_{u v}^{i u v}\\right) & \\left(U_{v u}^{i u v}\\right)\n\\end{array}\n$$\n\n(b) The vector $\\left(U_{u v}^{u v k}\\right) \\equiv\\left(S_{u}^{u v} T_{v}^{k}\\right)$ may be obtained by first taking the inner product $\\left(S_{l}^{i v} T_{v}^{k}\\right)$ and then contracting on $i=u=l$. Likewise for the other five vectors of $(a)$.\n\n\\section*{TESTS FOR TENSOR CHARACTER}\n4.9 Prove criterion (2) of Section 4.2 without invoking the Quotient Theorem.\n\nWe are to verify that $\\left(T_{i j}\\right)$ is a covariant tensor of order two if it is given that for every contravariant vector $\\left(V^{i}\\right), T_{i j} V^{i} \\equiv U_{j}$ are components of a covariant vector. Start out with the transformation law for $\\left(U_{j}\\right)$ [from $\\left(x^{i}\\right)$ to $\\left.\\left(\\bar{x}^{i}\\right)\\right]$ :\n\n$$\n\\bar{U}_{j}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad \\bar{T}_{i j} \\bar{V}^{i}=T_{i s} V^{i} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nNow substitute the transformation law for $\\bar{V}^{i}$ [from $\\left(\\bar{x}^{i}\\right)$ to $\\left.\\left(x^{i}\\right)\\right]$ :\n\n$$\n\\bar{T}_{i j} \\bar{V}^{i}=T_{i s}\\left(\\overline{\\bar{V}}^{p} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{p}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nReplace the dummy index $i$ by $p$ on the left and by $r$ on the right:\n\n$$\n\\bar{T}_{p j} \\bar{V}^{p}=T_{r s} \\bar{V}^{p} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad\\left(\\bar{T}_{p j}-T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right) \\bar{V}^{p}=0\n$$\n\nThe proof is concluded as in Example 4.4.\n\n4.10 Prove criterion (3) of Section 4.2.\n\nHere we must show that $\\left(T_{i j}\\right)$ is a covariant tensor, assuming that $T_{i j} U^{i} V^{j}$ is invariant. Using criterion (1), we conclude that $\\left(T_{i j} U^{i}\\right)$ is a covariant vector. Using criterion (2), it follows that since $\\left(U^{i}\\right)$ is arbitrary, $\\left(T_{i j}\\right)$ is a covariant tensor of order two, the desired conclusion.\n\n4.11 Prove criterion (4) of Section 4.2.\n\nWe wish to show that if $\\left(T_{i j}\\right)$ is a symmetric array such that $T_{i j} V^{i} V^{j}$ is an invariant for every contravariant vector $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a (symmetric) covariant tensor of order two.\n\nLet $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ denote arbitrary contravariant vectors and let $\\left(W^{i}\\right) \\equiv\\left(U^{i}+V^{i}\\right)$, a contravariant vector by $(4.2 a)$. Then,\n\n$$\n\\begin{aligned}\nT_{i j} W^{i} W^{j} & \\equiv T_{i j}\\left(U^{i}+V^{i}\\right)\\left(U^{j}+V^{j}\\right) \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} U^{j}+T_{i j} U^{i} V^{j}+T_{i j} V^{i} V^{j} \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} V^{j}+2 T_{i j} U^{i} V^{j}\n\\end{aligned}\n$$\n\nwhere the symmetry of $\\left(T_{i j}\\right)$ has been used in the last step. Now, by hypothesis, the left-hand side and the first two terms of the right-hand side of the above identity are invariants. Therefore, $T_{i j} U^{i} V^{j}$ must be an invariant, and the desired conclusion follows from criterion (3).\n\n4.12 Use Lemma 4.1 to write a proof of the Quotient Theorem, Theorem 4.2.\n\nIn the notation of the theorem and lemma, $S_{j_{1} j_{2} \\cdots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} \\cdot U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}}$ is a tensor of order zero, or an invariant, for arbitrary $\\mathbf{U}^{(\\alpha)}$ and $\\mathbf{V}_{(\\beta)}$; that is,\n\n$$\nT_{j_{1} j_{2} \\cdots j_{q} k}^{i_{1} i_{2} \\cdots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}} V^{k}\n$$\n\nis an invariant, with $\\left(V^{k}\\right)$ also arbitrary. It then follows from Lemma 4.1 (with $q$ replaced by $q+1$ ) that $\\left(T_{j_{1} j_{2} \\ldots j_{q^{k}}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor, contravariant of order $p$ and covariant of order $q+1$.\n\nFrom the above method of proof, it is clear that the Quotient Theorem is equally valid when the \"divisor\" is an arbitrary covariant vector. This form of the theorem will be used in Problem 4.13.\n\n4.13 Use the Quotient Theorem to prove Theorem 3.2.\n\nIf $\\mathbf{U}=\\left(U^{i}\\right)$ is a contravariant vector, the inner product\n\n$$\n\\mathbf{V}=\\mathbf{T} \\mathbf{U} \\equiv\\left(T_{i j} U^{j}\\right)\n$$\n\nis a covariant vector. Moreover, because $\\left[T_{i j}\\right]_{n n}$ has an inverse, it follows that as $\\mathbf{U}$ runs through all contravariant vectors, $\\mathbf{V}$ runs through all covariant vectors. Thus,\n\n$$\n\\mathbf{U}=\\mathbf{T}^{-1} \\mathbf{V} \\equiv\\left(T^{i l} V_{j}\\right)\n$$\n\nis a tensor for an arbitrary $\\left(V_{i}\\right)$, making $\\left(T^{i j}\\right)$ a contravariant tensor of order two.\n\n\\section*{TENSOR EQUATIONS}\n4.14 Prove that if $\\left(T_{j k l}^{i}\\right)$ is a tensor such that, in the $\\left(x^{i}\\right)$-system, $T_{j k l}^{i}=3 T_{l j k}^{i}$, then $T_{j k l}^{i}=3 T_{l j k}^{i}$ in all coordinate systems.\n\nWe must prove that $\\bar{T}_{j k l}^{i}=3 \\bar{T}_{l j k}^{i}$ in $\\left(\\bar{x}^{i}\\right)$. But\n\n$$\n\\begin{aligned}\n\\bar{T}_{j k l}^{i}-3 \\bar{T}_{l j k}^{i} & =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{t r s}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\\\\n& =\\left(T_{r s t}^{p}-3 T_{t r s}^{p}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{l}}=0\n\\end{aligned}\n$$\n\nas desired.\n\n\\subsection*{4.15 Prove Theorem 4.3.}\nBy Problem 3.14(a), the covariant transformation law has the matrix expression\n\n$$\n\\bar{T}=\\bar{J}^{T} T \\bar{J} \\quad \\text { whence } \\quad|\\bar{T}|=\\overline{\\mathscr{J}}^{2}|T|\n$$\n\nThus, $|T|=0$ implies $|\\bar{T}|=0$.\n\n4.16 Prove that if a mixed tensor $\\left(T_{j}^{i}\\right)$ can be expressed as the outer product of contravariant and covariant vectors $\\left(U^{i}\\right)$ and $\\left(V_{j}\\right)$ in one coordinate system, then $\\left(T_{j}^{i}\\right)$ is the outer product of those vectors in general.\n\nWe must prove that $\\bar{T}_{j}^{i}=\\bar{U}^{i} \\bar{V}_{j}$ for any admissible coordinate system $\\left(\\bar{x}^{i}\\right)$. But, by hypothesis,\n\n$$\n\\bar{T}_{j}^{i}-\\bar{U}^{i} \\bar{V}_{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-\\left(U^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)=\\left(T_{s}^{r}-U^{r} V_{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=0\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n4.17 If $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ are contravariant vectors, verify that $\\left(2 U^{i}+3 V^{i}\\right)$ is also a contravariant vector.\n\n4.18 Verify that the outer product of a contravariant vector and a covariant vector is a mixed tensor of order two.\n\n4.19 How many potentially different mixed tensors of order two can be defined by taking the outer product of $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$, then contracting twice?\n\n4.20 Show that if $T_{k l}^{i j}$ are tensor components, $T_{i j}^{i j}$ is an invariant.\n\n4.21 Prove that if $T_{j k l}^{i} U^{j} \\equiv S_{k l}^{i}$ are components of a tensor for any contravariant vector $\\left(U^{j}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Apply the Quotient Theorem to $\\left(M_{k l j}^{i}\\right) \\equiv\\left(T_{j k l}^{i}\\right)$. More generally, the Quotient Theorem is valid for all choices of the inner product.]\n\n4.22 Prove that if $T_{j k l}^{i} S^{k l} \\equiv U_{j}^{i}$ are tensor components for arbitrary contravariant tensors $\\left(S^{k l}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Follow Problem 4.9.]\n\n4.23 Prove that if $T_{j k l}^{i} U^{k} U^{l} \\equiv V_{j}^{i}$ are components of a tensor for an arbitrary contravariant vector $\\left(U^{i}\\right)$, and if $\\left(T_{j k l}^{i}\\right)$ is symmetric in the last two lower indices in all coordinate systems, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the type indicated.\n\n4.24 Show that Theorem 4.3 and Corollary 4.4 are equivalent.\n\n4.25 Prove the assertion of Example 4.7.\n\n4.26 Prove that if an invariant $E$ can be expressed as the inner product of vectors $\\left(U_{i}\\right)$ and $\\left(V^{i}\\right)$ in one coordinate system, then $E$ has that representation in any coordinate system.\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 5}", "\\section*{The Metric Tensor}\n\\subsection*{5.1 INTRODUCTION}\nThe notion of distance (or metric) is fundamental in applied mathematics. Frequently, the distance concept most useful in a particular application is non-Euclidean (under which the Pythagorean relation for geodesic right triangles is not valid). Tensor calculus provides a natural tool for the investigation of general formulations of distance; it studies not only non-Euclidean metrics but also the forms assumed by the Euclidean metric in particular coordinate systems.\n\nCalculus texts often contain derivations of arc-length formulas for polar coordinates that apparently apply only to that one coordinate system; here we develop a concise method for obtaining the arc-length formula for any admissible coordinate system. The theory culminates in later chapters with a method for distinguishing between a metric that is genuinely non-Euclidean and one that is Euclidean but disguised by the peculiarities of a particular system of coordinates.\n\n\\subsection*{5.2 ARC LENGTH IN EUCLIDEAN SPACE}\nThe classical expressions from calculus for arc length in various coordinate systems lead to a general formula of the type\n\n\n\\begin{equation*}\nL=\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}\\right|} d t \\tag{5.1a}\n\\end{equation*}\n\n\nwhere $g_{i j}=g_{i j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)=g_{j i}$ are functions of the coordinates and $L$ gives the length of the arc $a \\leqq t \\leqq b$ of the curve $x^{i}=x^{i}(t) \\quad(1 \\leqq i \\leqq n)$.\n\nEXAMPLE 5.1 The arc-length formula for Euclidean three-space in a rectangular coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$ may be recalled:\n\n$$\nL=\\int_{a}^{b} \\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(\\frac{d x^{3}}{d t}\\right)^{2}} d t=\\int_{a}^{b} \\sqrt{\\delta_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}} d t\n$$\n\nThis is $(5.1 a)$, with $g_{i j}=\\delta_{i j}$.\n\nThe formula in Example 5.1 has the equally informative differential form\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}=\\delta_{i j} d x^{i} d x^{j}\n$$\n\nMore generally, (5.1a) is equivalent to\n\n\n\\begin{equation*}\n\\pm d s^{2}=g_{i j} d x^{i} d x^{j} \\tag{5.1b}\n\\end{equation*}\n\n\nEXAMPLE 5.2 For convenient reference, formulas for the Euclidean metric in the nonrectangular coordinate systems heretofore considered are collected below.\n\nPolar coordinates: $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$; Fig. 3-1.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\tag{5.2}\n\\end{equation*}\n\n\nCylindrical coordinates: $\\left(x^{1}, x^{2}, x^{3}\\right)=(r, \\theta, z)$; Fig. 3-2.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2} \\tag{5.3}\n\\end{equation*}\n\n\nSpherical coordinates: $\\left(x^{1}, x^{2}, x^{3}\\right)=(\\rho, \\varphi, \\theta)$; Fig. 3-3.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(d x^{3}\\right)^{2} \\tag{5.4}\n\\end{equation*}\n\n\nAffine coordinates: (see Fig. 5-1).\n\n\n\\begin{align*}\nd s^{2}=\\left(d x^{1}\\right)^{2} & +\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2} \\\\\n& +2 \\cos \\alpha d x^{1} d x^{2}+2 \\cos \\beta d x^{1} d x^{3}+2 \\cos \\gamma d x^{2} d x^{3} \\tag{5.5}\n\\end{align*}\n\n\nFormula (5.5) is derived in Problem 5.9. Note that the matrix $\\left(g_{i j}\\right)$ defining the Euclidean metric is nondiagonal in affine coordinates.\n\nAlthough formulated for Euclidean space, (5.1) is extended, in the section that follows, to provide the distance concept for non-Euclidean spaces as well.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-061}\n\\end{center}\n\nFig. 5-1\n\n\\subsection*{5.3 GENERALIZED METRICS; THE METRIC TENSOR}\nAssume that a matrix field $\\mathbf{g}=\\left(g_{i j}\\right)$ exists satisfying in all (admissible) coordinate systems $\\left(x^{i}\\right)$ and in some (open) region of space:\\\\\nA. $\\mathbf{g}$ is of differentiability class $C^{2}$ (i.e., all second-order partial derivatives of the $g_{i j}$ exist and are continuous).\\\\\nB. $\\mathbf{g}$ is symmetric (i.e., $g_{i j}=g_{j i}$ ).\\\\\nC. $\\mathbf{g}$ is nonsingular (i.e., $\\left|g_{i j}\\right| \\neq 0$ ).\n\nD. The differential form $(5.1 \\mathrm{~b})$, and hence the distance concept generated by $\\mathbf{g}$, is invariant with respect to a change of coordinates.\n\nSometimes, particularly in geometric applications of tensors, a property str ger than $\\mathrm{C}$ above is assumed:\n\n$\\mathrm{C}^{\\prime} . \\quad \\mathbf{g}$ is positive definite [i.e., $g_{i j} v^{i} v^{j}>0$ for all nonzero vectors $\\mathbf{v}=\\left(v^{1}, v^{2}, \\ldots, v^{n}\\right)$ ].\n\nUnder property $\\mathrm{C}^{\\prime},\\left|g_{i j}\\right|$ and $g_{11}, g_{22}, \\ldots, g_{n n}$ are all positive. Furthermore, the inverse matrix field $\\mathbf{g}^{-1}$ is also positive definite.\n\nFor later use we define the arc-length parameter for a curve $\\mathscr{C}: x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$ :\n\n\n\\begin{equation*}\ns(t)=\\int_{a}^{t} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}} d u \\tag{5.6a}\n\\end{equation*}\n\n\nwhere $\\varepsilon=+1$ or -1 according as\n\n$$\ng_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u} \\geqq 0 \\quad \\text { or } \\quad g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}<0\n$$\n\nThe functional $\\varepsilon$ is called the indicator of the vector $\\left(d x^{i} / d u\\right)$ relative to the metric $\\left(g_{i j}\\right)$. One can, of course, use absolute value signs instead of the indicator, but the latter notation works better in algebraic manipulations. In terms of the arc-length parameter, the length of $\\mathscr{C}$ is $L=s(b)$.\n\nDifferentiating (5.6a) and squaring yields the equivalent formula\n\n\n\\begin{equation*}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} \\tag{5.6b}\n\\end{equation*}\n\n\nFinally, introducing the differentials\n\n$$\nd x^{i} \\equiv \\frac{d x^{i}(t)}{d t} d t\n$$\n\nthe values of which are independent of the choice of curve parameter, we retrieve $(5.1 b)$ as\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \\tag{5.6c}\n\\end{equation*}\n\n\nEXAMPLE 5.3 Suppose that on $\\mathbf{R}^{3}$ a matrix field is given in $\\left(x^{i}\\right)$ by\n\n$$\n\\left(g_{i j}\\right)=\\left[\\begin{array}{ccc}\n\\left(x^{1}\\right)^{2}-1 & 1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{2} & 0 \\\\\n0 & 0 & \\frac{64}{9}\n\\end{array}\\right] \\quad \\text { where } \\quad\\left[\\left(x^{1}\\right)^{2}-1\\right]\\left(x^{2}\\right)^{2} \\neq 1\n$$\n\n(a) Show that, if extended to all admissible coordinate systems according to the transformation law for covariant tensors, this matrix field is a metric; i.e., it satisfies properties A-D above. (b) For this metric, compute the arc-length parameter and the length of the curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array}{l}\nx^{1}=2 t-1 \\\\\nx^{2}=2 t^{2} \\\\\nx^{3}=t^{3}\n\\end{array} \\quad(0 \\leqq t \\leqq 1)\\right.\n$$\n\n(a) Property A obtains since $g_{i j}$ is a polynomial in $x^{1}$ and $x^{2}$ for each $i$, $j$. Since the matrix $\\left(g_{i j}\\right)$ is symmetric, property B holds. Since\n\n$$\n\\left|g_{i j}\\right|=\\frac{64}{9}\\left|\\begin{array}{cc}\n\\left(x^{1}\\right)^{2}-1 & 1 \\\\\n1 & \\left(x^{2}\\right)^{2}\n\\end{array}\\right|=\\frac{64}{9}\\left\\{\\left(x^{2}\\right)^{2}\\left[\\left(x^{1}\\right)^{2}-1\\right]-1\\right\\} \\neq 0\n$$\n\nproperty C obtains. Property D follows from Problem 4.5.\n\n(b) It is convenient, here and later, to rewrite $(5.6 b)$ as the matrix product\n\n\n\\begin{equation*}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{i}}{d t}\\right)^{T}\\left(g_{i j}\\right)\\left(\\frac{d x^{i}}{d t}\\right) \\tag{5.6d}\n\\end{equation*}\n\n\nAlong the given curve, this becomes\n\n$$\n\\begin{aligned}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2} & =\\left[\\begin{array}{lll}\n2 & 4 t & 3 t^{2}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n(2 t-1)^{2}-1 & 1 & 0 \\\\\n1 & \\left(2 t^{2}\\right)^{2} & 0 \\\\\n0 & 0 & \\frac{64}{9}\n\\end{array}\\right]\\left[\\begin{array}{c}\n2 \\\\\n4 t \\\\\n3 t^{2}\n\\end{array}\\right] \\\\\n& =64 t^{6}+64 t^{4}+16 t^{2}=\\left(8 t^{3}+4 t\\right)^{2}\n\\end{aligned}\n$$\n\nHence, $\\varepsilon=1$ and\n\n$$\ns(t)=\\int_{0}^{t}\\left(8 u^{3}+4 u\\right) d u=\\left[2 u^{4}+2 u^{2}\\right]_{0}^{t}=2 t^{4}+2 t^{2}\n$$\n\nfrom which $L=2(1)^{4}+2(1)^{2}=4$.\n\nThe properties postulated of $\\mathbf{g}$ make it a tensor, the so-called fundamental or metric tensor. In fact, property $D$ ensures that\n\n$$\ng_{i j} V^{i} V^{j} \\equiv E\n$$\n\nis an invariant for every contravariant vector $\\left(V^{i}\\right)=\\left(d x^{i} / d t\\right)$. (By solving an ordinary differential equation, one can exhibit the curve that possesses a given tangent vector.) Then, in view of property B, criterion (4) of Section 4.2 implies\n\nTheorem 5.1: The metric $\\mathbf{g}=\\left(g_{i j}\\right)$ is a covariant tensor of the second order.\n\nIn Problem 3.14(a), the matrix equation $U=J^{T} \\bar{U} J$ was found for the transformation of a second-order covariant tensor $\\mathbf{U}$. If $\\left(\\bar{x}^{i}\\right)$ is a rectangular system and $\\mathbf{U}=\\mathbf{g}$ is the Euclidean metric tensor, then in $\\left(x^{i}\\right), U=G$, and in $\\left(\\bar{x}^{i}\\right), \\bar{U}=\\bar{G}=I$; thus we have proved\n\nTheorem 5.2: If the Jacobian matrix of the transformation from a given coordinate system $\\left(x^{i}\\right)$ to a rectangular system $\\left(\\bar{x}^{i}\\right)$ is $J=\\left(\\partial \\bar{x}^{i} / \\partial x^{j}\\right)$, then the matrix $G \\equiv\\left(g_{i j}\\right)$ of the Euclidean metric tensor in the $\\left(x^{i}\\right)$-system is given by\n\n\n\\begin{equation*}\nG=J^{T} J \\tag{5.7}\n\\end{equation*}\n\n\nRemark 1: Equation (5.7) illustrates the following well-known result of matrix theory: Any symmetric, positive definite matrix $A$ has a nonsingular \"square root\" $C$ such that $A=C^{T} C$.\n\nIt should be emphasized that only the Euclidean metric admits of a representation of the form (5.7). For, by very definition, if $\\mathbf{g}$ is non-Euclidean, there exists no coordinate system $\\left(\\bar{x}^{i}\\right)$ in which $\\bar{G}=I$.\n\nEXAMPLE 5.4 Cylindrical coordinates $\\left(x^{i}\\right)$ and rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ are connected through\n\n$$\n\\bar{x}^{1}=x^{1} \\cos x^{2} \\quad \\bar{x}^{2}=x^{1} \\sin x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\nThus\n\n$$\nJ=\\left[\\begin{array}{ccc}\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0 \\\\\n\\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand the (Euclidean) metric for cylindrical coordinates is given by\n\n$$\n\\begin{aligned}\nG=J^{T} J & =\\left[\\begin{array}{ccc}\n\\cos x^{2} & \\sin x^{2} & 0 \\\\\n-x^{1} \\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0 \\\\\n\\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nor $g_{11}=g_{33}=1, g_{22}=\\left(x^{1}\\right)^{2}$, and $g_{i j}=0$ for $i \\neq j$. These results verify (5.3).\n\nIn spite of the apparent restriction to the Euclidean distance concept, in connection with such results as Theorem 5.2, the reader should keep in mind that one is free to choose as the metric tensor for $\\mathbf{R}^{n}$ any $\\mathbf{g}$ that obeys properties A-D above. For instance, it can be shown by methods to be developed later that the metric chosen in Example 5.3 is non-Euclidean.\n\n\\subsection*{5.4 CONJUGATE METRIC TENSOR; RAISING AND LOWERING INDICES}\nOne of the fundamental concepts of tensor calculus resides in the \"raising\" or \"lowering\" of indices in tensors. If we are given a contravariant vector $\\left(T^{i}\\right)$ and if, for the moment, $\\left(g_{i j}\\right)$ represents any covariant tensor of the second order, then we know (Problem 4.4) that the inner product $\\left(S_{i}\\right)=\\left(g_{i j} T^{j}\\right)$ is a covariant vector. Now, if $\\left(g_{i j}\\right)$ is in fact the metric tensor whereby distance in $\\mathbf{R}^{n}$ is defined, it will prove useful in many contexts to consider $\\left(S_{i}\\right)$ and $\\left(T^{i}\\right)$ as covariant and contravariant aspects of a single notion. Thus, we write $T_{i}$ instead of $S_{i}$ :\n\n$$\nT_{i}=g_{i j} T^{j}\n$$\n\nand say that taking the inner product with the metric tensor has lowered a contravariant index to a covariant index. to\n\nBecause the matrix $\\left(g_{i j}\\right)$ is invertible (property $\\mathrm{C}$ of Section 5.3), the above relation is equivalent\n\n$$\nT^{i}=g^{i j} T_{j}\n$$\n\nwhere $\\left(g^{i j}\\right)=\\left(g_{i j}\\right)^{-1}$; now we say that a covariant index has been raised to a contravariant index.\n\nDefinition 1: The inverse of the fundamental matrix field (metric tensor),\n\n$$\n\\left[g^{i j}\\right]_{n n}=\\left[g_{i j}\\right]_{n n}^{-1}\n$$\n\nis called the conjugate metric tensor.\n\nBoth metric tensors are freely applied to create new, more covariant $(\\mathbf{g})$ or more contravariant $\\left(\\mathbf{g}^{-1}\\right)$ counterparts to given tensors. Thus, starting with the mixed tensor $\\left(T_{k}^{i j}\\right)$,\n\n$$\n\\begin{aligned}\nT^{i j k} & \\equiv g^{i r} T_{r}^{j k} \\\\\nT_{i k}^{j} & \\equiv g_{i r} T_{k}^{j r}\n\\end{aligned}\n$$\n\nand\n\n$$\nT_{i j k} \\equiv g_{i s} T_{j k}^{s} \\equiv g_{i s} g_{j r} T_{k}^{s r} .\n$$\n\n\\subsection*{5.5 GENERALIZED INNER-PRODUCT SPACES}\nSuppose that a metric $\\mathbf{g}$ has been imposed on $\\mathbf{R}^{n}$ and that $\\mathbf{U}$ and $\\mathbf{V}$ are two vectors on the metric space. It is essential to the definition of a geometrically significant inner product UV that its value depend only on the vectors $\\mathbf{U}$ and $\\mathbf{V}$, and not on the particular coordinate system used to specify these vectors. (There are other requirements on an inner product, but they are secondary.) This fact motivates\n\nDefinition 2: To each pair of contravariant vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ is associated the real number\n\n\n\\begin{equation*}\n\\mathbf{U V} \\equiv g_{i j} U^{i} V^{j} \\equiv U^{i} V_{i} \\equiv U_{i} V^{i} \\tag{5.8}\n\\end{equation*}\n\n\ncalled the (generalized) inner product of $\\mathbf{U}$ and $\\mathbf{V}$.\n\nIn similar fashion, the inner product of two covariant vectors is defined as\n\n\n\\begin{equation*}\n\\mathbf{U} \\mathbf{V} \\equiv g^{i j} U_{i} V_{j} \\equiv U^{i} V_{i} \\equiv U_{i} V^{i} \\tag{5.9}\n\\end{equation*}\n\n\nconsistent with (5.8). We therefore have the rule: To obtain the inner product of two vectors of the same type, convert one vector to the opposite type and then take the tensor inner product.\n\nRemark 2: It follows from Problem 4.5-or, more fundamentally, from property D of $\\mathbf{g}$ - that the inner product (5.8) or (5.9) is an invariant, as required.\n\nAccording to (4.2), the set of all contravariant vectors on $\\mathbf{R}^{n}$ is a vector space, as is the set of all covariant vectors on $\\mathbf{R}^{n}$. With an inner product as defined above, these vector spaces become (generalized) inner-product spaces.\n\n\\subsection*{5.6 CONCEPTS OF LENGTH AND ANGLE}\nExpressions (2.7) and (2.8) readily extend to a generalized inner-product space, provided the metric is positive definite. The norm (or length) of an arbitrary vector $\\mathbf{V}=\\left(V^{i}\\right)$ or $\\mathbf{V}=\\left(V_{i}\\right)$ is the nonnegative real number\n\n\n\\begin{equation*}\n\\|\\mathbf{V}\\| \\equiv \\sqrt{\\mathbf{V}^{2}}=\\sqrt{V_{i} V^{i}} \\tag{5.10}\n\\end{equation*}\n\n\nRemark 3: The norm of a vector-and thus the notion of a normed linear space-can be defined abstractly (see Problem 5.14), without reference to an inner product.\n\nEXAMPLE 5.5 Show that under the Euclidean metric (5.2) for polar coordinates, the vectors\n\n$$\n\\left(U^{i}\\right)=\\left(3 / 5,4 / 5 x^{1}\\right) \\quad \\text { and } \\quad\\left(V^{i}\\right)=\\left(-4 / 5,3 / 5 x^{1}\\right)\n$$\n\nare orthonormal.\n\nUsing matrices, we have:\n\n$$\n\\begin{aligned}\n\\|\\mathbf{U}\\|^{2}=U^{i} U_{i}=g_{i j} U^{i} U^{j} & =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\frac{3}{5} \\\\\n\\frac{4}{5 x^{1}}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\frac{3}{5} \\\\\n\\frac{4 x^{1}}{5}\n\\end{array}\\right] \\\\\n& =\\frac{9}{25}+\\frac{16 x^{1}}{25 x^{1}}=1\n\\end{aligned}\n$$\n\nor $\\|\\mathbf{U}\\|=1$; likewise, $\\|\\mathbf{V}\\|=1$. Now we verify that the vectors are orthogonal:\n\n$$\n\\begin{aligned}\n\\mathbf{U V}=g_{i j} U^{i} V^{j} & =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n-\\frac{4}{5} \\\\\n\\frac{3}{5 x^{1}}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{r}\n-\\frac{4}{5} \\\\\n\\frac{3 x^{1}}{5}\n\\end{array}\\right] \\\\\n& =-\\frac{12}{25}+\\frac{12 x^{1}}{25 x^{1}}=0\n\\end{aligned}\n$$\n\nBoth normality and orthogonality depend, of course, on the metric alone, and not on the (polar) coordinate system.\n\nThe angle $\\theta$ between two non-null contravariant vectors $\\mathbf{U}$ and $\\mathbf{V}$ is defined by\n\n\n\\begin{equation*}\n\\cos \\theta \\equiv \\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|}=\\frac{g_{i j} U^{i} V^{j}}{\\sqrt{g_{p q} U^{p} U^{q}} \\sqrt{g_{r s} V^{r} V^{s}}} \\quad(0 \\leqq \\theta \\leqq \\pi) \\tag{5.11}\n\\end{equation*}\n\n\nThat $\\theta$ is well-defined follows from the Cauchy-Schwarz inequality, which may be written in the form\n\n$$\n-1 \\leqq \\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|} \\leqq 1\n$$\n\n\\section*{(see Problem 5.13).}\nThe tangent field to a family of smooth curves is a contravariant vector (Example 3.4), so that (5.11) yields the geometrical\n\nTheorem 5.3: In a general coordinate system, if $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ are the tangent vectors to two families of curves, then the families are mutually orthogonal if and only if $g_{i j} U^{i} V^{j}=0$.\n\nEXAMPLE 5.6 Show that each member of the family of curves given in polar coordinates by\n\n\n\\begin{equation*}\ne^{1 / r}=a(\\sec \\theta+\\tan \\theta) \\quad(a \\geqq 0) \\tag{1}\n\\end{equation*}\n\n\nis orthogonal to each of the curves (lima\u00e7ons of Pascal)\n\n\n\\begin{equation*}\nr=\\sin \\theta+c \\quad(c \\geqq 0) \\tag{2}\n\\end{equation*}\n\n\n(Figure 5-2 indicates the orthogonality of the curve $a=1$ to the family of lima\u00e7ons.)\n\nIn polar coordinates $x^{1}=r, x^{2}=\\theta$, and with curve parameter $t,(1)$ becomes-after taking logarithms-\n\n$$\n\\frac{1}{x^{1}}=\\ln a+\\ln |\\sec t+\\tan t| \\quad x^{2}=t\n$$\n\nWith curve parameter $u$, (2) becomes\n\n$$\nx^{1}=\\sin u+c \\quad x^{2}=u\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-066}\n\\end{center}\n\nFig. 5-2\n\nDifferentiation of $\\left(1^{\\prime}\\right)$ with respect to $t$ yields\n\n$$\n-\\frac{1}{\\left(x^{1}\\right)^{2}} \\frac{d x^{1}}{d t}=\\sec t=\\sec x^{2} \\quad \\frac{d x^{2}}{d t}=1\n$$\n\nso that the tangent vector to family $\\left(1^{\\prime}\\right)$ is\n\n$$\n\\left(U^{1}, U^{2}\\right)=\\left(-\\left(x^{1}\\right)^{2} \\sec x^{2}, 1\\right)\n$$\n\nSimilarly, the tangent vector to family $\\left(2^{\\prime}\\right)$ is\n\n$$\n\\left(V^{1}, V^{2}\\right)=(\\cos u, 1)=\\left(\\cos x^{2}, 1\\right)\n$$\n\nApplying Theorem 5.3, with the Euclidean metric tensor in polar coordinates,\n\n$$\n\\begin{aligned}\ng_{i j} U^{i} V^{j} & =g_{11} U^{1} V^{1}+g_{22} U^{2} V^{2}+0 \\\\\n& =(1)\\left[-\\left(x^{1}\\right)^{2} \\sec x^{2}\\right]\\left(\\cos x^{2}\\right)+\\left(x^{1}\\right)^{2}(1)(1) \\\\\n& \\ddots-\\left(x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}=0\n\\end{aligned}\n$$\n\nObserve that nonparametric forms of the tangent vectors are used in the orthogonality condition. This is necessary because the metric tensor at the intersection point $\\left(x^{1}, x^{2}\\right)$ of a curve (1) and a curve (2) depends on neither the parameter $t$ along (1) nor the parameter $u$ along (2).\n\n\\section*{Solved Problems}\n\\section*{ARC LENGTH}\n5.1 A curve is given in spherical coordinates $\\left(x^{i}\\right)$ by\n\n$$\nx^{1}=t \\quad x^{2}=\\arcsin \\frac{1}{t} \\quad x^{3}=\\sqrt{t^{2}-1}\n$$\n\nFind the length of the arc $1 \\leqq t \\leqq 2$.\n\nBy $(5.4)$,\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}\n$$\n\nso we first calculate the $\\left(d x^{i} / d t\\right)^{2}$ :\n\n$$\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}=1 \\quad\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\left(\\frac{-1 / t^{2}}{\\sqrt{1-(1 / t)^{2}}}\\right)^{2}=\\frac{1}{t^{2}\\left(t^{2}-1\\right)} \\quad\\left(\\frac{d x^{3}}{d t}\\right)^{2}=\\left(\\frac{1}{2} \\frac{2 t}{\\sqrt{t^{2}-1}}\\right)^{2}=\\frac{t^{2}}{t^{2}-1}\n$$\n\nThen\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=1+t^{2} \\cdot \\frac{1}{t^{2}\\left(t^{2}-1\\right)}+\\left(t \\cdot \\frac{1}{t}\\right)^{2} \\cdot \\frac{t^{2}}{t^{2}-1}=\\frac{2 t^{2}}{t^{2}-1}\n$$\n\nand $(5.1 a)$ gives\n\n$$\n\\left.L=\\int_{1}^{2} \\frac{\\sqrt{2} t}{\\sqrt{t^{2}-1}} d t=\\sqrt{2\\left(t^{2}-1\\right)}\\right]_{1}^{2}=\\sqrt{6}\n$$\n\n5.2 Find the length of the curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array}{l}\nx^{1}=1 \\\\\nx^{2}=t\n\\end{array} \\quad(1 \\leqq t \\leqq 2)\\right.\n$$\n\nif the metric is that of the hyperbolic plane $\\left(x^{2}>0\\right)$ :\n\n$$\ng_{11}=g_{22}=\\frac{1}{\\left(x^{2}\\right)^{2}} \\quad g_{12}=g_{21}=0\n$$\n\nSince $\\left(d x^{i} / d t\\right)=(0,1),(5.6 d)$ yields $(\\varepsilon=1)$\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{ll}\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{1}{t^{2}} & 0 \\\\\n0 & \\frac{1}{t^{2}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1\n\\end{array}\\right]=\\frac{1}{t^{2}} \\\\\nL=\\int_{1}^{2} \\frac{1}{t} d t=\\ln 2\n\\end{gathered}\n$$\n\nand\n\n\\section*{GENERALIZED METRICS}\n5.3 Is the form $d x^{2}+3 d x d y+4 d y^{2}+d z^{2}$ positive definite?\n\nIt must be determined whether the polynomial $Q \\equiv\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+4\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}$ is positive unless $u^{1}=u^{2}=u^{3}=0$. By completing the square,\n\n$$\nQ=\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+\\frac{9}{4}\\left(u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}=\\left(u^{1}+\\frac{3}{2} u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}\n$$\n\nAll terms are perfect squares with positive coefficients; hence the form is indeed positive definite.\n\n5.4 Show that the formula (5.1a) for arc length does not depend on the particular parameterization of the curve.\n\nGiven a curve $\\mathscr{C}: x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$, suppose that $x^{i}=x^{i}(\\bar{t}) \\quad(\\bar{a} \\leqq \\bar{t} \\leqq \\bar{b})$ is a different parameterization, where $\\bar{t}=\\phi(t)$, with $\\phi^{\\prime}(t)>0$ and $\\bar{a}=\\phi(a), \\bar{b}=\\phi(b)$. Then, by the chain rule and substitution rule for integrals,\n\n$$\n\\begin{aligned}\nL & =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}\\right|} d t=\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\left(\\phi^{\\prime}(t)\\right)^{2}\\right|} d t \\\\\n& =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} \\phi^{\\prime}(t) d t=\\int_{\\bar{a}}^{\\bar{b}} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} d \\bar{t}=\\bar{L}\n\\end{aligned}\n$$\n\n\\section*{TENSOR PROPERTY OF THE METRIC}\n5.5 Find the Euclidean metric tensor (in matrix form) for spherical coordinates, using Theorem 5.2 .\n\nSince spherical coordinates $\\left(x^{i}\\right)$ are connected to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{-1}=x^{1} \\sin x^{2} \\cos x^{3} \\quad \\bar{x}^{2}=\\bar{x}^{-1} \\sin x^{2} \\sin x^{3} \\quad \\bar{x}^{3}=x^{1} \\cos x^{2}\n$$\n\nwe have\n\n$$\nJ^{T} J=\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & \\sin x^{2} \\sin x^{3} & \\cos x^{2} \\\\\nx^{1} \\cos x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & -x^{1} \\sin x^{2} \\\\\n-x^{1} \\sin x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} & 0\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\cos x^{3} & -x^{1} \\sin x^{2} \\sin x^{3} \\\\\n\\sin x^{2} \\sin x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} \\\\\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0\n\\end{array}\\right]\n$$\n\nSince $G=J^{T} J$ is known to be symmetric (see Problem 2.4), we need only compute the elements on or above the main diagonal:\n\n$$\nG=\\left[\\begin{array}{ccc}\n\\left(\\sin ^{2} x^{2}\\right)(1)+\\cos ^{2} x^{2} & \\left(x^{1} \\sin x^{2} \\cos x^{2}\\right)(1)-x^{1} \\sin x^{2} \\cos x^{2} & g_{13} \\\\\ng_{21} & \\left(\\left(x^{1}\\right)^{2} \\cos ^{2} x^{2}\\right)(1)+\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2} & g_{23} \\\\\ng_{31} & g_{32} & \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)(1)\n\\end{array}\\right]\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& g_{13}=\\left(x^{1} \\sin ^{2} x^{2}\\right)\\left(-\\sin x^{3} \\cos x^{3}+\\cos x^{3} \\sin x^{3}\\right)=0 \\\\\n& g_{23}=\\left(\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}\\right)\\left(-\\cos x^{3} \\sin x^{3}+\\sin x^{3} \\cos x^{3}\\right)=0\n\\end{aligned}\n$$\n\nHence\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right]\n$$\n\n5.6 Find the components $g_{i j}$ of the Euclidean metric tensor in the special coordinate system $\\left(x^{i}\\right)$ defined from rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ by $x^{1}=\\bar{x}^{1}, x^{2}=\\exp \\left(\\bar{x}^{2}-\\bar{x}^{1}\\right)$.\n\nWe must compute $J^{T} J$, where $J$ is the Jacobian matrix of the transformation $\\bar{x}^{i}=\\bar{x}^{i}\\left(x^{1}, x^{2}\\right)$. Thus, we solve the above equations for the $\\bar{x}^{i}$ :\n\nHence\n\n$$\n\\bar{x}^{1}=x^{1} \\quad \\bar{x}^{2}=x^{1}+\\ln x^{2}\n$$\n\n$$\n\\begin{aligned}\n& \\text { Hence } \\quad J=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right] \\\\\n& \\text { and } G=\\left[\\begin{array}{cc}\n1 & 1 \\\\\n0 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & \\left(x^{2}\\right)^{-1} \\\\\n\\left(x^{2}\\right)^{-1} & \\left(x^{2}\\right)^{-2}\n\\end{array}\\right] \\\\\n& \\text { or } g_{11}=2, g_{10}=g_{01}=\\left(x^{2}\\right)^{-1}, g_{23}=\\left(x^{2}\\right)^{-2} \\text {. }\n\\end{aligned}\n$$\n\nor $g_{11}=2, g_{12}=g_{21}=\\left(x^{2}\\right)^{-1}, g_{22}=\\left(x^{2}\\right)^{-2}$.\n\n5.7 (a) Using the metric of Problem 5.6, calculate the length of the curve\n\n$$\n\\mathscr{C}: x^{1}=3 t, \\quad x^{2}=e^{t} \\quad(0 \\leqq t \\leqq 2)\n$$\n\n(b) Interpret geometrically.\n\n(a) First calculate the $d x^{i} / d t$ :\n\nThen\n\n$$\n\\frac{d x^{1}}{d t}=3 \\quad \\frac{d x^{2}}{d t}=e^{t}\n$$\n\nThen\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=2\\left(\\frac{d x^{1}}{d t}\\right)^{2}+2\\left(x^{2}\\right)^{-1}\\left(\\frac{d x^{1}}{d t}\\right)\\left(\\frac{d x^{2}}{d t}\\right)+\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2} \\\\\n=2(9)+2 e^{-t}(3)\\left(e^{t}\\right)+e^{-2 t}\\left(e^{2 t}\\right)=25 \\\\\nL=\\int_{0}^{2} 5 d t=10\n\\end{gathered}\n$$\n\nand\n\n(b) From the transformation equations of Problem 5.6, the curve is described in rectangular coordinates by $\\bar{x}^{2}=\\frac{4}{3} \\bar{x}^{1}$; it is therefore a straight line joining the points which correspond to $t=0$ and $t=2$, or $(0,0)$ and $(6,8)$. The distance from $(0,0)$ to $(6,8)$ is\n\n$$\n\\sqrt{6^{2}+8^{2}}=10\n$$\n\nas found in $(a)$.\n\n5.8 Making use of the Euclidean metric for cylindrical coordinates, (5.3), calculate the length of arc along the circular helix\n\n$$\n\\bar{x}^{1}=a \\cos t \\quad \\bar{x}^{2}=a \\sin t \\quad \\bar{x}^{3}=b t\n$$\n\nwith $a$ and $b$ positive constants, from $t=0$ to $t=c>0$. See Fig. 5-3.\n\nIn cylindrical coordinates $\\left(x^{i}\\right)$, where\n\n$$\n\\bar{x}^{1}=x^{1} \\cos x^{2} \\quad \\bar{x}^{2}=x^{1} \\sin x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\nthe helical arc is represented by the linear equations\n\n$$\n\\begin{gathered}\nx^{1}=a \\quad x^{2}=t \\quad x^{3}=b t \\quad(0 \\leqq t \\leqq c) \\\\\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\nb\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\na^{2} \\\\\nb\n\\end{array}\\right]=a^{2}+b^{2}\n\\end{gathered}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070(1)}\n\\end{center}\n\nFig. 5-3\n\nwhence\n\n$$\nL=\\int_{0}^{c} \\sqrt{a^{2}+b^{2}} d t=c \\sqrt{a^{2}+b^{2}}\n$$\n\n5.9 (Affine Coordinates in $\\mathbf{R}^{3}$ ) Carpenters taking measurements in a room notice that at the corner they had used as reference point the angles were not true. If the actual measures of the angles are as given in Fig. 5-4, what correction in the usual metric formula,\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{\\sum_{i=1}^{3}\\left(x_{1}^{i}-x_{2}^{i}\\right)^{2}}\n$$\n\nshould be made to compensate for the errors?\n\nWe are asked, in effect, to display $\\mathbf{g}=\\left(g_{i j}\\right)$ for three-dimensional affine coordinates $\\left(x^{i}\\right)$. Instead of applying Theorem 5.2, it is much simpler to recall from Problem 3.9 that position vectors are contravariant affine vectors-in particular, the unit vectors\n\n$$\n\\mathbf{u}=\\left(\\delta_{1}^{i}\\right) \\quad \\mathbf{v}=\\left(\\delta_{2}^{i}\\right) \\quad \\mathbf{w}=\\left(\\delta_{3}^{i}\\right)\n$$\n\nalong the oblique axes (Fig. 5-4). We can now use (5.11) in inverse fashion, to obtain:\n\n$$\n\\cos \\alpha=\\frac{g_{i j} \\delta_{1}^{i} \\delta_{2}^{j}}{\\sqrt{g_{p q} \\delta_{1}^{p} \\delta_{1}^{q}} \\sqrt{g_{r s} \\delta_{2}^{r} \\delta_{2}^{s}}}=\\frac{g_{12}}{\\sqrt{g_{11}} \\sqrt{g_{22}}}=g_{12}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070}\n\\end{center}\n\nFig. 5-4\\\\\nsince, obviously, $g_{11}=g_{22}=g_{33}=1 \\quad\\left( \\pm d s=d x^{1}\\right.$ for motion parallel to $\\mathbf{u}$; etc. $)$. Likewise,\n\n$$\n\\cos \\beta=g_{13} \\quad \\cos \\gamma=g_{23}\n$$\n\nand the complete symmetric matrix is\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & \\cos \\alpha & \\cos \\beta \\\\\n\\cos \\alpha & 1 & \\cos \\gamma \\\\\n\\cos \\beta & \\cos \\gamma & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & -0.01745 & -0.00873 \\\\\n-0.01745 & 1 & 0.01745 \\\\\n-0.00873 & 0.01745 & 1\n\\end{array}\\right]\n$$\n\nIt follows that the carpenters must use as the corrected distance formula\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{g_{i j}\\left(x_{1}^{i}-x_{2}^{i}\\right)\\left(x_{1}^{j}-x_{2}^{j}\\right)}\n$$\n\nwhere the $g_{i j}$ have the numerical values given above.\n\n\\section*{RAISING AND LOWERING INDICES}\n5.10 Given that $\\left(V^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{3}$, find its associated covariant vector $\\left(V_{i}\\right)$ in cylindrical coordinates $\\left(x^{i}\\right)$ under the Euclidean metric.\n\nSince\n\n$$\n\\left[g_{i j}\\right]_{33}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand $V_{i}=g_{i r} V^{r}$, we have in matrix form,\n\n$$\n\\left[\\begin{array}{l}\nV_{1} \\\\\nV_{2} \\\\\nV_{3}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nV^{1} \\\\\nV^{2} \\\\\nV^{3}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nV^{1} \\\\\n\\left(x^{1}\\right)^{2} V^{2} \\\\\nV^{3}\n\\end{array}\\right]\n$$\n\n5.11 Show that under orthogonal coordinate changes, starting with any particular system of rectangular coordinates, the raising and lowering of indices has no effect on tensors, consistent with the fact (Section 3.6) that there is no distinction between contravariant and covariant cartesian tensors.\n\nIt suffices to show merely that $g_{i j}=\\delta_{i j}=g^{i j}$ for any admissible coordinate system $\\left(x^{i}\\right)$, for then it will follow that\n\n$$\nT^{i}=\\delta^{i j} T_{j}=T_{i} \\quad T_{i j k}=\\delta_{i r} T_{j k}^{r}=T_{j k}^{i} \\quad T_{j k}^{i}=\\delta_{j r} T_{k}^{i r}=T_{k}^{i j}\n$$\n\nand so on. To that end, simply use formula (5.7), with $J=\\left(a_{i}^{i}\\right)$ an orthogonal matrix. Because $J^{T}=J^{-1}$, we have $G=J^{-1} J=I$, or $g_{i j}=\\delta_{i j}$, as desired. Since $G^{-1}=I^{-1}=I$, it is also the case that $g^{i j}=\\delta_{i j}$.\n\n\\section*{GENERALIZED NORM}\n5.12 Show that the length of any contravariant vector $\\left(V^{i}\\right)$ equals the length of its associated covariant vector $\\left(V_{i}\\right)$.\n\nBy definition,\n\n$$\n\\left\\|\\left(V^{i}\\right)\\right\\|=\\sqrt{g_{i j} V^{i} V^{j}} \\quad \\text { and } \\quad\\left\\|\\left(V_{i}\\right)\\right\\|=\\sqrt{g^{i j} V_{i} V_{j}}\n$$\n\nBut, since $V^{i}=g^{i r} V_{r}$ and $g_{i j}=g_{i i}$,\n\n$$\ng_{i j} V^{i} V^{j}=g_{i j}\\left(g^{i r} V_{r}\\right)\\left(g^{j s} V_{s}\\right)=g_{j i} g^{i r} g^{j s} V_{r} V_{s}=\\delta_{j}^{r} g^{j s} V_{r} V_{s}=g^{r s} V_{r} V_{s}\n$$\n\nand the two lengths are equal.\n\n5.13 Assuming a positive definite metric, show that the basic properties of the cartesian inner product $\\mathbf{U} \\cdot \\mathbf{V}$ are shared by the generalized inner product $\\mathbf{U V}$ of contravariant vectors.\n\n(a) $\\mathbf{U V}=\\mathbf{V U}$ (commutative property). Follows from symmetry of $\\left(g_{i j}\\right)$.\n\n(b) $\\mathbf{U}(\\mathbf{V}+\\mathbf{W})=\\mathbf{U V}+\\mathbf{U W}$ (distributive property). Follows from (1.2).\n\n(c) $\\quad(\\lambda \\mathbf{U}) \\mathbf{V}=\\mathbf{U}(\\lambda \\mathbf{V})=\\lambda(\\mathbf{U V})$ (associative property). Follows from $\\lambda U_{i} V^{i}=U_{i}\\left(\\lambda V^{i}\\right)=\\lambda U_{i} V^{i}$.\n\n(d) $\\mathbf{U}^{2} \\geqq 0$ with equality only if $\\mathbf{U}=\\mathbf{0}$ (positive-definiteness). Follows from the assumed positivedefiniteness of $\\left(g_{i j}\\right)$.\n\n(e) ( $\\mathbf{U V})^{2} \\leqq\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right)$ (Cauchy-Schwarz inequality). This may be derived from the other properties, as follows. If $\\mathbf{U}=\\mathbf{0}$, the inequality clearly holds. If $\\mathbf{U} \\neq \\mathbf{0}$, property ( $d$ ) ensures that the quadratic polynomial\n\n$$\nQ(\\lambda) \\equiv(\\lambda \\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2} \\lambda^{2}+2 \\mathbf{U} \\mathbf{V} \\lambda+\\mathbf{V}^{2}\n$$\n\nvanishes for at most one real value of $\\lambda$. Thus, the discriminant of $Q$ cannot be positive:\n\n$$\n(\\mathbf{U V})^{2}-\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right) \\leqq 0\n$$\n\nand this is the desired inequality.\n\n5.14 A generalized norm on a vector space is any real-valued functional $\\phi[]$ that satisfies\n\n(i) $\\phi[\\mathbf{V}] \\geqq 0$, with equality only if $\\mathbf{V}=\\mathbf{0}$;\n\n(ii) $\\phi[\\lambda \\mathbf{V}]=|\\lambda| \\phi[\\mathbf{V}]$;\n\n(iii) $\\phi[\\mathbf{U}+\\mathbf{V}] \\leqq \\phi[\\mathbf{U}]+\\phi[\\mathbf{V}]$ (triangle inequality).\n\nVerify these conditions for $\\phi[\\mathbf{V}]=\\|\\mathbf{V}\\|$, the inner-product norm under a positive definite metric.\n\n(i) and (ii) for $\\|\\mathbf{V}\\|$ are evident. As for (iii), the Cauchy-Schwarz inequality gives\n\n$$\n\\begin{aligned}\n\\|\\mathbf{U}+\\mathbf{V}\\|^{2} & =(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U} \\mathbf{V} \\\\\n& \\leqq\\|\\mathbf{U}\\|^{2}+\\|\\mathbf{V}\\|^{2}+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|=(\\|\\mathbf{U}\\|+\\|\\mathbf{V}\\|)^{2}\n\\end{aligned}\n$$\n\nfrom which (iii) follows at once.\n\n\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.15 Show that the angle between contravariant vectors is an invariant under a change of coordinate systems.\n\nThe defining expression (5.1) involves only inner products, which are invariants.\n\n5.16 In $\\mathbf{R}^{2}$ the family of curves $x^{2}=x^{1}-c$ (parameterized as $x^{1}=t, x^{2}=t-c$ ), has as its system of tangent vectors the vector field $\\mathbf{U}=(1,1)$, constant throughout $\\mathbf{R}^{2}$. If $\\left(x^{i}\\right)$ represent polar coordinates, find the family of orthogonal trajectories, and interpret geometrically.\n\nThe metric is given by\n\n$$\ng=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\nso, by Theorem 5.3, the orthogonality condition becomes\n\n$$\ng_{i j} U^{i} \\frac{d x^{j}}{d u}=(1)(1) \\frac{d x^{1}}{d u}+\\left(x^{1}\\right)^{2}(1) \\frac{d x^{2}}{d u}=0\n$$\n\nor, eliminating the differential $d u$,\n\n$$\nd x^{1}+\\left(x^{1}\\right)^{2} d x^{2}=0\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-073}\n\\end{center}\n\nFig. 5-5\n\nThis is a variables-separable differential equation, whose solution is\n\n$$\nx^{1}=\\frac{1}{x^{2}+d}\n$$\n\nThe given family of curves in the usual polar-coordinate notation is $r=\\theta+c$, which is a family of concentric spirals (solid curves in Fig. 5-5). The orthogonal trajectories,\n\n$$\nr=\\frac{1}{\\theta+d}\n$$\n\nare also spirals, each having an asymptote parallel to the line $\\theta=-d$; these are the dashed curves in Fig. 5-5.\n\nNote: To solve this problem in rectangular coordinates-that is, to find the orthogonal trajectories of the family\n\n$$\n\\frac{y}{x}=\\tan \\left(\\sqrt{x^{2}+y^{2}}-c\\right)\n$$\n\nunder the metric $\\left(g_{i j}\\right)=\\left(\\delta_{i j}\\right)$-would be difficult or impossible. Quite often, the complication of the metric involved in going over to a specialized curvilinear coordinate system is vastly outweighed by the degree to which the problem is simplified.\n\n5.17 Find the condition for two curves on a sphere of radius $a$ to be orthogonal, if the curves are represented in spherical coordinates by\n\n$$\n\\mathscr{C}_{1}: \\theta=f(\\varphi) \\quad \\text { and } \\quad \\mathscr{C}_{2}: \\theta=g(\\varphi)\n$$\n\nThe two curves can be parameterized in spherical coordinates $\\left(x^{i}\\right) \\equiv(\\rho, \\varphi, \\theta)$ by\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=t \\\\\n\\theta=f(t)\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=u \\\\\n\\theta=g(u)\n\\end{array}\\right.\\right.\n$$\n\nAt an intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$ the tangent vectors of $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ are, respectively,\n\n$$\n\\mathbf{U}=\\left(0,1, f^{\\prime}\\left(\\varphi_{0}\\right)\\right) \\quad \\text { and } \\quad \\mathbf{V}=\\left(0,1, g^{\\prime}\\left(\\varphi_{0}\\right)\\right)\n$$\n\nThese are orthogonal if and only if $g_{i j} U^{i} V^{j}=0$, or\n\n$$\n\\begin{aligned}\n0 & =\\left[\\begin{array}{lll}\n0 & 1 & f^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & \\left(a \\sin \\varphi_{0}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\ng^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right] \\\\\n& =0+a^{2}+\\left(a \\sin \\varphi_{0}\\right)^{2} f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=\\left(a^{2} \\sin ^{2} \\varphi_{0}\\right)\\left[\\csc ^{2} \\varphi_{0}+f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nHence, the desired criterion is that $f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=-\\csc ^{2} \\varphi_{0}$ at any intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$.\n\n5.18 Show that the contravariant vectors $\\mathbf{U}=\\left(0,1,2 b x^{2}\\right)$ and $\\mathbf{V}=\\left(0,-2 b x^{2},\\left(x^{1}\\right)^{2}\\right)$ are orthogonal under the Euclidean metric for cylindrical coordinates. Interpret geometrically along $x^{1}=a$, $x^{2}=t, x^{3}=b t^{2}$.\n\n$$\n\\begin{aligned}\ng_{i j} U^{i} V^{j} & =\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2}\\left(x^{1}\\right)^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right] \\\\\n& =0-2 b x^{2}\\left(x^{1}\\right)^{2}+2 b x^{2}\\left(x^{1}\\right)^{2}=0\n\\end{aligned}\n$$\n\nThe geometric interpretation is that $x^{1}=a, x^{2}=t, x^{3}=b t^{2}$, for real $t$, represents a sort of variable-pitch helix on the right circular cylinder $r=a$, having tangent field $\\mathbf{U}$. Therefore, any solution of\n\n\n\\begin{equation*}\n\\underbrace{\\frac{d x^{1}}{d u}=V^{1}=0}_{\\text {or } x^{1}=a} \\quad \\frac{d x^{2}}{d u}=V^{2}=-2 b x^{2} \\quad \\frac{d x^{3}}{d u}=V^{3}=a^{2} \\tag{1}\n\\end{equation*}\n\n\nwill represent a curve on that cylinder that is orthogonal to this pseudo-helix. See Problem 5.28.\n\n5.19 Show that in any coordinate system $\\left(x^{i}\\right)$ the contravariant vector (recall Problem 3.5) $\\mathbf{V} \\equiv\\left(g^{i \\alpha}\\right)$ is normal to the surface $x^{\\alpha}=$ const. $(\\alpha=1,2, \\ldots, n)$.\n\nBeing \"normal to a surface at the surface point $P$ \" means being orthogonal, at $P$, to the tangent vector of any curve lying in the surface and passing through $P$. Now, for the surface $x^{\\alpha}=$ const., any such tangent vector $\\mathbf{T}$ has as its $\\alpha$ th component\n\n$$\nT^{\\alpha}=\\frac{d x^{\\alpha}}{d t}=0\n$$\n\nWe then have:\n\n$$\n\\mathbf{V T} \\equiv g_{i j} V^{i} T^{j}=g_{i j} g^{i \\alpha} T^{j}=g_{j i} g^{i \\alpha} T^{j}=\\delta_{j}^{\\alpha} T^{j}=T^{\\alpha}=0\n$$\n\nand the proof is complete.\n\n5.20 Show that in any coordinate system $\\left(x^{i}\\right)$, the angle $\\theta$ between the normals to the surfaces $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. is given by\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}} \\quad \\text { (no sum) } \\tag{1}\n\\end{equation*}\n\n\nBy Problem 5.19, $\\mathbf{U}=\\left(g^{i \\alpha}\\right)$ and $\\mathbf{V}=\\left(g^{i \\beta}\\right)$ are the respective normals to $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. Therefore, by the definition (5.11)\n\n$$\n\\cos \\theta=\\frac{U V}{\\|U\\|\\|V\\|}=\\frac{g_{i j} g^{i \\alpha} g^{j \\beta}}{\\sqrt{g_{p q} g^{p \\alpha} g^{q \\alpha}} \\sqrt{g_{r s} g^{r \\beta} g^{s \\beta}}}=\\frac{\\delta_{j}^{\\alpha} g^{j \\beta}}{\\sqrt{\\delta_{q}^{\\alpha} g^{q \\alpha}} \\sqrt{\\delta_{s}^{\\beta} g^{s \\beta}}}=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}}\n$$\n\nIn consequence of (1), orthogonal coordinates are defined as those coordinate systems $\\left(x^{i}\\right)$ relative to which, at all points, $g^{i j}=0 \\quad(i \\neq j)$, or, equivalently, $g_{i j}=0 \\quad(i \\neq j)$. Obviously, orthogonal coordinates need not be rectangular: witness polar, cylindrical, and spherical coordinates.\n\n\\section*{Supplementary Problems}\n5.21 Using the Euclidean metric for polar coordinates, compute the length of arc for the curve\n\n$$\n\\mathscr{C}: x^{1}=2 a \\cos t, \\quad x^{2}=t \\quad(0 \\leqq t \\leqq \\pi / 2)\n$$\n\nand interpret geometrically.\n\n5.22 Is the form $Q\\left(u^{1}, u^{2}, u^{3}\\right) \\equiv 8\\left(u^{1}\\right)^{2}+\\left(u^{2}\\right)^{2}-6 u^{1} u^{3}+\\left(u^{3}\\right)^{2}$ positive definite?\n\n5.23 Using the metric\n\n$$\nG=\\left[\\begin{array}{rrc}\n12 & 4 & 0 \\\\\n4 & 1 & 1 \\\\\n0 & 1 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\ncalculate the length of the curve given by $x^{1}=3-t, x^{2}=6 t+3, x^{3}=\\ln t$, where $1 \\leqq t \\leqq e$.\n\n5.24 A draftsman calculated several distances between points on his drawing using a set of vertical lines and his $\\mathrm{T}$-square. He obtained the distance from $(1,2)$ to $(4,6)$ the usual way:\n\n$$\n\\sqrt{(4-1)^{2}+(6-2)^{2}}=5\n$$\n\nThen he noticed his T-square was out several degrees, throwing off all measurements. An accurate reading showed his T-square measured $95.8^{\\circ}$. Find, to three decimal places, the error committed in his calculations for the answer 5 obtained above. [Hint: Use Problem 5.9 in the special case $x_{1}^{3}=x_{2}^{3}=0$, with $\\alpha=95.8^{\\circ}$.]\n\n5.25 In curvilinear coordinates $\\left(x^{i}\\right)$, show that the contravariant vectors\n\n$$\n\\mathbf{U}=\\left(-x^{1} / x^{2}, 1,0\\right) \\quad \\mathbf{V}=\\left(1 / x^{2}, 0,0\\right)\n$$\n\nare an orthonormal pair, if $\\left(x^{i}\\right)$ is related to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=x^{2} \\quad \\bar{x}^{2}=x^{3} \\quad \\bar{x}^{3}=x^{1} x^{2}\n$$\n\nwhere $x^{2} \\neq 0$.\n\n5.26 Express in $\\left(x^{i}\\right)$ the covariant vectors associated with $\\mathbf{U}$ and $\\mathbf{V}$ of Problem 5.25.\n\n5.27 Even though $\\left(g_{i j}\\right)$ may define a non-Euclidean metric, prove that the norm (5.10) still obeys the following \"Euclidean\" laws: $(a)$ the law of cosines, $(b)$ the Pythagorean theorem.\n\n5.28 (a) Solve system (1) of Problem 5.18. (b) Does the solution found in (a) include all curves orthogonal to the given pseudo-helix? Explain.\n\n5.29 Find the family of orthogonal trajectories in polar coordinates for the family of spirals $x^{1}=c x^{2} \\quad(c=$ const.). [Hint: Parameterize the family as $x^{1}=c e^{t}, x^{2}=e^{t}$.]\n\n5.30 Find the condition for two curves, $z=f(\\theta)$ and $z=g(\\theta)$, on a right circular cylinder of radius $a$ to be orthogonal.\n\n5.31 Let $\\left(x^{i}\\right)$ be any coordinate system and $\\left(g_{i j}\\right)$ any positive definite metric tensor realized in that system. Define the coordinate axes as the curves $\\mathscr{C}_{\\alpha}: x^{i}=t \\delta_{\\alpha}^{i} \\quad(\\alpha=1,2, \\ldots n)$. Show that the angle $\\phi$ between the coordinate axes $\\mathscr{C}_{\\alpha}$ and $\\mathscr{C}_{\\beta}$ satisfies the relation\n\n$$\n\\cos \\phi=\\frac{g_{\\alpha \\beta}}{\\sqrt{g_{\\alpha \\alpha}} \\sqrt{g_{\\beta \\beta}}} \\quad \\text { (no sum) }\n$$\n\nand is thus distinct, in general, from the angle $\\theta$ of Problem 5.20.\n\n5.32 Refer to Problems 5.20 and 5.31. (a) What property must the metric tensor $\\left(g_{i j}\\right)$ possess in $\\left(x^{i}\\right)$ for the coordinate axis $\\mathscr{C}_{\\alpha}$ to be normal to the surface $x^{\\alpha}=$ const. (in which case $\\theta=\\phi$ )? $(b)$ Show that the property of $(a)$ is equivalent to the mutual orthogonality of the coordinate axes.\n\n5.33 Under the metric\n\n$$\nG=\\left[\\begin{array}{cc}\n1 & \\cos 2 x^{2} \\\\\n\\cos 2 x^{2} & 1\n\\end{array}\\right] \\quad\\left(2 x^{2} / \\pi \\text { nonintegral }\\right)\n$$\n\ncompute the norm of the vector $\\mathbf{V}=\\left(d x^{i} / d t\\right)$ evaluated along the curve $x^{1}=-\\sin 2 t, x^{2}=t$, and use it to find the arc length between $t=0$ and $t=\\pi / 2$.\n\n5.34 Under the Euclidean metric for spherical coordinates, (5.4), determine a particular family of curves that intersect\n\n$$\nx^{1}=a \\quad x^{2}=b t \\quad x^{3}=t\n$$\n\northogonally. (Cf. Problem 5.28.)\n\n"], "lesson": "\\section*{Chapter 5}\n\\section*{The Metric Tensor}\n\\subsection*{5.1 INTRODUCTION}\nThe notion of distance (or metric) is fundamental in applied mathematics. Frequently, the distance concept most useful in a particular application is non-Euclidean (under which the Pythagorean relation for geodesic right triangles is not valid). Tensor calculus provides a natural tool for the investigation of general formulations of distance; it studies not only non-Euclidean metrics but also the forms assumed by the Euclidean metric in particular coordinate systems.\n\nCalculus texts often contain derivations of arc-length formulas for polar coordinates that apparently apply only to that one coordinate system; here we develop a concise method for obtaining the arc-length formula for any admissible coordinate system. The theory culminates in later chapters with a method for distinguishing between a metric that is genuinely non-Euclidean and one that is Euclidean but disguised by the peculiarities of a particular system of coordinates.\n\n\\subsection*{5.2 ARC LENGTH IN EUCLIDEAN SPACE}\nThe classical expressions from calculus for arc length in various coordinate systems lead to a general formula of the type\n\n\n\\begin{equation*}\nL=\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}\\right|} d t \\tag{5.1a}\n\\end{equation*}\n\n\nwhere $g_{i j}=g_{i j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)=g_{j i}$ are functions of the coordinates and $L$ gives the length of the arc $a \\leqq t \\leqq b$ of the curve $x^{i}=x^{i}(t) \\quad(1 \\leqq i \\leqq n)$.\n\nEXAMPLE 5.1 The arc-length formula for Euclidean three-space in a rectangular coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$ may be recalled:\n\n$$\nL=\\int_{a}^{b} \\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(\\frac{d x^{3}}{d t}\\right)^{2}} d t=\\int_{a}^{b} \\sqrt{\\delta_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}} d t\n$$\n\nThis is $(5.1 a)$, with $g_{i j}=\\delta_{i j}$.\n\nThe formula in Example 5.1 has the equally informative differential form\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}=\\delta_{i j} d x^{i} d x^{j}\n$$\n\nMore generally, (5.1a) is equivalent to\n\n\n\\begin{equation*}\n\\pm d s^{2}=g_{i j} d x^{i} d x^{j} \\tag{5.1b}\n\\end{equation*}\n\n\nEXAMPLE 5.2 For convenient reference, formulas for the Euclidean metric in the nonrectangular coordinate systems heretofore considered are collected below.\n\nPolar coordinates: $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$; Fig. 3-1.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\tag{5.2}\n\\end{equation*}\n\n\nCylindrical coordinates: $\\left(x^{1}, x^{2}, x^{3}\\right)=(r, \\theta, z)$; Fig. 3-2.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2} \\tag{5.3}\n\\end{equation*}\n\n\nSpherical coordinates: $\\left(x^{1}, x^{2}, x^{3}\\right)=(\\rho, \\varphi, \\theta)$; Fig. 3-3.\n\n\n\\begin{equation*}\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(d x^{3}\\right)^{2} \\tag{5.4}\n\\end{equation*}\n\n\nAffine coordinates: (see Fig. 5-1).\n\n\n\\begin{align*}\nd s^{2}=\\left(d x^{1}\\right)^{2} & +\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2} \\\\\n& +2 \\cos \\alpha d x^{1} d x^{2}+2 \\cos \\beta d x^{1} d x^{3}+2 \\cos \\gamma d x^{2} d x^{3} \\tag{5.5}\n\\end{align*}\n\n\nFormula (5.5) is derived in Problem 5.9. Note that the matrix $\\left(g_{i j}\\right)$ defining the Euclidean metric is nondiagonal in affine coordinates.\n\nAlthough formulated for Euclidean space, (5.1) is extended, in the section that follows, to provide the distance concept for non-Euclidean spaces as well.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-061}\n\\end{center}\n\nFig. 5-1\n\n\\subsection*{5.3 GENERALIZED METRICS; THE METRIC TENSOR}\nAssume that a matrix field $\\mathbf{g}=\\left(g_{i j}\\right)$ exists satisfying in all (admissible) coordinate systems $\\left(x^{i}\\right)$ and in some (open) region of space:\\\\\nA. $\\mathbf{g}$ is of differentiability class $C^{2}$ (i.e., all second-order partial derivatives of the $g_{i j}$ exist and are continuous).\\\\\nB. $\\mathbf{g}$ is symmetric (i.e., $g_{i j}=g_{j i}$ ).\\\\\nC. $\\mathbf{g}$ is nonsingular (i.e., $\\left|g_{i j}\\right| \\neq 0$ ).\n\nD. The differential form $(5.1 \\mathrm{~b})$, and hence the distance concept generated by $\\mathbf{g}$, is invariant with respect to a change of coordinates.\n\nSometimes, particularly in geometric applications of tensors, a property str ger than $\\mathrm{C}$ above is assumed:\n\n$\\mathrm{C}^{\\prime} . \\quad \\mathbf{g}$ is positive definite [i.e., $g_{i j} v^{i} v^{j}>0$ for all nonzero vectors $\\mathbf{v}=\\left(v^{1}, v^{2}, \\ldots, v^{n}\\right)$ ].\n\nUnder property $\\mathrm{C}^{\\prime},\\left|g_{i j}\\right|$ and $g_{11}, g_{22}, \\ldots, g_{n n}$ are all positive. Furthermore, the inverse matrix field $\\mathbf{g}^{-1}$ is also positive definite.\n\nFor later use we define the arc-length parameter for a curve $\\mathscr{C}: x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$ :\n\n\n\\begin{equation*}\ns(t)=\\int_{a}^{t} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}} d u \\tag{5.6a}\n\\end{equation*}\n\n\nwhere $\\varepsilon=+1$ or -1 according as\n\n$$\ng_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u} \\geqq 0 \\quad \\text { or } \\quad g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}<0\n$$\n\nThe functional $\\varepsilon$ is called the indicator of the vector $\\left(d x^{i} / d u\\right)$ relative to the metric $\\left(g_{i j}\\right)$. One can, of course, use absolute value signs instead of the indicator, but the latter notation works better in algebraic manipulations. In terms of the arc-length parameter, the length of $\\mathscr{C}$ is $L=s(b)$.\n\nDifferentiating (5.6a) and squaring yields the equivalent formula\n\n\n\\begin{equation*}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} \\tag{5.6b}\n\\end{equation*}\n\n\nFinally, introducing the differentials\n\n$$\nd x^{i} \\equiv \\frac{d x^{i}(t)}{d t} d t\n$$\n\nthe values of which are independent of the choice of curve parameter, we retrieve $(5.1 b)$ as\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \\tag{5.6c}\n\\end{equation*}\n\n\nEXAMPLE 5.3 Suppose that on $\\mathbf{R}^{3}$ a matrix field is given in $\\left(x^{i}\\right)$ by\n\n$$\n\\left(g_{i j}\\right)=\\left[\\begin{array}{ccc}\n\\left(x^{1}\\right)^{2}-1 & 1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{2} & 0 \\\\\n0 & 0 & \\frac{64}{9}\n\\end{array}\\right] \\quad \\text { where } \\quad\\left[\\left(x^{1}\\right)^{2}-1\\right]\\left(x^{2}\\right)^{2} \\neq 1\n$$\n\n(a) Show that, if extended to all admissible coordinate systems according to the transformation law for covariant tensors, this matrix field is a metric; i.e., it satisfies properties A-D above. (b) For this metric, compute the arc-length parameter and the length of the curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array}{l}\nx^{1}=2 t-1 \\\\\nx^{2}=2 t^{2} \\\\\nx^{3}=t^{3}\n\\end{array} \\quad(0 \\leqq t \\leqq 1)\\right.\n$$\n\n(a) Property A obtains since $g_{i j}$ is a polynomial in $x^{1}$ and $x^{2}$ for each $i$, $j$. Since the matrix $\\left(g_{i j}\\right)$ is symmetric, property B holds. Since\n\n$$\n\\left|g_{i j}\\right|=\\frac{64}{9}\\left|\\begin{array}{cc}\n\\left(x^{1}\\right)^{2}-1 & 1 \\\\\n1 & \\left(x^{2}\\right)^{2}\n\\end{array}\\right|=\\frac{64}{9}\\left\\{\\left(x^{2}\\right)^{2}\\left[\\left(x^{1}\\right)^{2}-1\\right]-1\\right\\} \\neq 0\n$$\n\nproperty C obtains. Property D follows from Problem 4.5.\n\n(b) It is convenient, here and later, to rewrite $(5.6 b)$ as the matrix product\n\n\n\\begin{equation*}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{i}}{d t}\\right)^{T}\\left(g_{i j}\\right)\\left(\\frac{d x^{i}}{d t}\\right) \\tag{5.6d}\n\\end{equation*}\n\n\nAlong the given curve, this becomes\n\n$$\n\\begin{aligned}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2} & =\\left[\\begin{array}{lll}\n2 & 4 t & 3 t^{2}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n(2 t-1)^{2}-1 & 1 & 0 \\\\\n1 & \\left(2 t^{2}\\right)^{2} & 0 \\\\\n0 & 0 & \\frac{64}{9}\n\\end{array}\\right]\\left[\\begin{array}{c}\n2 \\\\\n4 t \\\\\n3 t^{2}\n\\end{array}\\right] \\\\\n& =64 t^{6}+64 t^{4}+16 t^{2}=\\left(8 t^{3}+4 t\\right)^{2}\n\\end{aligned}\n$$\n\nHence, $\\varepsilon=1$ and\n\n$$\ns(t)=\\int_{0}^{t}\\left(8 u^{3}+4 u\\right) d u=\\left[2 u^{4}+2 u^{2}\\right]_{0}^{t}=2 t^{4}+2 t^{2}\n$$\n\nfrom which $L=2(1)^{4}+2(1)^{2}=4$.\n\nThe properties postulated of $\\mathbf{g}$ make it a tensor, the so-called fundamental or metric tensor. In fact, property $D$ ensures that\n\n$$\ng_{i j} V^{i} V^{j} \\equiv E\n$$\n\nis an invariant for every contravariant vector $\\left(V^{i}\\right)=\\left(d x^{i} / d t\\right)$. (By solving an ordinary differential equation, one can exhibit the curve that possesses a given tangent vector.) Then, in view of property B, criterion (4) of Section 4.2 implies\n\nTheorem 5.1: The metric $\\mathbf{g}=\\left(g_{i j}\\right)$ is a covariant tensor of the second order.\n\nIn Problem 3.14(a), the matrix equation $U=J^{T} \\bar{U} J$ was found for the transformation of a second-order covariant tensor $\\mathbf{U}$. If $\\left(\\bar{x}^{i}\\right)$ is a rectangular system and $\\mathbf{U}=\\mathbf{g}$ is the Euclidean metric tensor, then in $\\left(x^{i}\\right), U=G$, and in $\\left(\\bar{x}^{i}\\right), \\bar{U}=\\bar{G}=I$; thus we have proved\n\nTheorem 5.2: If the Jacobian matrix of the transformation from a given coordinate system $\\left(x^{i}\\right)$ to a rectangular system $\\left(\\bar{x}^{i}\\right)$ is $J=\\left(\\partial \\bar{x}^{i} / \\partial x^{j}\\right)$, then the matrix $G \\equiv\\left(g_{i j}\\right)$ of the Euclidean metric tensor in the $\\left(x^{i}\\right)$-system is given by\n\n\n\\begin{equation*}\nG=J^{T} J \\tag{5.7}\n\\end{equation*}\n\n\nRemark 1: Equation (5.7) illustrates the following well-known result of matrix theory: Any symmetric, positive definite matrix $A$ has a nonsingular \"square root\" $C$ such that $A=C^{T} C$.\n\nIt should be emphasized that only the Euclidean metric admits of a representation of the form (5.7). For, by very definition, if $\\mathbf{g}$ is non-Euclidean, there exists no coordinate system $\\left(\\bar{x}^{i}\\right)$ in which $\\bar{G}=I$.\n\nEXAMPLE 5.4 Cylindrical coordinates $\\left(x^{i}\\right)$ and rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ are connected through\n\n$$\n\\bar{x}^{1}=x^{1} \\cos x^{2} \\quad \\bar{x}^{2}=x^{1} \\sin x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\nThus\n\n$$\nJ=\\left[\\begin{array}{ccc}\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0 \\\\\n\\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand the (Euclidean) metric for cylindrical coordinates is given by\n\n$$\n\\begin{aligned}\nG=J^{T} J & =\\left[\\begin{array}{ccc}\n\\cos x^{2} & \\sin x^{2} & 0 \\\\\n-x^{1} \\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0 \\\\\n\\sin x^{2} & x^{1} \\cos x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nor $g_{11}=g_{33}=1, g_{22}=\\left(x^{1}\\right)^{2}$, and $g_{i j}=0$ for $i \\neq j$. These results verify (5.3).\n\nIn spite of the apparent restriction to the Euclidean distance concept, in connection with such results as Theorem 5.2, the reader should keep in mind that one is free to choose as the metric tensor for $\\mathbf{R}^{n}$ any $\\mathbf{g}$ that obeys properties A-D above. For instance, it can be shown by methods to be developed later that the metric chosen in Example 5.3 is non-Euclidean.\n\n\\subsection*{5.4 CONJUGATE METRIC TENSOR; RAISING AND LOWERING INDICES}\nOne of the fundamental concepts of tensor calculus resides in the \"raising\" or \"lowering\" of indices in tensors. If we are given a contravariant vector $\\left(T^{i}\\right)$ and if, for the moment, $\\left(g_{i j}\\right)$ represents any covariant tensor of the second order, then we know (Problem 4.4) that the inner product $\\left(S_{i}\\right)=\\left(g_{i j} T^{j}\\right)$ is a covariant vector. Now, if $\\left(g_{i j}\\right)$ is in fact the metric tensor whereby distance in $\\mathbf{R}^{n}$ is defined, it will prove useful in many contexts to consider $\\left(S_{i}\\right)$ and $\\left(T^{i}\\right)$ as covariant and contravariant aspects of a single notion. Thus, we write $T_{i}$ instead of $S_{i}$ :\n\n$$\nT_{i}=g_{i j} T^{j}\n$$\n\nand say that taking the inner product with the metric tensor has lowered a contravariant index to a covariant index. to\n\nBecause the matrix $\\left(g_{i j}\\right)$ is invertible (property $\\mathrm{C}$ of Section 5.3), the above relation is equivalent\n\n$$\nT^{i}=g^{i j} T_{j}\n$$\n\nwhere $\\left(g^{i j}\\right)=\\left(g_{i j}\\right)^{-1}$; now we say that a covariant index has been raised to a contravariant index.\n\nDefinition 1: The inverse of the fundamental matrix field (metric tensor),\n\n$$\n\\left[g^{i j}\\right]_{n n}=\\left[g_{i j}\\right]_{n n}^{-1}\n$$\n\nis called the conjugate metric tensor.\n\nBoth metric tensors are freely applied to create new, more covariant $(\\mathbf{g})$ or more contravariant $\\left(\\mathbf{g}^{-1}\\right)$ counterparts to given tensors. Thus, starting with the mixed tensor $\\left(T_{k}^{i j}\\right)$,\n\n$$\n\\begin{aligned}\nT^{i j k} & \\equiv g^{i r} T_{r}^{j k} \\\\\nT_{i k}^{j} & \\equiv g_{i r} T_{k}^{j r}\n\\end{aligned}\n$$\n\nand\n\n$$\nT_{i j k} \\equiv g_{i s} T_{j k}^{s} \\equiv g_{i s} g_{j r} T_{k}^{s r} .\n$$\n\n\\subsection*{5.5 GENERALIZED INNER-PRODUCT SPACES}\nSuppose that a metric $\\mathbf{g}$ has been imposed on $\\mathbf{R}^{n}$ and that $\\mathbf{U}$ and $\\mathbf{V}$ are two vectors on the metric space. It is essential to the definition of a geometrically significant inner product UV that its value depend only on the vectors $\\mathbf{U}$ and $\\mathbf{V}$, and not on the particular coordinate system used to specify these vectors. (There are other requirements on an inner product, but they are secondary.) This fact motivates\n\nDefinition 2: To each pair of contravariant vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ is associated the real number\n\n\n\\begin{equation*}\n\\mathbf{U V} \\equiv g_{i j} U^{i} V^{j} \\equiv U^{i} V_{i} \\equiv U_{i} V^{i} \\tag{5.8}\n\\end{equation*}\n\n\ncalled the (generalized) inner product of $\\mathbf{U}$ and $\\mathbf{V}$.\n\nIn similar fashion, the inner product of two covariant vectors is defined as\n\n\n\\begin{equation*}\n\\mathbf{U} \\mathbf{V} \\equiv g^{i j} U_{i} V_{j} \\equiv U^{i} V_{i} \\equiv U_{i} V^{i} \\tag{5.9}\n\\end{equation*}\n\n\nconsistent with (5.8). We therefore have the rule: To obtain the inner product of two vectors of the same type, convert one vector to the opposite type and then take the tensor inner product.\n\nRemark 2: It follows from Problem 4.5-or, more fundamentally, from property D of $\\mathbf{g}$ - that the inner product (5.8) or (5.9) is an invariant, as required.\n\nAccording to (4.2), the set of all contravariant vectors on $\\mathbf{R}^{n}$ is a vector space, as is the set of all covariant vectors on $\\mathbf{R}^{n}$. With an inner product as defined above, these vector spaces become (generalized) inner-product spaces.\n\n\\subsection*{5.6 CONCEPTS OF LENGTH AND ANGLE}\nExpressions (2.7) and (2.8) readily extend to a generalized inner-product space, provided the metric is positive definite. The norm (or length) of an arbitrary vector $\\mathbf{V}=\\left(V^{i}\\right)$ or $\\mathbf{V}=\\left(V_{i}\\right)$ is the nonnegative real number\n\n\n\\begin{equation*}\n\\|\\mathbf{V}\\| \\equiv \\sqrt{\\mathbf{V}^{2}}=\\sqrt{V_{i} V^{i}} \\tag{5.10}\n\\end{equation*}\n\n\nRemark 3: The norm of a vector-and thus the notion of a normed linear space-can be defined abstractly (see Problem 5.14), without reference to an inner product.\n\nEXAMPLE 5.5 Show that under the Euclidean metric (5.2) for polar coordinates, the vectors\n\n$$\n\\left(U^{i}\\right)=\\left(3 / 5,4 / 5 x^{1}\\right) \\quad \\text { and } \\quad\\left(V^{i}\\right)=\\left(-4 / 5,3 / 5 x^{1}\\right)\n$$\n\nare orthonormal.\n\nUsing matrices, we have:\n\n$$\n\\begin{aligned}\n\\|\\mathbf{U}\\|^{2}=U^{i} U_{i}=g_{i j} U^{i} U^{j} & =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\frac{3}{5} \\\\\n\\frac{4}{5 x^{1}}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\frac{3}{5} \\\\\n\\frac{4 x^{1}}{5}\n\\end{array}\\right] \\\\\n& =\\frac{9}{25}+\\frac{16 x^{1}}{25 x^{1}}=1\n\\end{aligned}\n$$\n\nor $\\|\\mathbf{U}\\|=1$; likewise, $\\|\\mathbf{V}\\|=1$. Now we verify that the vectors are orthogonal:\n\n$$\n\\begin{aligned}\n\\mathbf{U V}=g_{i j} U^{i} V^{j} & =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n-\\frac{4}{5} \\\\\n\\frac{3}{5 x^{1}}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\n\\frac{3}{5} & \\frac{4}{5 x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{r}\n-\\frac{4}{5} \\\\\n\\frac{3 x^{1}}{5}\n\\end{array}\\right] \\\\\n& =-\\frac{12}{25}+\\frac{12 x^{1}}{25 x^{1}}=0\n\\end{aligned}\n$$\n\nBoth normality and orthogonality depend, of course, on the metric alone, and not on the (polar) coordinate system.\n\nThe angle $\\theta$ between two non-null contravariant vectors $\\mathbf{U}$ and $\\mathbf{V}$ is defined by\n\n\n\\begin{equation*}\n\\cos \\theta \\equiv \\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|}=\\frac{g_{i j} U^{i} V^{j}}{\\sqrt{g_{p q} U^{p} U^{q}} \\sqrt{g_{r s} V^{r} V^{s}}} \\quad(0 \\leqq \\theta \\leqq \\pi) \\tag{5.11}\n\\end{equation*}\n\n\nThat $\\theta$ is well-defined follows from the Cauchy-Schwarz inequality, which may be written in the form\n\n$$\n-1 \\leqq \\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|} \\leqq 1\n$$\n\n\\section*{(see Problem 5.13).}\nThe tangent field to a family of smooth curves is a contravariant vector (Example 3.4), so that (5.11) yields the geometrical\n\nTheorem 5.3: In a general coordinate system, if $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ are the tangent vectors to two families of curves, then the families are mutually orthogonal if and only if $g_{i j} U^{i} V^{j}=0$.\n\nEXAMPLE 5.6 Show that each member of the family of curves given in polar coordinates by\n\n\n\\begin{equation*}\ne^{1 / r}=a(\\sec \\theta+\\tan \\theta) \\quad(a \\geqq 0) \\tag{1}\n\\end{equation*}\n\n\nis orthogonal to each of the curves (lima\u00e7ons of Pascal)\n\n\n\\begin{equation*}\nr=\\sin \\theta+c \\quad(c \\geqq 0) \\tag{2}\n\\end{equation*}\n\n\n(Figure 5-2 indicates the orthogonality of the curve $a=1$ to the family of lima\u00e7ons.)\n\nIn polar coordinates $x^{1}=r, x^{2}=\\theta$, and with curve parameter $t,(1)$ becomes-after taking logarithms-\n\n$$\n\\frac{1}{x^{1}}=\\ln a+\\ln |\\sec t+\\tan t| \\quad x^{2}=t\n$$\n\nWith curve parameter $u$, (2) becomes\n\n$$\nx^{1}=\\sin u+c \\quad x^{2}=u\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-066}\n\\end{center}\n\nFig. 5-2\n\nDifferentiation of $\\left(1^{\\prime}\\right)$ with respect to $t$ yields\n\n$$\n-\\frac{1}{\\left(x^{1}\\right)^{2}} \\frac{d x^{1}}{d t}=\\sec t=\\sec x^{2} \\quad \\frac{d x^{2}}{d t}=1\n$$\n\nso that the tangent vector to family $\\left(1^{\\prime}\\right)$ is\n\n$$\n\\left(U^{1}, U^{2}\\right)=\\left(-\\left(x^{1}\\right)^{2} \\sec x^{2}, 1\\right)\n$$\n\nSimilarly, the tangent vector to family $\\left(2^{\\prime}\\right)$ is\n\n$$\n\\left(V^{1}, V^{2}\\right)=(\\cos u, 1)=\\left(\\cos x^{2}, 1\\right)\n$$\n\nApplying Theorem 5.3, with the Euclidean metric tensor in polar coordinates,\n\n$$\n\\begin{aligned}\ng_{i j} U^{i} V^{j} & =g_{11} U^{1} V^{1}+g_{22} U^{2} V^{2}+0 \\\\\n& =(1)\\left[-\\left(x^{1}\\right)^{2} \\sec x^{2}\\right]\\left(\\cos x^{2}\\right)+\\left(x^{1}\\right)^{2}(1)(1) \\\\\n& \\ddots-\\left(x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}=0\n\\end{aligned}\n$$\n\nObserve that nonparametric forms of the tangent vectors are used in the orthogonality condition. This is necessary because the metric tensor at the intersection point $\\left(x^{1}, x^{2}\\right)$ of a curve (1) and a curve (2) depends on neither the parameter $t$ along (1) nor the parameter $u$ along (2).\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{ARC LENGTH}\n5.1 A curve is given in spherical coordinates $\\left(x^{i}\\right)$ by\n\n$$\nx^{1}=t \\quad x^{2}=\\arcsin \\frac{1}{t} \\quad x^{3}=\\sqrt{t^{2}-1}\n$$\n\nFind the length of the arc $1 \\leqq t \\leqq 2$.\n\nBy $(5.4)$,\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}\n$$\n\nso we first calculate the $\\left(d x^{i} / d t\\right)^{2}$ :\n\n$$\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}=1 \\quad\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\left(\\frac{-1 / t^{2}}{\\sqrt{1-(1 / t)^{2}}}\\right)^{2}=\\frac{1}{t^{2}\\left(t^{2}-1\\right)} \\quad\\left(\\frac{d x^{3}}{d t}\\right)^{2}=\\left(\\frac{1}{2} \\frac{2 t}{\\sqrt{t^{2}-1}}\\right)^{2}=\\frac{t^{2}}{t^{2}-1}\n$$\n\nThen\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=1+t^{2} \\cdot \\frac{1}{t^{2}\\left(t^{2}-1\\right)}+\\left(t \\cdot \\frac{1}{t}\\right)^{2} \\cdot \\frac{t^{2}}{t^{2}-1}=\\frac{2 t^{2}}{t^{2}-1}\n$$\n\nand $(5.1 a)$ gives\n\n$$\n\\left.L=\\int_{1}^{2} \\frac{\\sqrt{2} t}{\\sqrt{t^{2}-1}} d t=\\sqrt{2\\left(t^{2}-1\\right)}\\right]_{1}^{2}=\\sqrt{6}\n$$\n\n5.2 Find the length of the curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array}{l}\nx^{1}=1 \\\\\nx^{2}=t\n\\end{array} \\quad(1 \\leqq t \\leqq 2)\\right.\n$$\n\nif the metric is that of the hyperbolic plane $\\left(x^{2}>0\\right)$ :\n\n$$\ng_{11}=g_{22}=\\frac{1}{\\left(x^{2}\\right)^{2}} \\quad g_{12}=g_{21}=0\n$$\n\nSince $\\left(d x^{i} / d t\\right)=(0,1),(5.6 d)$ yields $(\\varepsilon=1)$\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{ll}\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{1}{t^{2}} & 0 \\\\\n0 & \\frac{1}{t^{2}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1\n\\end{array}\\right]=\\frac{1}{t^{2}} \\\\\nL=\\int_{1}^{2} \\frac{1}{t} d t=\\ln 2\n\\end{gathered}\n$$\n\nand\n\n\\section*{GENERALIZED METRICS}\n5.3 Is the form $d x^{2}+3 d x d y+4 d y^{2}+d z^{2}$ positive definite?\n\nIt must be determined whether the polynomial $Q \\equiv\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+4\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}$ is positive unless $u^{1}=u^{2}=u^{3}=0$. By completing the square,\n\n$$\nQ=\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+\\frac{9}{4}\\left(u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}=\\left(u^{1}+\\frac{3}{2} u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}\n$$\n\nAll terms are perfect squares with positive coefficients; hence the form is indeed positive definite.\n\n5.4 Show that the formula (5.1a) for arc length does not depend on the particular parameterization of the curve.\n\nGiven a curve $\\mathscr{C}: x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$, suppose that $x^{i}=x^{i}(\\bar{t}) \\quad(\\bar{a} \\leqq \\bar{t} \\leqq \\bar{b})$ is a different parameterization, where $\\bar{t}=\\phi(t)$, with $\\phi^{\\prime}(t)>0$ and $\\bar{a}=\\phi(a), \\bar{b}=\\phi(b)$. Then, by the chain rule and substitution rule for integrals,\n\n$$\n\\begin{aligned}\nL & =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}\\right|} d t=\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\left(\\phi^{\\prime}(t)\\right)^{2}\\right|} d t \\\\\n& =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} \\phi^{\\prime}(t) d t=\\int_{\\bar{a}}^{\\bar{b}} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} d \\bar{t}=\\bar{L}\n\\end{aligned}\n$$\n\n\\section*{TENSOR PROPERTY OF THE METRIC}\n5.5 Find the Euclidean metric tensor (in matrix form) for spherical coordinates, using Theorem 5.2 .\n\nSince spherical coordinates $\\left(x^{i}\\right)$ are connected to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{-1}=x^{1} \\sin x^{2} \\cos x^{3} \\quad \\bar{x}^{2}=\\bar{x}^{-1} \\sin x^{2} \\sin x^{3} \\quad \\bar{x}^{3}=x^{1} \\cos x^{2}\n$$\n\nwe have\n\n$$\nJ^{T} J=\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & \\sin x^{2} \\sin x^{3} & \\cos x^{2} \\\\\nx^{1} \\cos x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & -x^{1} \\sin x^{2} \\\\\n-x^{1} \\sin x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} & 0\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\cos x^{3} & -x^{1} \\sin x^{2} \\sin x^{3} \\\\\n\\sin x^{2} \\sin x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} \\\\\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0\n\\end{array}\\right]\n$$\n\nSince $G=J^{T} J$ is known to be symmetric (see Problem 2.4), we need only compute the elements on or above the main diagonal:\n\n$$\nG=\\left[\\begin{array}{ccc}\n\\left(\\sin ^{2} x^{2}\\right)(1)+\\cos ^{2} x^{2} & \\left(x^{1} \\sin x^{2} \\cos x^{2}\\right)(1)-x^{1} \\sin x^{2} \\cos x^{2} & g_{13} \\\\\ng_{21} & \\left(\\left(x^{1}\\right)^{2} \\cos ^{2} x^{2}\\right)(1)+\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2} & g_{23} \\\\\ng_{31} & g_{32} & \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)(1)\n\\end{array}\\right]\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& g_{13}=\\left(x^{1} \\sin ^{2} x^{2}\\right)\\left(-\\sin x^{3} \\cos x^{3}+\\cos x^{3} \\sin x^{3}\\right)=0 \\\\\n& g_{23}=\\left(\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}\\right)\\left(-\\cos x^{3} \\sin x^{3}+\\sin x^{3} \\cos x^{3}\\right)=0\n\\end{aligned}\n$$\n\nHence\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right]\n$$\n\n5.6 Find the components $g_{i j}$ of the Euclidean metric tensor in the special coordinate system $\\left(x^{i}\\right)$ defined from rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ by $x^{1}=\\bar{x}^{1}, x^{2}=\\exp \\left(\\bar{x}^{2}-\\bar{x}^{1}\\right)$.\n\nWe must compute $J^{T} J$, where $J$ is the Jacobian matrix of the transformation $\\bar{x}^{i}=\\bar{x}^{i}\\left(x^{1}, x^{2}\\right)$. Thus, we solve the above equations for the $\\bar{x}^{i}$ :\n\nHence\n\n$$\n\\bar{x}^{1}=x^{1} \\quad \\bar{x}^{2}=x^{1}+\\ln x^{2}\n$$\n\n$$\n\\begin{aligned}\n& \\text { Hence } \\quad J=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right] \\\\\n& \\text { and } G=\\left[\\begin{array}{cc}\n1 & 1 \\\\\n0 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & \\left(x^{2}\\right)^{-1} \\\\\n\\left(x^{2}\\right)^{-1} & \\left(x^{2}\\right)^{-2}\n\\end{array}\\right] \\\\\n& \\text { or } g_{11}=2, g_{10}=g_{01}=\\left(x^{2}\\right)^{-1}, g_{23}=\\left(x^{2}\\right)^{-2} \\text {. }\n\\end{aligned}\n$$\n\nor $g_{11}=2, g_{12}=g_{21}=\\left(x^{2}\\right)^{-1}, g_{22}=\\left(x^{2}\\right)^{-2}$.\n\n5.7 (a) Using the metric of Problem 5.6, calculate the length of the curve\n\n$$\n\\mathscr{C}: x^{1}=3 t, \\quad x^{2}=e^{t} \\quad(0 \\leqq t \\leqq 2)\n$$\n\n(b) Interpret geometrically.\n\n(a) First calculate the $d x^{i} / d t$ :\n\nThen\n\n$$\n\\frac{d x^{1}}{d t}=3 \\quad \\frac{d x^{2}}{d t}=e^{t}\n$$\n\nThen\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=2\\left(\\frac{d x^{1}}{d t}\\right)^{2}+2\\left(x^{2}\\right)^{-1}\\left(\\frac{d x^{1}}{d t}\\right)\\left(\\frac{d x^{2}}{d t}\\right)+\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2} \\\\\n=2(9)+2 e^{-t}(3)\\left(e^{t}\\right)+e^{-2 t}\\left(e^{2 t}\\right)=25 \\\\\nL=\\int_{0}^{2} 5 d t=10\n\\end{gathered}\n$$\n\nand\n\n(b) From the transformation equations of Problem 5.6, the curve is described in rectangular coordinates by $\\bar{x}^{2}=\\frac{4}{3} \\bar{x}^{1}$; it is therefore a straight line joining the points which correspond to $t=0$ and $t=2$, or $(0,0)$ and $(6,8)$. The distance from $(0,0)$ to $(6,8)$ is\n\n$$\n\\sqrt{6^{2}+8^{2}}=10\n$$\n\nas found in $(a)$.\n\n5.8 Making use of the Euclidean metric for cylindrical coordinates, (5.3), calculate the length of arc along the circular helix\n\n$$\n\\bar{x}^{1}=a \\cos t \\quad \\bar{x}^{2}=a \\sin t \\quad \\bar{x}^{3}=b t\n$$\n\nwith $a$ and $b$ positive constants, from $t=0$ to $t=c>0$. See Fig. 5-3.\n\nIn cylindrical coordinates $\\left(x^{i}\\right)$, where\n\n$$\n\\bar{x}^{1}=x^{1} \\cos x^{2} \\quad \\bar{x}^{2}=x^{1} \\sin x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\nthe helical arc is represented by the linear equations\n\n$$\n\\begin{gathered}\nx^{1}=a \\quad x^{2}=t \\quad x^{3}=b t \\quad(0 \\leqq t \\leqq c) \\\\\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\nb\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\na^{2} \\\\\nb\n\\end{array}\\right]=a^{2}+b^{2}\n\\end{gathered}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070(1)}\n\\end{center}\n\nFig. 5-3\n\nwhence\n\n$$\nL=\\int_{0}^{c} \\sqrt{a^{2}+b^{2}} d t=c \\sqrt{a^{2}+b^{2}}\n$$\n\n5.9 (Affine Coordinates in $\\mathbf{R}^{3}$ ) Carpenters taking measurements in a room notice that at the corner they had used as reference point the angles were not true. If the actual measures of the angles are as given in Fig. 5-4, what correction in the usual metric formula,\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{\\sum_{i=1}^{3}\\left(x_{1}^{i}-x_{2}^{i}\\right)^{2}}\n$$\n\nshould be made to compensate for the errors?\n\nWe are asked, in effect, to display $\\mathbf{g}=\\left(g_{i j}\\right)$ for three-dimensional affine coordinates $\\left(x^{i}\\right)$. Instead of applying Theorem 5.2, it is much simpler to recall from Problem 3.9 that position vectors are contravariant affine vectors-in particular, the unit vectors\n\n$$\n\\mathbf{u}=\\left(\\delta_{1}^{i}\\right) \\quad \\mathbf{v}=\\left(\\delta_{2}^{i}\\right) \\quad \\mathbf{w}=\\left(\\delta_{3}^{i}\\right)\n$$\n\nalong the oblique axes (Fig. 5-4). We can now use (5.11) in inverse fashion, to obtain:\n\n$$\n\\cos \\alpha=\\frac{g_{i j} \\delta_{1}^{i} \\delta_{2}^{j}}{\\sqrt{g_{p q} \\delta_{1}^{p} \\delta_{1}^{q}} \\sqrt{g_{r s} \\delta_{2}^{r} \\delta_{2}^{s}}}=\\frac{g_{12}}{\\sqrt{g_{11}} \\sqrt{g_{22}}}=g_{12}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070}\n\\end{center}\n\nFig. 5-4\\\\\nsince, obviously, $g_{11}=g_{22}=g_{33}=1 \\quad\\left( \\pm d s=d x^{1}\\right.$ for motion parallel to $\\mathbf{u}$; etc. $)$. Likewise,\n\n$$\n\\cos \\beta=g_{13} \\quad \\cos \\gamma=g_{23}\n$$\n\nand the complete symmetric matrix is\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & \\cos \\alpha & \\cos \\beta \\\\\n\\cos \\alpha & 1 & \\cos \\gamma \\\\\n\\cos \\beta & \\cos \\gamma & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & -0.01745 & -0.00873 \\\\\n-0.01745 & 1 & 0.01745 \\\\\n-0.00873 & 0.01745 & 1\n\\end{array}\\right]\n$$\n\nIt follows that the carpenters must use as the corrected distance formula\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{g_{i j}\\left(x_{1}^{i}-x_{2}^{i}\\right)\\left(x_{1}^{j}-x_{2}^{j}\\right)}\n$$\n\nwhere the $g_{i j}$ have the numerical values given above.\n\n\\section*{RAISING AND LOWERING INDICES}\n5.10 Given that $\\left(V^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{3}$, find its associated covariant vector $\\left(V_{i}\\right)$ in cylindrical coordinates $\\left(x^{i}\\right)$ under the Euclidean metric.\n\nSince\n\n$$\n\\left[g_{i j}\\right]_{33}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand $V_{i}=g_{i r} V^{r}$, we have in matrix form,\n\n$$\n\\left[\\begin{array}{l}\nV_{1} \\\\\nV_{2} \\\\\nV_{3}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nV^{1} \\\\\nV^{2} \\\\\nV^{3}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nV^{1} \\\\\n\\left(x^{1}\\right)^{2} V^{2} \\\\\nV^{3}\n\\end{array}\\right]\n$$\n\n5.11 Show that under orthogonal coordinate changes, starting with any particular system of rectangular coordinates, the raising and lowering of indices has no effect on tensors, consistent with the fact (Section 3.6) that there is no distinction between contravariant and covariant cartesian tensors.\n\nIt suffices to show merely that $g_{i j}=\\delta_{i j}=g^{i j}$ for any admissible coordinate system $\\left(x^{i}\\right)$, for then it will follow that\n\n$$\nT^{i}=\\delta^{i j} T_{j}=T_{i} \\quad T_{i j k}=\\delta_{i r} T_{j k}^{r}=T_{j k}^{i} \\quad T_{j k}^{i}=\\delta_{j r} T_{k}^{i r}=T_{k}^{i j}\n$$\n\nand so on. To that end, simply use formula (5.7), with $J=\\left(a_{i}^{i}\\right)$ an orthogonal matrix. Because $J^{T}=J^{-1}$, we have $G=J^{-1} J=I$, or $g_{i j}=\\delta_{i j}$, as desired. Since $G^{-1}=I^{-1}=I$, it is also the case that $g^{i j}=\\delta_{i j}$.\n\n\\section*{GENERALIZED NORM}\n5.12 Show that the length of any contravariant vector $\\left(V^{i}\\right)$ equals the length of its associated covariant vector $\\left(V_{i}\\right)$.\n\nBy definition,\n\n$$\n\\left\\|\\left(V^{i}\\right)\\right\\|=\\sqrt{g_{i j} V^{i} V^{j}} \\quad \\text { and } \\quad\\left\\|\\left(V_{i}\\right)\\right\\|=\\sqrt{g^{i j} V_{i} V_{j}}\n$$\n\nBut, since $V^{i}=g^{i r} V_{r}$ and $g_{i j}=g_{i i}$,\n\n$$\ng_{i j} V^{i} V^{j}=g_{i j}\\left(g^{i r} V_{r}\\right)\\left(g^{j s} V_{s}\\right)=g_{j i} g^{i r} g^{j s} V_{r} V_{s}=\\delta_{j}^{r} g^{j s} V_{r} V_{s}=g^{r s} V_{r} V_{s}\n$$\n\nand the two lengths are equal.\n\n5.13 Assuming a positive definite metric, show that the basic properties of the cartesian inner product $\\mathbf{U} \\cdot \\mathbf{V}$ are shared by the generalized inner product $\\mathbf{U V}$ of contravariant vectors.\n\n(a) $\\mathbf{U V}=\\mathbf{V U}$ (commutative property). Follows from symmetry of $\\left(g_{i j}\\right)$.\n\n(b) $\\mathbf{U}(\\mathbf{V}+\\mathbf{W})=\\mathbf{U V}+\\mathbf{U W}$ (distributive property). Follows from (1.2).\n\n(c) $\\quad(\\lambda \\mathbf{U}) \\mathbf{V}=\\mathbf{U}(\\lambda \\mathbf{V})=\\lambda(\\mathbf{U V})$ (associative property). Follows from $\\lambda U_{i} V^{i}=U_{i}\\left(\\lambda V^{i}\\right)=\\lambda U_{i} V^{i}$.\n\n(d) $\\mathbf{U}^{2} \\geqq 0$ with equality only if $\\mathbf{U}=\\mathbf{0}$ (positive-definiteness). Follows from the assumed positivedefiniteness of $\\left(g_{i j}\\right)$.\n\n(e) ( $\\mathbf{U V})^{2} \\leqq\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right)$ (Cauchy-Schwarz inequality). This may be derived from the other properties, as follows. If $\\mathbf{U}=\\mathbf{0}$, the inequality clearly holds. If $\\mathbf{U} \\neq \\mathbf{0}$, property ( $d$ ) ensures that the quadratic polynomial\n\n$$\nQ(\\lambda) \\equiv(\\lambda \\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2} \\lambda^{2}+2 \\mathbf{U} \\mathbf{V} \\lambda+\\mathbf{V}^{2}\n$$\n\nvanishes for at most one real value of $\\lambda$. Thus, the discriminant of $Q$ cannot be positive:\n\n$$\n(\\mathbf{U V})^{2}-\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right) \\leqq 0\n$$\n\nand this is the desired inequality.\n\n5.14 A generalized norm on a vector space is any real-valued functional $\\phi[]$ that satisfies\n\n(i) $\\phi[\\mathbf{V}] \\geqq 0$, with equality only if $\\mathbf{V}=\\mathbf{0}$;\n\n(ii) $\\phi[\\lambda \\mathbf{V}]=|\\lambda| \\phi[\\mathbf{V}]$;\n\n(iii) $\\phi[\\mathbf{U}+\\mathbf{V}] \\leqq \\phi[\\mathbf{U}]+\\phi[\\mathbf{V}]$ (triangle inequality).\n\nVerify these conditions for $\\phi[\\mathbf{V}]=\\|\\mathbf{V}\\|$, the inner-product norm under a positive definite metric.\n\n(i) and (ii) for $\\|\\mathbf{V}\\|$ are evident. As for (iii), the Cauchy-Schwarz inequality gives\n\n$$\n\\begin{aligned}\n\\|\\mathbf{U}+\\mathbf{V}\\|^{2} & =(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U} \\mathbf{V} \\\\\n& \\leqq\\|\\mathbf{U}\\|^{2}+\\|\\mathbf{V}\\|^{2}+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|=(\\|\\mathbf{U}\\|+\\|\\mathbf{V}\\|)^{2}\n\\end{aligned}\n$$\n\nfrom which (iii) follows at once.\n\n\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.15 Show that the angle between contravariant vectors is an invariant under a change of coordinate systems.\n\nThe defining expression (5.1) involves only inner products, which are invariants.\n\n5.16 In $\\mathbf{R}^{2}$ the family of curves $x^{2}=x^{1}-c$ (parameterized as $x^{1}=t, x^{2}=t-c$ ), has as its system of tangent vectors the vector field $\\mathbf{U}=(1,1)$, constant throughout $\\mathbf{R}^{2}$. If $\\left(x^{i}\\right)$ represent polar coordinates, find the family of orthogonal trajectories, and interpret geometrically.\n\nThe metric is given by\n\n$$\ng=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\nso, by Theorem 5.3, the orthogonality condition becomes\n\n$$\ng_{i j} U^{i} \\frac{d x^{j}}{d u}=(1)(1) \\frac{d x^{1}}{d u}+\\left(x^{1}\\right)^{2}(1) \\frac{d x^{2}}{d u}=0\n$$\n\nor, eliminating the differential $d u$,\n\n$$\nd x^{1}+\\left(x^{1}\\right)^{2} d x^{2}=0\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-073}\n\\end{center}\n\nFig. 5-5\n\nThis is a variables-separable differential equation, whose solution is\n\n$$\nx^{1}=\\frac{1}{x^{2}+d}\n$$\n\nThe given family of curves in the usual polar-coordinate notation is $r=\\theta+c$, which is a family of concentric spirals (solid curves in Fig. 5-5). The orthogonal trajectories,\n\n$$\nr=\\frac{1}{\\theta+d}\n$$\n\nare also spirals, each having an asymptote parallel to the line $\\theta=-d$; these are the dashed curves in Fig. 5-5.\n\nNote: To solve this problem in rectangular coordinates-that is, to find the orthogonal trajectories of the family\n\n$$\n\\frac{y}{x}=\\tan \\left(\\sqrt{x^{2}+y^{2}}-c\\right)\n$$\n\nunder the metric $\\left(g_{i j}\\right)=\\left(\\delta_{i j}\\right)$-would be difficult or impossible. Quite often, the complication of the metric involved in going over to a specialized curvilinear coordinate system is vastly outweighed by the degree to which the problem is simplified.\n\n5.17 Find the condition for two curves on a sphere of radius $a$ to be orthogonal, if the curves are represented in spherical coordinates by\n\n$$\n\\mathscr{C}_{1}: \\theta=f(\\varphi) \\quad \\text { and } \\quad \\mathscr{C}_{2}: \\theta=g(\\varphi)\n$$\n\nThe two curves can be parameterized in spherical coordinates $\\left(x^{i}\\right) \\equiv(\\rho, \\varphi, \\theta)$ by\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=t \\\\\n\\theta=f(t)\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=u \\\\\n\\theta=g(u)\n\\end{array}\\right.\\right.\n$$\n\nAt an intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$ the tangent vectors of $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ are, respectively,\n\n$$\n\\mathbf{U}=\\left(0,1, f^{\\prime}\\left(\\varphi_{0}\\right)\\right) \\quad \\text { and } \\quad \\mathbf{V}=\\left(0,1, g^{\\prime}\\left(\\varphi_{0}\\right)\\right)\n$$\n\nThese are orthogonal if and only if $g_{i j} U^{i} V^{j}=0$, or\n\n$$\n\\begin{aligned}\n0 & =\\left[\\begin{array}{lll}\n0 & 1 & f^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & \\left(a \\sin \\varphi_{0}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\ng^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right] \\\\\n& =0+a^{2}+\\left(a \\sin \\varphi_{0}\\right)^{2} f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=\\left(a^{2} \\sin ^{2} \\varphi_{0}\\right)\\left[\\csc ^{2} \\varphi_{0}+f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nHence, the desired criterion is that $f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=-\\csc ^{2} \\varphi_{0}$ at any intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$.\n\n5.18 Show that the contravariant vectors $\\mathbf{U}=\\left(0,1,2 b x^{2}\\right)$ and $\\mathbf{V}=\\left(0,-2 b x^{2},\\left(x^{1}\\right)^{2}\\right)$ are orthogonal under the Euclidean metric for cylindrical coordinates. Interpret geometrically along $x^{1}=a$, $x^{2}=t, x^{3}=b t^{2}$.\n\n$$\n\\begin{aligned}\ng_{i j} U^{i} V^{j} & =\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2}\\left(x^{1}\\right)^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right] \\\\\n& =0-2 b x^{2}\\left(x^{1}\\right)^{2}+2 b x^{2}\\left(x^{1}\\right)^{2}=0\n\\end{aligned}\n$$\n\nThe geometric interpretation is that $x^{1}=a, x^{2}=t, x^{3}=b t^{2}$, for real $t$, represents a sort of variable-pitch helix on the right circular cylinder $r=a$, having tangent field $\\mathbf{U}$. Therefore, any solution of\n\n\n\\begin{equation*}\n\\underbrace{\\frac{d x^{1}}{d u}=V^{1}=0}_{\\text {or } x^{1}=a} \\quad \\frac{d x^{2}}{d u}=V^{2}=-2 b x^{2} \\quad \\frac{d x^{3}}{d u}=V^{3}=a^{2} \\tag{1}\n\\end{equation*}\n\n\nwill represent a curve on that cylinder that is orthogonal to this pseudo-helix. See Problem 5.28.\n\n5.19 Show that in any coordinate system $\\left(x^{i}\\right)$ the contravariant vector (recall Problem 3.5) $\\mathbf{V} \\equiv\\left(g^{i \\alpha}\\right)$ is normal to the surface $x^{\\alpha}=$ const. $(\\alpha=1,2, \\ldots, n)$.\n\nBeing \"normal to a surface at the surface point $P$ \" means being orthogonal, at $P$, to the tangent vector of any curve lying in the surface and passing through $P$. Now, for the surface $x^{\\alpha}=$ const., any such tangent vector $\\mathbf{T}$ has as its $\\alpha$ th component\n\n$$\nT^{\\alpha}=\\frac{d x^{\\alpha}}{d t}=0\n$$\n\nWe then have:\n\n$$\n\\mathbf{V T} \\equiv g_{i j} V^{i} T^{j}=g_{i j} g^{i \\alpha} T^{j}=g_{j i} g^{i \\alpha} T^{j}=\\delta_{j}^{\\alpha} T^{j}=T^{\\alpha}=0\n$$\n\nand the proof is complete.\n\n5.20 Show that in any coordinate system $\\left(x^{i}\\right)$, the angle $\\theta$ between the normals to the surfaces $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. is given by\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}} \\quad \\text { (no sum) } \\tag{1}\n\\end{equation*}\n\n\nBy Problem 5.19, $\\mathbf{U}=\\left(g^{i \\alpha}\\right)$ and $\\mathbf{V}=\\left(g^{i \\beta}\\right)$ are the respective normals to $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. Therefore, by the definition (5.11)\n\n$$\n\\cos \\theta=\\frac{U V}{\\|U\\|\\|V\\|}=\\frac{g_{i j} g^{i \\alpha} g^{j \\beta}}{\\sqrt{g_{p q} g^{p \\alpha} g^{q \\alpha}} \\sqrt{g_{r s} g^{r \\beta} g^{s \\beta}}}=\\frac{\\delta_{j}^{\\alpha} g^{j \\beta}}{\\sqrt{\\delta_{q}^{\\alpha} g^{q \\alpha}} \\sqrt{\\delta_{s}^{\\beta} g^{s \\beta}}}=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}}\n$$\n\nIn consequence of (1), orthogonal coordinates are defined as those coordinate systems $\\left(x^{i}\\right)$ relative to which, at all points, $g^{i j}=0 \\quad(i \\neq j)$, or, equivalently, $g_{i j}=0 \\quad(i \\neq j)$. Obviously, orthogonal coordinates need not be rectangular: witness polar, cylindrical, and spherical coordinates.\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n5.21 Using the Euclidean metric for polar coordinates, compute the length of arc for the curve\n\n$$\n\\mathscr{C}: x^{1}=2 a \\cos t, \\quad x^{2}=t \\quad(0 \\leqq t \\leqq \\pi / 2)\n$$\n\nand interpret geometrically.\n\n5.22 Is the form $Q\\left(u^{1}, u^{2}, u^{3}\\right) \\equiv 8\\left(u^{1}\\right)^{2}+\\left(u^{2}\\right)^{2}-6 u^{1} u^{3}+\\left(u^{3}\\right)^{2}$ positive definite?\n\n5.23 Using the metric\n\n$$\nG=\\left[\\begin{array}{rrc}\n12 & 4 & 0 \\\\\n4 & 1 & 1 \\\\\n0 & 1 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\ncalculate the length of the curve given by $x^{1}=3-t, x^{2}=6 t+3, x^{3}=\\ln t$, where $1 \\leqq t \\leqq e$.\n\n5.24 A draftsman calculated several distances between points on his drawing using a set of vertical lines and his $\\mathrm{T}$-square. He obtained the distance from $(1,2)$ to $(4,6)$ the usual way:\n\n$$\n\\sqrt{(4-1)^{2}+(6-2)^{2}}=5\n$$\n\nThen he noticed his T-square was out several degrees, throwing off all measurements. An accurate reading showed his T-square measured $95.8^{\\circ}$. Find, to three decimal places, the error committed in his calculations for the answer 5 obtained above. [Hint: Use Problem 5.9 in the special case $x_{1}^{3}=x_{2}^{3}=0$, with $\\alpha=95.8^{\\circ}$.]\n\n5.25 In curvilinear coordinates $\\left(x^{i}\\right)$, show that the contravariant vectors\n\n$$\n\\mathbf{U}=\\left(-x^{1} / x^{2}, 1,0\\right) \\quad \\mathbf{V}=\\left(1 / x^{2}, 0,0\\right)\n$$\n\nare an orthonormal pair, if $\\left(x^{i}\\right)$ is related to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=x^{2} \\quad \\bar{x}^{2}=x^{3} \\quad \\bar{x}^{3}=x^{1} x^{2}\n$$\n\nwhere $x^{2} \\neq 0$.\n\n5.26 Express in $\\left(x^{i}\\right)$ the covariant vectors associated with $\\mathbf{U}$ and $\\mathbf{V}$ of Problem 5.25.\n\n5.27 Even though $\\left(g_{i j}\\right)$ may define a non-Euclidean metric, prove that the norm (5.10) still obeys the following \"Euclidean\" laws: $(a)$ the law of cosines, $(b)$ the Pythagorean theorem.\n\n5.28 (a) Solve system (1) of Problem 5.18. (b) Does the solution found in (a) include all curves orthogonal to the given pseudo-helix? Explain.\n\n5.29 Find the family of orthogonal trajectories in polar coordinates for the family of spirals $x^{1}=c x^{2} \\quad(c=$ const.). [Hint: Parameterize the family as $x^{1}=c e^{t}, x^{2}=e^{t}$.]\n\n5.30 Find the condition for two curves, $z=f(\\theta)$ and $z=g(\\theta)$, on a right circular cylinder of radius $a$ to be orthogonal.\n\n5.31 Let $\\left(x^{i}\\right)$ be any coordinate system and $\\left(g_{i j}\\right)$ any positive definite metric tensor realized in that system. Define the coordinate axes as the curves $\\mathscr{C}_{\\alpha}: x^{i}=t \\delta_{\\alpha}^{i} \\quad(\\alpha=1,2, \\ldots n)$. Show that the angle $\\phi$ between the coordinate axes $\\mathscr{C}_{\\alpha}$ and $\\mathscr{C}_{\\beta}$ satisfies the relation\n\n$$\n\\cos \\phi=\\frac{g_{\\alpha \\beta}}{\\sqrt{g_{\\alpha \\alpha}} \\sqrt{g_{\\beta \\beta}}} \\quad \\text { (no sum) }\n$$\n\nand is thus distinct, in general, from the angle $\\theta$ of Problem 5.20.\n\n5.32 Refer to Problems 5.20 and 5.31. (a) What property must the metric tensor $\\left(g_{i j}\\right)$ possess in $\\left(x^{i}\\right)$ for the coordinate axis $\\mathscr{C}_{\\alpha}$ to be normal to the surface $x^{\\alpha}=$ const. (in which case $\\theta=\\phi$ )? $(b)$ Show that the property of $(a)$ is equivalent to the mutual orthogonality of the coordinate axes.\n\n5.33 Under the metric\n\n$$\nG=\\left[\\begin{array}{cc}\n1 & \\cos 2 x^{2} \\\\\n\\cos 2 x^{2} & 1\n\\end{array}\\right] \\quad\\left(2 x^{2} / \\pi \\text { nonintegral }\\right)\n$$\n\ncompute the norm of the vector $\\mathbf{V}=\\left(d x^{i} / d t\\right)$ evaluated along the curve $x^{1}=-\\sin 2 t, x^{2}=t$, and use it to find the arc length between $t=0$ and $t=\\pi / 2$.\n\n5.34 Under the Euclidean metric for spherical coordinates, (5.4), determine a particular family of curves that intersect\n\n$$\nx^{1}=a \\quad x^{2}=b t \\quad x^{3}=t\n$$\n\northogonally. (Cf. Problem 5.28.)\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 6}", "\\section*{The Derivative of a Tensor}\n\\subsection*{6.1 INADEQUACY OF ORDINARY DIFFERENTIATION}\nConsider a contravariant tensor $\\mathbf{T}=\\left(T^{i}(\\mathbf{x}(t))\\right)$ defined on the curve $\\mathscr{C}: \\mathbf{x}=\\mathbf{x}(t)$. Differentiating the transformation law\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $t$ gives\n\n$$\n\\frac{d \\bar{T}^{i}}{d t}=\\frac{d T^{r}}{d t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{s} \\partial x^{r}} \\frac{d x^{s}}{d t}\n$$\n\nwhich shows that the ordinary derivative of $\\mathbf{T}$ along the curve is a contravariant tensor when and only when the $\\bar{x}^{i}$ are linear functions of the $x^{r}$.\n\nTheorem 6.1: The derivative of a tensor is a tensor if and only if coordinate changes are restricted to linear transformations.\n\nEXAMPLE 6.1 With $\\mathbf{T}=d \\mathbf{x} / d t$ the tangent field along $\\mathscr{C}$ (under the choice $t=s=$ arc length), the classical formula for the curvature of $\\mathscr{C}$,\n\n$$\n\\kappa=\\left\\|\\frac{d \\mathbf{T}}{d t}\\right\\|\n$$\n\nwill hold in affine coordinates but will fail to define an invariant in curvilinear coordinates, since $d \\mathbf{T} / d t$ is not a general tensor. Clearly, to make the curvature of $\\mathscr{C}$ an intrinsic property, we require a more general concept of tensor differentiation. This will entail the introduction of some complicated, nontensorial objects called Christoffel symbols.\n\n\\subsection*{6.2 CHRISTOFFEL SYMBOLS OF THE FIRST KIND}\n\\section*{Definition and Basic Properties}\nThe $n^{3}$ functions\n\n\n\\begin{equation*}\n\\Gamma_{i j k} \\equiv \\frac{1}{2}\\left[\\frac{\\partial}{\\partial x^{i}}\\left(g_{j k}\\right)+\\frac{\\partial}{\\partial x^{j}}\\left(g_{k i}\\right)-\\frac{\\partial}{\\partial x^{k}}\\left(g_{i j}\\right)\\right] \\tag{6.1a}\n\\end{equation*}\n\n\nare the Christoffel symbols of the first kind. In order to simplify the notation here and elsewhere, we shall adopt the following convention: The partial derivative of a tensor with respect to $x^{k}$ will be indicated by a final subscript $k$. Thus,\n\n\n\\begin{equation*}\n\\Gamma_{i j k} \\equiv \\frac{1}{2}\\left(-g_{i j k}+g_{j k i}+g_{k i j}\\right) \\tag{6.1b}\n\\end{equation*}\n\n\nEXAMPLE 6.2 Compute the Christoffel symbols corresponding to the Euclidean metric for spherical coordinates:\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\n\\end{array}\\right]\n$$\n\nHere, $g_{221}=2 x^{1}, g_{331}=2 x^{1} \\sin ^{2} x^{2}, g_{332}=2\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$, and all other $g_{i j k}$ are zero. Hence, $\\Gamma_{i j k}=0$ unless the triplet $i j k$ includes precisely two $2 \\mathrm{~s}$ (six cases) or precisely two $3 \\mathrm{~s}$ (six cases):\n\n$$\n\\begin{aligned}\n-\\Gamma_{221}=\\frac{1}{2}\\left(-g_{221}+g_{212}+g_{122}\\right)=-x^{1} & \\bullet \\Gamma_{212}=\\frac{1}{2}\\left(-g_{212}+g_{122}+g_{221}\\right)=x^{1} & \\bullet \\Gamma_{122}=\\frac{1}{2}\\left(-g_{122}+g_{221}+g_{212}\\right)=x^{1} \\\\\n\\Gamma_{223}=\\frac{1}{2}\\left(-g_{223}+g_{232}+g_{322}\\right)=0 & \\Gamma_{232}=\\frac{1}{2}\\left(-g_{232}+g_{322}+g_{223}\\right)=0 & \\Gamma_{322}=\\frac{1}{2}\\left(-g_{322}+g_{223}+g_{233}\\right)=0\n\\end{aligned}\n$$\n\nand\n\n\\begin{itemize}\n  \\item $\\Gamma_{331}=\\frac{1}{2}\\left(-g_{331}+g_{313}+g_{133}\\right)=-x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{323}=\\frac{1}{2}\\left(-g_{323}+g_{233}+g_{332}\\right)=\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n  \\item $\\Gamma_{332}=\\frac{1}{2}\\left(-g_{332}+g_{323}+g_{233}\\right)=-\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n  \\item $\\Gamma_{133}=\\frac{1}{2}\\left(-g_{133}+g_{331}+g_{313}\\right)=x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{313}=\\frac{1}{2}\\left(-g_{313}+g_{133}+g_{331}\\right)=x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{233}=\\frac{1}{2}\\left(-g_{233}+g_{332}+g_{323}\\right)=\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n\\end{itemize}\n\n(The nine nonzero symbols are marked with bullets.)\n\nThe two basic properties of the Christoffel symbols of the first kind are:\n\n(i) $\\Gamma_{i j k}=\\Gamma_{j i k}$ (symmetry in the first two indices)\n\n(ii) all $\\Gamma_{i j k}$ vanish if all $g_{i j}$ are constant\n\nA useful formula results from simply permuting the subscripts in (6.1b) and summing:\n\n\n\\begin{equation*}\n\\frac{\\partial g_{i k}}{\\partial x^{j}}=\\Gamma_{i j k}+\\Gamma_{j k i} \\tag{6.2}\n\\end{equation*}\n\n\nThe converse of property (ii) follows at once from (6.2); thus:\n\nLemma 6.2: In any particular coordinate system, the Christoffel symbols uniformly vanish if and only if the metric tensor has constant components in that system.\n\n\\section*{Transformation Law}\nThe transformation law for the $\\Gamma_{i j k}$ can be inferred from that for the $g_{i j}$. By differentiation,\n\n$$\n\\bar{g}_{i j k}=\\frac{\\partial}{\\partial \\bar{x}^{k}}\\left(g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)=\\frac{\\partial g_{r s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{r s} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{j}}\n$$\n\nUse the chain rule on $\\partial g_{r s} / \\partial \\bar{x}^{k}$ :\n\n$$\n\\frac{\\partial g_{r s}}{\\partial \\bar{x}^{\\bar{k}}}=\\frac{\\partial g_{r s}}{\\partial x^{t}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\equiv g_{r s t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\nThen rewrite the expression with subscripts permuted cyclically, sum the three expressions (arrows couple terms which cancel out), and divide by 2 :\n\n$$\n\\begin{aligned}\n& -\\bar{g}_{i j k}=-g_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+g_{r s}\\left(-\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{j}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\\right) \\\\\n& \\bar{g}_{j k i}=g_{s t r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+g_{s r}\\left(\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{r}}{\\partial \\bar{x}^{k}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{k}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right) \\\\\n& \\bar{g}_{k i j}=g_{t r s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{s r}\\left(\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\\right)\n\\end{aligned}\n$$\n\ngive\n\n\n\\begin{equation*}\n\\bar{\\Gamma}_{i j k}=\\Gamma_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+g_{r s} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\tag{6.3}\n\\end{equation*}\n\n\nFrom the form of (6.3) it is clear that the set of Christoffel symbols is a third-order covariant affine tensor but is not a general tensor. Here again, conventional differentiation-this time, partial differentiation with respect to a coordinate-fails to produce more than an affine tensor (recall Problem 2.23).\n\n\\subsection*{6.3 CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n\\section*{Definition and Basic Properties}\nThe $n^{3}$ functions\n\n\n\\begin{equation*}\n\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r} \\tag{6.4}\n\\end{equation*}\n\n\nare the Christoffel symbols of the second kind. It should be noted that formula (6.4) is simply the result of raising the third subscript of the Christoffel symbol of the first kind, although here we are not dealing with tensors.\n\nEXAMPLE 6.3 Calculate the Christoffel symbols of the second kind for the Euclidean metric in polar coordinates.\n\nSince\n\n$$\nG=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\nwe have:\n\n$$\n\\begin{gathered}\n\\Gamma_{111}=\\frac{1}{2} g_{111}=0 \\quad \\Gamma_{121}=\\Gamma_{211}=\\frac{1}{2}\\left(-g_{121}+g_{211}+g_{112}\\right)=0 \\\\\n\\text { - } \\Gamma_{221}=\\frac{1}{2}\\left(-g_{221}+g_{212}+g_{122}\\right)=-x^{1} \\quad \\Gamma_{112}=\\frac{1}{2}\\left(-g_{112}+g_{121}+g_{211}\\right)=0 \\\\\n\\text { - } \\Gamma_{122}=\\Gamma_{212}=\\frac{1}{2}\\left(-g_{122}+g_{221}+g_{212}\\right)=x^{1} \\quad \\Gamma_{222}=\\frac{1}{2} g_{222}=0\n\\end{gathered}\n$$\n\nTo continue,\n\n$$\nG^{-1}=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{-2}\n\\end{array}\\right]\n$$\n\nFrom $g_{12}=0=g_{21}$, it follows that $\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r}=g^{i i} \\Gamma_{j k i}$ (no sum). Therefore, when $i=1$,\n\n$$\n\\text { - } \\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{j k}^{1}=0 \\quad \\text { otherwise }\n$$\n\nand when $i=2$,\n\n$$\n\\text { - } \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1 / x^{1} \\quad \\Gamma_{j k}^{2}=0 \\quad \\text { otherwise }\n$$\n\nThe basic properties of $\\Gamma_{i j k}$ carry over to $\\Gamma_{j k}^{i}$ :\n\n(i) $\\Gamma_{j k}^{i}=\\Gamma_{k j}^{i}$ (symmetry in the lower indices)\n\n(ii) all $\\Gamma_{j k}^{i}$ vanish if all $g_{i j}$ are constant\n\nFurthermore, by Problem 6.25, Lemma 6.2 holds for both first and second kinds of Christoffel symbols.\n\n\\section*{Transformation Law}\nStarting with\n\n$$\n\\bar{\\Gamma}_{j k}^{i}=\\bar{g}^{i r} \\bar{\\Gamma}_{j k r}=\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right) \\bar{\\Gamma}_{j k r}\n$$\n\nsubstitute for $\\bar{\\Gamma}_{j k r}$ from (6.3) to obtain\n\n$$\n\\begin{aligned}\n\\bar{\\Gamma}_{j k}^{i} & =\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right)\\left(\\Gamma_{u v w} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{w}}{\\partial \\bar{x}^{r}}\\right)+\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right)\\left(g_{u v} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{r}}\\right) \\\\\n& =g^{s t} \\Gamma_{u v w} \\delta_{t}^{w} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}}+g^{s t} g_{u v} \\delta_{t}^{v} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\\\\n& =g^{s t} \\Gamma_{u v t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}}+g^{s t} g_{u t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\nSince $g^{s t} \\Gamma_{u v t}=\\Gamma_{u v}^{s}$ and $g^{s t} g_{u t}=\\delta_{u}^{s}$, after changing indices this becomes\n\n\n\\begin{equation*}\n\\bar{\\Gamma}_{j k}^{i}=\\Gamma_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\tag{6.5}\n\\end{equation*}\n\n\nThe transformation law (6.5) shows that, like $\\left(\\Gamma_{i j k}\\right),\\left(\\Gamma_{j k}^{i}\\right)$ is merely an affine tensor.\n\nAn Important Formula\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}}=\\bar{\\Gamma}_{i j}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}-\\Gamma_{s t}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{j}} \\tag{6.6}\n\\end{equation*}\n\n\nSee Problem 6.24. Needless to say, (6.6) holds when barred and unbarred coordinates are interchanged.\n\n\\subsection*{6.4 COVARIANT DIFFERENTIATION}\nOf a Vector\n\nPartial differentiation of the transformation law\n\nof a covariant vector $\\mathbf{T}=\\left(T_{i}\\right)$ yields\n\n$$\n\\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T_{r}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+T_{r} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}}\n$$\n\nUsing the chain rule on the first term on the right, and formula (6.6) on the second, results in the equations\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}} & =\\frac{\\partial T_{r}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T_{r}\\left(\\bar{\\Gamma}_{i k}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}-\\Gamma_{s t}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\\right) \\\\\n& =\\frac{\\partial T_{r}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+\\bar{\\Gamma}_{i k}^{t} \\bar{T}_{t}-\\Gamma_{r s}^{t} T_{t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\nwhich rearrange to\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}}-\\bar{\\Gamma}_{i k}^{t} \\bar{T}_{t}=\\left(\\frac{\\partial T_{r}}{\\partial x^{s}}-\\Gamma_{r s}^{t} T_{t}\\right) \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nwhich is the defining law of a covariant tensor of order two. In other words, when the components of $\\partial \\mathbf{T} / \\partial x^{k}$ are corrected by subtracting certain linear combinations of the components of $\\mathbf{T}$ itself, the result is a tensor (and not just an affine tensor).\n\nDefinition 1: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a covariant vector $\\mathbf{T}=\\left(T_{i}\\right)$ is the tensor\n\n$$\n\\mathbf{T}_{, k}=\\left(T_{i, k}\\right) \\equiv\\left(\\frac{\\partial T_{i}}{\\partial x^{k}}-\\Gamma_{i k}^{t} T_{t}\\right)\n$$\n\nRemark 1: The two covariant indices are notated $i$ and,$k$ to emphasize that the second index arose from an operation with respect to the $\\mathrm{k}$ th coordinate.\n\nRemark 2: From Lemma 6.2, the covariant derivative and the partial derivative coincide when the $g_{i j}$ are constants (as in a rectangular coordinate system).\n\nA similar manipulation (Problem 6.7) of the contravariant vector law leads to\n\nDefinition 2: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a contravariant vector $\\mathbf{T}=\\left(T^{i}\\right)$ is the tensor\n\n$$\n\\mathbf{T}_{, k}=\\left(T_{, k}^{i}\\right) \\equiv\\left(\\frac{\\partial T^{i}}{\\partial x^{k}}+\\Gamma_{t k}^{i} T^{t}\\right)\n$$\n\n\\section*{Of Any Tensor}\nIn the general definition, each covariant index (subscript) gives rise to a linear \"correction term\" of the form given in Definition 1, and each contravariant index (superscript) gives rise to a term of the form given in Definition 2.\n\nDefinition 3: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a tensor $\\mathbf{T}=\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is the tensor $\\mathbf{T}_{, k}=\\left(T_{j_{1} j_{2} \\ldots j_{q}, k}^{i_{1} i_{2} \\ldots i_{p}}\\right)$, where\n\n\n\\begin{gather*}\nT_{j_{1} j_{2} \\ldots j_{q}, k}^{i_{1} i_{2} \\ldots i_{p}}=\\frac{\\partial T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}+\\Gamma_{t k}^{i_{1}} T_{j_{1} j_{2} \\ldots j_{q}}^{t i_{2} \\ldots i_{p}}+\\Gamma_{t k}^{i_{2}} T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} t \\ldots i_{p}}+\\cdots+\\Gamma_{t k}^{i_{p}} T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots t}}{\\partial x^{k}}+\\Gamma_{j_{1} k}^{t} T_{t_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}-\\Gamma_{j_{2} k}^{t} T_{j_{1} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}-\\cdots-\\Gamma_{j_{q} k}^{t} T_{j_{1} j_{2} \\ldots t}^{i_{1} i_{2} \\ldots i_{p}}\n\\end{gather*}\n\n\nThat $\\mathbf{T}_{k}$ actually is a tensor must, of course, be proved. This can be accomplished basically as in Problem 6.8, by use of Theorem 4.2 and an induction on the number of indices.\n\nTheorem 6.3: The covariant derivative of an arbitrary tensor is a tensor of which the covariant order exceeds that of the original tensor by exactly one.\n\n\\subsection*{6.5 ABSOLUTE DIFFERENTIATION ALONG A CURVE}\nBecause $\\left(T_{, j}^{i}\\right)$ is a tensor, the inner product of $\\left(T_{, j}^{i}\\right)$ with another tensor is also a tensor. Suppose that the other tensor is $\\left(d x^{i} / d t\\right)$, the tangent vector of the curve $\\mathscr{C}: x^{i}=x^{i}(t)$. Then the inner product\n\n$$\n\\left(T_{, r}^{i} \\frac{d x^{r}}{d t}\\right)\n$$\n\nis a tensor of the same type and order as the original tensor $\\left(T^{i}\\right)$. This tensor is known as the absolute derivative of $\\left(T^{i}\\right)$ along $\\mathscr{C}$, with components written as\n\n\n\\begin{equation*}\n\\left(\\frac{\\delta T^{i}}{\\delta t}\\right) \\equiv\\left(\\frac{d T^{i}}{d t}+\\Gamma_{r s}^{i} T^{r} \\frac{d x^{s}}{d t}\\right) \\quad \\text { where } \\quad T^{i}=T^{i}(\\mathbf{x}(t)) \\tag{6.8}\n\\end{equation*}\n\n\n(see Problem 6.12). It is clear that, again, in coordinate systems in which the $g_{i j}$ are constant, absolute differentiation reduces to ordinary differentiation.\n\nThe definition (6.8) is not an arbitrary one; in Problem 6.18 is proved\n\nTheorem 6.4 (Uniqueness of the Absolute Derivative): The only tensor derivable from a given tensor $\\left(T^{i}\\right)$ that coincides with the ordinary derivative ( $\\left.d T^{i} / d t\\right)$ along some curve in a rectangular coordinate system is the absolute derivative of $\\left(T^{i}\\right)$ along that curve.\n\nRemark 3: Theorem 6.4 concerns tensors with a given form in rectangular coordinates. Thus it presumes the Euclidean metric (see Section 3.1).\n\n\\section*{Acceleration in General Coordinates}\nIn rectangular coordinates, the acceleration vector of a particle is the time derivative of its velocity vector, or the second time derivative of its position function $\\mathbf{x}=\\left(x^{i}(t)\\right)$ :\n\n$$\n\\mathbf{a}=\\left(a^{i}\\right) \\equiv\\left(\\frac{d}{d t} \\frac{d x^{i}}{d t}\\right)=\\left(\\frac{d^{2} x^{i}}{d t^{2}}\\right)\n$$\n\nThe (Euclidean) length of this vector at time $t$ is the instantaneous acceleration of the particle:\n\n$$\na=\\sqrt{\\delta_{i j} a^{i} a^{j}}\n$$\n\nSince derivatives are taken along the particle's trajectory, the natural generalization of $\\frac{d}{d t}\\left(\\frac{d x^{i}}{d t}\\right)$\n\nis\n\n$$\n\\frac{\\delta}{\\delta t}\\left(\\frac{d x^{i}}{d t}\\right)=\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\n$$\n\nHence, in general coordinates, we take as the acceleration vector and the acceleration\n\n\n\\begin{gather*}\n\\mathbf{a}=\\left(a^{i}\\right) \\equiv\\left(\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right)  \\tag{6.9}\\\\\na=\\sqrt{\\left|g_{i j} a^{i} a^{j}\\right|} \\tag{6.10}\n\\end{gather*}\n\n\nNote that positive-definiteness of the metric is not assumed in (6.10).\n\n\\section*{Curvature in General Coordinates}\nIn Euclidean geometry an important role is played by the curvature of a curve $\\mathscr{C}: x^{i}=x^{i}(t)$, commonly defined as the norm of the second derivative of $\\left(x^{i}(s)\\right)$ :\n\n$$\n\\kappa(s)=\\sqrt{\\delta_{i j} \\frac{d^{2} x^{i}}{d s^{2}} \\frac{d^{2} x^{j}}{d s^{2}}}\n$$\n\nwhere $d s / d t=\\sqrt{\\delta_{i j}\\left(d x^{i} / d t\\right)\\left(d x^{i} / d t\\right)}$ gives the arc-length parameter. The obvious way to extend this concept as an invariant is again to use absolute differentiation. Writing\n\n\n\\begin{equation*}\n\\left(b^{i}\\right) \\equiv\\left(\\frac{\\delta}{\\delta s} \\frac{d x^{i}}{d s}\\right)=\\left(\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{p q}^{i} \\frac{d x^{p}}{d s} \\frac{d x^{q}}{d s}\\right) \\tag{6.11}\n\\end{equation*}\n\n\nwhere the arc-length parameter $s=s(t)$ is given by (5.6), we have:\n\n\n\\begin{equation*}\n\\kappa(s)=\\sqrt{\\left|g_{i j} b^{i} b^{j}\\right|} \\tag{6.12}\n\\end{equation*}\n\n\n\\section*{Geodesics}\nAn important application of (6.12) in curvilinear coordinates is the following. Suppose that we seek those curves for which $\\kappa=0$ (that is, the \"straight\" lines or geodesics). For positive definite metrics, this condition is equivalent to requiring that\n\n\n\\begin{equation*}\nb^{i}=\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{p q}^{i} \\frac{d x^{p}}{d s} \\frac{d x^{q}}{d s}=0 \\quad(i=1,2, \\ldots, n) \\tag{6.13}\n\\end{equation*}\n\n\nThe solution of this system of second-order differential equations will define the geodesics $x^{i}=x^{i}(s)$.\n\nEXAMPLE 6.4 In affine coordinates, where all $g_{i j}$ are constant and all Christoffel symbols vanish, integration of $(6.13)$ is immediate:\n\n$$\nx^{i}=\\alpha^{i} s+\\beta^{i} \\quad(i=1,2, \\ldots, n)\n$$\n\nwhere, $s$ being arc length, $g_{i j} \\alpha^{i} \\alpha^{j}=1$. Thus, from each point $\\mathbf{x}=\\boldsymbol{\\beta}$ of space there emanates a geodesic ray in every direction (unit vector) $\\boldsymbol{\\alpha}$.\n\n\\subsection*{6.6 RULES FOR TENSOR DIFFERENTIATION}\nConfidence in the preceding differentiation formulas should be considerably improved when it is learned (see Problem 6.15) that the same basic rules for differentiation from calculus carry over to covariant and absolute differentiation of tensors. For arbitrary tensors $\\mathbf{T}$ and $\\mathbf{S}$, we have:\n\n\\section*{Rules for Covariant Differentiation}\n\\$\\$\n\n\\$\\$\n\nSince the absolute derivative along a curve is the inner product of the covariant derivative and the tangent vector, the above rules for differentiation repeat:\n\n\\section*{Rules for Absolute Differentiation}\n$$\n\\begin{array}{ll} \n& \\frac{\\delta}{\\delta t}(\\mathbf{T}+\\mathbf{S})=\\frac{\\delta \\mathbf{T}}{\\delta t}+\\frac{\\delta \\mathbf{S}}{\\delta t} \\\\\n\\text { outer product } & \\frac{\\delta}{\\delta t}[\\mathbf{T S}]=\\left[\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}\\right]+\\left[\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\\right] \\\\\n\\text { inner product } & \\frac{\\delta}{\\delta t}(\\mathbf{T S})=\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}+\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\n\\end{array}\n$$\n\n\\section*{Solved Problems}\n\\section*{CHRISTOFFEL SYMBOLS OF THE FIRST KIND}\n6.1 Verify that $\\Gamma_{i j k}=\\Gamma_{j i k}$.\n\nBy definition,\n\n$$\n\\Gamma_{i j k}=\\frac{1}{2}\\left(-g_{i j k}+g_{j k i}+g_{k i j}\\right) \\quad \\text { and } \\quad \\Gamma_{j i k}=\\frac{1}{2}\\left(-g_{j i k}+g_{i k j}+g_{k j i}\\right)\n$$\n\nBut $g_{i j k}=g_{j i k}, g_{j k i}=g_{k j i}$, and $g_{k i j}=g_{i k i}$, by symmetry of $g_{i j}$, and the result follows.\n\n6.2 Show that if $\\left(g_{i j}\\right)$ is a diagonal matrix, then for all fixed subscripts $\\alpha$ and $\\beta \\neq \\alpha$ in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\alpha} \\quad($ not summed on $\\alpha$ )\n\n(b) $-\\Gamma_{\\alpha \\alpha \\beta}=\\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\quad$ (not summed on $\\alpha$ )\n\n(c) All remaining Christoffel symbols $\\Gamma_{i j k}$ are zero.\\\\\n(a) By definition, $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\alpha}$.\n\n(b) Since $\\alpha \\neq \\beta$,\n\n$$\n\\begin{aligned}\n& -\\Gamma_{\\alpha \\alpha \\beta}=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}\\right)=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+0+0\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\\\\n& \\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2}\\left(-0+0+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta}\n\\end{aligned}\n$$\n\n(c) Let $i, j, k$ be distinct subscripts. Then $g_{i j}=0$ and $g_{i j k}=0$, implying that $\\Gamma_{i j k}=0$.\n\n6.3 Is it true that if all $\\Gamma_{i j k}$ vanish in some coordinate system, then the metric tensor has constant components in every coordinate system?\n\nBy Lemma 6.2, the conclusion would be valid if the $\\Gamma_{i j k}$ vanished in every coordinate system. But $\\left(\\Gamma_{i j k}\\right)$ is not a tensor, and the conclusion is false. For instance, all $\\bar{\\Gamma}_{i j k}=0$ for the Euclidean metric in rectangular coordinates, but $g_{22}=\\left(x^{1}\\right)^{2}$ in spherical coordinates.\n\n\\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n6.4 If $\\left(g_{i j}\\right)$ is a diagonal matrix, show that for all fixed indices (no summation) in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\beta}^{\\alpha}=\\Gamma_{\\beta \\alpha}^{\\alpha}=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right)$\n\n(b) $\\Gamma_{\\beta \\beta}^{\\alpha}=-\\frac{1}{2 g_{\\alpha \\alpha}} g_{\\beta \\beta \\alpha} \\quad(\\alpha \\neq \\beta)$\n\n(c) All other $\\Gamma_{j k}^{i}$ vanish.\n\n(a) Both $\\left(g_{i j}\\right)$ and $\\left(g_{i j}\\right)^{-1}=\\left(g^{i j}\\right)$ are diagonal, with nonzero diagonal elements. Thus,\n\n\n\\begin{gather*}\n\\Gamma_{\\alpha \\beta}^{\\alpha}=g^{\\alpha j} \\Gamma_{\\alpha \\beta j}=g^{\\alpha \\alpha} \\Gamma_{\\alpha \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(\\frac{1}{2} \\frac{\\partial g_{\\alpha \\alpha}}{\\partial x^{\\beta}}\\right)=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right) \\\\\n\\Gamma_{\\beta \\beta}^{\\alpha}=g^{\\alpha \\alpha} \\Gamma_{\\beta \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(-\\frac{1}{2} g_{\\beta \\beta \\alpha}\\right) \\tag{b}\n\\end{gather*}\n\n\n(c) When $i, j, k$ are distinct, $\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r}=g^{i i} \\Gamma_{j k i}=0$ (not summed on $i$ ).\n\n6.5 Calculate the Christoffel symbols of the second kind for the Euclidean metric in spherical coordinates, using Problem 6.4.\n\nWe have $g_{11}=1, g_{22}=\\left(x^{1}\\right)^{2}$, and $g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}$. Noting that $g_{11}$ is a constant and that all $g_{\\alpha \\alpha}$ are independent of $x^{3}$, we obtain the following nonzero symbols from Problem 6.4(a):\n\n$$\n\\begin{aligned}\n& \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(x^{1}\\right)^{2}\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{31}^{3}=\\Gamma_{13}^{3}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{32}^{3}=\\Gamma_{23}^{3}=\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\cot x^{2}\n\\end{aligned}\n$$\n\nSimilarly, from Problem $6.4(b)$,\n\n$$\n\\begin{aligned}\n& \\Gamma_{22}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(x^{1}\\right)^{2}=-x^{1} \\\\\n& \\Gamma_{33}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-x^{1} \\sin ^{2} x^{2} \\\\\n& \\Gamma_{33}^{2}=-\\frac{1}{2\\left(x^{1}\\right)^{2}} \\frac{\\partial}{\\partial x^{2}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-\\sin x^{2} \\cos x^{2}\n\\end{aligned}\n$$\n\n6.6 Use (6.6) to find the most general 3-dimensional transformation $x^{i}=x^{i}(\\overline{\\mathbf{x}})$ of coordinates such that $\\left(x^{i}\\right)$ is rectangular and $\\left(\\bar{x}^{i}\\right)$ is any other coordinate system for which the Christoffel symbols are\n\n$$\n\\bar{\\Gamma}_{11}^{1}=1 \\quad \\bar{\\Gamma}_{22}^{2}=2 \\quad \\bar{\\Gamma}_{33}^{3}=3 \\quad \\text { all others }=0\n$$\n\nSince $\\Gamma_{s t}^{r}=0$, (6.6) reduces to the system of linear partial differential equations with constant coefficients:\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}}=\\bar{\\Gamma}_{i j}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}} \\tag{1}\n\\end{equation*}\n\n\nIt is simplest first to solve the intermediate, first-order system\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}^{r}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s}^{r} \\quad\\left(\\bar{u}_{s}^{r}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}\\right) \\tag{2}\n\\end{equation*}\n\n\nSince the systems (2) for $r=1,2,3$ are the same, temporarily replace $\\bar{u}_{s}^{r}$ by $\\bar{u}_{s}$, and $x^{r}$ by a single variable $x$; thus,\n\nFor $j=1$, (3) becomes\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s} \\tag{3}\n\\end{equation*}\n\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{1}}=\\bar{\\Gamma}_{11}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{11}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{11}^{3} \\bar{u}_{3}=\\bar{u}_{1} \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{2}}=\\bar{\\Gamma}_{21}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{21}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{21}^{3} \\bar{u}_{3}=0 \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{3}}=\\bar{\\Gamma}_{31}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{31}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{31}^{3} \\bar{u}_{3}=0\n\\end{aligned}\n$$\n\nHence $\\bar{u}_{1}$ is a function of $\\bar{x}^{1}$ alone, and the first differential equation integrates to give\n\n$$\n\\bar{u}_{1}=b_{1} \\exp \\bar{x}^{1} \\quad\\left(b_{1}=\\text { constant }\\right)\n$$\n\nIn the same way, we find for $j=2$ and $j=3$ :\n\n$$\n\\begin{array}{ll}\n\\bar{u}_{2}=b_{2} \\exp 2 \\bar{x}^{2} & \\left(b_{2}=\\text { constant }\\right) \\\\\n\\bar{u}_{3}=b_{3} \\exp 3 \\bar{x}^{3} & \\left(b_{3}=\\text { constant }\\right)\n\\end{array}\n$$\n\nNow we return to the equations $\\partial x / \\partial \\bar{x}^{i}=\\bar{u}_{i}$ with the solutions just found for the $\\bar{u}_{i}$.\n\n\n\\begin{equation*}\n\\frac{\\partial x}{\\partial \\bar{x}^{1}}=b_{1} \\exp \\bar{x}^{1} \\quad-\\frac{\\partial x}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\frac{\\partial x}{\\partial \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\tag{4}\n\\end{equation*}\n\n\nIntegration of the first equation (4) yields\n\n$$\nx=b_{1} \\exp \\bar{x}^{1}+\\varphi\\left(\\bar{x}^{2}, \\bar{x}^{3}\\right)\n$$\n\nand then the second and third equations give:\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\varphi}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\text { or } \\quad \\varphi=a_{2} \\exp 2 \\bar{x}^{2}+\\psi\\left(\\bar{x}^{3}\\right) \\\\\n& \\frac{d \\psi}{d \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\quad \\text { or } \\quad \\psi=a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n\\end{aligned}\n$$\n\nThis means that, with $a_{1}=b_{1}$,\n\n$$\nx=a_{1} \\exp \\bar{x}^{1}+a_{2} \\exp 2 \\bar{x}^{2}+a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n$$\n\nso that the general solution of (1) is\n\n\n\\begin{equation*}\nx^{r}=a_{1}^{r} \\exp \\bar{x}^{1}+a_{2}^{r} \\exp 2 \\bar{x}^{2}+a_{3}^{r} \\exp 3 \\bar{x}^{3}+a_{4}^{r} \\tag{5}\n\\end{equation*}\n\n\nfor $r=1,2,3$.\n\nThe constants $a_{4}^{r}$ in (5) are unimportant; they merely allow any point in $\\mathbf{R}^{3}$ to serve as the origin of the rectangular system $\\left(x^{r}\\right)$. The remaining constants may be chosen at will, subject to a single condition (see Problem 6.27).\n\n\\section*{COVARIANT DIFFERENTIATION}\n6.7 Establish the tensor character of $\\mathbf{T}_{, k}$ (Definition 2), where $\\mathbf{T}$ is a contravariant vector.\n\nBeginning with the transformation law\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\ntake the partial derivative with respect to $\\bar{x}^{k}$ and use the chain rule:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial}{\\partial x^{s}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{s} \\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nNow use (6.6), with barred and unbarred systems interchanged:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r}\\left(\\Gamma_{s r}^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}}-\\bar{\\Gamma}_{u v}^{i} \\frac{\\partial \\bar{x}^{u}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{v}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nSince $\\left(\\partial \\bar{x}^{u} / \\partial x^{s}\\right)\\left(\\partial x^{s} / \\partial \\bar{x}^{k}\\right)=\\delta_{k}^{u}$ and $T^{r}\\left(\\partial \\bar{x}^{v} / \\partial x^{r}\\right)=\\bar{T}^{\\nu}$, this becomes\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+\\Gamma_{s r}^{t} T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}-\\bar{\\Gamma}_{k v}^{i} \\bar{T}^{v}\n$$\n\nor (by factoring and using the symmetry of the Christoffel symbols)\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}+\\bar{\\Gamma}_{t k}^{i} \\bar{T}^{t}=\\left(\\frac{\\partial T^{r}}{\\partial x^{s}}+\\Gamma_{t s}^{r} T^{t}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\quad \\text { or } \\quad \\bar{T}_{, k}^{i}=T_{, s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\n6.8 Show that $\\left(T_{j, k}^{i}\\right)$, as defined by (6.7), is a tensor, using the previously proven facts that $T_{, k}^{i}$ and $T_{i, k}$ are tensorial for all tensors $\\left(T^{i}\\right)$ and $\\left(T_{i}\\right)$.\n\nLet $\\left(V_{i}\\right)$ be any vector and set $U_{j}=T_{j}^{r} V_{r}$. The covariant derivative of the tensor $\\left(U_{j}\\right)$ is the tensor $\\left(U_{j, k}\\right)$, where\n\n$$\nU_{j, k}=\\frac{\\partial U_{j}}{\\partial x^{k}}-\\Gamma_{j k}^{r} U_{r}=\\frac{\\partial}{\\partial x^{k}}\\left(T_{j}^{r} V_{r}\\right)-\\Gamma_{j k}^{r}\\left(T_{r}^{s} V_{s}\\right)=\\frac{\\partial T_{j}^{s}}{\\partial x^{k}} V_{s}+T_{j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{j k}^{r} T_{r}^{s} V_{s}\n$$\n\nBut\n\n$$\nV_{r, k}=\\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{r k}^{s} V_{s} \\quad \\text { or } \\quad \\frac{\\partial V_{r}}{\\partial x^{k}}=V_{r, k}+\\Gamma_{r k}^{s} V_{s}\n$$\n\nWhen the above expression for $\\partial V_{r} / \\partial x^{k}$ is substituted into the preceding equation and the terms rearranged, the result is:\n\n$$\n\\left(\\frac{\\partial T_{j}^{s}}{\\partial x^{k}}+\\Gamma_{r k}^{s} T_{j}^{r}-\\Gamma_{j k}^{r} T_{r}^{s}\\right) V_{s}=U_{j, k}-T_{j}^{r} V_{r, k}\n$$\n\ni.e.,\n\n$$\nT_{j, k}^{s} V_{s}=\\text { tensor component }\n$$\n\nIt follows at once from the Quotient Theorem (Theorem 4.2) that $\\left(T_{j, k}^{i}\\right)$ is a tensor.\n\n6.9 Extend the notion of covariant differentiation so that it will apply to invariants.\n\nFirst note that the partial derivative of an invariant is a tensor:\n\n$$\n\\frac{\\partial \\bar{E}}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nNow, under any reasonable definition, $\\left(E_{, i}\\right)$ must (1) be a tensor; (2) coincide with ( $\\left.\\partial E / \\partial x^{i}\\right)$ in rectangular coordinates. The obvious choice is therefore\n\n$$\n\\left(E_{, i}\\right) \\equiv\\left(\\frac{\\partial E}{\\partial x^{i}}\\right)\n$$\n\n6.10 Write the formula for the covariant derivative indicated by $T_{k, l}^{i j}$.\n\n$$\nT_{k, l}^{i j}=\\frac{\\partial T_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{r l}^{i} T_{k}^{r j}+\\Gamma_{r l}^{j} T_{k}^{i r}-\\Gamma_{k l}^{r} T_{r}^{i j}\n$$\n\n6.11 Prove that the metric tensor behaves like a constant under covariant differentiation; i.e., $g_{i j, k}=0$ for all $i, j, k$.\n\nBy definition, since $\\left(g_{i j}\\right)$ is covariant of order 2 ,\n\n$$\ng_{i j, k}=\\frac{\\partial g_{i j}}{\\partial x^{k}}-\\Gamma_{i k}^{r} g_{r j}-\\Gamma_{j k}^{r} g_{i r}=g_{i j k}-\\Gamma_{i k j}-\\Gamma_{j k i}=0\n$$\n\nby (6.2). (In a similar manner, it follows that $g^{i j}=0$; see Problem 6.34.)\n\nBecause of the above property of the metric tensor and its inverse, the operation of covariant differentiation commutes with those of raising and lowering indices. For example,\n\n$$\nT_{j, k}^{i}=\\left(g^{i r} T_{r j}\\right)_{, k}=g^{i r} T_{r j, k}\n$$\n\n\\section*{ABSOLUTE DIFFERENTIATION}\n6.12 Prove that (6.8) is the result of forming the inner product of the covariant derivative $\\left(T_{, j}^{i}\\right)$ with the tangent vector $\\left(d x^{i} / d t\\right)$ of the curve.\n\n$$\nT_{, j}^{i} \\frac{d x^{j}}{d t}=\\left(\\frac{\\partial T^{i}}{\\partial x^{j}}+\\Gamma_{r j}^{i} T^{r}\\right) \\frac{d x^{j}}{d t}=\\frac{\\partial T^{i}}{\\partial x^{j}} \\frac{d x^{j}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}=\\frac{d T^{i}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}\n$$\n\n6.13 A particle is in motion along the circular arc given parametrically in spherical coordinates by $x^{1}=b, x^{2}=\\pi / 4, x^{3}=\\omega t \\quad(t=$ time $)$. Find its acceleration using the formula (6.10) and compare with the result $a=r \\omega^{2}$ from elementary mechanics.\n\nFrom Problem 6.5, we have along the circle\n\n$$\n\\begin{array}{cc}\n\\Gamma_{22}^{1}=-x^{1}=-b & \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}=-b \\sin ^{2} \\frac{\\pi}{4}=-\\frac{b}{2} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2}=-\\sin \\frac{\\pi}{4} \\cos \\frac{\\pi}{4}=-\\frac{1}{2} \\\\\n\\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}=\\cot \\frac{\\pi}{4}=1\n\\end{array}\n$$\n\nwith all other symbols vanishing. The components of acceleration are, from (6.9),\n\n$$\n\\begin{aligned}\n& a^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{r s}^{1} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{b}{2}\\right)(\\omega)^{2}=-\\frac{b \\omega^{2}}{2} \\\\\n& a^{2}=\\frac{d^{2} x^{2}}{d t^{2}}+\\Gamma_{r s}^{2} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{1}{2}\\right)(\\omega)^{2}=-\\frac{\\omega^{2}}{2} \\\\\n& a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}+\\Gamma_{r s}^{3} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0\n\\end{aligned}\n$$\n\nTogether with the metric components along the circle,\n\n$$\ng_{11}=1 \\quad g_{22}=\\left(x^{1}\\right)^{2}=b^{2} \\quad g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}=\\frac{b^{2}}{2}\n$$\n\nthe acceleration components give, via (6.10),\n\n$$\na=\\sqrt{g_{i j} a^{i} a^{j}}=\\sqrt{(1)\\left(-b \\omega^{2} / 2\\right)^{2}+\\left(b^{2}\\right)\\left(-\\omega^{2} / 2\\right)^{2}+0}=b \\omega^{2} / \\sqrt{2}\n$$\n\nUpon introducing the radius of the circle, using (3.4) with $\\bar{x}^{1}=x=r$ and $x^{3}=0$,\n\nwe obtain $a=r \\omega^{2}$.\n\n$$\nr=b \\sin \\frac{\\pi}{4}=\\frac{b}{\\sqrt{2}}\n$$\n\n6.14 Verify that $x^{1}=a \\sec x^{2}$ is a geodesic for the Euclidean metric in polar coordinates. [In rectangular coordinates $(x, y)$, the curve is $x=a$, a vertical line.]\n\nFirst choose a parameterization for the curve; say,\n\n\n\\begin{align*}\n& x^{1}=a \\sec t  \\tag{1}\\\\\n& x^{2}=t\n\\end{align*} \\quad(-\\pi / 2<t<\\pi / 2)\n\n\nParameter $t$ is related to the arc-length parameter $s$ via\n\n$$\n\\begin{aligned}\n\\frac{d s}{d t} & =\\sqrt{g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+(a \\sec t)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{a^{2} \\sec ^{2} t \\tan ^{2} t+a^{2} \\sec ^{2} t} \\\\\n& =(a \\sec t) \\sqrt{1+\\tan ^{2} t}=a \\sec ^{2} t\n\\end{aligned}\n$$\n\nor\n\n$$\n\\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a}\n$$\n\nso that for any function $x(t)$\n\n$$\n\\begin{aligned}\n& \\frac{d x}{d s}=\\frac{d x}{d t} \\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t} \\\\\n& \\frac{d^{2} x}{d s^{2}}=\\frac{d}{d t}\\left(\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t}\\right) \\frac{d t}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x}{d t}\n\\end{aligned}\n$$\n\nNow, taking the nonzero Christoffel symbols from Example 6.3, we can rewrite the geodesic equations (6.13) in terms of the independent variable $t$ :\n\n$$\n0=\\frac{d^{2} x^{1}}{d s^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{1}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{1}}{d t}+\\left(-x^{1}\\right) \\frac{\\cos ^{4} t}{a^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{1}}{d t^{2}}-(2 \\tan t) \\frac{d x^{1}}{d t}-x^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\tag{2}\n\\end{equation*}\n\n\nand\n\n$$\n0=\\frac{d^{2} x^{2}}{d s^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d s} \\frac{d x^{2}}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{2}}{d t}+2\\left(\\frac{1}{x^{1}}\\right)\\left(\\frac{\\cos ^{2} t}{a}\\right)^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{2}}{d t^{2}}-(2 \\tan t) \\frac{d x^{2}}{d t}+\\left(\\frac{2}{x^{1}}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=0 \\tag{3}\n\\end{equation*}\n\n\nAll that remains is to verify that the functions (1) satisfy the system (2)-(3). Substituting in (2):\n\n$$\na\\left(\\sec t+2 \\sec t \\tan ^{2} t\\right)-(2 \\tan t)(a \\sec t \\tan t)-(a \\sec t)(1)=0\n$$\n\nSubstituting in (3):\n\n$$\n0-(2 \\tan t)(1)+\\left(\\frac{2}{a \\sec t}\\right)(a \\sec t \\tan t)(1)=0 \\quad \\text { QED }\n$$\n\n\\section*{DIFFERENTIATION RULES}\n6.15 Prove the rules for covariant differentiation stated in Section 6.6.\n\n(a) The sum rule obviously holds, as (6.7) is linear in the tensor components.\n\n(b) Let $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ and $\\mathbf{S}=\\left(S_{j}^{i}\\right)$ be two mixed tensors of order 2, with outer product $\\mathbf{U}=\\left(T_{r}^{p} S_{s}^{q}\\right)$. Then,\n\n$$\n\\begin{aligned}\n& T_{r, k}^{p} S_{s}^{q}+T_{r}^{p} S_{s, k}^{q} \\\\\n& \\quad=\\left(\\frac{\\partial T_{r}^{p}}{\\partial x^{k}}+\\Gamma_{t k}^{p} T_{r}^{t}-\\Gamma_{r k}^{t} T_{r}^{p}\\right) S_{s}^{q}+T_{r}^{p}\\left(\\frac{\\partial S_{s}^{q}}{\\partial x^{k}}+\\Gamma_{t i k}^{q} S_{s}^{t}-\\Gamma_{s k}^{t} S_{t}^{q}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\underbrace{\\left.\\frac{\\partial T_{r}^{p}}{\\partial x^{k}} S_{s}^{q}+T_{r}^{p} \\frac{\\partial S_{s}^{q}}{\\partial x^{k}}\\right)}_{\\partial U_{r s}^{p q} / \\partial x^{k}}+\\Gamma_{t k}^{p} U_{r s}^{t q}+\\Gamma_{t k}^{q} U_{r s}^{p t}-\\Gamma_{r k}^{t} U_{t s}^{p q}-\\Gamma_{s k}^{t} U_{r t}^{p q} \\\\\n& \\equiv U_{r s, k}^{p q}\n\\end{aligned}\n$$\n\nand this proof of the outer-product rule extends to arbitrary $\\mathbf{T}$ and $\\mathbf{S}$.\n\n(c) The inner-product rule follows from the outer-product rule and the following useful result: Contraction of indices and covariant differentiation commute. To prove this last, let $\\mathbf{R}=\\left(R_{k}^{i j}\\right)$. Then,\n\n$$\n\\begin{aligned}\nR_{k, l}^{i j} \\delta_{j}^{k} & =\\left(\\frac{\\partial R_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{i j}+\\Gamma_{t l}^{j} R_{k}^{i t}-\\Gamma_{k l}^{t} R_{t}^{i j}\\right) \\delta_{j}^{k} \\\\\n& =\\frac{\\partial R_{k}^{i k}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{t k}+0=\\left(R_{k}^{i k}\\right)_{, l} \\quad \\text { QED }\n\\end{aligned}\n$$\n\n6.16 Instead of Problem 6.15, why not: \"Each rule is a tensor equation that is valid in rectangular coordinates, where covariant differentiation reduces to partial differentiation. Therefore, each rule holds in every coordinate system.'?\n\nIf the space metric is non-Euclidean, there is no way to transform to a rectangular coordinate system (in which the rules would indeed hold).\n\n6.17 Infer the outer-product rule for absolute differentation from the corresponding rule for covariant differentiation.\n\nLet $\\mathbf{x}=\\mathbf{x}(t)$ be any curve, and $\\mathbf{T}(\\mathbf{x}(t))$ and $\\mathbf{S}(\\mathbf{x}(t))$ two tensors defined on the curve. Then,\n\n$$\n\\frac{\\delta}{\\delta t}[\\mathbf{T S}]=[\\mathbf{T S}]_{, k} \\frac{d x^{k}}{d t}=\\left(\\left[\\mathbf{T}_{, k} \\mathbf{S}\\right]+\\left[\\mathbf{T S}_{, k}\\right]\\right) \\frac{d x^{k}}{d t}=\\left[\\mathbf{T}_{, k} \\frac{d x^{k}}{d t} \\mathbf{S}\\right]+\\left[\\mathbf{T ~}_{, k} \\frac{d x^{k}}{d t}\\right]=\\left[\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}\\right]+\\left[\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\\right]\n$$\n\n\\section*{UNIQUENESS OF THE ABSOLUTE DERIVATIVE}\n6.18 Prove Theorem 6.4.\n\nDenote by $\\Delta \\mathbf{T} / \\Delta t$ any tensor that satisfies the hypothesis of the theorem. The tensor equation\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t}=\\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\nis valid in rectangular coordinates $\\left(x^{i}\\right)$, since, in $\\left(x^{i}\\right)$, both sides coincide with $d \\mathbf{T} / d t$. But then (Section 4.3) the equation holds in every coordinate system; i.e.,\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\n\\section*{Supplementary Problems}\n6.19 Find the general solution of the linear system\n\n$$\n\\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{j} \\partial x^{k}}=a_{j k}^{i}=\\text { const. }\n$$\n\nwith $a_{j k}^{i}$ symmetric in the two lower subscripts. [Hint: Set $y_{k}^{i}=\\partial \\bar{x}^{i} / \\partial x^{k}-a_{r k}^{i} x^{r}$.]\n\n6.20 A two-dimensional coordinate system $\\left(x^{i}\\right)$ is connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=2\\left(x^{1}\\right)^{2}+x^{2} \\quad \\bar{x}^{2}=-x^{1}+3 x^{2}\n$$\n\n(a) Exhibit the metric tensor in $\\left(x^{i}\\right)$.\n\n(b) Calculate the Christoffel symbols of the first kind for $\\left(x^{i}\\right)$ directly from the definition (6.1).\n\n6.21 (a) Derive the formula\n\n$$\n\\Gamma_{i j k}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{j}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is rectangular and $\\left(x^{i}\\right)$ is any other coordinate system. [Hint: Interchange barred and unbarred coordinate systems in (6.3)]. (b) Derive the analogous formula\n\n$$\n\\Gamma_{j k}^{i}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{k}} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is such that all $\\bar{g}_{i j}$ arc constant. [Hint: Interchange barred and unbarred coordinate systems in $(6.5)$.]\n\n6.22 Let the coordinate system $\\left(x^{i}\\right)$ be connected to a system of rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\quad \\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n$$\n\nUse Problem 6.21(b) to compute the nonzero Christoffel symbols of the second kind for $\\left(x^{i}\\right)$.\n\n6.23 If\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=-\\exp d_{1} x^{1}+\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{2}=2 \\exp d_{1} x^{1}-\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{3}=\\exp d_{1} x^{1}-2 \\exp d_{2} x^{2}+3 \\exp d_{3} x^{3}\n\\end{aligned}\n$$\n\nand if all $\\bar{\\Gamma}_{j k}^{i}=0$, find the $\\Gamma_{j k}^{i}$.\n\n6.24 Derive (6.6) by solving (6.5) for the second derivative and then changing indices.\n\n6.25 Prove that all $\\Gamma_{j k}^{i}$ vanish only if all $g_{i j}$ are constant.\n\n6.26 Calculate the nonzero Christoffel symbols of both kinds for the Euclidean metric in cylindrical coordinates, (5.3).\n\n6.27 Express the condition that the transformation (5) of Problem 6.6 be bijective (Section 2.6).\n\n6.28 Show that if $\\Gamma_{i j k}$ are constant, then $g_{i j}$ are linear in the variables $\\left(x^{i}\\right)$; but that this is not necessarily true if $\\Gamma_{j k}^{i}$ are constant. (For a counterexample, use the metric $g_{11}=\\exp 2 x^{1}, g_{12}=g_{21}=0, g_{22}=1$.)\n\n6.29 What is the most general two-dimensional transformation $\\bar{x}^{i}=\\bar{x}^{i}(\\mathbf{x})$ of coordinates such that $\\left(\\bar{x}^{i}\\right)$ are rectangular and the Christoffel symbols $\\Gamma_{j k}^{i}$ in $\\left(x^{i}\\right)$ are those for the metric of polar coordinates (Example 6.3)?\n\n6.30 Is the covariant derivative of a tensor with constant components equal to zero as in ordinary differentiation? Explain your answer.\n\n6.31 If $T_{j r s}^{i}$ are tensor components, write out the components of the covariant derivative, $T_{j r s, k}^{i}$.\n\n6.32 Show that $\\delta_{i, k}^{i}=0$ for all $i, j, k$.\n\n6.33 For any tensor $\\mathbf{T}$, verify that $(\\mathbf{g} * \\mathbf{T})_{, k}=\\mathbf{g} * \\mathbf{T}{ }_{, k}$, where $*$ denotes either an outer or inner product.\n\n6.34 Use Problem 6.32 and $g_{j r} g^{r i}=\\delta_{j}^{i}$ to show that the covariant derivative of $\\mathbf{g}^{-1}$ is zero.\n\n6.35 Use the recursive method of Problem 6.8 to verify that $\\left(T_{i j, k}\\right)$ is a tensor.\n\n6.36 Using tensor methods in polar coordinates, find the curvature of the circle\n\n$$\nx^{1}=b \\quad x^{2}=t\n$$\n\n6.37 If the metric for $\\left(x^{i}\\right)$ is\n\n$$\nG=\\left[\\begin{array}{cc}\n\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\n(a) write the differential equations of the geodesics in terms of the dependent variables $u=\\left(x^{1}\\right)^{2}$ and $v=x^{2} ;(b)$ integrate these equations and eliminate the arc-length parameter from the solution.\n\n6.38 Find the geodesics on the surface of a sphere of radius $a$ by $(a)$ writing the geodesic equations for the spherical coordinates $x^{2}$ and $x^{3}$ (the $x^{1}$-equation is trivial for $x^{1}=a=$ const. and may be ignored); $(b)$ exhibiting a particular solution of these two equations; and (c) generalizing on (b). Use Problem 6.5 for the Christoffel symbols.\n\n"], "lesson": "\\section*{Chapter 6}\n\\section*{The Derivative of a Tensor}\n\\subsection*{6.1 INADEQUACY OF ORDINARY DIFFERENTIATION}\nConsider a contravariant tensor $\\mathbf{T}=\\left(T^{i}(\\mathbf{x}(t))\\right)$ defined on the curve $\\mathscr{C}: \\mathbf{x}=\\mathbf{x}(t)$. Differentiating the transformation law\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $t$ gives\n\n$$\n\\frac{d \\bar{T}^{i}}{d t}=\\frac{d T^{r}}{d t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{s} \\partial x^{r}} \\frac{d x^{s}}{d t}\n$$\n\nwhich shows that the ordinary derivative of $\\mathbf{T}$ along the curve is a contravariant tensor when and only when the $\\bar{x}^{i}$ are linear functions of the $x^{r}$.\n\nTheorem 6.1: The derivative of a tensor is a tensor if and only if coordinate changes are restricted to linear transformations.\n\nEXAMPLE 6.1 With $\\mathbf{T}=d \\mathbf{x} / d t$ the tangent field along $\\mathscr{C}$ (under the choice $t=s=$ arc length), the classical formula for the curvature of $\\mathscr{C}$,\n\n$$\n\\kappa=\\left\\|\\frac{d \\mathbf{T}}{d t}\\right\\|\n$$\n\nwill hold in affine coordinates but will fail to define an invariant in curvilinear coordinates, since $d \\mathbf{T} / d t$ is not a general tensor. Clearly, to make the curvature of $\\mathscr{C}$ an intrinsic property, we require a more general concept of tensor differentiation. This will entail the introduction of some complicated, nontensorial objects called Christoffel symbols.\n\n\\subsection*{6.2 CHRISTOFFEL SYMBOLS OF THE FIRST KIND}\n\\section*{Definition and Basic Properties}\nThe $n^{3}$ functions\n\n\n\\begin{equation*}\n\\Gamma_{i j k} \\equiv \\frac{1}{2}\\left[\\frac{\\partial}{\\partial x^{i}}\\left(g_{j k}\\right)+\\frac{\\partial}{\\partial x^{j}}\\left(g_{k i}\\right)-\\frac{\\partial}{\\partial x^{k}}\\left(g_{i j}\\right)\\right] \\tag{6.1a}\n\\end{equation*}\n\n\nare the Christoffel symbols of the first kind. In order to simplify the notation here and elsewhere, we shall adopt the following convention: The partial derivative of a tensor with respect to $x^{k}$ will be indicated by a final subscript $k$. Thus,\n\n\n\\begin{equation*}\n\\Gamma_{i j k} \\equiv \\frac{1}{2}\\left(-g_{i j k}+g_{j k i}+g_{k i j}\\right) \\tag{6.1b}\n\\end{equation*}\n\n\nEXAMPLE 6.2 Compute the Christoffel symbols corresponding to the Euclidean metric for spherical coordinates:\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\n\\end{array}\\right]\n$$\n\nHere, $g_{221}=2 x^{1}, g_{331}=2 x^{1} \\sin ^{2} x^{2}, g_{332}=2\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$, and all other $g_{i j k}$ are zero. Hence, $\\Gamma_{i j k}=0$ unless the triplet $i j k$ includes precisely two $2 \\mathrm{~s}$ (six cases) or precisely two $3 \\mathrm{~s}$ (six cases):\n\n$$\n\\begin{aligned}\n-\\Gamma_{221}=\\frac{1}{2}\\left(-g_{221}+g_{212}+g_{122}\\right)=-x^{1} & \\bullet \\Gamma_{212}=\\frac{1}{2}\\left(-g_{212}+g_{122}+g_{221}\\right)=x^{1} & \\bullet \\Gamma_{122}=\\frac{1}{2}\\left(-g_{122}+g_{221}+g_{212}\\right)=x^{1} \\\\\n\\Gamma_{223}=\\frac{1}{2}\\left(-g_{223}+g_{232}+g_{322}\\right)=0 & \\Gamma_{232}=\\frac{1}{2}\\left(-g_{232}+g_{322}+g_{223}\\right)=0 & \\Gamma_{322}=\\frac{1}{2}\\left(-g_{322}+g_{223}+g_{233}\\right)=0\n\\end{aligned}\n$$\n\nand\n\n\\begin{itemize}\n  \\item $\\Gamma_{331}=\\frac{1}{2}\\left(-g_{331}+g_{313}+g_{133}\\right)=-x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{323}=\\frac{1}{2}\\left(-g_{323}+g_{233}+g_{332}\\right)=\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n  \\item $\\Gamma_{332}=\\frac{1}{2}\\left(-g_{332}+g_{323}+g_{233}\\right)=-\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n  \\item $\\Gamma_{133}=\\frac{1}{2}\\left(-g_{133}+g_{331}+g_{313}\\right)=x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{313}=\\frac{1}{2}\\left(-g_{313}+g_{133}+g_{331}\\right)=x^{1} \\sin ^{2} x^{2}$\n  \\item $\\Gamma_{233}=\\frac{1}{2}\\left(-g_{233}+g_{332}+g_{323}\\right)=\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}$\n\\end{itemize}\n\n(The nine nonzero symbols are marked with bullets.)\n\nThe two basic properties of the Christoffel symbols of the first kind are:\n\n(i) $\\Gamma_{i j k}=\\Gamma_{j i k}$ (symmetry in the first two indices)\n\n(ii) all $\\Gamma_{i j k}$ vanish if all $g_{i j}$ are constant\n\nA useful formula results from simply permuting the subscripts in (6.1b) and summing:\n\n\n\\begin{equation*}\n\\frac{\\partial g_{i k}}{\\partial x^{j}}=\\Gamma_{i j k}+\\Gamma_{j k i} \\tag{6.2}\n\\end{equation*}\n\n\nThe converse of property (ii) follows at once from (6.2); thus:\n\nLemma 6.2: In any particular coordinate system, the Christoffel symbols uniformly vanish if and only if the metric tensor has constant components in that system.\n\n\\section*{Transformation Law}\nThe transformation law for the $\\Gamma_{i j k}$ can be inferred from that for the $g_{i j}$. By differentiation,\n\n$$\n\\bar{g}_{i j k}=\\frac{\\partial}{\\partial \\bar{x}^{k}}\\left(g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)=\\frac{\\partial g_{r s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{r s} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{j}}\n$$\n\nUse the chain rule on $\\partial g_{r s} / \\partial \\bar{x}^{k}$ :\n\n$$\n\\frac{\\partial g_{r s}}{\\partial \\bar{x}^{\\bar{k}}}=\\frac{\\partial g_{r s}}{\\partial x^{t}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\equiv g_{r s t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\nThen rewrite the expression with subscripts permuted cyclically, sum the three expressions (arrows couple terms which cancel out), and divide by 2 :\n\n$$\n\\begin{aligned}\n& -\\bar{g}_{i j k}=-g_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+g_{r s}\\left(-\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{j}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\\right) \\\\\n& \\bar{g}_{j k i}=g_{s t r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+g_{s r}\\left(\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{r}}{\\partial \\bar{x}^{k}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{k}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right) \\\\\n& \\bar{g}_{k i j}=g_{t r s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+g_{s r}\\left(\\frac{\\partial^{2} x^{s}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\\right)\n\\end{aligned}\n$$\n\ngive\n\n\n\\begin{equation*}\n\\bar{\\Gamma}_{i j k}=\\Gamma_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+g_{r s} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\tag{6.3}\n\\end{equation*}\n\n\nFrom the form of (6.3) it is clear that the set of Christoffel symbols is a third-order covariant affine tensor but is not a general tensor. Here again, conventional differentiation-this time, partial differentiation with respect to a coordinate-fails to produce more than an affine tensor (recall Problem 2.23).\n\n\\subsection*{6.3 CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n\\section*{Definition and Basic Properties}\nThe $n^{3}$ functions\n\n\n\\begin{equation*}\n\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r} \\tag{6.4}\n\\end{equation*}\n\n\nare the Christoffel symbols of the second kind. It should be noted that formula (6.4) is simply the result of raising the third subscript of the Christoffel symbol of the first kind, although here we are not dealing with tensors.\n\nEXAMPLE 6.3 Calculate the Christoffel symbols of the second kind for the Euclidean metric in polar coordinates.\n\nSince\n\n$$\nG=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\nwe have:\n\n$$\n\\begin{gathered}\n\\Gamma_{111}=\\frac{1}{2} g_{111}=0 \\quad \\Gamma_{121}=\\Gamma_{211}=\\frac{1}{2}\\left(-g_{121}+g_{211}+g_{112}\\right)=0 \\\\\n\\text { - } \\Gamma_{221}=\\frac{1}{2}\\left(-g_{221}+g_{212}+g_{122}\\right)=-x^{1} \\quad \\Gamma_{112}=\\frac{1}{2}\\left(-g_{112}+g_{121}+g_{211}\\right)=0 \\\\\n\\text { - } \\Gamma_{122}=\\Gamma_{212}=\\frac{1}{2}\\left(-g_{122}+g_{221}+g_{212}\\right)=x^{1} \\quad \\Gamma_{222}=\\frac{1}{2} g_{222}=0\n\\end{gathered}\n$$\n\nTo continue,\n\n$$\nG^{-1}=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{-2}\n\\end{array}\\right]\n$$\n\nFrom $g_{12}=0=g_{21}$, it follows that $\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r}=g^{i i} \\Gamma_{j k i}$ (no sum). Therefore, when $i=1$,\n\n$$\n\\text { - } \\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{j k}^{1}=0 \\quad \\text { otherwise }\n$$\n\nand when $i=2$,\n\n$$\n\\text { - } \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1 / x^{1} \\quad \\Gamma_{j k}^{2}=0 \\quad \\text { otherwise }\n$$\n\nThe basic properties of $\\Gamma_{i j k}$ carry over to $\\Gamma_{j k}^{i}$ :\n\n(i) $\\Gamma_{j k}^{i}=\\Gamma_{k j}^{i}$ (symmetry in the lower indices)\n\n(ii) all $\\Gamma_{j k}^{i}$ vanish if all $g_{i j}$ are constant\n\nFurthermore, by Problem 6.25, Lemma 6.2 holds for both first and second kinds of Christoffel symbols.\n\n\\section*{Transformation Law}\nStarting with\n\n$$\n\\bar{\\Gamma}_{j k}^{i}=\\bar{g}^{i r} \\bar{\\Gamma}_{j k r}=\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right) \\bar{\\Gamma}_{j k r}\n$$\n\nsubstitute for $\\bar{\\Gamma}_{j k r}$ from (6.3) to obtain\n\n$$\n\\begin{aligned}\n\\bar{\\Gamma}_{j k}^{i} & =\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right)\\left(\\Gamma_{u v w} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{w}}{\\partial \\bar{x}^{r}}\\right)+\\left(g^{s t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{t}}\\right)\\left(g_{u v} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{r}}\\right) \\\\\n& =g^{s t} \\Gamma_{u v w} \\delta_{t}^{w} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}}+g^{s t} g_{u v} \\delta_{t}^{v} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\\\\n& =g^{s t} \\Gamma_{u v t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{k}}+g^{s t} g_{u t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}} \\frac{\\partial^{2} x^{u}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\nSince $g^{s t} \\Gamma_{u v t}=\\Gamma_{u v}^{s}$ and $g^{s t} g_{u t}=\\delta_{u}^{s}$, after changing indices this becomes\n\n\n\\begin{equation*}\n\\bar{\\Gamma}_{j k}^{i}=\\Gamma_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}+\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{j} \\partial \\bar{x}^{k}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\tag{6.5}\n\\end{equation*}\n\n\nThe transformation law (6.5) shows that, like $\\left(\\Gamma_{i j k}\\right),\\left(\\Gamma_{j k}^{i}\\right)$ is merely an affine tensor.\n\nAn Important Formula\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}}=\\bar{\\Gamma}_{i j}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}-\\Gamma_{s t}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{j}} \\tag{6.6}\n\\end{equation*}\n\n\nSee Problem 6.24. Needless to say, (6.6) holds when barred and unbarred coordinates are interchanged.\n\n\\subsection*{6.4 COVARIANT DIFFERENTIATION}\nOf a Vector\n\nPartial differentiation of the transformation law\n\nof a covariant vector $\\mathbf{T}=\\left(T_{i}\\right)$ yields\n\n$$\n\\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T_{r}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}+T_{r} \\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{k} \\partial \\bar{x}^{i}}\n$$\n\nUsing the chain rule on the first term on the right, and formula (6.6) on the second, results in the equations\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}} & =\\frac{\\partial T_{r}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T_{r}\\left(\\bar{\\Gamma}_{i k}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}-\\Gamma_{s t}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\\right) \\\\\n& =\\frac{\\partial T_{r}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+\\bar{\\Gamma}_{i k}^{t} \\bar{T}_{t}-\\Gamma_{r s}^{t} T_{t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n\\end{aligned}\n$$\n\nwhich rearrange to\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}^{k}}-\\bar{\\Gamma}_{i k}^{t} \\bar{T}_{t}=\\left(\\frac{\\partial T_{r}}{\\partial x^{s}}-\\Gamma_{r s}^{t} T_{t}\\right) \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nwhich is the defining law of a covariant tensor of order two. In other words, when the components of $\\partial \\mathbf{T} / \\partial x^{k}$ are corrected by subtracting certain linear combinations of the components of $\\mathbf{T}$ itself, the result is a tensor (and not just an affine tensor).\n\nDefinition 1: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a covariant vector $\\mathbf{T}=\\left(T_{i}\\right)$ is the tensor\n\n$$\n\\mathbf{T}_{, k}=\\left(T_{i, k}\\right) \\equiv\\left(\\frac{\\partial T_{i}}{\\partial x^{k}}-\\Gamma_{i k}^{t} T_{t}\\right)\n$$\n\nRemark 1: The two covariant indices are notated $i$ and,$k$ to emphasize that the second index arose from an operation with respect to the $\\mathrm{k}$ th coordinate.\n\nRemark 2: From Lemma 6.2, the covariant derivative and the partial derivative coincide when the $g_{i j}$ are constants (as in a rectangular coordinate system).\n\nA similar manipulation (Problem 6.7) of the contravariant vector law leads to\n\nDefinition 2: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a contravariant vector $\\mathbf{T}=\\left(T^{i}\\right)$ is the tensor\n\n$$\n\\mathbf{T}_{, k}=\\left(T_{, k}^{i}\\right) \\equiv\\left(\\frac{\\partial T^{i}}{\\partial x^{k}}+\\Gamma_{t k}^{i} T^{t}\\right)\n$$\n\n\\section*{Of Any Tensor}\nIn the general definition, each covariant index (subscript) gives rise to a linear \"correction term\" of the form given in Definition 1, and each contravariant index (superscript) gives rise to a term of the form given in Definition 2.\n\nDefinition 3: In any coordinate system $\\left(x^{i}\\right)$, the covariant derivative with respect to $x^{k}$ of a tensor $\\mathbf{T}=\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is the tensor $\\mathbf{T}_{, k}=\\left(T_{j_{1} j_{2} \\ldots j_{q}, k}^{i_{1} i_{2} \\ldots i_{p}}\\right)$, where\n\n\n\\begin{gather*}\nT_{j_{1} j_{2} \\ldots j_{q}, k}^{i_{1} i_{2} \\ldots i_{p}}=\\frac{\\partial T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}+\\Gamma_{t k}^{i_{1}} T_{j_{1} j_{2} \\ldots j_{q}}^{t i_{2} \\ldots i_{p}}+\\Gamma_{t k}^{i_{2}} T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} t \\ldots i_{p}}+\\cdots+\\Gamma_{t k}^{i_{p}} T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots t}}{\\partial x^{k}}+\\Gamma_{j_{1} k}^{t} T_{t_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}-\\Gamma_{j_{2} k}^{t} T_{j_{1} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p}}-\\cdots-\\Gamma_{j_{q} k}^{t} T_{j_{1} j_{2} \\ldots t}^{i_{1} i_{2} \\ldots i_{p}}\n\\end{gather*}\n\n\nThat $\\mathbf{T}_{k}$ actually is a tensor must, of course, be proved. This can be accomplished basically as in Problem 6.8, by use of Theorem 4.2 and an induction on the number of indices.\n\nTheorem 6.3: The covariant derivative of an arbitrary tensor is a tensor of which the covariant order exceeds that of the original tensor by exactly one.\n\n\\subsection*{6.5 ABSOLUTE DIFFERENTIATION ALONG A CURVE}\nBecause $\\left(T_{, j}^{i}\\right)$ is a tensor, the inner product of $\\left(T_{, j}^{i}\\right)$ with another tensor is also a tensor. Suppose that the other tensor is $\\left(d x^{i} / d t\\right)$, the tangent vector of the curve $\\mathscr{C}: x^{i}=x^{i}(t)$. Then the inner product\n\n$$\n\\left(T_{, r}^{i} \\frac{d x^{r}}{d t}\\right)\n$$\n\nis a tensor of the same type and order as the original tensor $\\left(T^{i}\\right)$. This tensor is known as the absolute derivative of $\\left(T^{i}\\right)$ along $\\mathscr{C}$, with components written as\n\n\n\\begin{equation*}\n\\left(\\frac{\\delta T^{i}}{\\delta t}\\right) \\equiv\\left(\\frac{d T^{i}}{d t}+\\Gamma_{r s}^{i} T^{r} \\frac{d x^{s}}{d t}\\right) \\quad \\text { where } \\quad T^{i}=T^{i}(\\mathbf{x}(t)) \\tag{6.8}\n\\end{equation*}\n\n\n(see Problem 6.12). It is clear that, again, in coordinate systems in which the $g_{i j}$ are constant, absolute differentiation reduces to ordinary differentiation.\n\nThe definition (6.8) is not an arbitrary one; in Problem 6.18 is proved\n\nTheorem 6.4 (Uniqueness of the Absolute Derivative): The only tensor derivable from a given tensor $\\left(T^{i}\\right)$ that coincides with the ordinary derivative ( $\\left.d T^{i} / d t\\right)$ along some curve in a rectangular coordinate system is the absolute derivative of $\\left(T^{i}\\right)$ along that curve.\n\nRemark 3: Theorem 6.4 concerns tensors with a given form in rectangular coordinates. Thus it presumes the Euclidean metric (see Section 3.1).\n\n\\section*{Acceleration in General Coordinates}\nIn rectangular coordinates, the acceleration vector of a particle is the time derivative of its velocity vector, or the second time derivative of its position function $\\mathbf{x}=\\left(x^{i}(t)\\right)$ :\n\n$$\n\\mathbf{a}=\\left(a^{i}\\right) \\equiv\\left(\\frac{d}{d t} \\frac{d x^{i}}{d t}\\right)=\\left(\\frac{d^{2} x^{i}}{d t^{2}}\\right)\n$$\n\nThe (Euclidean) length of this vector at time $t$ is the instantaneous acceleration of the particle:\n\n$$\na=\\sqrt{\\delta_{i j} a^{i} a^{j}}\n$$\n\nSince derivatives are taken along the particle's trajectory, the natural generalization of $\\frac{d}{d t}\\left(\\frac{d x^{i}}{d t}\\right)$\n\nis\n\n$$\n\\frac{\\delta}{\\delta t}\\left(\\frac{d x^{i}}{d t}\\right)=\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\n$$\n\nHence, in general coordinates, we take as the acceleration vector and the acceleration\n\n\n\\begin{gather*}\n\\mathbf{a}=\\left(a^{i}\\right) \\equiv\\left(\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right)  \\tag{6.9}\\\\\na=\\sqrt{\\left|g_{i j} a^{i} a^{j}\\right|} \\tag{6.10}\n\\end{gather*}\n\n\nNote that positive-definiteness of the metric is not assumed in (6.10).\n\n\\section*{Curvature in General Coordinates}\nIn Euclidean geometry an important role is played by the curvature of a curve $\\mathscr{C}: x^{i}=x^{i}(t)$, commonly defined as the norm of the second derivative of $\\left(x^{i}(s)\\right)$ :\n\n$$\n\\kappa(s)=\\sqrt{\\delta_{i j} \\frac{d^{2} x^{i}}{d s^{2}} \\frac{d^{2} x^{j}}{d s^{2}}}\n$$\n\nwhere $d s / d t=\\sqrt{\\delta_{i j}\\left(d x^{i} / d t\\right)\\left(d x^{i} / d t\\right)}$ gives the arc-length parameter. The obvious way to extend this concept as an invariant is again to use absolute differentiation. Writing\n\n\n\\begin{equation*}\n\\left(b^{i}\\right) \\equiv\\left(\\frac{\\delta}{\\delta s} \\frac{d x^{i}}{d s}\\right)=\\left(\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{p q}^{i} \\frac{d x^{p}}{d s} \\frac{d x^{q}}{d s}\\right) \\tag{6.11}\n\\end{equation*}\n\n\nwhere the arc-length parameter $s=s(t)$ is given by (5.6), we have:\n\n\n\\begin{equation*}\n\\kappa(s)=\\sqrt{\\left|g_{i j} b^{i} b^{j}\\right|} \\tag{6.12}\n\\end{equation*}\n\n\n\\section*{Geodesics}\nAn important application of (6.12) in curvilinear coordinates is the following. Suppose that we seek those curves for which $\\kappa=0$ (that is, the \"straight\" lines or geodesics). For positive definite metrics, this condition is equivalent to requiring that\n\n\n\\begin{equation*}\nb^{i}=\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{p q}^{i} \\frac{d x^{p}}{d s} \\frac{d x^{q}}{d s}=0 \\quad(i=1,2, \\ldots, n) \\tag{6.13}\n\\end{equation*}\n\n\nThe solution of this system of second-order differential equations will define the geodesics $x^{i}=x^{i}(s)$.\n\nEXAMPLE 6.4 In affine coordinates, where all $g_{i j}$ are constant and all Christoffel symbols vanish, integration of $(6.13)$ is immediate:\n\n$$\nx^{i}=\\alpha^{i} s+\\beta^{i} \\quad(i=1,2, \\ldots, n)\n$$\n\nwhere, $s$ being arc length, $g_{i j} \\alpha^{i} \\alpha^{j}=1$. Thus, from each point $\\mathbf{x}=\\boldsymbol{\\beta}$ of space there emanates a geodesic ray in every direction (unit vector) $\\boldsymbol{\\alpha}$.\n\n\\subsection*{6.6 RULES FOR TENSOR DIFFERENTIATION}\nConfidence in the preceding differentiation formulas should be considerably improved when it is learned (see Problem 6.15) that the same basic rules for differentiation from calculus carry over to covariant and absolute differentiation of tensors. For arbitrary tensors $\\mathbf{T}$ and $\\mathbf{S}$, we have:\n\n\\section*{Rules for Covariant Differentiation}\n\\$\\$\n\n\\$\\$\n\nSince the absolute derivative along a curve is the inner product of the covariant derivative and the tangent vector, the above rules for differentiation repeat:\n\n\\section*{Rules for Absolute Differentiation}\n$$\n\\begin{array}{ll} \n& \\frac{\\delta}{\\delta t}(\\mathbf{T}+\\mathbf{S})=\\frac{\\delta \\mathbf{T}}{\\delta t}+\\frac{\\delta \\mathbf{S}}{\\delta t} \\\\\n\\text { outer product } & \\frac{\\delta}{\\delta t}[\\mathbf{T S}]=\\left[\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}\\right]+\\left[\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\\right] \\\\\n\\text { inner product } & \\frac{\\delta}{\\delta t}(\\mathbf{T S})=\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}+\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\n\\end{array}\n$$\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{CHRISTOFFEL SYMBOLS OF THE FIRST KIND}\n6.1 Verify that $\\Gamma_{i j k}=\\Gamma_{j i k}$.\n\nBy definition,\n\n$$\n\\Gamma_{i j k}=\\frac{1}{2}\\left(-g_{i j k}+g_{j k i}+g_{k i j}\\right) \\quad \\text { and } \\quad \\Gamma_{j i k}=\\frac{1}{2}\\left(-g_{j i k}+g_{i k j}+g_{k j i}\\right)\n$$\n\nBut $g_{i j k}=g_{j i k}, g_{j k i}=g_{k j i}$, and $g_{k i j}=g_{i k i}$, by symmetry of $g_{i j}$, and the result follows.\n\n6.2 Show that if $\\left(g_{i j}\\right)$ is a diagonal matrix, then for all fixed subscripts $\\alpha$ and $\\beta \\neq \\alpha$ in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\alpha} \\quad($ not summed on $\\alpha$ )\n\n(b) $-\\Gamma_{\\alpha \\alpha \\beta}=\\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\quad$ (not summed on $\\alpha$ )\n\n(c) All remaining Christoffel symbols $\\Gamma_{i j k}$ are zero.\\\\\n(a) By definition, $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\alpha}$.\n\n(b) Since $\\alpha \\neq \\beta$,\n\n$$\n\\begin{aligned}\n& -\\Gamma_{\\alpha \\alpha \\beta}=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}\\right)=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+0+0\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\\\\n& \\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2}\\left(-0+0+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta}\n\\end{aligned}\n$$\n\n(c) Let $i, j, k$ be distinct subscripts. Then $g_{i j}=0$ and $g_{i j k}=0$, implying that $\\Gamma_{i j k}=0$.\n\n6.3 Is it true that if all $\\Gamma_{i j k}$ vanish in some coordinate system, then the metric tensor has constant components in every coordinate system?\n\nBy Lemma 6.2, the conclusion would be valid if the $\\Gamma_{i j k}$ vanished in every coordinate system. But $\\left(\\Gamma_{i j k}\\right)$ is not a tensor, and the conclusion is false. For instance, all $\\bar{\\Gamma}_{i j k}=0$ for the Euclidean metric in rectangular coordinates, but $g_{22}=\\left(x^{1}\\right)^{2}$ in spherical coordinates.\n\n\\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n6.4 If $\\left(g_{i j}\\right)$ is a diagonal matrix, show that for all fixed indices (no summation) in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\beta}^{\\alpha}=\\Gamma_{\\beta \\alpha}^{\\alpha}=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right)$\n\n(b) $\\Gamma_{\\beta \\beta}^{\\alpha}=-\\frac{1}{2 g_{\\alpha \\alpha}} g_{\\beta \\beta \\alpha} \\quad(\\alpha \\neq \\beta)$\n\n(c) All other $\\Gamma_{j k}^{i}$ vanish.\n\n(a) Both $\\left(g_{i j}\\right)$ and $\\left(g_{i j}\\right)^{-1}=\\left(g^{i j}\\right)$ are diagonal, with nonzero diagonal elements. Thus,\n\n\n\\begin{gather*}\n\\Gamma_{\\alpha \\beta}^{\\alpha}=g^{\\alpha j} \\Gamma_{\\alpha \\beta j}=g^{\\alpha \\alpha} \\Gamma_{\\alpha \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(\\frac{1}{2} \\frac{\\partial g_{\\alpha \\alpha}}{\\partial x^{\\beta}}\\right)=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right) \\\\\n\\Gamma_{\\beta \\beta}^{\\alpha}=g^{\\alpha \\alpha} \\Gamma_{\\beta \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(-\\frac{1}{2} g_{\\beta \\beta \\alpha}\\right) \\tag{b}\n\\end{gather*}\n\n\n(c) When $i, j, k$ are distinct, $\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r}=g^{i i} \\Gamma_{j k i}=0$ (not summed on $i$ ).\n\n6.5 Calculate the Christoffel symbols of the second kind for the Euclidean metric in spherical coordinates, using Problem 6.4.\n\nWe have $g_{11}=1, g_{22}=\\left(x^{1}\\right)^{2}$, and $g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}$. Noting that $g_{11}$ is a constant and that all $g_{\\alpha \\alpha}$ are independent of $x^{3}$, we obtain the following nonzero symbols from Problem 6.4(a):\n\n$$\n\\begin{aligned}\n& \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(x^{1}\\right)^{2}\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{31}^{3}=\\Gamma_{13}^{3}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{32}^{3}=\\Gamma_{23}^{3}=\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\cot x^{2}\n\\end{aligned}\n$$\n\nSimilarly, from Problem $6.4(b)$,\n\n$$\n\\begin{aligned}\n& \\Gamma_{22}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(x^{1}\\right)^{2}=-x^{1} \\\\\n& \\Gamma_{33}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-x^{1} \\sin ^{2} x^{2} \\\\\n& \\Gamma_{33}^{2}=-\\frac{1}{2\\left(x^{1}\\right)^{2}} \\frac{\\partial}{\\partial x^{2}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-\\sin x^{2} \\cos x^{2}\n\\end{aligned}\n$$\n\n6.6 Use (6.6) to find the most general 3-dimensional transformation $x^{i}=x^{i}(\\overline{\\mathbf{x}})$ of coordinates such that $\\left(x^{i}\\right)$ is rectangular and $\\left(\\bar{x}^{i}\\right)$ is any other coordinate system for which the Christoffel symbols are\n\n$$\n\\bar{\\Gamma}_{11}^{1}=1 \\quad \\bar{\\Gamma}_{22}^{2}=2 \\quad \\bar{\\Gamma}_{33}^{3}=3 \\quad \\text { all others }=0\n$$\n\nSince $\\Gamma_{s t}^{r}=0$, (6.6) reduces to the system of linear partial differential equations with constant coefficients:\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}}=\\bar{\\Gamma}_{i j}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}} \\tag{1}\n\\end{equation*}\n\n\nIt is simplest first to solve the intermediate, first-order system\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}^{r}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s}^{r} \\quad\\left(\\bar{u}_{s}^{r}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}\\right) \\tag{2}\n\\end{equation*}\n\n\nSince the systems (2) for $r=1,2,3$ are the same, temporarily replace $\\bar{u}_{s}^{r}$ by $\\bar{u}_{s}$, and $x^{r}$ by a single variable $x$; thus,\n\nFor $j=1$, (3) becomes\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s} \\tag{3}\n\\end{equation*}\n\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{1}}=\\bar{\\Gamma}_{11}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{11}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{11}^{3} \\bar{u}_{3}=\\bar{u}_{1} \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{2}}=\\bar{\\Gamma}_{21}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{21}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{21}^{3} \\bar{u}_{3}=0 \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{3}}=\\bar{\\Gamma}_{31}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{31}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{31}^{3} \\bar{u}_{3}=0\n\\end{aligned}\n$$\n\nHence $\\bar{u}_{1}$ is a function of $\\bar{x}^{1}$ alone, and the first differential equation integrates to give\n\n$$\n\\bar{u}_{1}=b_{1} \\exp \\bar{x}^{1} \\quad\\left(b_{1}=\\text { constant }\\right)\n$$\n\nIn the same way, we find for $j=2$ and $j=3$ :\n\n$$\n\\begin{array}{ll}\n\\bar{u}_{2}=b_{2} \\exp 2 \\bar{x}^{2} & \\left(b_{2}=\\text { constant }\\right) \\\\\n\\bar{u}_{3}=b_{3} \\exp 3 \\bar{x}^{3} & \\left(b_{3}=\\text { constant }\\right)\n\\end{array}\n$$\n\nNow we return to the equations $\\partial x / \\partial \\bar{x}^{i}=\\bar{u}_{i}$ with the solutions just found for the $\\bar{u}_{i}$.\n\n\n\\begin{equation*}\n\\frac{\\partial x}{\\partial \\bar{x}^{1}}=b_{1} \\exp \\bar{x}^{1} \\quad-\\frac{\\partial x}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\frac{\\partial x}{\\partial \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\tag{4}\n\\end{equation*}\n\n\nIntegration of the first equation (4) yields\n\n$$\nx=b_{1} \\exp \\bar{x}^{1}+\\varphi\\left(\\bar{x}^{2}, \\bar{x}^{3}\\right)\n$$\n\nand then the second and third equations give:\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\varphi}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\text { or } \\quad \\varphi=a_{2} \\exp 2 \\bar{x}^{2}+\\psi\\left(\\bar{x}^{3}\\right) \\\\\n& \\frac{d \\psi}{d \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\quad \\text { or } \\quad \\psi=a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n\\end{aligned}\n$$\n\nThis means that, with $a_{1}=b_{1}$,\n\n$$\nx=a_{1} \\exp \\bar{x}^{1}+a_{2} \\exp 2 \\bar{x}^{2}+a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n$$\n\nso that the general solution of (1) is\n\n\n\\begin{equation*}\nx^{r}=a_{1}^{r} \\exp \\bar{x}^{1}+a_{2}^{r} \\exp 2 \\bar{x}^{2}+a_{3}^{r} \\exp 3 \\bar{x}^{3}+a_{4}^{r} \\tag{5}\n\\end{equation*}\n\n\nfor $r=1,2,3$.\n\nThe constants $a_{4}^{r}$ in (5) are unimportant; they merely allow any point in $\\mathbf{R}^{3}$ to serve as the origin of the rectangular system $\\left(x^{r}\\right)$. The remaining constants may be chosen at will, subject to a single condition (see Problem 6.27).\n\n\\section*{COVARIANT DIFFERENTIATION}\n6.7 Establish the tensor character of $\\mathbf{T}_{, k}$ (Definition 2), where $\\mathbf{T}$ is a contravariant vector.\n\nBeginning with the transformation law\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\ntake the partial derivative with respect to $\\bar{x}^{k}$ and use the chain rule:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial}{\\partial x^{s}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{s} \\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nNow use (6.6), with barred and unbarred systems interchanged:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r}\\left(\\Gamma_{s r}^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}}-\\bar{\\Gamma}_{u v}^{i} \\frac{\\partial \\bar{x}^{u}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{v}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nSince $\\left(\\partial \\bar{x}^{u} / \\partial x^{s}\\right)\\left(\\partial x^{s} / \\partial \\bar{x}^{k}\\right)=\\delta_{k}^{u}$ and $T^{r}\\left(\\partial \\bar{x}^{v} / \\partial x^{r}\\right)=\\bar{T}^{\\nu}$, this becomes\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+\\Gamma_{s r}^{t} T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}-\\bar{\\Gamma}_{k v}^{i} \\bar{T}^{v}\n$$\n\nor (by factoring and using the symmetry of the Christoffel symbols)\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}+\\bar{\\Gamma}_{t k}^{i} \\bar{T}^{t}=\\left(\\frac{\\partial T^{r}}{\\partial x^{s}}+\\Gamma_{t s}^{r} T^{t}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\quad \\text { or } \\quad \\bar{T}_{, k}^{i}=T_{, s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\n6.8 Show that $\\left(T_{j, k}^{i}\\right)$, as defined by (6.7), is a tensor, using the previously proven facts that $T_{, k}^{i}$ and $T_{i, k}$ are tensorial for all tensors $\\left(T^{i}\\right)$ and $\\left(T_{i}\\right)$.\n\nLet $\\left(V_{i}\\right)$ be any vector and set $U_{j}=T_{j}^{r} V_{r}$. The covariant derivative of the tensor $\\left(U_{j}\\right)$ is the tensor $\\left(U_{j, k}\\right)$, where\n\n$$\nU_{j, k}=\\frac{\\partial U_{j}}{\\partial x^{k}}-\\Gamma_{j k}^{r} U_{r}=\\frac{\\partial}{\\partial x^{k}}\\left(T_{j}^{r} V_{r}\\right)-\\Gamma_{j k}^{r}\\left(T_{r}^{s} V_{s}\\right)=\\frac{\\partial T_{j}^{s}}{\\partial x^{k}} V_{s}+T_{j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{j k}^{r} T_{r}^{s} V_{s}\n$$\n\nBut\n\n$$\nV_{r, k}=\\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{r k}^{s} V_{s} \\quad \\text { or } \\quad \\frac{\\partial V_{r}}{\\partial x^{k}}=V_{r, k}+\\Gamma_{r k}^{s} V_{s}\n$$\n\nWhen the above expression for $\\partial V_{r} / \\partial x^{k}$ is substituted into the preceding equation and the terms rearranged, the result is:\n\n$$\n\\left(\\frac{\\partial T_{j}^{s}}{\\partial x^{k}}+\\Gamma_{r k}^{s} T_{j}^{r}-\\Gamma_{j k}^{r} T_{r}^{s}\\right) V_{s}=U_{j, k}-T_{j}^{r} V_{r, k}\n$$\n\ni.e.,\n\n$$\nT_{j, k}^{s} V_{s}=\\text { tensor component }\n$$\n\nIt follows at once from the Quotient Theorem (Theorem 4.2) that $\\left(T_{j, k}^{i}\\right)$ is a tensor.\n\n6.9 Extend the notion of covariant differentiation so that it will apply to invariants.\n\nFirst note that the partial derivative of an invariant is a tensor:\n\n$$\n\\frac{\\partial \\bar{E}}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nNow, under any reasonable definition, $\\left(E_{, i}\\right)$ must (1) be a tensor; (2) coincide with ( $\\left.\\partial E / \\partial x^{i}\\right)$ in rectangular coordinates. The obvious choice is therefore\n\n$$\n\\left(E_{, i}\\right) \\equiv\\left(\\frac{\\partial E}{\\partial x^{i}}\\right)\n$$\n\n6.10 Write the formula for the covariant derivative indicated by $T_{k, l}^{i j}$.\n\n$$\nT_{k, l}^{i j}=\\frac{\\partial T_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{r l}^{i} T_{k}^{r j}+\\Gamma_{r l}^{j} T_{k}^{i r}-\\Gamma_{k l}^{r} T_{r}^{i j}\n$$\n\n6.11 Prove that the metric tensor behaves like a constant under covariant differentiation; i.e., $g_{i j, k}=0$ for all $i, j, k$.\n\nBy definition, since $\\left(g_{i j}\\right)$ is covariant of order 2 ,\n\n$$\ng_{i j, k}=\\frac{\\partial g_{i j}}{\\partial x^{k}}-\\Gamma_{i k}^{r} g_{r j}-\\Gamma_{j k}^{r} g_{i r}=g_{i j k}-\\Gamma_{i k j}-\\Gamma_{j k i}=0\n$$\n\nby (6.2). (In a similar manner, it follows that $g^{i j}=0$; see Problem 6.34.)\n\nBecause of the above property of the metric tensor and its inverse, the operation of covariant differentiation commutes with those of raising and lowering indices. For example,\n\n$$\nT_{j, k}^{i}=\\left(g^{i r} T_{r j}\\right)_{, k}=g^{i r} T_{r j, k}\n$$\n\n\\section*{ABSOLUTE DIFFERENTIATION}\n6.12 Prove that (6.8) is the result of forming the inner product of the covariant derivative $\\left(T_{, j}^{i}\\right)$ with the tangent vector $\\left(d x^{i} / d t\\right)$ of the curve.\n\n$$\nT_{, j}^{i} \\frac{d x^{j}}{d t}=\\left(\\frac{\\partial T^{i}}{\\partial x^{j}}+\\Gamma_{r j}^{i} T^{r}\\right) \\frac{d x^{j}}{d t}=\\frac{\\partial T^{i}}{\\partial x^{j}} \\frac{d x^{j}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}=\\frac{d T^{i}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}\n$$\n\n6.13 A particle is in motion along the circular arc given parametrically in spherical coordinates by $x^{1}=b, x^{2}=\\pi / 4, x^{3}=\\omega t \\quad(t=$ time $)$. Find its acceleration using the formula (6.10) and compare with the result $a=r \\omega^{2}$ from elementary mechanics.\n\nFrom Problem 6.5, we have along the circle\n\n$$\n\\begin{array}{cc}\n\\Gamma_{22}^{1}=-x^{1}=-b & \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}=-b \\sin ^{2} \\frac{\\pi}{4}=-\\frac{b}{2} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2}=-\\sin \\frac{\\pi}{4} \\cos \\frac{\\pi}{4}=-\\frac{1}{2} \\\\\n\\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}=\\cot \\frac{\\pi}{4}=1\n\\end{array}\n$$\n\nwith all other symbols vanishing. The components of acceleration are, from (6.9),\n\n$$\n\\begin{aligned}\n& a^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{r s}^{1} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{b}{2}\\right)(\\omega)^{2}=-\\frac{b \\omega^{2}}{2} \\\\\n& a^{2}=\\frac{d^{2} x^{2}}{d t^{2}}+\\Gamma_{r s}^{2} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{1}{2}\\right)(\\omega)^{2}=-\\frac{\\omega^{2}}{2} \\\\\n& a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}+\\Gamma_{r s}^{3} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0\n\\end{aligned}\n$$\n\nTogether with the metric components along the circle,\n\n$$\ng_{11}=1 \\quad g_{22}=\\left(x^{1}\\right)^{2}=b^{2} \\quad g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}=\\frac{b^{2}}{2}\n$$\n\nthe acceleration components give, via (6.10),\n\n$$\na=\\sqrt{g_{i j} a^{i} a^{j}}=\\sqrt{(1)\\left(-b \\omega^{2} / 2\\right)^{2}+\\left(b^{2}\\right)\\left(-\\omega^{2} / 2\\right)^{2}+0}=b \\omega^{2} / \\sqrt{2}\n$$\n\nUpon introducing the radius of the circle, using (3.4) with $\\bar{x}^{1}=x=r$ and $x^{3}=0$,\n\nwe obtain $a=r \\omega^{2}$.\n\n$$\nr=b \\sin \\frac{\\pi}{4}=\\frac{b}{\\sqrt{2}}\n$$\n\n6.14 Verify that $x^{1}=a \\sec x^{2}$ is a geodesic for the Euclidean metric in polar coordinates. [In rectangular coordinates $(x, y)$, the curve is $x=a$, a vertical line.]\n\nFirst choose a parameterization for the curve; say,\n\n\n\\begin{align*}\n& x^{1}=a \\sec t  \\tag{1}\\\\\n& x^{2}=t\n\\end{align*} \\quad(-\\pi / 2<t<\\pi / 2)\n\n\nParameter $t$ is related to the arc-length parameter $s$ via\n\n$$\n\\begin{aligned}\n\\frac{d s}{d t} & =\\sqrt{g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+(a \\sec t)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{a^{2} \\sec ^{2} t \\tan ^{2} t+a^{2} \\sec ^{2} t} \\\\\n& =(a \\sec t) \\sqrt{1+\\tan ^{2} t}=a \\sec ^{2} t\n\\end{aligned}\n$$\n\nor\n\n$$\n\\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a}\n$$\n\nso that for any function $x(t)$\n\n$$\n\\begin{aligned}\n& \\frac{d x}{d s}=\\frac{d x}{d t} \\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t} \\\\\n& \\frac{d^{2} x}{d s^{2}}=\\frac{d}{d t}\\left(\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t}\\right) \\frac{d t}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x}{d t}\n\\end{aligned}\n$$\n\nNow, taking the nonzero Christoffel symbols from Example 6.3, we can rewrite the geodesic equations (6.13) in terms of the independent variable $t$ :\n\n$$\n0=\\frac{d^{2} x^{1}}{d s^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{1}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{1}}{d t}+\\left(-x^{1}\\right) \\frac{\\cos ^{4} t}{a^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{1}}{d t^{2}}-(2 \\tan t) \\frac{d x^{1}}{d t}-x^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\tag{2}\n\\end{equation*}\n\n\nand\n\n$$\n0=\\frac{d^{2} x^{2}}{d s^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d s} \\frac{d x^{2}}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{2}}{d t}+2\\left(\\frac{1}{x^{1}}\\right)\\left(\\frac{\\cos ^{2} t}{a}\\right)^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{2}}{d t^{2}}-(2 \\tan t) \\frac{d x^{2}}{d t}+\\left(\\frac{2}{x^{1}}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=0 \\tag{3}\n\\end{equation*}\n\n\nAll that remains is to verify that the functions (1) satisfy the system (2)-(3). Substituting in (2):\n\n$$\na\\left(\\sec t+2 \\sec t \\tan ^{2} t\\right)-(2 \\tan t)(a \\sec t \\tan t)-(a \\sec t)(1)=0\n$$\n\nSubstituting in (3):\n\n$$\n0-(2 \\tan t)(1)+\\left(\\frac{2}{a \\sec t}\\right)(a \\sec t \\tan t)(1)=0 \\quad \\text { QED }\n$$\n\n\\section*{DIFFERENTIATION RULES}\n6.15 Prove the rules for covariant differentiation stated in Section 6.6.\n\n(a) The sum rule obviously holds, as (6.7) is linear in the tensor components.\n\n(b) Let $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ and $\\mathbf{S}=\\left(S_{j}^{i}\\right)$ be two mixed tensors of order 2, with outer product $\\mathbf{U}=\\left(T_{r}^{p} S_{s}^{q}\\right)$. Then,\n\n$$\n\\begin{aligned}\n& T_{r, k}^{p} S_{s}^{q}+T_{r}^{p} S_{s, k}^{q} \\\\\n& \\quad=\\left(\\frac{\\partial T_{r}^{p}}{\\partial x^{k}}+\\Gamma_{t k}^{p} T_{r}^{t}-\\Gamma_{r k}^{t} T_{r}^{p}\\right) S_{s}^{q}+T_{r}^{p}\\left(\\frac{\\partial S_{s}^{q}}{\\partial x^{k}}+\\Gamma_{t i k}^{q} S_{s}^{t}-\\Gamma_{s k}^{t} S_{t}^{q}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\underbrace{\\left.\\frac{\\partial T_{r}^{p}}{\\partial x^{k}} S_{s}^{q}+T_{r}^{p} \\frac{\\partial S_{s}^{q}}{\\partial x^{k}}\\right)}_{\\partial U_{r s}^{p q} / \\partial x^{k}}+\\Gamma_{t k}^{p} U_{r s}^{t q}+\\Gamma_{t k}^{q} U_{r s}^{p t}-\\Gamma_{r k}^{t} U_{t s}^{p q}-\\Gamma_{s k}^{t} U_{r t}^{p q} \\\\\n& \\equiv U_{r s, k}^{p q}\n\\end{aligned}\n$$\n\nand this proof of the outer-product rule extends to arbitrary $\\mathbf{T}$ and $\\mathbf{S}$.\n\n(c) The inner-product rule follows from the outer-product rule and the following useful result: Contraction of indices and covariant differentiation commute. To prove this last, let $\\mathbf{R}=\\left(R_{k}^{i j}\\right)$. Then,\n\n$$\n\\begin{aligned}\nR_{k, l}^{i j} \\delta_{j}^{k} & =\\left(\\frac{\\partial R_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{i j}+\\Gamma_{t l}^{j} R_{k}^{i t}-\\Gamma_{k l}^{t} R_{t}^{i j}\\right) \\delta_{j}^{k} \\\\\n& =\\frac{\\partial R_{k}^{i k}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{t k}+0=\\left(R_{k}^{i k}\\right)_{, l} \\quad \\text { QED }\n\\end{aligned}\n$$\n\n6.16 Instead of Problem 6.15, why not: \"Each rule is a tensor equation that is valid in rectangular coordinates, where covariant differentiation reduces to partial differentiation. Therefore, each rule holds in every coordinate system.'?\n\nIf the space metric is non-Euclidean, there is no way to transform to a rectangular coordinate system (in which the rules would indeed hold).\n\n6.17 Infer the outer-product rule for absolute differentation from the corresponding rule for covariant differentiation.\n\nLet $\\mathbf{x}=\\mathbf{x}(t)$ be any curve, and $\\mathbf{T}(\\mathbf{x}(t))$ and $\\mathbf{S}(\\mathbf{x}(t))$ two tensors defined on the curve. Then,\n\n$$\n\\frac{\\delta}{\\delta t}[\\mathbf{T S}]=[\\mathbf{T S}]_{, k} \\frac{d x^{k}}{d t}=\\left(\\left[\\mathbf{T}_{, k} \\mathbf{S}\\right]+\\left[\\mathbf{T S}_{, k}\\right]\\right) \\frac{d x^{k}}{d t}=\\left[\\mathbf{T}_{, k} \\frac{d x^{k}}{d t} \\mathbf{S}\\right]+\\left[\\mathbf{T ~}_{, k} \\frac{d x^{k}}{d t}\\right]=\\left[\\frac{\\delta \\mathbf{T}}{\\delta t} \\mathbf{S}\\right]+\\left[\\mathbf{T} \\frac{\\delta \\mathbf{S}}{\\delta t}\\right]\n$$\n\n\\section*{UNIQUENESS OF THE ABSOLUTE DERIVATIVE}\n6.18 Prove Theorem 6.4.\n\nDenote by $\\Delta \\mathbf{T} / \\Delta t$ any tensor that satisfies the hypothesis of the theorem. The tensor equation\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t}=\\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\nis valid in rectangular coordinates $\\left(x^{i}\\right)$, since, in $\\left(x^{i}\\right)$, both sides coincide with $d \\mathbf{T} / d t$. But then (Section 4.3) the equation holds in every coordinate system; i.e.,\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n6.19 Find the general solution of the linear system\n\n$$\n\\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{j} \\partial x^{k}}=a_{j k}^{i}=\\text { const. }\n$$\n\nwith $a_{j k}^{i}$ symmetric in the two lower subscripts. [Hint: Set $y_{k}^{i}=\\partial \\bar{x}^{i} / \\partial x^{k}-a_{r k}^{i} x^{r}$.]\n\n6.20 A two-dimensional coordinate system $\\left(x^{i}\\right)$ is connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=2\\left(x^{1}\\right)^{2}+x^{2} \\quad \\bar{x}^{2}=-x^{1}+3 x^{2}\n$$\n\n(a) Exhibit the metric tensor in $\\left(x^{i}\\right)$.\n\n(b) Calculate the Christoffel symbols of the first kind for $\\left(x^{i}\\right)$ directly from the definition (6.1).\n\n6.21 (a) Derive the formula\n\n$$\n\\Gamma_{i j k}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{j}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is rectangular and $\\left(x^{i}\\right)$ is any other coordinate system. [Hint: Interchange barred and unbarred coordinate systems in (6.3)]. (b) Derive the analogous formula\n\n$$\n\\Gamma_{j k}^{i}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{k}} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is such that all $\\bar{g}_{i j}$ arc constant. [Hint: Interchange barred and unbarred coordinate systems in $(6.5)$.]\n\n6.22 Let the coordinate system $\\left(x^{i}\\right)$ be connected to a system of rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\quad \\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n$$\n\nUse Problem 6.21(b) to compute the nonzero Christoffel symbols of the second kind for $\\left(x^{i}\\right)$.\n\n6.23 If\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=-\\exp d_{1} x^{1}+\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{2}=2 \\exp d_{1} x^{1}-\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{3}=\\exp d_{1} x^{1}-2 \\exp d_{2} x^{2}+3 \\exp d_{3} x^{3}\n\\end{aligned}\n$$\n\nand if all $\\bar{\\Gamma}_{j k}^{i}=0$, find the $\\Gamma_{j k}^{i}$.\n\n6.24 Derive (6.6) by solving (6.5) for the second derivative and then changing indices.\n\n6.25 Prove that all $\\Gamma_{j k}^{i}$ vanish only if all $g_{i j}$ are constant.\n\n6.26 Calculate the nonzero Christoffel symbols of both kinds for the Euclidean metric in cylindrical coordinates, (5.3).\n\n6.27 Express the condition that the transformation (5) of Problem 6.6 be bijective (Section 2.6).\n\n6.28 Show that if $\\Gamma_{i j k}$ are constant, then $g_{i j}$ are linear in the variables $\\left(x^{i}\\right)$; but that this is not necessarily true if $\\Gamma_{j k}^{i}$ are constant. (For a counterexample, use the metric $g_{11}=\\exp 2 x^{1}, g_{12}=g_{21}=0, g_{22}=1$.)\n\n6.29 What is the most general two-dimensional transformation $\\bar{x}^{i}=\\bar{x}^{i}(\\mathbf{x})$ of coordinates such that $\\left(\\bar{x}^{i}\\right)$ are rectangular and the Christoffel symbols $\\Gamma_{j k}^{i}$ in $\\left(x^{i}\\right)$ are those for the metric of polar coordinates (Example 6.3)?\n\n6.30 Is the covariant derivative of a tensor with constant components equal to zero as in ordinary differentiation? Explain your answer.\n\n6.31 If $T_{j r s}^{i}$ are tensor components, write out the components of the covariant derivative, $T_{j r s, k}^{i}$.\n\n6.32 Show that $\\delta_{i, k}^{i}=0$ for all $i, j, k$.\n\n6.33 For any tensor $\\mathbf{T}$, verify that $(\\mathbf{g} * \\mathbf{T})_{, k}=\\mathbf{g} * \\mathbf{T}{ }_{, k}$, where $*$ denotes either an outer or inner product.\n\n6.34 Use Problem 6.32 and $g_{j r} g^{r i}=\\delta_{j}^{i}$ to show that the covariant derivative of $\\mathbf{g}^{-1}$ is zero.\n\n6.35 Use the recursive method of Problem 6.8 to verify that $\\left(T_{i j, k}\\right)$ is a tensor.\n\n6.36 Using tensor methods in polar coordinates, find the curvature of the circle\n\n$$\nx^{1}=b \\quad x^{2}=t\n$$\n\n6.37 If the metric for $\\left(x^{i}\\right)$ is\n\n$$\nG=\\left[\\begin{array}{cc}\n\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\n(a) write the differential equations of the geodesics in terms of the dependent variables $u=\\left(x^{1}\\right)^{2}$ and $v=x^{2} ;(b)$ integrate these equations and eliminate the arc-length parameter from the solution.\n\n6.38 Find the geodesics on the surface of a sphere of radius $a$ by $(a)$ writing the geodesic equations for the spherical coordinates $x^{2}$ and $x^{3}$ (the $x^{1}$-equation is trivial for $x^{1}=a=$ const. and may be ignored); $(b)$ exhibiting a particular solution of these two equations; and (c) generalizing on (b). Use Problem 6.5 for the Christoffel symbols.\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 7}", "\\section*{Riemannian Geometry of Curves}\n\\subsection*{7.1 INTRODUCTION}\nAt this point, some new terminology is introduced, which commemorates the general formulation of $n$-dimensional geometry by Bernhard Riemann (1826-1866).\n\nDefinition 1: A Riemannian space is the space $\\mathbf{R}^{n}$ coordinatized by $\\left(x^{i}\\right)$, together with a fundamental form or Riemannian metric, $g_{i j} d x^{i} d x^{j}$, where $\\mathbf{g}=\\left(g_{i j}\\right)$ obeys conditions A-D of Section 5.3.\n\nThus, in our preliminary treatment of angles, tangents, normals, and geodesic curves, in Chapters 5 and 6 , we already entered Riemannian geometry-though largely restricted to familiar 3-dimensional coordinate systems and a positive definite (Euclidean) metric. The present chapter focuses on the theory of curves in a Riemannian space with an indefinite metric. It also takes up geodesics from a different viewpoint.\n\n\\subsection*{7.2 LENGTH AND ANGLE UNDER AN INDEFINITE METRIC}\nFormulas (5.10) and (5.11) must be generalized to allow for changes in sign of the fundamental form.\n\nDefinition 2: The norm of an arbitrary (contravariant or covariant) vector $\\mathbf{V}$ is\n\n$$\n\\|\\mathbf{V}\\| \\equiv \\sqrt{\\varepsilon \\mathbf{V}^{2}}=\\sqrt{\\varepsilon V_{i} V^{i}} \\quad(\\varepsilon=\\varepsilon(\\mathbf{V}))\n$$\n\nwhere $\\varepsilon(\\quad)$ is the indicator function (Section 5.3).\n\nUnder this definition, $\\|\\mathbf{V}\\| \\geqq 0$, but it is possible that $\\|\\mathbf{V}\\|=0$ for $\\mathbf{V} \\neq \\mathbf{0}$; such a vector is called a null vector. Moreover, the triangle inequality is not necessarily obeyed by this norm (see Problem 7.8).\n\nIf $\\mathbf{V}(t)$ is the tangent field to the curve $x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$, then the length formula (5.1a) may be written as\n\n\n\\begin{equation*}\nL=\\int_{a}^{b} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}} d t=\\int_{a}^{b}\\|\\mathbf{V}(t)\\| d t \\tag{7.1}\n\\end{equation*}\n\n\nThe angle between non-null contravariant vectors is still defined by (5.11), provided the new norm is understood:\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|V\\|}=\\frac{g_{i j} U^{i} V^{j}}{\\sqrt{\\varepsilon_{1} g_{p q} U^{p} U^{q}} \\sqrt{\\varepsilon_{2} g_{r s} V^{r} V^{s}}} \\tag{7.2}\n\\end{equation*}\n\n\nwhere $\\varepsilon_{1}=\\varepsilon(\\mathbf{U})$ and $\\varepsilon_{2}=\\varepsilon(\\mathbf{V})$. Because of the indefiniteness of the metric, we must distinguish two possibilities in the application of (7.2).\n\nCase 1: $|\\mathbf{U V}| \\leqq\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|$ (the Cauchy-Schwarz inequality holds for $\\mathbf{U}$ and $\\mathbf{V}$ ). Then $\\theta$ is uniquely determined as a real number in the interval $[0, \\pi]$.\n\nCase 2: $|\\mathbf{U V}|>\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|$ (the Cauchy-Schwarz does not hold). Then (7.2) takes the form\n\n$$\n\\cos \\theta=k \\quad(|k|>1)\n$$\n\nwhich has an infinite number of solutions for $\\theta$, all of them complex. By convention, we always choose the solution\n\n$$\n\\theta= \\begin{cases}i \\ln \\left(k+\\sqrt{k^{2}-1}\\right) & k>1 \\\\ \\pi+i \\ln \\left(-k+\\sqrt{k^{2}-1}\\right) & k<-1\\end{cases}\n$$\n\nthat exhibits the proper limiting behavior as $k \\rightarrow 1^{+}$or $k \\rightarrow-1^{-}$.\n\nEXAMPLE 7.1 At the points of intersection, find the angles between the curves (i.e., between their tangents)\n\n$$\n\\mathscr{C}_{1}:\\left(x_{1}^{i}\\right)=\\left(t, 0,0, t^{2}\\right) \\quad \\mathscr{C}_{2}:\\left(x_{2}^{i}\\right)=\\left(u, 0,0,2-u^{2}\\right)\n$$\n\n( $t, u$ real), if the Riemannian metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}-\\left(d x^{4}\\right)^{2}\n$$\n\n[This is the metric of Special Relativity, with $x^{4} \\equiv$ (speed of light) $\\times($ time $)$.]\n\nThe curves meet in the two points $P(1,0,0,1)$ and $Q(-1,0,0,1)$. At $P$, (where $t=u=1$ ) the two tangent vectors are\n\n$$\n\\begin{aligned}\n& \\mathbf{U}_{P}=\\left(d x_{1}^{i} / d t\\right)_{P}=(1,0,0,2 t)_{P}=(1,0,0,2) \\\\\n& \\mathbf{V}_{P}=\\left(d x_{2}^{i} / d u\\right)_{P}=(1,0,0,-2 u)_{P}=(1,0,0,-2)\n\\end{aligned}\n$$\n\nso that (7.2) gives\n\n$$\n\\begin{aligned}\n\\cos \\theta_{P} & =\\frac{1(1)(1)+1(0)(0)+1(0)(0)-1(2)(-2)}{\\sqrt{\\varepsilon_{1}\\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(2)^{2}\\right]} \\sqrt{\\varepsilon_{2}\\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(-2)^{2}\\right]}} \\\\\n& =\\frac{5}{\\sqrt{+3} \\sqrt{+3}}=\\frac{5}{3}\n\\end{aligned}\n$$\n\nand $\\theta_{P}=i \\ln \\left[(5 / 3)+\\sqrt{(5 / 3)^{2}-1}\\right]=i \\ln 3$.\n\nSimilarly we calculate (for $-t=u=1$ )\n\nso that $\\theta_{Q}=\\theta_{P}$.\n\n$$\n\\mathbf{U}_{Q}=(1,0,0,-2)=\\mathbf{V}_{P} \\quad \\mathbf{V}_{Q}=(1,0,0,2)=\\mathbf{U}_{P}\n$$\n\n\\subsection*{7.3 NULL CURVES}\nIf $\\mathbf{g}$ is not required to be positive definite, a curve can have zero length.\n\nEXAMPLE 7.2 In $\\mathbf{R}^{4}$, under the metric of Example 7.1, consider the curve\n\n$$\nx^{1}=3 \\cos t \\quad x^{2}=3 \\sin t \\quad x^{3}=4 t \\quad x^{4}=5 t\n$$\n\nfor $0 \\leqq t \\leqq 1$. Along the curve,\n\n$$\n\\begin{aligned}\n\\left(\\frac{d x^{i}}{d t}\\right) & =(-3 \\sin t, 3 \\cos t, 4,5) \\\\\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2} & =g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=(-3 \\sin t)^{2}+(3 \\cos t)^{2}+(4)^{2}-(5)^{2}=0\n\\end{aligned}\n$$\n\nand so the arc length is\n\n$$\nL=\\int_{0}^{1} 0 d t=0\n$$\n\nA curve is null if it or any of its subarcs has zero length. Here, a subarc is understood to be nontrivial; that is, it consists of more than one point and corresponds to an interval $c \\leqq t \\leqq d$, where $c<d$. A curve is null at a point if for some value of the parameter $t$ the tangent vector is a null vector; i.e.,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=0\n$$\n\nThe set of $t$-values at which the curve is null is known as the null set of the curve.\n\nUnder the above definitions, a curve can be null without having zero length (if there is a subarc with zero length); but a curve having zero length is necessarily null at every point, and hence a null curve. Example 7.2 gives such a curve.\n\nEXAMPLE 7.3 Under the Riemannian metric\n\n$$\nG=\\left[\\begin{array}{cc}\n\\left(x^{1}\\right)^{2} & -1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\nthe curve $\\left(x^{1}, x^{2}\\right)=\\left(t,\\left|t^{3}\\right| / 6\\right)$ possesses a null subarc that renders the length of the curve much smaller than might be expected. In fact, because $d x^{1} / d t=1$ and $d x^{2} / d t=\\delta t^{2} / 2$, where $\\delta= \\pm 1$ and is positive if $t \\geqq 0$,\n\n$$\n\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\varepsilon\\left[\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{1}}{d t}\\right)^{2}-2 \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}\\right]=\\varepsilon\\left[t^{2}(1)-2(1)\\left(\\delta t^{2} / 2\\right)\\right]=\\varepsilon\\left(t^{2}-\\delta t^{2}\\right)\n$$\n\nSince the quantity following the indicator is nonnegative, $\\varepsilon=+1$ everywhere. But note that $t^{2}-\\delta t^{2}=0$ if $t \\geqq 0$. Hence,\n\n$$\n\\begin{aligned}\nL & =\\int_{-1}^{999} \\sqrt{t^{2}-\\delta t^{2}} d t=\\int_{-1}^{0} \\sqrt{2 t^{2}} d t+\\int_{0}^{999} 0 d t=\\sqrt{2} \\int_{-1}^{0}(-t) d t \\\\\n& =-\\sqrt{2} t^{2} /\\left.2\\right|_{-1} ^{0}=\\sqrt{2} / 2 \\approx 0.707\n\\end{aligned}\n$$\n\nThe interpretation in rectangular coordinates $\\left(x^{2}, x^{2}\\right)$ is queer: As a particle travels less than a millimeter along the curve, its \"shadow\" on the $x^{1}$-axis travels a meter!\n\n\\section*{Nonexistence of an Arc-Length Parameter}\nFor a positive definite metric, the arc-length parameter $s$ is well-defined by (5.6) as a strictly increasing function of the curve parameter $t$. (Then it is also the case that $t$ is a strictly increasing function of $s$.) This fact allowed us freely to convert between the two parameterizations in the Solved Problems to Chapter 6. However, it is clear that on a null curve, which possesses at least one interval $t_{1}<t<t_{2}$ of null points, it is impossible to define arc length $s$. rule,\n\nIndeed, even isolated points of nullity pose analytical problems. For if $s^{\\prime}\\left(t_{0}\\right)=0$, then the chain\n\n\n\\begin{equation*}\n\\frac{d x^{i}}{d s}=\\frac{d x^{i}}{d t} \\frac{1}{s^{\\prime}(t)} \\tag{7.3}\n\\end{equation*}\n\n\nbreaks down at $s_{0}$, the image of $t_{0}$. When necessary, we get around the difficulty by restricting attention to curves that are regular.\n\nDefinition 3: A curve is regular if it has no null points (i.e., $d s / d t>0$ ).\n\nIt will be further assumed that all curves are of sufficiently high differentiability class to permit the theory considered; usually, this will require the assumption that curves are of class $C^{2}$.\n\n\\subsection*{7.4 REGULAR CURVES: UNIT TANGENT VECTOR}\nLet a regular curve $\\mathscr{C}: x^{i}=x^{i}(s)$ be given in terms of the arc-length parameter; the tangent field is $\\mathbf{T} \\equiv\\left(d x^{i} / d s\\right)$. By definition of arc length,\n\n$$\ns=\\int_{0}^{s}\\|\\mathbf{T}(u)\\| d u\n$$\n\nand differentiation gives $1=\\|\\mathbf{T}(s)\\|$, showing that $\\mathbf{T}$ has unit length at each point of $\\mathscr{C}$.\n\nWhen it is inconvenient or impossible to convert to the arc-length parameter, we can, by (7.3),\\\\\nobtain $\\mathbf{T}$ by normalizing the tangent vector $\\mathbf{U}=\\left(d x^{i} / d t\\right)$ :\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{1}{\\|\\mathbf{U}\\|} \\mathbf{U}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U} \\tag{7.4}\n\\end{equation*}\n\n\nIn Problem 7.20 is proved the useful\n\nTheorem 7.1: The absolute derivative $\\delta \\mathbf{T} / \\delta s$ of the unit tangent vector $\\mathbf{T}$ is orthogonal to $\\mathbf{T}$.\n\n\\subsection*{7.5 REGULAR CURVES: \\\\\n UNIT PRINCIPAL NORMAL AND CURVATURE}\nAlso associated with a regular curve $\\mathscr{C}$ is a vector orthogonal to the tangent vector. It may be introduced in two ways: (1) as the normalized $\\delta \\mathbf{T} / \\delta s$, if it exists; (2) as any differentiable unit vector orthogonal to $\\mathbf{T}$ and proportional to $\\delta \\mathbf{T} / \\delta s$ when $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$. The latter definition is global in nature, and it applies to a larger class of curves than does the former.\n\n\\section*{Analytical (Local) Approach}\nAt any point of $\\mathscr{C}$ at which $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, define the unit principal normal as the vector\n\n\n\\begin{equation*}\n\\mathbf{N}_{0} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta s} /\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\| \\tag{7.5}\n\\end{equation*}\n\n\nThe absolute curvature is the scale factor in (7.5):\n\n\n\\begin{equation*}\n\\kappa_{0} \\equiv\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\sqrt{\\varepsilon g_{i j} \\frac{\\delta T^{i}}{\\delta s} \\frac{\\delta T^{j}}{\\delta s}} \\tag{7.6}\n\\end{equation*}\n\n\nThis notion of curvature was informally defined in (6.12).\n\nCalling this quantity \"curvature\" is suggestive of the fact that in rectangular coordinates $\\|\\delta \\mathbf{T} / \\delta s\\|=\\|d \\mathbf{T} / d s\\|$ measures the rate of change of the tangent vector with respect to distance, or how sharply $\\mathscr{C}$ \"bends\" at each point. Substitution of (7.6) into (7.5) yields one of the Frenet equations:\n\n\n\\begin{equation*}\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\kappa_{0} \\mathbf{N}_{0} \\quad\\left(\\kappa_{0} \\neq 0\\right) \\tag{7.7}\n\\end{equation*}\n\n\nWhile this approach is simple and concise, it does not apply to many curves we want to consider; for instance, a geodesic-as defined by (6.13) - will not possess a local normal $\\mathbf{N}_{0}$ at any point. Even if there is only one point of zero curvature and the metric is Euclidean, $\\mathbf{N}_{0}$ can have an essential point of discontinuity there.\n\nEXAMPLE 7.4 The simple cubic $y=x^{3}$ has an inflection point at the origin, or $s=0$ (by arrangement). As shown in Fig. 7-1,\n\n$$\n\\lim _{s \\rightarrow-0} \\mathbf{N}_{0}=(0,-1) \\quad \\lim _{s \\rightarrow+0} \\mathbf{N}_{0}=(0,1)\n$$\n\nTo verify this analytically, make the parameterization $x=t, y=t^{3}$, and calculate $\\mathbf{N}_{0}$ as a function of $t$ $\\left(s^{\\prime}(t)=\\sqrt{1+9 t^{4}}\\right)$.\n\n$$\n\\begin{aligned}\n& \\mathbf{U}=\\left(x^{\\prime}(t), y^{\\prime}(t)\\right)=\\left(1,3 t^{2}\\right) \\\\\n& \\mathbf{T}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U}=\\frac{1}{\\sqrt{1+9 t^{4}}}\\left(1,3 t^{2}\\right) \\\\\n& \\frac{d \\mathbf{T}}{d s}=\\frac{1}{s^{\\prime}(t)} \\frac{d \\mathbf{T}}{d t}=\\frac{6 t}{\\left(1+9 t^{4}\\right)^{2}}\\left(-3 t^{2}, 1\\right) \\\\\n& \\kappa_{0}=\\left\\|\\frac{d \\mathbf{T}}{d s}\\right\\|=\\frac{6|t|}{\\left(1+9 t^{4}\\right)^{3 / 2}}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-096}\n\\end{center}\n\nFig. 7-1\n\n$$\n\\mathbf{N}_{0}=\\frac{1}{\\kappa_{0}} \\frac{d \\mathbf{T}}{d s}=\\frac{t /|t|}{\\sqrt{1+9 t^{4}}}\\left(--3 t^{2}, 1\\right) \\quad(t \\neq 0)\n$$\n\nThe scalar factor $t /|t|$ accounts for the discontinuity in $\\mathbf{N}_{0}$ at $t=0 \\quad(s=0)$.\n\n\\section*{Geometric (Global) Approach}\nA unit principal normal to a regular curve $\\mathscr{C}$ is any contravariant vector $\\mathbf{N}=\\left(N^{i}(s)\\right)$ such that, along $\\mathscr{C}$,\\\\\nA. $\\quad N^{i}$ is continuously differentiable (class $C^{1}$ ) for each $i$;\\\\\nB. $\\|\\mathbf{N}\\|=1$;\\\\\nC. $\\mathbf{N}$ is orthogonal to the unit tangent vector $\\mathbf{T}$, and is a scalar multiple of $\\delta \\mathbf{T} / \\delta s$ wherever $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$.\n\nThe curvature under this development is defined as\n\n\n\\begin{equation*}\n\\kappa \\equiv \\varepsilon \\mathbf{N} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\varepsilon g_{i j} N^{i} \\frac{\\delta T^{j}}{\\delta s} \\quad(\\varepsilon=\\varepsilon(\\mathbf{N})) \\tag{7.8}\n\\end{equation*}\n\n\nIf the metric is positive definite, the Frenet equation\n\n\n\\begin{equation*}\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\kappa \\mathbf{N} \\tag{7.9}\n\\end{equation*}\n\n\nholds unrestrictedly along a regular curve (see Problem 7.13).\n\nEXAMPLE 7.5 For the curve of Example 7.4, conditions A, B, and C allow precisely two possibilities for N:\n\n$$\n\\mathbf{N}=\\frac{+1}{\\sqrt{1+9 t^{4}}}\\left(-3 t^{2}, 1\\right) \\quad \\text { or } \\quad \\mathbf{N}=\\frac{-1}{\\sqrt{1+9 t^{4}}}\\left(-3 t^{2}, 1\\right)\n$$\n\nfor $-\\infty<t<\\infty$. Geometrically, these amount to reversing the normal arrows in either the left half or the right half of Fig. 7-1. The corresponding formulas for curvature are $(\\varepsilon \\equiv 1)$\n\n$$\n\\kappa=\\frac{6 t}{\\left(1+9 t^{4}\\right)^{3 / 2}} \\quad \\text { or } \\quad \\kappa=\\frac{-6 t}{\\left(1+9 t^{4}\\right)^{3 / 2}}\n$$\n\nOn curves having everywhere a non-null $\\delta \\mathbf{T} / \\delta s$, either $\\mathbf{N} \\equiv \\mathbf{N}_{0}$ (with $\\kappa=\\kappa_{0}$ ) or $\\mathbf{N} \\equiv-\\mathbf{N}_{0}$ (with $\\kappa=-\\kappa_{0}$ ). Thus, the global concept applies to all curves covered by the local concept and, in addition, to all regular planar curves (see Problem 7.14) and all analytic curves (curves for which the $x^{i}$ are representable as convergent Taylor series in $s$ ).\n\n\\subsection*{7.6 GEODESICS AS SHORTEST ARCS}\nWhen the metric is positive definite, a geodesic may be defined by the zero-curvature conditions (6.13), or, equivalently, by the condition that for any two of its points sufficiently close together, its length between the two points is least among all curves joining those points.\n\nThe minimum-length development employs a variational argument. We need to assume that all curves under consideration are class $C^{2}$ (that is, the parametric functions which represent them have continuous second-order derivatives). Let $x^{i}=x^{i}(t)$ represent a shortest curve (geodesic) passing through $A=\\left(x^{i}(a)\\right)$ and $B=\\left(x^{i}(b)\\right)$, where $b-a$ is as small as necessary. Embed the geodesic in a one-parameter family of $C^{2}$ curves passing through $A$ and $B$ :\n\n$$\nx^{i}=X^{i}(t, u) \\equiv x^{i}(t)+(t-a)(b-t) u \\phi^{i}(t)\n$$\n\nwhere the multipliers $\\phi^{i}(t)$ are arbitrary twice-differentiable functions. The length of a curve in this family is given by\n\n$$\nL(u)=\\int_{a}^{b} \\sqrt{\\varepsilon g_{i j} \\frac{\\partial X^{i}}{\\partial t} \\frac{\\partial X^{j}}{\\partial t}} d t \\equiv \\int_{a}^{b} \\sqrt{w(t, u)} d t\n$$\n\nwith $\\varepsilon=1$ for a positive-definite metric. Since $X^{i}(t, 0)=x^{i}(t) \\quad(i=1,2, \\ldots, n)$, the function $L(u)$ must have a local minimum at $u=0$. Standard calculus techniques yield the following expression of the necessary condition $L^{\\prime}(0)=0$ :\n\n\n\\begin{equation*}\n\\int_{a}^{b}\\left[w^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}-\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right)\\right](t-a)(b-t) \\phi^{k}(t) d t=0 \\tag{7.10}\n\\end{equation*}\n\n\nin which\n\n\n\\begin{equation*}\nw \\equiv w(t, 0)=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} \\tag{7.11}\n\\end{equation*}\n\n\nSince $(t-a)(b-t)>0$ on $(a, b)$ and $\\phi^{k}(t)$ may be chosen arbitrarily, the bracketed expression in (7.10) must vanish identically over $(a, b)$, for $k=1,2, \\ldots, n$; this leads to (Problem 7.21)\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad(i=1,2, \\ldots, n) \\tag{7.12}\n\\end{equation*}\n\n\nSystem (7.12), with $w$ defined by (7.11), are the differential equations for the geodesics of Riemannian space, in terms of the arbitrary curve parameter $t$. Assuming that these geodesics will be regular curves, we may choose $t=s=\\operatorname{arc}$ length. Then\n\n$$\nw=\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d s}{d s}\\right)^{2}=(1)^{2}=1 \\quad \\text { and } \\quad \\frac{d w}{d s}=0\n$$\n\nso that $(7.12)$ becomes\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d s} \\frac{d x^{k}}{d s}=0 \\quad(i=1,2, \\ldots, n) \\tag{7.13}\n\\end{equation*}\n\n\nwhich is precisely (6.13).\n\nIt must be emphasized that $L^{\\prime}(0)=0$ is only a necessary condition for minimum length, so that the geodesics are found among the solutions of (7.12) or (7.13).\n\n\\section*{Null Geodesics}\nConsider the case of indefinite metrics and class $\\mathscr{C}^{2}$ curves which may have one or more null points. Since, at a null point, $w=0$ in (7.11) the variational theory breaks down, because $L(u)$ fails\\\\\nto be differentiable at such a point. Analogous to the zero-curvature approach, we consider the more general condition for geodesics\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t} \\equiv \\frac{\\delta U^{i}}{\\delta t}=0 \\quad(i=1,2, \\ldots, n) \\tag{7.14}\n\\end{equation*}\n\n\nwhere $U=U^{i}=\\left(d x^{i} / d t\\right)$ is the tangent vector field. By properties of absolute differentiation,\n\n$$\n\\frac{d w}{d t}=\\frac{d}{d t}\\left(\\varepsilon g_{i j} U^{i} U^{j}\\right)=\\frac{\\delta}{\\delta t}\\left(\\varepsilon g_{i j} U^{i} U^{j}\\right)=2 \\varepsilon g_{i j} U^{i} \\frac{\\delta U^{j}}{\\delta t}=0\n$$\n\nalong a solution curve to (7.14); so $w=$ const. along the curve. Since the curve has at least one point of nullity, $w=0$ at all points, whence the curve is a null curve-called a null geodesic. In summary, the following system of $n+1$ ordinary differential equations in the $n$ unknown functions $x^{i}(t)$ will determine the null geodesics:\n\n$$\n\\begin{aligned}\n& \\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0 \\quad(i=1,2, \\ldots, n) \\\\\n& g_{r s} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0\n\\end{aligned}\n$$\n\nEXAMPLE 7.6 If the $g_{i j}$ are constants, (7.15) has the expected general solution\n\n$$\nx^{i}=x_{0}^{i}+\\alpha^{i} t \\quad \\text { with } \\quad g_{i j} \\alpha^{i} \\alpha^{j}=0\n$$\n\nImagining the $x^{i}$ to be rectangular coordinates, we interpret the null geodesics as a bundle of straight lines issuing from the arbitrary point $\\mathbf{x}_{0}$; each line is in the direction of some null vector $\\boldsymbol{\\alpha}$. By elimination of the $\\alpha^{i}$ the equation of the bundle is found to be\n\n$$\ng_{i j}\\left(x^{i}-x_{0}^{i}\\right)\\left(x^{j}-x_{0}^{j}\\right)=0\n$$\n\nIn particular, for the space of Special Relativity $\\left(g_{11}=g_{22}=g_{33}=-g_{44}=1, g_{i j}=0\\right.$ for $i \\neq j$ ), the null geodesics compose the $45^{\\circ}$ cone\n\n-see Fig. 7-2.\n\n$$\n\\left(x^{1}-x_{0}^{1}\\right)^{2}+\\left(x^{2}-x_{0}^{2}\\right)^{2}+\\left(x^{3}-x_{0}^{3}\\right)^{2}=\\left(x^{4}-x_{0}^{4}\\right)^{2}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-098}\n\\end{center}\n\nFig. 7-2\n\n\\section*{Solved Problems}\n\\section*{LENGTH IN RIEMANNIAN SPACE}\n7.1 Determine the indicator of the tangent vector $\\mathrm{U}$ to the curve\n\n$$\nx^{1}=t^{3} \\quad x^{2}=t^{2} \\quad x^{3}=t\n$$\n\n$(-\\infty<t<\\infty)$ if the fundamental form is\n\n(a) $\\left(d x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{3}\\right)^{2}-6 d x^{1} d x^{3}+2 x^{1} x^{2} d x^{2} d x^{3}$\n\n(b) $\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+3\\left(d x^{3}\\right)^{2}$\n\n(a) $\\left(3 t^{2}\\right)^{2}+t^{4}(2 t)^{2}+t^{6}(1)^{2}-6\\left(3 t^{2}\\right)(1)+2\\left(t^{3}\\right)\\left(t^{2}\\right)(2 t)(1)=9 t^{6}+9 t^{4}-18 t^{2}=9 t^{2}\\left(t^{2}+2\\right)\\left(t^{2}-1\\right)$\n\nSince $t^{2}+2$ is always positive,\n\n$$\n\\varepsilon(\\mathbf{U})=\\left\\{\\begin{array}{rc}\n+1 & t \\geqq 1 \\\\\n-1 & 0<t<1 \\\\\n+1 & t=0 \\\\\n-1 & -1<t<0 \\\\\n+1 & t \\leqq-1\n\\end{array}\\right.\n$$\n\n(b) $\\varepsilon(\\mathbf{U}) \\equiv+1$, because the form is positive definite.\n\n7.2 Show that the following matrix defines a Riemannian metric on $\\mathbf{R}^{2}$ :\n\n$$\nG=\\left[\\begin{array}{rr}\nx^{2} & -x^{1} \\\\\n-x^{1} & x^{2}\n\\end{array}\\right] \\quad\\left(x^{1}>0,-x^{1}<x^{2}<x^{1}\\right)\n$$\n\nWe must show that conditions A-D of Section 5.3 are satisfied.\n\nA. Since each $g_{i j}$ is linear in the $x^{i}$, it is differentiable to any order.\n\nB. By observation, the matrix is symmetric.\\\\\nC. $\\left|g_{i j}\\right|=\\left(x^{2}\\right)^{2}-\\left(x^{1}\\right)^{2}<0$ over the given domain.\n\nD. Extend the matrix to a tensor $\\mathbf{g}$ by using the tensor transformation laws to define the $\\bar{g}_{i j}$ in terms of the $g_{i j}$. This will then make the quadratic form $g_{i j} d x^{i} d x^{j}$, hence the distance formula, an invariant.\n\n7.3 Find the null set of the curve $\\mathscr{C}: x^{2}=\\left(x^{1}\\right)^{2} \\quad\\left(x^{1}>0\\right)$ under the metric of Problem 7.2.\n\nLet $\\mathscr{C}$ be parameterized by $x^{1}=t, x^{2}=t^{2} \\quad(t>0)$. Then, along $\\mathscr{C}$,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{cc}\nt^{2} & -t \\\\\n-t & t^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n1 \\\\\n2 t\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{c}\n-t^{2} \\\\\n-t+2 t^{3}\n\\end{array}\\right]=t^{2}\\left(4 t^{2}-3\\right)\n$$\n\nwhich, for positive $t$, vanishes only at $t=\\sqrt{3} / 2$.\n\n7.4 Find the arc length of the curve $\\mathscr{C}$ in Problem 7.3 from $x^{1}=0$ to $x^{1}=1$.\n\nAgain using $t=x^{1}$, observe that\n\nHence,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}<0 \\quad \\text { for } \\quad 0<t<\\sqrt{3} / 2\n$$\n\n$$\n\\begin{aligned}\nL & =\\int_{0}^{1} \\sqrt{\\varepsilon t^{2}\\left(4 t^{2}-3\\right)} d t=\\int_{0}^{\\sqrt{3} / 2} t \\sqrt{-\\left(4 t^{2}-3\\right)} d t+\\int_{\\sqrt{3} / 2}^{1} t \\sqrt{4 t^{2}-3} d t \\\\\n& =-\\left.\\frac{1}{12}\\left(3-4 t^{2}\\right)^{3 / 2}\\right|_{0} ^{\\sqrt{3} / 2}+\\left.\\frac{1}{12}\\left(4 t^{2}-3\\right)^{3 / 2}\\right|_{\\sqrt{3} / 2} ^{1}=\\frac{3 \\sqrt{3}+1}{12} \\approx 0.516\n\\end{aligned}\n$$\n\n7.5 Write $g \\equiv \\operatorname{det} G$ for the determinant of a Riemannian metric. Prove that $|g|$ is a differentiable function of the coordinates.\n\nApplying the chain rule to $|g|=\\sqrt{g^{2}}$, we have\n\n\n\\begin{equation*}\n\\frac{\\partial|g|}{\\partial x^{i}}=\\frac{g}{|g|} \\frac{\\partial g}{\\partial x^{i}} \\tag{1}\n\\end{equation*}\n\n\nSince $\\partial g / \\partial x^{i}$ exists (Property A) and $|g| \\neq 0$ (Property C), the right-hand side of (1) is well-defined.\n\n7.6 Show that under the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2}-\\left(d x^{4}\\right)^{2}$ (another version of the metric for Special Relativity), the curve\n\n$$\nx^{1}=A \\sinh t \\quad x^{2}=A \\cosh t \\quad x^{3}=B t \\quad x^{4}=C t \\quad(0 \\leqq t \\leqq 1)\n$$\n\nwith $A^{2}=B^{2}+C^{2}$, is null at each of its points.\n\n$$\n\\begin{aligned}\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} & =\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}-\\left(\\frac{d x^{3}}{d t}\\right)^{2}-\\left(\\frac{d x^{4}}{d t}\\right)^{2} \\\\\n& =(A \\cosh t)^{2}-(A \\sinh t)^{2}-B^{2}-C^{2} \\\\\n& =A^{2}\\left(\\cosh ^{2} t-\\sinh ^{2} t\\right)-B^{2}-C^{2}=A^{2}-B^{2}-C^{2} \\equiv 0\n\\end{aligned}\n$$\n\n7.7 At the point of intersection $(0,0)$, find the angle between the curves\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=2 t-2 \\\\\nx^{2}=t^{2}-1\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u^{4}-1 \\\\\nx^{2}=25 u^{2}+50 u-75\n\\end{array}\\right.\\right.\n$$\n\nif the Riemannian metric is given by $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}$.\n\nAt $t=1, \\mathbf{T} \\equiv\\left(d x^{i} / d t\\right)=(2,2)$; at $u=1, \\mathbf{U} \\equiv\\left(d x^{i} / d u\\right)=(4,100)$. Hence, using matrices,\n\n$$\n\\begin{aligned}\n& \\mathbf{T U}=\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=-200 \\\\\n& \\|\\mathbf{T}\\|^{2}=\\varepsilon_{1}\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n2 \\\\\n2\n\\end{array}\\right]=\\left(\\varepsilon_{1}\\right)(-4)=4 \\\\\n& \\|\\mathbf{U}\\|^{2}=\\varepsilon_{2}\\left[\\begin{array}{ll}\n4 & 100\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=\\left(\\varepsilon_{2}\\right)(-784)=784\n\\end{aligned}\n$$\n\n$$\n\\text { and } \\quad \\cos \\theta=\\frac{-200}{\\sqrt{4} \\sqrt{784}}=-\\frac{25}{7}\n$$\n\nThis is Case 2 of Section 7.2; we have\n\n$$\n\\theta=\\pi+i \\ln \\left(\\frac{25}{7}+\\sqrt{\\left(\\frac{25}{7}\\right)^{2}-1}\\right)=\\pi+i \\ln 7\n$$\n\n7.8 Verify that the vectors of Problem 7.7 do not obey the triangle inequality.\n\nAs calculated, $\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|=2+28=30$. But\n\n$$\n\\|\\mathbf{T}+\\mathbf{U}\\|^{2}=\\varepsilon_{3}\\left[\\begin{array}{ll}\n6 & 102\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n6 \\\\\n102\n\\end{array}\\right]=\\varepsilon_{3}(-1188)=1188\n$$\n\nwhence $\\|\\mathbf{T}+\\mathbf{U}\\| \\approx 34.46>\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|$.\n\n\\section*{ARC-LENGTH PARAMETER, UNIT TANGENT VECTOR}\n7.9 Let $\\mathscr{C}: x^{i}=x^{i}(t)$ be any non-null curve. (a) Prove that $\\operatorname{arc}$ length along $\\mathscr{C}$ is defined as a strictly increasing function of $t$. (b) Exhibit the arc-length parameterization of $\\mathscr{C}$.\\\\\n(a)\n\nFor $t_{1}<t_{2}$, the Mean-Value Theorem of calculus gives\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left(t_{2}-t_{1}\\right) s^{\\prime}(\\tau) \\quad\\left(t_{1}<\\tau<t_{2}\\right)\n$$\n\nThe right-hand side is nonnegative, so that $s\\left(t_{1}\\right) \\leqq s\\left(t_{2}\\right)$. But, in view of the identity\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left[s\\left(t_{2}\\right)-s\\left(t_{3}\\right)\\right]+\\left[s\\left(t_{3}\\right)-s\\left(t_{1}\\right)\\right]\n$$\n\nwhere $t_{3}$ is any point in $\\left(t_{1}, t_{2}\\right)$, the equality $s\\left(t_{1}\\right)=c=s\\left(t_{2}\\right)$ would imply $s\\left(t_{3}\\right)=c$; i.e., $s(t)$ would be constant on $\\left[t_{1}, t_{2}\\right]$, making $s^{\\prime}(t) \\equiv 0$ on $\\left(t_{1}, t_{2}\\right)$ and thus making $\\mathscr{C}$ a null curve. We conclude that\n\n$$\ns\\left(t_{1}\\right)<s\\left(t_{2}\\right) \\quad \\text { whenever } \\quad t_{1}<t_{2}\n$$\n\n(b) The strictly increasing function $s(t)$ will possess a strictly increasing inverse; denote it as $t=\\theta(s)$. Then $\\mathscr{C}$ admits the parameterization $x^{i}=x^{i}(\\theta(s))$.\n\n7.10 (a) In rectangular coordinates $\\left(x^{1}, x^{2}\\right)$ but adopting the metric of Problem 7.7, find the null points of the parabola $\\mathscr{C}: x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$. (b) Show that the arc-length parameterization of $\\mathscr{C}$ is differentiable to all orders except at the null points. (c) Find the length of $\\mathscr{C}$.\n\n\n\\begin{equation*}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}-2 \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=1-4 t \\tag{a}\n\\end{equation*}\n\n\nso there is only one null point, at $t=1 / 4$.\n\n\n\\begin{equation*}\ns=\\int_{0}^{t} \\sqrt{\\varepsilon(1-4 u)} d u \\tag{b}\n\\end{equation*}\n\n\nThus, for $0 \\leqq t \\leqq 1 / 4$,\n\n$$\ns=\\int_{0}^{t} \\sqrt{1-4 u} d u=\\frac{1}{6}\\left[1-(1-4 t)^{3 / 2}\\right]\n$$\n\nand, for $1 / 4 \\leqq t \\leqq 1 / 2$,\n\n$$\ns=\\int_{0}^{1 / 4} \\sqrt{1-4 u} d u+\\int_{1 / 4}^{t} \\sqrt{4 u-1} d u=\\frac{1}{6}\\left[1+(4 t-1)^{3 / 2}\\right]\n$$\n\nInversion of these formulas gives\n\n\\[\nt=\\theta(s)=\\left\\{\\begin{array}{lr}\n\\frac{1}{4}\\left[1-(1-6 s)^{2 / 3}\\right] & 0 \\leqq s \\leqq 1 / 6  \\tag{1}\\\\\n\\frac{1}{4}\\left[1+(6 s-1)^{2 / 3}\\right] & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array}\\right.\n\\]\n\nIt is evident that $\\theta(s)$ is infinitely differentiable except at the null point $s=1 / 6$ (the image of $t=1 / 4)$; the same will be true of the functions $x^{1}=\\theta(s), x^{2}=\\theta^{2}(s)$.\n\n(c) Set $t=1 / 2$ in the applicable expression for $s$ :\n\n$$\ns=\\frac{1}{6}\\left[1+(2-1)^{3 / 2}\\right]=\\frac{1}{3}\n$$\n\n7.11 Find the arc length of the same curve $\\mathscr{C}$ as in Problem 7.10, but with the normal Euclidean metric, $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}$.\n\nNow\n\n$$\n\\frac{d s}{d t}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{4 t^{2}+1}\n$$\n\nso that\n\n$$\nL=\\int_{0}^{1 / 2} \\sqrt{4 t^{2}+1} d t=\\left[\\frac{t}{2} \\sqrt{4 t^{2}+1}+\\frac{1}{4} \\ln \\left(2 t+\\sqrt{4 t^{2}+1}\\right)\\right]_{0}^{1 / 2}=\\frac{\\sqrt{2}+\\ln (1+\\sqrt{2})}{4} \\approx 0.574\n$$\n\nas compared to $L \\approx 0.333$ in Problem 7.10 .\n\n7.12 Using the arc-length parameterization found for the curve $\\mathscr{C}$ in Problem 7.10(b), compute the components $T^{i}(s)$ of the tangent vector and verify that this vector has unit length for all $s \\neq 1 / 6$.\n\nWe have $\\left(T^{i}\\right)=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$, where $\\theta=\\theta(s)$ is the function defined by (1) in Problem $7.10(b)$. Hence,\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon\\left(\\theta^{\\prime 2}-4 \\theta \\theta^{\\prime 2}\\right)=\\varepsilon(1-4 \\theta) \\theta^{\\prime 2}\n$$\n\nBut, by (1) of Problem 7.10(b),\n\n$$\n1-4 \\theta=\\left\\{\\begin{array}{rrr}\n(1-6 s)^{2 / 3} & 0 \\leqq s \\leqq 1 / 6 \\\\\n-(6 s-1)^{2 / 3} & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array} \\quad \\theta^{\\prime}=\\left\\{\\begin{array}{rr}\n(1-6 s)^{-1 / 3} & 0<s<1 / 6 \\\\\n(6 s-1)^{-1 / 3} & 1 / 6<s<1 / 3\n\\end{array}\\right.\\right.\n$$\n\nTherefore, $\\|\\mathbf{T}\\|^{2}=(\\varepsilon)( \\pm 1)=+1$, or $\\|\\mathbf{T}\\| \\equiv 1 \\quad(s \\neq 1 / 6)$.\n\n\\section*{UNIT PRINCIPAL NORMAL, CURVATURE}\n7.13 Prove that the Frenet equation (7.9) holds at each point of a regular curve when the metric is positive definite.\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, we have (from property $\\mathrm{C}$ of $\\mathbf{N}$ ),\n\n\n\\begin{equation*}\n\\mathbf{N}=\\lambda \\frac{\\delta \\mathbf{T}}{\\delta s} \\tag{1}\n\\end{equation*}\n\n\nfrom some real $\\lambda$. Take the inner product with the vector $\\mathbf{N}$ in (1); with $\\varepsilon=\\varepsilon(\\mathbf{N})$,\n\n\n\\begin{equation*}\n\\varepsilon \\mathbf{N}^{2}=\\varepsilon \\lambda \\mathbf{N} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\lambda \\kappa \\quad \\text { or } \\quad 1=\\lambda \\kappa \\tag{2}\n\\end{equation*}\n\n\nThen $\\lambda=1 / \\kappa$, and substitution into (1) yields (7.9).\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\|=0$, both $\\delta \\mathbf{T} / \\delta s=\\mathbf{0}$ (because the metric is positive definite) and $\\kappa=0$ (by $(7.8)$ ); the Frenet equation then holds trivially.\n\n7.14 For any regular two-dimensional curve $\\mathscr{C}: x^{i}=x^{i}(s)$, define the contravariant vector\n\n\n\\begin{equation*}\n\\mathbf{N}=\\left(N^{i}\\right) \\equiv\\left(-T_{2} / \\sqrt{|g|}, T_{1} / \\sqrt{|g|}\\right) \\tag{7.16}\n\\end{equation*}\n\n\nwhere $\\mathbf{T}=\\left(T^{i}\\right)$ is the unit tangent vector along $\\mathscr{C}$ and $g=\\operatorname{det}\\left(g_{i j}\\right)$. Show that $\\mathbf{N}$ is a global unit normal for $\\mathscr{C}$.\n\nWe must show that the three properties of Section 7.5 are possessed by the given vector (except possibly at null points).\n\nA. Since $\\mathscr{C}$ is regular, the $T^{i}$, and with them the $T_{i}=g_{i j} T^{j}$, are in $C^{1}$. The same is true of $|g|$ (Problem 7.5), which function is strictly positive. Therefore, the $N^{i}$ are also in $C^{1}$.\n\nB. By (2.11), $g^{11}=g_{22} / g, g^{12}=g^{21}=-g_{12} / g$, and $g^{22}=g_{11} / g$. Hence,\n\n$$\n\\begin{aligned}\n\\|\\mathbf{N}\\|^{2} & =\\left|g_{11}\\left(T_{2}^{2} /|g|\\right)+2 g_{12}\\left(-T_{1} T_{2} /|g|\\right)+g_{22}\\left(T_{1}^{2} /|g|\\right)\\right| \\\\\n& =\\frac{1}{|g|}\\left|g g^{22} T_{2}^{2}+2 g g^{12} T_{1} T_{2}+g g^{11} T_{1}^{2}\\right| \\\\\n& =\\frac{|g|}{|g|}\\left|g^{i j} T_{i} T_{j}\\right|=\\left|T^{j} T_{j}\\right|=\\|\\mathbf{T}\\|^{2}\n\\end{aligned}\n$$\n\nand so $\\|\\mathbf{N}\\|=\\|\\mathbf{T}\\|=1$.\\\\\nC. $\\mathbf{N}$ is orthogonal to $\\mathbf{T}$ :\n\n$$\nN^{i} T_{i}=-\\frac{T_{2}}{\\sqrt{|g|}} T_{1}+\\frac{T_{1}}{\\sqrt{|g|}} T_{2}=0\n$$\n\nFurthermore, when $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, then $\\mathbf{N}_{0}$ is defined and is also a vector orthogonal to $\\mathbf{T}$ (by Theorem 7.1). In two dimensions this implies $\\mathbf{N}= \\pm \\mathbf{N}_{0}=\\lambda(\\delta \\mathbf{T} / \\delta s)$.\n\n7.15 For the curve and metric of Problem 7.10, determine the local normal $\\mathbf{N}_{0}$ and, using Problem 7.14, a global normal $\\mathbf{N}$. Verify that the two stand in the proper relationship.\n\nWe have $g_{11}=1, g_{12}=g_{21}=-1, g_{22}=0$ (all constants), and $\\mathbf{T}=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$; therefore, for $s \\neq 1 / 6$,\n\nand\n\n$$\n\\begin{aligned}\n& \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=\\left(\\theta^{\\prime \\prime}, 2 \\theta^{\\prime 2}+2 \\theta \\theta^{\\prime \\prime}\\right) \\\\\n& =\\left\\{\\begin{array}{lr}\n\\left(2(1-6 s)^{-4 / 3},(1-6 s)^{-2 / 3}+(1-6 s)^{-4 / 3}\\right) & 0<s<1 / 6 \\\\\n\\left(-2(6 s-1)^{-4 / 3},(6 s-1)^{-2 / 3}-(6 s-1)^{-4 / 3}\\right) & 1 / 6<s<1 / 3\n\\end{array}\\right. \\\\\n& \\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\sqrt{\\varepsilon g_{i j} \\frac{d T^{i}}{d s} \\frac{d T^{j}}{d s}}=\\left\\{\\begin{array}{l}\n2(1-6 s)^{-1} \\\\\n2(6 s-1)^{-1}\n\\end{array}\\right. \\\\\n& \\text { Thus, } \\quad \\mathbf{N}_{0}=\\frac{\\delta \\mathbf{T}}{\\delta s} /\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{1 / 3}+\\frac{1}{2}(1-6 s)^{-1 / 3}\\right) \\\\\n\\left(-(6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{1 / 3}-\\frac{1}{2}(6 s-1)^{-1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nWith $g=-1$, Problem 7.14 gives $(s \\neq 1 / 6)$ :\n\n$$\n\\begin{aligned}\n& T_{1}=g_{1 j} T^{j}=T^{1}-T^{2}=\\theta^{\\prime}(1-2 \\theta)=\\left\\{\\begin{array}{l}\n\\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3} \\\\\n\\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\n\\end{array}\\right. \\\\\n& T_{2}=g_{2 j} T^{j}=-T^{1}=-\\theta^{\\prime}=\\left\\{\\begin{array}{l}\n-(1-6 s)^{-1 / 3} \\\\\n-(6 s-1)^{-1 / 3}\n\\end{array}\\right. \\\\\n& \\mathbf{N}=\\left(-T_{2}, T_{1}\\right)=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3}\\right) \\\\\n\\left((6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nIt is seen that, as expected, $\\mathbf{N}=+\\mathbf{N}_{0}$ for $s<1 / 6$ and $\\mathbf{N}=-\\mathbf{N}_{0}$ for $s>1 / 6$. neither $\\mathbf{N}_{0}$ nor $\\mathbf{N}$ is defined at the null point $s=1 / 6$. For comparison, recall the situation in Examples 7.4 and 7.5: there the discontinuity in $\\mathbf{N}_{0}$ occurred at a regular point (the cubic has no null points under the Euclidean metric), and $\\mathbf{N}$ (either choice) was defined everywhere.\n\n7.16 Under the metric of Special Relativity (Example 7.1), a regular curve $\\mathscr{C}$ is given by\n\n$$\nx^{1}=s^{2} \\quad x^{2}=\\frac{3 s}{5} \\quad x^{3}=\\frac{4 s}{5} \\quad x^{4}=s^{2}\n$$\n\nfor $0 \\leqq s \\leqq 1$. (a) Verify that $s$ is arc length for $\\mathscr{C}$, and show that the absolute derivative, $\\delta \\mathbf{T} / \\delta s$, of $\\mathbf{T}$ is a null vector at every point of the curve (hence, a local principal normal $\\mathbf{N}_{0}$ is nowhere defined on $\\mathscr{C}$ ). Construct a global principal normal for $\\mathscr{C}$ in such a manner that the corresponding curvature function is nonzero. Is more than one curvature function possible?\n\n(a) We have $\\left(T^{i}\\right)=(2 s, 3 / 5,4 / 5,2 s)$ and\n\n$$\n\\left|g_{i j} T^{i} T^{j}\\right|=\\left|4 s^{2}+(9 / 25)+(16 / 25)-4 s^{2}\\right|=1\n$$\n\nhence, $s$ is an arc-length parameter. Also, since the $g_{i j}$ are constant, all Christoffel symbols vanish and\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=(2,0,0,2) \\quad\\left\\|\\frac{\\delta T}{\\delta s}\\right\\|=\\sqrt{\\left|2^{2}+0^{2}+0^{2}-2^{2}\\right|}=0\n$$\n\nfor all $s$.\n\n(b) Any differentiable unit vector orthogonal to $\\mathbf{T}$ will do for $\\mathbf{N}$, which then determines the curvature through (7.8). In the orthonormality conditions\n\n$$\n\\begin{aligned}\n& 2 s N^{1}+\\frac{3}{5} N^{2}+\\frac{4}{5} N^{3}-2 s N^{4}=0 \\\\\n& \\left(N^{1}\\right)^{2}+\\left(N^{2}\\right)^{2}+\\left(N^{3}\\right)^{2}-\\left(N^{4}\\right)^{2}= \\pm 1\n\\end{aligned}\n$$\n\nwe may successively set $N^{1}=N^{4}=0, N^{3}=N^{4}=0$, and $N^{2}=N^{4}=0$ to obtain three candidate normals:\n\n$$\n\\begin{gathered}\n\\mathbf{N}_{1}=\\left(0,-\\frac{4}{5}, \\frac{3}{5}, 0\\right) \\quad \\mathbf{N}_{2}=\\frac{1}{\\sqrt{(9 / 25)+4 s^{2}}}\\left(-\\frac{3}{5}, 2 s, 0,0\\right) \\\\\n\\mathbf{N}_{3}=\\frac{1}{\\sqrt{(16 / 25)+4 s^{2}}}\\left(-\\frac{4}{5}, 0,2 s, 0\\right)\n\\end{gathered}\n$$\n\nThe constant $\\mathbf{N}_{1}$ yields $\\kappa_{1} \\equiv 0$; but $\\mathbf{N}_{2}$ and $\\mathbf{N}_{3}$ yield the distinct curvature functions\n\n$$\n\\kappa_{2}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{9} s^{2}}} \\quad \\kappa_{3}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{16} s^{2}}}\n$$\n\nNote that the Frenet equation is invalid for all these normals.\n\n7.17 Refer to Problems 7.10 and 7.15. Calculate the curvature functions $\\kappa_{0}$ and $\\kappa$, and discuss the variability of $\\kappa_{0}$ over the parabolic arc $0 \\leqq s \\leqq 1 / 3$.\n\nOur previous results show that both curvatures are defined everywhere except $s=1 / 6$, with $\\kappa=\\kappa_{0}$ on $0 \\leqq s<1 / 6$ and $\\kappa=-\\kappa_{0}$ on $1 / 6<s \\leqq 1 / 3$ (cf. Problem 7.13). By Problem 7.15,\n\n$$\n\\kappa_{0}=\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\frac{2}{|1-6 s|} \\quad(s \\neq 1 / 6)\n$$\n\nwhence\n\n$$\n\\kappa=\\frac{2}{1-6 s} \\quad(s \\neq 1 / 6)\n$$\n\nIt is seen that $\\kappa_{0}$ has the same value, 2, at $s=0$ (the vertex, or point of greatest Euclidean curvature) and the undistinguished point $s=1 / 3$. Moreover, near the ordinary (from the Euclidean viewpoint) point $s=1 / 6$, the absolute curvature becomes arbitrarily large.\n\n7.18 (a) For any regular two-dimensional curve, derive the formula for absolute curvature\n\n\n\\begin{equation*}\n\\kappa_{0}=\\sqrt{|g|}\\left|T^{1} \\frac{\\delta T^{2}}{\\delta s}-T^{2} \\frac{\\delta T^{1}}{\\delta s}\\right| \\tag{7.17}\n\\end{equation*}\n\n\n(b) Use (7.17) to check Problem 7.17.\n\n(a) By (7.8) and the remarks made following Example 7.5,\n\n\n\\begin{equation*}\n\\kappa_{0}=|\\kappa|=\\left|N_{j} \\frac{\\delta T^{j}}{\\delta s}\\right|=\\left|N_{1} \\frac{\\delta T^{1}}{\\delta s}+N_{2} \\frac{\\delta T^{2}}{\\delta s}\\right| \\tag{1}\n\\end{equation*}\n\n\nChoosing the global normal $\\left(N^{i}\\right)$ established in Problem 7.14, we have:\n\n$$\n\\begin{aligned}\nN_{1} & =g_{11} N^{1}+g_{12} N^{2}=\\left(g g^{22}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(-g g^{21}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{21} T_{1}+g^{22} T_{2}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}} T^{2} \\\\\nN_{2} & =g_{21} N^{1}+g_{22} N^{2}=\\left(-g g^{12}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(g g^{11}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right) \\\\\n& =\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{11} T_{1}+g^{12} T_{2}\\right)=\\frac{\\gamma}{\\sqrt{|g|}} T^{1}\n\\end{aligned}\n$$\n\nSubstitution of these components in (1) yields (7.17).\n\n(b) For the metric of Problem 7.10,\n\n$$\nG=\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$g=\\operatorname{det} G=-1$ and absolute derivatives reduce to ordinary derivatives. Thus we can rewrite (7.17) in terms of the curve parameter $t$, as follows:\n\n$$\n\\kappa_{0}=\\left|T^{1} \\frac{d T^{2}}{d t} \\frac{d t}{d s}-T^{2} \\frac{d T^{1}}{d t} \\frac{d t}{d s}\\right|=\\frac{1}{s^{\\prime}(t)}\\left|T^{1} \\frac{d T^{2}}{d t}-T^{2} \\frac{d T^{1}}{d t}\\right|=\\frac{\\left(T^{1}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{2}}{T^{1}}\\right)\\right|\n$$\n\nSubstitution of $s^{\\prime}(t)=\\sqrt{|1-4 t|} \\quad(t \\neq 1 / 4)$ and the components of the unit tangent vector,\n\n$$\nT^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t} \\equiv \\frac{1}{s^{\\prime}(t)} \\quad T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{2 t}{s^{\\prime}(t)}\n$$\n\ngives:\n\n$$\n\\kappa_{0}=\\frac{2}{\\left(s^{\\prime}(t)\\right)^{3}}=\\frac{2}{|1-4 t|^{3 / 2}} \\quad(t \\neq 1 / 4)\n$$\n\nFrom Problem 7.12,\n\n$$\n|1-4 t|=|1-4 \\theta(s)|=|1-6 s|^{2 / 3}\n$$\n\nyielding exact agreement with Problem 7.17.\n\n7.19 Compute the absolute curvature of the logarithmic curve $\\mathscr{C}: x^{1}=t, x^{2}=a \\ln t$, for $\\frac{1}{2} \\leqq t<a$, if the Riemannian metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}\n$$\n\nAs the $g_{i j}$ are constants (with $g=-1$ ), we can proceed as in Problem 7.18(b). This time, the most convenient version of $(7.17)$ is\n\n$$\n\\kappa_{0}=\\sqrt{|g|} \\frac{\\left(T^{2}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{1}}{T^{2}}\\right)\\right|\n$$\n\nSubstituting\n\n$$\n\\begin{aligned}\n& s^{\\prime}(t)=\\sqrt{\\left|\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right|}=\\sqrt{\\left|1-\\frac{a^{2}}{t^{2}}\\right|}=\\frac{1}{t} \\sqrt{a^{2}-t^{2}}(\\neq 0) \\\\\n& T^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t}=\\frac{t}{\\sqrt{a^{2}-t^{2}}} \\\\\n& T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{a}{\\sqrt{a^{2}-t^{2}}}\n\\end{aligned}\n$$\n\nwe find: $\\kappa_{0}=a t\\left(a^{2}-t^{2}\\right)^{-3 / 2}$.\n\n\\subsection*{7.20 Prove Theorem 7.1.}\nAlong a regular curve we have\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon \\mathbf{T} \\mathbf{T}=1 \\quad \\text { or } \\quad \\mathbf{T T}=\\varepsilon\n$$\n\nwhere the indicator $\\varepsilon$ is constant, $|\\varepsilon|=1$, on the curve. By the inner-product rule for absolute differentiation, and the fact that the absolute derivative of an invariant is the ordinary derivative,\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s} \\mathbf{T}+\\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s} \\equiv 2 \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d}{d s}(\\varepsilon)=0 \\quad \\text { or } \\quad \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=0\n$$\n\n\\section*{GEODESICS}\n\\subsection*{7.21 Establish (7.12).}\nStart with the conditions\n\n\n\\begin{equation*}\nw^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy use of the product and chain rules, the expression on the right may be written\n\n$$\n-w^{-3 / 2} \\frac{d w}{d t}\\left(g_{i k} \\frac{d x^{i}}{d t}\\right)+2 w^{-1 / 2}\\left(\\frac{\\partial g_{i k}}{\\partial x^{j}} \\frac{d x^{j}}{d t}\\right) \\frac{d x^{i}}{d t}+2 w^{-1 / 2} g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nPut this back in (1), multiply both sides by $w^{1 / 2}$, and go over to the notation $g_{i j k} \\equiv \\partial g_{i j} / \\partial x^{k}$ :\n\n$$\ng_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=-w^{-1} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}+2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nwhich rearranges to\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nMaking use of the symmetry of $g_{i j}$, the third term on the left may be split into two similar terms, yielding\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{j k i} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{k i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nDivide by 2 , multiply by $g^{p k}$, and sum on $k$ :\n\n$$\n\\delta_{i}^{p} \\frac{d^{2} x^{i}}{d t^{2}}-g^{p k} \\Gamma_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{2 w} \\delta_{i}^{p} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad \\text { or } \\quad \\frac{d^{2} x^{p}}{d t^{2}}+\\Gamma_{j k}^{p} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{p}}{d t}\n$$\n\nwhich is $(7.12)$.\n\n7.22 In a Riemannian 2-space with fundamental form $\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, determine $(a)$ the regular geodesics, $(b)$ the null geodesics.\n\nHere $g_{11}=1, g_{12}=g_{21}=0, g_{22}=-\\left(x^{2}\\right)^{-2}$; Problem 6.4 gives\n\n$$\n\\Gamma_{22}^{2}=\\frac{d}{d x^{2}}\\left[\\frac{1}{2} \\ln \\left(x^{2}\\right)^{-2}\\right]=-\\frac{1}{x^{2}}\n$$\n\nas the only nonvanishing Christoffel symbol.\n\n(a) The system (7.13) becomes\n\n$$\n\\frac{d^{2} x^{1}}{d s^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d s^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=0\n$$\n\nThe first equation integrates to $x^{1}=a s+x_{0}^{1}$. In the second, let $u \\equiv d x^{2} / d s$ :\n\n$$\nu \\frac{d u}{d x^{2}}-\\frac{1}{x^{2}} u^{2}=0 \\quad \\text { or } \\quad \\frac{d u}{u}=\\frac{d x^{2}}{x^{2}} \\quad \\text { or } \\quad u=c x^{2}\n$$\n\nfrom which\n\n$$\n\\frac{d x^{2}}{d s}=c x^{2} \\quad \\text { or } \\quad \\frac{d x^{2}}{x^{2}}=c d s \\quad \\text { or } \\quad x^{2}=x_{0}^{2} e^{c s}\n$$\n\nAs our notation indicates, an arbitrary point $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ is the origin $(s=0)$ of a family of geodesics that seems to depend on two parameters, $a$ and $c$. However, $s$ must represent arc length, so that\n\n$$\n\\pm 1=\\left(\\frac{d x^{1}}{d s}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=a^{2}-c^{2}\n$$\n\nHence, either $a^{2}=c^{2}+1$ (the fundamental form is positive) or $c^{2}=a^{2}+1$ (the fundamental form is negative). Both cases may be accounted for by a single parameter, $\\lambda$, if $s$ is eliminated between the parametric equations for $x^{1}$ and $x^{2}$ :\n\nregular geodesics $\\quad x^{2}=x_{0}^{2} \\exp \\left[\\lambda\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad(|\\lambda| \\neq 1)$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-107}\n\\end{center}\n\nFig. 7-3\n\n(b) System (7.14), in $t$, becomes\n\n$$\n\\begin{gathered}\n\\frac{d^{2} x^{1}}{d t^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\\\\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0\n\\end{gathered}\n$$\n\nIt is clear that the solution may be found by formally replacing $s$ by $t$ in part ( $a$ ) and setting $a^{2}=c^{2}$. Thus, the null geodesics through $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ are given by\n\n$$\n\\text { null geodesics } \\quad x^{2}=x_{0}^{2} \\exp \\left[+\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad \\text { and } \\quad x^{2}=x_{0}^{2} \\exp \\left[-\\left(x^{1}-x_{0}^{1}\\right)\\right]\n$$\n\nNote that the null geodesics correspond to the exceptional values $\\lambda= \\pm 1$ in part (a). Figure 7-3 is a sketch of the geodesics through the point $(1,-1)$ in cartesian coordinates.\n\n7.23 Without converting to arc length, verify that in spherical coordinates, under the Euclidean metric\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2} d x^{3}\\right)^{2}\n$$\n\nany curve of the form $\\mathscr{C}: x^{1}=a \\sec t, x^{2}=t+b, x^{3}=c(a, b, c$ constant) is a geodesic. (It should be apparent that $\\mathscr{C}$ is a straight line.)\n\nThe equations (7.12) must be verified. The Christoffel symbols $\\Gamma_{j k}^{i}$ for spherical coordinates are (Problem 6.5):\n\n$$\ni=1 \\quad \\Gamma_{22}^{1}=-x^{1}, \\quad \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}\n$$\n\n$$\n\\begin{array}{lll}\ni=2 & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}, & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2} \\\\\ni=3 & \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}, & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}\n\\end{array}\n$$\n\nThe derivatives of the $x^{i}(t)$ are:\n\n$$\n\\begin{gathered}\n\\frac{d x^{1}}{d t}=a \\sec t \\tan t, \\quad \\frac{d^{2} x^{1}}{d t^{2}}=(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right) \\\\\n\\frac{d x^{2}}{d t}=1, \\quad \\frac{d^{2} x^{2}}{d t^{2}}=0 \\quad \\text { and } \\quad \\frac{d x^{3}}{d t}=\\frac{d^{2} x^{3}}{d t^{2}}=0\n\\end{gathered}\n$$\n\nWith $\\varepsilon \\equiv 1,(7.11)$ gives\n\nand\n\n$$\n\\begin{aligned}\n& w=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n&=(a \\sec t \\tan t)^{2}+(a \\sec t)^{2}(1)^{2}+0=a^{2} \\sec ^{4} t \\\\\n& \\frac{1}{2 w} \\frac{d w}{d t}=\\frac{\\left(4 a^{2} \\sec ^{3} t\\right)(\\sec t \\tan t)}{2 a^{2} \\sec ^{4} t}=2 \\tan t\n\\end{aligned}\n$$\n\nFor convenience in the verification of (7.12), let LS denote the left side, and RS the right side, of the equation in question. We obtain:\n\n$$\n\\begin{aligned}\ni=\\mathbf{1} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n& =(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right)-(a \\sec t)(1)^{2}+0=2 a \\sec t \\tan ^{2} t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{1}}{d t}=(2 \\tan t)(a \\sec t \\tan t)=2 a \\sec t \\tan ^{2} t=\\mathrm{LS} \\\\\ni=\\mathbf{2} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{2}}{d t^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\frac{2}{a \\sec t}(a \\sec t \\tan t)(1)+0=2 \\tan t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{2}}{d t}=2 \\tan t=\\mathrm{LS} \\\\\n\\boldsymbol{i}=\\mathbf{3} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{3}}{d t^{2}}+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0 \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{3}}{d t}=0=\\mathrm{LS}\n\\end{aligned}\n$$\n\n\\section*{Supplementary Problems}\n7.24 Determine the fundamental indicator $\\varepsilon(\\mathbf{U})$ if $\\left(U^{i}\\right)=(2 t,-2 t, 1)$ at the point $\\left(x^{i}\\right)=\\left(t^{2},-t^{2}, t\\right)$. The Riemannian metric is given by\n\n$$\n\\left(g_{i j}\\right)=\\left[\\begin{array}{ccc}\n2 x^{1} & x^{3} & 0 \\\\\nx^{3} & 2 x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right] \\quad\\left(4 x^{1} x^{2} \\neq\\left(x^{3}\\right)^{2}\\right)\n$$\n\n7.25 Find the null points of the curve $\\mathscr{C}: x^{1}=t, x^{2}=t^{4} \\quad$ ( $t$ real), if the metric is\n\n$$\n\\varepsilon d s^{2}=8\\left(x^{1} d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n\n7.26 Find the arc length of the curve in Problem 7.25 if $0 \\leqq t \\leqq 2$.\n\n7.27 Find the null points of the curve $\\mathscr{C}: x^{1}=t^{3}+1, x^{2}=t^{2}, x^{3}=t$, if the metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(x^{3} d x^{3}\\right)^{2}\n$$\n\n7.28 Find the arc length of the curve in Problem 7.27 if $\\frac{1}{2} \\leqq t \\leqq 1$.\n\n7.29 Find the angle between the curves\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=5 t \\\\\nx^{2}=2 \\\\\nx^{3}=3 t\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u \\\\\nx^{2}=2 \\\\\nx^{3}=3 u^{2} / 25\n\\end{array}\\right.\\right.\n$$\n\nat each of the points of intersection, if the fundamental form is $\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2}$.\n\n7.30 If $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2},(a)$ find the length $L$ of the curve $\\mathscr{C}: x^{1}=12 t^{2}, x^{2}=8 t^{3}$, for $0 \\leqq t \\leqq 2$. (b) Find an arc-length parameterization, $x^{i}=x^{i}(s)$, for $\\mathscr{C}$, with $s=0$ corresponding to $t=1$. (c) Show that the $x^{i}(s)$ are differentiable to all orders except at points of nullity.\n\n7.31 Find the arc length of the curve of Problem.7.30, but with the Euclidean metric.\n\n7.32 Compute $\\mathbf{T}=\\left(d x^{i} / d s\\right)$ from the arc-length parameterization found in Problem 7.30 and verify that $\\mathbf{T}$ has unit length at all points except $s=0$.\n\n7.33 Calculate the components $N^{i}$ of the unit principal normal of the curve of Problem 7.30, using (7.16) (Problem 7.14).\n\n7.34 Calculate both the curvature $\\kappa$ and the absolute curvature $\\kappa_{0}$ for the curve of Problem 7.30. Discuss the numerical behavior of $\\kappa_{0}$ along the curve.\n\n7.35 Use the formula of Problem 7.18(b) to confirm the value of $\\kappa_{0}$ found in Problem 7.34.\n\n7.36 Compute $\\kappa_{0}$ under the Euclidean metric for the curve of Problem 7.30; compare with the result obtained in Problem 7.34. For convenience, let $t=0$ correspond to $s=8$.\n\n7.37 Without calculating an arc-length parameter, find the vectors $\\mathbf{T}$ and $\\mathbf{N}$, and the curvature $\\kappa$, for the \"parabola\" $x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$ under the Riemannian metric\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n\n7.38 Show that the first-quadrant portion $\\left(x^{i}>0\\right)$ of the hypocycloid $\\mathscr{H}$ of four cusps\n\n$$\n\\left(x^{1}\\right)^{2 / 3}+\\left(x^{2}\\right)^{2 / 3}=a^{2 / 3} \\quad(a>0)\n$$\n\nmay be parameterized as $x^{1}=a \\cos ^{3} t, x^{2}=a \\sin ^{3} t$, with $0 \\leqq t \\leqq \\pi / 2$. Find the arc length under the two metrics\n\n$$\n\\text { (a) } \\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2} \\quad \\text { (b) } \\quad d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}\n$$\n\n(c) Without computing an arc-length parameter, find $\\mathbf{T}$ and $\\kappa_{0}$ for $\\mathscr{H}$ under both metrics.\n\n7.39 (a) Determine the Christoffel symbols of the second kind for the Riemannian metric $\\varepsilon d s^{2}=x^{1}\\left(d x^{1}\\right)^{2}+$ $x^{2}\\left(d x^{2}\\right)^{2}$. (b) Without converting to an arc-length parameter, verify that all curves $x^{1}=t^{2}, x^{2}=$ $\\left(a t^{3}+b\\right)^{2 / 3}$, where $a$ and $b$ are arbitrary constants, are geodesics.\n\n"], "lesson": "\\section*{Chapter 7}\n\\section*{Riemannian Geometry of Curves}\n\\subsection*{7.1 INTRODUCTION}\nAt this point, some new terminology is introduced, which commemorates the general formulation of $n$-dimensional geometry by Bernhard Riemann (1826-1866).\n\nDefinition 1: A Riemannian space is the space $\\mathbf{R}^{n}$ coordinatized by $\\left(x^{i}\\right)$, together with a fundamental form or Riemannian metric, $g_{i j} d x^{i} d x^{j}$, where $\\mathbf{g}=\\left(g_{i j}\\right)$ obeys conditions A-D of Section 5.3.\n\nThus, in our preliminary treatment of angles, tangents, normals, and geodesic curves, in Chapters 5 and 6 , we already entered Riemannian geometry-though largely restricted to familiar 3-dimensional coordinate systems and a positive definite (Euclidean) metric. The present chapter focuses on the theory of curves in a Riemannian space with an indefinite metric. It also takes up geodesics from a different viewpoint.\n\n\\subsection*{7.2 LENGTH AND ANGLE UNDER AN INDEFINITE METRIC}\nFormulas (5.10) and (5.11) must be generalized to allow for changes in sign of the fundamental form.\n\nDefinition 2: The norm of an arbitrary (contravariant or covariant) vector $\\mathbf{V}$ is\n\n$$\n\\|\\mathbf{V}\\| \\equiv \\sqrt{\\varepsilon \\mathbf{V}^{2}}=\\sqrt{\\varepsilon V_{i} V^{i}} \\quad(\\varepsilon=\\varepsilon(\\mathbf{V}))\n$$\n\nwhere $\\varepsilon(\\quad)$ is the indicator function (Section 5.3).\n\nUnder this definition, $\\|\\mathbf{V}\\| \\geqq 0$, but it is possible that $\\|\\mathbf{V}\\|=0$ for $\\mathbf{V} \\neq \\mathbf{0}$; such a vector is called a null vector. Moreover, the triangle inequality is not necessarily obeyed by this norm (see Problem 7.8).\n\nIf $\\mathbf{V}(t)$ is the tangent field to the curve $x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$, then the length formula (5.1a) may be written as\n\n\n\\begin{equation*}\nL=\\int_{a}^{b} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}} d t=\\int_{a}^{b}\\|\\mathbf{V}(t)\\| d t \\tag{7.1}\n\\end{equation*}\n\n\nThe angle between non-null contravariant vectors is still defined by (5.11), provided the new norm is understood:\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{\\mathbf{U V}}{\\|\\mathbf{U}\\|\\|V\\|}=\\frac{g_{i j} U^{i} V^{j}}{\\sqrt{\\varepsilon_{1} g_{p q} U^{p} U^{q}} \\sqrt{\\varepsilon_{2} g_{r s} V^{r} V^{s}}} \\tag{7.2}\n\\end{equation*}\n\n\nwhere $\\varepsilon_{1}=\\varepsilon(\\mathbf{U})$ and $\\varepsilon_{2}=\\varepsilon(\\mathbf{V})$. Because of the indefiniteness of the metric, we must distinguish two possibilities in the application of (7.2).\n\nCase 1: $|\\mathbf{U V}| \\leqq\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|$ (the Cauchy-Schwarz inequality holds for $\\mathbf{U}$ and $\\mathbf{V}$ ). Then $\\theta$ is uniquely determined as a real number in the interval $[0, \\pi]$.\n\nCase 2: $|\\mathbf{U V}|>\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|$ (the Cauchy-Schwarz does not hold). Then (7.2) takes the form\n\n$$\n\\cos \\theta=k \\quad(|k|>1)\n$$\n\nwhich has an infinite number of solutions for $\\theta$, all of them complex. By convention, we always choose the solution\n\n$$\n\\theta= \\begin{cases}i \\ln \\left(k+\\sqrt{k^{2}-1}\\right) & k>1 \\\\ \\pi+i \\ln \\left(-k+\\sqrt{k^{2}-1}\\right) & k<-1\\end{cases}\n$$\n\nthat exhibits the proper limiting behavior as $k \\rightarrow 1^{+}$or $k \\rightarrow-1^{-}$.\n\nEXAMPLE 7.1 At the points of intersection, find the angles between the curves (i.e., between their tangents)\n\n$$\n\\mathscr{C}_{1}:\\left(x_{1}^{i}\\right)=\\left(t, 0,0, t^{2}\\right) \\quad \\mathscr{C}_{2}:\\left(x_{2}^{i}\\right)=\\left(u, 0,0,2-u^{2}\\right)\n$$\n\n( $t, u$ real), if the Riemannian metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}-\\left(d x^{4}\\right)^{2}\n$$\n\n[This is the metric of Special Relativity, with $x^{4} \\equiv$ (speed of light) $\\times($ time $)$.]\n\nThe curves meet in the two points $P(1,0,0,1)$ and $Q(-1,0,0,1)$. At $P$, (where $t=u=1$ ) the two tangent vectors are\n\n$$\n\\begin{aligned}\n& \\mathbf{U}_{P}=\\left(d x_{1}^{i} / d t\\right)_{P}=(1,0,0,2 t)_{P}=(1,0,0,2) \\\\\n& \\mathbf{V}_{P}=\\left(d x_{2}^{i} / d u\\right)_{P}=(1,0,0,-2 u)_{P}=(1,0,0,-2)\n\\end{aligned}\n$$\n\nso that (7.2) gives\n\n$$\n\\begin{aligned}\n\\cos \\theta_{P} & =\\frac{1(1)(1)+1(0)(0)+1(0)(0)-1(2)(-2)}{\\sqrt{\\varepsilon_{1}\\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(2)^{2}\\right]} \\sqrt{\\varepsilon_{2}\\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(-2)^{2}\\right]}} \\\\\n& =\\frac{5}{\\sqrt{+3} \\sqrt{+3}}=\\frac{5}{3}\n\\end{aligned}\n$$\n\nand $\\theta_{P}=i \\ln \\left[(5 / 3)+\\sqrt{(5 / 3)^{2}-1}\\right]=i \\ln 3$.\n\nSimilarly we calculate (for $-t=u=1$ )\n\nso that $\\theta_{Q}=\\theta_{P}$.\n\n$$\n\\mathbf{U}_{Q}=(1,0,0,-2)=\\mathbf{V}_{P} \\quad \\mathbf{V}_{Q}=(1,0,0,2)=\\mathbf{U}_{P}\n$$\n\n\\subsection*{7.3 NULL CURVES}\nIf $\\mathbf{g}$ is not required to be positive definite, a curve can have zero length.\n\nEXAMPLE 7.2 In $\\mathbf{R}^{4}$, under the metric of Example 7.1, consider the curve\n\n$$\nx^{1}=3 \\cos t \\quad x^{2}=3 \\sin t \\quad x^{3}=4 t \\quad x^{4}=5 t\n$$\n\nfor $0 \\leqq t \\leqq 1$. Along the curve,\n\n$$\n\\begin{aligned}\n\\left(\\frac{d x^{i}}{d t}\\right) & =(-3 \\sin t, 3 \\cos t, 4,5) \\\\\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2} & =g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=(-3 \\sin t)^{2}+(3 \\cos t)^{2}+(4)^{2}-(5)^{2}=0\n\\end{aligned}\n$$\n\nand so the arc length is\n\n$$\nL=\\int_{0}^{1} 0 d t=0\n$$\n\nA curve is null if it or any of its subarcs has zero length. Here, a subarc is understood to be nontrivial; that is, it consists of more than one point and corresponds to an interval $c \\leqq t \\leqq d$, where $c<d$. A curve is null at a point if for some value of the parameter $t$ the tangent vector is a null vector; i.e.,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=0\n$$\n\nThe set of $t$-values at which the curve is null is known as the null set of the curve.\n\nUnder the above definitions, a curve can be null without having zero length (if there is a subarc with zero length); but a curve having zero length is necessarily null at every point, and hence a null curve. Example 7.2 gives such a curve.\n\nEXAMPLE 7.3 Under the Riemannian metric\n\n$$\nG=\\left[\\begin{array}{cc}\n\\left(x^{1}\\right)^{2} & -1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\nthe curve $\\left(x^{1}, x^{2}\\right)=\\left(t,\\left|t^{3}\\right| / 6\\right)$ possesses a null subarc that renders the length of the curve much smaller than might be expected. In fact, because $d x^{1} / d t=1$ and $d x^{2} / d t=\\delta t^{2} / 2$, where $\\delta= \\pm 1$ and is positive if $t \\geqq 0$,\n\n$$\n\\varepsilon g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\varepsilon\\left[\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{1}}{d t}\\right)^{2}-2 \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}\\right]=\\varepsilon\\left[t^{2}(1)-2(1)\\left(\\delta t^{2} / 2\\right)\\right]=\\varepsilon\\left(t^{2}-\\delta t^{2}\\right)\n$$\n\nSince the quantity following the indicator is nonnegative, $\\varepsilon=+1$ everywhere. But note that $t^{2}-\\delta t^{2}=0$ if $t \\geqq 0$. Hence,\n\n$$\n\\begin{aligned}\nL & =\\int_{-1}^{999} \\sqrt{t^{2}-\\delta t^{2}} d t=\\int_{-1}^{0} \\sqrt{2 t^{2}} d t+\\int_{0}^{999} 0 d t=\\sqrt{2} \\int_{-1}^{0}(-t) d t \\\\\n& =-\\sqrt{2} t^{2} /\\left.2\\right|_{-1} ^{0}=\\sqrt{2} / 2 \\approx 0.707\n\\end{aligned}\n$$\n\nThe interpretation in rectangular coordinates $\\left(x^{2}, x^{2}\\right)$ is queer: As a particle travels less than a millimeter along the curve, its \"shadow\" on the $x^{1}$-axis travels a meter!\n\n\\section*{Nonexistence of an Arc-Length Parameter}\nFor a positive definite metric, the arc-length parameter $s$ is well-defined by (5.6) as a strictly increasing function of the curve parameter $t$. (Then it is also the case that $t$ is a strictly increasing function of $s$.) This fact allowed us freely to convert between the two parameterizations in the ", "solved_problems": "Solved Problems to Chapter 6. However, it is clear that on a null curve, which possesses at least one interval $t_{1}<t<t_{2}$ of null points, it is impossible to define arc length $s$. rule,\n\nIndeed, even isolated points of nullity pose analytical problems. For if $s^{\\prime}\\left(t_{0}\\right)=0$, then the chain\n\n\n\\begin{equation*}\n\\frac{d x^{i}}{d s}=\\frac{d x^{i}}{d t} \\frac{1}{s^{\\prime}(t)} \\tag{7.3}\n\\end{equation*}\n\n\nbreaks down at $s_{0}$, the image of $t_{0}$. When necessary, we get around the difficulty by restricting attention to curves that are regular.\n\nDefinition 3: A curve is regular if it has no null points (i.e., $d s / d t>0$ ).\n\nIt will be further assumed that all curves are of sufficiently high differentiability class to permit the theory considered; usually, this will require the assumption that curves are of class $C^{2}$.\n\n\\subsection*{7.4 REGULAR CURVES: UNIT TANGENT VECTOR}\nLet a regular curve $\\mathscr{C}: x^{i}=x^{i}(s)$ be given in terms of the arc-length parameter; the tangent field is $\\mathbf{T} \\equiv\\left(d x^{i} / d s\\right)$. By definition of arc length,\n\n$$\ns=\\int_{0}^{s}\\|\\mathbf{T}(u)\\| d u\n$$\n\nand differentiation gives $1=\\|\\mathbf{T}(s)\\|$, showing that $\\mathbf{T}$ has unit length at each point of $\\mathscr{C}$.\n\nWhen it is inconvenient or impossible to convert to the arc-length parameter, we can, by (7.3),\\\\\nobtain $\\mathbf{T}$ by normalizing the tangent vector $\\mathbf{U}=\\left(d x^{i} / d t\\right)$ :\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{1}{\\|\\mathbf{U}\\|} \\mathbf{U}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U} \\tag{7.4}\n\\end{equation*}\n\n\nIn Problem 7.20 is proved the useful\n\nTheorem 7.1: The absolute derivative $\\delta \\mathbf{T} / \\delta s$ of the unit tangent vector $\\mathbf{T}$ is orthogonal to $\\mathbf{T}$.\n\n\\subsection*{7.5 REGULAR CURVES: \\\\\n UNIT PRINCIPAL NORMAL AND CURVATURE}\nAlso associated with a regular curve $\\mathscr{C}$ is a vector orthogonal to the tangent vector. It may be introduced in two ways: (1) as the normalized $\\delta \\mathbf{T} / \\delta s$, if it exists; (2) as any differentiable unit vector orthogonal to $\\mathbf{T}$ and proportional to $\\delta \\mathbf{T} / \\delta s$ when $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$. The latter definition is global in nature, and it applies to a larger class of curves than does the former.\n\n\\section*{Analytical (Local) Approach}\nAt any point of $\\mathscr{C}$ at which $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, define the unit principal normal as the vector\n\n\n\\begin{equation*}\n\\mathbf{N}_{0} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta s} /\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\| \\tag{7.5}\n\\end{equation*}\n\n\nThe absolute curvature is the scale factor in (7.5):\n\n\n\\begin{equation*}\n\\kappa_{0} \\equiv\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\sqrt{\\varepsilon g_{i j} \\frac{\\delta T^{i}}{\\delta s} \\frac{\\delta T^{j}}{\\delta s}} \\tag{7.6}\n\\end{equation*}\n\n\nThis notion of curvature was informally defined in (6.12).\n\nCalling this quantity \"curvature\" is suggestive of the fact that in rectangular coordinates $\\|\\delta \\mathbf{T} / \\delta s\\|=\\|d \\mathbf{T} / d s\\|$ measures the rate of change of the tangent vector with respect to distance, or how sharply $\\mathscr{C}$ \"bends\" at each point. Substitution of (7.6) into (7.5) yields one of the Frenet equations:\n\n\n\\begin{equation*}\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\kappa_{0} \\mathbf{N}_{0} \\quad\\left(\\kappa_{0} \\neq 0\\right) \\tag{7.7}\n\\end{equation*}\n\n\nWhile this approach is simple and concise, it does not apply to many curves we want to consider; for instance, a geodesic-as defined by (6.13) - will not possess a local normal $\\mathbf{N}_{0}$ at any point. Even if there is only one point of zero curvature and the metric is Euclidean, $\\mathbf{N}_{0}$ can have an essential point of discontinuity there.\n\nEXAMPLE 7.4 The simple cubic $y=x^{3}$ has an inflection point at the origin, or $s=0$ (by arrangement). As shown in Fig. 7-1,\n\n$$\n\\lim _{s \\rightarrow-0} \\mathbf{N}_{0}=(0,-1) \\quad \\lim _{s \\rightarrow+0} \\mathbf{N}_{0}=(0,1)\n$$\n\nTo verify this analytically, make the parameterization $x=t, y=t^{3}$, and calculate $\\mathbf{N}_{0}$ as a function of $t$ $\\left(s^{\\prime}(t)=\\sqrt{1+9 t^{4}}\\right)$.\n\n$$\n\\begin{aligned}\n& \\mathbf{U}=\\left(x^{\\prime}(t), y^{\\prime}(t)\\right)=\\left(1,3 t^{2}\\right) \\\\\n& \\mathbf{T}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U}=\\frac{1}{\\sqrt{1+9 t^{4}}}\\left(1,3 t^{2}\\right) \\\\\n& \\frac{d \\mathbf{T}}{d s}=\\frac{1}{s^{\\prime}(t)} \\frac{d \\mathbf{T}}{d t}=\\frac{6 t}{\\left(1+9 t^{4}\\right)^{2}}\\left(-3 t^{2}, 1\\right) \\\\\n& \\kappa_{0}=\\left\\|\\frac{d \\mathbf{T}}{d s}\\right\\|=\\frac{6|t|}{\\left(1+9 t^{4}\\right)^{3 / 2}}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-096}\n\\end{center}\n\nFig. 7-1\n\n$$\n\\mathbf{N}_{0}=\\frac{1}{\\kappa_{0}} \\frac{d \\mathbf{T}}{d s}=\\frac{t /|t|}{\\sqrt{1+9 t^{4}}}\\left(--3 t^{2}, 1\\right) \\quad(t \\neq 0)\n$$\n\nThe scalar factor $t /|t|$ accounts for the discontinuity in $\\mathbf{N}_{0}$ at $t=0 \\quad(s=0)$.\n\n\\section*{Geometric (Global) Approach}\nA unit principal normal to a regular curve $\\mathscr{C}$ is any contravariant vector $\\mathbf{N}=\\left(N^{i}(s)\\right)$ such that, along $\\mathscr{C}$,\\\\\nA. $\\quad N^{i}$ is continuously differentiable (class $C^{1}$ ) for each $i$;\\\\\nB. $\\|\\mathbf{N}\\|=1$;\\\\\nC. $\\mathbf{N}$ is orthogonal to the unit tangent vector $\\mathbf{T}$, and is a scalar multiple of $\\delta \\mathbf{T} / \\delta s$ wherever $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$.\n\nThe curvature under this development is defined as\n\n\n\\begin{equation*}\n\\kappa \\equiv \\varepsilon \\mathbf{N} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\varepsilon g_{i j} N^{i} \\frac{\\delta T^{j}}{\\delta s} \\quad(\\varepsilon=\\varepsilon(\\mathbf{N})) \\tag{7.8}\n\\end{equation*}\n\n\nIf the metric is positive definite, the Frenet equation\n\n\n\\begin{equation*}\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\kappa \\mathbf{N} \\tag{7.9}\n\\end{equation*}\n\n\nholds unrestrictedly along a regular curve (see Problem 7.13).\n\nEXAMPLE 7.5 For the curve of Example 7.4, conditions A, B, and C allow precisely two possibilities for N:\n\n$$\n\\mathbf{N}=\\frac{+1}{\\sqrt{1+9 t^{4}}}\\left(-3 t^{2}, 1\\right) \\quad \\text { or } \\quad \\mathbf{N}=\\frac{-1}{\\sqrt{1+9 t^{4}}}\\left(-3 t^{2}, 1\\right)\n$$\n\nfor $-\\infty<t<\\infty$. Geometrically, these amount to reversing the normal arrows in either the left half or the right half of Fig. 7-1. The corresponding formulas for curvature are $(\\varepsilon \\equiv 1)$\n\n$$\n\\kappa=\\frac{6 t}{\\left(1+9 t^{4}\\right)^{3 / 2}} \\quad \\text { or } \\quad \\kappa=\\frac{-6 t}{\\left(1+9 t^{4}\\right)^{3 / 2}}\n$$\n\nOn curves having everywhere a non-null $\\delta \\mathbf{T} / \\delta s$, either $\\mathbf{N} \\equiv \\mathbf{N}_{0}$ (with $\\kappa=\\kappa_{0}$ ) or $\\mathbf{N} \\equiv-\\mathbf{N}_{0}$ (with $\\kappa=-\\kappa_{0}$ ). Thus, the global concept applies to all curves covered by the local concept and, in addition, to all regular planar curves (see Problem 7.14) and all analytic curves (curves for which the $x^{i}$ are representable as convergent Taylor series in $s$ ).\n\n\\subsection*{7.6 GEODESICS AS SHORTEST ARCS}\nWhen the metric is positive definite, a geodesic may be defined by the zero-curvature conditions (6.13), or, equivalently, by the condition that for any two of its points sufficiently close together, its length between the two points is least among all curves joining those points.\n\nThe minimum-length development employs a variational argument. We need to assume that all curves under consideration are class $C^{2}$ (that is, the parametric functions which represent them have continuous second-order derivatives). Let $x^{i}=x^{i}(t)$ represent a shortest curve (geodesic) passing through $A=\\left(x^{i}(a)\\right)$ and $B=\\left(x^{i}(b)\\right)$, where $b-a$ is as small as necessary. Embed the geodesic in a one-parameter family of $C^{2}$ curves passing through $A$ and $B$ :\n\n$$\nx^{i}=X^{i}(t, u) \\equiv x^{i}(t)+(t-a)(b-t) u \\phi^{i}(t)\n$$\n\nwhere the multipliers $\\phi^{i}(t)$ are arbitrary twice-differentiable functions. The length of a curve in this family is given by\n\n$$\nL(u)=\\int_{a}^{b} \\sqrt{\\varepsilon g_{i j} \\frac{\\partial X^{i}}{\\partial t} \\frac{\\partial X^{j}}{\\partial t}} d t \\equiv \\int_{a}^{b} \\sqrt{w(t, u)} d t\n$$\n\nwith $\\varepsilon=1$ for a positive-definite metric. Since $X^{i}(t, 0)=x^{i}(t) \\quad(i=1,2, \\ldots, n)$, the function $L(u)$ must have a local minimum at $u=0$. Standard calculus techniques yield the following expression of the necessary condition $L^{\\prime}(0)=0$ :\n\n\n\\begin{equation*}\n\\int_{a}^{b}\\left[w^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}-\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right)\\right](t-a)(b-t) \\phi^{k}(t) d t=0 \\tag{7.10}\n\\end{equation*}\n\n\nin which\n\n\n\\begin{equation*}\nw \\equiv w(t, 0)=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} \\tag{7.11}\n\\end{equation*}\n\n\nSince $(t-a)(b-t)>0$ on $(a, b)$ and $\\phi^{k}(t)$ may be chosen arbitrarily, the bracketed expression in (7.10) must vanish identically over $(a, b)$, for $k=1,2, \\ldots, n$; this leads to (Problem 7.21)\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad(i=1,2, \\ldots, n) \\tag{7.12}\n\\end{equation*}\n\n\nSystem (7.12), with $w$ defined by (7.11), are the differential equations for the geodesics of Riemannian space, in terms of the arbitrary curve parameter $t$. Assuming that these geodesics will be regular curves, we may choose $t=s=\\operatorname{arc}$ length. Then\n\n$$\nw=\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d s}{d s}\\right)^{2}=(1)^{2}=1 \\quad \\text { and } \\quad \\frac{d w}{d s}=0\n$$\n\nso that $(7.12)$ becomes\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d s} \\frac{d x^{k}}{d s}=0 \\quad(i=1,2, \\ldots, n) \\tag{7.13}\n\\end{equation*}\n\n\nwhich is precisely (6.13).\n\nIt must be emphasized that $L^{\\prime}(0)=0$ is only a necessary condition for minimum length, so that the geodesics are found among the solutions of (7.12) or (7.13).\n\n\\section*{Null Geodesics}\nConsider the case of indefinite metrics and class $\\mathscr{C}^{2}$ curves which may have one or more null points. Since, at a null point, $w=0$ in (7.11) the variational theory breaks down, because $L(u)$ fails\\\\\nto be differentiable at such a point. Analogous to the zero-curvature approach, we consider the more general condition for geodesics\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t} \\equiv \\frac{\\delta U^{i}}{\\delta t}=0 \\quad(i=1,2, \\ldots, n) \\tag{7.14}\n\\end{equation*}\n\n\nwhere $U=U^{i}=\\left(d x^{i} / d t\\right)$ is the tangent vector field. By properties of absolute differentiation,\n\n$$\n\\frac{d w}{d t}=\\frac{d}{d t}\\left(\\varepsilon g_{i j} U^{i} U^{j}\\right)=\\frac{\\delta}{\\delta t}\\left(\\varepsilon g_{i j} U^{i} U^{j}\\right)=2 \\varepsilon g_{i j} U^{i} \\frac{\\delta U^{j}}{\\delta t}=0\n$$\n\nalong a solution curve to (7.14); so $w=$ const. along the curve. Since the curve has at least one point of nullity, $w=0$ at all points, whence the curve is a null curve-called a null geodesic. In summary, the following system of $n+1$ ordinary differential equations in the $n$ unknown functions $x^{i}(t)$ will determine the null geodesics:\n\n$$\n\\begin{aligned}\n& \\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0 \\quad(i=1,2, \\ldots, n) \\\\\n& g_{r s} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0\n\\end{aligned}\n$$\n\nEXAMPLE 7.6 If the $g_{i j}$ are constants, (7.15) has the expected general solution\n\n$$\nx^{i}=x_{0}^{i}+\\alpha^{i} t \\quad \\text { with } \\quad g_{i j} \\alpha^{i} \\alpha^{j}=0\n$$\n\nImagining the $x^{i}$ to be rectangular coordinates, we interpret the null geodesics as a bundle of straight lines issuing from the arbitrary point $\\mathbf{x}_{0}$; each line is in the direction of some null vector $\\boldsymbol{\\alpha}$. By elimination of the $\\alpha^{i}$ the equation of the bundle is found to be\n\n$$\ng_{i j}\\left(x^{i}-x_{0}^{i}\\right)\\left(x^{j}-x_{0}^{j}\\right)=0\n$$\n\nIn particular, for the space of Special Relativity $\\left(g_{11}=g_{22}=g_{33}=-g_{44}=1, g_{i j}=0\\right.$ for $i \\neq j$ ), the null geodesics compose the $45^{\\circ}$ cone\n\n-see Fig. 7-2.\n\n$$\n\\left(x^{1}-x_{0}^{1}\\right)^{2}+\\left(x^{2}-x_{0}^{2}\\right)^{2}+\\left(x^{3}-x_{0}^{3}\\right)^{2}=\\left(x^{4}-x_{0}^{4}\\right)^{2}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-098}\n\\end{center}\n\nFig. 7-2\n\n\\section*{Solved Problems}\n\\section*{LENGTH IN RIEMANNIAN SPACE}\n7.1 Determine the indicator of the tangent vector $\\mathrm{U}$ to the curve\n\n$$\nx^{1}=t^{3} \\quad x^{2}=t^{2} \\quad x^{3}=t\n$$\n\n$(-\\infty<t<\\infty)$ if the fundamental form is\n\n(a) $\\left(d x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{3}\\right)^{2}-6 d x^{1} d x^{3}+2 x^{1} x^{2} d x^{2} d x^{3}$\n\n(b) $\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+3\\left(d x^{3}\\right)^{2}$\n\n(a) $\\left(3 t^{2}\\right)^{2}+t^{4}(2 t)^{2}+t^{6}(1)^{2}-6\\left(3 t^{2}\\right)(1)+2\\left(t^{3}\\right)\\left(t^{2}\\right)(2 t)(1)=9 t^{6}+9 t^{4}-18 t^{2}=9 t^{2}\\left(t^{2}+2\\right)\\left(t^{2}-1\\right)$\n\nSince $t^{2}+2$ is always positive,\n\n$$\n\\varepsilon(\\mathbf{U})=\\left\\{\\begin{array}{rc}\n+1 & t \\geqq 1 \\\\\n-1 & 0<t<1 \\\\\n+1 & t=0 \\\\\n-1 & -1<t<0 \\\\\n+1 & t \\leqq-1\n\\end{array}\\right.\n$$\n\n(b) $\\varepsilon(\\mathbf{U}) \\equiv+1$, because the form is positive definite.\n\n7.2 Show that the following matrix defines a Riemannian metric on $\\mathbf{R}^{2}$ :\n\n$$\nG=\\left[\\begin{array}{rr}\nx^{2} & -x^{1} \\\\\n-x^{1} & x^{2}\n\\end{array}\\right] \\quad\\left(x^{1}>0,-x^{1}<x^{2}<x^{1}\\right)\n$$\n\nWe must show that conditions A-D of Section 5.3 are satisfied.\n\nA. Since each $g_{i j}$ is linear in the $x^{i}$, it is differentiable to any order.\n\nB. By observation, the matrix is symmetric.\\\\\nC. $\\left|g_{i j}\\right|=\\left(x^{2}\\right)^{2}-\\left(x^{1}\\right)^{2}<0$ over the given domain.\n\nD. Extend the matrix to a tensor $\\mathbf{g}$ by using the tensor transformation laws to define the $\\bar{g}_{i j}$ in terms of the $g_{i j}$. This will then make the quadratic form $g_{i j} d x^{i} d x^{j}$, hence the distance formula, an invariant.\n\n7.3 Find the null set of the curve $\\mathscr{C}: x^{2}=\\left(x^{1}\\right)^{2} \\quad\\left(x^{1}>0\\right)$ under the metric of Problem 7.2.\n\nLet $\\mathscr{C}$ be parameterized by $x^{1}=t, x^{2}=t^{2} \\quad(t>0)$. Then, along $\\mathscr{C}$,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{cc}\nt^{2} & -t \\\\\n-t & t^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n1 \\\\\n2 t\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{c}\n-t^{2} \\\\\n-t+2 t^{3}\n\\end{array}\\right]=t^{2}\\left(4 t^{2}-3\\right)\n$$\n\nwhich, for positive $t$, vanishes only at $t=\\sqrt{3} / 2$.\n\n7.4 Find the arc length of the curve $\\mathscr{C}$ in Problem 7.3 from $x^{1}=0$ to $x^{1}=1$.\n\nAgain using $t=x^{1}$, observe that\n\nHence,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}<0 \\quad \\text { for } \\quad 0<t<\\sqrt{3} / 2\n$$\n\n$$\n\\begin{aligned}\nL & =\\int_{0}^{1} \\sqrt{\\varepsilon t^{2}\\left(4 t^{2}-3\\right)} d t=\\int_{0}^{\\sqrt{3} / 2} t \\sqrt{-\\left(4 t^{2}-3\\right)} d t+\\int_{\\sqrt{3} / 2}^{1} t \\sqrt{4 t^{2}-3} d t \\\\\n& =-\\left.\\frac{1}{12}\\left(3-4 t^{2}\\right)^{3 / 2}\\right|_{0} ^{\\sqrt{3} / 2}+\\left.\\frac{1}{12}\\left(4 t^{2}-3\\right)^{3 / 2}\\right|_{\\sqrt{3} / 2} ^{1}=\\frac{3 \\sqrt{3}+1}{12} \\approx 0.516\n\\end{aligned}\n$$\n\n7.5 Write $g \\equiv \\operatorname{det} G$ for the determinant of a Riemannian metric. Prove that $|g|$ is a differentiable function of the coordinates.\n\nApplying the chain rule to $|g|=\\sqrt{g^{2}}$, we have\n\n\n\\begin{equation*}\n\\frac{\\partial|g|}{\\partial x^{i}}=\\frac{g}{|g|} \\frac{\\partial g}{\\partial x^{i}} \\tag{1}\n\\end{equation*}\n\n\nSince $\\partial g / \\partial x^{i}$ exists (Property A) and $|g| \\neq 0$ (Property C), the right-hand side of (1) is well-defined.\n\n7.6 Show that under the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2}-\\left(d x^{4}\\right)^{2}$ (another version of the metric for Special Relativity), the curve\n\n$$\nx^{1}=A \\sinh t \\quad x^{2}=A \\cosh t \\quad x^{3}=B t \\quad x^{4}=C t \\quad(0 \\leqq t \\leqq 1)\n$$\n\nwith $A^{2}=B^{2}+C^{2}$, is null at each of its points.\n\n$$\n\\begin{aligned}\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} & =\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}-\\left(\\frac{d x^{3}}{d t}\\right)^{2}-\\left(\\frac{d x^{4}}{d t}\\right)^{2} \\\\\n& =(A \\cosh t)^{2}-(A \\sinh t)^{2}-B^{2}-C^{2} \\\\\n& =A^{2}\\left(\\cosh ^{2} t-\\sinh ^{2} t\\right)-B^{2}-C^{2}=A^{2}-B^{2}-C^{2} \\equiv 0\n\\end{aligned}\n$$\n\n7.7 At the point of intersection $(0,0)$, find the angle between the curves\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=2 t-2 \\\\\nx^{2}=t^{2}-1\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u^{4}-1 \\\\\nx^{2}=25 u^{2}+50 u-75\n\\end{array}\\right.\\right.\n$$\n\nif the Riemannian metric is given by $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}$.\n\nAt $t=1, \\mathbf{T} \\equiv\\left(d x^{i} / d t\\right)=(2,2)$; at $u=1, \\mathbf{U} \\equiv\\left(d x^{i} / d u\\right)=(4,100)$. Hence, using matrices,\n\n$$\n\\begin{aligned}\n& \\mathbf{T U}=\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=-200 \\\\\n& \\|\\mathbf{T}\\|^{2}=\\varepsilon_{1}\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n2 \\\\\n2\n\\end{array}\\right]=\\left(\\varepsilon_{1}\\right)(-4)=4 \\\\\n& \\|\\mathbf{U}\\|^{2}=\\varepsilon_{2}\\left[\\begin{array}{ll}\n4 & 100\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=\\left(\\varepsilon_{2}\\right)(-784)=784\n\\end{aligned}\n$$\n\n$$\n\\text { and } \\quad \\cos \\theta=\\frac{-200}{\\sqrt{4} \\sqrt{784}}=-\\frac{25}{7}\n$$\n\nThis is Case 2 of Section 7.2; we have\n\n$$\n\\theta=\\pi+i \\ln \\left(\\frac{25}{7}+\\sqrt{\\left(\\frac{25}{7}\\right)^{2}-1}\\right)=\\pi+i \\ln 7\n$$\n\n7.8 Verify that the vectors of Problem 7.7 do not obey the triangle inequality.\n\nAs calculated, $\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|=2+28=30$. But\n\n$$\n\\|\\mathbf{T}+\\mathbf{U}\\|^{2}=\\varepsilon_{3}\\left[\\begin{array}{ll}\n6 & 102\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n6 \\\\\n102\n\\end{array}\\right]=\\varepsilon_{3}(-1188)=1188\n$$\n\nwhence $\\|\\mathbf{T}+\\mathbf{U}\\| \\approx 34.46>\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|$.\n\n\\section*{ARC-LENGTH PARAMETER, UNIT TANGENT VECTOR}\n7.9 Let $\\mathscr{C}: x^{i}=x^{i}(t)$ be any non-null curve. (a) Prove that $\\operatorname{arc}$ length along $\\mathscr{C}$ is defined as a strictly increasing function of $t$. (b) Exhibit the arc-length parameterization of $\\mathscr{C}$.\\\\\n(a)\n\nFor $t_{1}<t_{2}$, the Mean-Value Theorem of calculus gives\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left(t_{2}-t_{1}\\right) s^{\\prime}(\\tau) \\quad\\left(t_{1}<\\tau<t_{2}\\right)\n$$\n\nThe right-hand side is nonnegative, so that $s\\left(t_{1}\\right) \\leqq s\\left(t_{2}\\right)$. But, in view of the identity\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left[s\\left(t_{2}\\right)-s\\left(t_{3}\\right)\\right]+\\left[s\\left(t_{3}\\right)-s\\left(t_{1}\\right)\\right]\n$$\n\nwhere $t_{3}$ is any point in $\\left(t_{1}, t_{2}\\right)$, the equality $s\\left(t_{1}\\right)=c=s\\left(t_{2}\\right)$ would imply $s\\left(t_{3}\\right)=c$; i.e., $s(t)$ would be constant on $\\left[t_{1}, t_{2}\\right]$, making $s^{\\prime}(t) \\equiv 0$ on $\\left(t_{1}, t_{2}\\right)$ and thus making $\\mathscr{C}$ a null curve. We conclude that\n\n$$\ns\\left(t_{1}\\right)<s\\left(t_{2}\\right) \\quad \\text { whenever } \\quad t_{1}<t_{2}\n$$\n\n(b) The strictly increasing function $s(t)$ will possess a strictly increasing inverse; denote it as $t=\\theta(s)$. Then $\\mathscr{C}$ admits the parameterization $x^{i}=x^{i}(\\theta(s))$.\n\n7.10 (a) In rectangular coordinates $\\left(x^{1}, x^{2}\\right)$ but adopting the metric of Problem 7.7, find the null points of the parabola $\\mathscr{C}: x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$. (b) Show that the arc-length parameterization of $\\mathscr{C}$ is differentiable to all orders except at the null points. (c) Find the length of $\\mathscr{C}$.\n\n\n\\begin{equation*}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}-2 \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=1-4 t \\tag{a}\n\\end{equation*}\n\n\nso there is only one null point, at $t=1 / 4$.\n\n\n\\begin{equation*}\ns=\\int_{0}^{t} \\sqrt{\\varepsilon(1-4 u)} d u \\tag{b}\n\\end{equation*}\n\n\nThus, for $0 \\leqq t \\leqq 1 / 4$,\n\n$$\ns=\\int_{0}^{t} \\sqrt{1-4 u} d u=\\frac{1}{6}\\left[1-(1-4 t)^{3 / 2}\\right]\n$$\n\nand, for $1 / 4 \\leqq t \\leqq 1 / 2$,\n\n$$\ns=\\int_{0}^{1 / 4} \\sqrt{1-4 u} d u+\\int_{1 / 4}^{t} \\sqrt{4 u-1} d u=\\frac{1}{6}\\left[1+(4 t-1)^{3 / 2}\\right]\n$$\n\nInversion of these formulas gives\n\n\\[\nt=\\theta(s)=\\left\\{\\begin{array}{lr}\n\\frac{1}{4}\\left[1-(1-6 s)^{2 / 3}\\right] & 0 \\leqq s \\leqq 1 / 6  \\tag{1}\\\\\n\\frac{1}{4}\\left[1+(6 s-1)^{2 / 3}\\right] & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array}\\right.\n\\]\n\nIt is evident that $\\theta(s)$ is infinitely differentiable except at the null point $s=1 / 6$ (the image of $t=1 / 4)$; the same will be true of the functions $x^{1}=\\theta(s), x^{2}=\\theta^{2}(s)$.\n\n(c) Set $t=1 / 2$ in the applicable expression for $s$ :\n\n$$\ns=\\frac{1}{6}\\left[1+(2-1)^{3 / 2}\\right]=\\frac{1}{3}\n$$\n\n7.11 Find the arc length of the same curve $\\mathscr{C}$ as in Problem 7.10, but with the normal Euclidean metric, $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}$.\n\nNow\n\n$$\n\\frac{d s}{d t}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{4 t^{2}+1}\n$$\n\nso that\n\n$$\nL=\\int_{0}^{1 / 2} \\sqrt{4 t^{2}+1} d t=\\left[\\frac{t}{2} \\sqrt{4 t^{2}+1}+\\frac{1}{4} \\ln \\left(2 t+\\sqrt{4 t^{2}+1}\\right)\\right]_{0}^{1 / 2}=\\frac{\\sqrt{2}+\\ln (1+\\sqrt{2})}{4} \\approx 0.574\n$$\n\nas compared to $L \\approx 0.333$ in Problem 7.10 .\n\n7.12 Using the arc-length parameterization found for the curve $\\mathscr{C}$ in Problem 7.10(b), compute the components $T^{i}(s)$ of the tangent vector and verify that this vector has unit length for all $s \\neq 1 / 6$.\n\nWe have $\\left(T^{i}\\right)=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$, where $\\theta=\\theta(s)$ is the function defined by (1) in Problem $7.10(b)$. Hence,\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon\\left(\\theta^{\\prime 2}-4 \\theta \\theta^{\\prime 2}\\right)=\\varepsilon(1-4 \\theta) \\theta^{\\prime 2}\n$$\n\nBut, by (1) of Problem 7.10(b),\n\n$$\n1-4 \\theta=\\left\\{\\begin{array}{rrr}\n(1-6 s)^{2 / 3} & 0 \\leqq s \\leqq 1 / 6 \\\\\n-(6 s-1)^{2 / 3} & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array} \\quad \\theta^{\\prime}=\\left\\{\\begin{array}{rr}\n(1-6 s)^{-1 / 3} & 0<s<1 / 6 \\\\\n(6 s-1)^{-1 / 3} & 1 / 6<s<1 / 3\n\\end{array}\\right.\\right.\n$$\n\nTherefore, $\\|\\mathbf{T}\\|^{2}=(\\varepsilon)( \\pm 1)=+1$, or $\\|\\mathbf{T}\\| \\equiv 1 \\quad(s \\neq 1 / 6)$.\n\n\\section*{UNIT PRINCIPAL NORMAL, CURVATURE}\n7.13 Prove that the Frenet equation (7.9) holds at each point of a regular curve when the metric is positive definite.\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, we have (from property $\\mathrm{C}$ of $\\mathbf{N}$ ),\n\n\n\\begin{equation*}\n\\mathbf{N}=\\lambda \\frac{\\delta \\mathbf{T}}{\\delta s} \\tag{1}\n\\end{equation*}\n\n\nfrom some real $\\lambda$. Take the inner product with the vector $\\mathbf{N}$ in (1); with $\\varepsilon=\\varepsilon(\\mathbf{N})$,\n\n\n\\begin{equation*}\n\\varepsilon \\mathbf{N}^{2}=\\varepsilon \\lambda \\mathbf{N} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\lambda \\kappa \\quad \\text { or } \\quad 1=\\lambda \\kappa \\tag{2}\n\\end{equation*}\n\n\nThen $\\lambda=1 / \\kappa$, and substitution into (1) yields (7.9).\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\|=0$, both $\\delta \\mathbf{T} / \\delta s=\\mathbf{0}$ (because the metric is positive definite) and $\\kappa=0$ (by $(7.8)$ ); the Frenet equation then holds trivially.\n\n7.14 For any regular two-dimensional curve $\\mathscr{C}: x^{i}=x^{i}(s)$, define the contravariant vector\n\n\n\\begin{equation*}\n\\mathbf{N}=\\left(N^{i}\\right) \\equiv\\left(-T_{2} / \\sqrt{|g|}, T_{1} / \\sqrt{|g|}\\right) \\tag{7.16}\n\\end{equation*}\n\n\nwhere $\\mathbf{T}=\\left(T^{i}\\right)$ is the unit tangent vector along $\\mathscr{C}$ and $g=\\operatorname{det}\\left(g_{i j}\\right)$. Show that $\\mathbf{N}$ is a global unit normal for $\\mathscr{C}$.\n\nWe must show that the three properties of Section 7.5 are possessed by the given vector (except possibly at null points).\n\nA. Since $\\mathscr{C}$ is regular, the $T^{i}$, and with them the $T_{i}=g_{i j} T^{j}$, are in $C^{1}$. The same is true of $|g|$ (Problem 7.5), which function is strictly positive. Therefore, the $N^{i}$ are also in $C^{1}$.\n\nB. By (2.11), $g^{11}=g_{22} / g, g^{12}=g^{21}=-g_{12} / g$, and $g^{22}=g_{11} / g$. Hence,\n\n$$\n\\begin{aligned}\n\\|\\mathbf{N}\\|^{2} & =\\left|g_{11}\\left(T_{2}^{2} /|g|\\right)+2 g_{12}\\left(-T_{1} T_{2} /|g|\\right)+g_{22}\\left(T_{1}^{2} /|g|\\right)\\right| \\\\\n& =\\frac{1}{|g|}\\left|g g^{22} T_{2}^{2}+2 g g^{12} T_{1} T_{2}+g g^{11} T_{1}^{2}\\right| \\\\\n& =\\frac{|g|}{|g|}\\left|g^{i j} T_{i} T_{j}\\right|=\\left|T^{j} T_{j}\\right|=\\|\\mathbf{T}\\|^{2}\n\\end{aligned}\n$$\n\nand so $\\|\\mathbf{N}\\|=\\|\\mathbf{T}\\|=1$.\\\\\nC. $\\mathbf{N}$ is orthogonal to $\\mathbf{T}$ :\n\n$$\nN^{i} T_{i}=-\\frac{T_{2}}{\\sqrt{|g|}} T_{1}+\\frac{T_{1}}{\\sqrt{|g|}} T_{2}=0\n$$\n\nFurthermore, when $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, then $\\mathbf{N}_{0}$ is defined and is also a vector orthogonal to $\\mathbf{T}$ (by Theorem 7.1). In two dimensions this implies $\\mathbf{N}= \\pm \\mathbf{N}_{0}=\\lambda(\\delta \\mathbf{T} / \\delta s)$.\n\n7.15 For the curve and metric of Problem 7.10, determine the local normal $\\mathbf{N}_{0}$ and, using Problem 7.14, a global normal $\\mathbf{N}$. Verify that the two stand in the proper relationship.\n\nWe have $g_{11}=1, g_{12}=g_{21}=-1, g_{22}=0$ (all constants), and $\\mathbf{T}=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$; therefore, for $s \\neq 1 / 6$,\n\nand\n\n$$\n\\begin{aligned}\n& \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=\\left(\\theta^{\\prime \\prime}, 2 \\theta^{\\prime 2}+2 \\theta \\theta^{\\prime \\prime}\\right) \\\\\n& =\\left\\{\\begin{array}{lr}\n\\left(2(1-6 s)^{-4 / 3},(1-6 s)^{-2 / 3}+(1-6 s)^{-4 / 3}\\right) & 0<s<1 / 6 \\\\\n\\left(-2(6 s-1)^{-4 / 3},(6 s-1)^{-2 / 3}-(6 s-1)^{-4 / 3}\\right) & 1 / 6<s<1 / 3\n\\end{array}\\right. \\\\\n& \\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\sqrt{\\varepsilon g_{i j} \\frac{d T^{i}}{d s} \\frac{d T^{j}}{d s}}=\\left\\{\\begin{array}{l}\n2(1-6 s)^{-1} \\\\\n2(6 s-1)^{-1}\n\\end{array}\\right. \\\\\n& \\text { Thus, } \\quad \\mathbf{N}_{0}=\\frac{\\delta \\mathbf{T}}{\\delta s} /\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{1 / 3}+\\frac{1}{2}(1-6 s)^{-1 / 3}\\right) \\\\\n\\left(-(6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{1 / 3}-\\frac{1}{2}(6 s-1)^{-1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nWith $g=-1$, Problem 7.14 gives $(s \\neq 1 / 6)$ :\n\n$$\n\\begin{aligned}\n& T_{1}=g_{1 j} T^{j}=T^{1}-T^{2}=\\theta^{\\prime}(1-2 \\theta)=\\left\\{\\begin{array}{l}\n\\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3} \\\\\n\\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\n\\end{array}\\right. \\\\\n& T_{2}=g_{2 j} T^{j}=-T^{1}=-\\theta^{\\prime}=\\left\\{\\begin{array}{l}\n-(1-6 s)^{-1 / 3} \\\\\n-(6 s-1)^{-1 / 3}\n\\end{array}\\right. \\\\\n& \\mathbf{N}=\\left(-T_{2}, T_{1}\\right)=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3}\\right) \\\\\n\\left((6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nIt is seen that, as expected, $\\mathbf{N}=+\\mathbf{N}_{0}$ for $s<1 / 6$ and $\\mathbf{N}=-\\mathbf{N}_{0}$ for $s>1 / 6$. neither $\\mathbf{N}_{0}$ nor $\\mathbf{N}$ is defined at the null point $s=1 / 6$. For comparison, recall the situation in Examples 7.4 and 7.5: there the discontinuity in $\\mathbf{N}_{0}$ occurred at a regular point (the cubic has no null points under the Euclidean metric), and $\\mathbf{N}$ (either choice) was defined everywhere.\n\n7.16 Under the metric of Special Relativity (Example 7.1), a regular curve $\\mathscr{C}$ is given by\n\n$$\nx^{1}=s^{2} \\quad x^{2}=\\frac{3 s}{5} \\quad x^{3}=\\frac{4 s}{5} \\quad x^{4}=s^{2}\n$$\n\nfor $0 \\leqq s \\leqq 1$. (a) Verify that $s$ is arc length for $\\mathscr{C}$, and show that the absolute derivative, $\\delta \\mathbf{T} / \\delta s$, of $\\mathbf{T}$ is a null vector at every point of the curve (hence, a local principal normal $\\mathbf{N}_{0}$ is nowhere defined on $\\mathscr{C}$ ). Construct a global principal normal for $\\mathscr{C}$ in such a manner that the corresponding curvature function is nonzero. Is more than one curvature function possible?\n\n(a) We have $\\left(T^{i}\\right)=(2 s, 3 / 5,4 / 5,2 s)$ and\n\n$$\n\\left|g_{i j} T^{i} T^{j}\\right|=\\left|4 s^{2}+(9 / 25)+(16 / 25)-4 s^{2}\\right|=1\n$$\n\nhence, $s$ is an arc-length parameter. Also, since the $g_{i j}$ are constant, all Christoffel symbols vanish and\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=(2,0,0,2) \\quad\\left\\|\\frac{\\delta T}{\\delta s}\\right\\|=\\sqrt{\\left|2^{2}+0^{2}+0^{2}-2^{2}\\right|}=0\n$$\n\nfor all $s$.\n\n(b) Any differentiable unit vector orthogonal to $\\mathbf{T}$ will do for $\\mathbf{N}$, which then determines the curvature through (7.8). In the orthonormality conditions\n\n$$\n\\begin{aligned}\n& 2 s N^{1}+\\frac{3}{5} N^{2}+\\frac{4}{5} N^{3}-2 s N^{4}=0 \\\\\n& \\left(N^{1}\\right)^{2}+\\left(N^{2}\\right)^{2}+\\left(N^{3}\\right)^{2}-\\left(N^{4}\\right)^{2}= \\pm 1\n\\end{aligned}\n$$\n\nwe may successively set $N^{1}=N^{4}=0, N^{3}=N^{4}=0$, and $N^{2}=N^{4}=0$ to obtain three candidate normals:\n\n$$\n\\begin{gathered}\n\\mathbf{N}_{1}=\\left(0,-\\frac{4}{5}, \\frac{3}{5}, 0\\right) \\quad \\mathbf{N}_{2}=\\frac{1}{\\sqrt{(9 / 25)+4 s^{2}}}\\left(-\\frac{3}{5}, 2 s, 0,0\\right) \\\\\n\\mathbf{N}_{3}=\\frac{1}{\\sqrt{(16 / 25)+4 s^{2}}}\\left(-\\frac{4}{5}, 0,2 s, 0\\right)\n\\end{gathered}\n$$\n\nThe constant $\\mathbf{N}_{1}$ yields $\\kappa_{1} \\equiv 0$; but $\\mathbf{N}_{2}$ and $\\mathbf{N}_{3}$ yield the distinct curvature functions\n\n$$\n\\kappa_{2}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{9} s^{2}}} \\quad \\kappa_{3}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{16} s^{2}}}\n$$\n\nNote that the Frenet equation is invalid for all these normals.\n\n7.17 Refer to Problems 7.10 and 7.15. Calculate the curvature functions $\\kappa_{0}$ and $\\kappa$, and discuss the variability of $\\kappa_{0}$ over the parabolic arc $0 \\leqq s \\leqq 1 / 3$.\n\nOur previous results show that both curvatures are defined everywhere except $s=1 / 6$, with $\\kappa=\\kappa_{0}$ on $0 \\leqq s<1 / 6$ and $\\kappa=-\\kappa_{0}$ on $1 / 6<s \\leqq 1 / 3$ (cf. Problem 7.13). By Problem 7.15,\n\n$$\n\\kappa_{0}=\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\frac{2}{|1-6 s|} \\quad(s \\neq 1 / 6)\n$$\n\nwhence\n\n$$\n\\kappa=\\frac{2}{1-6 s} \\quad(s \\neq 1 / 6)\n$$\n\nIt is seen that $\\kappa_{0}$ has the same value, 2, at $s=0$ (the vertex, or point of greatest Euclidean curvature) and the undistinguished point $s=1 / 3$. Moreover, near the ordinary (from the Euclidean viewpoint) point $s=1 / 6$, the absolute curvature becomes arbitrarily large.\n\n7.18 (a) For any regular two-dimensional curve, derive the formula for absolute curvature\n\n\n\\begin{equation*}\n\\kappa_{0}=\\sqrt{|g|}\\left|T^{1} \\frac{\\delta T^{2}}{\\delta s}-T^{2} \\frac{\\delta T^{1}}{\\delta s}\\right| \\tag{7.17}\n\\end{equation*}\n\n\n(b) Use (7.17) to check Problem 7.17.\n\n(a) By (7.8) and the remarks made following Example 7.5,\n\n\n\\begin{equation*}\n\\kappa_{0}=|\\kappa|=\\left|N_{j} \\frac{\\delta T^{j}}{\\delta s}\\right|=\\left|N_{1} \\frac{\\delta T^{1}}{\\delta s}+N_{2} \\frac{\\delta T^{2}}{\\delta s}\\right| \\tag{1}\n\\end{equation*}\n\n\nChoosing the global normal $\\left(N^{i}\\right)$ established in Problem 7.14, we have:\n\n$$\n\\begin{aligned}\nN_{1} & =g_{11} N^{1}+g_{12} N^{2}=\\left(g g^{22}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(-g g^{21}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{21} T_{1}+g^{22} T_{2}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}} T^{2} \\\\\nN_{2} & =g_{21} N^{1}+g_{22} N^{2}=\\left(-g g^{12}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(g g^{11}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right) \\\\\n& =\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{11} T_{1}+g^{12} T_{2}\\right)=\\frac{\\gamma}{\\sqrt{|g|}} T^{1}\n\\end{aligned}\n$$\n\nSubstitution of these components in (1) yields (7.17).\n\n(b) For the metric of Problem 7.10,\n\n$$\nG=\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$g=\\operatorname{det} G=-1$ and absolute derivatives reduce to ordinary derivatives. Thus we can rewrite (7.17) in terms of the curve parameter $t$, as follows:\n\n$$\n\\kappa_{0}=\\left|T^{1} \\frac{d T^{2}}{d t} \\frac{d t}{d s}-T^{2} \\frac{d T^{1}}{d t} \\frac{d t}{d s}\\right|=\\frac{1}{s^{\\prime}(t)}\\left|T^{1} \\frac{d T^{2}}{d t}-T^{2} \\frac{d T^{1}}{d t}\\right|=\\frac{\\left(T^{1}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{2}}{T^{1}}\\right)\\right|\n$$\n\nSubstitution of $s^{\\prime}(t)=\\sqrt{|1-4 t|} \\quad(t \\neq 1 / 4)$ and the components of the unit tangent vector,\n\n$$\nT^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t} \\equiv \\frac{1}{s^{\\prime}(t)} \\quad T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{2 t}{s^{\\prime}(t)}\n$$\n\ngives:\n\n$$\n\\kappa_{0}=\\frac{2}{\\left(s^{\\prime}(t)\\right)^{3}}=\\frac{2}{|1-4 t|^{3 / 2}} \\quad(t \\neq 1 / 4)\n$$\n\nFrom Problem 7.12,\n\n$$\n|1-4 t|=|1-4 \\theta(s)|=|1-6 s|^{2 / 3}\n$$\n\nyielding exact agreement with Problem 7.17.\n\n7.19 Compute the absolute curvature of the logarithmic curve $\\mathscr{C}: x^{1}=t, x^{2}=a \\ln t$, for $\\frac{1}{2} \\leqq t<a$, if the Riemannian metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}\n$$\n\nAs the $g_{i j}$ are constants (with $g=-1$ ), we can proceed as in Problem 7.18(b). This time, the most convenient version of $(7.17)$ is\n\n$$\n\\kappa_{0}=\\sqrt{|g|} \\frac{\\left(T^{2}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{1}}{T^{2}}\\right)\\right|\n$$\n\nSubstituting\n\n$$\n\\begin{aligned}\n& s^{\\prime}(t)=\\sqrt{\\left|\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right|}=\\sqrt{\\left|1-\\frac{a^{2}}{t^{2}}\\right|}=\\frac{1}{t} \\sqrt{a^{2}-t^{2}}(\\neq 0) \\\\\n& T^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t}=\\frac{t}{\\sqrt{a^{2}-t^{2}}} \\\\\n& T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{a}{\\sqrt{a^{2}-t^{2}}}\n\\end{aligned}\n$$\n\nwe find: $\\kappa_{0}=a t\\left(a^{2}-t^{2}\\right)^{-3 / 2}$.\n\n\\subsection*{7.20 Prove Theorem 7.1.}\nAlong a regular curve we have\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon \\mathbf{T} \\mathbf{T}=1 \\quad \\text { or } \\quad \\mathbf{T T}=\\varepsilon\n$$\n\nwhere the indicator $\\varepsilon$ is constant, $|\\varepsilon|=1$, on the curve. By the inner-product rule for absolute differentiation, and the fact that the absolute derivative of an invariant is the ordinary derivative,\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s} \\mathbf{T}+\\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s} \\equiv 2 \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d}{d s}(\\varepsilon)=0 \\quad \\text { or } \\quad \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=0\n$$\n\n\\section*{GEODESICS}\n\\subsection*{7.21 Establish (7.12).}\nStart with the conditions\n\n\n\\begin{equation*}\nw^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy use of the product and chain rules, the expression on the right may be written\n\n$$\n-w^{-3 / 2} \\frac{d w}{d t}\\left(g_{i k} \\frac{d x^{i}}{d t}\\right)+2 w^{-1 / 2}\\left(\\frac{\\partial g_{i k}}{\\partial x^{j}} \\frac{d x^{j}}{d t}\\right) \\frac{d x^{i}}{d t}+2 w^{-1 / 2} g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nPut this back in (1), multiply both sides by $w^{1 / 2}$, and go over to the notation $g_{i j k} \\equiv \\partial g_{i j} / \\partial x^{k}$ :\n\n$$\ng_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=-w^{-1} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}+2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nwhich rearranges to\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nMaking use of the symmetry of $g_{i j}$, the third term on the left may be split into two similar terms, yielding\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{j k i} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{k i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nDivide by 2 , multiply by $g^{p k}$, and sum on $k$ :\n\n$$\n\\delta_{i}^{p} \\frac{d^{2} x^{i}}{d t^{2}}-g^{p k} \\Gamma_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{2 w} \\delta_{i}^{p} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad \\text { or } \\quad \\frac{d^{2} x^{p}}{d t^{2}}+\\Gamma_{j k}^{p} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{p}}{d t}\n$$\n\nwhich is $(7.12)$.\n\n7.22 In a Riemannian 2-space with fundamental form $\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, determine $(a)$ the regular geodesics, $(b)$ the null geodesics.\n\nHere $g_{11}=1, g_{12}=g_{21}=0, g_{22}=-\\left(x^{2}\\right)^{-2}$; Problem 6.4 gives\n\n$$\n\\Gamma_{22}^{2}=\\frac{d}{d x^{2}}\\left[\\frac{1}{2} \\ln \\left(x^{2}\\right)^{-2}\\right]=-\\frac{1}{x^{2}}\n$$\n\nas the only nonvanishing Christoffel symbol.\n\n(a) The system (7.13) becomes\n\n$$\n\\frac{d^{2} x^{1}}{d s^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d s^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=0\n$$\n\nThe first equation integrates to $x^{1}=a s+x_{0}^{1}$. In the second, let $u \\equiv d x^{2} / d s$ :\n\n$$\nu \\frac{d u}{d x^{2}}-\\frac{1}{x^{2}} u^{2}=0 \\quad \\text { or } \\quad \\frac{d u}{u}=\\frac{d x^{2}}{x^{2}} \\quad \\text { or } \\quad u=c x^{2}\n$$\n\nfrom which\n\n$$\n\\frac{d x^{2}}{d s}=c x^{2} \\quad \\text { or } \\quad \\frac{d x^{2}}{x^{2}}=c d s \\quad \\text { or } \\quad x^{2}=x_{0}^{2} e^{c s}\n$$\n\nAs our notation indicates, an arbitrary point $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ is the origin $(s=0)$ of a family of geodesics that seems to depend on two parameters, $a$ and $c$. However, $s$ must represent arc length, so that\n\n$$\n\\pm 1=\\left(\\frac{d x^{1}}{d s}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=a^{2}-c^{2}\n$$\n\nHence, either $a^{2}=c^{2}+1$ (the fundamental form is positive) or $c^{2}=a^{2}+1$ (the fundamental form is negative). Both cases may be accounted for by a single parameter, $\\lambda$, if $s$ is eliminated between the parametric equations for $x^{1}$ and $x^{2}$ :\n\nregular geodesics $\\quad x^{2}=x_{0}^{2} \\exp \\left[\\lambda\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad(|\\lambda| \\neq 1)$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-107}\n\\end{center}\n\nFig. 7-3\n\n(b) System (7.14), in $t$, becomes\n\n$$\n\\begin{gathered}\n\\frac{d^{2} x^{1}}{d t^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\\\\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0\n\\end{gathered}\n$$\n\nIt is clear that the solution may be found by formally replacing $s$ by $t$ in part ( $a$ ) and setting $a^{2}=c^{2}$. Thus, the null geodesics through $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ are given by\n\n$$\n\\text { null geodesics } \\quad x^{2}=x_{0}^{2} \\exp \\left[+\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad \\text { and } \\quad x^{2}=x_{0}^{2} \\exp \\left[-\\left(x^{1}-x_{0}^{1}\\right)\\right]\n$$\n\nNote that the null geodesics correspond to the exceptional values $\\lambda= \\pm 1$ in part (a). Figure 7-3 is a sketch of the geodesics through the point $(1,-1)$ in cartesian coordinates.\n\n7.23 Without converting to arc length, verify that in spherical coordinates, under the Euclidean metric\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2} d x^{3}\\right)^{2}\n$$\n\nany curve of the form $\\mathscr{C}: x^{1}=a \\sec t, x^{2}=t+b, x^{3}=c(a, b, c$ constant) is a geodesic. (It should be apparent that $\\mathscr{C}$ is a straight line.)\n\nThe equations (7.12) must be verified. The Christoffel symbols $\\Gamma_{j k}^{i}$ for spherical coordinates are (Problem 6.5):\n\n$$\ni=1 \\quad \\Gamma_{22}^{1}=-x^{1}, \\quad \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}\n$$\n\n$$\n\\begin{array}{lll}\ni=2 & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}, & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2} \\\\\ni=3 & \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}, & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}\n\\end{array}\n$$\n\nThe derivatives of the $x^{i}(t)$ are:\n\n$$\n\\begin{gathered}\n\\frac{d x^{1}}{d t}=a \\sec t \\tan t, \\quad \\frac{d^{2} x^{1}}{d t^{2}}=(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right) \\\\\n\\frac{d x^{2}}{d t}=1, \\quad \\frac{d^{2} x^{2}}{d t^{2}}=0 \\quad \\text { and } \\quad \\frac{d x^{3}}{d t}=\\frac{d^{2} x^{3}}{d t^{2}}=0\n\\end{gathered}\n$$\n\nWith $\\varepsilon \\equiv 1,(7.11)$ gives\n\nand\n\n$$\n\\begin{aligned}\n& w=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n&=(a \\sec t \\tan t)^{2}+(a \\sec t)^{2}(1)^{2}+0=a^{2} \\sec ^{4} t \\\\\n& \\frac{1}{2 w} \\frac{d w}{d t}=\\frac{\\left(4 a^{2} \\sec ^{3} t\\right)(\\sec t \\tan t)}{2 a^{2} \\sec ^{4} t}=2 \\tan t\n\\end{aligned}\n$$\n\nFor convenience in the verification of (7.12), let LS denote the left side, and RS the right side, of the equation in question. We obtain:\n\n$$\n\\begin{aligned}\ni=\\mathbf{1} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n& =(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right)-(a \\sec t)(1)^{2}+0=2 a \\sec t \\tan ^{2} t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{1}}{d t}=(2 \\tan t)(a \\sec t \\tan t)=2 a \\sec t \\tan ^{2} t=\\mathrm{LS} \\\\\ni=\\mathbf{2} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{2}}{d t^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\frac{2}{a \\sec t}(a \\sec t \\tan t)(1)+0=2 \\tan t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{2}}{d t}=2 \\tan t=\\mathrm{LS} \\\\\n\\boldsymbol{i}=\\mathbf{3} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{3}}{d t^{2}}+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0 \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{3}}{d t}=0=\\mathrm{LS}\n\\end{aligned}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n7.24 Determine the fundamental indicator $\\varepsilon(\\mathbf{U})$ if $\\left(U^{i}\\right)=(2 t,-2 t, 1)$ at the point $\\left(x^{i}\\right)=\\left(t^{2},-t^{2}, t\\right)$. The Riemannian metric is given by\n\n$$\n\\left(g_{i j}\\right)=\\left[\\begin{array}{ccc}\n2 x^{1} & x^{3} & 0 \\\\\nx^{3} & 2 x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right] \\quad\\left(4 x^{1} x^{2} \\neq\\left(x^{3}\\right)^{2}\\right)\n$$\n\n7.25 Find the null points of the curve $\\mathscr{C}: x^{1}=t, x^{2}=t^{4} \\quad$ ( $t$ real), if the metric is\n\n$$\n\\varepsilon d s^{2}=8\\left(x^{1} d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n\n7.26 Find the arc length of the curve in Problem 7.25 if $0 \\leqq t \\leqq 2$.\n\n7.27 Find the null points of the curve $\\mathscr{C}: x^{1}=t^{3}+1, x^{2}=t^{2}, x^{3}=t$, if the metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(x^{3} d x^{3}\\right)^{2}\n$$\n\n7.28 Find the arc length of the curve in Problem 7.27 if $\\frac{1}{2} \\leqq t \\leqq 1$.\n\n7.29 Find the angle between the curves\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=5 t \\\\\nx^{2}=2 \\\\\nx^{3}=3 t\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u \\\\\nx^{2}=2 \\\\\nx^{3}=3 u^{2} / 25\n\\end{array}\\right.\\right.\n$$\n\nat each of the points of intersection, if the fundamental form is $\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2}$.\n\n7.30 If $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2},(a)$ find the length $L$ of the curve $\\mathscr{C}: x^{1}=12 t^{2}, x^{2}=8 t^{3}$, for $0 \\leqq t \\leqq 2$. (b) Find an arc-length parameterization, $x^{i}=x^{i}(s)$, for $\\mathscr{C}$, with $s=0$ corresponding to $t=1$. (c) Show that the $x^{i}(s)$ are differentiable to all orders except at points of nullity.\n\n7.31 Find the arc length of the curve of Problem.7.30, but with the Euclidean metric.\n\n7.32 Compute $\\mathbf{T}=\\left(d x^{i} / d s\\right)$ from the arc-length parameterization found in Problem 7.30 and verify that $\\mathbf{T}$ has unit length at all points except $s=0$.\n\n7.33 Calculate the components $N^{i}$ of the unit principal normal of the curve of Problem 7.30, using (7.16) (Problem 7.14).\n\n7.34 Calculate both the curvature $\\kappa$ and the absolute curvature $\\kappa_{0}$ for the curve of Problem 7.30. Discuss the numerical behavior of $\\kappa_{0}$ along the curve.\n\n7.35 Use the formula of Problem 7.18(b) to confirm the value of $\\kappa_{0}$ found in Problem 7.34.\n\n7.36 Compute $\\kappa_{0}$ under the Euclidean metric for the curve of Problem 7.30; compare with the result obtained in Problem 7.34. For convenience, let $t=0$ correspond to $s=8$.\n\n7.37 Without calculating an arc-length parameter, find the vectors $\\mathbf{T}$ and $\\mathbf{N}$, and the curvature $\\kappa$, for the \"parabola\" $x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$ under the Riemannian metric\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n\n7.38 Show that the first-quadrant portion $\\left(x^{i}>0\\right)$ of the hypocycloid $\\mathscr{H}$ of four cusps\n\n$$\n\\left(x^{1}\\right)^{2 / 3}+\\left(x^{2}\\right)^{2 / 3}=a^{2 / 3} \\quad(a>0)\n$$\n\nmay be parameterized as $x^{1}=a \\cos ^{3} t, x^{2}=a \\sin ^{3} t$, with $0 \\leqq t \\leqq \\pi / 2$. Find the arc length under the two metrics\n\n$$\n\\text { (a) } \\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2} \\quad \\text { (b) } \\quad d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}\n$$\n\n(c) Without computing an arc-length parameter, find $\\mathbf{T}$ and $\\kappa_{0}$ for $\\mathscr{H}$ under both metrics.\n\n7.39 (a) Determine the Christoffel symbols of the second kind for the Riemannian metric $\\varepsilon d s^{2}=x^{1}\\left(d x^{1}\\right)^{2}+$ $x^{2}\\left(d x^{2}\\right)^{2}$. (b) Without converting to an arc-length parameter, verify that all curves $x^{1}=t^{2}, x^{2}=$ $\\left(a t^{3}+b\\right)^{2 / 3}$, where $a$ and $b$ are arbitrary constants, are geodesics.\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 8}", "\\section*{Riemannian Curvature}\n\\subsection*{8.1 THE RIEMANN TENSOR}\nThe Riemann tensor emerges from an analysis of a simple question. Starting with a covariant vector $\\left(V_{i}\\right)$ and taking the covariant derivative with respect to $x^{j}$ and then with respect to $x^{k}$ produces the third-order tensor\n\n$$\n\\left(\\left(V_{i}\\right)_{, j}\\right)_{, k} \\equiv\\left(V_{i, j k}\\right)\n$$\n\nDoes the order of differentiation matter, or does $V_{i, j k}=V_{i, k j}$ hold in general?\n\nStandard hypotheses concerning differentiability suffice to guarantee that the partial derivative of order two is order-independent,\n\n$$\n\\frac{\\partial^{2} V_{i}}{\\partial x^{j} \\partial x^{k}}=\\frac{\\partial^{2} V_{i}}{\\partial x^{k} \\partial x^{j}}\n$$\n\nbut due to the presence of Christoffel symbols, such hypotheses do not extend to covariant differentiation. The following formula is established in Problem 8.1:\n\nwhere\n\n\n\\begin{gather*}\nV_{j, k l}-V_{j, l k}=R_{j k l}^{i} V_{i}  \\tag{8.1}\\\\\nR_{j k l}^{i} \\equiv \\frac{\\partial \\Gamma_{j l}^{i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial x^{l}}+\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i} \\tag{8.2}\n\\end{gather*}\n\n\nThe Quotient Theorem (covariant form) immediately implies\n\nTheorem 8.1: The $n^{4}$ components defined by (8.2) are those of a fourth-order tensor, contravariant of order one, covariant of order three.\n\n$\\left(R_{j k l}^{i}\\right)$ is called the Riemann (or Riemann-Christoffel) tensor of the second kind; lowering the contravariant index produces\n\n\n\\begin{equation*}\nR_{i j k l} \\equiv g_{i r} R_{j k l}^{r} \\tag{8.3}\n\\end{equation*}\n\n\nthe Riemann tensor of the first kind.\n\nIn answer to our original question, we may now say that covariant differentiation is orderdependent unless the metric is such as to make the Riemann tensor (either kind) vanish.\n\n\\subsection*{8.2 PROPERTIES OF THE RIEMANN TENSOR}\n\\section*{Two Important Formulas}\nThe Riemann tensor of the first kind can be introduced independently via the following formula (see Problem 8.4):\n\n\n\\begin{equation*}\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r} \\tag{8.4}\n\\end{equation*}\n\n\nFrom (8.4) there follows\n\n\n\\begin{equation*}\nR_{i j k l}=\\frac{1}{2}\\left(\\frac{\\partial^{2} g_{i l}}{\\partial x^{j} \\partial x^{k}}+\\frac{\\partial^{2} g_{j k}}{\\partial x^{i} \\partial x^{l}}-\\frac{\\partial^{2} g_{i k}}{\\partial x^{j} \\partial x^{l}}-\\frac{\\partial^{2} g_{j l}}{\\partial x^{i} \\partial x^{k}}\\right)+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r} \\tag{8.5}\n\\end{equation*}\n\n\nEXAMPLE 8.1 Calculate the components $R_{i j k l}$ of the Riemann tensor for the metric of Problem 7.22,\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(d x^{2}\\right)^{2}\n$$\n\nThe nonvanishing Christoffel symbols are $\\Gamma_{22}^{2}=-\\left(x^{2}\\right)^{-1}$ and $\\Gamma_{222}=g_{22} \\Gamma_{22}^{2}=\\left(x^{2}\\right)^{-3}$. The partial-derivative terms in (8.4) vanish unless all indices are 2; but then the two terms cancel. Likewise the Christoffel-symbol terms either vanish or cancel. We conclude that all sixteen components $R_{i j k l}=0$.\n\n\\section*{Symmetry Properties}\nInterchange of $k$ and $l$ in (8.2) shows that $R_{j k l}^{i}=-R_{j l k}^{i}$, whence $R_{i j k l}=-R_{i j l k}$. This and two other symmetry properties are easily established at this point; Bianchi's (first) identity will be demonstrated in Chapter 9.\n\n\n\\begin{align*}\n\\text { first skew symmetry } & R_{i j k l}=-R_{j i k l} \\\\\n\\text { second skew symmetry } & R_{i j k l}=-R_{i j l k}  \\tag{8.6}\\\\\n\\text { block symmetry } & R_{i j k l}=R_{k l i j} \\\\\n\\text { Bianchi's identity } & R_{i j k l}+R_{i k l j}+R_{i l j k}=0\n\\end{align*}\n\n\n\\section*{Number of Independent Components}\nWe shall count the separate types of potentially nonzero components, using the above symmetry properties. The first two properties imply that $R_{a a c d}$ and $R_{a b c c}$ (not summed on $a$ or $c$ ) are zero. In the following list, we agree not to sum on repeated indices.\n\n(A) Type $R_{a b a b}, a<b: \\quad n_{A}={ }_{n} C_{2}=n(n-1) / 2$\n\n(B) Type $R_{a b a c}, b<c: \\quad n_{B}=3 \\cdot{ }_{n} C_{3}=n(n-1)(n-2) / 2$\n\n(C) Type $R_{a b c d}$ or $R_{a c b d}, a<b<c<d$ (for type $R_{a d b c}$, use Bianchi's identity): $n_{C}=2 \\cdot{ }_{n} C_{4}=$ $n(n-1)(n-2)(n-3) / 12$\n\nIn (A) the count is of combinations of $n$ numbers two at a time (for $a$ and $b$ ). In (B) one partitions the index strings into the three groups (with ${ }_{n} C_{3}$ in each group) for which\n\n$$\na<b<c \\quad b<a<c \\quad b<c<a\n$$\n\nEither subtype of (C) has as many members as there are combinations of $n$ numbers four at a time (for $a, b, c$, and $d$ ).\n\nSumming $n_{A}, n_{B}$, and $n_{C}$, we prove\n\nTheorem 8.2: There are a total of $n^{2}\\left(n^{2}-1\\right) / 12$ components of the Riemann tensor $\\left(R_{i j k l}\\right)$ that are not identically zero and that are independent from the rest.\n\nCorollary 8.3: In two-dimensional Riemannian space, the only components of the Riemann tensor not identically zero are $R_{1212}=R_{2121}=-R_{1221}=-R_{2112}$.\n\nEXAMPLE 8.2 For the metric of spherical coordinates,\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2} d x^{3}\\right)^{2}\n$$\n\nlist and calculate the nonzero components $R_{i j k l}$, if any.\n\nBy Theorem 8.2 with $n=3$, there are six potentially nonzero components:\n\n(A) $R_{1212}, R_{1313}, R_{2323}$\n\n(B) $R_{1213}, R_{1232}\\left(=R_{2123}\\right), R_{1323}\\left(=R_{3132}\\right)$\n\nBecause $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (diagonal metric tensor; no summation), we may instead compute the mixed components. From Problem 6.5,\n\n$$\n\\begin{array}{ll}\ni=1 & \\Gamma_{22}^{1}=-x^{1}, \\quad \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2} \\\\\ni=\\mathbf{2} & \\Gamma_{13}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}, \\quad \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2} \\\\\ni=\\mathbf{3} & \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}, \\quad \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}\n\\end{array}\n$$\n\nand (8.2) gives:\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=-1-\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=0 \\\\\nR_{313}^{1} & =\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{31}^{1}}{\\partial x^{1}}+\\Gamma_{33}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{31}^{r} \\Gamma_{r 3}^{1}=-\\sin ^{2} x^{2}+\\Gamma_{33}^{1} \\Gamma_{11}^{1}+\\Gamma_{31}^{3} \\Gamma_{33}^{1}=0 \\\\\nR_{323}^{2} & =\\frac{\\partial \\Gamma_{33}^{2}}{\\partial x^{2}}-\\frac{\\partial \\Gamma_{32}^{2}}{\\partial x^{3}}+\\Gamma_{33}^{r} \\Gamma_{r 2}^{2}-\\Gamma_{32}^{r} \\Gamma_{r 3}^{2}=-\\cos 2 x^{2}+\\Gamma_{33}^{1} \\Gamma_{12}^{2}-\\Gamma_{32}^{3} \\Gamma_{33}^{2}=-\\cos 2 x^{2}-\\sin ^{2} x^{2}+\\cos ^{2} x^{2}=0 \\\\\nR_{213}^{1} & =\\frac{\\partial \\Gamma_{23}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{3}}+\\Gamma_{23}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 3}^{1}=\\Gamma_{23}^{3} \\Gamma_{31}^{1}-\\Gamma_{21}^{2} \\Gamma_{23}^{1}=0 \\\\\nR_{232}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{3}}-\\frac{\\partial \\Gamma_{23}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 3}^{1}-\\Gamma_{23}^{r} \\Gamma_{r 2}^{1}=\\Gamma_{22}^{1} \\Gamma_{13}^{1}-\\Gamma_{23}^{3} \\Gamma_{32}^{1}=0 \\\\\nR_{323}^{1} & =\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{2}}-\\frac{\\partial \\Gamma_{32}^{1}}{\\partial x^{3}}+\\Gamma_{33}^{r} \\Gamma_{r 2}^{1}-\\Gamma_{22}^{r} \\Gamma_{r 3}^{1}=-2 x^{1} \\sin x^{2} \\cos x^{2}+\\Gamma_{33}^{2} \\Gamma_{22}^{1}-\\Gamma_{32}^{3} \\Gamma_{33}^{1} \\\\\n& =-2 x^{1} \\sin x^{2} \\cos x^{2}+x^{1} \\sin x^{2} \\cos x^{2}+\\left(\\cot x^{2}\\right)\\left(x^{1} \\sin ^{2} x^{2}\\right)=0\n\\end{aligned}\n$$\n\nTherefore, $R_{i j k l}=0$ for all $i, j, k, l$.\n\n\\subsection*{8.3 RIEMANNIAN CURVATURE}\nThe Riemannian (or sectional) curvature relative to a given metric $\\left(g_{i j}\\right)$ is defined for each pair of (contravariant) vectors $\\mathbf{U}=\\left(U^{i}\\right), \\mathbf{V}=\\left(V^{i}\\right)$ as\n\n\n\\begin{equation*}\n\\mathrm{K}=\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})=\\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}} \\quad\\left(G_{p q r s} \\equiv g_{p r} g_{q s}-g_{p s} g_{q r}\\right) \\tag{8.7}\n\\end{equation*}\n\n\nThis sort of curvature depends not only on position, but also on a pair of directions selected at each point (the vectors $\\mathbf{U}$ and $\\mathbf{V}$ ). By contrast, the curvature $\\kappa$ of a curve depends only on the points along the curve. Although it would seem desirable for $\\mathrm{K}$ to depend only on the points of space, to demand this would impose severe and unrealistic restrictions, as will become apparent in Chapter 9.\n\nEXAMPLE 8.3 The numerator of (8.7) is an invariant, because $\\left(R_{i j k l}\\right)$ is a tensor. As for the denominator, the identity\n\n\n\\begin{equation*}\nG_{p q r s} V_{(1)}^{p} V_{(2)}^{q} V_{(3)}^{r} V_{(4)}^{s}=\\left(\\mathbf{V}_{(1)} \\mathbf{V}_{(3)}\\right)\\left(\\mathbf{V}_{(2)} \\mathbf{V}_{(4)}\\right)-\\left(\\mathbf{V}_{(1)} \\mathbf{V}_{(4)}\\right)\\left(\\mathbf{V}_{(2)} \\mathbf{V}_{(3)}\\right) \\tag{8.8}\n\\end{equation*}\n\n\nimplies that the denominator is an invariant and proves (Lemma 4.1) that $\\left(G_{i j k l}\\right)$ is also a tensor. It follows that $\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})$ is an invariant, and thus it serves to generalize the Gaussian curvature of a surface to higher dimensions.\n\nHelpful in the calculation of $\\mathrm{K}$ is the fact that the $G_{i j k l}$ possess exactly the same symmetries as the $R_{i j k l}$ (see Problem 8.19). Moreover, if $g=\\left(g_{i j}\\right)$ is diagonal, all the nonzero $G_{i j k l}$ will be derivable from the type-A terms\n\n$$\nG_{a b a b}=g_{a a} g_{b b} \\quad(a<b ; \\text { no summation })\n$$\n\nEXAMPLE 8.4 Evaluate the Riemannian curvature at any point $\\left(x^{i}\\right)$ of Riemannian 3 -space in the directions (a) $\\mathbf{U}=(1,0,0)$ and $\\mathbf{V}=(0,1,1)$, and $(b) \\mathbf{U}=(0,1,0)$ and $\\mathbf{V}=(1,1,0)$, if the metric is given by\n\n$$\ng_{11}=1 \\quad g_{22}=2 x^{1} \\quad g_{33}=2 x^{2} \\quad g_{i j}=0 \\quad \\text { if } \\quad i \\neq j\n$$\n\nFrom Problem 6.4, the nonzero Christoffel symbols are:\n\n$$\n\\Gamma_{22}^{1}=-1 \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{2 x^{1}} \\quad \\Gamma_{33}^{2}=-\\frac{1}{2 x^{1}} \\quad \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\frac{1}{2 x^{2}}\n$$\n\nSince $n=3$, only six (by Theorem 8.2) components of the Riemann tensor need be considered: $R_{1212}, R_{1313}$, $R_{2323}, R_{1213}, R_{2123}$ and $R_{3132}$. The metric being diagonal, we compute:\n\n$$\n\\begin{gathered}\nR_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=0-0+0-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=\\frac{1}{2 x^{1}} \\\\\nR_{313}^{1}=0 \\quad R_{323}^{2}=\\frac{1}{4 x^{1} x^{2}} \\quad R_{213}^{1}=0 \\quad R_{123}^{2}=0 \\quad R_{132}^{3}=\\frac{1}{4 x^{1} x^{2}}\n\\end{gathered}\n$$\n\nwhich yield the three terms\n\n(A) $R_{1212}=g_{11} R_{212}^{1}=1 / 2 x^{1}, R_{2323}=g_{22} R_{323}^{2}=1 / 2 x^{2}$\n\n(B) $R_{3132}=g_{33} R_{132}^{3}=1 / 2 x^{1}$\n\nTheorem 8.2 also applies to the $G_{i j k l}$; but we may take the shortcut indicated in Example 8.3:\n\n(A) $G_{1212}=g_{11} g_{22}=2 x^{1}, G_{1313}=g_{11} g_{33}=2 x^{2}, \\quad G_{2323}=g_{22} g_{33}=4 x^{1} x^{2}$\n\nLet us now give an expanded form of (8.7) in the case that type-C terms are absent. It is convenient to define the $n^{4}$ functions\n\n\\[\nW_{i j k l} \\equiv\\left|\\begin{array}{cc}\nU^{i} & U^{j}  \\tag{8.9}\\\\\nV^{i} & V^{j}\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{k} & U^{l} \\\\\nV^{k} & V^{l}\n\\end{array}\\right|\n\\]\n\nof two vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$. Observe that the $W_{i j k l}$ possess all the symmetries of the $R_{i j k l}$ (or the $G_{i j k l}$ ). Looking at the numerator of (8.7), we see that a given type-A coefficient from the basic set generates, via its skew symmetries, the 4 terms\n\n$$\nR_{a b a b}\\left(U^{a} V^{b} U^{a} V^{b}-U^{b} V^{a} U^{a} V^{b}-U^{a} V^{b} U^{b} V^{a}+U^{b} V^{a} U^{b} V^{a}\\right)=R_{a b a b} W_{a b a b}\n$$\n\nand these precisely exhaust the $2 \\times 2=4$ terms in which the coefficient $R_{i j k i}$ involves the same distinct integers, $a$ and $b$, in the first pair of indices as in the second pair. A given type-B coefficient from the basic set will generate, via its skew symmetries and block symmetry, the 8 terms\n\n$$\nR_{a b a c}\\left(W_{a b a c}+W_{a c a b}\\right)=2 R_{a b a c} W_{a b a c}\n$$\n\nand these exactly correspond to the $2^{2} \\times 2=8$ ways of writing $R_{i j k l}$ such that the first and second index-pairs contain the common integer $a$ but are otherwise composed of distinct integers $b$ and $c$. Analyzing the denominator of (8.7) in the same fashion, we obtain as the desired formula:\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{\\sum_{\\text {type A }} R_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} R_{a b a c} W_{a b a c}}{\\sum_{\\text {type A }} G_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} G_{a b a c} W_{a b a c}} \\tag{8.10}\n\\end{equation*}\n\n\nIt is understood that the summation convention does not operate in (8.10); the indicated summations are over all the nonzero, independent $R_{i j k l}\\left(G_{i j k l}\\right)$, according to type. Now to the problem at hand.\n\n(a) For the data, (8.10) becomes\n\nFor\n\n$$\n\\mathrm{K}=\\frac{R_{1212} W_{1212}+R_{2323} W_{2323}+2 R_{3132} W_{3132}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}\n$$\n\nwe have\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{U} \\\\\n\\mathbf{V}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{array}\\right]\n$$\n\n$$\n\\begin{array}{ll}\nW_{1212}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right|^{2}=1 & W_{2323}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 1\n\\end{array}\\right|^{2}=0 \\\\\nW_{3132}=\\left|\\begin{array}{ll}\n0 & 1 \\\\\n1 & 0\n\\end{array}\\right|\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 1\n\\end{array}\\right|=0 & W_{1313}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right|^{2}=1\n\\end{array}\n$$\n\nand so\n\n$$\n\\mathrm{K}=\\frac{\\left(1 / 2 x^{1}\\right)(1)+\\left(1 / 2 x^{2}\\right)(0)+2\\left(1 / 2 x^{1}\\right)(0)}{\\left(2 x^{1}\\right)(1)+\\left(2 x^{2}\\right)(1)+\\left(4 x^{1} x^{2}\\right)(0)}=\\frac{1}{4 x^{1}\\left(x^{1}+x^{2}\\right)}\n$$\n\n(b) For\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{U} \\\\\n\\mathbf{V}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 0\n\\end{array}\\right]\n$$\n\nwe have\n\nwhence\n\n$$\n\\begin{array}{ll}\nW_{1212}=\\left|\\begin{array}{ll}\n0 & 1 \\\\\n1 & 1\n\\end{array}\\right|^{2}=1 & W_{2323}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n1 & 0\n\\end{array}\\right|^{2}=0 \\\\\nW_{3132}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n0 & 1\n\\end{array}\\right|\\left|\\begin{array}{ll}\n0 & 1 \\\\\n0 & 1\n\\end{array}\\right|=0 & W_{1313}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 0\n\\end{array}\\right|^{2}=0\n\\end{array}\n$$\n\n$$\n\\mathrm{K}=\\frac{\\left(1 / 2 x^{1}\\right)(1)+0+0}{\\left(2 x^{1}\\right)(1)+0+0}=\\frac{1}{4\\left(x^{1}\\right)^{2}}\n$$\n\n\\section*{Observations on the Curvature Formula}\nI. If $n=2,(8.7)$ reduces to\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}} \\equiv \\frac{R_{1212}}{g} \\tag{8.11}\n\\end{equation*}\n\n\n(see Problem 8.7). Thus, at a given point in Riemannian 2-space, the curvature is determined by the $g_{i j}$ and their derivatives, and is independent of the directions $\\mathbf{U}$ and $\\mathbf{V}$.\n\nII. The extension of (8.10) to include type-C terms is as follows:\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{\\sum_{\\text {type A }} R_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} R_{a b a c} W_{a b a c}+2 \\sum_{\\text {type C }} R_{a b c d}\\left(W_{a b c d}-W_{a d b c}\\right)+2 \\sum_{\\text {type C }} R_{a c b d}\\left(W_{a c b d}-W_{a d c b}\\right)}{\\sum_{\\text {type A }} G_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} G_{a b a c} W_{a b a c}+2 \\sum_{\\text {type C }} G_{a b c d}\\left(W_{a b c d}-W_{a d b c}\\right)+2 \\sum_{\\text {type C }} G_{a c b d}\\left(W_{a c b d}-W_{a d c b}\\right)} \\tag{8.12}\n\\end{equation*}\n\n\n(see Problem 8.9).\n\nIII. If linearly independent $\\mathbf{U}$ and $\\mathbf{V}$ are replaced by independent linear combinations of themselves, the curvature is unaffected; i.e.,\n\n\n\\begin{equation*}\n\\mathrm{K}(\\mathbf{x} ; \\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V})=\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V}) \\tag{8.13}\n\\end{equation*}\n\n\nTherefore, at a given point $\\mathbf{x}$, the curvature will have a value, not for each pair of vectors $\\mathbf{U}$ and $\\mathbf{V}$, but for each 2-flat passing through $\\mathbf{x}$.\n\n\\section*{Isotropic Points}\nIf the Riemannian curvature at $\\mathbf{x}$ does not change with the orientation of a 2 -flat through $\\mathbf{x}$, then $\\mathbf{x}$ is called isotropic. From (8.11), we have\n\nTheorem 8.4: All points of a two-dimensional Riemannian space are isotropic.\n\nIt is not immediately clear whether any metric ( $g_{i j}$ ) could lead to isotropic points in $\\mathbf{R}^{n}, n \\geqq 3$. But such is the case. Indeed, as is shown in Problem $8.12, \\mathbf{R}^{3}$ under a hyperbolic metric is isotropic at any point.\n\n\\subsection*{8.4 THE RICCI TENSOR}\nA brief look will be given a tensor that is of importance in Relativity. The Ricci tensor of the first kind is defined as a contraction of the Riemann tensor of the second kind:\n\n\n\\begin{equation*}\nR_{i j} \\equiv R_{i j k}^{k}=\\frac{\\partial \\Gamma_{i k}^{k}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{k}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{k}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{k} \\tag{8.14}\n\\end{equation*}\n\n\nRaising an index yields the Ricci tensor of the second kind:\n\n\n\\begin{equation*}\nR_{j}^{i} \\equiv g^{i k} R_{k j} \\tag{8.15}\n\\end{equation*}\n\n\nBy use of the following simple consequence of Laplace's expansion (2.5):\n\nLemma 8.5: Let $A=\\left[a_{i j}(\\mathbf{x})\\right]_{n n}$ be a nonsingular matrix of multivariate functions, with inverse $B=\\left[b_{i j}(\\mathbf{x})\\right]_{n n}$. Then\n\n$$\n\\frac{\\partial}{\\partial x^{i}}(\\ln |\\operatorname{det} A|) \\equiv \\frac{1}{\\operatorname{det} A} \\frac{\\partial}{\\partial x^{i}}(\\operatorname{det} A)=b_{s r} \\frac{\\partial a_{r s}}{\\partial x^{i}}\n$$\n\nthe definition (8.14) may be put into a form (Problem 8.14) that makes evident the symmetry of the $R_{i j}$.\n\n\n\\begin{equation*}\nR_{i j}=\\frac{\\partial^{2}}{\\partial x^{i} \\partial x^{j}}(\\ln \\sqrt{|g|})-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{r}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{r}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s} \\tag{8.16}\n\\end{equation*}\n\n\nHere, as always, $g \\equiv \\operatorname{det} G$.\n\nTheorem 8.6: The Ricci tensor is symmetric.\n\nAfter raising a subscript to define the Ricci tensor of the second kind, $R_{j}^{i}=g^{i s} R_{s j}$, and then contracting on the remaining pair of indices, the important invariant $R \\equiv R_{i}^{i}$ results, called the Ricci (or scalar) curvature. By (8.16),\n\n\n\\begin{equation*}\nR=g^{i j}\\left[\\frac{\\partial^{2}}{\\partial x^{i} \\partial x^{j}}(\\ln \\sqrt{|g|})-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{r}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{r}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}\\right] \\tag{8.17}\n\\end{equation*}\n\n\n\\section*{Solved Problems}\n\\section*{THE RIEMANN TENSOR}\n\\subsection*{8.1 Prove (8.1).}\nBy definition of the covariant derivative,\n\n\n\\begin{equation*}\nV_{i, j k}=\\left(V_{i, j}\\right)_{, k}=\\frac{\\partial}{\\partial x^{k}}\\left(V_{i, j}\\right)-\\Gamma_{i k}^{r}\\left(V_{r, j}\\right)-\\Gamma_{j k}^{r}\\left(V_{i, r}\\right) \\tag{1}\n\\end{equation*}\n\n\nSubstitute\n\n$$\nV_{i, j}=\\frac{\\partial V_{i}}{\\partial x^{j}}-\\Gamma_{i j}^{s} V_{s}\n$$\n\nin (1), carry out the differentiation, and remove parentheses:\n\n\n\\begin{equation*}\nV_{i, j k}=\\frac{\\partial^{2} V_{i}}{\\partial x^{k} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}-\\Gamma_{i j}^{s} \\frac{\\partial V_{s}}{\\partial x^{k}}-\\Gamma_{i k}^{r} \\frac{\\partial V_{r}}{\\partial x^{j}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}-\\Gamma_{j k}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{j k}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{2}\n\\end{equation*}\n\n\nInterchanging $j$ and $k$ yields\n\n\n\\begin{equation*}\nV_{i, k j}=\\frac{\\partial^{2} V_{i}}{\\partial x^{j} \\partial x^{k}}-\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i k}^{s} \\frac{\\partial V_{s}}{\\partial x^{j}}-\\Gamma_{i j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}+\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s}-\\Gamma_{k j}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{k j}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{3}\n\\end{equation*}\n\n\nSubtracting (3) from (2), one sees that the first, third, fourth, sixth, and seventh terms on the right of (2) cancel with the first, fourth, third, sixth, and seventh terms on the right of (3), leaving\n\n$$\n\\begin{aligned}\nV_{i, j k}-V_{i, k j} & =-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}+\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s} \\\\\n& =\\left(\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s}\\right) V_{s}=R_{i j k}^{s} V_{s}\n\\end{aligned}\n$$\n\n8.2 Show that at any point where the Christoffel symbols vanish,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=0\n$$\n\nIn this case the expression for $R_{j k l}^{i}$ reduces to just $\\partial \\Gamma_{j l}^{i} / \\partial x^{k}-\\partial \\Gamma_{j k}^{i} / \\partial x^{l}$. Therefore,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=\\frac{\\partial \\Gamma_{j l}^{i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial x^{l}}+\\frac{\\partial \\Gamma_{k j}^{i}}{\\partial x^{l}}-\\frac{\\partial \\Gamma_{k l}^{i}}{\\partial x^{j}}+\\frac{\\partial \\Gamma_{l k}^{i}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{l j}^{i}}{\\partial x^{k}}\n$$\n\nAs all the terms cancel, the desired relationship is proved.\n\n8.3 Prove that for an arbitrary second-order covariant tensor $\\left(T_{i j}\\right)$\n\n$$\nT_{i j, k l}-T_{i j, l k}=R_{i k l}^{s} T_{s j}+R_{j k l}^{s} T_{i s}\n$$\n\n(The general formula,\n\n\n\\begin{equation*}\nT_{i_{1} i_{2} \\ldots i_{p}, k l}-T_{i_{1} i_{2} \\ldots i_{p}, l k}=\\sum_{q=1}^{p} R_{i_{q} k l}^{s} T_{i_{1} \\ldots i_{q-1} s i_{q+1} \\ldots i_{p}} \\tag{8.18}\n\\end{equation*}\n\n\nwhich is credited to Ricci, is similarly established.)\n\nA direct approach would be quite tedious; instead, first establish that\n\n\n\\begin{equation*}\nV_{, j k}^{i}-V_{, k j}^{i}=-R_{s j k}^{i} V^{s} \\tag{1}\n\\end{equation*}\n\n\nfor any contravariant vector $\\left(V^{i}\\right)$ (see Problem 8.16). Now observe that $\\left(V^{q} T_{i q}\\right)$ is a covariant vector, to which (8.1) applies. Thus,\n\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, k l}-\\left(V^{q} T_{i q}\\right)_{, l k}=R_{i k l}^{s} V^{q} T_{s q} \\tag{2}\n\\end{equation*}\n\n\nBy the inner-product rule for covariant differentiation,\n\n\n\\begin{align*}\n& \\left(V^{q} T_{i q}\\right)_{, k}=V_{, k}^{q} T_{i q}+V^{q} T_{i q, k}  \\tag{3}\\\\\n& \\left(V^{q} T_{i q}\\right)_{, k l}=V_{, k l}^{q} T_{i q}+V_{, k}^{q} T_{i q, l}+V_{, l}^{q} T_{i q, k}+V^{q} T_{i q, k l}\n\\end{align*}\n\n\nInterchange $k$ and $l$ :\n\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, l k}=V_{, l k}^{q} T_{i q}+V_{, l}^{q} T_{i q, k}+V_{, k}^{q} T_{i q, l}+V^{q} T_{i q, l k} \\tag{4}\n\\end{equation*}\n\n\nSubtraction of (4) from (3) will cancel the middle two terms on the right-hand sides, leaving\n\n\n\\begin{equation*}\nR_{i k l}^{s} V^{q} T_{s q}=\\left(V_{, k l}^{q}-V_{, l k}^{q}\\right) T_{i q}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q} \\tag{5}\n\\end{equation*}\n\n\nNow use (1) in the right member of (5):\n\n$$\nR_{i k l}^{s} V^{q} T_{s q}=-R_{q k l}^{s} V^{q} T_{i s}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q}\n$$\n\nwhich may be rearranged into\n\n$$\n\\left[\\left(T_{i q, k l}-T_{i q, l k}\\right)-\\left(R_{i k l}^{s} T_{s q}+R_{q k l}^{s} T_{i s}\\right)\\right] V^{q}=0\n$$\n\nBut $\\left(V^{i}\\right)$ is arbitrary, so the bracketed expression must vanish. QED\n\n\\section*{PROPERTIES OF THE RIEMANN TENSOR}\n8.4 Establish (8.4).\n\nBy definition,\n\n$$\n\\begin{aligned}\nR_{i j k l} & =g_{i s} R_{j k l}^{s}=g_{i s} \\frac{\\partial \\Gamma_{j l}^{s}}{\\partial x^{k}}-g_{i s} \\frac{\\partial \\Gamma_{j k}^{s}}{\\partial x^{l}}+g_{i s} \\Gamma_{j l}^{r} \\Gamma_{r k}^{s}-g_{i s} \\Gamma_{j k}^{r} \\Gamma_{r l}^{s} \\\\\n& =\\frac{\\partial\\left(g_{i s} \\Gamma_{j l}^{s}\\right)}{\\partial x^{k}}-\\frac{\\partial g_{i s}}{\\partial x^{k}} \\Gamma_{j l}^{s}-\\frac{\\partial\\left(g_{i s} \\Gamma_{j k}^{s}\\right)}{\\partial x^{l}}+\\frac{\\partial g_{i s}}{\\partial x^{l}} \\Gamma_{j k}^{s}+\\Gamma_{j l}^{r} \\Gamma_{r k i}-\\Gamma_{j k}^{r} \\Gamma_{r l i}\n\\end{aligned}\n$$\n\n$$\n=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{j k}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{r l i}\\right)-\\Gamma_{j l}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{k}}-\\Gamma_{r k i}\\right)\n$$\n\nRecall from (6.2) that for arbitrary index $l$,\n\n$$\n\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{l r i}=\\Gamma_{i l r}\n$$\n\nBy substitution,\n\n$$\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r}\n$$\n\n8.5 Establish the first skew-symmetry property, $R_{i j k l}=-R_{j i k l}$.\n\nTo save writing, let\n\n$$\nG_{k l}^{i j} \\equiv \\frac{1}{2}\\left(\\frac{\\partial^{2} g_{i j}}{\\partial x^{k} \\partial x^{l}}+\\frac{\\partial^{2} g_{k l}}{\\partial x^{i} \\partial x^{j}}\\right) \\quad \\text { and } \\quad H_{k l}^{i j} \\equiv \\Gamma_{i j r} \\Gamma_{k l}^{r}\n$$\n\nNote the obvious symmetry properties\n\n$$\nG_{k l}^{i j}=G_{k l}^{j i}=G_{l k}^{i j} \\quad \\text { and } \\quad H_{k l}^{i j}=H_{k l}^{j i}=H_{l k}^{i j}\n$$\n\nAlso, it is clear that $G_{k l}^{i j}=G_{i j}^{k l}$; furthermore,\n\n$$\nH_{k l}^{i j}=\\left(g_{r s} \\Gamma_{i j}^{s}\\right) \\Gamma_{k l}^{r}=\\Gamma_{i j}^{s}\\left(g_{s r} \\Gamma_{k l}^{r}\\right)=\\Gamma_{i j}^{s} \\Gamma_{k l s}=H_{i j}^{k l}\n$$\n\nNow, by (8.5),\n\nand\n\n$$\n\\begin{aligned}\n& R_{i j k l}=G_{j k}^{i l}-G_{j l}^{i k}+H_{j k}^{i l}-H_{j l}^{i k} \\\\\n& R_{j i k l}=G_{i k}^{j l}-G_{i l}^{j k}+H_{i k}^{j l}-H_{i l}^{j k}=G_{j l}^{i k}-G_{j k}^{i l}+H_{j l}^{i k}-H_{j k}^{i l}=-R_{i j k l}\n\\end{aligned}\n$$\n\n8.6 List the independent, potentially nonzero components of $R_{i j k l}$ for $n=5$ and verify the formula of Theorem 8.2 in this case.\n\nType A: $R_{1212}, R_{1313}, R_{1414}, R_{1515}$\n\n$R_{2323}, R_{2424}, R_{2525}$\n\n$R_{3434}, R_{3535}$\n\n$R_{4545}$\n\nType B: $R_{1213}, R_{1214}, R_{1215}, R_{1314}, R_{1315}, R_{1415}$\n\n$R_{2123}, R_{2124}, R_{2125}, R_{2324}, R_{2325}, R_{2425}$\n\n$R_{3132}, R_{3134}, R_{3135}, R_{3234}, R_{3235}, R_{3435}$\n\n$R_{4142}, R_{4143}, R_{4145}, R_{4243}, R_{4245}, R_{4345}$\n\n$R_{5152}, R_{5153}, R_{5154}, R_{5253}, R_{5254}, R_{5354}$\n\nType C: $R_{1234}, R_{1235}, R_{1245}, R_{1345}, R_{2345}$\n\n$R_{1324}, R_{1325}, R_{1425}, R_{1435}, R_{2435}$\n\nThere are 10 components of types $\\mathrm{A}$ and $\\mathrm{C}$ each, and 30 of type B; or 50 altogether. From the formula,\n\n$$\n\\frac{n^{2}\\left(n^{2}-1\\right)}{12}=\\frac{5^{2}\\left(5^{2}-1\\right)}{12}=\\frac{(25)(24)}{12}=50\n$$\n\n\\section*{RIEMANNIAN CURVATURE}\n8.7 Prove (8.11).\n\nBy Corollary 8.3 and the corresponding result for the $G_{i j k l}$,\n\n$$\n\\mathrm{K}=\\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}}=\\frac{R_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}{G_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}=\\frac{R_{1212}}{G_{1212}}=\\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}}\n$$\n\n8.8 Calculate $\\mathrm{K}$ for the Riemannian metric $\\varepsilon d s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, using the result of Problem 8.7. 6.4 ,\n\nWe have only to calculate $R_{1212}=g_{11} R_{212}^{1}$. The nonvanishing Christoffel symbols are, by Problem\n\n$$\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}}\n$$\n\nConsequently,\n\nand\n\n$$\n\\begin{gathered}\nR_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n=\\frac{1}{\\left(x^{1}\\right)^{2}}-\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right)\\left(-\\frac{1}{x^{1}}\\right)=\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\n\\mathrm{~K}=\\frac{g_{11} R_{212}^{1}}{g_{11} g_{22}}=\\frac{R_{212}^{1}}{g_{22}}=\\frac{\\left(x^{1}\\right)^{-2}}{-\\left(x^{1}\\right)^{-2}}=-1 .\n\\end{gathered}\n$$\n\n8.9 Derive the form (8.12) of the curvature equation.\n\nWe need only establish the summations over the type-C terms in the numerator; the rest of the work was done in Example 8.4.\n\nFirst of all, let us verify that all $R_{i j k l}$ with $i j k l$ a permutation of $a b c d$, where $a<b<c<d$ are distinct integers, are generated by the skew and block symmetries of the three components $R_{a b c d}, R_{a c b d}$, and $R_{a d b c}$. Examination of Table 8-1, which uses an obvious notation for the symmetry operators, shows that all $4 !=24$ permutations are accounted for. Consequently, the type-C part of the numerator of (8.12) is [cf. the equation preceding $(8.10)$ ]\n\n\n\\begin{equation*}\n2 \\sum R_{a b c d} W_{a b c d}+2 \\sum R_{a c b d} W_{a c b d}+2 \\sum R_{a d b c} W_{a d b c} \\tag{1}\n\\end{equation*}\n\n\nTable 8-1\n\n\\begin{center}\n\\begin{tabular}{|c|l|l|l|}\n\\hline\n\\multirow{2}{*}{}\\begin{tabular}{c}\nSymmetry \\\\\nOperator \\\\\n\\end{tabular} & \\multicolumn{3}{|c|}{Subscript Chain} \\\\\n\\cline { 2 - 4 }\n & $\\boldsymbol{a b c d}$ & $\\boldsymbol{a} \\boldsymbol{b} \\boldsymbol{d}$ & $\\boldsymbol{a d b} \\boldsymbol{c}$ \\\\\n\\hline\n$\\mathrm{I}$ & $a b c d$ & $a c b d$ & $a d b c$ \\\\\n$\\mathrm{~S}_{1}$ & $b a c d$ & $c a b d$ & $d a b c$ \\\\\n$\\mathrm{~S}_{2}$ & $a b d c$ & $a c d b$ & $a d c b$ \\\\\n$\\mathrm{~S}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{2} \\mathrm{~S}_{1}$ & $b a d c$ & $c a d b$ & $d a c b$ \\\\\n\\hline\n$\\mathrm{B}$ & $c d a b$ & $b \\bar{d} a c$ & $b \\bar{b} a \\bar{d}$ \\\\\n$\\mathrm{BS}_{1}=\\mathrm{S}_{2} \\mathrm{~B}$ & $c d b a$ & $b d c a$ & $b c d a$ \\\\\n$\\mathrm{BS}_{2}=\\mathrm{S}_{1} \\mathrm{~B}$ & $d c a b$ & $d b a c$ & $c b a d$ \\\\\n$\\mathrm{BS}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{1} \\mathrm{~S}_{2} \\mathrm{~B}$ & $d c b a$ & $d b c a$ & $c b d a$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nThe first summation is over all $a<b<c<d$ that yield a nonzero $R_{a b c d}$ in the basic set; similarly for the second summation. The third summation does not involve the basic set, but the symmetries of $R_{i j k l}$ (shared by $W_{i j k l}$ ) allow its absorption in the first two summations. Thus, by Bianchi's identity,\n\n\n\\begin{equation*}\n2 R_{a d b c} W_{a d b c}=2\\left(-R_{a b c d}-R_{a c d b}\\right) W_{a d b c}=-2 R_{a b c d} W_{a d b c}-2 R_{a c b d} W_{a d c b} \\tag{2}\n\\end{equation*}\n\n\nand substitution of (2) in (1) produces the expression given in (8.12).\n\n8.10 Prove (8.13).\n\nWe have\n\n$$\n\\begin{aligned}\nW_{i j k l}(\\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V}) & =\\left|\\begin{array}{cc}\n\\lambda U^{i}+\\nu V^{i} & \\lambda U^{j}+\\nu V^{j} \\\\\n\\mu U^{i}+\\omega V^{i} & \\mu U^{j}+\\omega V^{j}\n\\end{array}\\right|\\left|\\begin{array}{cc}\n\\lambda U^{k}+\\nu V^{k} & \\lambda U^{l}+\\nu V^{l} \\\\\n\\mu U^{k}+\\omega V^{k} & \\mu U^{l}+\\omega V^{l}\n\\end{array}\\right| \\\\\n& =\\left|\\begin{array}{cc}\n\\lambda & \\nu \\\\\n\\mu & \\omega\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{i} & U^{j} \\\\\nV^{i} & V^{j}\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{k} & U^{l} \\\\\nV^{k} & V^{l}\n\\end{array}\\right|=(\\lambda \\omega-\\nu \\mu)^{2} W_{i j k l}(\\mathbf{U}, \\mathbf{V})\n\\end{aligned}\n$$\n\nso that the quantity $(\\lambda \\omega-\\nu \\mu)^{2}$ factors out of all terms in (8.12) for $\\mathrm{K}(\\mathbf{x} ; \\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V})$, leaving $\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})$.\n\n8.11 Find the isotropic points in the Riemannian space $\\mathbf{R}^{3}$ with metric\n\n$$\ng_{11}=1 \\quad g_{22}=g_{33}=\\left(x^{1}\\right)^{2}+1 \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nand calculate the curvature $\\mathrm{K}$ at those points.\n\nFollow Example 8.4. By Problem 6.4, the nonzero Christoffel symbols are\n\n$$\n\\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{33}^{1}=-x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\n$$\n\nThen:\n\nwhich give\n\n$$\n\\begin{aligned}\n& R_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{313}^{1}=\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{1}}+\\Gamma_{33}^{1} \\Gamma_{11}^{1}-\\Gamma_{31}^{3} \\Gamma_{33}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{323}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}=-x^{1} \\cdot \\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}=-\\frac{\\left(x^{1}\\right)^{2}}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{aligned}\n$$\n\n(A) $\\quad R_{1212}=g_{11} R_{212}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{1313}=g_{11} R_{313}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{2323}=g_{22} R_{323}^{2}=-\\left(x^{1}\\right)^{2}$\n\nThe corresponding terms for the denominator of $(8.10)$ are\n\n\n\\begin{align*}\nG_{1212}=g_{11} g_{22} & =\\left(x^{1}\\right)^{2}+1, \\quad G_{1313}=g_{11} g_{33}=\\left(x^{1}\\right)^{2}+1, \\quad G_{2323}=g_{22} g_{33}=\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2}  \\tag{A}\\\\\n\\mathrm{~K} & =\\frac{-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1212}-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1313}-\\left(x^{1}\\right)^{2} W_{2323}}{\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1212}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2} W_{2323}} \\\\\n& =-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-2} \\frac{W_{1212}+W_{1313}+\\left(x^{1}\\right)^{2}\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}{W_{1212}+W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}\n\\end{align*}\n\n\nIf $\\mathrm{K}$ is to be independent of the $W_{i j k l}$ (which vary with ine direction of the 2 -flat), then $\\left(x^{1}\\right)^{2}=1$, or $x^{1}= \\pm 1$. Therefore, the isotropic points compose two surfaces, on which the curvature has the value $\\mathrm{K}=-[1+1]^{-2} \\cdot 1=-1 / 4$.\n\n8.12 Show that every point of $\\mathbf{R}^{3}$ is isotropic for the metric\n\n$$\nd s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{3}\\right)^{2}\n$$\n\nProblem 6.4 gives as the nonvanishing Christoffel symbols:\n\n$$\n\\begin{gathered}\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=\\frac{1}{x^{1}} \\quad \\Gamma_{33}^{1}=\\frac{1}{x^{1}} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=-\\frac{1}{x^{1}}\n\\end{gathered}\n$$\n\nAs in earlier problems, we proceed to calculate a basic set of $R_{i j k l}$, via $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (no sum).\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=-\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& =-\\frac{1}{\\left(x^{1}\\right)^{2}}+\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right) \\frac{1}{x^{1}}=-\\frac{1}{\\left(x^{1}\\right)^{2}}\n\\end{aligned}\n$$\n\nSimilarly, $R_{313}^{1}=-1 /\\left(x^{1}\\right)^{2}$. For the remainder, the partial-derivative terms all drop out, yielding\n\n$$\n\\begin{gathered}\nR_{323}^{2}=\\Gamma_{33}^{r} \\Gamma_{r 2}^{2}-\\Gamma_{32}^{r} \\Gamma_{r 3}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}-0=-\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\nR_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{gathered}\n$$\n\nOur basic set is thus\n\n(A) $R_{1212}=R_{1313}=R_{2323}=-1 /\\left(x^{1}\\right)^{4}$\n\nand, by Example 8.3,\n\n(A) $G_{1212}=G_{1313}=G_{2323}=1 /\\left(x^{1}\\right)^{4}$\n\nis a basic set of $G_{i j k l}$. Formula (8.10) or (8.12) now gives\n\n$$\n\\mathrm{K}=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}=\\frac{\\left[-\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}{\\left[\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}=-1\n$$\n\nIt is seen that this Riemannian space is more than just isotropic; it is a space of constant curvature.\n\n\\section*{THE RICCI TENSOR}\n8.13 For the metric of Example 8.4, calculate (a) $R_{i j},(b) R_{j}^{i}$, (c) $R$.\n\n(a) From $R_{i j}=R_{i j k}^{k}=R_{i j 1}^{1}+R_{i j 2}^{2}+R_{i j 3}^{3}$ and the fact that $g_{i j}=0$ for $i \\neq j$, it follows that\n\n\n\\begin{equation*}\nR_{i j}=g^{11} R_{1 i j 1}+g^{22} R_{2 i j 2}+g^{33} R_{3 i j 3} \\tag{1}\n\\end{equation*}\n\n\nwhere $g^{11}=1, g^{22}=1 / 2 x^{1}, g^{33}=1 / 2 x^{2}$. Now, a basic set of the $R_{i j k l}$ was computed as\n\n$$\nR_{1221}\\left(=-R_{1212}\\right)=-\\frac{1}{2 x^{1}} \\quad R_{2332}\\left(=-R_{2323}\\right)=-\\frac{1}{2 x^{2}} \\quad R_{3123}\\left(=-R_{3132}\\right)=-\\frac{1}{2 x^{1}}\n$$\n\nand the only other nonzero components of the form $R_{\\text {aija }}$ generated by these are\n\n$$\nR_{2112}=-\\frac{1}{2 x^{1}} \\quad R_{3223}=-\\frac{1}{2 x^{2}} \\quad R_{3213}=-\\frac{1}{2 x^{1}}\n$$\n\nHence, the nonzero $R_{i j}$ may be read off from (1) as\n\n\n\\begin{gather*}\nR_{11}=g^{22} R_{2112}=-\\frac{1}{4\\left(x^{1}\\right)^{2}} \\\\\nR_{22}=g^{11} R_{1221}+g^{33} R_{3223}=-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}} \\\\\nR_{33}=g^{22} R_{2332}=-\\frac{1}{4 x^{1} x^{2}} \\\\\nR_{12}=g^{33} R_{3123}=-\\frac{1}{4 x^{1} x^{2}}=g^{33} R_{3213}=R_{21} \\\\\nR_{j}^{i}=g^{i k} R_{k j}=g^{i i} R_{i j} \\quad \\text { (no summation on } i \\text { ) }  \\tag{b}\\\\\n=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33} \\\\\n=(1)\\left[-\\frac{1}{4\\left(x^{1}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{1}}\\right)\\left[-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{2}}\\right)\\left(-\\frac{1}{4 x^{1} x^{2}}\\right)=-\\frac{x^{1}+2\\left(x^{2}\\right)^{2}}{\\left(2 x^{1} x^{2}\\right)^{2}}\n\\end{gather*}\n\n\n$$\nR=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33}\n$$\n\n8.14 Derive (8.16) from (8.14).\n\nFormula (8.14) involves two summations of the form $\\Gamma_{i s}^{s}$. By (6.4) and (6.1b),\n\n$$\n\\begin{aligned}\n\\Gamma_{i s}^{s} & =g^{s r} \\Gamma_{i s r}=\\frac{1}{2} g^{s r}\\left(-g_{i s r}+g_{s r i}+g_{r i s}\\right)=-\\frac{1}{2} g^{s r} g_{s i r}+\\frac{1}{2} g^{s r} g_{s r i}+\\frac{1}{2} g^{r s} g_{s i r} \\\\\n& =\\frac{1}{2} g^{s r} g_{r s i} \\equiv \\frac{1}{2} g^{s r} \\frac{\\partial g_{r s}}{\\partial x^{i}}=\\frac{\\partial}{\\partial x^{i}}(\\ln \\sqrt{|g|})\n\\end{aligned}\n$$\n\nwhere Lemma 8.5 was used in the last step. Now substitute in (8.14):\n\n$$\n\\begin{aligned}\nR_{i j} & =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\frac{\\partial(\\ln \\sqrt{|g|})}{\\partial x^{r}} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\left(\\frac{1}{\\sqrt{|g|}} \\sqrt{|g|} \\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\frac{1}{\\sqrt{|g|}} \\frac{\\partial(\\sqrt{|g|})}{\\partial x^{s}} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{s}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}\n\\end{aligned}\n$$\n\n\\section*{Supplementary Problems}\n8.15 The absolute partial derivatives of a tensor $\\mathbf{T}=\\left(T_{j}^{i} \\ldots\\right)$ defined on a 2-manifold $\\mathscr{M}: x^{i}=x^{i}(u, v)$ are defined as\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta u} \\equiv\\left(T_{j}^{i} \\ldots, k, \\frac{\\partial x^{k}}{\\partial u}\\right) \\quad \\text { and } \\quad \\frac{\\delta \\mathbf{T}}{\\delta v} \\equiv\\left(T_{j}^{i} \\ldots, \\frac{\\partial x^{k}}{\\partial v}\\right)^{\\dot{*}}\n$$\n\nSince $\\left(\\partial x^{i} / \\partial u\\right)$ and $\\left(\\partial x^{i} / \\partial v\\right)$ are vectors, the inner products produce a pair of tensors of the same type and order as $\\mathbf{T}$; thus the operation of absolute partial differentiation may be repeated indefinitely. Prove that if $\\left(V^{i}\\right)$ is any contravariant vector defined on $\\mathcal{M}$,\n\n$$\n\\frac{\\delta}{\\delta u}\\left(\\frac{\\delta V^{i}}{\\delta v}\\right)-\\frac{\\delta}{\\delta v}\\left(\\frac{\\delta V^{i}}{\\delta u}\\right)=R_{s k l}^{i} V^{s} \\frac{\\partial x^{k}}{\\partial u} \\frac{\\partial x^{l}}{\\partial v}\n$$\n\n[Hint: Expand the left side and use Problem 8.16.]\n\n8.16 Prove that for any vector $\\left(V^{i}\\right), V_{, k l}^{i}-V_{, l k}^{i}=-R_{s k l}^{i} V^{s}$.\n\n8.17 For an arbitrary second-order contravariant tensor $\\left(T^{i j}\\right)$, show that\n\n$$\nT_{, k l}^{i j}-T_{, l k}^{i j}=-R_{s k l}^{i} T^{s j}-R_{s k l}^{j} T^{i s}\n$$\n\n[Hint: Lower superscripts and use Problem 8.3.]\n\n8.18 For an arbitrary mixed tensor $\\left(T_{j}^{i}\\right)$ show that\n\n$$\nT_{j, k l}^{i}-T_{j, l k}^{i}=-R_{s k l}^{i} T_{j}^{s}+R_{j k l}^{s} T_{s}^{i}\n$$\n\n8.19 Verify the symmetry properties (8.6) for the $G_{i j k l}[\\operatorname{see}(8.7)]$ and for the $W_{i j k l}[\\operatorname{see}(8.9)]$.\n\n8.20 Derive (8.5) from (8.4). [Hint: It is helpful to adopt the notation $g_{i j k l}$ for $\\partial^{2} g_{i j} / \\partial x^{k} \\partial x^{l}$.]\n\n8.21 List the independent (nonzero) components of $R_{i j k i}$ when $n=4$ and verify Theorem 8.2 for this case.\n\n8.22 Calculate the Riemannian curvature K for the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 x^{1}\\left(d x^{2}\\right)^{2}$.\n\n8.23 Confirm that $\\mathrm{K}=0$ for the Euclidean metric of polar coordinates,\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}\n$$\n\n$(a)$ by a calculation; $(b)$ by noting that $\\mathrm{K}$ is an invariant.\n\n8.24 Rework Example 8.4 for the pairs $(a) \\mathbf{U}_{(1)}=(1,0,1), \\mathbf{V}_{(1)}=(1,1,1)$ and $(b) \\mathbf{U}_{(2)}=(0,1,0), \\mathbf{V}_{(2)}=$ $(2,1,2)$. (c) Explain why the answers should be the same for $(a)$ and $(b)$.\n\n8.25 Let the surface of the 3 -sphere of radius $a$ be metrized by setting $x^{1}=a$ in spherical coordinates and then allowing $x^{1}, x^{2}$ to replace $x^{2}, x^{3}$, respectively:\n\n$$\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a \\sin x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}\n$$\n\nDetermine $\\mathrm{K}$ for this non-Euclidean $\\mathbf{R}^{2}$.\n\n8.26 If the metric for Riemannian $\\mathbf{R}^{3}$ is given by\n\n$$\ng_{11}=f\\left(x^{2}\\right) \\quad g_{22}=g\\left(x^{2}\\right) \\quad g_{33}=h\\left(x^{2}\\right)\n$$\n\nand $g_{i j}=0$ for $i \\neq j$, write explicit formulas for $(a) \\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right),(b) R$.\n\n8.27 Specialize the results of Problem 8.26 to the case $f\\left(x^{2}\\right) \\equiv g\\left(x^{2}\\right) \\equiv h\\left(x^{2}\\right)$.\n\n8.28 Find the isotropic points for the Riemannian metric\n\n$$\nd s^{2}=\\left(\\ln x^{2}\\right)\\left(d x^{1}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{2}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{3}\\right)^{2} \\quad\\left(x^{2}>1\\right)\n$$\n\nand find the curvature $\\mathrm{K}$ at those points. [Hint: Use Problem 8.27.]\n\n8.29 Show that $\\mathbf{R}^{3}$ under the metric\n\n$$\ng_{11}=e^{x^{2}} \\quad g_{22}=1 \\quad g_{33}=e^{x^{2}} \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nhas constant Riemannian curvature with all points isotropic, and find that curvature.\n\n8.30 Show that in a Riemannian 2-space [for which (8.11) holds]: (a) $R_{i j}=-g_{i j} \\mathrm{~K},(b) R_{j}^{i}=-\\delta_{j}^{i} \\mathrm{~K}$, and (c) $R=-2 \\mathrm{~K}$.\n\n8.31 Calculate the Ricci tensor $R_{i j}$ for Problem 8.13 using (8.16), and compare your answers with those obtained earlier.\n\n8.32 Use Problem 8.30 to calculate the Ricci tensors of both kinds and the curvature invariant for the spherical metric of Problem 8.25.\n\n8.33 Calculate the Ricci tensors of both kinds and the curvature invariant for the (hyperbolic) metric of Problem 8.12. [Hint: Problem 8.27 can be used to good advantage here.]\n\n8.34 Prove that for any tensor $\\left(T^{i j}\\right)$, symmetric or not, $T_{, i j}^{i j}=T_{, j i}^{i j}$. [Hint: Use Problem 8.17 and the symmetry of the Ricci tensor.]\n\n8.35 Is identical vanishing equivalent for the Riemannian curvature and the Ricci curvature invariant? Can you find an example where one is zero everywhere but not the other?\n\n8.36 Is constancy in space equivalent for the two curvatures $\\mathrm{K}$ and $R$ ?\n\n"], "lesson": "\\section*{Chapter 8}\n\\section*{Riemannian Curvature}\n\\subsection*{8.1 THE RIEMANN TENSOR}\nThe Riemann tensor emerges from an analysis of a simple question. Starting with a covariant vector $\\left(V_{i}\\right)$ and taking the covariant derivative with respect to $x^{j}$ and then with respect to $x^{k}$ produces the third-order tensor\n\n$$\n\\left(\\left(V_{i}\\right)_{, j}\\right)_{, k} \\equiv\\left(V_{i, j k}\\right)\n$$\n\nDoes the order of differentiation matter, or does $V_{i, j k}=V_{i, k j}$ hold in general?\n\nStandard hypotheses concerning differentiability suffice to guarantee that the partial derivative of order two is order-independent,\n\n$$\n\\frac{\\partial^{2} V_{i}}{\\partial x^{j} \\partial x^{k}}=\\frac{\\partial^{2} V_{i}}{\\partial x^{k} \\partial x^{j}}\n$$\n\nbut due to the presence of Christoffel symbols, such hypotheses do not extend to covariant differentiation. The following formula is established in Problem 8.1:\n\nwhere\n\n\n\\begin{gather*}\nV_{j, k l}-V_{j, l k}=R_{j k l}^{i} V_{i}  \\tag{8.1}\\\\\nR_{j k l}^{i} \\equiv \\frac{\\partial \\Gamma_{j l}^{i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial x^{l}}+\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i} \\tag{8.2}\n\\end{gather*}\n\n\nThe Quotient Theorem (covariant form) immediately implies\n\nTheorem 8.1: The $n^{4}$ components defined by (8.2) are those of a fourth-order tensor, contravariant of order one, covariant of order three.\n\n$\\left(R_{j k l}^{i}\\right)$ is called the Riemann (or Riemann-Christoffel) tensor of the second kind; lowering the contravariant index produces\n\n\n\\begin{equation*}\nR_{i j k l} \\equiv g_{i r} R_{j k l}^{r} \\tag{8.3}\n\\end{equation*}\n\n\nthe Riemann tensor of the first kind.\n\nIn answer to our original question, we may now say that covariant differentiation is orderdependent unless the metric is such as to make the Riemann tensor (either kind) vanish.\n\n\\subsection*{8.2 PROPERTIES OF THE RIEMANN TENSOR}\n\\section*{Two Important Formulas}\nThe Riemann tensor of the first kind can be introduced independently via the following formula (see Problem 8.4):\n\n\n\\begin{equation*}\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r} \\tag{8.4}\n\\end{equation*}\n\n\nFrom (8.4) there follows\n\n\n\\begin{equation*}\nR_{i j k l}=\\frac{1}{2}\\left(\\frac{\\partial^{2} g_{i l}}{\\partial x^{j} \\partial x^{k}}+\\frac{\\partial^{2} g_{j k}}{\\partial x^{i} \\partial x^{l}}-\\frac{\\partial^{2} g_{i k}}{\\partial x^{j} \\partial x^{l}}-\\frac{\\partial^{2} g_{j l}}{\\partial x^{i} \\partial x^{k}}\\right)+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r} \\tag{8.5}\n\\end{equation*}\n\n\nEXAMPLE 8.1 Calculate the components $R_{i j k l}$ of the Riemann tensor for the metric of Problem 7.22,\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(d x^{2}\\right)^{2}\n$$\n\nThe nonvanishing Christoffel symbols are $\\Gamma_{22}^{2}=-\\left(x^{2}\\right)^{-1}$ and $\\Gamma_{222}=g_{22} \\Gamma_{22}^{2}=\\left(x^{2}\\right)^{-3}$. The partial-derivative terms in (8.4) vanish unless all indices are 2; but then the two terms cancel. Likewise the Christoffel-symbol terms either vanish or cancel. We conclude that all sixteen components $R_{i j k l}=0$.\n\n\\section*{Symmetry Properties}\nInterchange of $k$ and $l$ in (8.2) shows that $R_{j k l}^{i}=-R_{j l k}^{i}$, whence $R_{i j k l}=-R_{i j l k}$. This and two other symmetry properties are easily established at this point; Bianchi's (first) identity will be demonstrated in Chapter 9.\n\n\n\\begin{align*}\n\\text { first skew symmetry } & R_{i j k l}=-R_{j i k l} \\\\\n\\text { second skew symmetry } & R_{i j k l}=-R_{i j l k}  \\tag{8.6}\\\\\n\\text { block symmetry } & R_{i j k l}=R_{k l i j} \\\\\n\\text { Bianchi's identity } & R_{i j k l}+R_{i k l j}+R_{i l j k}=0\n\\end{align*}\n\n\n\\section*{Number of Independent Components}\nWe shall count the separate types of potentially nonzero components, using the above symmetry properties. The first two properties imply that $R_{a a c d}$ and $R_{a b c c}$ (not summed on $a$ or $c$ ) are zero. In the following list, we agree not to sum on repeated indices.\n\n(A) Type $R_{a b a b}, a<b: \\quad n_{A}={ }_{n} C_{2}=n(n-1) / 2$\n\n(B) Type $R_{a b a c}, b<c: \\quad n_{B}=3 \\cdot{ }_{n} C_{3}=n(n-1)(n-2) / 2$\n\n(C) Type $R_{a b c d}$ or $R_{a c b d}, a<b<c<d$ (for type $R_{a d b c}$, use Bianchi's identity): $n_{C}=2 \\cdot{ }_{n} C_{4}=$ $n(n-1)(n-2)(n-3) / 12$\n\nIn (A) the count is of combinations of $n$ numbers two at a time (for $a$ and $b$ ). In (B) one partitions the index strings into the three groups (with ${ }_{n} C_{3}$ in each group) for which\n\n$$\na<b<c \\quad b<a<c \\quad b<c<a\n$$\n\nEither subtype of (C) has as many members as there are combinations of $n$ numbers four at a time (for $a, b, c$, and $d$ ).\n\nSumming $n_{A}, n_{B}$, and $n_{C}$, we prove\n\nTheorem 8.2: There are a total of $n^{2}\\left(n^{2}-1\\right) / 12$ components of the Riemann tensor $\\left(R_{i j k l}\\right)$ that are not identically zero and that are independent from the rest.\n\nCorollary 8.3: In two-dimensional Riemannian space, the only components of the Riemann tensor not identically zero are $R_{1212}=R_{2121}=-R_{1221}=-R_{2112}$.\n\nEXAMPLE 8.2 For the metric of spherical coordinates,\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2} d x^{3}\\right)^{2}\n$$\n\nlist and calculate the nonzero components $R_{i j k l}$, if any.\n\nBy Theorem 8.2 with $n=3$, there are six potentially nonzero components:\n\n(A) $R_{1212}, R_{1313}, R_{2323}$\n\n(B) $R_{1213}, R_{1232}\\left(=R_{2123}\\right), R_{1323}\\left(=R_{3132}\\right)$\n\nBecause $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (diagonal metric tensor; no summation), we may instead compute the mixed components. From Problem 6.5,\n\n$$\n\\begin{array}{ll}\ni=1 & \\Gamma_{22}^{1}=-x^{1}, \\quad \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2} \\\\\ni=\\mathbf{2} & \\Gamma_{13}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}, \\quad \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2} \\\\\ni=\\mathbf{3} & \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}, \\quad \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}\n\\end{array}\n$$\n\nand (8.2) gives:\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=-1-\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=0 \\\\\nR_{313}^{1} & =\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{31}^{1}}{\\partial x^{1}}+\\Gamma_{33}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{31}^{r} \\Gamma_{r 3}^{1}=-\\sin ^{2} x^{2}+\\Gamma_{33}^{1} \\Gamma_{11}^{1}+\\Gamma_{31}^{3} \\Gamma_{33}^{1}=0 \\\\\nR_{323}^{2} & =\\frac{\\partial \\Gamma_{33}^{2}}{\\partial x^{2}}-\\frac{\\partial \\Gamma_{32}^{2}}{\\partial x^{3}}+\\Gamma_{33}^{r} \\Gamma_{r 2}^{2}-\\Gamma_{32}^{r} \\Gamma_{r 3}^{2}=-\\cos 2 x^{2}+\\Gamma_{33}^{1} \\Gamma_{12}^{2}-\\Gamma_{32}^{3} \\Gamma_{33}^{2}=-\\cos 2 x^{2}-\\sin ^{2} x^{2}+\\cos ^{2} x^{2}=0 \\\\\nR_{213}^{1} & =\\frac{\\partial \\Gamma_{23}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{3}}+\\Gamma_{23}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 3}^{1}=\\Gamma_{23}^{3} \\Gamma_{31}^{1}-\\Gamma_{21}^{2} \\Gamma_{23}^{1}=0 \\\\\nR_{232}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{3}}-\\frac{\\partial \\Gamma_{23}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 3}^{1}-\\Gamma_{23}^{r} \\Gamma_{r 2}^{1}=\\Gamma_{22}^{1} \\Gamma_{13}^{1}-\\Gamma_{23}^{3} \\Gamma_{32}^{1}=0 \\\\\nR_{323}^{1} & =\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{2}}-\\frac{\\partial \\Gamma_{32}^{1}}{\\partial x^{3}}+\\Gamma_{33}^{r} \\Gamma_{r 2}^{1}-\\Gamma_{22}^{r} \\Gamma_{r 3}^{1}=-2 x^{1} \\sin x^{2} \\cos x^{2}+\\Gamma_{33}^{2} \\Gamma_{22}^{1}-\\Gamma_{32}^{3} \\Gamma_{33}^{1} \\\\\n& =-2 x^{1} \\sin x^{2} \\cos x^{2}+x^{1} \\sin x^{2} \\cos x^{2}+\\left(\\cot x^{2}\\right)\\left(x^{1} \\sin ^{2} x^{2}\\right)=0\n\\end{aligned}\n$$\n\nTherefore, $R_{i j k l}=0$ for all $i, j, k, l$.\n\n\\subsection*{8.3 RIEMANNIAN CURVATURE}\nThe Riemannian (or sectional) curvature relative to a given metric $\\left(g_{i j}\\right)$ is defined for each pair of (contravariant) vectors $\\mathbf{U}=\\left(U^{i}\\right), \\mathbf{V}=\\left(V^{i}\\right)$ as\n\n\n\\begin{equation*}\n\\mathrm{K}=\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})=\\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}} \\quad\\left(G_{p q r s} \\equiv g_{p r} g_{q s}-g_{p s} g_{q r}\\right) \\tag{8.7}\n\\end{equation*}\n\n\nThis sort of curvature depends not only on position, but also on a pair of directions selected at each point (the vectors $\\mathbf{U}$ and $\\mathbf{V}$ ). By contrast, the curvature $\\kappa$ of a curve depends only on the points along the curve. Although it would seem desirable for $\\mathrm{K}$ to depend only on the points of space, to demand this would impose severe and unrealistic restrictions, as will become apparent in Chapter 9.\n\nEXAMPLE 8.3 The numerator of (8.7) is an invariant, because $\\left(R_{i j k l}\\right)$ is a tensor. As for the denominator, the identity\n\n\n\\begin{equation*}\nG_{p q r s} V_{(1)}^{p} V_{(2)}^{q} V_{(3)}^{r} V_{(4)}^{s}=\\left(\\mathbf{V}_{(1)} \\mathbf{V}_{(3)}\\right)\\left(\\mathbf{V}_{(2)} \\mathbf{V}_{(4)}\\right)-\\left(\\mathbf{V}_{(1)} \\mathbf{V}_{(4)}\\right)\\left(\\mathbf{V}_{(2)} \\mathbf{V}_{(3)}\\right) \\tag{8.8}\n\\end{equation*}\n\n\nimplies that the denominator is an invariant and proves (Lemma 4.1) that $\\left(G_{i j k l}\\right)$ is also a tensor. It follows that $\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})$ is an invariant, and thus it serves to generalize the Gaussian curvature of a surface to higher dimensions.\n\nHelpful in the calculation of $\\mathrm{K}$ is the fact that the $G_{i j k l}$ possess exactly the same symmetries as the $R_{i j k l}$ (see Problem 8.19). Moreover, if $g=\\left(g_{i j}\\right)$ is diagonal, all the nonzero $G_{i j k l}$ will be derivable from the type-A terms\n\n$$\nG_{a b a b}=g_{a a} g_{b b} \\quad(a<b ; \\text { no summation })\n$$\n\nEXAMPLE 8.4 Evaluate the Riemannian curvature at any point $\\left(x^{i}\\right)$ of Riemannian 3 -space in the directions (a) $\\mathbf{U}=(1,0,0)$ and $\\mathbf{V}=(0,1,1)$, and $(b) \\mathbf{U}=(0,1,0)$ and $\\mathbf{V}=(1,1,0)$, if the metric is given by\n\n$$\ng_{11}=1 \\quad g_{22}=2 x^{1} \\quad g_{33}=2 x^{2} \\quad g_{i j}=0 \\quad \\text { if } \\quad i \\neq j\n$$\n\nFrom Problem 6.4, the nonzero Christoffel symbols are:\n\n$$\n\\Gamma_{22}^{1}=-1 \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{2 x^{1}} \\quad \\Gamma_{33}^{2}=-\\frac{1}{2 x^{1}} \\quad \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\frac{1}{2 x^{2}}\n$$\n\nSince $n=3$, only six (by Theorem 8.2) components of the Riemann tensor need be considered: $R_{1212}, R_{1313}$, $R_{2323}, R_{1213}, R_{2123}$ and $R_{3132}$. The metric being diagonal, we compute:\n\n$$\n\\begin{gathered}\nR_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=0-0+0-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=\\frac{1}{2 x^{1}} \\\\\nR_{313}^{1}=0 \\quad R_{323}^{2}=\\frac{1}{4 x^{1} x^{2}} \\quad R_{213}^{1}=0 \\quad R_{123}^{2}=0 \\quad R_{132}^{3}=\\frac{1}{4 x^{1} x^{2}}\n\\end{gathered}\n$$\n\nwhich yield the three terms\n\n(A) $R_{1212}=g_{11} R_{212}^{1}=1 / 2 x^{1}, R_{2323}=g_{22} R_{323}^{2}=1 / 2 x^{2}$\n\n(B) $R_{3132}=g_{33} R_{132}^{3}=1 / 2 x^{1}$\n\nTheorem 8.2 also applies to the $G_{i j k l}$; but we may take the shortcut indicated in Example 8.3:\n\n(A) $G_{1212}=g_{11} g_{22}=2 x^{1}, G_{1313}=g_{11} g_{33}=2 x^{2}, \\quad G_{2323}=g_{22} g_{33}=4 x^{1} x^{2}$\n\nLet us now give an expanded form of (8.7) in the case that type-C terms are absent. It is convenient to define the $n^{4}$ functions\n\n\\[\nW_{i j k l} \\equiv\\left|\\begin{array}{cc}\nU^{i} & U^{j}  \\tag{8.9}\\\\\nV^{i} & V^{j}\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{k} & U^{l} \\\\\nV^{k} & V^{l}\n\\end{array}\\right|\n\\]\n\nof two vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$. Observe that the $W_{i j k l}$ possess all the symmetries of the $R_{i j k l}$ (or the $G_{i j k l}$ ). Looking at the numerator of (8.7), we see that a given type-A coefficient from the basic set generates, via its skew symmetries, the 4 terms\n\n$$\nR_{a b a b}\\left(U^{a} V^{b} U^{a} V^{b}-U^{b} V^{a} U^{a} V^{b}-U^{a} V^{b} U^{b} V^{a}+U^{b} V^{a} U^{b} V^{a}\\right)=R_{a b a b} W_{a b a b}\n$$\n\nand these precisely exhaust the $2 \\times 2=4$ terms in which the coefficient $R_{i j k i}$ involves the same distinct integers, $a$ and $b$, in the first pair of indices as in the second pair. A given type-B coefficient from the basic set will generate, via its skew symmetries and block symmetry, the 8 terms\n\n$$\nR_{a b a c}\\left(W_{a b a c}+W_{a c a b}\\right)=2 R_{a b a c} W_{a b a c}\n$$\n\nand these exactly correspond to the $2^{2} \\times 2=8$ ways of writing $R_{i j k l}$ such that the first and second index-pairs contain the common integer $a$ but are otherwise composed of distinct integers $b$ and $c$. Analyzing the denominator of (8.7) in the same fashion, we obtain as the desired formula:\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{\\sum_{\\text {type A }} R_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} R_{a b a c} W_{a b a c}}{\\sum_{\\text {type A }} G_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} G_{a b a c} W_{a b a c}} \\tag{8.10}\n\\end{equation*}\n\n\nIt is understood that the summation convention does not operate in (8.10); the indicated summations are over all the nonzero, independent $R_{i j k l}\\left(G_{i j k l}\\right)$, according to type. Now to the problem at hand.\n\n(a) For the data, (8.10) becomes\n\nFor\n\n$$\n\\mathrm{K}=\\frac{R_{1212} W_{1212}+R_{2323} W_{2323}+2 R_{3132} W_{3132}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}\n$$\n\nwe have\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{U} \\\\\n\\mathbf{V}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{array}\\right]\n$$\n\n$$\n\\begin{array}{ll}\nW_{1212}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right|^{2}=1 & W_{2323}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 1\n\\end{array}\\right|^{2}=0 \\\\\nW_{3132}=\\left|\\begin{array}{ll}\n0 & 1 \\\\\n1 & 0\n\\end{array}\\right|\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 1\n\\end{array}\\right|=0 & W_{1313}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right|^{2}=1\n\\end{array}\n$$\n\nand so\n\n$$\n\\mathrm{K}=\\frac{\\left(1 / 2 x^{1}\\right)(1)+\\left(1 / 2 x^{2}\\right)(0)+2\\left(1 / 2 x^{1}\\right)(0)}{\\left(2 x^{1}\\right)(1)+\\left(2 x^{2}\\right)(1)+\\left(4 x^{1} x^{2}\\right)(0)}=\\frac{1}{4 x^{1}\\left(x^{1}+x^{2}\\right)}\n$$\n\n(b) For\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{U} \\\\\n\\mathbf{V}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & 0 \\\\\n1 & 1 & 0\n\\end{array}\\right]\n$$\n\nwe have\n\nwhence\n\n$$\n\\begin{array}{ll}\nW_{1212}=\\left|\\begin{array}{ll}\n0 & 1 \\\\\n1 & 1\n\\end{array}\\right|^{2}=1 & W_{2323}=\\left|\\begin{array}{ll}\n1 & 0 \\\\\n1 & 0\n\\end{array}\\right|^{2}=0 \\\\\nW_{3132}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n0 & 1\n\\end{array}\\right|\\left|\\begin{array}{ll}\n0 & 1 \\\\\n0 & 1\n\\end{array}\\right|=0 & W_{1313}=\\left|\\begin{array}{ll}\n0 & 0 \\\\\n1 & 0\n\\end{array}\\right|^{2}=0\n\\end{array}\n$$\n\n$$\n\\mathrm{K}=\\frac{\\left(1 / 2 x^{1}\\right)(1)+0+0}{\\left(2 x^{1}\\right)(1)+0+0}=\\frac{1}{4\\left(x^{1}\\right)^{2}}\n$$\n\n\\section*{Observations on the Curvature Formula}\nI. If $n=2,(8.7)$ reduces to\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}} \\equiv \\frac{R_{1212}}{g} \\tag{8.11}\n\\end{equation*}\n\n\n(see Problem 8.7). Thus, at a given point in Riemannian 2-space, the curvature is determined by the $g_{i j}$ and their derivatives, and is independent of the directions $\\mathbf{U}$ and $\\mathbf{V}$.\n\nII. The extension of (8.10) to include type-C terms is as follows:\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{\\sum_{\\text {type A }} R_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} R_{a b a c} W_{a b a c}+2 \\sum_{\\text {type C }} R_{a b c d}\\left(W_{a b c d}-W_{a d b c}\\right)+2 \\sum_{\\text {type C }} R_{a c b d}\\left(W_{a c b d}-W_{a d c b}\\right)}{\\sum_{\\text {type A }} G_{a b a b} W_{a b a b}+2 \\sum_{\\text {type B }} G_{a b a c} W_{a b a c}+2 \\sum_{\\text {type C }} G_{a b c d}\\left(W_{a b c d}-W_{a d b c}\\right)+2 \\sum_{\\text {type C }} G_{a c b d}\\left(W_{a c b d}-W_{a d c b}\\right)} \\tag{8.12}\n\\end{equation*}\n\n\n(see Problem 8.9).\n\nIII. If linearly independent $\\mathbf{U}$ and $\\mathbf{V}$ are replaced by independent linear combinations of themselves, the curvature is unaffected; i.e.,\n\n\n\\begin{equation*}\n\\mathrm{K}(\\mathbf{x} ; \\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V})=\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V}) \\tag{8.13}\n\\end{equation*}\n\n\nTherefore, at a given point $\\mathbf{x}$, the curvature will have a value, not for each pair of vectors $\\mathbf{U}$ and $\\mathbf{V}$, but for each 2-flat passing through $\\mathbf{x}$.\n\n\\section*{Isotropic Points}\nIf the Riemannian curvature at $\\mathbf{x}$ does not change with the orientation of a 2 -flat through $\\mathbf{x}$, then $\\mathbf{x}$ is called isotropic. From (8.11), we have\n\nTheorem 8.4: All points of a two-dimensional Riemannian space are isotropic.\n\nIt is not immediately clear whether any metric ( $g_{i j}$ ) could lead to isotropic points in $\\mathbf{R}^{n}, n \\geqq 3$. But such is the case. Indeed, as is shown in Problem $8.12, \\mathbf{R}^{3}$ under a hyperbolic metric is isotropic at any point.\n\n\\subsection*{8.4 THE RICCI TENSOR}\nA brief look will be given a tensor that is of importance in Relativity. The Ricci tensor of the first kind is defined as a contraction of the Riemann tensor of the second kind:\n\n\n\\begin{equation*}\nR_{i j} \\equiv R_{i j k}^{k}=\\frac{\\partial \\Gamma_{i k}^{k}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{k}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{k}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{k} \\tag{8.14}\n\\end{equation*}\n\n\nRaising an index yields the Ricci tensor of the second kind:\n\n\n\\begin{equation*}\nR_{j}^{i} \\equiv g^{i k} R_{k j} \\tag{8.15}\n\\end{equation*}\n\n\nBy use of the following simple consequence of Laplace's expansion (2.5):\n\nLemma 8.5: Let $A=\\left[a_{i j}(\\mathbf{x})\\right]_{n n}$ be a nonsingular matrix of multivariate functions, with inverse $B=\\left[b_{i j}(\\mathbf{x})\\right]_{n n}$. Then\n\n$$\n\\frac{\\partial}{\\partial x^{i}}(\\ln |\\operatorname{det} A|) \\equiv \\frac{1}{\\operatorname{det} A} \\frac{\\partial}{\\partial x^{i}}(\\operatorname{det} A)=b_{s r} \\frac{\\partial a_{r s}}{\\partial x^{i}}\n$$\n\nthe definition (8.14) may be put into a form (Problem 8.14) that makes evident the symmetry of the $R_{i j}$.\n\n\n\\begin{equation*}\nR_{i j}=\\frac{\\partial^{2}}{\\partial x^{i} \\partial x^{j}}(\\ln \\sqrt{|g|})-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{r}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{r}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s} \\tag{8.16}\n\\end{equation*}\n\n\nHere, as always, $g \\equiv \\operatorname{det} G$.\n\nTheorem 8.6: The Ricci tensor is symmetric.\n\nAfter raising a subscript to define the Ricci tensor of the second kind, $R_{j}^{i}=g^{i s} R_{s j}$, and then contracting on the remaining pair of indices, the important invariant $R \\equiv R_{i}^{i}$ results, called the Ricci (or scalar) curvature. By (8.16),\n\n\n\\begin{equation*}\nR=g^{i j}\\left[\\frac{\\partial^{2}}{\\partial x^{i} \\partial x^{j}}(\\ln \\sqrt{|g|})-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{r}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{r}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}\\right] \\tag{8.17}\n\\end{equation*}\n\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{THE RIEMANN TENSOR}\n\\subsection*{8.1 Prove (8.1).}\nBy definition of the covariant derivative,\n\n\n\\begin{equation*}\nV_{i, j k}=\\left(V_{i, j}\\right)_{, k}=\\frac{\\partial}{\\partial x^{k}}\\left(V_{i, j}\\right)-\\Gamma_{i k}^{r}\\left(V_{r, j}\\right)-\\Gamma_{j k}^{r}\\left(V_{i, r}\\right) \\tag{1}\n\\end{equation*}\n\n\nSubstitute\n\n$$\nV_{i, j}=\\frac{\\partial V_{i}}{\\partial x^{j}}-\\Gamma_{i j}^{s} V_{s}\n$$\n\nin (1), carry out the differentiation, and remove parentheses:\n\n\n\\begin{equation*}\nV_{i, j k}=\\frac{\\partial^{2} V_{i}}{\\partial x^{k} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}-\\Gamma_{i j}^{s} \\frac{\\partial V_{s}}{\\partial x^{k}}-\\Gamma_{i k}^{r} \\frac{\\partial V_{r}}{\\partial x^{j}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}-\\Gamma_{j k}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{j k}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{2}\n\\end{equation*}\n\n\nInterchanging $j$ and $k$ yields\n\n\n\\begin{equation*}\nV_{i, k j}=\\frac{\\partial^{2} V_{i}}{\\partial x^{j} \\partial x^{k}}-\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i k}^{s} \\frac{\\partial V_{s}}{\\partial x^{j}}-\\Gamma_{i j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}+\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s}-\\Gamma_{k j}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{k j}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{3}\n\\end{equation*}\n\n\nSubtracting (3) from (2), one sees that the first, third, fourth, sixth, and seventh terms on the right of (2) cancel with the first, fourth, third, sixth, and seventh terms on the right of (3), leaving\n\n$$\n\\begin{aligned}\nV_{i, j k}-V_{i, k j} & =-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}+\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s} \\\\\n& =\\left(\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s}\\right) V_{s}=R_{i j k}^{s} V_{s}\n\\end{aligned}\n$$\n\n8.2 Show that at any point where the Christoffel symbols vanish,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=0\n$$\n\nIn this case the expression for $R_{j k l}^{i}$ reduces to just $\\partial \\Gamma_{j l}^{i} / \\partial x^{k}-\\partial \\Gamma_{j k}^{i} / \\partial x^{l}$. Therefore,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=\\frac{\\partial \\Gamma_{j l}^{i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial x^{l}}+\\frac{\\partial \\Gamma_{k j}^{i}}{\\partial x^{l}}-\\frac{\\partial \\Gamma_{k l}^{i}}{\\partial x^{j}}+\\frac{\\partial \\Gamma_{l k}^{i}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{l j}^{i}}{\\partial x^{k}}\n$$\n\nAs all the terms cancel, the desired relationship is proved.\n\n8.3 Prove that for an arbitrary second-order covariant tensor $\\left(T_{i j}\\right)$\n\n$$\nT_{i j, k l}-T_{i j, l k}=R_{i k l}^{s} T_{s j}+R_{j k l}^{s} T_{i s}\n$$\n\n(The general formula,\n\n\n\\begin{equation*}\nT_{i_{1} i_{2} \\ldots i_{p}, k l}-T_{i_{1} i_{2} \\ldots i_{p}, l k}=\\sum_{q=1}^{p} R_{i_{q} k l}^{s} T_{i_{1} \\ldots i_{q-1} s i_{q+1} \\ldots i_{p}} \\tag{8.18}\n\\end{equation*}\n\n\nwhich is credited to Ricci, is similarly established.)\n\nA direct approach would be quite tedious; instead, first establish that\n\n\n\\begin{equation*}\nV_{, j k}^{i}-V_{, k j}^{i}=-R_{s j k}^{i} V^{s} \\tag{1}\n\\end{equation*}\n\n\nfor any contravariant vector $\\left(V^{i}\\right)$ (see Problem 8.16). Now observe that $\\left(V^{q} T_{i q}\\right)$ is a covariant vector, to which (8.1) applies. Thus,\n\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, k l}-\\left(V^{q} T_{i q}\\right)_{, l k}=R_{i k l}^{s} V^{q} T_{s q} \\tag{2}\n\\end{equation*}\n\n\nBy the inner-product rule for covariant differentiation,\n\n\n\\begin{align*}\n& \\left(V^{q} T_{i q}\\right)_{, k}=V_{, k}^{q} T_{i q}+V^{q} T_{i q, k}  \\tag{3}\\\\\n& \\left(V^{q} T_{i q}\\right)_{, k l}=V_{, k l}^{q} T_{i q}+V_{, k}^{q} T_{i q, l}+V_{, l}^{q} T_{i q, k}+V^{q} T_{i q, k l}\n\\end{align*}\n\n\nInterchange $k$ and $l$ :\n\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, l k}=V_{, l k}^{q} T_{i q}+V_{, l}^{q} T_{i q, k}+V_{, k}^{q} T_{i q, l}+V^{q} T_{i q, l k} \\tag{4}\n\\end{equation*}\n\n\nSubtraction of (4) from (3) will cancel the middle two terms on the right-hand sides, leaving\n\n\n\\begin{equation*}\nR_{i k l}^{s} V^{q} T_{s q}=\\left(V_{, k l}^{q}-V_{, l k}^{q}\\right) T_{i q}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q} \\tag{5}\n\\end{equation*}\n\n\nNow use (1) in the right member of (5):\n\n$$\nR_{i k l}^{s} V^{q} T_{s q}=-R_{q k l}^{s} V^{q} T_{i s}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q}\n$$\n\nwhich may be rearranged into\n\n$$\n\\left[\\left(T_{i q, k l}-T_{i q, l k}\\right)-\\left(R_{i k l}^{s} T_{s q}+R_{q k l}^{s} T_{i s}\\right)\\right] V^{q}=0\n$$\n\nBut $\\left(V^{i}\\right)$ is arbitrary, so the bracketed expression must vanish. QED\n\n\\section*{PROPERTIES OF THE RIEMANN TENSOR}\n8.4 Establish (8.4).\n\nBy definition,\n\n$$\n\\begin{aligned}\nR_{i j k l} & =g_{i s} R_{j k l}^{s}=g_{i s} \\frac{\\partial \\Gamma_{j l}^{s}}{\\partial x^{k}}-g_{i s} \\frac{\\partial \\Gamma_{j k}^{s}}{\\partial x^{l}}+g_{i s} \\Gamma_{j l}^{r} \\Gamma_{r k}^{s}-g_{i s} \\Gamma_{j k}^{r} \\Gamma_{r l}^{s} \\\\\n& =\\frac{\\partial\\left(g_{i s} \\Gamma_{j l}^{s}\\right)}{\\partial x^{k}}-\\frac{\\partial g_{i s}}{\\partial x^{k}} \\Gamma_{j l}^{s}-\\frac{\\partial\\left(g_{i s} \\Gamma_{j k}^{s}\\right)}{\\partial x^{l}}+\\frac{\\partial g_{i s}}{\\partial x^{l}} \\Gamma_{j k}^{s}+\\Gamma_{j l}^{r} \\Gamma_{r k i}-\\Gamma_{j k}^{r} \\Gamma_{r l i}\n\\end{aligned}\n$$\n\n$$\n=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{j k}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{r l i}\\right)-\\Gamma_{j l}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{k}}-\\Gamma_{r k i}\\right)\n$$\n\nRecall from (6.2) that for arbitrary index $l$,\n\n$$\n\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{l r i}=\\Gamma_{i l r}\n$$\n\nBy substitution,\n\n$$\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r}\n$$\n\n8.5 Establish the first skew-symmetry property, $R_{i j k l}=-R_{j i k l}$.\n\nTo save writing, let\n\n$$\nG_{k l}^{i j} \\equiv \\frac{1}{2}\\left(\\frac{\\partial^{2} g_{i j}}{\\partial x^{k} \\partial x^{l}}+\\frac{\\partial^{2} g_{k l}}{\\partial x^{i} \\partial x^{j}}\\right) \\quad \\text { and } \\quad H_{k l}^{i j} \\equiv \\Gamma_{i j r} \\Gamma_{k l}^{r}\n$$\n\nNote the obvious symmetry properties\n\n$$\nG_{k l}^{i j}=G_{k l}^{j i}=G_{l k}^{i j} \\quad \\text { and } \\quad H_{k l}^{i j}=H_{k l}^{j i}=H_{l k}^{i j}\n$$\n\nAlso, it is clear that $G_{k l}^{i j}=G_{i j}^{k l}$; furthermore,\n\n$$\nH_{k l}^{i j}=\\left(g_{r s} \\Gamma_{i j}^{s}\\right) \\Gamma_{k l}^{r}=\\Gamma_{i j}^{s}\\left(g_{s r} \\Gamma_{k l}^{r}\\right)=\\Gamma_{i j}^{s} \\Gamma_{k l s}=H_{i j}^{k l}\n$$\n\nNow, by (8.5),\n\nand\n\n$$\n\\begin{aligned}\n& R_{i j k l}=G_{j k}^{i l}-G_{j l}^{i k}+H_{j k}^{i l}-H_{j l}^{i k} \\\\\n& R_{j i k l}=G_{i k}^{j l}-G_{i l}^{j k}+H_{i k}^{j l}-H_{i l}^{j k}=G_{j l}^{i k}-G_{j k}^{i l}+H_{j l}^{i k}-H_{j k}^{i l}=-R_{i j k l}\n\\end{aligned}\n$$\n\n8.6 List the independent, potentially nonzero components of $R_{i j k l}$ for $n=5$ and verify the formula of Theorem 8.2 in this case.\n\nType A: $R_{1212}, R_{1313}, R_{1414}, R_{1515}$\n\n$R_{2323}, R_{2424}, R_{2525}$\n\n$R_{3434}, R_{3535}$\n\n$R_{4545}$\n\nType B: $R_{1213}, R_{1214}, R_{1215}, R_{1314}, R_{1315}, R_{1415}$\n\n$R_{2123}, R_{2124}, R_{2125}, R_{2324}, R_{2325}, R_{2425}$\n\n$R_{3132}, R_{3134}, R_{3135}, R_{3234}, R_{3235}, R_{3435}$\n\n$R_{4142}, R_{4143}, R_{4145}, R_{4243}, R_{4245}, R_{4345}$\n\n$R_{5152}, R_{5153}, R_{5154}, R_{5253}, R_{5254}, R_{5354}$\n\nType C: $R_{1234}, R_{1235}, R_{1245}, R_{1345}, R_{2345}$\n\n$R_{1324}, R_{1325}, R_{1425}, R_{1435}, R_{2435}$\n\nThere are 10 components of types $\\mathrm{A}$ and $\\mathrm{C}$ each, and 30 of type B; or 50 altogether. From the formula,\n\n$$\n\\frac{n^{2}\\left(n^{2}-1\\right)}{12}=\\frac{5^{2}\\left(5^{2}-1\\right)}{12}=\\frac{(25)(24)}{12}=50\n$$\n\n\\section*{RIEMANNIAN CURVATURE}\n8.7 Prove (8.11).\n\nBy Corollary 8.3 and the corresponding result for the $G_{i j k l}$,\n\n$$\n\\mathrm{K}=\\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}}=\\frac{R_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}{G_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}=\\frac{R_{1212}}{G_{1212}}=\\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}}\n$$\n\n8.8 Calculate $\\mathrm{K}$ for the Riemannian metric $\\varepsilon d s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, using the result of Problem 8.7. 6.4 ,\n\nWe have only to calculate $R_{1212}=g_{11} R_{212}^{1}$. The nonvanishing Christoffel symbols are, by Problem\n\n$$\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}}\n$$\n\nConsequently,\n\nand\n\n$$\n\\begin{gathered}\nR_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n=\\frac{1}{\\left(x^{1}\\right)^{2}}-\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right)\\left(-\\frac{1}{x^{1}}\\right)=\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\n\\mathrm{~K}=\\frac{g_{11} R_{212}^{1}}{g_{11} g_{22}}=\\frac{R_{212}^{1}}{g_{22}}=\\frac{\\left(x^{1}\\right)^{-2}}{-\\left(x^{1}\\right)^{-2}}=-1 .\n\\end{gathered}\n$$\n\n8.9 Derive the form (8.12) of the curvature equation.\n\nWe need only establish the summations over the type-C terms in the numerator; the rest of the work was done in Example 8.4.\n\nFirst of all, let us verify that all $R_{i j k l}$ with $i j k l$ a permutation of $a b c d$, where $a<b<c<d$ are distinct integers, are generated by the skew and block symmetries of the three components $R_{a b c d}, R_{a c b d}$, and $R_{a d b c}$. Examination of Table 8-1, which uses an obvious notation for the symmetry operators, shows that all $4 !=24$ permutations are accounted for. Consequently, the type-C part of the numerator of (8.12) is [cf. the equation preceding $(8.10)$ ]\n\n\n\\begin{equation*}\n2 \\sum R_{a b c d} W_{a b c d}+2 \\sum R_{a c b d} W_{a c b d}+2 \\sum R_{a d b c} W_{a d b c} \\tag{1}\n\\end{equation*}\n\n\nTable 8-1\n\n\\begin{center}\n\\begin{tabular}{|c|l|l|l|}\n\\hline\n\\multirow{2}{*}{}\\begin{tabular}{c}\nSymmetry \\\\\nOperator \\\\\n\\end{tabular} & \\multicolumn{3}{|c|}{Subscript Chain} \\\\\n\\cline { 2 - 4 }\n & $\\boldsymbol{a b c d}$ & $\\boldsymbol{a} \\boldsymbol{b} \\boldsymbol{d}$ & $\\boldsymbol{a d b} \\boldsymbol{c}$ \\\\\n\\hline\n$\\mathrm{I}$ & $a b c d$ & $a c b d$ & $a d b c$ \\\\\n$\\mathrm{~S}_{1}$ & $b a c d$ & $c a b d$ & $d a b c$ \\\\\n$\\mathrm{~S}_{2}$ & $a b d c$ & $a c d b$ & $a d c b$ \\\\\n$\\mathrm{~S}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{2} \\mathrm{~S}_{1}$ & $b a d c$ & $c a d b$ & $d a c b$ \\\\\n\\hline\n$\\mathrm{B}$ & $c d a b$ & $b \\bar{d} a c$ & $b \\bar{b} a \\bar{d}$ \\\\\n$\\mathrm{BS}_{1}=\\mathrm{S}_{2} \\mathrm{~B}$ & $c d b a$ & $b d c a$ & $b c d a$ \\\\\n$\\mathrm{BS}_{2}=\\mathrm{S}_{1} \\mathrm{~B}$ & $d c a b$ & $d b a c$ & $c b a d$ \\\\\n$\\mathrm{BS}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{1} \\mathrm{~S}_{2} \\mathrm{~B}$ & $d c b a$ & $d b c a$ & $c b d a$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nThe first summation is over all $a<b<c<d$ that yield a nonzero $R_{a b c d}$ in the basic set; similarly for the second summation. The third summation does not involve the basic set, but the symmetries of $R_{i j k l}$ (shared by $W_{i j k l}$ ) allow its absorption in the first two summations. Thus, by Bianchi's identity,\n\n\n\\begin{equation*}\n2 R_{a d b c} W_{a d b c}=2\\left(-R_{a b c d}-R_{a c d b}\\right) W_{a d b c}=-2 R_{a b c d} W_{a d b c}-2 R_{a c b d} W_{a d c b} \\tag{2}\n\\end{equation*}\n\n\nand substitution of (2) in (1) produces the expression given in (8.12).\n\n8.10 Prove (8.13).\n\nWe have\n\n$$\n\\begin{aligned}\nW_{i j k l}(\\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V}) & =\\left|\\begin{array}{cc}\n\\lambda U^{i}+\\nu V^{i} & \\lambda U^{j}+\\nu V^{j} \\\\\n\\mu U^{i}+\\omega V^{i} & \\mu U^{j}+\\omega V^{j}\n\\end{array}\\right|\\left|\\begin{array}{cc}\n\\lambda U^{k}+\\nu V^{k} & \\lambda U^{l}+\\nu V^{l} \\\\\n\\mu U^{k}+\\omega V^{k} & \\mu U^{l}+\\omega V^{l}\n\\end{array}\\right| \\\\\n& =\\left|\\begin{array}{cc}\n\\lambda & \\nu \\\\\n\\mu & \\omega\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{i} & U^{j} \\\\\nV^{i} & V^{j}\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{k} & U^{l} \\\\\nV^{k} & V^{l}\n\\end{array}\\right|=(\\lambda \\omega-\\nu \\mu)^{2} W_{i j k l}(\\mathbf{U}, \\mathbf{V})\n\\end{aligned}\n$$\n\nso that the quantity $(\\lambda \\omega-\\nu \\mu)^{2}$ factors out of all terms in (8.12) for $\\mathrm{K}(\\mathbf{x} ; \\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V})$, leaving $\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})$.\n\n8.11 Find the isotropic points in the Riemannian space $\\mathbf{R}^{3}$ with metric\n\n$$\ng_{11}=1 \\quad g_{22}=g_{33}=\\left(x^{1}\\right)^{2}+1 \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nand calculate the curvature $\\mathrm{K}$ at those points.\n\nFollow Example 8.4. By Problem 6.4, the nonzero Christoffel symbols are\n\n$$\n\\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{33}^{1}=-x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\n$$\n\nThen:\n\nwhich give\n\n$$\n\\begin{aligned}\n& R_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{313}^{1}=\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{1}}+\\Gamma_{33}^{1} \\Gamma_{11}^{1}-\\Gamma_{31}^{3} \\Gamma_{33}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{323}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}=-x^{1} \\cdot \\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}=-\\frac{\\left(x^{1}\\right)^{2}}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{aligned}\n$$\n\n(A) $\\quad R_{1212}=g_{11} R_{212}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{1313}=g_{11} R_{313}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{2323}=g_{22} R_{323}^{2}=-\\left(x^{1}\\right)^{2}$\n\nThe corresponding terms for the denominator of $(8.10)$ are\n\n\n\\begin{align*}\nG_{1212}=g_{11} g_{22} & =\\left(x^{1}\\right)^{2}+1, \\quad G_{1313}=g_{11} g_{33}=\\left(x^{1}\\right)^{2}+1, \\quad G_{2323}=g_{22} g_{33}=\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2}  \\tag{A}\\\\\n\\mathrm{~K} & =\\frac{-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1212}-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1313}-\\left(x^{1}\\right)^{2} W_{2323}}{\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1212}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2} W_{2323}} \\\\\n& =-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-2} \\frac{W_{1212}+W_{1313}+\\left(x^{1}\\right)^{2}\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}{W_{1212}+W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}\n\\end{align*}\n\n\nIf $\\mathrm{K}$ is to be independent of the $W_{i j k l}$ (which vary with ine direction of the 2 -flat), then $\\left(x^{1}\\right)^{2}=1$, or $x^{1}= \\pm 1$. Therefore, the isotropic points compose two surfaces, on which the curvature has the value $\\mathrm{K}=-[1+1]^{-2} \\cdot 1=-1 / 4$.\n\n8.12 Show that every point of $\\mathbf{R}^{3}$ is isotropic for the metric\n\n$$\nd s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{3}\\right)^{2}\n$$\n\nProblem 6.4 gives as the nonvanishing Christoffel symbols:\n\n$$\n\\begin{gathered}\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=\\frac{1}{x^{1}} \\quad \\Gamma_{33}^{1}=\\frac{1}{x^{1}} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=-\\frac{1}{x^{1}}\n\\end{gathered}\n$$\n\nAs in earlier problems, we proceed to calculate a basic set of $R_{i j k l}$, via $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (no sum).\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=-\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& =-\\frac{1}{\\left(x^{1}\\right)^{2}}+\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right) \\frac{1}{x^{1}}=-\\frac{1}{\\left(x^{1}\\right)^{2}}\n\\end{aligned}\n$$\n\nSimilarly, $R_{313}^{1}=-1 /\\left(x^{1}\\right)^{2}$. For the remainder, the partial-derivative terms all drop out, yielding\n\n$$\n\\begin{gathered}\nR_{323}^{2}=\\Gamma_{33}^{r} \\Gamma_{r 2}^{2}-\\Gamma_{32}^{r} \\Gamma_{r 3}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}-0=-\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\nR_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{gathered}\n$$\n\nOur basic set is thus\n\n(A) $R_{1212}=R_{1313}=R_{2323}=-1 /\\left(x^{1}\\right)^{4}$\n\nand, by Example 8.3,\n\n(A) $G_{1212}=G_{1313}=G_{2323}=1 /\\left(x^{1}\\right)^{4}$\n\nis a basic set of $G_{i j k l}$. Formula (8.10) or (8.12) now gives\n\n$$\n\\mathrm{K}=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}=\\frac{\\left[-\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}{\\left[\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}=-1\n$$\n\nIt is seen that this Riemannian space is more than just isotropic; it is a space of constant curvature.\n\n\\section*{THE RICCI TENSOR}\n8.13 For the metric of Example 8.4, calculate (a) $R_{i j},(b) R_{j}^{i}$, (c) $R$.\n\n(a) From $R_{i j}=R_{i j k}^{k}=R_{i j 1}^{1}+R_{i j 2}^{2}+R_{i j 3}^{3}$ and the fact that $g_{i j}=0$ for $i \\neq j$, it follows that\n\n\n\\begin{equation*}\nR_{i j}=g^{11} R_{1 i j 1}+g^{22} R_{2 i j 2}+g^{33} R_{3 i j 3} \\tag{1}\n\\end{equation*}\n\n\nwhere $g^{11}=1, g^{22}=1 / 2 x^{1}, g^{33}=1 / 2 x^{2}$. Now, a basic set of the $R_{i j k l}$ was computed as\n\n$$\nR_{1221}\\left(=-R_{1212}\\right)=-\\frac{1}{2 x^{1}} \\quad R_{2332}\\left(=-R_{2323}\\right)=-\\frac{1}{2 x^{2}} \\quad R_{3123}\\left(=-R_{3132}\\right)=-\\frac{1}{2 x^{1}}\n$$\n\nand the only other nonzero components of the form $R_{\\text {aija }}$ generated by these are\n\n$$\nR_{2112}=-\\frac{1}{2 x^{1}} \\quad R_{3223}=-\\frac{1}{2 x^{2}} \\quad R_{3213}=-\\frac{1}{2 x^{1}}\n$$\n\nHence, the nonzero $R_{i j}$ may be read off from (1) as\n\n\n\\begin{gather*}\nR_{11}=g^{22} R_{2112}=-\\frac{1}{4\\left(x^{1}\\right)^{2}} \\\\\nR_{22}=g^{11} R_{1221}+g^{33} R_{3223}=-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}} \\\\\nR_{33}=g^{22} R_{2332}=-\\frac{1}{4 x^{1} x^{2}} \\\\\nR_{12}=g^{33} R_{3123}=-\\frac{1}{4 x^{1} x^{2}}=g^{33} R_{3213}=R_{21} \\\\\nR_{j}^{i}=g^{i k} R_{k j}=g^{i i} R_{i j} \\quad \\text { (no summation on } i \\text { ) }  \\tag{b}\\\\\n=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33} \\\\\n=(1)\\left[-\\frac{1}{4\\left(x^{1}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{1}}\\right)\\left[-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{2}}\\right)\\left(-\\frac{1}{4 x^{1} x^{2}}\\right)=-\\frac{x^{1}+2\\left(x^{2}\\right)^{2}}{\\left(2 x^{1} x^{2}\\right)^{2}}\n\\end{gather*}\n\n\n$$\nR=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33}\n$$\n\n8.14 Derive (8.16) from (8.14).\n\nFormula (8.14) involves two summations of the form $\\Gamma_{i s}^{s}$. By (6.4) and (6.1b),\n\n$$\n\\begin{aligned}\n\\Gamma_{i s}^{s} & =g^{s r} \\Gamma_{i s r}=\\frac{1}{2} g^{s r}\\left(-g_{i s r}+g_{s r i}+g_{r i s}\\right)=-\\frac{1}{2} g^{s r} g_{s i r}+\\frac{1}{2} g^{s r} g_{s r i}+\\frac{1}{2} g^{r s} g_{s i r} \\\\\n& =\\frac{1}{2} g^{s r} g_{r s i} \\equiv \\frac{1}{2} g^{s r} \\frac{\\partial g_{r s}}{\\partial x^{i}}=\\frac{\\partial}{\\partial x^{i}}(\\ln \\sqrt{|g|})\n\\end{aligned}\n$$\n\nwhere Lemma 8.5 was used in the last step. Now substitute in (8.14):\n\n$$\n\\begin{aligned}\nR_{i j} & =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\frac{\\partial(\\ln \\sqrt{|g|})}{\\partial x^{r}} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\left(\\frac{1}{\\sqrt{|g|}} \\sqrt{|g|} \\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\frac{1}{\\sqrt{|g|}} \\frac{\\partial(\\sqrt{|g|})}{\\partial x^{s}} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{s}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}\n\\end{aligned}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n8.15 The absolute partial derivatives of a tensor $\\mathbf{T}=\\left(T_{j}^{i} \\ldots\\right)$ defined on a 2-manifold $\\mathscr{M}: x^{i}=x^{i}(u, v)$ are defined as\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta u} \\equiv\\left(T_{j}^{i} \\ldots, k, \\frac{\\partial x^{k}}{\\partial u}\\right) \\quad \\text { and } \\quad \\frac{\\delta \\mathbf{T}}{\\delta v} \\equiv\\left(T_{j}^{i} \\ldots, \\frac{\\partial x^{k}}{\\partial v}\\right)^{\\dot{*}}\n$$\n\nSince $\\left(\\partial x^{i} / \\partial u\\right)$ and $\\left(\\partial x^{i} / \\partial v\\right)$ are vectors, the inner products produce a pair of tensors of the same type and order as $\\mathbf{T}$; thus the operation of absolute partial differentiation may be repeated indefinitely. Prove that if $\\left(V^{i}\\right)$ is any contravariant vector defined on $\\mathcal{M}$,\n\n$$\n\\frac{\\delta}{\\delta u}\\left(\\frac{\\delta V^{i}}{\\delta v}\\right)-\\frac{\\delta}{\\delta v}\\left(\\frac{\\delta V^{i}}{\\delta u}\\right)=R_{s k l}^{i} V^{s} \\frac{\\partial x^{k}}{\\partial u} \\frac{\\partial x^{l}}{\\partial v}\n$$\n\n[Hint: Expand the left side and use Problem 8.16.]\n\n8.16 Prove that for any vector $\\left(V^{i}\\right), V_{, k l}^{i}-V_{, l k}^{i}=-R_{s k l}^{i} V^{s}$.\n\n8.17 For an arbitrary second-order contravariant tensor $\\left(T^{i j}\\right)$, show that\n\n$$\nT_{, k l}^{i j}-T_{, l k}^{i j}=-R_{s k l}^{i} T^{s j}-R_{s k l}^{j} T^{i s}\n$$\n\n[Hint: Lower superscripts and use Problem 8.3.]\n\n8.18 For an arbitrary mixed tensor $\\left(T_{j}^{i}\\right)$ show that\n\n$$\nT_{j, k l}^{i}-T_{j, l k}^{i}=-R_{s k l}^{i} T_{j}^{s}+R_{j k l}^{s} T_{s}^{i}\n$$\n\n8.19 Verify the symmetry properties (8.6) for the $G_{i j k l}[\\operatorname{see}(8.7)]$ and for the $W_{i j k l}[\\operatorname{see}(8.9)]$.\n\n8.20 Derive (8.5) from (8.4). [Hint: It is helpful to adopt the notation $g_{i j k l}$ for $\\partial^{2} g_{i j} / \\partial x^{k} \\partial x^{l}$.]\n\n8.21 List the independent (nonzero) components of $R_{i j k i}$ when $n=4$ and verify Theorem 8.2 for this case.\n\n8.22 Calculate the Riemannian curvature K for the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 x^{1}\\left(d x^{2}\\right)^{2}$.\n\n8.23 Confirm that $\\mathrm{K}=0$ for the Euclidean metric of polar coordinates,\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}\n$$\n\n$(a)$ by a calculation; $(b)$ by noting that $\\mathrm{K}$ is an invariant.\n\n8.24 Rework Example 8.4 for the pairs $(a) \\mathbf{U}_{(1)}=(1,0,1), \\mathbf{V}_{(1)}=(1,1,1)$ and $(b) \\mathbf{U}_{(2)}=(0,1,0), \\mathbf{V}_{(2)}=$ $(2,1,2)$. (c) Explain why the answers should be the same for $(a)$ and $(b)$.\n\n8.25 Let the surface of the 3 -sphere of radius $a$ be metrized by setting $x^{1}=a$ in spherical coordinates and then allowing $x^{1}, x^{2}$ to replace $x^{2}, x^{3}$, respectively:\n\n$$\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a \\sin x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}\n$$\n\nDetermine $\\mathrm{K}$ for this non-Euclidean $\\mathbf{R}^{2}$.\n\n8.26 If the metric for Riemannian $\\mathbf{R}^{3}$ is given by\n\n$$\ng_{11}=f\\left(x^{2}\\right) \\quad g_{22}=g\\left(x^{2}\\right) \\quad g_{33}=h\\left(x^{2}\\right)\n$$\n\nand $g_{i j}=0$ for $i \\neq j$, write explicit formulas for $(a) \\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right),(b) R$.\n\n8.27 Specialize the results of Problem 8.26 to the case $f\\left(x^{2}\\right) \\equiv g\\left(x^{2}\\right) \\equiv h\\left(x^{2}\\right)$.\n\n8.28 Find the isotropic points for the Riemannian metric\n\n$$\nd s^{2}=\\left(\\ln x^{2}\\right)\\left(d x^{1}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{2}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{3}\\right)^{2} \\quad\\left(x^{2}>1\\right)\n$$\n\nand find the curvature $\\mathrm{K}$ at those points. [Hint: Use Problem 8.27.]\n\n8.29 Show that $\\mathbf{R}^{3}$ under the metric\n\n$$\ng_{11}=e^{x^{2}} \\quad g_{22}=1 \\quad g_{33}=e^{x^{2}} \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nhas constant Riemannian curvature with all points isotropic, and find that curvature.\n\n8.30 Show that in a Riemannian 2-space [for which (8.11) holds]: (a) $R_{i j}=-g_{i j} \\mathrm{~K},(b) R_{j}^{i}=-\\delta_{j}^{i} \\mathrm{~K}$, and (c) $R=-2 \\mathrm{~K}$.\n\n8.31 Calculate the Ricci tensor $R_{i j}$ for Problem 8.13 using (8.16), and compare your answers with those obtained earlier.\n\n8.32 Use Problem 8.30 to calculate the Ricci tensors of both kinds and the curvature invariant for the spherical metric of Problem 8.25.\n\n8.33 Calculate the Ricci tensors of both kinds and the curvature invariant for the (hyperbolic) metric of Problem 8.12. [Hint: Problem 8.27 can be used to good advantage here.]\n\n8.34 Prove that for any tensor $\\left(T^{i j}\\right)$, symmetric or not, $T_{, i j}^{i j}=T_{, j i}^{i j}$. [Hint: Use Problem 8.17 and the symmetry of the Ricci tensor.]\n\n8.35 Is identical vanishing equivalent for the Riemannian curvature and the Ricci curvature invariant? Can you find an example where one is zero everywhere but not the other?\n\n8.36 Is constancy in space equivalent for the two curvatures $\\mathrm{K}$ and $R$ ?\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 9}", "\\section*{Spaces of Constant Curvature; Normal Coordinates}\n\\subsection*{9.1 ZERO CURVATURE AND THE EUCLIDEAN METRIC}\nA fundamental question has run unanswered through preceding chapters: How can one tell whether a given metrization of $\\mathbf{R}^{n}$ is Euclidean or not? To be sure that the meaning of \"Euclidean\" is clear, let us make the formal\n\nDefinition 1: A Riemannian metric $\\mathbf{g}=\\left(g_{i j}\\right)$, specified in a coordinate system $\\left(x^{i}\\right)$, is the Euclidean metric if, under some permissible coordinate transformation (3.1), $\\overline{\\mathbf{g}}=\\left(\\delta_{i j}\\right)$.\n\nNow, a coordinate system $\\left(\\bar{x}^{i}\\right.$ ) in which $\\bar{g}_{i j}=\\delta_{i j}$ is (by Definition 1 of Chapter 3) a rectangular system. Hence our question amounts to: Does a given Riemannian space admit rectangular coordinates or does it not?\n\nSuppose that a rectangular system $\\left(\\bar{x}^{i}\\right)$ does exist. Then $\\overline{\\mathrm{K}}=0$, since all Christoffel symbols vanish in $\\left(\\bar{x}^{i}\\right)$. But Riemannian curvature is an invariant, so that $\\mathrm{K}=0$ in the original coordinates $\\left(x^{i}\\right)$ as well. Moreover, by invariance,\n\n$$\ng_{i j} U^{i} U^{j}=\\bar{U}^{i} \\bar{U}^{i} \\geqq 0\n$$\n\nThus, the necessity part of the following theorem is immediate.\n\nTheorem 9.1: A Riemannian metric $\\left(g_{i j}\\right)$ is the Euclidean metric if and only if the Riemannian curvature $\\mathrm{K}$ is zero at all points and the metric is positive definite.\n\nTo prove the sufficiency portion, we set up a system of first-order partial differential equations for $n$ rectangular coordinates $\\bar{x}^{i}$ as functions of the given coordinates $x^{j}(j=1,2, \\ldots, n)$. The system that immediately comes to mind (Theorem 5.2) is $G=J^{T} J$, or\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}^{k}}{\\partial x^{i}} \\frac{\\partial \\bar{x}^{k}}{\\partial x^{j}}=g_{i j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\tag{9.1}\n\\end{equation*}\n\n\nBut (9.1) is generally intractable because of its nonlinearity. Instead, we select the linear system that results when barred and unbarred coordinates are interchanged in (6.6) and then the $\\bar{\\Gamma}_{j k}^{i}$ are equated to zero:\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\bar{x}^{k}}{\\partial x^{i} \\partial x^{j}}=\\Gamma_{i j}^{r}(\\mathbf{x}) \\frac{\\partial \\bar{x}^{k}}{\\partial x^{r}} \\tag{9.2}\n\\end{equation*}\n\n\nSetting $w \\equiv \\bar{x}^{k}$ and $u_{i} \\equiv \\partial \\bar{x}^{k} / \\partial x^{i}$ yields the desired first-order system\n\n\n\\begin{align*}\n& \\frac{\\partial w}{\\partial x^{i}}=u_{i} \\\\\n& \\frac{\\partial u_{i}}{\\partial x^{j}}=\\Gamma_{i j}^{r} u_{r} \\tag{9.3}\n\\end{align*}\n\n\nEXAMPLE 9.1 It is proved in Problems 9.7 and 9.8 that when $\\mathrm{K} \\equiv 0,(9.3)$ is solvable for a coordinate system $\\left(\\bar{x}^{k}\\right)$ for which all $\\bar{g}_{i j}$ are constants (i.e., all $\\bar{\\Gamma}_{j k}^{i}=0$ ); from these coordinates, rectangular coordinates can be reached, provided $\\left(g_{i j}\\right)$ is positive definite. To make these results plausible, consider the two-dimensional metric\n\n$$\ng_{11}=1 \\quad g_{12}=g_{21}=0 \\quad g_{22}=\\left(x^{2}\\right)^{2}\n$$\n\nThis metric is obviously positive definite and, because the only nonvanishing Christoffel symbol is $\\Gamma_{22}^{2}=1 / x^{2}$, it has $R_{1212}=0=\\mathrm{K}$. It is possible to solve (9.1) directly for the corresponding cartesian coordinates, and then to verify that that solution is contained in the general solution to (9.3).\n\nIntroduce the notation\n\n\n\\begin{equation*}\nf_{1} \\equiv \\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} \\quad f_{2} \\equiv \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} \\quad f_{3} \\equiv \\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} \\quad f_{4} \\equiv \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}} \\tag{1}\n\\end{equation*}\n\n\nwhereby (9.1) becomes the algebraic system\n\n\n\\begin{align*}\nf_{1}^{2}+f_{3}^{2} & =1 \\\\\nf_{1} f_{2}+f_{3} f_{4} & =0  \\tag{2}\\\\\nf_{2}^{2}+f_{4}^{2} & =\\left(x^{2}\\right)^{2}\n\\end{align*}\n\n\nSystem (2) can be solved for three of the $f_{i}$ in terms of the fourth-say, $f_{1}$ :\n\n\n\\begin{equation*}\nf_{1}=f_{1} \\quad f_{2}=x^{2} \\sqrt{1-f_{1}^{2}} \\quad f_{3}=-\\sqrt{1-f_{1}^{2}} \\quad f_{4}=x^{2} f_{1} \\tag{3}\n\\end{equation*}\n\n\nNow (1) becomes two simple first-order systems in $\\bar{x}^{1}$ alone and $\\bar{x}^{2}$ alone:\n\n$$\n\\text { I: }\\left\\{\\begin{array} { l } \n{ \\frac { \\partial \\overline { x } ^ { 1 } } { \\partial x ^ { 1 } } = f _ { 1 } } \\\\\n{ \\frac { \\partial \\overline { x } ^ { 1 } } { \\partial x ^ { 2 } } = x ^ { 2 } \\sqrt { 1 - f _ { 1 } ^ { 2 } } }\n\\end{array} \\quad \\text { and } \\quad \\text { II: } \\left\\{\\begin{array}{l}\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}}=-\\sqrt{1-f_{1}^{2}} \\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}}=x^{2} f_{1}\n\\end{array}\\right.\\right.\n$$\n\nThe unknown function $f_{1}$ is determined by the requirements that the two equations I and the two equations II both be compatible:\n\n$$\n\\frac{\\partial f_{1}}{\\partial x^{2}}=\\frac{\\partial}{\\partial x^{1}}\\left(x^{2} \\sqrt{1-f_{1}^{2}}\\right) \\quad \\text { and } \\quad \\frac{\\partial}{\\partial x^{2}}\\left(-\\sqrt{1-f_{1}^{2}}\\right)=\\frac{\\partial}{\\partial x^{1}}\\left(x^{2} f_{1}\\right)\n$$\n\nThe only function satisfying these two compatibility conditions is\n\n$$\nf_{1}=\\text { const. }=\\cos \\phi\n$$\n\nand I and II immediately integrate to give\n\n\n\\begin{align*}\n& \\bar{x}^{1}=x^{1} \\cos \\phi+\\frac{1}{2}\\left(x^{2}\\right)^{2} \\sin \\phi+c \\\\\n& \\bar{x}^{2}=-x^{1} \\sin \\phi+\\frac{1}{2}\\left(x^{2}\\right)^{2} \\cos \\phi+d \\tag{4}\n\\end{align*}\n\n\nWe are, of course, free to set $\\phi=c=d=0$ in (4).\n\nTurning to $(9.3)$, we have to solve\n\n$$\n\\begin{aligned}\n& \\text { (1) } \\frac{\\partial w}{\\partial x^{1}}=u_{1}, \\quad \\frac{\\partial w}{\\partial x^{2}}=u_{2} \\\\\n& \\begin{array}{ll}\n\\text { (2) } \\frac{\\partial u_{1}}{\\partial x^{1}}=0, \\frac{\\partial u_{1}}{\\partial x^{2}}=0 & \\text { (3) } \\frac{\\partial u_{2}}{\\partial x^{1}}=0, \\frac{\\partial u_{2}}{\\partial x^{2}}=u_{2} \\Gamma_{22}^{2}=\\frac{u_{2}}{x^{2}}\n\\end{array}\n\\end{aligned}\n$$\n\nNote that these equations include their own compatibility conditions! For instance, the second equation (2) and the first equation (3) ensure the compatibility of the two equations (1). The fact that system (9.3) is automatically compatible whenever $\\mathrm{K}=0$ is crucial to the proof of Theorem 9.1. Integrating the above equations in the order (3)-(2)-(1), we get:\n\n$$\nw=a_{1} x^{1}+a_{2}\\left(x^{2}\\right)^{2}+a_{3} \\quad\\left(a_{1}, a_{2}, a_{3}=\\text { const. }\\right)\n$$\n\nor, replacing the index $k$,\n\n\n\\begin{equation*}\n\\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\\left(x^{2}\\right)^{2}+a_{3}^{k} \\quad\\left(a_{i}^{k}=\\text { const. }\\right) \\tag{5}\n\\end{equation*}\n\n\nAs announced, (5) includes (4).\n\nFor subsequent use, the following compatibility theorem for quasilinear systems [which include linear systems such as (9.3)] is stated here, without proof:\n\nTheorem 9.2: The quasilinear first-order system\n\n$$\n\\frac{\\partial u_{\\lambda}}{\\partial x^{j}}=F_{\\lambda j}\\left(u_{0}, u_{1}, \\ldots, u_{m}, x^{1}, x^{2}, \\ldots, x^{n}\\right) \\quad(\\lambda=0,1, \\ldots, m ; j=1,2, \\ldots, n)\n$$\n\nwhere the functions $F_{\\lambda j}$ are of differentiability class $C^{1}$, has a nontrivial solution for the $u_{\\lambda}$, bounded over some region of $\\mathbf{R}^{n}$, if and only if\n\n$$\n\\frac{\\partial F_{\\lambda j}}{\\partial u_{\\nu}} F_{\\nu k}+\\frac{\\partial F_{\\lambda j}}{\\partial x^{k}}=\\frac{\\partial F_{\\lambda k}}{\\partial u_{\\nu}} F_{\\nu j}+\\frac{\\partial F_{\\lambda k}}{\\partial x^{j}} \\quad(\\lambda=0,1, \\ldots, m ; 1 \\leqq j<k \\leqq n)\n$$\n\n[The $\\nu$-summations run from 0 to $m$.]\n\n\\subsection*{9.2 FLAT RIEMANNIAN SPACES}\nA Riemannian space, or the determining metric, is termed flat if there is a transformation of coordinates $\\bar{x}^{i}=\\bar{x}^{i}(\\mathbf{x})$ that puts the metric into the standard form\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=\\varepsilon_{1}\\left(d \\bar{x}^{1}\\right)^{2}+\\varepsilon_{2}\\left(d \\bar{x}^{2}\\right)^{2}+\\cdots+\\varepsilon_{n}\\left(d \\bar{x}^{n}\\right)^{2} \\tag{9.4}\n\\end{equation*}\n\n\nwhere $\\varepsilon_{i}= \\pm 1$ for each $i$. This condition generalizes the concept of the Euclidean metric. The essential distinction between the two concepts revolves about positive-definiteness; the analogue to Theorem 9.1 with positive-definiteness removed is:\n\nTheorem 9.3: A Riemannian space is flat if and only if $\\mathrm{K}=0$ at all points.\n\nCorollary 9.4: If $\\mathrm{K}=0$, then $R=0$.\n\nProof: If $\\mathrm{K}=0$, then by Theorem 9.3 the space is flat, and hence the $\\bar{g}_{i j}$ are constant for some coordinate system $\\left(\\bar{x}^{i}\\right)$. It follows that all $\\bar{\\Gamma}_{i j k}, \\bar{\\Gamma}_{j k}^{i}, \\bar{R}_{j k l}^{i}, \\bar{R}_{i j}$, and $\\bar{R}_{j}^{i}$ vanish. Therefore, $\\bar{R}=\\bar{R}_{i}^{i}=0$, and since Ricci curvature is invariant, $R=0$.\n\nRemark 1: Problem 8.35 shows that the converse of Corollary 9.4 does not hold.\n\nEXAMPLE 9.2 Consider the Riemannian metric\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2}+4\\left(x^{3}\\right)^{2}\\left(d x^{3}\\right)^{2}-4\\left(x^{4}\\right)^{2}\\left(d x^{4}\\right)^{2}\n$$\n\n(a) Calculate the Riemannian curvature. (b) Find a solution of system (9.3) from which it may be inferred that the space is flat.\n\n(a) Using Problem 6.4, we find as the nonvanishing Christoffel symbols\n\n$$\n\\Gamma_{22}^{2}=\\frac{1}{x^{2}} \\quad \\Gamma_{33}^{3}=\\frac{1}{x^{3}} \\quad \\Gamma_{44}^{4}=\\frac{1}{x^{4}}\n$$\n\nBecause $\\Gamma_{j k}^{i}=0$ unless $i=j=k$, the partial-derivative terms drop out of (8.2), leaving\n\n$$\nR_{j k l}^{i}=\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i}=\\Gamma_{i i}^{i} \\Gamma_{i i}^{i}-\\Gamma_{i i}^{i} \\Gamma_{i i}^{i}=0 \\quad \\text { (not summed) }\n$$\n\nwhich in turn implies that $R_{i j k l}=0$ and $\\mathrm{K}=0$.\n\n(b) For the above-calculated Christoffel symbols\n\n$$\n\\frac{\\partial u_{1}}{\\partial x^{1}}=0 \\quad \\frac{\\partial u_{2}}{\\partial x^{2}}=\\frac{u_{2}}{x^{2}} \\quad \\frac{\\partial u_{3}}{\\partial x^{3}}=\\frac{u_{3}}{x^{3}} \\quad \\frac{\\partial u_{4}}{\\partial x^{4}}=\\frac{u_{4}}{x^{4}}\n$$\n\nwith $\\partial u_{i} / \\partial x_{j}=0$ for $i \\neq j$. Integrating,\n\n$$\nu_{1}=f_{1}\\left(x^{2}, x^{3}, x^{4}\\right) \\quad u_{2}=x^{2} f_{2}\\left(x^{1}, x^{3}, x^{4}\\right) \\quad u_{3}=x^{3} f_{3}\\left(x^{1}, x^{2}, x^{4}\\right) \\quad u_{4}=x^{4} f_{4}\\left(x^{1}, x^{2}, x^{3}\\right)\n$$\n\nfor arbitrary functions $f_{i}$. But the remaining equations (9.3), $\\partial w / \\partial x^{i}=u_{i}$, give rise to the compatibility relations\n\n$$\n\\frac{\\partial u_{i}}{\\partial x^{j}}=\\frac{\\partial u_{j}}{\\partial x^{i}}\n$$\n\nwhich are satisfied only if $f_{i}=c_{i}=$ const. Therefore,\n\n$$\nw=a_{1} x^{1}+a_{2}\\left(x^{2}\\right)^{2}+a_{3}\\left(x^{3}\\right)^{2}+a_{4}\\left(x^{4}\\right)^{2}+a_{5}\n$$\n\nand the transformation must be of the general form\n\n$$\n\\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\\left(x^{2}\\right)^{2}+a_{3}^{k}\\left(x^{3}\\right)^{2}+a_{4}^{k}\\left(x^{4}\\right)^{2}+a_{5}^{k} \\quad\\left(a_{i}^{k} \\text { constants }\\right)\n$$\n\nWe wish to specialize the constants so that the covariant law $G=J^{T} \\bar{G} J$ will hold, with $\\bar{G}$ corresponding to (9.4). As a preliminary guess, set\n\n$$\n\\left[a_{i}^{k}\\right]_{45}=\\left[\\begin{array}{ccccc}\nb_{1} & 0 & 0 & 0 & 0 \\\\\n0 & b_{2} & 0 & 0 & 0 \\\\\n0 & 0 & b_{3} & 0 & 0 \\\\\n0 & 0 & 0 & b_{4} & 0\n\\end{array}\\right]\n$$\n\nso that the covariant law becomes\n\n$$\n\\left[\\begin{array}{llll}\n1 & & & \\\\\n& 4\\left(x^{2}\\right)^{2} & & \\\\\n& & 4\\left(x^{3}\\right)^{2} & \\\\\n& & & -4\\left(x^{4}\\right)^{2}\n\\end{array}\\right]=\\left[\\begin{array}{llll}\nb_{1} & & & \\\\\n& 2 b_{2} x^{2} & & \\\\\n& & 2 b_{3} x^{3} & \\\\\n& & & 2 b_{4} x^{4}\n\\end{array}\\right]\\left[\\begin{array}{llll}\n\\varepsilon_{1} & & & \\\\\n& \\varepsilon_{2} & & \\\\\n& & \\varepsilon_{3} & \\\\\n& & & \\varepsilon_{4}\n\\end{array}\\right]\\left[\\begin{array}{llll}\nb_{1} & & \\\\\n& 2 b_{2} x^{2} & & \\\\\n& & 2 b_{3} x^{3} & \\\\\n& & & 2 b_{4} x^{4}\n\\end{array}\\right]\n$$\n\nBy inspection, the choice $b_{1}=b_{2}=b_{3}=b_{4}=1$ will render $\\varepsilon_{1}=\\varepsilon_{2}=\\varepsilon_{3}=-\\varepsilon_{4}=1$.\n\nIn connection with (9.4) there is an interesting theorem (Sylvester's law of inertia). Define as the signature of a flat metric $\\left(g_{i j}\\right)$ the ordered $n$-tuple\n\n$$\n\\left(\\operatorname{sgn} \\varepsilon_{1}, \\operatorname{sgn} \\varepsilon_{2}, \\ldots, \\operatorname{sgn} \\varepsilon_{n}\\right)\n$$\n\ncomposed of the signs of the coefficients in the standard form (i.e., the signs of $\\bar{g}_{11}, \\ldots, \\bar{g}_{n n}$ ).\n\nTheorem 9.5: The signature of a flat metric is uniquely determined up to order.\n\n\\subsection*{9.3 NORMAL COORDINATES}\nIt is possible to introduce local, quasirectangular coordinates in Riemannian space the use of which greatly simplifies the proofs of certain complicated tensor identities.\n\nLet $O$ denote an arbitrary point of $\\mathbf{R}^{n}$, and $\\mathbf{p}=\\left(p^{i}\\right)$ an arbitrary direction (unit vector) at $O$. Assuming a positive-definite metric, consider the differential equations for geodesics,\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d s} \\frac{d x^{k}}{d s}=0 \\tag{9.5}\n\\end{equation*}\n\n\n[cf. (7.13)], along with initial conditions\n\n\n\\begin{equation*}\n\\left.\\frac{d x^{i}}{d s}\\right|_{s=0}=p^{i} \\tag{9.6}\n\\end{equation*}\n\n\nHere the arc-length parameter is chosen to make $s=0$ at $O$.\n\nRemark 2: Under an indefinite metric, there could exist directions at $O$ in which arc length could not be defined; see, e.g., Problem 7.22. There would then be no hope of satisfying (9.6) with $\\left(p^{i}\\right)$ arbitrary.\n\nIt can be shown that for a given $\\mathbf{p}$, the system (9.5)-(9.6) has a unique solution; moreover, for each point $P$ in some neighborhood $\\mathcal{N}$ of $O$, there is a unique choice of direction $\\mathbf{p}$ at $O$ such that the solution curve $x^{i}=x^{i}(s)$ (a geodesic) passes through $P$. Accordingly, for each $P$ in $\\mathcal{N}$, take as the coordinates of $P$\n\n\n\\begin{equation*}\ny^{i}=s p^{i} \\tag{9.7}\n\\end{equation*}\n\n\nwhere $s$ is the distance along the geodesic from $O$ to $P$. The numbers $\\left(y^{i}\\right)$ are called the normal coordinates (or geodesic or Riemannian coordinates) of $P$.\n\nEXAMPLE 9.3 Show that if the Riemannian metric $d s^{2}=g_{i i} d x^{i} d x^{j}$ for $\\mathbf{R}^{2}$ is Euclidean and there is a point $O$\\\\\nat which $g_{12}=0$, then normal coordinates $\\left(y^{i}\\right)$ with origin $O$ are constant multiples of $\\left(z^{i}\\right)$, for some rectangular coordinate system $\\left(z^{i}\\right)$.\n\nBecause $g_{12}=0$ at point $O$, the vectors $\\mathbf{T}=\\left(1 / \\sqrt{g_{11}}, 0\\right)$ and $\\mathbf{S}=\\left(0,1 / \\sqrt{g_{22}}\\right)$ are, at $O$, an orthonormal pair. The space, being Euclidean, admits a rectangular coordinate system; in particular, a system ( $z^{i}$ ) with origin $O$ and unit vectors $\\mathbf{T}$ and $\\mathbf{S}$ (Fig. 9-1). Again because the space is Euclidean, the straight line segment $O P$ is the unique geodesic connecting $O$ with the arbitrary point $P$. With $s=\\overline{O P}$ and $\\mathbf{p}$ the direction vector of $O P$, we have the vector equation\n\n$$\nz^{1} \\mathbf{T}+z^{2} \\mathbf{S}=s \\mathbf{p}\n$$\n\nor componentwise,\n\n$$\nz^{1}\\left(\\frac{1}{\\sqrt{g_{11}}}\\right)=s p^{1} \\equiv y^{1} \\quad \\text { and } \\quad z^{2}\\left(\\frac{1}{\\sqrt{g_{22}}}\\right)=s p^{2} \\equiv y^{2}\n$$\n\nQED.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-127}\n\\end{center}\n\nFig. 9-1\n\nThe chief value of Riemannian coordinates resides in the following theorem (Problem 9.10).\n\nTheorem 9.6: If the metric tensor $\\left(g_{i j}\\right)$ is positive definite, then, at the origin of a Riemannian coordinate system $\\left(y^{i}\\right)$, all $\\partial g_{i j} / \\partial y^{k}, \\partial g^{i j} / \\partial y^{k}, \\Gamma_{i j k}$, and $\\Gamma_{j k}^{i}$ are zero.\n\nRemark 3: Recall that neither the partial derivatives of the metric tensor nor the Christoffel symbols are tensorial. Thus, their $\\left(y^{i}\\right)$-representations can vanish at $O$ without their $\\left(x^{i}\\right)$-representations doing so. For instance, because the transformation between $\\left(x^{i}\\right)$ and $\\left(y^{i}\\right)$ has $J=I$ at $O,(6.5)$ gives:\n\n$$\n\\left.\\Gamma_{j k}^{i}(\\mathbf{x})\\right|_{O}=\\left.\\frac{\\partial^{2} y^{i}}{\\partial x^{j} \\partial x^{k}}\\right|_{O}\n$$\n\nThe right side is generally nonzero, unless the coordinate transformation happens to be linear.\n\nEXAMPLE 9.4 Prove Bianchi's first identity, $R_{i j k l}+R_{i k l j}+R_{i l j k}=0$.\n\nTheorem 9.6 implies that at $O$, the origin of normal coordinates,\n\n$$\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial y^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial y^{l}}\n$$\n\nIf we use the notation $\\Gamma_{i j k l}$ for $\\partial \\Gamma_{i j k} / \\partial y^{l}$, for arbitrary $i, j, k, l$, then\n\n$$\n\\begin{aligned}\n& R_{i j k l}=\\Gamma_{j l i k}-\\Gamma_{j k i l} \\\\\n& R_{i k l j}=\\Gamma_{k j i l}-\\Gamma_{k l i j} \\\\\n& R_{i l j k}=\\Gamma_{l k i j}-\\Gamma_{l j i k}\n\\end{aligned}\n$$\n\nOn summing these three relations and observing the cancellations which take place, we see that the desired identity holds at $O$ in the coordinates $\\left(y^{i}\\right)$. This tensor identity must therefore remain valid at $O$ in the alias coordinates $\\left(x^{i}\\right)$. But $O$ is any point of $\\mathbf{R}^{n}$, and the proof is complete.\n\nEXAMPLE 9.5 Prove Bianchi's second identity,\n\n\n\\begin{equation*}\nR_{i j k l, u}+R_{i j l u, k}+R_{i j u k, l}=0 \\tag{9.8}\n\\end{equation*}\n\n\nWorking with the Riemann tensor of the second kind, we have, at the origin $O$ of normal coordinates,\n\n$$\n\\begin{aligned}\nR_{j k l, u}^{i} & =\\frac{\\partial R_{j k l}^{i}}{\\partial y^{u}}=\\frac{\\partial}{\\partial y^{u}}\\left(\\frac{\\partial \\Gamma_{j l}^{i}}{\\partial y^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial y^{l}}+\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i}\\right) \\\\\n& =\\Gamma_{j l k u}^{i}-\\Gamma_{j k l u}^{i}\n\\end{aligned}\n$$\n\nsince terms like $\\left(\\partial \\Gamma_{j l}^{r} / \\partial y^{u}\\right) \\Gamma_{r k}^{i}$ vanish along with the $\\Gamma_{r k}^{i}$ at $O$. From this, permutation of subscripts yields\n\n$$\nR_{j k l, u}^{i}+R_{j l u, k}^{i}+R_{j u k, l}^{i}=0\n$$\n\nat $O$, and the validity of $(9.8)$ at $O$ follows from the fact that covariant differentiation commutes with the lowering of a superscript (Problem 6.11). We conclude, as in Example 9.4, that (9.8) holds generally in $\\left(x^{i}\\right)$.\n\nA positive definite metric has been tacitly assumed, both here and in Example 9.4. The assumption can be dropped; see Problem 9.13.\n\n\\subsection*{9.4 SCHUR'S THEOREM}\nFrom Chapter 8 it is known that although every point of Riemannian two-space is isotropic, the curvature ( $=R_{1212} / g$ ) can still vary from one isotropic point to the next. However, Problems 8.11, $8.12,8.28$, and 8.29 suggest that a different situation prevails in $\\mathbf{R}^{3}$. To prove the general theorem, known as Schur's theorem, it is necessary to establish a preliminary result, a generalization of (8.11).\n\nLemma 9.7: At an isotropic point of $\\mathbf{R}^{n}$ the Riemannian curvature is given by\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{R_{a b c d}}{g_{a c} g_{b d}-g_{a d} g_{b c}} \\equiv \\frac{R_{a b c d}}{G_{a b c d}} \\tag{9.9}\n\\end{equation*}\n\n\nfor any specific subscript string such that $G_{a b c d} \\neq 0$. [If $G_{a b c d}=0$, then $R_{a b c d}=0$ also.] For a proof, see Problem 9.8.\n\nTheorem 9.8 (Schur's Theorem): If all points in some neighborhood $\\mathcal{N}$ in a Riemannian $\\mathbf{R}^{n}$ are isotropic and $n \\geqq 3$, then $\\mathrm{K}$ is constant throughout that neighborhood.\n\nFor a proof, see Problem 9.14.\n\n\\subsection*{9.5 THE EINSTEIN TENSOR}\nThe Einstein tensor is defined in terms of the Ricci tensor $R_{i j}$ and the curvature invariant $R$ (Section 8.4):\n\n\n\\begin{equation*}\nG_{j}^{i} \\equiv R_{j}^{i}-\\frac{1}{2} \\delta_{j}^{i} R \\tag{9.10}\n\\end{equation*}\n\n\nIt is clear that $\\left(G_{j}^{i}\\right)$ is in fact a mixed tensor of order two.\n\nAs a direct generalization of the notion of the divergence of a vector field $\\mathbf{V}=\\left(V^{i}\\right)$ relative to\\\\\nrectangular coordinates $\\left(x^{i}\\right)$,\n\n$$\n\\operatorname{div} \\mathbf{V}=\\frac{\\partial V^{1}}{\\partial x^{1}}+\\frac{\\partial V^{2}}{\\partial x^{2}}+\\cdots+\\frac{\\partial V^{n}}{\\partial x^{n}} \\equiv \\frac{\\partial V^{r}}{\\partial x^{r}}\n$$\n\nwe define the divergence of the general tensor $\\mathbf{T}=\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p} \\ldots i_{p}}\\right)$ with respect to its $k$ th contravariant index to be the tensor\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{T} \\equiv\\left(T_{j_{1} j_{2} \\ldots j_{q}, r}^{i_{1} i_{2} \\ldots r, i_{p}}\\right) \\tag{9.11}\n\\end{equation*}\n\n\nIn Problem 9.15 is proved\n\nTheorem 9.9: For any Riemannian metric, the divergence of the Einstein tensor is zero at all points.\n\n\\section*{Solved Problems}\n\\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}\n9.1 Test the compatibility conditions (Theorem 9.2) for the system\n\n\n\\begin{equation*}\n\\frac{\\partial u_{0}}{\\partial x^{1}}=\\frac{u_{0}}{x^{1}} \\quad \\frac{\\partial u_{0}}{\\partial x^{2}}=2 x^{2} u_{0} \\tag{1}\n\\end{equation*}\n\n\nIf it is compatible, solve the system.\n\nIn the notation of Theorem 9.2, there is only the condition corresponding to $\\lambda=0, j=1, k=2$ to be satisfied.\n\n$$\n\\begin{gathered}\n\\frac{\\partial F_{01}}{\\partial u_{0}} F_{02}+\\frac{\\partial F_{01}}{\\partial x^{2}} \\stackrel{?}{=} \\frac{\\partial F_{02}}{\\partial u_{0}} F_{01}+\\frac{\\partial F_{02}}{\\partial x^{1}} \\\\\n\\frac{\\partial}{\\partial u_{0}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\cdot 2 x^{2} u_{0}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\stackrel{?}{=} \\frac{\\partial}{\\partial u_{0}}\\left(2 x^{2} u_{0}\\right) \\cdot \\frac{u_{0}}{x^{1}}+\\frac{\\partial}{\\partial x^{1}}\\left(2 x^{2} u_{0}\\right) \\\\\n\\frac{2 x^{2} u_{0}}{x^{1}}=\\frac{2 x^{2} u_{0}}{x^{1}}\n\\end{gathered}\n$$\n\nTherefore, the system is compatible. The first equation (1) integrates to $u_{0}=x^{1} \\phi\\left(x^{2}\\right)$; the second equation then gives\n\n$$\nx^{1} \\phi^{\\prime}=2 x^{2} x^{1} \\phi \\quad \\text { whence } \\quad \\phi=c \\exp \\left(x^{2}\\right)^{2}\n$$\n\nHence the solution of (1) is $u_{0}=c x^{1} \\exp \\left(x^{2}\\right)^{2}$.\n\n9.2 Show that $\\mathbf{R}^{3}$ under the metric $d s^{2}=\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{1}\\right)^{2}+\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}$ is Euclidean.\n\nThis metric has $g_{33}=$ const., and $g_{11}$ and $g_{22}$ independent of $x^{3}$. Problem 6.4 then shows that $\\Gamma_{j k}^{i}=0$ whenever $i, j$, or $k$ equals 3 ; consequently, of the six independent components of the Riemann tensor, only $R_{1212}$ is possibly nonzero. But (from Problem 6.4), with $z \\equiv\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}$,\n\n$$\n\\begin{array}{lll}\n\\Gamma_{11}^{1}=\\frac{x^{1}}{z} & \\Gamma_{12}^{1}=\\Gamma_{21}^{1}=\\frac{x^{2}}{z} & \\Gamma_{22}^{1}=-\\frac{x^{1}}{z} \\\\\n\\Gamma_{11}^{2}=-\\frac{x^{2}}{z} & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{z} & \\Gamma_{22}^{2}=\\frac{x^{2}}{z}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}+\\Gamma_{22}^{2} \\Gamma_{21}^{1}-\\Gamma_{21}^{1} \\Gamma_{12}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& \\left.=\\frac{-z+x^{1}\\left(2 x^{1}\\right)}{z^{2}}-\\frac{z-x^{2}\\left(2 x^{2}\\right)}{z^{2}}+\\left(-\\frac{x^{1}}{z}\\right)\\left(\\frac{x^{1}}{z}\\right)+\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right)-\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right) \\right\\rvert\\,-\\frac{x^{1}}{z}\\left(-\\frac{x^{1}}{z}\\right)=0\n\\end{aligned}\n$$\n\nConsequently, $R_{1212}=0=\\mathrm{K}$. As the metric is clearly positive definite, Theorem 9.1 implies that the space is Euclidean.\n\n9.3 For the Euclidean space of Problem 9.2, exhibit a transformation from the given coordinate system $\\left(x^{i}\\right)$ to a rectangular system $\\left(\\bar{x}^{i}\\right)$.\n\nUsing the Christoffel symbols as calculated in Problem 9.2, we obtain from (9.3) the following system for the $u_{i}$ :\n\n\\[\n\\begin{array}{lll}\n\\frac{\\partial u_{1}}{\\partial x^{1}}=\\frac{x^{1} u_{1}-x^{2} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{2}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{2}}{\\partial x^{1}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{2}}=\\frac{-x^{1} u_{1}+x^{2} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{3}}{\\partial x^{1}}=0 & \\frac{\\partial u_{3}}{\\partial x^{2}}=0 & \\frac{\\partial u_{3}}{\\partial x^{3}}=0 \\tag{3}\n\\end{array}\n\\]\n\nThus $u_{1}$ and $u_{2}$ are functions of $x^{1}, x^{2}$ alone, and $u_{3}=$ const. Since the $g_{i j}$ are all polynomials of degree 2 in $x^{1}, x^{2}$, use the method of undetermined coefficients, assuming polynomial forms\n\n$$\nu_{i}=a_{i}\\left(x^{1}\\right)^{2}+b_{i} x^{1} x^{2}+c_{i}\\left(x^{2}\\right)^{2}+d_{i} x^{1}+e_{i} x^{2}+f_{i} \\quad(i=1,2)\n$$\n\nThe (compatibility) relation $\\partial u_{1} / \\partial x^{2}=\\partial u_{2} / \\partial x^{1}$ implied by the second equation (1) and the first equation (2) requires\n\n$$\nb_{1}=2 a_{2} \\quad 2 c_{1}=b_{2} \\quad e_{1}=d_{2}\n$$\n\nSimilarly, $\\partial u_{1} / \\partial x^{1}=-\\partial u_{2} / \\partial x^{2}$ implies\n\n$$\n2 a_{1}=-b_{2} \\quad b_{1}=-2 c_{2} \\quad d_{1}=-e_{2}\n$$\n\nUsing the first equation (1), or $z\\left(\\partial u_{1} / \\partial x^{1}\\right)=x^{1} u_{1}-x^{2} u_{2}$, we get:\n\n$$\na_{1}=0 \\quad a_{2}=0 \\quad c_{1}=b_{2} \\quad b_{1}=-c_{2} \\quad d_{1}=-e_{2} \\quad f_{1}=0=-f_{2}\n$$\n\nIt follows that $b_{1}=b_{2}=c_{1}=c_{2}=0$, and therefore (renotating $d_{1}$ and $e_{1}$ )\n\n$$\nu_{1}=a x^{1}+b x^{2} \\quad u_{2}=b x^{1}-a x^{2} \\quad u_{3}=c\n$$\n\n[Note: This solution of (1)-(2)-(3) may be obtained by the method of characteristics, without any prior assumptions.]\n\nThe first equations (9.3),\n\n$$\n\\frac{\\partial w}{\\partial x^{1}}=a x^{1}+b x^{2} \\quad \\frac{\\partial w}{\\partial x^{2}}=b x^{1}-a x^{2} \\quad \\frac{\\partial w}{\\partial x^{3}}=c\n$$\n\nmay now be integrated to give\n\n$$\nw=\\frac{a}{2}\\left(x^{1}\\right)^{2}+b x^{1} x^{2}-\\frac{a}{2}\\left(x^{2}\\right)^{2}+c x^{3}+d\n$$\n\nor, replacing $\\bar{x}^{k}$ and corresponding superscripts, and with $d=0$,\n\n$$\n\\bar{x}^{k}=\\frac{a^{k}}{2}\\left(x^{1}\\right)^{2}+b^{k} x^{1} x^{2}-\\frac{a^{k}}{2}\\left(x^{2}\\right)^{2}+c^{k}\\left(x^{2}\\right)^{2}\n$$\n\nIt is clear that we may take $c^{1}=c^{2}=0=a^{3}=b^{3}$ and $c^{3}=1$ :\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\frac{1}{2} a^{1}\\left(x^{1}\\right)^{2}+b^{1} x^{1} x^{2}-\\frac{1}{2} a^{1}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{2}=\\frac{1}{2} a^{2}\\left(x^{1}\\right)^{2}+b^{2} x^{1} x^{2}-\\frac{1}{2} a^{2}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{3}=x^{3}\n\\end{aligned}\n$$\n\nThe Jacobian matrix is\n\n$$\nJ=\\left[\\begin{array}{ccc}\na^{1} x^{1}+b^{1} x^{2} & b^{1} x^{1}-a^{1} x^{2} & 0 \\\\\na^{2} x^{1}+b^{2} x^{2} & b^{2} x^{1}-a^{2} x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nSince $J^{T} J=G$, we must have\n\n$$\n\\left(a^{1}\\right)^{2}+\\left(a^{2}\\right)^{2}=1 \\quad a^{1} b^{1}+a^{2} b^{2}=0 \\quad\\left(b^{1}\\right)^{2}+\\left(b^{2}\\right)^{2}=1\n$$\n\nso take $a^{1}=0, a^{2}=1, b^{2}=0, b^{1}=1$. The transformation is, finally,\n\n$$\n\\bar{x}^{1}=x^{1} x^{2} \\quad \\bar{x}^{2}=\\frac{1}{2}\\left[\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\right] \\quad \\bar{x}^{3}=x^{3}\n$$\n\n\\section*{FLAT RIEMANNIAN SPACES}\n9.4 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n\nSince the metric is not positive definite, it cannot be Euclidean. To determine flatness, it suffices to examine $R_{1212}=g_{11} R_{212}^{1}$. But Problem 6.4 shows that $R_{212}^{1}=0$; hence the space is flat.\n\n9.5 Show that if the metric tensor is constant, the space is flat and the coordinate transformation $\\bar{x}=A x$, where $A$ is a rank- $n$ matrix of eigenvectors of $G=\\left(g_{i j}\\right)$, diagonalizes the metric (i.e., $\\bar{g}_{i j}=0$ if $i \\neq j$ ).\n\nSince all partial derivatives of $g_{i j}$ are zero, all Christoffel symbols will vanish and all $R_{i j k l}=0$, making $\\mathrm{K}=0$. Thus, by Theorem 9.3, the space is flat. By Chapters 2 and 3, if $\\bar{x}=A x$, then $J=A$ and\n\n$$\nG=J^{T} \\bar{G} J=A^{T} \\bar{G} A\n$$\n\nHowever, since $G$ is real and symmetric, its eigenvectors form an orthogonal matrix which we now choose as $A$, with\n\n$$\nA G A^{-1}=A G A^{T}=D \\quad \\text { (diagonal matrix of eigenvalues of } G \\text { ) }\n$$\n\nHence, $\\bar{G}=A G A^{T}=D \\quad$ QED.\n\n9.6 Find the signature of the flat metric\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}-2\\left(d x^{3}\\right)^{2}+2\\left(d x^{4}\\right)^{2}-4 d x^{2} d x^{3}-4 d x^{2} d x^{4}-10 d x^{3} d x^{4}\n$$\n\nis\n\nIn view of Problem 9.5, it suffices to find the eigenvalues $\\lambda$ of $G=\\left(g_{i j}\\right)$. The characteristic equation\n\n$$\n\\begin{aligned}\n|G-\\lambda I| & =\\left|\\begin{array}{cccc}\n4-\\lambda & 0 & 0 & 0 \\\\\n0 & 5-\\lambda & -2 & -2 \\\\\n0 & -2 & -2-\\lambda & -5 \\\\\n0 & -2 & -5 & 2-\\lambda\n\\end{array}\\right| \\\\\n& =(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & -2 & -2 \\\\\n-2 & -2-\\lambda & -5 \\\\\n-2 & -5 & 2-\\lambda\n\\end{array}\\right|=-(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & 2 & 0 \\\\\n-2 & 2+\\lambda & -3+\\lambda \\\\\n-2 & 5 & 7-\\lambda\n\\end{array}\\right| \\\\\n& =-(4-\\lambda)\\left[(5-\\lambda)\\left(29-\\lambda^{2}\\right)+8(5-\\lambda)\\right]=-(4-\\lambda)(5-\\lambda)\\left(37-\\lambda^{2}\\right)=0\n\\end{aligned}\n$$\n\nfrom which the eigenvalues are $\\lambda=+4,+5,+\\sqrt{37},-\\sqrt{37}$. This means that there is a transformation which changes the metric into the form\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}+\\sqrt{37}\\left(d x^{3}\\right)^{2}-\\sqrt{37}\\left(d x^{4}\\right)^{2}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left(d \\bar{x}^{2}\\right)^{2}+\\left(d \\bar{x}^{3}\\right)^{2}-\\left(d \\bar{x}^{4}\\right)^{2}\n$$\n\nwith the obvious change of coordinates. Hence, the signature is $(+++-)$, or some permutation thereof (Theorem 9.5).\n\n9.7 Show that the conditions $R_{i j k l}=0$ are sufficient for the compatibility of (9.3).\n\nIn the notation of Theorem 9.2, (9.3) takes the form (with $m=n$ )\n\n$$\n\\begin{array}{ll}\n\\boldsymbol{\\lambda}=\\mathbf{0} & \\frac{\\partial u_{0}}{\\partial x^{j}}=F_{0 j} \\equiv u_{j} \\\\\n\\lambda>0 & \\frac{\\partial u_{\\lambda}}{\\partial x^{j}}=F_{\\lambda j} \\equiv u_{r} \\Gamma_{\\lambda j}^{r}(\\mathbf{x})\n\\end{array}\n$$\n\nThe corresponding compatibility conditions are\n\n$$\n\\boldsymbol{\\lambda}=\\mathbf{0} \\quad \\delta_{j}^{\\nu} u_{r} \\Gamma_{\\nu k}^{r}=\\delta_{k}^{\\nu} u_{r} \\Gamma_{\\nu j}^{r}\n$$\n\nor $u_{r} \\Gamma_{j k}^{r}=u_{r} \\Gamma_{k j}^{r}$, which holds trivially, and\n\n$$\n\\boldsymbol{\\lambda}>\\mathbf{0} \\quad \\delta_{r}^{\\nu} \\Gamma_{\\lambda j}^{r} u_{s} \\Gamma_{\\nu k}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}=\\delta_{r}^{\\nu} \\Gamma_{\\lambda k}^{r} u_{s} \\Gamma_{\\nu j}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}\n$$\n\nwhich rearranges to\n\n$$\n(\\underbrace{\\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}+\\Gamma_{\\lambda j}^{s} \\Gamma_{s k}^{r}-\\Gamma_{\\lambda k}^{s} \\Gamma_{s j}^{r}}_{R_{\\lambda k j}^{r}}) u_{r}=0\n$$\n\nThus, $R_{r \\lambda k j}=0$ forces $R_{\\lambda k j}^{r}=0$ and compatibility.\n\n\\subsection*{9.8 Prove Lemma 9.7.}\nAs $\\left(R_{i j k l}\\right)$ and $\\left(G_{i j k l}\\right)$ are tensors [see Example 8.3] and $\\mathrm{K}$ is an invariant,\n\n$$\n\\left(T_{i j k l}\\right) \\equiv\\left(R_{i j k l}-\\mathrm{K} G_{i j k l}\\right)\n$$\n\nis a tensor of the same type and order. It must be proved that all $T_{i j k l}=0$ at an isotropic point $P$. Since $\\mathrm{K}$ is independent of direction at $P$, so are the $T_{i j k l}$; and (8.7) gives\n\n\n\\begin{equation*}\nT_{i j k l} U^{i} V^{j} U^{k} V^{l}=0 \\quad\\left(T_{i j k l}=T_{i j k l}(P)\\right) \\tag{1}\n\\end{equation*}\n\n\nIf we define the second-order tensor $\\left(S_{i k}\\right) \\equiv\\left(T_{i j k l} V^{j} V^{l}\\right)$, we find that $S_{i k}=S_{k i}$, and by (1), $S_{i k} U^{i} U^{k}=0$ at $P$ for any $\\left(U^{i}\\right)$. It follows that all $S_{i k}=0$ at $P$. Now set $V^{i}=\\delta_{a}^{i}$. Then, at $P$,\n\n$$\n0=S_{i k}=T_{i j k l} \\delta_{a}^{j} \\delta_{a}^{l}=T_{i a k a}\n$$\n\nfor arbitrary (fixed) index $a$. Next set $V^{i}=\\delta_{a}^{i}+\\delta_{b}^{i}$ for arbitrary fixed indices $a$ and $b$ :\n\n$$\n0=T_{i j k l} V^{j} V^{l}=T_{i j k l}\\left(\\delta_{a}^{j}+\\delta_{b}^{j}\\right)\\left(\\delta_{a}^{l}+\\delta_{b}^{l}\\right)=T_{i a k a}+T_{i a k b}+T_{i b k a}+T_{i b k b}\n$$\n\nor $T_{i a k b}+T_{i b k a}=0$. Therefore, since $T_{i j k l}$ obeys the same symmetry laws as $R_{i j k l}$ and $G_{i j k l}$,\n\n\n\\begin{align*}\nT_{i j k l}-T_{i l j k} & =0  \\tag{2}\\\\\nT_{i j k l}+T_{i k l j}+T_{i l j k} & =0 \\tag{3}\n\\end{align*}\n\n\nAdding (2) and (3),\n\n\n\\begin{equation*}\n2 T_{i j k l}+T_{i k l j}=0 \\tag{4}\n\\end{equation*}\n\n\nBut, from (2), $T_{i k l j}=T_{i j k l}$, so that (4) implies $T_{i j k l}=0$, as desired.\n\n\\subsection*{9.9 Prove Theorems 9.1 and 9.3.}\nWe already know that if the space is either Euclidean or flat, $K \\equiv 0$. Suppose, conversely, that $K \\equiv 0$; then every point is isotropic, and Lemma 9.7 implies that all $R_{i j k l}$ vanish. It then follows from Problem 9.7 that there exists a coordinate system $\\left(\\bar{x}^{i}\\right)$ for which $\\bar{\\Gamma}_{j k}^{i}=0$ or $\\bar{g}_{i j}=$ const. By Problem 9.5 , there exists another coordinate system, $\\left(y^{i}\\right)$, in which the metric takes the form (for real constants $a_{i}$ )\n\n$$\n\\varepsilon d s^{2}=\\varepsilon_{1} a_{1}^{2}\\left(d y^{1}\\right)^{2}+\\varepsilon_{2} a_{2}^{2}\\left(d y^{2}\\right)^{3}+\\cdots+\\varepsilon_{n} a_{n}^{2}\\left(d y^{n}\\right)^{2}\n$$\n\nThe transformation $\\bar{y}^{1}=a_{1} y^{1}, \\bar{y}^{2}=a_{2} y^{2}, \\ldots, \\bar{y}^{n}=a_{n} y^{n}$ now reduces the metric to\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=\\varepsilon_{1}\\left(d \\bar{y}^{1}\\right)^{2}+\\varepsilon_{2}\\left(d \\bar{y}^{2}\\right)^{2}+\\cdots+\\varepsilon_{n}\\left(d \\bar{y}^{n}\\right)^{2} \\tag{1}\n\\end{equation*}\n\n\nand the space is flat. This proves Theorem 9.3. If the given metric is positive definite, then in $(1), \\varepsilon_{i}=1$ for each $i$. In this case the metric is Euclidean, proving Theorem 9.1.\n\n\\section*{NORMAL COORDINATES}\n\\subsection*{9.10 Prove Theorem 9.6.}\nIf $\\left(y^{i}\\right)$ are normal coordinates, then the geodesic through $O$ and any point $P$ in some neighborhood $\\mathcal{N}$ of $O$ has the parametric form\n\n$$\ny^{i}=s p^{i} \\quad\\left(p^{i}=\\text { const. }\\right)\n$$\n\nThis geodesic thus obeys the differential equations\n\n$$\n\\frac{d y^{i}}{d s}=p^{i} \\quad \\text { and } \\quad \\frac{d^{2} y^{i}}{d s^{2}}=0\n$$\n\nBut it must also satisfy (9.5), $\\delta \\mathbf{T} / \\delta s=0$, in the coordinates $\\left(y^{i}\\right)$ :\n\n$$\n\\frac{d^{2} y^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d y^{j}}{d s} \\frac{d y^{k}}{d s}=0\n$$\n\nThus, by substit\u0131tion, $\\Gamma_{j k}^{i} p^{j} p^{k}=0$ for all directions $\\left(p^{i}\\right)$ at $O$. But $\\Gamma_{j k}^{i}$ is symmetric for each $i$; hence, $\\Gamma_{j k}^{i}=0$ at $O$ for all $i, j, k$. Also, $\\Gamma_{i j k}=g_{k r} \\Gamma_{i j}^{r}=0$; hence, $\\partial g_{i j} / \\partial y^{k}=0$ at $O$, by (6.2). Finally, since $g^{i j} g_{j r}=\\delta_{r}^{i}$, the product rule for differentiation yields $\\partial g^{i j} / \\partial y^{k}=0$ at $O$.\n\n9.11 Prove that at the origin of a Riemannian coordinate system $\\left(y^{i}\\right)$,\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}} \\quad(\\text { all } j \\text { and } k ; \\text { summed on } i)\n$$\n\nSince $\\Gamma_{j k}^{i}$ and $\\partial g^{i j} / \\partial y^{k}$ all vanish at the origin $O$ of the Riemannian coordinate system,\n\n\n\\begin{equation*}\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial}{\\partial y^{k}}\\left(g^{i r} \\Gamma_{j i r}\\right)=g^{i r} \\frac{\\partial}{\\partial y^{k}}\\left[\\frac{1}{2}\\left(-g_{j i r}+g_{i r j}+g_{r j i}\\right)\\right]=\\frac{1}{2} g^{i r}\\left(-g_{j i r k}+g_{i r j k}+g_{r j i k}\\right) \\tag{1}\n\\end{equation*}\n\n\nat $O$, with $g_{i j k l} \\equiv \\partial^{2} g_{i j} / \\partial y^{k} \\partial y^{l}$. But, since $g^{i r}=g^{r i}$,\n\n$$\ng^{i r} g_{j i r k}=g^{r i} g_{j i r k}=g^{i r} g_{j r i k}=g^{i r} g_{r j i k}\n$$\n\nand (1) becomes\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{1}{2} g^{i r} g_{i r j k}=\\frac{1}{2} g^{i r} g_{i r k j}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}}\n$$\n\n9.12 Prove the identity $R_{i j k l, u}+R_{i l j k, u}=R_{i k u l, j}+R_{i k j u, l}$.\n\nCovariant differentiation of Bianchi's first identity, (8.6), gives $R_{i j k l, u}+R_{i k l j, u}+R_{i l j k, u}=0$. Then the second identity, $(9.8)$, yields\n\n$$\nR_{i j k l, u}+R_{i l j k, u}=-R_{i k l j, u}=R_{i k j u, l}+R_{i k u l, j}\n$$\n\n9.13 Show that Bianchi's identities remain valid under an indefinite metric.\n\nOne can appeal to the topological fact that, at a given point $P$ of $\\mathbf{R}^{n}$, the directions for which a given metric $\\left(g_{i j}\\right)$ is indefinite span, at worst, a hyperplane. Hence, normal coordinates are possible along geodesics whose tangent vectors $\\left(p^{i}\\right)$ at $P$ do not lie in the hyperplane; Problem 9.10 gives $\\Gamma_{j k}^{i} p^{j} p^{k}=0$ for these directions. But the $\\Gamma_{i k}^{i}$ are continuous, and any direction in the hyperplane is the limit of a sequence of directions not in the hyperplane. It follows that $\\Gamma_{j k}^{i} p^{i} p^{j}=0$ for all $\\left(p^{i}\\right)$, yielding Theorem 9.6 and the Bianchi identities.\n\n\\section*{SCHUR'S THEOREM}\n9.14 Prove Schur's theorem (Theorem 9.8).\n\nBy Lemma 9.7, $R_{i j k l}=G_{i j k l} \\mathrm{~K}$ throughout $\\mathcal{N}$. Take the covariant derivative of both sides with respect to $x^{u}$, then permute indices ( $G_{i j k l, u}=0$ because $g_{i j, u}=0$ in general):\n\n$$\nR_{i j k l, u}=G_{i j k l} \\mathrm{~K}_{, u} \\quad R_{i j l u, k}=G_{i j l u} \\mathrm{~K}_{, k} \\quad R_{i j u k, l}=G_{i j u k} \\mathrm{~K}_{, l}\n$$\n\nAdd the three equations and apply $(9.8)$ :\n\n\n\\begin{equation*}\nG_{i j k l} \\mathrm{~K}_{, u}+G_{i j l u} \\mathrm{~K}_{, k}+G_{i j u k} \\mathrm{~K}_{, l}=0 \\tag{1}\n\\end{equation*}\n\n\nMultiply both sides of (1) by $g^{i k} g^{j l}$ and sum. Since\n\n$$\n\\begin{aligned}\n& g^{i k} g^{j l} G_{i j k l}=g^{i k} g^{j l}\\left(g_{i k} g_{j l}-g_{i l} g_{j k}\\right)=\\delta_{k}^{k} \\delta_{l}^{l}-\\delta_{l}^{k} \\delta_{k}^{l}=n^{2}-n \\\\\n& g^{i k} g^{j l} G_{i j l u}=g^{i k} g^{j l}\\left(g_{i l} g_{j u}-g_{i u} g_{j l}\\right)=\\delta_{l}^{k} \\delta_{u}^{l}-\\delta_{u}^{k} \\delta_{l}^{l}=\\delta_{u}^{k}-n \\delta_{u}^{k} \\\\\n& g^{i k} g^{j l} G_{i j u k}=g^{i k} g^{j l}\\left(g_{i u} g_{j k}-g_{i k} g_{j u}\\right)=\\delta_{u}^{k} \\delta_{k}^{l}-\\delta_{k}^{k} \\delta_{u}^{l}=\\delta_{u}^{l}-n \\delta_{u}^{l}\n\\end{aligned}\n$$\n\nthat summation yields the relation\n\n$$\n\\begin{aligned}\n0 & =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+\\left(\\delta_{u}^{k}-n \\delta_{u}^{k}\\right) \\mathrm{K}_{, k}+\\left(\\delta_{u}^{l}-n \\delta_{u}^{l}\\right) \\mathrm{K}_{, t} \\\\\n& =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}=(n-2)(n-1) \\mathrm{K}_{, u}\n\\end{aligned}\n$$\n\nFor $n \\geqq 3, \\mathrm{~K}_{, u}=\\partial \\mathrm{K} / \\partial x^{u}=0$. Since $u$ was arbitrary, $\\mathrm{K}$ must be constant over $\\mathcal{N}$. QED\n\n\\section*{THE EINSTEIN TENSOR}\n\\subsection*{9.15 Prove Theorem 9.9.}\nWe must prove that $G_{i, r}^{r}=0$. Multiply both sides of $(9.8)$ by $g^{i l} g^{j k}$ and sum:\n\n$$\n\\begin{aligned}\n0 & =g^{i l} g^{j k} R_{i j k l, u}-g^{i l} g^{j k} R_{i j u l, k}-g^{i l} g^{j k} R_{j i u k, l} \\\\\n& =g^{j k} R_{j k l, u}^{l}-g^{j k} R_{j u l, k}^{l}-g^{i l} R_{i u k, l}^{k}=g^{j k} R_{j k, u}-g^{j k} R_{j u, k}-g^{i l} R_{i u, l} \\\\\n& =R_{k, u}^{k}-R_{u, k}^{k}-R_{u, l}^{l}=2\\left(\\frac{1}{2} R_{, u}-R_{u, k}^{k}\\right)\n\\end{aligned}\n$$\n\nor, changing $u$ to $i$ and $k$ to $r, \\frac{1}{2} \\delta_{i}^{r} R_{, r}-R_{i, r}^{r}=0$. But, by Problem 6.32, $\\delta_{i, j}^{r}=0$ for all $i, j, r$; hence,\n\n$$\n\\left(R_{i}^{r}-\\frac{1}{2} \\delta_{i}^{r} R\\right)_{, r}=0 \\quad \\text { or } \\quad G_{i, r}^{r}=0\n$$\n\n9.16 Show that $G_{i j}$, the associated Einstein tensor obtained by lowering the index $i$ in $G_{j}^{i}$, is symmetric.\n\nBy definition,\n\n$$\nG_{i j}=g_{i k} G_{j}^{k}=g_{i k}\\left(R_{j}^{k}-\\frac{1}{2} \\delta_{j}^{k} R\\right)=R_{i j}-\\frac{1}{2} g_{i j} R\n$$\n\nwhich is obviously symmetric (by symmetry of the Ricci tensor).\n\n\\section*{Supplementary Problems}\n9.17 Solve, if compatible, the 'system $\\partial u_{\\lambda} / \\partial x^{j}=F_{\\lambda j}$, with\n\n$$\n\\begin{aligned}\n& F_{01}=x^{2} / 2 u_{0} \\quad F_{02}=x^{1} / 2 u_{0} \\\\\n& F_{01}=u_{0} x^{1} \\quad F_{02}=u_{1} x^{2} \\quad F_{11}=u_{0} x^{1} \\quad F_{12}=u_{1} x^{2}\n\\end{aligned}\n$$\n\n9.18 Verify that $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}$ represents the Euclidean metric (in polar coordinates).\n\n9.19 Consider the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1} d x^{2}\\right)^{2}-\\left(x^{1} d x^{3}\\right)^{2}$. Show that $R_{1212}=2$ and that, therefore, the space is not flat.\n\n9.20 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n\n9.21 Determine whether the following metric is flat and/or Euclidean:\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{3}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}\n$$\n\n9.22 Find the signature of the metric for $\\mathbf{R}^{3}$ given by\n\n$$\n\\varepsilon d s^{2}=2\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+5\\left(d x^{3}\\right)^{2}-8 d x^{1} d x^{2}-4 d x^{1} d x^{3}-d x^{2} d x^{3}\n$$\n\n9.23 Prove that $R_{i j k}^{i}=0$. [Hint: Use the first of (8.6).]\n\n9.24 Use Problem 9.11 to obtain a simplified proof for Problem 8.34.\n\n9.25 Show that the Einstein invariant, $G \\equiv G_{i}^{i}$, vanishes if the space is flat. [Hint: Use Corollary 9.4.]\n\n9.26 In the general theory of relativity one encounters the Schwarzschild metric,\n\n$$\n\\varepsilon d s^{2}=e^{\\varphi}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left[\\left(d x^{2}\\right)^{2}+\\left(\\sin ^{2} x^{2}\\right)\\left(d x^{3}\\right)^{2}\\right]-e^{\\psi}\\left(d x^{4}\\right)^{2}\n$$\n\nwhere both $\\varphi$ and $\\psi$ are functions of $x^{1}$ and $x^{4}$ only. Calculate the nonzero components of the Einstein tensor.\n\n"], "lesson": "\\section*{Chapter 9}\n\\section*{Spaces of Constant Curvature; Normal Coordinates}\n\\subsection*{9.1 ZERO CURVATURE AND THE EUCLIDEAN METRIC}\nA fundamental question has run unanswered through preceding chapters: How can one tell whether a given metrization of $\\mathbf{R}^{n}$ is Euclidean or not? To be sure that the meaning of \"Euclidean\" is clear, let us make the formal\n\nDefinition 1: A Riemannian metric $\\mathbf{g}=\\left(g_{i j}\\right)$, specified in a coordinate system $\\left(x^{i}\\right)$, is the Euclidean metric if, under some permissible coordinate transformation (3.1), $\\overline{\\mathbf{g}}=\\left(\\delta_{i j}\\right)$.\n\nNow, a coordinate system $\\left(\\bar{x}^{i}\\right.$ ) in which $\\bar{g}_{i j}=\\delta_{i j}$ is (by Definition 1 of Chapter 3) a rectangular system. Hence our question amounts to: Does a given Riemannian space admit rectangular coordinates or does it not?\n\nSuppose that a rectangular system $\\left(\\bar{x}^{i}\\right)$ does exist. Then $\\overline{\\mathrm{K}}=0$, since all Christoffel symbols vanish in $\\left(\\bar{x}^{i}\\right)$. But Riemannian curvature is an invariant, so that $\\mathrm{K}=0$ in the original coordinates $\\left(x^{i}\\right)$ as well. Moreover, by invariance,\n\n$$\ng_{i j} U^{i} U^{j}=\\bar{U}^{i} \\bar{U}^{i} \\geqq 0\n$$\n\nThus, the necessity part of the following theorem is immediate.\n\nTheorem 9.1: A Riemannian metric $\\left(g_{i j}\\right)$ is the Euclidean metric if and only if the Riemannian curvature $\\mathrm{K}$ is zero at all points and the metric is positive definite.\n\nTo prove the sufficiency portion, we set up a system of first-order partial differential equations for $n$ rectangular coordinates $\\bar{x}^{i}$ as functions of the given coordinates $x^{j}(j=1,2, \\ldots, n)$. The system that immediately comes to mind (Theorem 5.2) is $G=J^{T} J$, or\n\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}^{k}}{\\partial x^{i}} \\frac{\\partial \\bar{x}^{k}}{\\partial x^{j}}=g_{i j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\tag{9.1}\n\\end{equation*}\n\n\nBut (9.1) is generally intractable because of its nonlinearity. Instead, we select the linear system that results when barred and unbarred coordinates are interchanged in (6.6) and then the $\\bar{\\Gamma}_{j k}^{i}$ are equated to zero:\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\bar{x}^{k}}{\\partial x^{i} \\partial x^{j}}=\\Gamma_{i j}^{r}(\\mathbf{x}) \\frac{\\partial \\bar{x}^{k}}{\\partial x^{r}} \\tag{9.2}\n\\end{equation*}\n\n\nSetting $w \\equiv \\bar{x}^{k}$ and $u_{i} \\equiv \\partial \\bar{x}^{k} / \\partial x^{i}$ yields the desired first-order system\n\n\n\\begin{align*}\n& \\frac{\\partial w}{\\partial x^{i}}=u_{i} \\\\\n& \\frac{\\partial u_{i}}{\\partial x^{j}}=\\Gamma_{i j}^{r} u_{r} \\tag{9.3}\n\\end{align*}\n\n\nEXAMPLE 9.1 It is proved in Problems 9.7 and 9.8 that when $\\mathrm{K} \\equiv 0,(9.3)$ is solvable for a coordinate system $\\left(\\bar{x}^{k}\\right)$ for which all $\\bar{g}_{i j}$ are constants (i.e., all $\\bar{\\Gamma}_{j k}^{i}=0$ ); from these coordinates, rectangular coordinates can be reached, provided $\\left(g_{i j}\\right)$ is positive definite. To make these results plausible, consider the two-dimensional metric\n\n$$\ng_{11}=1 \\quad g_{12}=g_{21}=0 \\quad g_{22}=\\left(x^{2}\\right)^{2}\n$$\n\nThis metric is obviously positive definite and, because the only nonvanishing Christoffel symbol is $\\Gamma_{22}^{2}=1 / x^{2}$, it has $R_{1212}=0=\\mathrm{K}$. It is possible to solve (9.1) directly for the corresponding cartesian coordinates, and then to verify that that solution is contained in the general solution to (9.3).\n\nIntroduce the notation\n\n\n\\begin{equation*}\nf_{1} \\equiv \\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} \\quad f_{2} \\equiv \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} \\quad f_{3} \\equiv \\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} \\quad f_{4} \\equiv \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}} \\tag{1}\n\\end{equation*}\n\n\nwhereby (9.1) becomes the algebraic system\n\n\n\\begin{align*}\nf_{1}^{2}+f_{3}^{2} & =1 \\\\\nf_{1} f_{2}+f_{3} f_{4} & =0  \\tag{2}\\\\\nf_{2}^{2}+f_{4}^{2} & =\\left(x^{2}\\right)^{2}\n\\end{align*}\n\n\nSystem (2) can be solved for three of the $f_{i}$ in terms of the fourth-say, $f_{1}$ :\n\n\n\\begin{equation*}\nf_{1}=f_{1} \\quad f_{2}=x^{2} \\sqrt{1-f_{1}^{2}} \\quad f_{3}=-\\sqrt{1-f_{1}^{2}} \\quad f_{4}=x^{2} f_{1} \\tag{3}\n\\end{equation*}\n\n\nNow (1) becomes two simple first-order systems in $\\bar{x}^{1}$ alone and $\\bar{x}^{2}$ alone:\n\n$$\n\\text { I: }\\left\\{\\begin{array} { l } \n{ \\frac { \\partial \\overline { x } ^ { 1 } } { \\partial x ^ { 1 } } = f _ { 1 } } \\\\\n{ \\frac { \\partial \\overline { x } ^ { 1 } } { \\partial x ^ { 2 } } = x ^ { 2 } \\sqrt { 1 - f _ { 1 } ^ { 2 } } }\n\\end{array} \\quad \\text { and } \\quad \\text { II: } \\left\\{\\begin{array}{l}\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}}=-\\sqrt{1-f_{1}^{2}} \\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}}=x^{2} f_{1}\n\\end{array}\\right.\\right.\n$$\n\nThe unknown function $f_{1}$ is determined by the requirements that the two equations I and the two equations II both be compatible:\n\n$$\n\\frac{\\partial f_{1}}{\\partial x^{2}}=\\frac{\\partial}{\\partial x^{1}}\\left(x^{2} \\sqrt{1-f_{1}^{2}}\\right) \\quad \\text { and } \\quad \\frac{\\partial}{\\partial x^{2}}\\left(-\\sqrt{1-f_{1}^{2}}\\right)=\\frac{\\partial}{\\partial x^{1}}\\left(x^{2} f_{1}\\right)\n$$\n\nThe only function satisfying these two compatibility conditions is\n\n$$\nf_{1}=\\text { const. }=\\cos \\phi\n$$\n\nand I and II immediately integrate to give\n\n\n\\begin{align*}\n& \\bar{x}^{1}=x^{1} \\cos \\phi+\\frac{1}{2}\\left(x^{2}\\right)^{2} \\sin \\phi+c \\\\\n& \\bar{x}^{2}=-x^{1} \\sin \\phi+\\frac{1}{2}\\left(x^{2}\\right)^{2} \\cos \\phi+d \\tag{4}\n\\end{align*}\n\n\nWe are, of course, free to set $\\phi=c=d=0$ in (4).\n\nTurning to $(9.3)$, we have to solve\n\n$$\n\\begin{aligned}\n& \\text { (1) } \\frac{\\partial w}{\\partial x^{1}}=u_{1}, \\quad \\frac{\\partial w}{\\partial x^{2}}=u_{2} \\\\\n& \\begin{array}{ll}\n\\text { (2) } \\frac{\\partial u_{1}}{\\partial x^{1}}=0, \\frac{\\partial u_{1}}{\\partial x^{2}}=0 & \\text { (3) } \\frac{\\partial u_{2}}{\\partial x^{1}}=0, \\frac{\\partial u_{2}}{\\partial x^{2}}=u_{2} \\Gamma_{22}^{2}=\\frac{u_{2}}{x^{2}}\n\\end{array}\n\\end{aligned}\n$$\n\nNote that these equations include their own compatibility conditions! For instance, the second equation (2) and the first equation (3) ensure the compatibility of the two equations (1). The fact that system (9.3) is automatically compatible whenever $\\mathrm{K}=0$ is crucial to the proof of Theorem 9.1. Integrating the above equations in the order (3)-(2)-(1), we get:\n\n$$\nw=a_{1} x^{1}+a_{2}\\left(x^{2}\\right)^{2}+a_{3} \\quad\\left(a_{1}, a_{2}, a_{3}=\\text { const. }\\right)\n$$\n\nor, replacing the index $k$,\n\n\n\\begin{equation*}\n\\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\\left(x^{2}\\right)^{2}+a_{3}^{k} \\quad\\left(a_{i}^{k}=\\text { const. }\\right) \\tag{5}\n\\end{equation*}\n\n\nAs announced, (5) includes (4).\n\nFor subsequent use, the following compatibility theorem for quasilinear systems [which include linear systems such as (9.3)] is stated here, without proof:\n\nTheorem 9.2: The quasilinear first-order system\n\n$$\n\\frac{\\partial u_{\\lambda}}{\\partial x^{j}}=F_{\\lambda j}\\left(u_{0}, u_{1}, \\ldots, u_{m}, x^{1}, x^{2}, \\ldots, x^{n}\\right) \\quad(\\lambda=0,1, \\ldots, m ; j=1,2, \\ldots, n)\n$$\n\nwhere the functions $F_{\\lambda j}$ are of differentiability class $C^{1}$, has a nontrivial solution for the $u_{\\lambda}$, bounded over some region of $\\mathbf{R}^{n}$, if and only if\n\n$$\n\\frac{\\partial F_{\\lambda j}}{\\partial u_{\\nu}} F_{\\nu k}+\\frac{\\partial F_{\\lambda j}}{\\partial x^{k}}=\\frac{\\partial F_{\\lambda k}}{\\partial u_{\\nu}} F_{\\nu j}+\\frac{\\partial F_{\\lambda k}}{\\partial x^{j}} \\quad(\\lambda=0,1, \\ldots, m ; 1 \\leqq j<k \\leqq n)\n$$\n\n[The $\\nu$-summations run from 0 to $m$.]\n\n\\subsection*{9.2 FLAT RIEMANNIAN SPACES}\nA Riemannian space, or the determining metric, is termed flat if there is a transformation of coordinates $\\bar{x}^{i}=\\bar{x}^{i}(\\mathbf{x})$ that puts the metric into the standard form\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=\\varepsilon_{1}\\left(d \\bar{x}^{1}\\right)^{2}+\\varepsilon_{2}\\left(d \\bar{x}^{2}\\right)^{2}+\\cdots+\\varepsilon_{n}\\left(d \\bar{x}^{n}\\right)^{2} \\tag{9.4}\n\\end{equation*}\n\n\nwhere $\\varepsilon_{i}= \\pm 1$ for each $i$. This condition generalizes the concept of the Euclidean metric. The essential distinction between the two concepts revolves about positive-definiteness; the analogue to Theorem 9.1 with positive-definiteness removed is:\n\nTheorem 9.3: A Riemannian space is flat if and only if $\\mathrm{K}=0$ at all points.\n\nCorollary 9.4: If $\\mathrm{K}=0$, then $R=0$.\n\nProof: If $\\mathrm{K}=0$, then by Theorem 9.3 the space is flat, and hence the $\\bar{g}_{i j}$ are constant for some coordinate system $\\left(\\bar{x}^{i}\\right)$. It follows that all $\\bar{\\Gamma}_{i j k}, \\bar{\\Gamma}_{j k}^{i}, \\bar{R}_{j k l}^{i}, \\bar{R}_{i j}$, and $\\bar{R}_{j}^{i}$ vanish. Therefore, $\\bar{R}=\\bar{R}_{i}^{i}=0$, and since Ricci curvature is invariant, $R=0$.\n\nRemark 1: Problem 8.35 shows that the converse of Corollary 9.4 does not hold.\n\nEXAMPLE 9.2 Consider the Riemannian metric\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2}+4\\left(x^{3}\\right)^{2}\\left(d x^{3}\\right)^{2}-4\\left(x^{4}\\right)^{2}\\left(d x^{4}\\right)^{2}\n$$\n\n(a) Calculate the Riemannian curvature. (b) Find a solution of system (9.3) from which it may be inferred that the space is flat.\n\n(a) Using Problem 6.4, we find as the nonvanishing Christoffel symbols\n\n$$\n\\Gamma_{22}^{2}=\\frac{1}{x^{2}} \\quad \\Gamma_{33}^{3}=\\frac{1}{x^{3}} \\quad \\Gamma_{44}^{4}=\\frac{1}{x^{4}}\n$$\n\nBecause $\\Gamma_{j k}^{i}=0$ unless $i=j=k$, the partial-derivative terms drop out of (8.2), leaving\n\n$$\nR_{j k l}^{i}=\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i}=\\Gamma_{i i}^{i} \\Gamma_{i i}^{i}-\\Gamma_{i i}^{i} \\Gamma_{i i}^{i}=0 \\quad \\text { (not summed) }\n$$\n\nwhich in turn implies that $R_{i j k l}=0$ and $\\mathrm{K}=0$.\n\n(b) For the above-calculated Christoffel symbols\n\n$$\n\\frac{\\partial u_{1}}{\\partial x^{1}}=0 \\quad \\frac{\\partial u_{2}}{\\partial x^{2}}=\\frac{u_{2}}{x^{2}} \\quad \\frac{\\partial u_{3}}{\\partial x^{3}}=\\frac{u_{3}}{x^{3}} \\quad \\frac{\\partial u_{4}}{\\partial x^{4}}=\\frac{u_{4}}{x^{4}}\n$$\n\nwith $\\partial u_{i} / \\partial x_{j}=0$ for $i \\neq j$. Integrating,\n\n$$\nu_{1}=f_{1}\\left(x^{2}, x^{3}, x^{4}\\right) \\quad u_{2}=x^{2} f_{2}\\left(x^{1}, x^{3}, x^{4}\\right) \\quad u_{3}=x^{3} f_{3}\\left(x^{1}, x^{2}, x^{4}\\right) \\quad u_{4}=x^{4} f_{4}\\left(x^{1}, x^{2}, x^{3}\\right)\n$$\n\nfor arbitrary functions $f_{i}$. But the remaining equations (9.3), $\\partial w / \\partial x^{i}=u_{i}$, give rise to the compatibility relations\n\n$$\n\\frac{\\partial u_{i}}{\\partial x^{j}}=\\frac{\\partial u_{j}}{\\partial x^{i}}\n$$\n\nwhich are satisfied only if $f_{i}=c_{i}=$ const. Therefore,\n\n$$\nw=a_{1} x^{1}+a_{2}\\left(x^{2}\\right)^{2}+a_{3}\\left(x^{3}\\right)^{2}+a_{4}\\left(x^{4}\\right)^{2}+a_{5}\n$$\n\nand the transformation must be of the general form\n\n$$\n\\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\\left(x^{2}\\right)^{2}+a_{3}^{k}\\left(x^{3}\\right)^{2}+a_{4}^{k}\\left(x^{4}\\right)^{2}+a_{5}^{k} \\quad\\left(a_{i}^{k} \\text { constants }\\right)\n$$\n\nWe wish to specialize the constants so that the covariant law $G=J^{T} \\bar{G} J$ will hold, with $\\bar{G}$ corresponding to (9.4). As a preliminary guess, set\n\n$$\n\\left[a_{i}^{k}\\right]_{45}=\\left[\\begin{array}{ccccc}\nb_{1} & 0 & 0 & 0 & 0 \\\\\n0 & b_{2} & 0 & 0 & 0 \\\\\n0 & 0 & b_{3} & 0 & 0 \\\\\n0 & 0 & 0 & b_{4} & 0\n\\end{array}\\right]\n$$\n\nso that the covariant law becomes\n\n$$\n\\left[\\begin{array}{llll}\n1 & & & \\\\\n& 4\\left(x^{2}\\right)^{2} & & \\\\\n& & 4\\left(x^{3}\\right)^{2} & \\\\\n& & & -4\\left(x^{4}\\right)^{2}\n\\end{array}\\right]=\\left[\\begin{array}{llll}\nb_{1} & & & \\\\\n& 2 b_{2} x^{2} & & \\\\\n& & 2 b_{3} x^{3} & \\\\\n& & & 2 b_{4} x^{4}\n\\end{array}\\right]\\left[\\begin{array}{llll}\n\\varepsilon_{1} & & & \\\\\n& \\varepsilon_{2} & & \\\\\n& & \\varepsilon_{3} & \\\\\n& & & \\varepsilon_{4}\n\\end{array}\\right]\\left[\\begin{array}{llll}\nb_{1} & & \\\\\n& 2 b_{2} x^{2} & & \\\\\n& & 2 b_{3} x^{3} & \\\\\n& & & 2 b_{4} x^{4}\n\\end{array}\\right]\n$$\n\nBy inspection, the choice $b_{1}=b_{2}=b_{3}=b_{4}=1$ will render $\\varepsilon_{1}=\\varepsilon_{2}=\\varepsilon_{3}=-\\varepsilon_{4}=1$.\n\nIn connection with (9.4) there is an interesting theorem (Sylvester's law of inertia). Define as the signature of a flat metric $\\left(g_{i j}\\right)$ the ordered $n$-tuple\n\n$$\n\\left(\\operatorname{sgn} \\varepsilon_{1}, \\operatorname{sgn} \\varepsilon_{2}, \\ldots, \\operatorname{sgn} \\varepsilon_{n}\\right)\n$$\n\ncomposed of the signs of the coefficients in the standard form (i.e., the signs of $\\bar{g}_{11}, \\ldots, \\bar{g}_{n n}$ ).\n\nTheorem 9.5: The signature of a flat metric is uniquely determined up to order.\n\n\\subsection*{9.3 NORMAL COORDINATES}\nIt is possible to introduce local, quasirectangular coordinates in Riemannian space the use of which greatly simplifies the proofs of certain complicated tensor identities.\n\nLet $O$ denote an arbitrary point of $\\mathbf{R}^{n}$, and $\\mathbf{p}=\\left(p^{i}\\right)$ an arbitrary direction (unit vector) at $O$. Assuming a positive-definite metric, consider the differential equations for geodesics,\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d s} \\frac{d x^{k}}{d s}=0 \\tag{9.5}\n\\end{equation*}\n\n\n[cf. (7.13)], along with initial conditions\n\n\n\\begin{equation*}\n\\left.\\frac{d x^{i}}{d s}\\right|_{s=0}=p^{i} \\tag{9.6}\n\\end{equation*}\n\n\nHere the arc-length parameter is chosen to make $s=0$ at $O$.\n\nRemark 2: Under an indefinite metric, there could exist directions at $O$ in which arc length could not be defined; see, e.g., Problem 7.22. There would then be no hope of satisfying (9.6) with $\\left(p^{i}\\right)$ arbitrary.\n\nIt can be shown that for a given $\\mathbf{p}$, the system (9.5)-(9.6) has a unique solution; moreover, for each point $P$ in some neighborhood $\\mathcal{N}$ of $O$, there is a unique choice of direction $\\mathbf{p}$ at $O$ such that the solution curve $x^{i}=x^{i}(s)$ (a geodesic) passes through $P$. Accordingly, for each $P$ in $\\mathcal{N}$, take as the coordinates of $P$\n\n\n\\begin{equation*}\ny^{i}=s p^{i} \\tag{9.7}\n\\end{equation*}\n\n\nwhere $s$ is the distance along the geodesic from $O$ to $P$. The numbers $\\left(y^{i}\\right)$ are called the normal coordinates (or geodesic or Riemannian coordinates) of $P$.\n\nEXAMPLE 9.3 Show that if the Riemannian metric $d s^{2}=g_{i i} d x^{i} d x^{j}$ for $\\mathbf{R}^{2}$ is Euclidean and there is a point $O$\\\\\nat which $g_{12}=0$, then normal coordinates $\\left(y^{i}\\right)$ with origin $O$ are constant multiples of $\\left(z^{i}\\right)$, for some rectangular coordinate system $\\left(z^{i}\\right)$.\n\nBecause $g_{12}=0$ at point $O$, the vectors $\\mathbf{T}=\\left(1 / \\sqrt{g_{11}}, 0\\right)$ and $\\mathbf{S}=\\left(0,1 / \\sqrt{g_{22}}\\right)$ are, at $O$, an orthonormal pair. The space, being Euclidean, admits a rectangular coordinate system; in particular, a system ( $z^{i}$ ) with origin $O$ and unit vectors $\\mathbf{T}$ and $\\mathbf{S}$ (Fig. 9-1). Again because the space is Euclidean, the straight line segment $O P$ is the unique geodesic connecting $O$ with the arbitrary point $P$. With $s=\\overline{O P}$ and $\\mathbf{p}$ the direction vector of $O P$, we have the vector equation\n\n$$\nz^{1} \\mathbf{T}+z^{2} \\mathbf{S}=s \\mathbf{p}\n$$\n\nor componentwise,\n\n$$\nz^{1}\\left(\\frac{1}{\\sqrt{g_{11}}}\\right)=s p^{1} \\equiv y^{1} \\quad \\text { and } \\quad z^{2}\\left(\\frac{1}{\\sqrt{g_{22}}}\\right)=s p^{2} \\equiv y^{2}\n$$\n\nQED.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-127}\n\\end{center}\n\nFig. 9-1\n\nThe chief value of Riemannian coordinates resides in the following theorem (Problem 9.10).\n\nTheorem 9.6: If the metric tensor $\\left(g_{i j}\\right)$ is positive definite, then, at the origin of a Riemannian coordinate system $\\left(y^{i}\\right)$, all $\\partial g_{i j} / \\partial y^{k}, \\partial g^{i j} / \\partial y^{k}, \\Gamma_{i j k}$, and $\\Gamma_{j k}^{i}$ are zero.\n\nRemark 3: Recall that neither the partial derivatives of the metric tensor nor the Christoffel symbols are tensorial. Thus, their $\\left(y^{i}\\right)$-representations can vanish at $O$ without their $\\left(x^{i}\\right)$-representations doing so. For instance, because the transformation between $\\left(x^{i}\\right)$ and $\\left(y^{i}\\right)$ has $J=I$ at $O,(6.5)$ gives:\n\n$$\n\\left.\\Gamma_{j k}^{i}(\\mathbf{x})\\right|_{O}=\\left.\\frac{\\partial^{2} y^{i}}{\\partial x^{j} \\partial x^{k}}\\right|_{O}\n$$\n\nThe right side is generally nonzero, unless the coordinate transformation happens to be linear.\n\nEXAMPLE 9.4 Prove Bianchi's first identity, $R_{i j k l}+R_{i k l j}+R_{i l j k}=0$.\n\nTheorem 9.6 implies that at $O$, the origin of normal coordinates,\n\n$$\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial y^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial y^{l}}\n$$\n\nIf we use the notation $\\Gamma_{i j k l}$ for $\\partial \\Gamma_{i j k} / \\partial y^{l}$, for arbitrary $i, j, k, l$, then\n\n$$\n\\begin{aligned}\n& R_{i j k l}=\\Gamma_{j l i k}-\\Gamma_{j k i l} \\\\\n& R_{i k l j}=\\Gamma_{k j i l}-\\Gamma_{k l i j} \\\\\n& R_{i l j k}=\\Gamma_{l k i j}-\\Gamma_{l j i k}\n\\end{aligned}\n$$\n\nOn summing these three relations and observing the cancellations which take place, we see that the desired identity holds at $O$ in the coordinates $\\left(y^{i}\\right)$. This tensor identity must therefore remain valid at $O$ in the alias coordinates $\\left(x^{i}\\right)$. But $O$ is any point of $\\mathbf{R}^{n}$, and the proof is complete.\n\nEXAMPLE 9.5 Prove Bianchi's second identity,\n\n\n\\begin{equation*}\nR_{i j k l, u}+R_{i j l u, k}+R_{i j u k, l}=0 \\tag{9.8}\n\\end{equation*}\n\n\nWorking with the Riemann tensor of the second kind, we have, at the origin $O$ of normal coordinates,\n\n$$\n\\begin{aligned}\nR_{j k l, u}^{i} & =\\frac{\\partial R_{j k l}^{i}}{\\partial y^{u}}=\\frac{\\partial}{\\partial y^{u}}\\left(\\frac{\\partial \\Gamma_{j l}^{i}}{\\partial y^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial y^{l}}+\\Gamma_{j l}^{r} \\Gamma_{r k}^{i}-\\Gamma_{j k}^{r} \\Gamma_{r l}^{i}\\right) \\\\\n& =\\Gamma_{j l k u}^{i}-\\Gamma_{j k l u}^{i}\n\\end{aligned}\n$$\n\nsince terms like $\\left(\\partial \\Gamma_{j l}^{r} / \\partial y^{u}\\right) \\Gamma_{r k}^{i}$ vanish along with the $\\Gamma_{r k}^{i}$ at $O$. From this, permutation of subscripts yields\n\n$$\nR_{j k l, u}^{i}+R_{j l u, k}^{i}+R_{j u k, l}^{i}=0\n$$\n\nat $O$, and the validity of $(9.8)$ at $O$ follows from the fact that covariant differentiation commutes with the lowering of a superscript (Problem 6.11). We conclude, as in Example 9.4, that (9.8) holds generally in $\\left(x^{i}\\right)$.\n\nA positive definite metric has been tacitly assumed, both here and in Example 9.4. The assumption can be dropped; see Problem 9.13.\n\n\\subsection*{9.4 SCHUR'S THEOREM}\nFrom Chapter 8 it is known that although every point of Riemannian two-space is isotropic, the curvature ( $=R_{1212} / g$ ) can still vary from one isotropic point to the next. However, Problems 8.11, $8.12,8.28$, and 8.29 suggest that a different situation prevails in $\\mathbf{R}^{3}$. To prove the general theorem, known as Schur's theorem, it is necessary to establish a preliminary result, a generalization of (8.11).\n\nLemma 9.7: At an isotropic point of $\\mathbf{R}^{n}$ the Riemannian curvature is given by\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{R_{a b c d}}{g_{a c} g_{b d}-g_{a d} g_{b c}} \\equiv \\frac{R_{a b c d}}{G_{a b c d}} \\tag{9.9}\n\\end{equation*}\n\n\nfor any specific subscript string such that $G_{a b c d} \\neq 0$. [If $G_{a b c d}=0$, then $R_{a b c d}=0$ also.] For a proof, see Problem 9.8.\n\nTheorem 9.8 (Schur's Theorem): If all points in some neighborhood $\\mathcal{N}$ in a Riemannian $\\mathbf{R}^{n}$ are isotropic and $n \\geqq 3$, then $\\mathrm{K}$ is constant throughout that neighborhood.\n\nFor a proof, see Problem 9.14.\n\n\\subsection*{9.5 THE EINSTEIN TENSOR}\nThe Einstein tensor is defined in terms of the Ricci tensor $R_{i j}$ and the curvature invariant $R$ (Section 8.4):\n\n\n\\begin{equation*}\nG_{j}^{i} \\equiv R_{j}^{i}-\\frac{1}{2} \\delta_{j}^{i} R \\tag{9.10}\n\\end{equation*}\n\n\nIt is clear that $\\left(G_{j}^{i}\\right)$ is in fact a mixed tensor of order two.\n\nAs a direct generalization of the notion of the divergence of a vector field $\\mathbf{V}=\\left(V^{i}\\right)$ relative to\\\\\nrectangular coordinates $\\left(x^{i}\\right)$,\n\n$$\n\\operatorname{div} \\mathbf{V}=\\frac{\\partial V^{1}}{\\partial x^{1}}+\\frac{\\partial V^{2}}{\\partial x^{2}}+\\cdots+\\frac{\\partial V^{n}}{\\partial x^{n}} \\equiv \\frac{\\partial V^{r}}{\\partial x^{r}}\n$$\n\nwe define the divergence of the general tensor $\\mathbf{T}=\\left(T_{j_{1} j_{2} \\ldots j_{q}}^{i_{1} i_{2} \\ldots i_{p} \\ldots i_{p}}\\right)$ with respect to its $k$ th contravariant index to be the tensor\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{T} \\equiv\\left(T_{j_{1} j_{2} \\ldots j_{q}, r}^{i_{1} i_{2} \\ldots r, i_{p}}\\right) \\tag{9.11}\n\\end{equation*}\n\n\nIn Problem 9.15 is proved\n\nTheorem 9.9: For any Riemannian metric, the divergence of the Einstein tensor is zero at all points.\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}\n9.1 Test the compatibility conditions (Theorem 9.2) for the system\n\n\n\\begin{equation*}\n\\frac{\\partial u_{0}}{\\partial x^{1}}=\\frac{u_{0}}{x^{1}} \\quad \\frac{\\partial u_{0}}{\\partial x^{2}}=2 x^{2} u_{0} \\tag{1}\n\\end{equation*}\n\n\nIf it is compatible, solve the system.\n\nIn the notation of Theorem 9.2, there is only the condition corresponding to $\\lambda=0, j=1, k=2$ to be satisfied.\n\n$$\n\\begin{gathered}\n\\frac{\\partial F_{01}}{\\partial u_{0}} F_{02}+\\frac{\\partial F_{01}}{\\partial x^{2}} \\stackrel{?}{=} \\frac{\\partial F_{02}}{\\partial u_{0}} F_{01}+\\frac{\\partial F_{02}}{\\partial x^{1}} \\\\\n\\frac{\\partial}{\\partial u_{0}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\cdot 2 x^{2} u_{0}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\stackrel{?}{=} \\frac{\\partial}{\\partial u_{0}}\\left(2 x^{2} u_{0}\\right) \\cdot \\frac{u_{0}}{x^{1}}+\\frac{\\partial}{\\partial x^{1}}\\left(2 x^{2} u_{0}\\right) \\\\\n\\frac{2 x^{2} u_{0}}{x^{1}}=\\frac{2 x^{2} u_{0}}{x^{1}}\n\\end{gathered}\n$$\n\nTherefore, the system is compatible. The first equation (1) integrates to $u_{0}=x^{1} \\phi\\left(x^{2}\\right)$; the second equation then gives\n\n$$\nx^{1} \\phi^{\\prime}=2 x^{2} x^{1} \\phi \\quad \\text { whence } \\quad \\phi=c \\exp \\left(x^{2}\\right)^{2}\n$$\n\nHence the solution of (1) is $u_{0}=c x^{1} \\exp \\left(x^{2}\\right)^{2}$.\n\n9.2 Show that $\\mathbf{R}^{3}$ under the metric $d s^{2}=\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{1}\\right)^{2}+\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}$ is Euclidean.\n\nThis metric has $g_{33}=$ const., and $g_{11}$ and $g_{22}$ independent of $x^{3}$. Problem 6.4 then shows that $\\Gamma_{j k}^{i}=0$ whenever $i, j$, or $k$ equals 3 ; consequently, of the six independent components of the Riemann tensor, only $R_{1212}$ is possibly nonzero. But (from Problem 6.4), with $z \\equiv\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}$,\n\n$$\n\\begin{array}{lll}\n\\Gamma_{11}^{1}=\\frac{x^{1}}{z} & \\Gamma_{12}^{1}=\\Gamma_{21}^{1}=\\frac{x^{2}}{z} & \\Gamma_{22}^{1}=-\\frac{x^{1}}{z} \\\\\n\\Gamma_{11}^{2}=-\\frac{x^{2}}{z} & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{z} & \\Gamma_{22}^{2}=\\frac{x^{2}}{z}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}+\\Gamma_{22}^{2} \\Gamma_{21}^{1}-\\Gamma_{21}^{1} \\Gamma_{12}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& \\left.=\\frac{-z+x^{1}\\left(2 x^{1}\\right)}{z^{2}}-\\frac{z-x^{2}\\left(2 x^{2}\\right)}{z^{2}}+\\left(-\\frac{x^{1}}{z}\\right)\\left(\\frac{x^{1}}{z}\\right)+\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right)-\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right) \\right\\rvert\\,-\\frac{x^{1}}{z}\\left(-\\frac{x^{1}}{z}\\right)=0\n\\end{aligned}\n$$\n\nConsequently, $R_{1212}=0=\\mathrm{K}$. As the metric is clearly positive definite, Theorem 9.1 implies that the space is Euclidean.\n\n9.3 For the Euclidean space of Problem 9.2, exhibit a transformation from the given coordinate system $\\left(x^{i}\\right)$ to a rectangular system $\\left(\\bar{x}^{i}\\right)$.\n\nUsing the Christoffel symbols as calculated in Problem 9.2, we obtain from (9.3) the following system for the $u_{i}$ :\n\n\\[\n\\begin{array}{lll}\n\\frac{\\partial u_{1}}{\\partial x^{1}}=\\frac{x^{1} u_{1}-x^{2} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{2}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{2}}{\\partial x^{1}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{2}}=\\frac{-x^{1} u_{1}+x^{2} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{3}}{\\partial x^{1}}=0 & \\frac{\\partial u_{3}}{\\partial x^{2}}=0 & \\frac{\\partial u_{3}}{\\partial x^{3}}=0 \\tag{3}\n\\end{array}\n\\]\n\nThus $u_{1}$ and $u_{2}$ are functions of $x^{1}, x^{2}$ alone, and $u_{3}=$ const. Since the $g_{i j}$ are all polynomials of degree 2 in $x^{1}, x^{2}$, use the method of undetermined coefficients, assuming polynomial forms\n\n$$\nu_{i}=a_{i}\\left(x^{1}\\right)^{2}+b_{i} x^{1} x^{2}+c_{i}\\left(x^{2}\\right)^{2}+d_{i} x^{1}+e_{i} x^{2}+f_{i} \\quad(i=1,2)\n$$\n\nThe (compatibility) relation $\\partial u_{1} / \\partial x^{2}=\\partial u_{2} / \\partial x^{1}$ implied by the second equation (1) and the first equation (2) requires\n\n$$\nb_{1}=2 a_{2} \\quad 2 c_{1}=b_{2} \\quad e_{1}=d_{2}\n$$\n\nSimilarly, $\\partial u_{1} / \\partial x^{1}=-\\partial u_{2} / \\partial x^{2}$ implies\n\n$$\n2 a_{1}=-b_{2} \\quad b_{1}=-2 c_{2} \\quad d_{1}=-e_{2}\n$$\n\nUsing the first equation (1), or $z\\left(\\partial u_{1} / \\partial x^{1}\\right)=x^{1} u_{1}-x^{2} u_{2}$, we get:\n\n$$\na_{1}=0 \\quad a_{2}=0 \\quad c_{1}=b_{2} \\quad b_{1}=-c_{2} \\quad d_{1}=-e_{2} \\quad f_{1}=0=-f_{2}\n$$\n\nIt follows that $b_{1}=b_{2}=c_{1}=c_{2}=0$, and therefore (renotating $d_{1}$ and $e_{1}$ )\n\n$$\nu_{1}=a x^{1}+b x^{2} \\quad u_{2}=b x^{1}-a x^{2} \\quad u_{3}=c\n$$\n\n[Note: This solution of (1)-(2)-(3) may be obtained by the method of characteristics, without any prior assumptions.]\n\nThe first equations (9.3),\n\n$$\n\\frac{\\partial w}{\\partial x^{1}}=a x^{1}+b x^{2} \\quad \\frac{\\partial w}{\\partial x^{2}}=b x^{1}-a x^{2} \\quad \\frac{\\partial w}{\\partial x^{3}}=c\n$$\n\nmay now be integrated to give\n\n$$\nw=\\frac{a}{2}\\left(x^{1}\\right)^{2}+b x^{1} x^{2}-\\frac{a}{2}\\left(x^{2}\\right)^{2}+c x^{3}+d\n$$\n\nor, replacing $\\bar{x}^{k}$ and corresponding superscripts, and with $d=0$,\n\n$$\n\\bar{x}^{k}=\\frac{a^{k}}{2}\\left(x^{1}\\right)^{2}+b^{k} x^{1} x^{2}-\\frac{a^{k}}{2}\\left(x^{2}\\right)^{2}+c^{k}\\left(x^{2}\\right)^{2}\n$$\n\nIt is clear that we may take $c^{1}=c^{2}=0=a^{3}=b^{3}$ and $c^{3}=1$ :\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\frac{1}{2} a^{1}\\left(x^{1}\\right)^{2}+b^{1} x^{1} x^{2}-\\frac{1}{2} a^{1}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{2}=\\frac{1}{2} a^{2}\\left(x^{1}\\right)^{2}+b^{2} x^{1} x^{2}-\\frac{1}{2} a^{2}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{3}=x^{3}\n\\end{aligned}\n$$\n\nThe Jacobian matrix is\n\n$$\nJ=\\left[\\begin{array}{ccc}\na^{1} x^{1}+b^{1} x^{2} & b^{1} x^{1}-a^{1} x^{2} & 0 \\\\\na^{2} x^{1}+b^{2} x^{2} & b^{2} x^{1}-a^{2} x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nSince $J^{T} J=G$, we must have\n\n$$\n\\left(a^{1}\\right)^{2}+\\left(a^{2}\\right)^{2}=1 \\quad a^{1} b^{1}+a^{2} b^{2}=0 \\quad\\left(b^{1}\\right)^{2}+\\left(b^{2}\\right)^{2}=1\n$$\n\nso take $a^{1}=0, a^{2}=1, b^{2}=0, b^{1}=1$. The transformation is, finally,\n\n$$\n\\bar{x}^{1}=x^{1} x^{2} \\quad \\bar{x}^{2}=\\frac{1}{2}\\left[\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\right] \\quad \\bar{x}^{3}=x^{3}\n$$\n\n\\section*{FLAT RIEMANNIAN SPACES}\n9.4 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n\nSince the metric is not positive definite, it cannot be Euclidean. To determine flatness, it suffices to examine $R_{1212}=g_{11} R_{212}^{1}$. But Problem 6.4 shows that $R_{212}^{1}=0$; hence the space is flat.\n\n9.5 Show that if the metric tensor is constant, the space is flat and the coordinate transformation $\\bar{x}=A x$, where $A$ is a rank- $n$ matrix of eigenvectors of $G=\\left(g_{i j}\\right)$, diagonalizes the metric (i.e., $\\bar{g}_{i j}=0$ if $i \\neq j$ ).\n\nSince all partial derivatives of $g_{i j}$ are zero, all Christoffel symbols will vanish and all $R_{i j k l}=0$, making $\\mathrm{K}=0$. Thus, by Theorem 9.3, the space is flat. By Chapters 2 and 3, if $\\bar{x}=A x$, then $J=A$ and\n\n$$\nG=J^{T} \\bar{G} J=A^{T} \\bar{G} A\n$$\n\nHowever, since $G$ is real and symmetric, its eigenvectors form an orthogonal matrix which we now choose as $A$, with\n\n$$\nA G A^{-1}=A G A^{T}=D \\quad \\text { (diagonal matrix of eigenvalues of } G \\text { ) }\n$$\n\nHence, $\\bar{G}=A G A^{T}=D \\quad$ QED.\n\n9.6 Find the signature of the flat metric\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}-2\\left(d x^{3}\\right)^{2}+2\\left(d x^{4}\\right)^{2}-4 d x^{2} d x^{3}-4 d x^{2} d x^{4}-10 d x^{3} d x^{4}\n$$\n\nis\n\nIn view of Problem 9.5, it suffices to find the eigenvalues $\\lambda$ of $G=\\left(g_{i j}\\right)$. The characteristic equation\n\n$$\n\\begin{aligned}\n|G-\\lambda I| & =\\left|\\begin{array}{cccc}\n4-\\lambda & 0 & 0 & 0 \\\\\n0 & 5-\\lambda & -2 & -2 \\\\\n0 & -2 & -2-\\lambda & -5 \\\\\n0 & -2 & -5 & 2-\\lambda\n\\end{array}\\right| \\\\\n& =(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & -2 & -2 \\\\\n-2 & -2-\\lambda & -5 \\\\\n-2 & -5 & 2-\\lambda\n\\end{array}\\right|=-(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & 2 & 0 \\\\\n-2 & 2+\\lambda & -3+\\lambda \\\\\n-2 & 5 & 7-\\lambda\n\\end{array}\\right| \\\\\n& =-(4-\\lambda)\\left[(5-\\lambda)\\left(29-\\lambda^{2}\\right)+8(5-\\lambda)\\right]=-(4-\\lambda)(5-\\lambda)\\left(37-\\lambda^{2}\\right)=0\n\\end{aligned}\n$$\n\nfrom which the eigenvalues are $\\lambda=+4,+5,+\\sqrt{37},-\\sqrt{37}$. This means that there is a transformation which changes the metric into the form\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}+\\sqrt{37}\\left(d x^{3}\\right)^{2}-\\sqrt{37}\\left(d x^{4}\\right)^{2}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left(d \\bar{x}^{2}\\right)^{2}+\\left(d \\bar{x}^{3}\\right)^{2}-\\left(d \\bar{x}^{4}\\right)^{2}\n$$\n\nwith the obvious change of coordinates. Hence, the signature is $(+++-)$, or some permutation thereof (Theorem 9.5).\n\n9.7 Show that the conditions $R_{i j k l}=0$ are sufficient for the compatibility of (9.3).\n\nIn the notation of Theorem 9.2, (9.3) takes the form (with $m=n$ )\n\n$$\n\\begin{array}{ll}\n\\boldsymbol{\\lambda}=\\mathbf{0} & \\frac{\\partial u_{0}}{\\partial x^{j}}=F_{0 j} \\equiv u_{j} \\\\\n\\lambda>0 & \\frac{\\partial u_{\\lambda}}{\\partial x^{j}}=F_{\\lambda j} \\equiv u_{r} \\Gamma_{\\lambda j}^{r}(\\mathbf{x})\n\\end{array}\n$$\n\nThe corresponding compatibility conditions are\n\n$$\n\\boldsymbol{\\lambda}=\\mathbf{0} \\quad \\delta_{j}^{\\nu} u_{r} \\Gamma_{\\nu k}^{r}=\\delta_{k}^{\\nu} u_{r} \\Gamma_{\\nu j}^{r}\n$$\n\nor $u_{r} \\Gamma_{j k}^{r}=u_{r} \\Gamma_{k j}^{r}$, which holds trivially, and\n\n$$\n\\boldsymbol{\\lambda}>\\mathbf{0} \\quad \\delta_{r}^{\\nu} \\Gamma_{\\lambda j}^{r} u_{s} \\Gamma_{\\nu k}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}=\\delta_{r}^{\\nu} \\Gamma_{\\lambda k}^{r} u_{s} \\Gamma_{\\nu j}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}\n$$\n\nwhich rearranges to\n\n$$\n(\\underbrace{\\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}+\\Gamma_{\\lambda j}^{s} \\Gamma_{s k}^{r}-\\Gamma_{\\lambda k}^{s} \\Gamma_{s j}^{r}}_{R_{\\lambda k j}^{r}}) u_{r}=0\n$$\n\nThus, $R_{r \\lambda k j}=0$ forces $R_{\\lambda k j}^{r}=0$ and compatibility.\n\n\\subsection*{9.8 Prove Lemma 9.7.}\nAs $\\left(R_{i j k l}\\right)$ and $\\left(G_{i j k l}\\right)$ are tensors [see Example 8.3] and $\\mathrm{K}$ is an invariant,\n\n$$\n\\left(T_{i j k l}\\right) \\equiv\\left(R_{i j k l}-\\mathrm{K} G_{i j k l}\\right)\n$$\n\nis a tensor of the same type and order. It must be proved that all $T_{i j k l}=0$ at an isotropic point $P$. Since $\\mathrm{K}$ is independent of direction at $P$, so are the $T_{i j k l}$; and (8.7) gives\n\n\n\\begin{equation*}\nT_{i j k l} U^{i} V^{j} U^{k} V^{l}=0 \\quad\\left(T_{i j k l}=T_{i j k l}(P)\\right) \\tag{1}\n\\end{equation*}\n\n\nIf we define the second-order tensor $\\left(S_{i k}\\right) \\equiv\\left(T_{i j k l} V^{j} V^{l}\\right)$, we find that $S_{i k}=S_{k i}$, and by (1), $S_{i k} U^{i} U^{k}=0$ at $P$ for any $\\left(U^{i}\\right)$. It follows that all $S_{i k}=0$ at $P$. Now set $V^{i}=\\delta_{a}^{i}$. Then, at $P$,\n\n$$\n0=S_{i k}=T_{i j k l} \\delta_{a}^{j} \\delta_{a}^{l}=T_{i a k a}\n$$\n\nfor arbitrary (fixed) index $a$. Next set $V^{i}=\\delta_{a}^{i}+\\delta_{b}^{i}$ for arbitrary fixed indices $a$ and $b$ :\n\n$$\n0=T_{i j k l} V^{j} V^{l}=T_{i j k l}\\left(\\delta_{a}^{j}+\\delta_{b}^{j}\\right)\\left(\\delta_{a}^{l}+\\delta_{b}^{l}\\right)=T_{i a k a}+T_{i a k b}+T_{i b k a}+T_{i b k b}\n$$\n\nor $T_{i a k b}+T_{i b k a}=0$. Therefore, since $T_{i j k l}$ obeys the same symmetry laws as $R_{i j k l}$ and $G_{i j k l}$,\n\n\n\\begin{align*}\nT_{i j k l}-T_{i l j k} & =0  \\tag{2}\\\\\nT_{i j k l}+T_{i k l j}+T_{i l j k} & =0 \\tag{3}\n\\end{align*}\n\n\nAdding (2) and (3),\n\n\n\\begin{equation*}\n2 T_{i j k l}+T_{i k l j}=0 \\tag{4}\n\\end{equation*}\n\n\nBut, from (2), $T_{i k l j}=T_{i j k l}$, so that (4) implies $T_{i j k l}=0$, as desired.\n\n\\subsection*{9.9 Prove Theorems 9.1 and 9.3.}\nWe already know that if the space is either Euclidean or flat, $K \\equiv 0$. Suppose, conversely, that $K \\equiv 0$; then every point is isotropic, and Lemma 9.7 implies that all $R_{i j k l}$ vanish. It then follows from Problem 9.7 that there exists a coordinate system $\\left(\\bar{x}^{i}\\right)$ for which $\\bar{\\Gamma}_{j k}^{i}=0$ or $\\bar{g}_{i j}=$ const. By Problem 9.5 , there exists another coordinate system, $\\left(y^{i}\\right)$, in which the metric takes the form (for real constants $a_{i}$ )\n\n$$\n\\varepsilon d s^{2}=\\varepsilon_{1} a_{1}^{2}\\left(d y^{1}\\right)^{2}+\\varepsilon_{2} a_{2}^{2}\\left(d y^{2}\\right)^{3}+\\cdots+\\varepsilon_{n} a_{n}^{2}\\left(d y^{n}\\right)^{2}\n$$\n\nThe transformation $\\bar{y}^{1}=a_{1} y^{1}, \\bar{y}^{2}=a_{2} y^{2}, \\ldots, \\bar{y}^{n}=a_{n} y^{n}$ now reduces the metric to\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=\\varepsilon_{1}\\left(d \\bar{y}^{1}\\right)^{2}+\\varepsilon_{2}\\left(d \\bar{y}^{2}\\right)^{2}+\\cdots+\\varepsilon_{n}\\left(d \\bar{y}^{n}\\right)^{2} \\tag{1}\n\\end{equation*}\n\n\nand the space is flat. This proves Theorem 9.3. If the given metric is positive definite, then in $(1), \\varepsilon_{i}=1$ for each $i$. In this case the metric is Euclidean, proving Theorem 9.1.\n\n\\section*{NORMAL COORDINATES}\n\\subsection*{9.10 Prove Theorem 9.6.}\nIf $\\left(y^{i}\\right)$ are normal coordinates, then the geodesic through $O$ and any point $P$ in some neighborhood $\\mathcal{N}$ of $O$ has the parametric form\n\n$$\ny^{i}=s p^{i} \\quad\\left(p^{i}=\\text { const. }\\right)\n$$\n\nThis geodesic thus obeys the differential equations\n\n$$\n\\frac{d y^{i}}{d s}=p^{i} \\quad \\text { and } \\quad \\frac{d^{2} y^{i}}{d s^{2}}=0\n$$\n\nBut it must also satisfy (9.5), $\\delta \\mathbf{T} / \\delta s=0$, in the coordinates $\\left(y^{i}\\right)$ :\n\n$$\n\\frac{d^{2} y^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d y^{j}}{d s} \\frac{d y^{k}}{d s}=0\n$$\n\nThus, by substit\u0131tion, $\\Gamma_{j k}^{i} p^{j} p^{k}=0$ for all directions $\\left(p^{i}\\right)$ at $O$. But $\\Gamma_{j k}^{i}$ is symmetric for each $i$; hence, $\\Gamma_{j k}^{i}=0$ at $O$ for all $i, j, k$. Also, $\\Gamma_{i j k}=g_{k r} \\Gamma_{i j}^{r}=0$; hence, $\\partial g_{i j} / \\partial y^{k}=0$ at $O$, by (6.2). Finally, since $g^{i j} g_{j r}=\\delta_{r}^{i}$, the product rule for differentiation yields $\\partial g^{i j} / \\partial y^{k}=0$ at $O$.\n\n9.11 Prove that at the origin of a Riemannian coordinate system $\\left(y^{i}\\right)$,\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}} \\quad(\\text { all } j \\text { and } k ; \\text { summed on } i)\n$$\n\nSince $\\Gamma_{j k}^{i}$ and $\\partial g^{i j} / \\partial y^{k}$ all vanish at the origin $O$ of the Riemannian coordinate system,\n\n\n\\begin{equation*}\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial}{\\partial y^{k}}\\left(g^{i r} \\Gamma_{j i r}\\right)=g^{i r} \\frac{\\partial}{\\partial y^{k}}\\left[\\frac{1}{2}\\left(-g_{j i r}+g_{i r j}+g_{r j i}\\right)\\right]=\\frac{1}{2} g^{i r}\\left(-g_{j i r k}+g_{i r j k}+g_{r j i k}\\right) \\tag{1}\n\\end{equation*}\n\n\nat $O$, with $g_{i j k l} \\equiv \\partial^{2} g_{i j} / \\partial y^{k} \\partial y^{l}$. But, since $g^{i r}=g^{r i}$,\n\n$$\ng^{i r} g_{j i r k}=g^{r i} g_{j i r k}=g^{i r} g_{j r i k}=g^{i r} g_{r j i k}\n$$\n\nand (1) becomes\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{1}{2} g^{i r} g_{i r j k}=\\frac{1}{2} g^{i r} g_{i r k j}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}}\n$$\n\n9.12 Prove the identity $R_{i j k l, u}+R_{i l j k, u}=R_{i k u l, j}+R_{i k j u, l}$.\n\nCovariant differentiation of Bianchi's first identity, (8.6), gives $R_{i j k l, u}+R_{i k l j, u}+R_{i l j k, u}=0$. Then the second identity, $(9.8)$, yields\n\n$$\nR_{i j k l, u}+R_{i l j k, u}=-R_{i k l j, u}=R_{i k j u, l}+R_{i k u l, j}\n$$\n\n9.13 Show that Bianchi's identities remain valid under an indefinite metric.\n\nOne can appeal to the topological fact that, at a given point $P$ of $\\mathbf{R}^{n}$, the directions for which a given metric $\\left(g_{i j}\\right)$ is indefinite span, at worst, a hyperplane. Hence, normal coordinates are possible along geodesics whose tangent vectors $\\left(p^{i}\\right)$ at $P$ do not lie in the hyperplane; Problem 9.10 gives $\\Gamma_{j k}^{i} p^{j} p^{k}=0$ for these directions. But the $\\Gamma_{i k}^{i}$ are continuous, and any direction in the hyperplane is the limit of a sequence of directions not in the hyperplane. It follows that $\\Gamma_{j k}^{i} p^{i} p^{j}=0$ for all $\\left(p^{i}\\right)$, yielding Theorem 9.6 and the Bianchi identities.\n\n\\section*{SCHUR'S THEOREM}\n9.14 Prove Schur's theorem (Theorem 9.8).\n\nBy Lemma 9.7, $R_{i j k l}=G_{i j k l} \\mathrm{~K}$ throughout $\\mathcal{N}$. Take the covariant derivative of both sides with respect to $x^{u}$, then permute indices ( $G_{i j k l, u}=0$ because $g_{i j, u}=0$ in general):\n\n$$\nR_{i j k l, u}=G_{i j k l} \\mathrm{~K}_{, u} \\quad R_{i j l u, k}=G_{i j l u} \\mathrm{~K}_{, k} \\quad R_{i j u k, l}=G_{i j u k} \\mathrm{~K}_{, l}\n$$\n\nAdd the three equations and apply $(9.8)$ :\n\n\n\\begin{equation*}\nG_{i j k l} \\mathrm{~K}_{, u}+G_{i j l u} \\mathrm{~K}_{, k}+G_{i j u k} \\mathrm{~K}_{, l}=0 \\tag{1}\n\\end{equation*}\n\n\nMultiply both sides of (1) by $g^{i k} g^{j l}$ and sum. Since\n\n$$\n\\begin{aligned}\n& g^{i k} g^{j l} G_{i j k l}=g^{i k} g^{j l}\\left(g_{i k} g_{j l}-g_{i l} g_{j k}\\right)=\\delta_{k}^{k} \\delta_{l}^{l}-\\delta_{l}^{k} \\delta_{k}^{l}=n^{2}-n \\\\\n& g^{i k} g^{j l} G_{i j l u}=g^{i k} g^{j l}\\left(g_{i l} g_{j u}-g_{i u} g_{j l}\\right)=\\delta_{l}^{k} \\delta_{u}^{l}-\\delta_{u}^{k} \\delta_{l}^{l}=\\delta_{u}^{k}-n \\delta_{u}^{k} \\\\\n& g^{i k} g^{j l} G_{i j u k}=g^{i k} g^{j l}\\left(g_{i u} g_{j k}-g_{i k} g_{j u}\\right)=\\delta_{u}^{k} \\delta_{k}^{l}-\\delta_{k}^{k} \\delta_{u}^{l}=\\delta_{u}^{l}-n \\delta_{u}^{l}\n\\end{aligned}\n$$\n\nthat summation yields the relation\n\n$$\n\\begin{aligned}\n0 & =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+\\left(\\delta_{u}^{k}-n \\delta_{u}^{k}\\right) \\mathrm{K}_{, k}+\\left(\\delta_{u}^{l}-n \\delta_{u}^{l}\\right) \\mathrm{K}_{, t} \\\\\n& =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}=(n-2)(n-1) \\mathrm{K}_{, u}\n\\end{aligned}\n$$\n\nFor $n \\geqq 3, \\mathrm{~K}_{, u}=\\partial \\mathrm{K} / \\partial x^{u}=0$. Since $u$ was arbitrary, $\\mathrm{K}$ must be constant over $\\mathcal{N}$. QED\n\n\\section*{THE EINSTEIN TENSOR}\n\\subsection*{9.15 Prove Theorem 9.9.}\nWe must prove that $G_{i, r}^{r}=0$. Multiply both sides of $(9.8)$ by $g^{i l} g^{j k}$ and sum:\n\n$$\n\\begin{aligned}\n0 & =g^{i l} g^{j k} R_{i j k l, u}-g^{i l} g^{j k} R_{i j u l, k}-g^{i l} g^{j k} R_{j i u k, l} \\\\\n& =g^{j k} R_{j k l, u}^{l}-g^{j k} R_{j u l, k}^{l}-g^{i l} R_{i u k, l}^{k}=g^{j k} R_{j k, u}-g^{j k} R_{j u, k}-g^{i l} R_{i u, l} \\\\\n& =R_{k, u}^{k}-R_{u, k}^{k}-R_{u, l}^{l}=2\\left(\\frac{1}{2} R_{, u}-R_{u, k}^{k}\\right)\n\\end{aligned}\n$$\n\nor, changing $u$ to $i$ and $k$ to $r, \\frac{1}{2} \\delta_{i}^{r} R_{, r}-R_{i, r}^{r}=0$. But, by Problem 6.32, $\\delta_{i, j}^{r}=0$ for all $i, j, r$; hence,\n\n$$\n\\left(R_{i}^{r}-\\frac{1}{2} \\delta_{i}^{r} R\\right)_{, r}=0 \\quad \\text { or } \\quad G_{i, r}^{r}=0\n$$\n\n9.16 Show that $G_{i j}$, the associated Einstein tensor obtained by lowering the index $i$ in $G_{j}^{i}$, is symmetric.\n\nBy definition,\n\n$$\nG_{i j}=g_{i k} G_{j}^{k}=g_{i k}\\left(R_{j}^{k}-\\frac{1}{2} \\delta_{j}^{k} R\\right)=R_{i j}-\\frac{1}{2} g_{i j} R\n$$\n\nwhich is obviously symmetric (by symmetry of the Ricci tensor).\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n9.17 Solve, if compatible, the 'system $\\partial u_{\\lambda} / \\partial x^{j}=F_{\\lambda j}$, with\n\n$$\n\\begin{aligned}\n& F_{01}=x^{2} / 2 u_{0} \\quad F_{02}=x^{1} / 2 u_{0} \\\\\n& F_{01}=u_{0} x^{1} \\quad F_{02}=u_{1} x^{2} \\quad F_{11}=u_{0} x^{1} \\quad F_{12}=u_{1} x^{2}\n\\end{aligned}\n$$\n\n9.18 Verify that $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}$ represents the Euclidean metric (in polar coordinates).\n\n9.19 Consider the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1} d x^{2}\\right)^{2}-\\left(x^{1} d x^{3}\\right)^{2}$. Show that $R_{1212}=2$ and that, therefore, the space is not flat.\n\n9.20 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n\n9.21 Determine whether the following metric is flat and/or Euclidean:\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{3}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}\n$$\n\n9.22 Find the signature of the metric for $\\mathbf{R}^{3}$ given by\n\n$$\n\\varepsilon d s^{2}=2\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+5\\left(d x^{3}\\right)^{2}-8 d x^{1} d x^{2}-4 d x^{1} d x^{3}-d x^{2} d x^{3}\n$$\n\n9.23 Prove that $R_{i j k}^{i}=0$. [Hint: Use the first of (8.6).]\n\n9.24 Use Problem 9.11 to obtain a simplified proof for Problem 8.34.\n\n9.25 Show that the Einstein invariant, $G \\equiv G_{i}^{i}$, vanishes if the space is flat. [Hint: Use Corollary 9.4.]\n\n9.26 In the general theory of relativity one encounters the Schwarzschild metric,\n\n$$\n\\varepsilon d s^{2}=e^{\\varphi}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left[\\left(d x^{2}\\right)^{2}+\\left(\\sin ^{2} x^{2}\\right)\\left(d x^{3}\\right)^{2}\\right]-e^{\\psi}\\left(d x^{4}\\right)^{2}\n$$\n\nwhere both $\\varphi$ and $\\psi$ are functions of $x^{1}$ and $x^{4}$ only. Calculate the nonzero components of the Einstein tensor.\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 10}", "\\section*{Tensors in Euclidean Geometry}\n\\subsection*{10.1 INTRODUCTION}\nThere exists a starting correlation between formulas of differential geometry, developed to answer questions about curves and surfaces in Euclidean 3-space, and tensor identities previously introduced to handle changes of coordinate systems. Differential geometry was used to great advantage by Einstein in his development of relativity.\n\nThe metric will be assumed to be the Euclidean metric, and to emphasize this fact we shall designate the space by $\\mathbf{E}^{3}$, which means $\\mathbf{R}^{3}$ with the metric\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}\n$$\n\nMoreover, we shall use the familiar notation $(x, y, z)$ in place of $\\left(x^{1}, x^{2}, x^{3}\\right)$.\n\n\\subsection*{10.2 CURVE THEORY; THE MOVING FRAME}\nA curve $\\mathscr{C}$ in $\\mathbf{E}^{3}$ is the image of a class $C^{3}$ mapping, $\\mathbf{r}$, from an interval $\\mathscr{I}$ of real numbers into $\\mathbf{E}^{3}$, as indicated in Fig. 10-1. The image of the real number $t$ in $\\mathscr{I}$ will be denoted\n\n\n\\begin{equation*}\n\\mathbf{r}(t) \\equiv(x(t), y(t), z(t)) \\tag{10.1}\n\\end{equation*}\n\n\na vector field of class $C^{3}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-136}\n\\end{center}\n\nFig. 10-1\n\n\\section*{Regular Curves}\nThe tangent vector of $\\mathscr{C}$ is given by\n\n\n\\begin{equation*}\n\\frac{d \\mathbf{r}}{d t} \\equiv \\dot{\\mathbf{r}}=\\left(\\frac{d x}{d t}, \\frac{d y}{d t}, \\frac{d z}{d t}\\right) \\tag{10.2}\n\\end{equation*}\n\n\n$\\mathscr{C}$ is said to be regular if $\\mathbf{r}(t) \\neq \\mathbf{0}$ for each $t$ in $\\mathscr{I}$.\n\nRemark 1: This corresponds to the definition of regularity, given in Section 7.3, in the case of a positive definite metric.\n\nEXAMPLE 10.1 An elliptical helix (Fig. 10-2) is a helix lying on an elliptical cylinder $x^{2} / a^{2}+y^{2} / b^{2}=1$ in $x y z$-space; it is given by $\\mathscr{C}: x=a \\cos t, \\quad y=b \\sin t, \\quad z=c t$, with $\\mathscr{F}$ the entire real line. The pitch is defined as the number $c$. If $a=b$, the helix is called circular, with radius $a$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137}\n\\end{center}\n\nFig. 10-2\n\nEXAMPLE 10.2 The space curve $\\mathscr{C}: x=t, \\quad y=a t^{2}, \\quad z=b t^{3} \\quad(\\mathscr{I}=\\mathbf{R})$ captures the salient local features of all curves; it is known as the twisted cubic. As indicated in Fig. 10-3, the projection of $\\mathscr{C}$ in the $x y$-plane is a parabola, $y=a x^{2}$; its projection in the $x z$-plane is a standard cubic curve, $z=b x^{3}$; in the $y z$-plane, the semicubical parabola $(y / a)^{3}=(z / b)^{2}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137(1)}\n\\end{center}\n\nFig. 10-3\n\n\\section*{Arc Length}\nSince the Euclidean metric is positive definite, every regular curve has an arc-length parameterization $\\mathbf{r}=\\mathbf{r}(s)$, such that\n\n\n\\begin{equation*}\ns=\\int_{a}^{t}\\left\\|\\frac{d \\mathbf{r}}{d u}\\right\\| d u \\quad \\text { or } \\quad \\frac{d s}{d t}=\\|\\dot{\\mathbf{r}}\\| \\tag{10.3}\n\\end{equation*}\n\n\n(The dot, as in $\\dot{\\mathbf{r}}$, is used to denote differentiation with respect to $t$, and a prime, as in $\\mathbf{r}^{\\prime}$, denotes differentiation with respect to $s$.) The mapping $t \\rightarrow s$ defined by (10.3) has the inverse relation $s \\rightarrow t$ given explicitly by $t=\\varphi(s)$, where $\\varphi$ is also differentiable:\n\n\n\\begin{equation*}\n\\frac{d t}{d s}=\\varphi^{\\prime}(s)=\\frac{1}{\\|\\dot{\\mathbf{r}}\\|} \\tag{10.4}\n\\end{equation*}\n\n\n\\section*{The Moving Frame}\nThree vectors of fundamental importance to curve theory will now be discussed. Two of them were introduced in Chapter 7: the unit tangent vector-the (unique) vector\n\n$$\n\\mathbf{T} \\equiv \\mathbf{r}^{\\prime}=\\left(\\frac{d x}{d s}, \\frac{d y}{d s}, \\frac{d z}{d s}\\right)\n$$\n\n\\begin{itemize}\n  \\item and the unit principal normal-any unit, class $C^{1}$ vector $\\mathbf{N}$ that is orthogonal to $\\mathbf{T}$ and is parallel to $\\mathbf{T}^{\\prime}$ wherever $\\mathbf{T}^{\\prime} \\neq \\mathbf{0}$. The binormal vector associated with a curve is the unit vector $\\mathbf{B} \\equiv \\mathbf{T} \\times \\mathbf{N}$ [for the cross product, see (2.10)]; B is uniquely determined once $\\mathbf{N}$ has been chosen.\n\\end{itemize}\n\nNot all regular curves have a principal normal vector (see Problem 10.1). However, it was proved in Problem 7.14 that all planar curves possess a principal normal, of the form\n\n$$\n\\mathbf{N}=(-\\sin \\theta, \\cos \\theta, 0) \\quad(\\text { plane } z=0)\n$$\n\nif $\\mathbf{T}=(\\cos \\theta, \\sin \\theta, 0)$. The following result provides further information.\n\nTheorem 10.1: Every planar curve has a principal normal vector. If a space curve has a principal normal vector, that vector lies in the plane of the curve for any nonstraight planar segment of the curve. Along any straight-line segment, the principal normal can be chosen as any class $C^{1}$ vector orthogonal to the unit tangent vector.\n\nAt each point of $\\mathscr{C}$ where $\\mathbf{N}$ can be defined, the mutually orthogonal triplet of unit vectors $\\mathbf{T}, \\mathbf{N}$, B constitutes a right-handed system of basis elements for $\\mathbf{E}^{3}$. This triad, which changes continuously along $\\mathscr{C}$ (Fig. 10-4), is often called the moving frame or moving triad; the plane of $\\mathbf{T}$ and $\\mathbf{N}$ is known as the osculating plane.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-138}\n\\end{center}\n\nFig. 10-4\n\nThe moving frame has been defined for the arc-length parameterization. When it is necessary to use the original parameter $t$ instead, the following expressions may be established (Problem 10.4) for any point at which $\\dot{\\mathbf{r}} \\neq \\mathbf{0}$ and $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$ :\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|} \\quad \\mathbf{N}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\quad \\mathbf{B}=\\varepsilon \\frac{\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\tag{10.5}\n\\end{equation*}\n\n\nHere, $\\varepsilon= \\pm 1$, the choice of sign depending on the choice of $\\mathbf{N}$ as a class $C^{1}$ vector.\n\n\\subsection*{10.3 CURVATURE AND TORSION}\nTwo important numbers, or more accurately, scalar fields, are associated with space curves.\n\nDefinition 1: The curvature $\\kappa$ and torsion $\\tau$ of a curve $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}(s) \\quad$ in $\\mathbf{E}^{3}$ are, respectively, the real numbers\n\n\n\\begin{equation*}\n\\kappa \\equiv \\mathbf{N T}^{\\prime} \\quad \\text { and } \\quad \\tau \\equiv-\\mathbf{N B}^{\\prime} \\tag{10.6}\n\\end{equation*}\n\n\nThe sign of $\\kappa$ will depend on that chosen for $\\mathbf{N}$; however, since $\\mathbf{B}$ and $\\mathbf{B}^{\\prime}$ change in sign together with $\\mathbf{N}, \\tau$ is uniquely determined.\n\nIt foliows (cf. Problem 7.13) that the absolute values of curvature and torsion are given by\n\n\n\\begin{equation*}\n\\kappa_{0} \\equiv|\\kappa|=\\left\\|\\mathbf{T}^{\\prime}\\right\\| \\quad \\text { and } \\quad \\tau_{0} \\equiv|\\tau|=\\left\\|\\mathbf{B}^{\\prime}\\right\\| \\tag{10.7}\n\\end{equation*}\n\n\nThus, $\\kappa_{0}$ measures the absolute rate of change of the unit tangent vector and the amount of \"bending\" a curve possesses at any given point, while $\\tau_{0}$ measures the absolute rate of change of the binormal and the tendency of the curve to \"twist\" out of its osculating plane at each point. The significance of negative values for $\\kappa$ and $\\tau$ will become apparent later.\n\nRemark 2: It can be shown that the two functions $\\kappa=\\kappa(s)$ and $\\tau=\\tau(s)$ determine the curve $\\mathscr{C}$ up to a rigid motion in $\\mathbf{E}^{3}$.\n\nIn the $t$-parameterization of $\\mathscr{C}$, we have (Problem 10.7):\n\n\n\\begin{equation*}\n\\kappa=\\frac{\\varepsilon\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\quad \\text { and } \\quad \\tau=\\frac{\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}} \\tag{10.8}\n\\end{equation*}\n\n\nwhere $\\varepsilon= \\pm 1$ and $[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]$ represents the $3 \\times 3$ matrix having as row vectors $\\dot{\\mathbf{r}}, \\ddot{\\mathbf{r}}$, and $\\ddot{\\mathbf{r}}$. [Recall the identity\n\n$$\n\\mathbf{a} \\cdot(\\mathbf{b} \\times \\mathbf{c})=\\operatorname{det}[\\mathbf{a} \\mathbf{b} \\mathbf{c}]\n$$\n\nfor the triple scalar product of three vectors.]\n\n\\section*{Serret-Frenet Formulas}\nThe derivatives of the vectors composing the moving triad are given by\n\n\\[\n\\left.\\begin{array}{l}\n\\mathbf{T}^{\\prime}=\\kappa \\mathbf{N}  \\tag{10.9}\\\\\n\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B} \\quad \\text { or } \\quad\\left[\\begin{array}{l}\n\\mathbf{T} \\\\\n\\mathbf{N} \\\\\n\\mathbf{B}\n\\end{array} \\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}\\right.\n\\end{array}\\right]^{\\prime}=\\left[\\begin{array}{rrr}\n0 & \\kappa & 0 \\\\\n-\\kappa & 0 & \\tau \\\\\n0 & -\\tau & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathbf{T} \\\\\n\\mathbf{N} \\\\\n\\mathbf{B}\n\\end{array}\\right]\n\\]\n\nNote the skew-symmetry of the coefficient matrix. The first of these formulas was established in Problem 7.13; the other two are derived in Problem 10.8.\n\n\\subsection*{10.4 REGULAR SURFACES}\nSurfaces are generally encountered in the calculus in the form $z=F(x, y)$; that is, as graphs of two-variable functions in three-dimensional space. Here, however, it is more convenient to adopt the\n\nDefinition 2: A surface $\\mathscr{S}$ in $\\mathbf{E}^{2}$ is the image of a $C^{3}$ vector function,\n\n$$\n\\mathbf{r}\\left(x^{1}, x^{2}\\right)=\\left(f\\left(x^{1}, x^{2}\\right), g\\left(x^{1}, x^{2}\\right), h\\left(x^{1}, x^{2}\\right)\\right)\n$$\n\nwhich maps some region $\\mathscr{V}$ of $\\mathbf{E}^{2}$ into $\\mathbf{E}^{3}$.\n\n(See Fig. 10-5; in general, primes will designate objects in the parameter plane ( $x^{i}$ ) corresponding to those on the surface in $x y z$-space.) The coordinate breakdown of the mapping $\\mathbf{r}$,\n\n\n\\begin{equation*}\nx=f\\left(x^{1}, x^{2}\\right) \\quad y=g\\left(x^{1}, x^{2}\\right) \\quad z=h\\left(x^{1}, x^{2}\\right) \\tag{10.10}\n\\end{equation*}\n\n\nis called the Gaussian form or representation of $\\mathscr{S}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-140}\n\\end{center}\n\nFig. 10-5\n\nPoint $P$ is a regular point of $\\mathscr{S}$ if\n\n\\[\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\times \\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k}  \\tag{10.11}\\\\\n\\frac{\\partial f}{\\partial x^{1}} & \\frac{\\partial g}{\\partial x^{1}} & \\frac{\\partial h}{\\partial x^{1}} \\\\\n\\frac{\\partial f}{\\partial x^{2}} & \\frac{\\partial g}{\\partial x^{2}} & \\frac{\\partial h}{\\partial x^{2}}\n\\end{array}\\right| \\neq \\mathbf{0}\n\\]\n\nat $P^{\\prime}$; otherwise, $P$ is a singular point. If every point of $\\mathscr{S}$ is a regular point, then $\\mathscr{S}$ is a regular surface.\n\nRemark 3: Condition (10.11) is tantamount to the linear independence of the two vectors $\\left(\\partial \\mathbf{r} / \\partial x^{1}\\right)_{P}$ and $\\left(\\partial \\mathbf{r} / \\partial x^{2}\\right)_{P}$. Equivalently, and of more geometrical interest, the condition ensures that every curve in $\\mathscr{S}$ through $P$ which we take to be the image under $\\mathbf{r}$ of a regular curve in $\\mathscr{V}$ through $P^{\\prime}$, is, in a neighborhood of $P$, regular in the sense of Section 10.2 .\n\nEXAMPLE 10.3 For a $C^{3}$ function $F$, show that the graph $z=F(x, y)$ is a regular surface.\n\nThe surface has the Gaussian representation\n\n$$\nx=x^{1} \\quad y=x^{2} \\quad z=F\\left(x^{1}, x^{2}\\right)\n$$\n\nand thus\n\n$$\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\times \\frac{\\partial \\mathbf{r}}{\\partial x^{2}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n1 & 0 & \\partial F / \\partial x^{1} \\\\\n0 & 1 & \\partial F / \\partial x^{2}\n\\end{array}\\right|=\\left(-\\frac{\\partial F}{\\partial x^{1}},-\\frac{\\partial F}{\\partial x^{2}}, 1\\right) \\neq \\mathbf{0}\n$$\n\nat an arbitrary surface point $P$. (This would be true if $F$ were merely class $C^{1}$.)\n\n\\section*{Subscript Notation for Partial Derivatives}\nFrom now on, write\n\n$$\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\equiv \\mathbf{r}_{1} \\quad \\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\equiv \\mathbf{r}_{2} \\quad \\frac{\\partial^{2} \\mathbf{r}}{\\partial x^{1} \\partial x^{1}}=\\mathbf{r}_{11} \\quad \\text { etc. }\n$$\n\nso that, e.g., (10.11) takes the compact form $\\mathbf{r}_{1} \\times \\mathbf{r}_{2} \\neq \\mathbf{0}$.\n\n\\subsection*{10.5 PARAMETRIC LINES; TANGENT SPACE}\nLet $\\left(x^{i}\\right)$ be taken as coordinates-for the moment, rectangular coordinates-in the parameter plane $\\mathbf{E}^{2}$, yielding two (orthogonal) families of coordinate lines:\n\n$$\n\\left\\{\\begin{array} { l } \n{ x ^ { 1 } = t } \\\\\n{ x ^ { 2 } = d }\n\\end{array} \\quad \\text { and } \\quad \\left\\{\\begin{array}{l}\nx^{1}=c \\\\\nx^{2}=\\sigma\n\\end{array}\\right.\\right.\n$$\n\nIf $(c, d)$ runs over $\\mathscr{V}$ (the pre-image of surface $\\mathscr{S}$ ), then the images under $\\mathbf{r}$ of these two families are the two sets of parametric lines (or coordinate curves) on $\\mathscr{S}$ :\n\n$$\n\\underbrace{\\mathbf{r}=\\mathbf{r}(t, d) \\equiv \\mathbf{p}(t)}_{\\boldsymbol{x}^{1} \\text {-curves }} \\quad \\underbrace{\\mathbf{r}=\\mathbf{r}(c, \\sigma) \\equiv \\mathbf{q}(\\sigma)}_{x^{2} \\text {-curves }}\n$$\n\nFigure 10-6 suggests that the net of parametric lines is orthogonal also. This is not, of course, true in general. In fact, since the tangent fields to the $x^{1}$-curves and the $x^{2}$-curves are respectively $d \\mathbf{p} / d t=\\mathbf{r}_{1}$ and $d \\mathbf{q} / d \\sigma=\\mathbf{r}_{2}$, the net is orthogonal if and only if $\\mathbf{r}_{1} \\mathbf{r}_{2}=0$ at every point of $\\mathscr{S}$.\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-141}\n\nFig. 10-6\n\nFor surface curves in general, the tangent vector of a curve passing through $\\mathbf{r}(c, d)$ is a linear combination of the vectors $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$, as the following analysis shows. Let the curve be given in the parameter plane as $\\mathscr{C}^{\\prime}: x^{1}=x^{1}(t), x^{2}=x^{2}(t)$; then the corresponding curve on the surface is\n\n$$\n\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right) \\equiv \\mathbf{r}(t)\n$$\n\nwith tangent vector\n\n\n\\begin{equation*}\n\\dot{\\mathbf{r}}=\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\frac{d x^{1}}{d t}+\\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\frac{d x^{2}}{d t} \\equiv u^{1} \\mathbf{r}_{1}+u^{2} \\mathbf{r}_{2} \\equiv u^{i} \\mathbf{r}_{i} \\tag{10.12}\n\\end{equation*}\n\n\nHere, $u^{1} \\equiv d x^{1} / d t, u^{2} \\equiv d x^{2} / d t$, so that the vector $\\left(u^{i}\\right)$ in the parameter plane is the tangent to $\\mathscr{C}^{\\prime}$ at $P^{\\prime}$ (see Fig. 10-6).\n\nDefinition 3: The collection of linear combinations of the vectors $\\mathbf{r}_{1}(P)$ and $\\mathbf{r}_{2}(P)$ is called the tangent space of $\\mathscr{S}$ at $P$. The unit surface normal is the unit vector $\\mathbf{n}$ in the direction of $\\mathbf{r}_{1} \\times \\mathbf{r}_{2}$ :\n\n\n\\begin{equation*}\n\\mathbf{n}=\\frac{1}{E}\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right) \\quad\\left(E \\equiv\\left\\|\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right\\|>0\\right) \\tag{10.13}\n\\end{equation*}\n\n\nThe geometric realization of the tangent space is obviously the tangent plane at $P$, and the surface normal can be identified with a line segment through $P$ perpendicular to this tangent plane; that is, orthogonal to the surface at $P$, as indicated in Fig. 10-6.\n\nTo summarize this whole affair, the linearly independent (by regularity) triad of vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}$ forms a moving frame for the surface, as shown in Fig. 10-7, much in the manner that a moving triad exists for a regular curve having a principal normal.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-142}\n\\end{center}\n\nFig. 10-7\n\n\\subsection*{10.6 FIRST FUNDAMENTAL FORM}\nConsider a curve on the regular surface $\\mathscr{S}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}, x^{2}\\right)$ given by $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right) \\equiv \\mathbf{r}(t)$, with pre-image $\\mathscr{C}^{\\prime}: x^{i}=x^{i}(t)$ in the parameter plane. Using (10.12) and recalling that the (Euclidean) inner product is distributive over linear combinations of vectors, arc length along $\\mathscr{C}$ is calculated as\n\n\n\\begin{equation*}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\|\\dot{\\mathbf{r}}\\|^{2}=\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}=\\left(u^{i} \\mathbf{r}_{i}\\right)\\left(u^{j} \\mathbf{r}_{j}\\right) \\equiv g_{i j} u^{i} u^{j} \\tag{10.14a}\n\\end{equation*}\n\n\nin which\n\n\n\\begin{equation*}\ng_{i j}=\\mathbf{r}_{i} \\mathbf{r}_{j} \\quad(1 \\leqq i, j \\leqq 2) \\tag{10.15}\n\\end{equation*}\n\n\nand, as above, $u^{i}=d x^{i} / d t$. In the equivalent differential form,\n\n\n\\begin{equation*}\nd s^{2}=g_{i j} d x^{i} d x^{j} \\equiv \\mathrm{I} \\tag{10.14b}\n\\end{equation*}\n\n\nthe arc-length formula is known as the First Fundamental Form (abbreviated FFF) of the surface $\\mathscr{S}$. In view of (10.12) and the regularity of $\\mathscr{S},\\|\\dot{\\mathbf{r}}\\|=0$ if and only if $u^{1}=u^{2}=0$; this proves\n\nLemma 10.2: The FFF of a regular surface is positive definite.\n\nLemma 10.2 implies that $g \\equiv \\operatorname{det}\\left(g_{i j}\\right)>0$; in fact, we can use Lagrange's identity,\n\n$$\n\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right)^{2}=\\left(\\mathbf{r}_{1}^{2}\\right)\\left(\\mathbf{r}_{2}^{2}\\right)-\\left(\\mathbf{r}_{1} \\mathbf{r}_{2}\\right)^{2}\n$$\n\nto establish that\n\n\n\\begin{equation*}\ng=E^{2} \\tag{10.16}\n\\end{equation*}\n\n\ncf. (10.13).\n\nEXAMPLE 10.4 Compute the FFF for the right helicoid (Fig. 10-8),\n\n$$\n\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, a x^{2}\\right)\n$$\n\nWe have:\n\nwhence\n\n$$\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, a\\right)\n$$\n\nwhence\n\n$$\ng_{11}=\\mathbf{r}_{1}^{2}=\\cos ^{2} x^{2}+\\sin ^{2} x^{2}+0^{2}=1\n$$\n\n$$\n\\begin{aligned}\n& g_{12}=\\mathbf{r}_{1} \\mathbf{r}_{2}=\\left(\\cos x^{2}\\right)\\left(-x^{1} \\sin x^{2}\\right)+\\left(\\sin x^{2}\\right)\\left(x^{1} \\cos x^{2}\\right)=0 \\\\\n& g_{22}=\\mathbf{r}_{2}^{2}=\\left(-x^{1}\\right)^{2}\\left(\\sin ^{2} x^{2}\\right)+\\left(x^{1}\\right)^{2}\\left(\\cos ^{2} x^{2}\\right)+a^{2}=\\left(x^{1}\\right)^{2}+a^{2}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\mathbf{I}=\\left(d x^{1}\\right)^{2}+\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]\\left(d x^{2}\\right)^{2}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-143}\n\\end{center}\n\nFig. 10-8\n\nAlong with the FFF, tensor calculus enters the picture. For the intrinsic properties of a particular surface $\\mathscr{S}$ in $\\mathbf{E}^{3}$ (the properties defined by measurements of distance on the surface) are all implicit in $(10.14 b)$, which can be interpreted as a particular Riemannian metrization of the parameter plane. Thus, the study of intrinsic properties of surfaces becomes the tensor analysis of Riemannian metrics in $\\mathbf{R}^{2}$ - and this may be conducted without any reference to $\\mathbf{E}^{3}$ whatever. Observe that the metrics under consideration will all be positive definite (Lemma 10.2) but not necessarily Euclidean (see Theorem 9.1). Accordingly, we shall drop the designation $\\mathbf{E}^{2}$ for the parameter plane, which shall henceforth be referred to general coordinates $\\left(x^{i}\\right)$.\n\nEXAMPLE 10.5 The metric for $\\mathbf{R}^{2}$ corresponding to the right helicoid (Example 10.4) is non-Euclidean, as is demonstrated in Problem 10.27. Now the parameters $x^{1}$ and $x^{2}$, which are actual polar coordinates in the $x y$-plane of $\\mathbf{E}^{3}$ (see Fig. 10-8), formally keep that significance when the plane is considered abstractly as parameter space. This is an instance of the formal use of a familiar coordinate system in a non-Euclidean space, as mentioned in Section 3.1.\n\n\\section*{Unit Tangent Vector}\nIf $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right)$ is any curve on $\\mathscr{S}$, then by (10.12) and (10.14a),\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{j k} u^{j} u^{k}}} \\tag{10.17}\n\\end{equation*}\n\n\n\\section*{Angle Between Two Curves}\nLet $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ be two intersecting curves on $\\mathscr{S}$ that correspond to $x^{i}=\\phi^{i}(t)$ and $x^{i}=\\psi^{i}(\\sigma) \\quad(i=$ $1,2)$ in the parameter plane. Writing $u^{i} \\equiv d \\phi^{i} / d t$ and $v^{i} \\equiv d \\psi^{i} / d \\sigma$, we have for the angle $\\theta$ between $\\mathbf{T}_{1}$ of $\\mathscr{C}_{1}$ and $\\mathbf{T}_{2}$ of $\\mathscr{C}_{2}$ :\n\n\n\\begin{equation*}\n\\cos \\theta=\\mathbf{T}_{1} \\mathbf{T}_{2}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{p q} u^{p} u^{q}}} \\cdot \\frac{v^{j} \\mathbf{r}_{j}}{\\sqrt{g_{r s} v^{r} v^{s}}}=\\frac{g_{i j} u^{i} v^{j}}{\\sqrt{g_{p q} u^{p} u^{q}} \\sqrt{g_{r s} v^{r} v^{s}}} \\tag{10.18}\n\\end{equation*}\n\n\nCompare (5.11).\n\nTheorem 10.3: The angle between the two parametric lines through a surface point is\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{g_{12}}{\\sqrt{g_{11}} \\sqrt{g_{22}}} \\tag{10.19}\n\\end{equation*}\n\n\nCorollary 10.4: The two families of parametric lines form an orthogonal net if and only if $g_{12}=0$ at every point of $\\mathscr{S}$.\n\n\\subsection*{10.7 GEODESICS ON A SURFACE}\nA further link with tensors is provided by the concept of geodesics for regular surfaces. One can intuitively imagine stretching a string between two points on a surface, and pulling it tight: on a sphere this would lead to a great circular arc, and on a right circular cylinder, a helical arc. Since from our point of view the surface is disregarded and $\\left(g_{i j}\\right)$ is taken as a metric for the parameter plane, the problem has already been worked out (Section 7.6).\n\nRelative to the FFF of $\\mathscr{S}$, define the Christoffel symbols through formulas (6.1) and (6.4), $n=2$. [Problem 10.48 gives an equivalent \"extrinsic\" definition, in terms of the vector $\\mathbf{r}$.] Then a geodesic on $\\mathscr{S}$ is any curve $\\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right)$ in the surface whose pre-image in the parameter $\\mathbf{R}^{2}$ satisfies the system of differential equations (7.11)-(7.12); if $t=s=$ arc length, the governing system is (7.13). [Remember that the (non-Euclidean) distance measured by $s$ in $\\mathbf{R}^{2}$ is the Euclidean distance along the geodesic as a curve in $\\mathbf{E}^{3}$.]\n\nSimilarly, harking back to Section 6.5, the intrinsic curvature of a curve $\\mathscr{C}$ in $\\mathscr{S}$ is the function\n\n\n\\begin{equation*}\n\\tilde{\\kappa}(s)=\\sqrt{g_{i j} b^{i} b^{j}} \\tag{10.20}\n\\end{equation*}\n\n\n-cf. (6.12)-where the intrinsic curvature vector ( $b^{i}$ ) (in $\\mathbf{R}^{2}$ ) is given by (6.11).\n\nRemark 4: Intrinsic curvature can be shown to be the instantaneous rate of change of the angle between the tangent vector of $\\mathscr{C}$ and another vector in the tangent space that is \"transported parallelly\" along the curve. Here, the term \"parallel\" refers to a certain generalization of Euclidean parallelism (see Problem 10.22).\n\nTheorem 10.5: A curve on a surface is a geodesic if and only if its intrinsic curvature $\\tilde{\\kappa}$ is identically zero.\n\nIn contrast to the above intrinsic characterization of geodesics, there is an interesting and useful extrinsic characterization, proved as Problem 10.18. It adds a visual dimension that often allows the immediate identification of a geodesic.\n\nTheorem 10.6: A curve on a regular surface is a geodesic if and only if a principal normal $\\mathbf{N}$ of the curve can be chosen that coincides with the surface normal $\\mathbf{n}$ at all points along the curve.\n\n\\subsection*{10.8 SECOND FUNDAMENTAL FORM}\nBy taking the dot product of the surface normal with the second partial derivatives of $\\mathbf{r}$ with respect to $x^{1}$ and $x^{2}$,\n\n\n\\begin{equation*}\nf_{i j} \\equiv \\mathbf{n} \\mathbf{r}_{i j} \\tag{10.21}\n\\end{equation*}\n\n\nwe generate the coefficients of the Second Fundamental Form (SFF) of a surface:\n\n\n\\begin{equation*}\nf_{i j} d x^{i} d x^{j} \\equiv \\mathrm{II} \\tag{10.22}\n\\end{equation*}\n\n\n\\section*{Curvature of a Normal Section}\nIf $\\mathscr{F}$ is a plane containing the surface normal $\\mathbf{n}$ at some point $P$ of $\\mathscr{S}$ (Fig. 10-9), the curvature of the normal section of $\\mathscr{S}$ (the curve of intersection of $\\mathscr{S}$ and $\\mathscr{F}$ ), denoted $\\mathscr{C}_{\\mathscr{F}}$, is given at point $P$ by the formula\n\n\n\\begin{equation*}\n\\kappa_{\\mathscr{F}}=\\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}=\\frac{\\mathrm{II}}{\\mathrm{I}} \\tag{10.23}\n\\end{equation*}\n\n\nwhere $\\left(u^{i}\\right)=\\left(d x^{i} / d t\\right)$ gives the direction, at $P^{\\prime}$, of the curve corresponding to $C_{\\mathscr{F}}$ in the parameter plane; see Problem 10.23.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-145}\n\\end{center}\n\nFig. 10-9\n\nAs $\\mathscr{F}$ rotates about $\\mathbf{n}$, the curvature $\\kappa_{\\mathscr{F}}$ of $\\mathscr{C}_{\\mathscr{F}}$ at $P$ is periodic and will reach an absolute maximum and an absolute minimum; let\n\n\n\\begin{equation*}\n\\max \\kappa_{\\mathscr{F}} \\equiv \\kappa_{1} \\quad \\min \\kappa_{\\mathscr{F}} \\equiv \\kappa_{2} \\tag{10.24}\n\\end{equation*}\n\n\nThe two section curves having these two extremal curvatures are called principal curves, and their directions are the principal directions. If $\\kappa_{1}=\\kappa_{2}$ at $P$, all the normal sections at $P$ have the same curvature and unique principal directions do not exist. (In this case, $P$ is called an umbilical point of the surface.)\n\n\\section*{Surface Curvature}\nTwo measures of the curvature of a surface $\\mathscr{S}$ are commonly used.\n\nDefinition 4: The Gaussian curvature of $\\mathscr{S}$ at point $P$ is the number $\\mathrm{K}=\\kappa_{1} \\kappa_{2}$; the mean curvature is the number $\\mathrm{H}=\\kappa_{1}+\\kappa_{2}$.\n\nIt will be proved in Problem 10.25 that the extreme curvatures $\\kappa_{1}$ and $\\kappa_{2}$ are the roots of the following quadratic equation in $\\lambda$ :\n\n\n\\begin{equation*}\n\\left(g_{11} g_{22}-g_{12}^{2}\\right) \\lambda^{2}-\\left(f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}\\right) \\lambda+\\left(f_{11} f_{22}-f_{12}^{2}\\right)=0 \\tag{10.25}\n\\end{equation*}\n\n\nThe relations between the roots and the coefficients of a polynomial equation then give:\n\n\n\\begin{equation*}\n\\mathbf{K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}} \\quad \\mathbf{H}=\\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}} \\tag{10.26}\n\\end{equation*}\n\n\n\\subsection*{10.9 STRUCTURE FORMULAS FOR SURFACES}\nTwo fundamental sets of relationships involve the parts of the moving triad of a surface, $\\left(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}\\right)$.\n\n\\section*{Equations of Weingarten}\nSince $\\mathbf{n}^{2}=1, \\partial\\left(\\mathbf{n}^{2}\\right) / \\partial x^{i}=2 \\mathbf{n n}_{i}=0 \\quad(i=1,2)$. Hence, for each $i, \\mathbf{n}_{i}$ lies in the tangent space: $\\mathbf{n}_{i}=u_{i}^{1} \\mathbf{r}_{1}+u_{i}^{2} \\mathbf{r}_{2}$, for certain scalars $u_{i}^{k}$. Similarly, from orthogonality,\n\n$$\n0=\\left(\\mathbf{n r}_{i}\\right)_{j}=\\mathbf{n}_{j} \\mathbf{r}_{i}+\\mathbf{n} \\mathbf{r}_{i j} \\quad \\text { or } \\quad \\mathbf{n}_{j} \\mathbf{r}_{i}=-f_{i j}\n$$\n\nIt follows (Problem 10.28) that\n\n\n\\begin{equation*}\n\\mathbf{n}_{i}=-g^{j k} f_{i j} \\mathbf{r}_{k} \\tag{10.27a}\n\\end{equation*}\n\n\nfor $i=1,2$. From the explicit form of the inverse metric matrix $\\left(g^{i j}\\right),(10.27 a)$ may be spelled out as follows:\n\n\n\\begin{align*}\n& \\mathbf{n}_{1}=\\frac{g_{12} f_{12}-g_{22} f_{11}}{g} \\mathbf{r}_{1}+\\frac{g_{12} f_{11}-g_{11} f_{12}}{g} \\mathbf{r}_{2} \\\\\n& \\mathbf{n}_{2}=\\frac{g_{12} f_{22}-g_{22} f_{12}}{g} \\mathbf{r}_{1}+\\frac{g_{12} f_{12}-g_{11} f_{22}}{g} \\mathbf{r}_{2} \\tag{10.27b}\n\\end{align*}\n\n\n\\section*{Equations of Gauss}\nSince $\\left(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}\\right)$ is a basis for $\\mathbf{E}^{3}$, we can write $\\mathbf{r}_{i j}=u_{i j}^{1} \\mathbf{r}_{1}+u_{i j}^{2} \\mathbf{r}_{2}+u_{i j}^{3} \\mathbf{n}$. Evaluation of the coefficients (Problem 10.29) leads to\n\n\n\\begin{equation*}\n\\mathbf{r}_{i j}=\\Gamma_{i j}^{k} \\mathbf{r}_{k}+f_{i j} \\mathbf{n} \\tag{10.28}\n\\end{equation*}\n\n\n\\section*{An Identity Between FFF and SFF}\nSince $\\mathbf{r}_{i j k}=\\mathbf{r}_{i k j}$, (10.28) implies $\\left(\\Gamma_{i j}^{s} \\mathbf{r}_{s}+f_{i j} \\mathbf{n}\\right)_{k}=\\left(\\Gamma_{i k}^{s} \\mathbf{r}_{s}+f_{i k} \\mathbf{n}\\right)_{j}$, or\n\n$$\n\\left(\\Gamma_{i j}^{s}\\right)_{k} \\mathbf{r}_{s}+\\Gamma_{i j}^{s} \\mathbf{r}_{s k}+f_{i j k} \\mathbf{n}+f_{i j} \\mathbf{n}_{k}=\\left(\\Gamma_{i k}^{s}\\right)_{j} \\mathbf{r}_{s}+\\Gamma_{i k}^{s} \\mathbf{r}_{s j}+f_{i k j} \\mathbf{n}+f_{i k} \\mathbf{n}_{j}\n$$\n\nDot both sides with $\\mathbf{r}_{l}$ and use the definition $\\mathbf{r}_{l} \\mathbf{r}_{s} \\equiv g_{l s}$ and the relations $\\mathbf{r}_{l} \\mathbf{r}_{s k}=\\Gamma_{s k l}$ (Problem 10.48) and $\\mathbf{r}_{l} \\mathbf{n}=0$ :\n\n$$\n\\left(\\Gamma_{i j}^{s}\\right)_{k} g_{s l}+\\Gamma_{i j}^{s} \\Gamma_{s k l}+f_{i j} \\mathbf{n}_{k} \\mathbf{r}_{l}=\\left(\\Gamma_{i k}^{s}\\right)_{j} g_{s l}+\\Gamma_{i k}^{s} \\Gamma_{s j l}+f_{i k} \\mathbf{n}_{j} \\mathbf{r}_{l}\n$$\n\nNow substitute for the $\\mathbf{n}_{i}$ from (10.27a) and use $\\mathbf{r}_{t} \\mathbf{r}_{l} \\equiv g_{t l}$ and $g^{s t} g_{t l}=\\delta_{l}^{s}$ to simplify the result:\n\n$$\n-f_{i j} f_{k l}+f_{i k} f_{j l}=g_{s l}\\left(\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\dot{\\Gamma}_{r k}^{s}\\right)\n$$\n\nFinally, introducing the Riemann tensor via (8.2) and (8.3), we obtain\n\n\n\\begin{equation*}\nR_{i i k l}=f_{i k} f_{i l}-f_{i l} f_{i k} \\tag{10.29}\n\\end{equation*}\n\n\nThe left member of (10.29) depends only on the coefficients of I together with their first and second derivatives; the right member depends only on the coefficients of II. This essential compatibility relation between the two fundamental forms must hold at every point of a regular surface.\n\n\\section*{The 'Most Excellent Theorem' of Gauss}\nBy (10.26) and (10.29),\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{R_{1212}}{g} \\tag{10.30}\n\\end{equation*}\n\n\nThus, the numerator of $\\mathrm{K}$ can be derived entirely from the FFF. Since the denominator is also obviously from the FFF, we have:\n\nTheorem 10.7 (Theorema Egregium): The Gaussian curvature is an intrinsic property of a surface, depending only on the First Fundamental Form and its derivatives.\n\nRemark 5: The motive for the definition (8.7) of Riemannian curvature is now apparent.\n\n\\subsection*{10.10 ISOMETRIES}\nThe practical question of whether inhabitants of a fog-enshrouded planet could, solely by measuring distances on the surface of the planet, determine its curvature, is answered in the affirmative by Theorem 10.7. A further important conclusion can be drawn.\n\nSuppose that two surfaces, $\\mathscr{S}^{(1)}: \\mathbf{r}^{(1)}=\\mathbf{r}^{(1)}\\left(x^{1}, x^{2}\\right)$ and $\\mathscr{P}^{(2)}: \\mathbf{r}^{(2)}=\\mathbf{r}^{(2)}\\left(x^{1}, x^{2}\\right)$, are defined over the same region $\\mathscr{V}$ of the plane and that the First Fundamental Forms agree on $\\mathscr{V}$. This will obviously set up a correspondence between $\\mathscr{S}^{(1)}$ and $\\mathscr{S}^{(2)}$ in $\\mathbf{E}^{3}$ that is bijective between small patches (induced by the neighborhoods of $\\mathscr{V}$ over which both $\\mathbf{r}^{(i)}$ are bijective) of the two surfaces. This correspondence is called a local isometry between $\\mathscr{S}^{(1)}$ and $\\mathscr{S}^{(2)}$ because the two surfaces are, patch for patch, metrically identical. But then (Theorem 10.7) the Gaussian curvatures $K^{(1)}$ and $K^{(2)}$ must be equal at corresponding points.\n\nTheorem 10.8: If two surfaces are locally isometric, their Gaussian curvatures are identical.\n\nIn the case of constant Gaussian curvature K, Beltrami's theorem tells us that there is a parameterization for $\\mathscr{S}$ for which the FFF takes on the form:\n\n$$\n\\begin{array}{ll}\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\sinh ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=-1 / a^{2}<0 \\\\\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=0 \\\\\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\sin ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=1 / a^{2}>0\n\\end{array}\n$$\n\nEXAMPLE 10.6 The plane and the sphere are surfaces of constant zero curvature and constant positive curvature, respectively. For a surface of constant negative curvature, see Problem 10.49.\n\nBeltrami's theorem implies a partial converse of Theorem 10.8:\n\nTheorem 10.9 (Minding's Theorem): If two surfaces are of the same constant Gaussian curvature, they are locally isometric.\n\nRemark 6: A proof of Theorem 10.9 for zero curvature was given in Problem 9.9.\n\n\\section*{Solved Problems}\n\\section*{CURVE THEORY; THE MOVING FRAME}\n10.1 The curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array} { l } \n{ x = t } \\\\\n{ y = t ^ { 4 } } \\\\\n{ z = 0 }\n\\end{array} \\quad ( t < 0 ) \\quad \\left\\{\\begin{array}{l}\nx=t \\\\\ny=0 \\\\\nz=t^{4}\n\\end{array} \\quad(t \\geqq 0)\\right.\\right.\n$$\n\nlies partly in the $x y$-plane and partly in the $x z$-plane (Fig. 10-10). Show that it is regular of class $C^{3}$, but that it possesses no principal normal vector.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-148}\n\\end{center}\n\nFig. 10-10\n\nThe component functions for $\\mathbf{r}(t)$ are\n\n$$\nx(t)=t \\quad y(t)=\\left\\{\\begin{array}{ll}\nt^{4} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad z(t)= \\begin{cases}0 & t<0 \\\\\nt^{4} & t \\geqq 0\\end{cases}\\right.\n$$\n\nWhen $t<0, \\dot{y}(t)=4 t^{3}$. As $t \\rightarrow 0$,\n\n$$\n\\lim _{t \\rightarrow-0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow-0} \\frac{t^{4}}{t}=0 \\quad \\lim _{t \\rightarrow+0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow+0} \\frac{0}{t}=0\n$$\n\nhence, $y(t)$ is differentiable at $t=0$. Clearly, $\\dot{y}(t)=0$ for $t>0$. A similar analysis applies to $z(t)$. Hence:\n\n$$\n\\dot{y}(t)=\\left\\{\\begin{array}{ll}\n4 t^{3} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dot{z}(t)= \\begin{cases}0 & t<0 \\\\\n4 t^{3} & t \\geqq 0\\end{cases}\\right.\n$$\n\nwhich are continuous functions. Continuing the analysis up to the third derivatives:\n\n$$\n\\dddot{y}(t)=\\left\\{\\begin{array}{ll}\n24 t & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dddot{z}(t)= \\begin{cases}0 & t<0 \\\\\n24 t & t \\geqq 0\\end{cases}\\right.\n$$\n\nHence, $x(t)$ being differentiable to all orders, $\\mathbf{r}(t)$ is of class $C^{3}$. Furthermore, because $\\dot{x}(t) \\equiv 1, \\dot{\\mathbf{r}}(t) \\neq 0$ for all $t$ and $\\mathscr{C}$ is regular. However, the principal normal, which exists for the separate parts of $\\mathscr{C}$ (lying in the $x y$-plane for $t<0$ and in the $x z$-plane for $t>0$ ), cannot possibly be continuous at $t=0$, let alone differentiable. Hence, $\\mathscr{C}$ does not possess a principal normal.\n\n10.2 (a) Describe the curve $\\mathbf{r}=\\left(\\cos t, \\sin t, \\tan ^{-1} t\\right)$, where $0 \\leqq t$ and where the principal value of the arctangent is understood. (b) Find the arc length between the points $\\mathbf{r}(0)$ and $\\mathbf{r}(1)$.\\\\\n(a) This is a form of the circular helix, except that the pitch decreases with increasing $t$. The curve lies on the right circular cylinder $x^{2}+y^{2}=1$; beginning at $(1,0,0)$, it winds around the cylinder and approaches the circle $x^{2}+y^{2}=1, z=\\pi / 2$ asymptotically as $t \\rightarrow \\infty$.\n\n(b)\n\n$$\n\\dot{\\mathbf{r}}=\\left(-\\sin t, \\cos t, \\frac{1}{t^{2}+1}\\right) \\quad \\text { or } \\quad \\frac{d s}{d t}=\\sqrt{\\sin ^{2} t+\\cos ^{2} t+\\frac{1}{\\left(t^{2}+1\\right)^{2}}}\n$$\n\nA numerical method of integration is required. Using Simpson's rule on a programmable calculator, one obtains\n\n$$\nL=\\int_{0}^{1} \\frac{\\sqrt{t^{4}+2 t^{2}+2}}{t^{2}+1} d t \\approx 1.27797806\n$$\n\n10.3 Find the moving frame for the curve\n\n$$\n\\mathscr{C}: \\mathbf{r}=\\left(\\frac{3-3 t^{3}}{5}, \\frac{4+4 t^{3}}{5}, 3 t\\right) \\quad(t \\text { real })\n$$\n\nShow that the binormal vector $\\mathbf{B}$ is constant, so that the curve is actually planar.\n\nMaking the calculations required in (10.5):\n\nand\n\n$$\n\\dot{\\mathbf{r}}=\\left(\\frac{-9 t^{2}}{5}, \\frac{12 t^{2}}{5}, 3\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{\\frac{81}{25} t^{4}+\\frac{144}{25} t^{4}+9}=3 \\sqrt{t^{4}+1}\n$$\n\n$$\n\\mathbf{T}=\\frac{\\left(-9 t^{2} / 5,12 t^{2} / 5,3\\right)}{3 \\sqrt{t^{4}+1}}=\\frac{\\left(-3 t^{2}, 4 t^{2}, 5\\right)}{5 \\sqrt{t^{4}+1}}\n$$\n\n$$\n\\begin{aligned}\n& \\ddot{\\mathbf{r}}=\\left(\\frac{-18 t}{5}, \\frac{24 t}{5}, 0\\right) \\\\\n& \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{9}{5} t^{2} & \\frac{12}{5} t^{2} & 3 \\\\\n-\\frac{18 t}{5} & \\frac{24 t}{5} & 0\n\\end{array}\\right|=\\left(-\\frac{72 t}{5},-\\frac{54 t}{5}, 0\\right)=\\frac{-18 t}{5}(4,3,0) \\\\\n& \\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|=\\frac{18|t|}{5} \\sqrt{4^{2}+3^{2}+0^{2}}=18|t| \\\\\n& (\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}=\\left(9 t^{4}+9\\right)\\left(-\\frac{18}{5} t, \\frac{24}{5} t, 0\\right)=\\left(-\\frac{162}{5} t^{5}-\\frac{162}{5} t, \\frac{216}{5} t^{5}+\\frac{216}{5} t, 0\\right) \\\\\n& (\\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(\\frac{9 \\cdot 18}{25} t^{3}+\\frac{24 \\cdot 12}{25} t^{3}+0\\right)\\left(-\\frac{9}{5} t^{2}, \\frac{12}{5} t^{2}, 3\\right)=\\left(-\\frac{162}{5} t^{5}, \\frac{216}{5} t^{5}, 54 t^{3}\\right) \\\\\n& (\\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(-\\frac{162}{5} t, \\frac{216}{5} t,-54 t^{3}\\right)=18 t\\left(-\\frac{9}{5}, \\frac{12}{5},-3 t^{2}\\right) \\\\\n& \\quad \\mathbf{N}=\\varepsilon \\frac{-18 t\\left(9 / 5,-12 / 5,3 t^{2}\\right)}{\\left(3 \\sqrt{t^{4}+1}\\right)(18|t|)}=-\\frac{\\varepsilon t}{|t|} \\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}}\n\\end{aligned}\n$$\n\nand\n\nNow choose $\\varepsilon=+1$ when $t<0$ and -1 otherwise, making\n\n$$\n\\mathbf{N}=\\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}} \\quad \\mathbf{B}=\\varepsilon \\frac{(-18 t / 5)(4,3,0)}{18|t|}=\\frac{-\\varepsilon t}{|t|}\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)=\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)\n$$\n\n10.4 Establish the general formulas (10.5) for the moving frame of the curve $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}(t)$, with an arbitrary parameter $t$.\n\nBy definition,\n\n$$\n\\frac{d s}{d t}=\\|\\dot{\\mathbf{r}}\\| \\equiv(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{1 / 2} \\quad \\text { or } \\quad \\frac{d t}{d s}=\\|\\dot{\\mathbf{r}}\\|^{-1}\n$$\n\nand we have at once for the unit tangent vector\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\dot{\\mathbf{r}} \\frac{d t}{d s}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\nTo obtain a principal normal, first calculate\n\n$$\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\| \\equiv \\frac{d}{d t}(\\dot{\\mathbf{r}})^{1 / 2}=\\frac{1}{2}(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{-1 / 2}(\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}}+\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}})=\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\n(Note the general formula $d\\|\\mathbf{u}\\| / d t=\\mathbf{u \\dot { u }} /\\|\\mathbf{u}\\|$.) Hence,\n\nand\n\n$$\n\\begin{gathered}\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=-\\|\\dot{\\mathbf{r}}\\|^{-2} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|=-\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\dot{\\mathbf{T}}=\\ddot{\\mathbf{r}} \\frac{d t}{d s}+\\dot{\\mathbf{r}} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=\\frac{\\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}-\\frac{(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\mathbf{T}^{\\prime}=\\dot{\\mathbf{T}} \\frac{d t}{d s}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{4}}=-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\n\\end{gathered}\n$$\n\nwhere the last step used the vector identify $\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}$. It follows that $\\mathbf{N}$ can be constructed by normalizing the vector\n\n$$\n\\mathbf{N}^{*} \\equiv-\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\n$$\n\nSince $\\dot{\\mathbf{r}}$ and $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$ are orthogonal, $\\left\\|\\mathbf{N}^{*}\\right\\|=\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|$ and so, provided $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{N}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nFinally, for the binormal vector, with $\\mathbf{v} \\equiv \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|} \\times \\varepsilon \\frac{\\mathbf{v} \\times \\dot{\\mathbf{r}}}{\\|\\mathbf{v}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\mathbf{v}-(\\dot{\\mathbf{r}} \\mathbf{v}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\mathbf{v}-(0) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$\n\n\\section*{CURVATURE AND TORSION}\n10.5 Find the curvature and the torsion of the circular helix\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c=\\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nwhere $s$ is arc length.\n\nBy differentiation with respect to arc length,\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\left(-\\frac{a}{c} \\sin \\frac{s}{c}, \\frac{a}{c} \\cos \\frac{s}{c}, \\frac{b}{c}\\right) \\quad \\mathbf{T}^{\\prime}=\\left(-\\frac{a}{c^{2}} \\cos \\frac{s}{c},-\\frac{a}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nNormalizing $\\mathbf{T}^{\\prime}$, choose\n\n$$\n\\mathbf{N}=\\left(-\\cos \\frac{s}{c},-\\sin \\frac{s}{c}, 0\\right)\n$$\n\nand, correspondingly,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{a}{c} \\sin \\frac{s}{c} & \\frac{a}{c} \\cos \\frac{s}{c} & \\frac{b}{c} \\\\\n-\\cos \\frac{s}{c} & -\\sin \\frac{s}{c} & 0\n\\end{array}\\right|=\\left(\\frac{b}{c} \\sin \\frac{s}{c},-\\frac{b}{c} \\cos \\frac{s}{c}, \\frac{a}{c}\\right)\n$$\n\n$$\n\\mathbf{B}^{\\prime}=\\left(\\frac{b}{c^{2}} \\cos \\frac{s}{c}, \\frac{b}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nThen, by (10.6),\n\n$$\n\\kappa=\\frac{a}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{a}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{a}{c^{2}} \\quad \\tau=\\frac{b}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{b}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{b}{c^{2}}\n$$\n\n[If we introduce the \"time\" parameter $t=c s$, we then have:\n\n$$\n\\frac{d z}{d t}=\\frac{b}{c^{2}}=\\tau\n$$\n\ni.e. the rate at which the helix rises out of the $x y$-plane (its osculating plane at $t=0$ ) is given by its (constant) torsion.]\n\n10.6 Find the curvature and torsion of the curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(t$ real $)$.\n\nUse the formulas $(10.8)$ :\n\n$$\n\\begin{gathered}\n\\dot{\\mathbf{r}}=\\left(2 t+\\sqrt{2}, 2 t-\\sqrt{2}, 2 t^{2}\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{(2 t+\\sqrt{2})^{2}+(2 t-\\sqrt{2})^{2}+4 t^{4}}=2\\left(t^{2}+1\\right) \\\\\n\\ddot{\\mathbf{r}}=(2,2,4 t) \\quad \\ddot{\\mathbf{r}}=(0,0,4) \\\\\n\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n2 t+\\sqrt{2} & 2 t-\\sqrt{2} & 2 t^{2} \\\\\n2 & 2 & 4 t\n\\end{array}\\right|=4\\left(t^{2}-t \\sqrt{2},-\\left(t^{2}+t \\sqrt{2}\\right), \\sqrt{2}\\right) \\\\\n(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})^{2}=16\\left[\\left(t^{2}-t \\sqrt{2}\\right)^{2}+\\left(t^{2}+t \\sqrt{2}\\right)^{2}+2\\right]=32\\left(t^{2}+1\\right)^{2} \\\\\n\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]=\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=(0,0,4) \\cdot 4\\left(t^{2}-t \\sqrt{2},-t^{2}-t \\sqrt{2}, \\sqrt{2}\\right)=16 \\sqrt{2}\n\\end{gathered}\n$$\n\nHence\n\n$$\n\\kappa=\\frac{\\varepsilon \\sqrt{32\\left(t^{2}+1\\right)^{2}}}{8\\left(t^{2}+1\\right)^{3}}=\\frac{\\varepsilon}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}} \\quad \\tau=\\frac{16 \\sqrt{2}}{32\\left(t^{2}+1\\right)^{2}}=\\frac{1}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}}\n$$\n\n\\subsection*{10.7 Prove (10.8).}\nUsing the results of Problem 10.4, we have\n\n$$\n\\begin{aligned}\n\\kappa=\\mathbf{N T}^{\\prime} & =\\left(\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\\right) \\cdot\\left(-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\\right)=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\\|^{2}}{\\|\\dot{\\mathbf{r}}\\|^{5}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\\\\n& =\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2} \\sin ^{2}(\\pi / 2)}{\\|\\dot{\\mathbf{r}}\\|^{5} \\mid \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}{\\|\\dot{\\mathbf{r}}\\|^{3}}\n\\end{aligned}\n$$\n\nThe torsion requires the computation of $\\mathbf{B}^{\\prime}$. By (10.5),\n\n$$\n\\varepsilon \\mathbf{B}=\\frac{\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\equiv \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$\n\nwhence\n\n$$\n\\varepsilon_{\\varepsilon} \\dot{\\mathbf{B}}=\\frac{d}{d t}\\left(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\right)=\\frac{1}{\\|\\mathbf{v}\\|} \\dot{\\mathbf{v}}+\\frac{d}{d t}\\left(\\frac{1}{\\|\\mathbf{v}\\|}\\right) \\mathbf{v}=\\frac{\\dot{\\mathbf{v}}}{\\|\\mathbf{v}\\|}-\\frac{(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}=\\frac{\\|\\mathbf{v}\\|^{2} \\dot{\\mathbf{v}}-(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}\n$$\n\nBut $\\dot{\\mathbf{r}}=d(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) / d t=(\\ddot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})+(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$; hence,\n\n$$\n\\varepsilon \\mathbf{B}^{\\prime}=\\frac{\\varepsilon \\dot{\\mathbf{B}}}{\\|\\dot{\\mathbf{r}}\\|}=\\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})-[(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})](\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{3}}\n$$\n\nDot this with\n\n$$\n\\varepsilon \\mathbf{N}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nfrom (10.5), and use $\\mathbf{u} \\cdot(\\mathbf{u} \\times \\mathbf{w})=0$ :\n\n$$\n\\begin{gathered}\n\\mathbf{N B}^{\\prime}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}[\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})]-0-0+0}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{4}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2}(-\\operatorname{det}[\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}])}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}} \\\\\n\\tau=\\frac{\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}}\n\\end{gathered}\n$$\n\nor\n\n10.8 Prove $(a) \\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B},(b) \\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}$.\n\n(a) Since $\\mathbf{N N}=1, \\mathbf{N}^{\\prime}$ is orthogonal to $\\mathbf{N}$, which puts it in the plane of $\\mathbf{T}$ and $\\mathbf{B}$. Therefore, for certain real $\\lambda$ and $\\mu$,\n\n\n\\begin{equation*}\n\\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}+\\mu \\mathbf{B} \\tag{1}\n\\end{equation*}\n\n\nDot both sides by $\\mathbf{T}$, then by $\\mathbf{B}$, and use $\\mathbf{T N}=0, \\kappa=\\mathbf{N T}^{\\prime}$, and $\\tau=-\\mathbf{N B}^{\\prime}$ :\n\n$$\n\\begin{aligned}\n& \\mathbf{T} \\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}^{2}+\\mu \\mathbf{T B}=\\lambda \\quad \\text { or } \\quad \\lambda=-\\mathbf{T}^{\\prime} \\mathbf{N}=-\\kappa \\\\\n& \\mathbf{B N}^{\\prime}=\\tau=\\lambda \\mathbf{B T}+\\mu \\mathbf{B}^{2}=\\mu\n\\end{aligned}\n$$\n\nSubstitution for $\\lambda$ and $\\mu$ in (1) then yields the desired results.\n\n(b) From $\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}$ and part (a),\n\n$$\n\\begin{aligned}\n\\mathbf{B}^{\\prime} & =\\mathbf{T}^{\\prime} \\times \\mathbf{N}+\\mathbf{T} \\times \\mathbf{N}^{\\prime}=(\\kappa \\mathbf{N}) \\times \\mathbf{N}+\\mathbf{T} \\times(-\\boldsymbol{\\kappa} \\mathbf{T}+\\tau \\mathbf{B}) \\\\\n& =0+0+\\tau(\\mathbf{T} \\times \\mathbf{B})=\\tau(-\\mathbf{N})=-\\tau \\mathbf{N}\n\\end{aligned}\n$$\n\n10.9 Prove that if a curve has $\\kappa^{\\prime}=0$ at some point, then $\\mathbf{N}^{\\prime \\prime}$ is orthogonal to $\\mathbf{T}$ at that point.\n\nFrom $\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B}$, it follows that $\\mathbf{N}^{\\prime \\prime}=-\\kappa^{\\prime} \\mathbf{T}-\\kappa \\mathbf{T}^{\\prime}+\\tau^{\\prime} \\mathbf{B}+\\tau \\mathbf{B}^{\\prime}$. But $\\kappa^{\\prime}=0$; and from the Serret-Frenet formulas for $\\mathbf{T}^{\\prime}$ and $\\mathbf{B}^{\\prime}$ we obtain\n\n$$\n\\mathbf{N}^{\\prime \\prime}=-\\kappa(\\kappa \\mathbf{N})+\\tau^{\\prime} \\mathbf{B}+\\tau(-\\tau \\mathbf{N})=\\left(-\\kappa^{2}-\\tau^{2}\\right) \\mathbf{N}+\\tau^{\\prime} \\mathbf{B}\n$$\n\nAs $\\mathbf{N}^{\\prime \\prime}$ is in the plane of $\\mathbf{N}$ and $\\mathbf{B}$, it is orthogonal to $\\mathbf{T}$.\n\n\\section*{SURFACES IN EUCLIDEAN SPACE}\n10.10 Show that a surface of revolution is regular and exhibit the unit surface normal.\n\nThe Gaussian form of a surface of revolution about the $z$-axis (Fig. 10-11) is\n\n$$\n\\mathbf{r}=\\left(f\\left(x^{1}\\right) \\cos x^{2}, f\\left(x^{1}\\right) \\sin x^{2}, g\\left(x^{1}\\right)\\right) \\quad\\left(f\\left(x^{1}\\right)>0\\right)\n$$\n\nso\n\n$$\n\\mathbf{r}_{1}=\\left(f^{\\prime}\\left(x^{1}\\right) \\cos x^{2}, f^{\\prime}\\left(x^{1}\\right) \\sin x^{2}, g^{\\prime}\\left(x^{1}\\right)\\right) \\quad \\mathbf{r}_{2}=\\left(-f\\left(x^{1}\\right) \\sin x^{2}, f\\left(x^{1}\\right) \\cos x^{2}, 0\\right)\n$$\n\nand $\\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left(-f g^{\\prime} \\cos x^{2},-f g^{\\prime} \\sin x^{2}, f f^{\\prime}\\left(\\cos ^{2} x^{2}+\\sin ^{2} x^{2}\\right)\\right)$, with norm\n\n$$\nE=\\sqrt{f^{2} g^{\\prime 2} \\cos ^{2} x^{2}+f^{2} g^{\\prime 2} \\sin ^{2} x^{2}+f^{2} f^{\\prime 2}}=f \\sqrt{f^{\\prime 2}+g^{\\prime 2}}\n$$\n\nNow $f=f\\left(x^{1}\\right) \\neq 0$; further, the generating curve is regular, which means that, with $t=x^{1}$, the tangent vector of that curve,\n\n$$\n\\left(\\frac{d x}{d t}, 0, \\frac{d z}{d t}\\right)=\\left(f^{\\prime}, 0, g^{\\prime}\\right)\n$$\n\nis non-null and $f^{\\prime 2}+g^{\\prime 2} \\neq 0$. Therefore, $E \\neq 0$ and the surface is regular.\n\nThe unit surface normal is\n\n$$\n\\mathbf{n}=\\frac{1}{E}\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right)=\\left(-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\cos x^{2},-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\sin x^{2}, \\frac{f^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153(1)}\n\\end{center}\n\nFig. 10-11\n\n10.11 Identify the $x^{1}$ - and $x^{2}$-curves for the right helicoid (Example 10.4) and describe the behavior of the unit surface normal along an $x^{1}$-curve.\n\nThe $x^{1}$-curves $\\left(x^{2}=\\right.$ const.) are given by\n\n$$\n\\mathbf{r}=\\left(0,0, a x^{2}\\right)+x^{1}\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad\\left(x^{1} \\geqq 0\\right)\n$$\n\nthus, they are rays parallel to the $x y$-plane. The $x^{2}$-curves $\\left(x^{1}=\\right.$ const.) are given by\n\n$$\n\\sqrt{x^{2}+y^{2}}=x^{1} \\quad z=a x^{2}\n$$\n\ni.e., circular helices of radii $x^{1}$.\n\nWe have:\n\n$$\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, a\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153}\n\\end{center}\n\nFig. 10-12\n\n$$\n\\begin{aligned}\n& \\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\cos x^{2} & \\sin x^{2} & 0 \\\\\n-x^{1} \\sin x^{2} & x^{1} \\cos x^{2} & a\n\\end{array}\\right|=\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n& \\mathbf{n}=\\frac{\\mathbf{r}_{1} \\times \\mathbf{r}_{2}}{\\left\\|\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right\\|}=\\left(\\frac{a \\sin x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{-a \\cos x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{x^{1}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}\\right) \\\\\n&=(\\cos \\omega) \\mathbf{u}+(\\sin \\omega) \\mathbf{v}\n\\end{aligned}\n$$\n\nand\n\nwhere $\\omega \\equiv \\tan ^{-1}\\left(x^{1} / a\\right), \\mathbf{u} \\equiv\\left(\\sin x^{2},-\\cos x^{2}, 0\\right), \\mathbf{v} \\equiv(0,0,1)$. On an $\\mathbf{x}^{1}$-ray, $\\mathbf{u}$ and $\\mathbf{v}$ are fixed unit vectors, while $\\omega$ increases from 0 to a $\\pi / 2$ as $x^{1}$ increases from 0 to $\\infty$. Thus, $\\mathbf{n}$ traces out a quarter-circle as the ray is described (see Fig. 10-12).\n\n10.12 Find the FFF for any surface of revolution, and specialize to a right circular cone.\n\nWith $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ as obtained in Problem 10.10,\n\n$$\n\\begin{aligned}\n& g_{11}=\\mathbf{r}_{1} \\mathbf{r}_{1}=\\left(f^{\\prime} \\cos x^{2}\\right)^{2}+\\left(f^{\\prime} \\sin x^{2}\\right)^{2}+\\left(g^{\\prime}\\right)^{2}=f^{\\prime 2}+g^{\\prime 2} \\\\\n& g_{12}=g_{21}=\\mathbf{r}_{1} \\mathbf{r}_{2}=-f^{\\prime} f \\cos x^{2} \\sin x^{2}+f^{\\prime} f \\sin x^{2} \\cos x^{2}+\\left(g^{\\prime}\\right)(0)=0 \\\\\n& g_{22}=\\mathbf{r}_{2} \\mathbf{r}_{2}=\\left(-f \\sin x^{2}\\right)^{2}+\\left(f \\cos x^{2}\\right)^{2}+0^{2}=f^{2}\n\\end{aligned}\n$$\n\nand\n\n\n\\begin{equation*}\n\\mathrm{I}=\\left(f^{\\prime 2}+g^{\\prime 2}\\right)\\left(d x^{1}\\right)^{2}+f^{2}\\left(d x^{2}\\right)^{2} \\tag{1}\n\\end{equation*}\n\n\nFor a right circular cone (Fig. 10-13), $f=x^{1}$ and $g=a x^{1}$; hence,\n\n\n\\begin{equation*}\n\\mathrm{I}=\\left(1+a^{2}\\right)\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\tag{2}\n\\end{equation*}\n\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154}\n\\end{center}\n\nFig. 10-13\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154(1)}\n\\end{center}\n\nFig. 10-14\n\n10.13 Find the FFF for the catenoid (Fig. 10-14) and compute the length of the curve given by $x^{1}=t, x^{2}=t \\quad(0 \\leqq t \\leqq \\ln (1+\\sqrt{2}))$.\n\nHere $f\\left(x^{1}\\right)=a \\cosh x^{1}, g\\left(x^{1}\\right)=a x^{1}$, and (1) of Problem 10.12 gives, along the curve,\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}=2 a^{2} \\cosh ^{2} t\n$$\n\nand\n\n$$\nL=a \\sqrt{2} \\int_{0}^{\\ln (1+\\sqrt{2})} \\cosh t d t=a \\sqrt{2} \\sinh [\\ln (1+\\sqrt{2})]=a \\sqrt{2}\n$$\n\n10.14 Let $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ be two curves on the right circular cone $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, 2 x^{1}\\right)$ whose pre-images in the parameter plane are\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=3-t \\\\\nx^{2}=t / 2\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=\\sigma>0 \\\\\nx^{2}=\\sigma^{2}\n\\end{array}\\right.\\right.\n$$\n\nAt the point of intersection, find the angle between $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$, and show that orthogonality in the $x^{1} x^{2}$-plane does not carry over to the cone.\n\nThe intersection point $P^{\\prime}$ of the two pre-image curves is determined by the simultaneous equations\n\n$$\n3-t=\\sigma \\quad \\text { and } \\quad \\frac{t}{2}=\\sigma^{2}\n$$\n\nwhich give $t=2, \\tau=1$, and $P^{\\prime}=(1,1)$. Thus, the two tangent vectors at $P^{\\prime}$ are:\n\n$$\n\\left(u^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d t}\\right)\\right|_{t=2}=\\left(-1, \\frac{1}{2}\\right) \\quad\\left(v^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d \\sigma}\\right)\\right|_{\\sigma=1}=(1,2)\n$$\n\nConsidered in the Euclidean sense, $\\left(u^{i}\\right)$ and $\\left(v^{i}\\right)$ are orthogonal.\n\nTo express the angle between tangents at the image of $P^{\\prime}$, we adopt the metric (2) of Problem 10.12 (with $a=2$ ) and apply (10.18) for $x^{1}=1, x^{2}=1$ :\n\n$$\n\\cos \\theta=\\frac{\\left(1+2^{2}\\right)(-1)(1)+(1)^{2}\\left(\\frac{1}{2}\\right)(2)}{D}=\\frac{-4}{D} \\neq 0\n$$\n\nTherefore, the curves are not orthogonal at the image of $P^{\\prime}$.\n\n10.15 Prove Theorem 10.3 and verify Corollary 10.4 geometrically for the right helicoid (Example 10.4) and for any surface of revolution (Problem 10.12).\n\nThe proof consists merely in taking $\\left(u^{i}\\right)=(1,0)$ and $\\left(v^{i}\\right)=(0,1)$ in (10.18). (Compare Problem 5.31.)\n\nAs is clear from Problem 10.11, the right helicoid is a ruled surface, generated by a half-line (an $x^{1}$-curve), pivoted on the $z$-axis, that rotates parallel to the $x y$-plane while the pivot point travels up the $z$-axis. A given point $P$ of the generator thus describes a helical $x^{2}$-curve (Fig. 10-8), which is necessarily everywhere orthogonal to the generator (i.e., to the $x^{1}$-curves). As for surfaces of revolution, it is clear that the parameter lines that match the revolved planar curve ( $x^{1}$-curves, or meridians) and the circles traced by individual points of the planar curve ( $x^{2}$-curves, or parallels of latitude) are mutually orthogonal. By previous computations, $g_{12}=0$ for both the helicoid and for the general surface of revolution.\n\n10.16 Show that under a change of coordinates $x^{1}=x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}=x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$ in the plane, the surface metric $\\left(g_{i j}\\right)$ transforms as a second-order covariant tensor.\n\nWe have by substitution $\\mathbf{r}\\left(x^{1}, x^{2}\\right)=\\mathbf{r}\\left(x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)\\right) \\equiv \\overline{\\mathbf{r}}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$, the latter being the \"new\" parameterization for $\\mathscr{S}$. To compute the metric under this parameterization, write (by the chain rule for partial derivatives and the bilinearity of the inner product)\n\n$$\n\\begin{aligned}\n\\bar{g}_{i j} & =\\overline{\\mathbf{r}}_{i} \\overline{\\mathbf{r}}_{j} \\equiv \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{j}}=\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{p}} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{q}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right)=\\left(\\mathbf{r}_{p} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\mathbf{r}_{q} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right) \\\\\n& =\\mathbf{r}_{p} \\mathbf{r}_{q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}} \\equiv g_{p q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\n\\end{aligned}\n$$\n\nwhich is the correct formula for tensor character.\n\n\\section*{GEODESICS}\n10.17 (a) Find the Christoffel symbols of the second kind for the sphere of radius $a$. (b) Verify that the great circles passing through the north and south poles (i.e., the $x^{1}$-curves) are geodesics.\\\\\n(a) The FFF for the sphere of radius $a$ may be calculated from Problem 10.12:\n\n$$\ng_{11}=a^{2} \\quad g_{12}=0=g_{21} \\quad g_{22}=a^{2} \\sin ^{2} x^{1}\n$$\n\nThe formulas from Problem 6.4 can be used, since $\\left(g_{i j}\\right)$ is diagonal; the nonzero Christoffel symbols are found to be:\n\n$$\n\\Gamma_{22}^{1}=-\\sin x^{1} \\cos x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\cot x^{1}\n$$\n\n(b) We want to show that the family of curves $x^{1}=t, x^{2}=d=$ const. are integral curves of the differential system (7.11)-(7.12), which may be conveniently written as\n\n$$\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2} \\frac{d x^{i}}{d t}\\left[\\frac{d}{d t} \\ln \\left(g_{j k} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}\\right)\\right]\n$$\n\nor, for the given metric,\n\n$$\n\\begin{array}{ll}\ni=1 & \\frac{d^{2} x^{1}}{d t^{2}}-\\left(\\sin x^{1} \\cos x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\frac{1}{2} \\frac{d x^{1}}{d t}\\left[\\frac{d}{d t} \\ln \\left(a^{2}\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(a^{2} \\sin ^{2} x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right)\\right] \\\\\ni=2 & \\frac{d^{2} x^{2}}{d t^{2}}+\\left(2 \\cot x^{1}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=\\frac{1}{2} \\frac{d x^{2}}{d t}\\left[\\frac{d}{d t} \\ln \\mathrm{I}\\right.\n\\end{array}\n$$\n\nSince $d x^{1} / d t=1$ and $d x^{2} / d t=0$, both equations reduce to $0=0$, and the verification is complete.\n\n10.18 Prove Theorem 10.6: A curve on a regular surface is a geodesic if and only if, by proper choice of the principal normal, $\\mathbf{N}=\\mathbf{n}$.\n\nLet any curve on the surface be given by $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(s), x^{2}(s)\\right)$, where $s=\\operatorname{arc}$ length, Then,\n\n$$\n\\mathbf{T}=\\mathbf{r}_{i} \\frac{d x^{i}}{d s}\n$$\n\nand the first formula (10.9) gives\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s}\\left(\\frac{\\partial \\mathbf{r}_{i}}{\\partial x^{j}} \\frac{d x^{j}}{d s}\\right) \\equiv \\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\tag{1}\n\\end{equation*}\n\n\nDot both sides of (1) by the vector $\\mathbf{r}_{k}$ and use the resuit of Problem 10.48:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i} \\mathbf{r}_{k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\mathbf{r}_{k} \\equiv \\frac{d^{2} x^{i}}{d s^{2}} g_{i k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\Gamma_{i j k} \\tag{2}\n\\end{equation*}\n\n\nMultiply both sides of (2) by $g^{k l}$ and sum on $k$ :\n\n\n\\begin{equation*}\ng^{k l} \\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\delta_{i}^{l}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} g^{k l} \\Gamma_{i j k}=\\frac{d^{2} x^{l}}{d s^{2}}+\\Gamma_{i j}^{l} \\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\tag{3}\n\\end{equation*}\n\n\nNow if $\\mathscr{C}$ is a geodesic, the right side of (3) vanishes, and this implies that $\\kappa \\mathbf{r}_{k} \\mathbf{N}=0$ for $k=1,2$. If $\\kappa \\neq 0$, then $\\mathbf{r}_{1} \\mathbf{N}=0=\\mathbf{r}_{2} \\mathbf{N}$; so that $\\mathbf{N}$ is orthogonal to both $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ (thus to the tangent plane). Therefore, but for orientation, $\\mathbf{N}=\\mathbf{n}$. If $\\kappa=0$ at some point $P$ and there is a sequence of points along the curve approaching $P$ for which $\\kappa \\neq 0$, then, by continuity, $\\mathbf{N}=\\mathbf{n}$. Otherwise, $\\kappa=0$ on an interval and the curve is a straight line on that interval, in which case its principal normal $\\mathbf{N}$ can be chosen to agree with $\\mathbf{n}$. Conversely, if the curve has the property that $\\mathbf{N}=\\mathbf{n}$ at all points, then $\\mathbf{r}_{k} \\mathbf{N}=\\mathbf{r}_{k} \\mathbf{n}=0$ and the left side of (3) vanishes, showing that the pre-image of $\\mathscr{C}$ satisfies the differential equations for a geodesic.\n\n10.19 Apply Theorem 10.6 to the plane sections of a torus.\n\nIn Fig. 10-15 is shown a torus and various examples of plane sections. In (a), an elliptical-shaped vertical section, the section cannot be a geodesic because the surface normal at $P$ does not lie in the plane of the curve (which contains the curve normal). In (b), a horizontal section that is a circle, again the surface normals do not lie in the plane, and this section is not a geodesic. The circles shown in $(c)$ and $(d)$ are geodesics, since the surface normal will coincide with a correctly chosen principal normal of the curve.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157}\n\\end{center}\n\nFig. 10-15\n\n10.20 Demonstrate that the intrinsic curvature $\\tilde{\\kappa}$ of a curve on a surface can be different from its curvature $\\kappa$ as a curve in $\\mathbf{E}^{3}$.\n\nOne example is a circle on a sphere of radius $a$. If the circle also has radius $a$, it is a great circle and, hence, a geodesic with zero intrinsic curvature. But its curvature as a (planar) curve in $\\mathbf{E}^{3}$ is $1 / a$. Another example is the circular helix: its curvature is nonzero as a curve in $\\mathbf{E}^{3}$, but as a geodesic on a circular cylinder its intrinsic curvature is zero.\n\n\\section*{SECOND FUNDAMENTAL FORM}\n10.21 (a) Find the SFF for the right circular cone of Problem 10.12. (b) At the point $P(1,0, a)$, calculate the curvature of the normal section having the direction $\\mathbf{j}$ at $P$ (see Fig. 10-16).\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157(1)}\n\\end{center}\n\nFig. 10-16\\\\\n(a) From $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, a x^{1}\\right) \\quad\\left(x^{1}>0\\right)$, we obtain:\n\n$$\n\\begin{array}{ccc}\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, a\\right) & \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, 0\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) & \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) & \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{array}\n$$\n\nand by Problem 10.10, $\\mathbf{n}=\\left(a^{2}+1\\right)^{-1 / 2}\\left(-a \\cos x^{2},-a \\sin x^{2}, 1\\right)$. The coefficients in II are thus $f_{12} \\equiv \\mathbf{n r}_{11}=0, f_{12}=f_{21} \\equiv \\mathbf{n} \\mathbf{r}_{12}=0, f_{22} \\equiv \\mathbf{n r}_{22}=\\left(a^{2}+1\\right)^{-1 / 2} a x^{1}$.\n\n(b) The direction $\\mathbf{j}$ at $P$ corresponds to the direction $\\left(u^{1}, u^{2}\\right)$ at $P^{\\prime}=(1,0)$ in the parameter plane, where\n\n$$\n\\begin{aligned}\n\\mathbf{j} & =u^{1} \\mathbf{r}_{1}(P)+u^{2} \\mathbf{r}_{2}(P) \\\\\n(0,1,0) & =u^{1}(1,0, a)+u^{2}(0,1,0) \\\\\n(0,1,0) & =\\left(u^{1}, u^{2}, a u^{1}\\right)\n\\end{aligned}\n$$\n\nThus, $u^{1}=0$ and $u^{2}=1$. Appropriating I from Problem 10.12, we have\n\n$$\n\\kappa_{g f}=\\frac{\\mathrm{II}\\left(u^{1}, u^{2}\\right)}{\\mathrm{I}\\left(u^{1}, u^{2}\\right)}=\\frac{f_{22}(P)\\left(u^{2}\\right)^{2}}{\\left(a^{2}+1\\right)\\left(u^{1}\\right)^{2}+(1)^{2}\\left(u^{2}\\right)^{2}}=f_{22}(P)=\\frac{a}{\\sqrt{a^{2}+1}}\n$$\n\n10.22 Develop geometrically a notion of \"parallel transport\" of a vector along a curve $\\mathscr{C}$ on a regular surface $\\mathscr{S}$.\n\nImagine $\\mathscr{C}$, and with it $\\mathscr{S}$, as being rolled without slipping onto a fixed plane $\\mathscr{F}$, in such fashion that the point of contact is aways on $\\mathscr{C}$ and the tangent plane to $\\mathscr{S}$ at the point of contacts always coincides with $\\mathscr{F}$. This maps $\\mathscr{C}$ to a (planar) curve $\\mathscr{C}^{*}$ in $\\mathscr{F}$ that has the same arc-length parameter and the same tangent vector. Then, any vector in $\\mathscr{F}$ that is attached to the point of contact and that remains parallel to itself (in the ordinary Euclidean sense) as the contact point describes $\\mathscr{C}^{*}$ may-under the inverse mapping $\\mathscr{C}^{*} \\rightarrow \\mathscr{C}$-be considered as undergoing parallel transport along $\\mathscr{C}$. In general, parallel transport of a given vector around a closed curve on a surface does not reproduce the initial vector.\n\n10.23 Prove (10.23).\n\nStart with the formula for the unit tangent vector of any curve $\\mathscr{C}$ on $\\mathscr{S}$ :\n\n$$\n\\mathbf{T}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{k l} u^{k} u^{i}}} \\quad\\left(u^{i} \\equiv \\frac{d x^{i}}{d t}\\right)\n$$\n\nThen\n\n\n\\begin{equation*}\n\\dot{\\mathbf{T}}=\\frac{d}{d t}\\left(\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}}\\right) \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\dot{\\mathbf{r}}_{i}=Q^{i} \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i j} u^{j} \\tag{1}\n\\end{equation*}\n\n\nwhere $Q^{i}$ is an abbreviation for the scalar coefficient of $\\mathbf{r}_{i}$. Now the Frenet formula gives $\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=$ $\\dot{\\mathbf{T}} / \\sqrt{g_{k l} u^{k} u^{\\prime}}$; together with (1), this yields:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\frac{Q_{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i}+\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\tag{2}\n\\end{equation*}\n\n\nDot both sides of (2) with $\\mathbf{n}$ (the surface normal) and use the fact that $\\mathbf{r}_{i} \\mathbf{n}=0$ for each $i$ :\n\n\n\\begin{equation*}\n\\kappa \\mathbf{n} \\mathbf{N}=\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\mathbf{n} \\equiv \\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} f_{i j} \\tag{3}\n\\end{equation*}\n\n\nIf $\\mathscr{C}$ is a normal section $\\mathscr{C}_{\\mathscr{F}}$ at $P$, and $\\kappa, \\mathbf{N}$, and the right side of (3) are all evaluated at $P$, then $\\kappa=\\kappa_{\\mathscr{F}}$, $\\mathbf{n N}=\\mathbf{n}^{2}=1$, and (3) becomes the desired expression\n\n$$\n\\kappa_{\\mathscr{F}}=\\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}\n$$\n\n\\section*{CURVATURE OF SURFACES}\n10.24 Show that the maximum and minimum values of the function\n\n$$\nF(\\mathbf{u})=\\frac{a_{i j} u^{i} u^{j}}{b_{k l} u^{k} u^{l}}=\\frac{\\mathbf{u}^{T} A \\mathbf{u}}{\\mathbf{u}^{T} B \\mathbf{u}}\n$$\n\nwhere $A=\\left[a_{i j}\\right]_{22}, B=\\left[b_{i j}\\right]_{22}, \\mathbf{u}=\\left(u^{1}, u^{2}\\right) \\neq(0,0)$, with $B$ positive definite, are the two roots of the quadratic equation in $\\lambda$\n\n\\[\n\\operatorname{det}(A-\\lambda B) \\equiv\\left|\\begin{array}{ll}\na_{11}-\\lambda b_{11} & a_{12}-\\lambda b_{12}  \\tag{1}\\\\\na_{21}-\\lambda b_{21} & a_{22}-\\lambda b_{22}\n\\end{array}\\right|=0\n\\]\n\n(hence, eigenvalues of $B^{-1} A$ ), and that the extreme values of $F$ occur for vectors $\\mathbf{u}$ satisfying $(A-x B) \\mathbf{u}=\\mathbf{0}$, where $x$ takes on the two eigenvalues of $B^{-1} A$ (hence, eigenvectors of $B^{-1} A$ ).\n\nWe may assume without loss of generality that $A$ and $B$ are symmetric. Let $\\mathscr{G}$ be any simple closed curve in the $u^{1} u^{2}$-plane having the origin in its interior. The Weierstrass theorem guarantees that $F(\\mathbf{u})$ asumes a largest value on $\\mathscr{G}$; say, $F(\\mathbf{w})=M$. Because $F$ is constant on rays emanating from the origin $(F(\\lambda \\mathbf{u})=F(\\mathbf{u})$ for any $\\lambda \\neq 0)$, the absolute maximum on $\\mathscr{G}$ is both an absolute and a relative maximum in the $u^{1} u^{2}$-plane; hence, the gradient of $F$ must vanish at $\\mathbf{w}$. We have:\n\n$$\n\\begin{gathered}\n\\frac{\\partial F(\\mathbf{u})}{\\partial u^{P}}=\\frac{\\left(b_{k l} u^{k} u^{l}\\right)\\left(2 a_{p j} u^{j}\\right)-\\left(a_{i j} u^{i} u^{j}\\right)\\left(2 b_{p l} u^{l}\\right)}{\\left(b_{k l} u^{k} u^{l}\\right)^{2}}=\\frac{2}{b_{k l} u^{k} u^{l}}\\left[a_{p j} u^{j}-F(\\mathbf{u})\\left(b_{p l} u^{l}\\right)\\right] \\\\\n\\nabla F(\\mathbf{u})=\\frac{2}{\\mathbf{u}^{T} B \\mathbf{u}}[A \\mathbf{u}-F(\\mathbf{u}) B \\mathbf{u}]\n\\end{gathered}\n$$\n\nor\n\nTherefore, $A \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0}$, which shows (i) that $M$ is an eigenvalue of $B^{-1} A$ and thus is a root of the characteristic equation (1); (ii) that $\\mathbf{w}$ is an eigenvector belonging to $M$.\n\nA like consideration of the minimum value, $m$, of $F$ on $\\mathscr{G}$ leads to the other eigenvalue and associated eigenvector.\n\n10.25 Prove that the extreme normal curvatures $\\kappa_{1}, \\kappa_{2}$ are the two roots of the quadratic equation $(10.25)$.\n\nIn Problem 10.24, take $a_{i j}=f_{i j}$ and $b_{k l}=g_{k l}$; expand (1) to obtain (10.25).\n\n10.26 Prove that the two normal section curves through $P$ on $\\mathscr{S}$ giving rise to $\\max \\kappa_{\\mathscr{F}}=\\kappa_{1}$ and $\\min \\kappa_{\\mathscr{F}}=\\kappa_{2}$ are orthogonal when $\\kappa_{1} \\neq \\kappa_{2}$ (that is, when $P$ is not an umbilical point on $\\mathscr{S}$ ).\n\nLet us prove the general result, in the notation of Problem 10.24. We have:\n\n$$\nA \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0} \\quad A \\mathbf{v}-m B \\mathbf{v}=\\mathbf{0}\n$$\n\nWith the inner product of column vectors defined as $\\mathbf{p} \\cdot \\mathbf{q} \\equiv \\mathbf{p}^{T} B \\mathbf{q}$, multiply the first equation by $\\mathbf{v}^{T}$ and the second by $\\mathbf{w}^{T}$, and subtract:\n\n$$\n(m-M) \\mathbf{v} \\cdot \\mathbf{w}=0\n$$\n\nHence, if $m \\neq M, \\mathbf{v}$ and $\\mathbf{w}$ are orthogonal.\n\n10.27 Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for the right helicoid. Show that as $x^{1} \\rightarrow \\infty, \\mathrm{K}$ tends to zero (the surface becomes \"flatter\" as the distance from its axis increases without bound).\n\nFrom Problem 10.11 and Example 10.4,\n\n$$\n\\begin{gathered}\n\\mathbf{n}=\\frac{1}{\\sqrt{\\left(x^{1}\\right)^{2}+a^{2}}}\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) \\quad \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) \\quad \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{gathered}\n$$\n\nso that\n\nand\n\n$$\n\\begin{gathered}\nf_{11}=\\mathbf{n r}_{11}=0 \\quad f_{12}=f_{21}=-a / \\sqrt{\\left(x^{1}\\right)^{2}+a^{2}} \\quad f_{22}=0 \\\\\n\\mathrm{~K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0-a^{2} /\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}=-\\frac{a^{2}}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]^{2}} \\rightarrow 0 \\quad \\text { as } x^{1} \\rightarrow \\infty \\\\\n\\mathrm{H}=\\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0+0-2(0)}{g}=0\n\\end{gathered}\n$$\n\n\\section*{STRUCTURE FORMULAS; ISOMETRIES}\n10.28 Complete the proof of $(10.27 a)$.\n\nThe relations $\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}$ and $\\mathbf{n}_{j} \\mathbf{r}_{i}=-f_{i j}$ imply\n\n$$\n-f_{i j}=u_{j}^{k} g_{k i}\n$$\n\nMultiply both sides by $g^{i s}$ and sum over $i$, obtaining $u_{j}^{s}=-g^{l s} f_{l j}$. Hence,\n\n$$\n\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}=-g^{l k} f_{l i} \\mathbf{r}_{k}=-g^{l k} f_{i l} \\mathbf{r}_{k}\n$$\n\n10.29 Prove (10.28).\n\nLet the equation $\\mathbf{r}_{i j}=u_{i j}^{1} \\mathbf{r}_{1}+u_{i j}^{2} \\mathbf{r}_{2}+u_{i j}^{3} \\mathbf{n}$ be rewritten as $\\mathbf{r}_{i j}=u_{i j}^{s} \\mathbf{r}_{s}+u_{i j}^{3} \\mathbf{n}$. Dot with $\\mathbf{n}$, obtaining $u_{i j}^{3}=f_{i j}$; therefore,\n\n\n\\begin{equation*}\n\\mathbf{r}_{i j}=u_{i j}^{s} \\mathbf{r}_{s}+f_{i j} \\mathbf{n} \\tag{1}\n\\end{equation*}\n\n\nDot (1) with $\\mathbf{r}_{k}$ and use Problem 10.48:\n\n$$\n\\mathbf{r}_{i j} \\mathbf{r}_{k}=u_{i j}^{s} \\mathbf{r}_{s} \\mathbf{r}_{k}+0 \\quad \\text { or } \\quad \\Gamma_{i j k}=u_{i j}^{s} g_{s k}\n$$\n\nSolve for $u_{i j}^{s}:$\n\n$$\ng^{k t} \\Gamma_{i j k}=u_{i j}^{s} g^{k t} g_{s k} \\quad \\text { or } \\quad \\Gamma_{i j}^{t}=u_{i j}^{s} \\delta_{s}^{t}=u_{i j}^{t}\n$$\n\nSubstitute back into (1):\n\n$$\n\\mathbf{r}_{i j}=\\Gamma_{i j}^{t} \\mathbf{r}_{t}+f_{i j} \\mathbf{n}\n$$\n\n\\section*{Supplementary Problems}\n10.30 (a) Describe geometrically the curve whose parametric vector equation is\n\n$$\n\\mathbf{r}=\\left(\\cos t, \\sin t,(1-t)^{-1}\\right) \\quad(0 \\leqq t<1)\n$$\n\nWhat happens as $t \\rightarrow 1$ ? (b) Use a programmable calculator and Simpson's rule to find the arc length for $0 \\leqq t \\leqq 1 / 2$ accurate to 6 places.\n\n10.31 Find the exact length of the space curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(-1 \\leqq t \\leqq 1)$.\n\n10.32 (a) Using the arc-length parameterization of the right circular helix,\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c \\equiv \\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nfind the coordinate equations of the tangent line to the helix at any point $P \\equiv \\mathbf{r}(s)$. (b) Show that the tangent line intersects the $x y$-plane at a point $Q \\equiv \\mathbf{r}^{*}(s)$ such that $P Q=s$. (c) By thinking of a string wound along the helix, interpret the result of $(b)$.\n\n10.33 Show that for the curve $y=x^{5}$ in the $x y$-plane, parameterized as $\\mathbf{r}=\\left(t, t^{5}, 0\\right)$, the vector $\\mathbf{T}^{\\prime} /\\left\\|\\mathbf{T}^{\\prime}\\right\\|$ has an essential point of discontinuity at $t=0$.\n\n10.34 Find the curvature and the torsion of the curve $\\mathbf{r}=\\left(t, t^{5}+a, t^{5}-a\\right)$.\n\n10.35 Prove that a curve is planar if and only if its torsion vanishes.\n\n10.36 Prove that a planar curve with constant nonzero curvature $\\kappa$ is a circle. [Hint: $\\mathbf{T}=(\\cos \\theta, \\sin \\theta, 0)$ and $\\mathbf{N}=(-\\sin \\theta, \\cos \\theta, 0)$ imply $\\kappa=\\theta^{\\prime}$ or $\\theta=\\kappa s+a$; show that the radius is $\\left.1 / \\kappa.\\right]$\n\n10.37 Verify the Serret-Frenet formulas for the circular helix.\n\n10.38 Show that if included in the range of the map $\\mathbf{r}\\left(x^{1}, x^{2}\\right)$, the vertex of the right circular cone is a singular point.\n\n10.39 Calculate the unit normal for the catenoid as parameterized in Fig. 10-14 and show that the surface is regular.\n\n10.40 Find the length of the curve on the right helicoid (Example 10.4) given by $x^{1}=t^{2}, x^{2}=\\ln t$, with $1 \\leqq t \\leqq 2$, in the special case when the parameter $a=1$.\n\n10.41 Find the two possible directions for a curve $\\mathscr{C}$ ' in the parameter plane whose image on the paraboloid of Fig. 10-17 meets the circle $x^{2}+y^{2}=4, z=4$ at $P(0,2,4)$ at an angle of $\\pi / 3$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-161}\n\\end{center}\n\nFig. 10-17\n\n10.42 Use Theorem 10.6 to show that an elliptical helix is not in general a geodesic on an elliptical cylinder.\n\n10.43 Calculate the Christoffel symbols of the second kind for the right helicoid (Example 10.4). Show that circular helices on the surface are geodesics.\n\n10.44 Exhibit the SFF for the general surface of revolution (Problem 10.10).\n\n10.45 Establish the formulas below for any surface of revolution, with $G \\equiv g^{\\prime} / f^{\\prime}$ (see Problems 10.12 and $10.44)$ :\n\n$$\n\\mathrm{K}=\\frac{G G^{\\prime}}{f f^{\\prime}\\left(1+G^{2}\\right)^{2}} \\quad \\text { and } \\quad \\mathrm{H}=\\frac{f G^{\\prime}+g^{\\prime}\\left(1+G^{2}\\right)}{f\\left|f^{\\prime}\\right|\\left(1+G^{2}\\right)^{3 / 2}}\n$$\n\nUse these formulas to verify that a sphere of radius $a$ has Gaussian curvature $1 / a^{2}$ and mean curvature $-2 / a$.\n\n10.46 (a) Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for two different parameterizations of the paraboloid $z=a\\left(x^{2}+y^{2}\\right)$ : (i) as the surface of revolution for which $f=x^{1}, g=a\\left(x^{1}\\right)^{2}$; (ii) as the surface $\\mathbf{r}=\\left(\\bar{x}^{1}, \\bar{x}^{2}, a\\left(\\bar{x}^{1}\\right)^{2}+a\\left(\\bar{x}^{2}\\right)^{2}\\right) .(b)$ Interpret the results of $(a)$.\n\n10.47 Infer from Problem 10.45 that $\\mathrm{H} \\equiv 0$ for any catenoid. [A surface with $\\mathrm{H}=0$ at all points is called a minimal surface. Among minimal surfaces are those that solve \"soap-bubble\" problems, which require a minimum in surface area.]\n\n10.48 Prove that $\\Gamma_{i j k}=\\mathbf{r}_{i j} \\mathbf{r}_{k}$. Hint: $\\left.\\quad\\left(\\mathbf{r}_{i} \\mathbf{r}_{j}\\right)_{k}=\\mathbf{r}_{i k} \\mathbf{r}_{j}+\\mathbf{r}_{i} \\mathbf{r}_{j k}.\\right]$\n\n10.49 Surfaces for which the Gaussian curvature is a negative constant are very rare. One such surface can be constructed as follows. (a) A tractrix is the involute of a catenary (see Problem 10.32(c)). Write the vector equation for the involute of the catenary $\\mathbf{r}=\\left(a \\cosh x^{1}, 0, a x^{1}\\right)$ (see Fig. 10-14). (b) Using Problem 10.45, show that $\\mathrm{K}=-1 / a^{2}$ for the tractroid generated by revolving the tractrix of $(a)$ about the $z$-axis.\n\n10.50 Prove that the catenoid,\n\n$$\nI=\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2}\n$$\n\nand the right helicoid,\n\n$$\n\\mathrm{I}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left[\\left(\\bar{x}^{1}\\right)^{2}+a^{2}\\right]\\left(d \\bar{x}^{2}\\right)^{2}\n$$\n\nare locally isometric.\n\n"], "lesson": "\\section*{Chapter 10}\n\\section*{Tensors in Euclidean Geometry}\n\\subsection*{10.1 INTRODUCTION}\nThere exists a starting correlation between formulas of differential geometry, developed to answer questions about curves and surfaces in Euclidean 3-space, and tensor identities previously introduced to handle changes of coordinate systems. Differential geometry was used to great advantage by Einstein in his development of relativity.\n\nThe metric will be assumed to be the Euclidean metric, and to emphasize this fact we shall designate the space by $\\mathbf{E}^{3}$, which means $\\mathbf{R}^{3}$ with the metric\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}\n$$\n\nMoreover, we shall use the familiar notation $(x, y, z)$ in place of $\\left(x^{1}, x^{2}, x^{3}\\right)$.\n\n\\subsection*{10.2 CURVE THEORY; THE MOVING FRAME}\nA curve $\\mathscr{C}$ in $\\mathbf{E}^{3}$ is the image of a class $C^{3}$ mapping, $\\mathbf{r}$, from an interval $\\mathscr{I}$ of real numbers into $\\mathbf{E}^{3}$, as indicated in Fig. 10-1. The image of the real number $t$ in $\\mathscr{I}$ will be denoted\n\n\n\\begin{equation*}\n\\mathbf{r}(t) \\equiv(x(t), y(t), z(t)) \\tag{10.1}\n\\end{equation*}\n\n\na vector field of class $C^{3}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-136}\n\\end{center}\n\nFig. 10-1\n\n\\section*{Regular Curves}\nThe tangent vector of $\\mathscr{C}$ is given by\n\n\n\\begin{equation*}\n\\frac{d \\mathbf{r}}{d t} \\equiv \\dot{\\mathbf{r}}=\\left(\\frac{d x}{d t}, \\frac{d y}{d t}, \\frac{d z}{d t}\\right) \\tag{10.2}\n\\end{equation*}\n\n\n$\\mathscr{C}$ is said to be regular if $\\mathbf{r}(t) \\neq \\mathbf{0}$ for each $t$ in $\\mathscr{I}$.\n\nRemark 1: This corresponds to the definition of regularity, given in Section 7.3, in the case of a positive definite metric.\n\nEXAMPLE 10.1 An elliptical helix (Fig. 10-2) is a helix lying on an elliptical cylinder $x^{2} / a^{2}+y^{2} / b^{2}=1$ in $x y z$-space; it is given by $\\mathscr{C}: x=a \\cos t, \\quad y=b \\sin t, \\quad z=c t$, with $\\mathscr{F}$ the entire real line. The pitch is defined as the number $c$. If $a=b$, the helix is called circular, with radius $a$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137}\n\\end{center}\n\nFig. 10-2\n\nEXAMPLE 10.2 The space curve $\\mathscr{C}: x=t, \\quad y=a t^{2}, \\quad z=b t^{3} \\quad(\\mathscr{I}=\\mathbf{R})$ captures the salient local features of all curves; it is known as the twisted cubic. As indicated in Fig. 10-3, the projection of $\\mathscr{C}$ in the $x y$-plane is a parabola, $y=a x^{2}$; its projection in the $x z$-plane is a standard cubic curve, $z=b x^{3}$; in the $y z$-plane, the semicubical parabola $(y / a)^{3}=(z / b)^{2}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137(1)}\n\\end{center}\n\nFig. 10-3\n\n\\section*{Arc Length}\nSince the Euclidean metric is positive definite, every regular curve has an arc-length parameterization $\\mathbf{r}=\\mathbf{r}(s)$, such that\n\n\n\\begin{equation*}\ns=\\int_{a}^{t}\\left\\|\\frac{d \\mathbf{r}}{d u}\\right\\| d u \\quad \\text { or } \\quad \\frac{d s}{d t}=\\|\\dot{\\mathbf{r}}\\| \\tag{10.3}\n\\end{equation*}\n\n\n(The dot, as in $\\dot{\\mathbf{r}}$, is used to denote differentiation with respect to $t$, and a prime, as in $\\mathbf{r}^{\\prime}$, denotes differentiation with respect to $s$.) The mapping $t \\rightarrow s$ defined by (10.3) has the inverse relation $s \\rightarrow t$ given explicitly by $t=\\varphi(s)$, where $\\varphi$ is also differentiable:\n\n\n\\begin{equation*}\n\\frac{d t}{d s}=\\varphi^{\\prime}(s)=\\frac{1}{\\|\\dot{\\mathbf{r}}\\|} \\tag{10.4}\n\\end{equation*}\n\n\n\\section*{The Moving Frame}\nThree vectors of fundamental importance to curve theory will now be discussed. Two of them were introduced in Chapter 7: the unit tangent vector-the (unique) vector\n\n$$\n\\mathbf{T} \\equiv \\mathbf{r}^{\\prime}=\\left(\\frac{d x}{d s}, \\frac{d y}{d s}, \\frac{d z}{d s}\\right)\n$$\n\n\\begin{itemize}\n  \\item and the unit principal normal-any unit, class $C^{1}$ vector $\\mathbf{N}$ that is orthogonal to $\\mathbf{T}$ and is parallel to $\\mathbf{T}^{\\prime}$ wherever $\\mathbf{T}^{\\prime} \\neq \\mathbf{0}$. The binormal vector associated with a curve is the unit vector $\\mathbf{B} \\equiv \\mathbf{T} \\times \\mathbf{N}$ [for the cross product, see (2.10)]; B is uniquely determined once $\\mathbf{N}$ has been chosen.\n\\end{itemize}\n\nNot all regular curves have a principal normal vector (see Problem 10.1). However, it was proved in Problem 7.14 that all planar curves possess a principal normal, of the form\n\n$$\n\\mathbf{N}=(-\\sin \\theta, \\cos \\theta, 0) \\quad(\\text { plane } z=0)\n$$\n\nif $\\mathbf{T}=(\\cos \\theta, \\sin \\theta, 0)$. The following result provides further information.\n\nTheorem 10.1: Every planar curve has a principal normal vector. If a space curve has a principal normal vector, that vector lies in the plane of the curve for any nonstraight planar segment of the curve. Along any straight-line segment, the principal normal can be chosen as any class $C^{1}$ vector orthogonal to the unit tangent vector.\n\nAt each point of $\\mathscr{C}$ where $\\mathbf{N}$ can be defined, the mutually orthogonal triplet of unit vectors $\\mathbf{T}, \\mathbf{N}$, B constitutes a right-handed system of basis elements for $\\mathbf{E}^{3}$. This triad, which changes continuously along $\\mathscr{C}$ (Fig. 10-4), is often called the moving frame or moving triad; the plane of $\\mathbf{T}$ and $\\mathbf{N}$ is known as the osculating plane.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-138}\n\\end{center}\n\nFig. 10-4\n\nThe moving frame has been defined for the arc-length parameterization. When it is necessary to use the original parameter $t$ instead, the following expressions may be established (Problem 10.4) for any point at which $\\dot{\\mathbf{r}} \\neq \\mathbf{0}$ and $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$ :\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|} \\quad \\mathbf{N}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\quad \\mathbf{B}=\\varepsilon \\frac{\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\tag{10.5}\n\\end{equation*}\n\n\nHere, $\\varepsilon= \\pm 1$, the choice of sign depending on the choice of $\\mathbf{N}$ as a class $C^{1}$ vector.\n\n\\subsection*{10.3 CURVATURE AND TORSION}\nTwo important numbers, or more accurately, scalar fields, are associated with space curves.\n\nDefinition 1: The curvature $\\kappa$ and torsion $\\tau$ of a curve $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}(s) \\quad$ in $\\mathbf{E}^{3}$ are, respectively, the real numbers\n\n\n\\begin{equation*}\n\\kappa \\equiv \\mathbf{N T}^{\\prime} \\quad \\text { and } \\quad \\tau \\equiv-\\mathbf{N B}^{\\prime} \\tag{10.6}\n\\end{equation*}\n\n\nThe sign of $\\kappa$ will depend on that chosen for $\\mathbf{N}$; however, since $\\mathbf{B}$ and $\\mathbf{B}^{\\prime}$ change in sign together with $\\mathbf{N}, \\tau$ is uniquely determined.\n\nIt foliows (cf. Problem 7.13) that the absolute values of curvature and torsion are given by\n\n\n\\begin{equation*}\n\\kappa_{0} \\equiv|\\kappa|=\\left\\|\\mathbf{T}^{\\prime}\\right\\| \\quad \\text { and } \\quad \\tau_{0} \\equiv|\\tau|=\\left\\|\\mathbf{B}^{\\prime}\\right\\| \\tag{10.7}\n\\end{equation*}\n\n\nThus, $\\kappa_{0}$ measures the absolute rate of change of the unit tangent vector and the amount of \"bending\" a curve possesses at any given point, while $\\tau_{0}$ measures the absolute rate of change of the binormal and the tendency of the curve to \"twist\" out of its osculating plane at each point. The significance of negative values for $\\kappa$ and $\\tau$ will become apparent later.\n\nRemark 2: It can be shown that the two functions $\\kappa=\\kappa(s)$ and $\\tau=\\tau(s)$ determine the curve $\\mathscr{C}$ up to a rigid motion in $\\mathbf{E}^{3}$.\n\nIn the $t$-parameterization of $\\mathscr{C}$, we have (Problem 10.7):\n\n\n\\begin{equation*}\n\\kappa=\\frac{\\varepsilon\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\quad \\text { and } \\quad \\tau=\\frac{\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}} \\tag{10.8}\n\\end{equation*}\n\n\nwhere $\\varepsilon= \\pm 1$ and $[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]$ represents the $3 \\times 3$ matrix having as row vectors $\\dot{\\mathbf{r}}, \\ddot{\\mathbf{r}}$, and $\\ddot{\\mathbf{r}}$. [Recall the identity\n\n$$\n\\mathbf{a} \\cdot(\\mathbf{b} \\times \\mathbf{c})=\\operatorname{det}[\\mathbf{a} \\mathbf{b} \\mathbf{c}]\n$$\n\nfor the triple scalar product of three vectors.]\n\n\\section*{Serret-Frenet Formulas}\nThe derivatives of the vectors composing the moving triad are given by\n\n\\[\n\\left.\\begin{array}{l}\n\\mathbf{T}^{\\prime}=\\kappa \\mathbf{N}  \\tag{10.9}\\\\\n\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B} \\quad \\text { or } \\quad\\left[\\begin{array}{l}\n\\mathbf{T} \\\\\n\\mathbf{N} \\\\\n\\mathbf{B}\n\\end{array} \\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}\\right.\n\\end{array}\\right]^{\\prime}=\\left[\\begin{array}{rrr}\n0 & \\kappa & 0 \\\\\n-\\kappa & 0 & \\tau \\\\\n0 & -\\tau & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathbf{T} \\\\\n\\mathbf{N} \\\\\n\\mathbf{B}\n\\end{array}\\right]\n\\]\n\nNote the skew-symmetry of the coefficient matrix. The first of these formulas was established in Problem 7.13; the other two are derived in Problem 10.8.\n\n\\subsection*{10.4 REGULAR SURFACES}\nSurfaces are generally encountered in the calculus in the form $z=F(x, y)$; that is, as graphs of two-variable functions in three-dimensional space. Here, however, it is more convenient to adopt the\n\nDefinition 2: A surface $\\mathscr{S}$ in $\\mathbf{E}^{2}$ is the image of a $C^{3}$ vector function,\n\n$$\n\\mathbf{r}\\left(x^{1}, x^{2}\\right)=\\left(f\\left(x^{1}, x^{2}\\right), g\\left(x^{1}, x^{2}\\right), h\\left(x^{1}, x^{2}\\right)\\right)\n$$\n\nwhich maps some region $\\mathscr{V}$ of $\\mathbf{E}^{2}$ into $\\mathbf{E}^{3}$.\n\n(See Fig. 10-5; in general, primes will designate objects in the parameter plane ( $x^{i}$ ) corresponding to those on the surface in $x y z$-space.) The coordinate breakdown of the mapping $\\mathbf{r}$,\n\n\n\\begin{equation*}\nx=f\\left(x^{1}, x^{2}\\right) \\quad y=g\\left(x^{1}, x^{2}\\right) \\quad z=h\\left(x^{1}, x^{2}\\right) \\tag{10.10}\n\\end{equation*}\n\n\nis called the Gaussian form or representation of $\\mathscr{S}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-140}\n\\end{center}\n\nFig. 10-5\n\nPoint $P$ is a regular point of $\\mathscr{S}$ if\n\n\\[\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\times \\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k}  \\tag{10.11}\\\\\n\\frac{\\partial f}{\\partial x^{1}} & \\frac{\\partial g}{\\partial x^{1}} & \\frac{\\partial h}{\\partial x^{1}} \\\\\n\\frac{\\partial f}{\\partial x^{2}} & \\frac{\\partial g}{\\partial x^{2}} & \\frac{\\partial h}{\\partial x^{2}}\n\\end{array}\\right| \\neq \\mathbf{0}\n\\]\n\nat $P^{\\prime}$; otherwise, $P$ is a singular point. If every point of $\\mathscr{S}$ is a regular point, then $\\mathscr{S}$ is a regular surface.\n\nRemark 3: Condition (10.11) is tantamount to the linear independence of the two vectors $\\left(\\partial \\mathbf{r} / \\partial x^{1}\\right)_{P}$ and $\\left(\\partial \\mathbf{r} / \\partial x^{2}\\right)_{P}$. Equivalently, and of more geometrical interest, the condition ensures that every curve in $\\mathscr{S}$ through $P$ which we take to be the image under $\\mathbf{r}$ of a regular curve in $\\mathscr{V}$ through $P^{\\prime}$, is, in a neighborhood of $P$, regular in the sense of Section 10.2 .\n\nEXAMPLE 10.3 For a $C^{3}$ function $F$, show that the graph $z=F(x, y)$ is a regular surface.\n\nThe surface has the Gaussian representation\n\n$$\nx=x^{1} \\quad y=x^{2} \\quad z=F\\left(x^{1}, x^{2}\\right)\n$$\n\nand thus\n\n$$\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\times \\frac{\\partial \\mathbf{r}}{\\partial x^{2}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n1 & 0 & \\partial F / \\partial x^{1} \\\\\n0 & 1 & \\partial F / \\partial x^{2}\n\\end{array}\\right|=\\left(-\\frac{\\partial F}{\\partial x^{1}},-\\frac{\\partial F}{\\partial x^{2}}, 1\\right) \\neq \\mathbf{0}\n$$\n\nat an arbitrary surface point $P$. (This would be true if $F$ were merely class $C^{1}$.)\n\n\\section*{Subscript Notation for Partial Derivatives}\nFrom now on, write\n\n$$\n\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\equiv \\mathbf{r}_{1} \\quad \\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\equiv \\mathbf{r}_{2} \\quad \\frac{\\partial^{2} \\mathbf{r}}{\\partial x^{1} \\partial x^{1}}=\\mathbf{r}_{11} \\quad \\text { etc. }\n$$\n\nso that, e.g., (10.11) takes the compact form $\\mathbf{r}_{1} \\times \\mathbf{r}_{2} \\neq \\mathbf{0}$.\n\n\\subsection*{10.5 PARAMETRIC LINES; TANGENT SPACE}\nLet $\\left(x^{i}\\right)$ be taken as coordinates-for the moment, rectangular coordinates-in the parameter plane $\\mathbf{E}^{2}$, yielding two (orthogonal) families of coordinate lines:\n\n$$\n\\left\\{\\begin{array} { l } \n{ x ^ { 1 } = t } \\\\\n{ x ^ { 2 } = d }\n\\end{array} \\quad \\text { and } \\quad \\left\\{\\begin{array}{l}\nx^{1}=c \\\\\nx^{2}=\\sigma\n\\end{array}\\right.\\right.\n$$\n\nIf $(c, d)$ runs over $\\mathscr{V}$ (the pre-image of surface $\\mathscr{S}$ ), then the images under $\\mathbf{r}$ of these two families are the two sets of parametric lines (or coordinate curves) on $\\mathscr{S}$ :\n\n$$\n\\underbrace{\\mathbf{r}=\\mathbf{r}(t, d) \\equiv \\mathbf{p}(t)}_{\\boldsymbol{x}^{1} \\text {-curves }} \\quad \\underbrace{\\mathbf{r}=\\mathbf{r}(c, \\sigma) \\equiv \\mathbf{q}(\\sigma)}_{x^{2} \\text {-curves }}\n$$\n\nFigure 10-6 suggests that the net of parametric lines is orthogonal also. This is not, of course, true in general. In fact, since the tangent fields to the $x^{1}$-curves and the $x^{2}$-curves are respectively $d \\mathbf{p} / d t=\\mathbf{r}_{1}$ and $d \\mathbf{q} / d \\sigma=\\mathbf{r}_{2}$, the net is orthogonal if and only if $\\mathbf{r}_{1} \\mathbf{r}_{2}=0$ at every point of $\\mathscr{S}$.\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-141}\n\nFig. 10-6\n\nFor surface curves in general, the tangent vector of a curve passing through $\\mathbf{r}(c, d)$ is a linear combination of the vectors $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$, as the following analysis shows. Let the curve be given in the parameter plane as $\\mathscr{C}^{\\prime}: x^{1}=x^{1}(t), x^{2}=x^{2}(t)$; then the corresponding curve on the surface is\n\n$$\n\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right) \\equiv \\mathbf{r}(t)\n$$\n\nwith tangent vector\n\n\n\\begin{equation*}\n\\dot{\\mathbf{r}}=\\frac{\\partial \\mathbf{r}}{\\partial x^{1}} \\frac{d x^{1}}{d t}+\\frac{\\partial \\mathbf{r}}{\\partial x^{2}} \\frac{d x^{2}}{d t} \\equiv u^{1} \\mathbf{r}_{1}+u^{2} \\mathbf{r}_{2} \\equiv u^{i} \\mathbf{r}_{i} \\tag{10.12}\n\\end{equation*}\n\n\nHere, $u^{1} \\equiv d x^{1} / d t, u^{2} \\equiv d x^{2} / d t$, so that the vector $\\left(u^{i}\\right)$ in the parameter plane is the tangent to $\\mathscr{C}^{\\prime}$ at $P^{\\prime}$ (see Fig. 10-6).\n\nDefinition 3: The collection of linear combinations of the vectors $\\mathbf{r}_{1}(P)$ and $\\mathbf{r}_{2}(P)$ is called the tangent space of $\\mathscr{S}$ at $P$. The unit surface normal is the unit vector $\\mathbf{n}$ in the direction of $\\mathbf{r}_{1} \\times \\mathbf{r}_{2}$ :\n\n\n\\begin{equation*}\n\\mathbf{n}=\\frac{1}{E}\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right) \\quad\\left(E \\equiv\\left\\|\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right\\|>0\\right) \\tag{10.13}\n\\end{equation*}\n\n\nThe geometric realization of the tangent space is obviously the tangent plane at $P$, and the surface normal can be identified with a line segment through $P$ perpendicular to this tangent plane; that is, orthogonal to the surface at $P$, as indicated in Fig. 10-6.\n\nTo summarize this whole affair, the linearly independent (by regularity) triad of vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}$ forms a moving frame for the surface, as shown in Fig. 10-7, much in the manner that a moving triad exists for a regular curve having a principal normal.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-142}\n\\end{center}\n\nFig. 10-7\n\n\\subsection*{10.6 FIRST FUNDAMENTAL FORM}\nConsider a curve on the regular surface $\\mathscr{S}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}, x^{2}\\right)$ given by $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right) \\equiv \\mathbf{r}(t)$, with pre-image $\\mathscr{C}^{\\prime}: x^{i}=x^{i}(t)$ in the parameter plane. Using (10.12) and recalling that the (Euclidean) inner product is distributive over linear combinations of vectors, arc length along $\\mathscr{C}$ is calculated as\n\n\n\\begin{equation*}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\|\\dot{\\mathbf{r}}\\|^{2}=\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}=\\left(u^{i} \\mathbf{r}_{i}\\right)\\left(u^{j} \\mathbf{r}_{j}\\right) \\equiv g_{i j} u^{i} u^{j} \\tag{10.14a}\n\\end{equation*}\n\n\nin which\n\n\n\\begin{equation*}\ng_{i j}=\\mathbf{r}_{i} \\mathbf{r}_{j} \\quad(1 \\leqq i, j \\leqq 2) \\tag{10.15}\n\\end{equation*}\n\n\nand, as above, $u^{i}=d x^{i} / d t$. In the equivalent differential form,\n\n\n\\begin{equation*}\nd s^{2}=g_{i j} d x^{i} d x^{j} \\equiv \\mathrm{I} \\tag{10.14b}\n\\end{equation*}\n\n\nthe arc-length formula is known as the First Fundamental Form (abbreviated FFF) of the surface $\\mathscr{S}$. In view of (10.12) and the regularity of $\\mathscr{S},\\|\\dot{\\mathbf{r}}\\|=0$ if and only if $u^{1}=u^{2}=0$; this proves\n\nLemma 10.2: The FFF of a regular surface is positive definite.\n\nLemma 10.2 implies that $g \\equiv \\operatorname{det}\\left(g_{i j}\\right)>0$; in fact, we can use Lagrange's identity,\n\n$$\n\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right)^{2}=\\left(\\mathbf{r}_{1}^{2}\\right)\\left(\\mathbf{r}_{2}^{2}\\right)-\\left(\\mathbf{r}_{1} \\mathbf{r}_{2}\\right)^{2}\n$$\n\nto establish that\n\n\n\\begin{equation*}\ng=E^{2} \\tag{10.16}\n\\end{equation*}\n\n\ncf. (10.13).\n\nEXAMPLE 10.4 Compute the FFF for the right helicoid (Fig. 10-8),\n\n$$\n\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, a x^{2}\\right)\n$$\n\nWe have:\n\nwhence\n\n$$\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, a\\right)\n$$\n\nwhence\n\n$$\ng_{11}=\\mathbf{r}_{1}^{2}=\\cos ^{2} x^{2}+\\sin ^{2} x^{2}+0^{2}=1\n$$\n\n$$\n\\begin{aligned}\n& g_{12}=\\mathbf{r}_{1} \\mathbf{r}_{2}=\\left(\\cos x^{2}\\right)\\left(-x^{1} \\sin x^{2}\\right)+\\left(\\sin x^{2}\\right)\\left(x^{1} \\cos x^{2}\\right)=0 \\\\\n& g_{22}=\\mathbf{r}_{2}^{2}=\\left(-x^{1}\\right)^{2}\\left(\\sin ^{2} x^{2}\\right)+\\left(x^{1}\\right)^{2}\\left(\\cos ^{2} x^{2}\\right)+a^{2}=\\left(x^{1}\\right)^{2}+a^{2}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\mathbf{I}=\\left(d x^{1}\\right)^{2}+\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]\\left(d x^{2}\\right)^{2}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-143}\n\\end{center}\n\nFig. 10-8\n\nAlong with the FFF, tensor calculus enters the picture. For the intrinsic properties of a particular surface $\\mathscr{S}$ in $\\mathbf{E}^{3}$ (the properties defined by measurements of distance on the surface) are all implicit in $(10.14 b)$, which can be interpreted as a particular Riemannian metrization of the parameter plane. Thus, the study of intrinsic properties of surfaces becomes the tensor analysis of Riemannian metrics in $\\mathbf{R}^{2}$ - and this may be conducted without any reference to $\\mathbf{E}^{3}$ whatever. Observe that the metrics under consideration will all be positive definite (Lemma 10.2) but not necessarily Euclidean (see Theorem 9.1). Accordingly, we shall drop the designation $\\mathbf{E}^{2}$ for the parameter plane, which shall henceforth be referred to general coordinates $\\left(x^{i}\\right)$.\n\nEXAMPLE 10.5 The metric for $\\mathbf{R}^{2}$ corresponding to the right helicoid (Example 10.4) is non-Euclidean, as is demonstrated in Problem 10.27. Now the parameters $x^{1}$ and $x^{2}$, which are actual polar coordinates in the $x y$-plane of $\\mathbf{E}^{3}$ (see Fig. 10-8), formally keep that significance when the plane is considered abstractly as parameter space. This is an instance of the formal use of a familiar coordinate system in a non-Euclidean space, as mentioned in Section 3.1.\n\n\\section*{Unit Tangent Vector}\nIf $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right)$ is any curve on $\\mathscr{S}$, then by (10.12) and (10.14a),\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{j k} u^{j} u^{k}}} \\tag{10.17}\n\\end{equation*}\n\n\n\\section*{Angle Between Two Curves}\nLet $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ be two intersecting curves on $\\mathscr{S}$ that correspond to $x^{i}=\\phi^{i}(t)$ and $x^{i}=\\psi^{i}(\\sigma) \\quad(i=$ $1,2)$ in the parameter plane. Writing $u^{i} \\equiv d \\phi^{i} / d t$ and $v^{i} \\equiv d \\psi^{i} / d \\sigma$, we have for the angle $\\theta$ between $\\mathbf{T}_{1}$ of $\\mathscr{C}_{1}$ and $\\mathbf{T}_{2}$ of $\\mathscr{C}_{2}$ :\n\n\n\\begin{equation*}\n\\cos \\theta=\\mathbf{T}_{1} \\mathbf{T}_{2}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{p q} u^{p} u^{q}}} \\cdot \\frac{v^{j} \\mathbf{r}_{j}}{\\sqrt{g_{r s} v^{r} v^{s}}}=\\frac{g_{i j} u^{i} v^{j}}{\\sqrt{g_{p q} u^{p} u^{q}} \\sqrt{g_{r s} v^{r} v^{s}}} \\tag{10.18}\n\\end{equation*}\n\n\nCompare (5.11).\n\nTheorem 10.3: The angle between the two parametric lines through a surface point is\n\n\n\\begin{equation*}\n\\cos \\theta=\\frac{g_{12}}{\\sqrt{g_{11}} \\sqrt{g_{22}}} \\tag{10.19}\n\\end{equation*}\n\n\nCorollary 10.4: The two families of parametric lines form an orthogonal net if and only if $g_{12}=0$ at every point of $\\mathscr{S}$.\n\n\\subsection*{10.7 GEODESICS ON A SURFACE}\nA further link with tensors is provided by the concept of geodesics for regular surfaces. One can intuitively imagine stretching a string between two points on a surface, and pulling it tight: on a sphere this would lead to a great circular arc, and on a right circular cylinder, a helical arc. Since from our point of view the surface is disregarded and $\\left(g_{i j}\\right)$ is taken as a metric for the parameter plane, the problem has already been worked out (Section 7.6).\n\nRelative to the FFF of $\\mathscr{S}$, define the Christoffel symbols through formulas (6.1) and (6.4), $n=2$. [Problem 10.48 gives an equivalent \"extrinsic\" definition, in terms of the vector $\\mathbf{r}$.] Then a geodesic on $\\mathscr{S}$ is any curve $\\mathbf{r}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t)\\right)$ in the surface whose pre-image in the parameter $\\mathbf{R}^{2}$ satisfies the system of differential equations (7.11)-(7.12); if $t=s=$ arc length, the governing system is (7.13). [Remember that the (non-Euclidean) distance measured by $s$ in $\\mathbf{R}^{2}$ is the Euclidean distance along the geodesic as a curve in $\\mathbf{E}^{3}$.]\n\nSimilarly, harking back to Section 6.5, the intrinsic curvature of a curve $\\mathscr{C}$ in $\\mathscr{S}$ is the function\n\n\n\\begin{equation*}\n\\tilde{\\kappa}(s)=\\sqrt{g_{i j} b^{i} b^{j}} \\tag{10.20}\n\\end{equation*}\n\n\n-cf. (6.12)-where the intrinsic curvature vector ( $b^{i}$ ) (in $\\mathbf{R}^{2}$ ) is given by (6.11).\n\nRemark 4: Intrinsic curvature can be shown to be the instantaneous rate of change of the angle between the tangent vector of $\\mathscr{C}$ and another vector in the tangent space that is \"transported parallelly\" along the curve. Here, the term \"parallel\" refers to a certain generalization of Euclidean parallelism (see Problem 10.22).\n\nTheorem 10.5: A curve on a surface is a geodesic if and only if its intrinsic curvature $\\tilde{\\kappa}$ is identically zero.\n\nIn contrast to the above intrinsic characterization of geodesics, there is an interesting and useful extrinsic characterization, proved as Problem 10.18. It adds a visual dimension that often allows the immediate identification of a geodesic.\n\nTheorem 10.6: A curve on a regular surface is a geodesic if and only if a principal normal $\\mathbf{N}$ of the curve can be chosen that coincides with the surface normal $\\mathbf{n}$ at all points along the curve.\n\n\\subsection*{10.8 SECOND FUNDAMENTAL FORM}\nBy taking the dot product of the surface normal with the second partial derivatives of $\\mathbf{r}$ with respect to $x^{1}$ and $x^{2}$,\n\n\n\\begin{equation*}\nf_{i j} \\equiv \\mathbf{n} \\mathbf{r}_{i j} \\tag{10.21}\n\\end{equation*}\n\n\nwe generate the coefficients of the Second Fundamental Form (SFF) of a surface:\n\n\n\\begin{equation*}\nf_{i j} d x^{i} d x^{j} \\equiv \\mathrm{II} \\tag{10.22}\n\\end{equation*}\n\n\n\\section*{Curvature of a Normal Section}\nIf $\\mathscr{F}$ is a plane containing the surface normal $\\mathbf{n}$ at some point $P$ of $\\mathscr{S}$ (Fig. 10-9), the curvature of the normal section of $\\mathscr{S}$ (the curve of intersection of $\\mathscr{S}$ and $\\mathscr{F}$ ), denoted $\\mathscr{C}_{\\mathscr{F}}$, is given at point $P$ by the formula\n\n\n\\begin{equation*}\n\\kappa_{\\mathscr{F}}=\\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}=\\frac{\\mathrm{II}}{\\mathrm{I}} \\tag{10.23}\n\\end{equation*}\n\n\nwhere $\\left(u^{i}\\right)=\\left(d x^{i} / d t\\right)$ gives the direction, at $P^{\\prime}$, of the curve corresponding to $C_{\\mathscr{F}}$ in the parameter plane; see Problem 10.23.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-145}\n\\end{center}\n\nFig. 10-9\n\nAs $\\mathscr{F}$ rotates about $\\mathbf{n}$, the curvature $\\kappa_{\\mathscr{F}}$ of $\\mathscr{C}_{\\mathscr{F}}$ at $P$ is periodic and will reach an absolute maximum and an absolute minimum; let\n\n\n\\begin{equation*}\n\\max \\kappa_{\\mathscr{F}} \\equiv \\kappa_{1} \\quad \\min \\kappa_{\\mathscr{F}} \\equiv \\kappa_{2} \\tag{10.24}\n\\end{equation*}\n\n\nThe two section curves having these two extremal curvatures are called principal curves, and their directions are the principal directions. If $\\kappa_{1}=\\kappa_{2}$ at $P$, all the normal sections at $P$ have the same curvature and unique principal directions do not exist. (In this case, $P$ is called an umbilical point of the surface.)\n\n\\section*{Surface Curvature}\nTwo measures of the curvature of a surface $\\mathscr{S}$ are commonly used.\n\nDefinition 4: The Gaussian curvature of $\\mathscr{S}$ at point $P$ is the number $\\mathrm{K}=\\kappa_{1} \\kappa_{2}$; the mean curvature is the number $\\mathrm{H}=\\kappa_{1}+\\kappa_{2}$.\n\nIt will be proved in Problem 10.25 that the extreme curvatures $\\kappa_{1}$ and $\\kappa_{2}$ are the roots of the following quadratic equation in $\\lambda$ :\n\n\n\\begin{equation*}\n\\left(g_{11} g_{22}-g_{12}^{2}\\right) \\lambda^{2}-\\left(f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}\\right) \\lambda+\\left(f_{11} f_{22}-f_{12}^{2}\\right)=0 \\tag{10.25}\n\\end{equation*}\n\n\nThe relations between the roots and the coefficients of a polynomial equation then give:\n\n\n\\begin{equation*}\n\\mathbf{K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}} \\quad \\mathbf{H}=\\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}} \\tag{10.26}\n\\end{equation*}\n\n\n\\subsection*{10.9 STRUCTURE FORMULAS FOR SURFACES}\nTwo fundamental sets of relationships involve the parts of the moving triad of a surface, $\\left(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}\\right)$.\n\n\\section*{Equations of Weingarten}\nSince $\\mathbf{n}^{2}=1, \\partial\\left(\\mathbf{n}^{2}\\right) / \\partial x^{i}=2 \\mathbf{n n}_{i}=0 \\quad(i=1,2)$. Hence, for each $i, \\mathbf{n}_{i}$ lies in the tangent space: $\\mathbf{n}_{i}=u_{i}^{1} \\mathbf{r}_{1}+u_{i}^{2} \\mathbf{r}_{2}$, for certain scalars $u_{i}^{k}$. Similarly, from orthogonality,\n\n$$\n0=\\left(\\mathbf{n r}_{i}\\right)_{j}=\\mathbf{n}_{j} \\mathbf{r}_{i}+\\mathbf{n} \\mathbf{r}_{i j} \\quad \\text { or } \\quad \\mathbf{n}_{j} \\mathbf{r}_{i}=-f_{i j}\n$$\n\nIt follows (Problem 10.28) that\n\n\n\\begin{equation*}\n\\mathbf{n}_{i}=-g^{j k} f_{i j} \\mathbf{r}_{k} \\tag{10.27a}\n\\end{equation*}\n\n\nfor $i=1,2$. From the explicit form of the inverse metric matrix $\\left(g^{i j}\\right),(10.27 a)$ may be spelled out as follows:\n\n\n\\begin{align*}\n& \\mathbf{n}_{1}=\\frac{g_{12} f_{12}-g_{22} f_{11}}{g} \\mathbf{r}_{1}+\\frac{g_{12} f_{11}-g_{11} f_{12}}{g} \\mathbf{r}_{2} \\\\\n& \\mathbf{n}_{2}=\\frac{g_{12} f_{22}-g_{22} f_{12}}{g} \\mathbf{r}_{1}+\\frac{g_{12} f_{12}-g_{11} f_{22}}{g} \\mathbf{r}_{2} \\tag{10.27b}\n\\end{align*}\n\n\n\\section*{Equations of Gauss}\nSince $\\left(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\mathbf{n}\\right)$ is a basis for $\\mathbf{E}^{3}$, we can write $\\mathbf{r}_{i j}=u_{i j}^{1} \\mathbf{r}_{1}+u_{i j}^{2} \\mathbf{r}_{2}+u_{i j}^{3} \\mathbf{n}$. Evaluation of the coefficients (Problem 10.29) leads to\n\n\n\\begin{equation*}\n\\mathbf{r}_{i j}=\\Gamma_{i j}^{k} \\mathbf{r}_{k}+f_{i j} \\mathbf{n} \\tag{10.28}\n\\end{equation*}\n\n\n\\section*{An Identity Between FFF and SFF}\nSince $\\mathbf{r}_{i j k}=\\mathbf{r}_{i k j}$, (10.28) implies $\\left(\\Gamma_{i j}^{s} \\mathbf{r}_{s}+f_{i j} \\mathbf{n}\\right)_{k}=\\left(\\Gamma_{i k}^{s} \\mathbf{r}_{s}+f_{i k} \\mathbf{n}\\right)_{j}$, or\n\n$$\n\\left(\\Gamma_{i j}^{s}\\right)_{k} \\mathbf{r}_{s}+\\Gamma_{i j}^{s} \\mathbf{r}_{s k}+f_{i j k} \\mathbf{n}+f_{i j} \\mathbf{n}_{k}=\\left(\\Gamma_{i k}^{s}\\right)_{j} \\mathbf{r}_{s}+\\Gamma_{i k}^{s} \\mathbf{r}_{s j}+f_{i k j} \\mathbf{n}+f_{i k} \\mathbf{n}_{j}\n$$\n\nDot both sides with $\\mathbf{r}_{l}$ and use the definition $\\mathbf{r}_{l} \\mathbf{r}_{s} \\equiv g_{l s}$ and the relations $\\mathbf{r}_{l} \\mathbf{r}_{s k}=\\Gamma_{s k l}$ (Problem 10.48) and $\\mathbf{r}_{l} \\mathbf{n}=0$ :\n\n$$\n\\left(\\Gamma_{i j}^{s}\\right)_{k} g_{s l}+\\Gamma_{i j}^{s} \\Gamma_{s k l}+f_{i j} \\mathbf{n}_{k} \\mathbf{r}_{l}=\\left(\\Gamma_{i k}^{s}\\right)_{j} g_{s l}+\\Gamma_{i k}^{s} \\Gamma_{s j l}+f_{i k} \\mathbf{n}_{j} \\mathbf{r}_{l}\n$$\n\nNow substitute for the $\\mathbf{n}_{i}$ from (10.27a) and use $\\mathbf{r}_{t} \\mathbf{r}_{l} \\equiv g_{t l}$ and $g^{s t} g_{t l}=\\delta_{l}^{s}$ to simplify the result:\n\n$$\n-f_{i j} f_{k l}+f_{i k} f_{j l}=g_{s l}\\left(\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\dot{\\Gamma}_{r k}^{s}\\right)\n$$\n\nFinally, introducing the Riemann tensor via (8.2) and (8.3), we obtain\n\n\n\\begin{equation*}\nR_{i i k l}=f_{i k} f_{i l}-f_{i l} f_{i k} \\tag{10.29}\n\\end{equation*}\n\n\nThe left member of (10.29) depends only on the coefficients of I together with their first and second derivatives; the right member depends only on the coefficients of II. This essential compatibility relation between the two fundamental forms must hold at every point of a regular surface.\n\n\\section*{The 'Most Excellent Theorem' of Gauss}\nBy (10.26) and (10.29),\n\n\n\\begin{equation*}\n\\mathrm{K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{R_{1212}}{g} \\tag{10.30}\n\\end{equation*}\n\n\nThus, the numerator of $\\mathrm{K}$ can be derived entirely from the FFF. Since the denominator is also obviously from the FFF, we have:\n\nTheorem 10.7 (Theorema Egregium): The Gaussian curvature is an intrinsic property of a surface, depending only on the First Fundamental Form and its derivatives.\n\nRemark 5: The motive for the definition (8.7) of Riemannian curvature is now apparent.\n\n\\subsection*{10.10 ISOMETRIES}\nThe practical question of whether inhabitants of a fog-enshrouded planet could, solely by measuring distances on the surface of the planet, determine its curvature, is answered in the affirmative by Theorem 10.7. A further important conclusion can be drawn.\n\nSuppose that two surfaces, $\\mathscr{S}^{(1)}: \\mathbf{r}^{(1)}=\\mathbf{r}^{(1)}\\left(x^{1}, x^{2}\\right)$ and $\\mathscr{P}^{(2)}: \\mathbf{r}^{(2)}=\\mathbf{r}^{(2)}\\left(x^{1}, x^{2}\\right)$, are defined over the same region $\\mathscr{V}$ of the plane and that the First Fundamental Forms agree on $\\mathscr{V}$. This will obviously set up a correspondence between $\\mathscr{S}^{(1)}$ and $\\mathscr{S}^{(2)}$ in $\\mathbf{E}^{3}$ that is bijective between small patches (induced by the neighborhoods of $\\mathscr{V}$ over which both $\\mathbf{r}^{(i)}$ are bijective) of the two surfaces. This correspondence is called a local isometry between $\\mathscr{S}^{(1)}$ and $\\mathscr{S}^{(2)}$ because the two surfaces are, patch for patch, metrically identical. But then (Theorem 10.7) the Gaussian curvatures $K^{(1)}$ and $K^{(2)}$ must be equal at corresponding points.\n\nTheorem 10.8: If two surfaces are locally isometric, their Gaussian curvatures are identical.\n\nIn the case of constant Gaussian curvature K, Beltrami's theorem tells us that there is a parameterization for $\\mathscr{S}$ for which the FFF takes on the form:\n\n$$\n\\begin{array}{ll}\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\sinh ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=-1 / a^{2}<0 \\\\\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=0 \\\\\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\sin ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2} & \\text { if } \\mathbf{K}=1 / a^{2}>0\n\\end{array}\n$$\n\nEXAMPLE 10.6 The plane and the sphere are surfaces of constant zero curvature and constant positive curvature, respectively. For a surface of constant negative curvature, see Problem 10.49.\n\nBeltrami's theorem implies a partial converse of Theorem 10.8:\n\nTheorem 10.9 (Minding's Theorem): If two surfaces are of the same constant Gaussian curvature, they are locally isometric.\n\nRemark 6: A proof of Theorem 10.9 for zero curvature was given in Problem 9.9.\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{CURVE THEORY; THE MOVING FRAME}\n10.1 The curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array} { l } \n{ x = t } \\\\\n{ y = t ^ { 4 } } \\\\\n{ z = 0 }\n\\end{array} \\quad ( t < 0 ) \\quad \\left\\{\\begin{array}{l}\nx=t \\\\\ny=0 \\\\\nz=t^{4}\n\\end{array} \\quad(t \\geqq 0)\\right.\\right.\n$$\n\nlies partly in the $x y$-plane and partly in the $x z$-plane (Fig. 10-10). Show that it is regular of class $C^{3}$, but that it possesses no principal normal vector.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-148}\n\\end{center}\n\nFig. 10-10\n\nThe component functions for $\\mathbf{r}(t)$ are\n\n$$\nx(t)=t \\quad y(t)=\\left\\{\\begin{array}{ll}\nt^{4} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad z(t)= \\begin{cases}0 & t<0 \\\\\nt^{4} & t \\geqq 0\\end{cases}\\right.\n$$\n\nWhen $t<0, \\dot{y}(t)=4 t^{3}$. As $t \\rightarrow 0$,\n\n$$\n\\lim _{t \\rightarrow-0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow-0} \\frac{t^{4}}{t}=0 \\quad \\lim _{t \\rightarrow+0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow+0} \\frac{0}{t}=0\n$$\n\nhence, $y(t)$ is differentiable at $t=0$. Clearly, $\\dot{y}(t)=0$ for $t>0$. A similar analysis applies to $z(t)$. Hence:\n\n$$\n\\dot{y}(t)=\\left\\{\\begin{array}{ll}\n4 t^{3} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dot{z}(t)= \\begin{cases}0 & t<0 \\\\\n4 t^{3} & t \\geqq 0\\end{cases}\\right.\n$$\n\nwhich are continuous functions. Continuing the analysis up to the third derivatives:\n\n$$\n\\dddot{y}(t)=\\left\\{\\begin{array}{ll}\n24 t & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dddot{z}(t)= \\begin{cases}0 & t<0 \\\\\n24 t & t \\geqq 0\\end{cases}\\right.\n$$\n\nHence, $x(t)$ being differentiable to all orders, $\\mathbf{r}(t)$ is of class $C^{3}$. Furthermore, because $\\dot{x}(t) \\equiv 1, \\dot{\\mathbf{r}}(t) \\neq 0$ for all $t$ and $\\mathscr{C}$ is regular. However, the principal normal, which exists for the separate parts of $\\mathscr{C}$ (lying in the $x y$-plane for $t<0$ and in the $x z$-plane for $t>0$ ), cannot possibly be continuous at $t=0$, let alone differentiable. Hence, $\\mathscr{C}$ does not possess a principal normal.\n\n10.2 (a) Describe the curve $\\mathbf{r}=\\left(\\cos t, \\sin t, \\tan ^{-1} t\\right)$, where $0 \\leqq t$ and where the principal value of the arctangent is understood. (b) Find the arc length between the points $\\mathbf{r}(0)$ and $\\mathbf{r}(1)$.\\\\\n(a) This is a form of the circular helix, except that the pitch decreases with increasing $t$. The curve lies on the right circular cylinder $x^{2}+y^{2}=1$; beginning at $(1,0,0)$, it winds around the cylinder and approaches the circle $x^{2}+y^{2}=1, z=\\pi / 2$ asymptotically as $t \\rightarrow \\infty$.\n\n(b)\n\n$$\n\\dot{\\mathbf{r}}=\\left(-\\sin t, \\cos t, \\frac{1}{t^{2}+1}\\right) \\quad \\text { or } \\quad \\frac{d s}{d t}=\\sqrt{\\sin ^{2} t+\\cos ^{2} t+\\frac{1}{\\left(t^{2}+1\\right)^{2}}}\n$$\n\nA numerical method of integration is required. Using Simpson's rule on a programmable calculator, one obtains\n\n$$\nL=\\int_{0}^{1} \\frac{\\sqrt{t^{4}+2 t^{2}+2}}{t^{2}+1} d t \\approx 1.27797806\n$$\n\n10.3 Find the moving frame for the curve\n\n$$\n\\mathscr{C}: \\mathbf{r}=\\left(\\frac{3-3 t^{3}}{5}, \\frac{4+4 t^{3}}{5}, 3 t\\right) \\quad(t \\text { real })\n$$\n\nShow that the binormal vector $\\mathbf{B}$ is constant, so that the curve is actually planar.\n\nMaking the calculations required in (10.5):\n\nand\n\n$$\n\\dot{\\mathbf{r}}=\\left(\\frac{-9 t^{2}}{5}, \\frac{12 t^{2}}{5}, 3\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{\\frac{81}{25} t^{4}+\\frac{144}{25} t^{4}+9}=3 \\sqrt{t^{4}+1}\n$$\n\n$$\n\\mathbf{T}=\\frac{\\left(-9 t^{2} / 5,12 t^{2} / 5,3\\right)}{3 \\sqrt{t^{4}+1}}=\\frac{\\left(-3 t^{2}, 4 t^{2}, 5\\right)}{5 \\sqrt{t^{4}+1}}\n$$\n\n$$\n\\begin{aligned}\n& \\ddot{\\mathbf{r}}=\\left(\\frac{-18 t}{5}, \\frac{24 t}{5}, 0\\right) \\\\\n& \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{9}{5} t^{2} & \\frac{12}{5} t^{2} & 3 \\\\\n-\\frac{18 t}{5} & \\frac{24 t}{5} & 0\n\\end{array}\\right|=\\left(-\\frac{72 t}{5},-\\frac{54 t}{5}, 0\\right)=\\frac{-18 t}{5}(4,3,0) \\\\\n& \\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|=\\frac{18|t|}{5} \\sqrt{4^{2}+3^{2}+0^{2}}=18|t| \\\\\n& (\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}=\\left(9 t^{4}+9\\right)\\left(-\\frac{18}{5} t, \\frac{24}{5} t, 0\\right)=\\left(-\\frac{162}{5} t^{5}-\\frac{162}{5} t, \\frac{216}{5} t^{5}+\\frac{216}{5} t, 0\\right) \\\\\n& (\\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(\\frac{9 \\cdot 18}{25} t^{3}+\\frac{24 \\cdot 12}{25} t^{3}+0\\right)\\left(-\\frac{9}{5} t^{2}, \\frac{12}{5} t^{2}, 3\\right)=\\left(-\\frac{162}{5} t^{5}, \\frac{216}{5} t^{5}, 54 t^{3}\\right) \\\\\n& (\\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(-\\frac{162}{5} t, \\frac{216}{5} t,-54 t^{3}\\right)=18 t\\left(-\\frac{9}{5}, \\frac{12}{5},-3 t^{2}\\right) \\\\\n& \\quad \\mathbf{N}=\\varepsilon \\frac{-18 t\\left(9 / 5,-12 / 5,3 t^{2}\\right)}{\\left(3 \\sqrt{t^{4}+1}\\right)(18|t|)}=-\\frac{\\varepsilon t}{|t|} \\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}}\n\\end{aligned}\n$$\n\nand\n\nNow choose $\\varepsilon=+1$ when $t<0$ and -1 otherwise, making\n\n$$\n\\mathbf{N}=\\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}} \\quad \\mathbf{B}=\\varepsilon \\frac{(-18 t / 5)(4,3,0)}{18|t|}=\\frac{-\\varepsilon t}{|t|}\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)=\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)\n$$\n\n10.4 Establish the general formulas (10.5) for the moving frame of the curve $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}(t)$, with an arbitrary parameter $t$.\n\nBy definition,\n\n$$\n\\frac{d s}{d t}=\\|\\dot{\\mathbf{r}}\\| \\equiv(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{1 / 2} \\quad \\text { or } \\quad \\frac{d t}{d s}=\\|\\dot{\\mathbf{r}}\\|^{-1}\n$$\n\nand we have at once for the unit tangent vector\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\dot{\\mathbf{r}} \\frac{d t}{d s}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\nTo obtain a principal normal, first calculate\n\n$$\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\| \\equiv \\frac{d}{d t}(\\dot{\\mathbf{r}})^{1 / 2}=\\frac{1}{2}(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{-1 / 2}(\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}}+\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}})=\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\n(Note the general formula $d\\|\\mathbf{u}\\| / d t=\\mathbf{u \\dot { u }} /\\|\\mathbf{u}\\|$.) Hence,\n\nand\n\n$$\n\\begin{gathered}\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=-\\|\\dot{\\mathbf{r}}\\|^{-2} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|=-\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\dot{\\mathbf{T}}=\\ddot{\\mathbf{r}} \\frac{d t}{d s}+\\dot{\\mathbf{r}} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=\\frac{\\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}-\\frac{(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\mathbf{T}^{\\prime}=\\dot{\\mathbf{T}} \\frac{d t}{d s}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{4}}=-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\n\\end{gathered}\n$$\n\nwhere the last step used the vector identify $\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}$. It follows that $\\mathbf{N}$ can be constructed by normalizing the vector\n\n$$\n\\mathbf{N}^{*} \\equiv-\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\n$$\n\nSince $\\dot{\\mathbf{r}}$ and $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$ are orthogonal, $\\left\\|\\mathbf{N}^{*}\\right\\|=\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|$ and so, provided $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{N}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nFinally, for the binormal vector, with $\\mathbf{v} \\equiv \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|} \\times \\varepsilon \\frac{\\mathbf{v} \\times \\dot{\\mathbf{r}}}{\\|\\mathbf{v}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\mathbf{v}-(\\dot{\\mathbf{r}} \\mathbf{v}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\mathbf{v}-(0) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$\n\n\\section*{CURVATURE AND TORSION}\n10.5 Find the curvature and the torsion of the circular helix\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c=\\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nwhere $s$ is arc length.\n\nBy differentiation with respect to arc length,\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\left(-\\frac{a}{c} \\sin \\frac{s}{c}, \\frac{a}{c} \\cos \\frac{s}{c}, \\frac{b}{c}\\right) \\quad \\mathbf{T}^{\\prime}=\\left(-\\frac{a}{c^{2}} \\cos \\frac{s}{c},-\\frac{a}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nNormalizing $\\mathbf{T}^{\\prime}$, choose\n\n$$\n\\mathbf{N}=\\left(-\\cos \\frac{s}{c},-\\sin \\frac{s}{c}, 0\\right)\n$$\n\nand, correspondingly,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{a}{c} \\sin \\frac{s}{c} & \\frac{a}{c} \\cos \\frac{s}{c} & \\frac{b}{c} \\\\\n-\\cos \\frac{s}{c} & -\\sin \\frac{s}{c} & 0\n\\end{array}\\right|=\\left(\\frac{b}{c} \\sin \\frac{s}{c},-\\frac{b}{c} \\cos \\frac{s}{c}, \\frac{a}{c}\\right)\n$$\n\n$$\n\\mathbf{B}^{\\prime}=\\left(\\frac{b}{c^{2}} \\cos \\frac{s}{c}, \\frac{b}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nThen, by (10.6),\n\n$$\n\\kappa=\\frac{a}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{a}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{a}{c^{2}} \\quad \\tau=\\frac{b}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{b}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{b}{c^{2}}\n$$\n\n[If we introduce the \"time\" parameter $t=c s$, we then have:\n\n$$\n\\frac{d z}{d t}=\\frac{b}{c^{2}}=\\tau\n$$\n\ni.e. the rate at which the helix rises out of the $x y$-plane (its osculating plane at $t=0$ ) is given by its (constant) torsion.]\n\n10.6 Find the curvature and torsion of the curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(t$ real $)$.\n\nUse the formulas $(10.8)$ :\n\n$$\n\\begin{gathered}\n\\dot{\\mathbf{r}}=\\left(2 t+\\sqrt{2}, 2 t-\\sqrt{2}, 2 t^{2}\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{(2 t+\\sqrt{2})^{2}+(2 t-\\sqrt{2})^{2}+4 t^{4}}=2\\left(t^{2}+1\\right) \\\\\n\\ddot{\\mathbf{r}}=(2,2,4 t) \\quad \\ddot{\\mathbf{r}}=(0,0,4) \\\\\n\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n2 t+\\sqrt{2} & 2 t-\\sqrt{2} & 2 t^{2} \\\\\n2 & 2 & 4 t\n\\end{array}\\right|=4\\left(t^{2}-t \\sqrt{2},-\\left(t^{2}+t \\sqrt{2}\\right), \\sqrt{2}\\right) \\\\\n(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})^{2}=16\\left[\\left(t^{2}-t \\sqrt{2}\\right)^{2}+\\left(t^{2}+t \\sqrt{2}\\right)^{2}+2\\right]=32\\left(t^{2}+1\\right)^{2} \\\\\n\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]=\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=(0,0,4) \\cdot 4\\left(t^{2}-t \\sqrt{2},-t^{2}-t \\sqrt{2}, \\sqrt{2}\\right)=16 \\sqrt{2}\n\\end{gathered}\n$$\n\nHence\n\n$$\n\\kappa=\\frac{\\varepsilon \\sqrt{32\\left(t^{2}+1\\right)^{2}}}{8\\left(t^{2}+1\\right)^{3}}=\\frac{\\varepsilon}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}} \\quad \\tau=\\frac{16 \\sqrt{2}}{32\\left(t^{2}+1\\right)^{2}}=\\frac{1}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}}\n$$\n\n\\subsection*{10.7 Prove (10.8).}\nUsing the results of Problem 10.4, we have\n\n$$\n\\begin{aligned}\n\\kappa=\\mathbf{N T}^{\\prime} & =\\left(\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\\right) \\cdot\\left(-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\\right)=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\\|^{2}}{\\|\\dot{\\mathbf{r}}\\|^{5}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\\\\n& =\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2} \\sin ^{2}(\\pi / 2)}{\\|\\dot{\\mathbf{r}}\\|^{5} \\mid \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}{\\|\\dot{\\mathbf{r}}\\|^{3}}\n\\end{aligned}\n$$\n\nThe torsion requires the computation of $\\mathbf{B}^{\\prime}$. By (10.5),\n\n$$\n\\varepsilon \\mathbf{B}=\\frac{\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\equiv \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$\n\nwhence\n\n$$\n\\varepsilon_{\\varepsilon} \\dot{\\mathbf{B}}=\\frac{d}{d t}\\left(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\right)=\\frac{1}{\\|\\mathbf{v}\\|} \\dot{\\mathbf{v}}+\\frac{d}{d t}\\left(\\frac{1}{\\|\\mathbf{v}\\|}\\right) \\mathbf{v}=\\frac{\\dot{\\mathbf{v}}}{\\|\\mathbf{v}\\|}-\\frac{(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}=\\frac{\\|\\mathbf{v}\\|^{2} \\dot{\\mathbf{v}}-(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}\n$$\n\nBut $\\dot{\\mathbf{r}}=d(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) / d t=(\\ddot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})+(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$; hence,\n\n$$\n\\varepsilon \\mathbf{B}^{\\prime}=\\frac{\\varepsilon \\dot{\\mathbf{B}}}{\\|\\dot{\\mathbf{r}}\\|}=\\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})-[(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})](\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{3}}\n$$\n\nDot this with\n\n$$\n\\varepsilon \\mathbf{N}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nfrom (10.5), and use $\\mathbf{u} \\cdot(\\mathbf{u} \\times \\mathbf{w})=0$ :\n\n$$\n\\begin{gathered}\n\\mathbf{N B}^{\\prime}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}[\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})]-0-0+0}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{4}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2}(-\\operatorname{det}[\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}])}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}} \\\\\n\\tau=\\frac{\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}}\n\\end{gathered}\n$$\n\nor\n\n10.8 Prove $(a) \\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B},(b) \\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}$.\n\n(a) Since $\\mathbf{N N}=1, \\mathbf{N}^{\\prime}$ is orthogonal to $\\mathbf{N}$, which puts it in the plane of $\\mathbf{T}$ and $\\mathbf{B}$. Therefore, for certain real $\\lambda$ and $\\mu$,\n\n\n\\begin{equation*}\n\\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}+\\mu \\mathbf{B} \\tag{1}\n\\end{equation*}\n\n\nDot both sides by $\\mathbf{T}$, then by $\\mathbf{B}$, and use $\\mathbf{T N}=0, \\kappa=\\mathbf{N T}^{\\prime}$, and $\\tau=-\\mathbf{N B}^{\\prime}$ :\n\n$$\n\\begin{aligned}\n& \\mathbf{T} \\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}^{2}+\\mu \\mathbf{T B}=\\lambda \\quad \\text { or } \\quad \\lambda=-\\mathbf{T}^{\\prime} \\mathbf{N}=-\\kappa \\\\\n& \\mathbf{B N}^{\\prime}=\\tau=\\lambda \\mathbf{B T}+\\mu \\mathbf{B}^{2}=\\mu\n\\end{aligned}\n$$\n\nSubstitution for $\\lambda$ and $\\mu$ in (1) then yields the desired results.\n\n(b) From $\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}$ and part (a),\n\n$$\n\\begin{aligned}\n\\mathbf{B}^{\\prime} & =\\mathbf{T}^{\\prime} \\times \\mathbf{N}+\\mathbf{T} \\times \\mathbf{N}^{\\prime}=(\\kappa \\mathbf{N}) \\times \\mathbf{N}+\\mathbf{T} \\times(-\\boldsymbol{\\kappa} \\mathbf{T}+\\tau \\mathbf{B}) \\\\\n& =0+0+\\tau(\\mathbf{T} \\times \\mathbf{B})=\\tau(-\\mathbf{N})=-\\tau \\mathbf{N}\n\\end{aligned}\n$$\n\n10.9 Prove that if a curve has $\\kappa^{\\prime}=0$ at some point, then $\\mathbf{N}^{\\prime \\prime}$ is orthogonal to $\\mathbf{T}$ at that point.\n\nFrom $\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B}$, it follows that $\\mathbf{N}^{\\prime \\prime}=-\\kappa^{\\prime} \\mathbf{T}-\\kappa \\mathbf{T}^{\\prime}+\\tau^{\\prime} \\mathbf{B}+\\tau \\mathbf{B}^{\\prime}$. But $\\kappa^{\\prime}=0$; and from the Serret-Frenet formulas for $\\mathbf{T}^{\\prime}$ and $\\mathbf{B}^{\\prime}$ we obtain\n\n$$\n\\mathbf{N}^{\\prime \\prime}=-\\kappa(\\kappa \\mathbf{N})+\\tau^{\\prime} \\mathbf{B}+\\tau(-\\tau \\mathbf{N})=\\left(-\\kappa^{2}-\\tau^{2}\\right) \\mathbf{N}+\\tau^{\\prime} \\mathbf{B}\n$$\n\nAs $\\mathbf{N}^{\\prime \\prime}$ is in the plane of $\\mathbf{N}$ and $\\mathbf{B}$, it is orthogonal to $\\mathbf{T}$.\n\n\\section*{SURFACES IN EUCLIDEAN SPACE}\n10.10 Show that a surface of revolution is regular and exhibit the unit surface normal.\n\nThe Gaussian form of a surface of revolution about the $z$-axis (Fig. 10-11) is\n\n$$\n\\mathbf{r}=\\left(f\\left(x^{1}\\right) \\cos x^{2}, f\\left(x^{1}\\right) \\sin x^{2}, g\\left(x^{1}\\right)\\right) \\quad\\left(f\\left(x^{1}\\right)>0\\right)\n$$\n\nso\n\n$$\n\\mathbf{r}_{1}=\\left(f^{\\prime}\\left(x^{1}\\right) \\cos x^{2}, f^{\\prime}\\left(x^{1}\\right) \\sin x^{2}, g^{\\prime}\\left(x^{1}\\right)\\right) \\quad \\mathbf{r}_{2}=\\left(-f\\left(x^{1}\\right) \\sin x^{2}, f\\left(x^{1}\\right) \\cos x^{2}, 0\\right)\n$$\n\nand $\\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left(-f g^{\\prime} \\cos x^{2},-f g^{\\prime} \\sin x^{2}, f f^{\\prime}\\left(\\cos ^{2} x^{2}+\\sin ^{2} x^{2}\\right)\\right)$, with norm\n\n$$\nE=\\sqrt{f^{2} g^{\\prime 2} \\cos ^{2} x^{2}+f^{2} g^{\\prime 2} \\sin ^{2} x^{2}+f^{2} f^{\\prime 2}}=f \\sqrt{f^{\\prime 2}+g^{\\prime 2}}\n$$\n\nNow $f=f\\left(x^{1}\\right) \\neq 0$; further, the generating curve is regular, which means that, with $t=x^{1}$, the tangent vector of that curve,\n\n$$\n\\left(\\frac{d x}{d t}, 0, \\frac{d z}{d t}\\right)=\\left(f^{\\prime}, 0, g^{\\prime}\\right)\n$$\n\nis non-null and $f^{\\prime 2}+g^{\\prime 2} \\neq 0$. Therefore, $E \\neq 0$ and the surface is regular.\n\nThe unit surface normal is\n\n$$\n\\mathbf{n}=\\frac{1}{E}\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right)=\\left(-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\cos x^{2},-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\sin x^{2}, \\frac{f^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153(1)}\n\\end{center}\n\nFig. 10-11\n\n10.11 Identify the $x^{1}$ - and $x^{2}$-curves for the right helicoid (Example 10.4) and describe the behavior of the unit surface normal along an $x^{1}$-curve.\n\nThe $x^{1}$-curves $\\left(x^{2}=\\right.$ const.) are given by\n\n$$\n\\mathbf{r}=\\left(0,0, a x^{2}\\right)+x^{1}\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad\\left(x^{1} \\geqq 0\\right)\n$$\n\nthus, they are rays parallel to the $x y$-plane. The $x^{2}$-curves $\\left(x^{1}=\\right.$ const.) are given by\n\n$$\n\\sqrt{x^{2}+y^{2}}=x^{1} \\quad z=a x^{2}\n$$\n\ni.e., circular helices of radii $x^{1}$.\n\nWe have:\n\n$$\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, a\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153}\n\\end{center}\n\nFig. 10-12\n\n$$\n\\begin{aligned}\n& \\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\cos x^{2} & \\sin x^{2} & 0 \\\\\n-x^{1} \\sin x^{2} & x^{1} \\cos x^{2} & a\n\\end{array}\\right|=\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n& \\mathbf{n}=\\frac{\\mathbf{r}_{1} \\times \\mathbf{r}_{2}}{\\left\\|\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right\\|}=\\left(\\frac{a \\sin x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{-a \\cos x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{x^{1}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}\\right) \\\\\n&=(\\cos \\omega) \\mathbf{u}+(\\sin \\omega) \\mathbf{v}\n\\end{aligned}\n$$\n\nand\n\nwhere $\\omega \\equiv \\tan ^{-1}\\left(x^{1} / a\\right), \\mathbf{u} \\equiv\\left(\\sin x^{2},-\\cos x^{2}, 0\\right), \\mathbf{v} \\equiv(0,0,1)$. On an $\\mathbf{x}^{1}$-ray, $\\mathbf{u}$ and $\\mathbf{v}$ are fixed unit vectors, while $\\omega$ increases from 0 to a $\\pi / 2$ as $x^{1}$ increases from 0 to $\\infty$. Thus, $\\mathbf{n}$ traces out a quarter-circle as the ray is described (see Fig. 10-12).\n\n10.12 Find the FFF for any surface of revolution, and specialize to a right circular cone.\n\nWith $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ as obtained in Problem 10.10,\n\n$$\n\\begin{aligned}\n& g_{11}=\\mathbf{r}_{1} \\mathbf{r}_{1}=\\left(f^{\\prime} \\cos x^{2}\\right)^{2}+\\left(f^{\\prime} \\sin x^{2}\\right)^{2}+\\left(g^{\\prime}\\right)^{2}=f^{\\prime 2}+g^{\\prime 2} \\\\\n& g_{12}=g_{21}=\\mathbf{r}_{1} \\mathbf{r}_{2}=-f^{\\prime} f \\cos x^{2} \\sin x^{2}+f^{\\prime} f \\sin x^{2} \\cos x^{2}+\\left(g^{\\prime}\\right)(0)=0 \\\\\n& g_{22}=\\mathbf{r}_{2} \\mathbf{r}_{2}=\\left(-f \\sin x^{2}\\right)^{2}+\\left(f \\cos x^{2}\\right)^{2}+0^{2}=f^{2}\n\\end{aligned}\n$$\n\nand\n\n\n\\begin{equation*}\n\\mathrm{I}=\\left(f^{\\prime 2}+g^{\\prime 2}\\right)\\left(d x^{1}\\right)^{2}+f^{2}\\left(d x^{2}\\right)^{2} \\tag{1}\n\\end{equation*}\n\n\nFor a right circular cone (Fig. 10-13), $f=x^{1}$ and $g=a x^{1}$; hence,\n\n\n\\begin{equation*}\n\\mathrm{I}=\\left(1+a^{2}\\right)\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\tag{2}\n\\end{equation*}\n\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154}\n\\end{center}\n\nFig. 10-13\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154(1)}\n\\end{center}\n\nFig. 10-14\n\n10.13 Find the FFF for the catenoid (Fig. 10-14) and compute the length of the curve given by $x^{1}=t, x^{2}=t \\quad(0 \\leqq t \\leqq \\ln (1+\\sqrt{2}))$.\n\nHere $f\\left(x^{1}\\right)=a \\cosh x^{1}, g\\left(x^{1}\\right)=a x^{1}$, and (1) of Problem 10.12 gives, along the curve,\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}=2 a^{2} \\cosh ^{2} t\n$$\n\nand\n\n$$\nL=a \\sqrt{2} \\int_{0}^{\\ln (1+\\sqrt{2})} \\cosh t d t=a \\sqrt{2} \\sinh [\\ln (1+\\sqrt{2})]=a \\sqrt{2}\n$$\n\n10.14 Let $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ be two curves on the right circular cone $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, 2 x^{1}\\right)$ whose pre-images in the parameter plane are\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=3-t \\\\\nx^{2}=t / 2\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=\\sigma>0 \\\\\nx^{2}=\\sigma^{2}\n\\end{array}\\right.\\right.\n$$\n\nAt the point of intersection, find the angle between $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$, and show that orthogonality in the $x^{1} x^{2}$-plane does not carry over to the cone.\n\nThe intersection point $P^{\\prime}$ of the two pre-image curves is determined by the simultaneous equations\n\n$$\n3-t=\\sigma \\quad \\text { and } \\quad \\frac{t}{2}=\\sigma^{2}\n$$\n\nwhich give $t=2, \\tau=1$, and $P^{\\prime}=(1,1)$. Thus, the two tangent vectors at $P^{\\prime}$ are:\n\n$$\n\\left(u^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d t}\\right)\\right|_{t=2}=\\left(-1, \\frac{1}{2}\\right) \\quad\\left(v^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d \\sigma}\\right)\\right|_{\\sigma=1}=(1,2)\n$$\n\nConsidered in the Euclidean sense, $\\left(u^{i}\\right)$ and $\\left(v^{i}\\right)$ are orthogonal.\n\nTo express the angle between tangents at the image of $P^{\\prime}$, we adopt the metric (2) of Problem 10.12 (with $a=2$ ) and apply (10.18) for $x^{1}=1, x^{2}=1$ :\n\n$$\n\\cos \\theta=\\frac{\\left(1+2^{2}\\right)(-1)(1)+(1)^{2}\\left(\\frac{1}{2}\\right)(2)}{D}=\\frac{-4}{D} \\neq 0\n$$\n\nTherefore, the curves are not orthogonal at the image of $P^{\\prime}$.\n\n10.15 Prove Theorem 10.3 and verify Corollary 10.4 geometrically for the right helicoid (Example 10.4) and for any surface of revolution (Problem 10.12).\n\nThe proof consists merely in taking $\\left(u^{i}\\right)=(1,0)$ and $\\left(v^{i}\\right)=(0,1)$ in (10.18). (Compare Problem 5.31.)\n\nAs is clear from Problem 10.11, the right helicoid is a ruled surface, generated by a half-line (an $x^{1}$-curve), pivoted on the $z$-axis, that rotates parallel to the $x y$-plane while the pivot point travels up the $z$-axis. A given point $P$ of the generator thus describes a helical $x^{2}$-curve (Fig. 10-8), which is necessarily everywhere orthogonal to the generator (i.e., to the $x^{1}$-curves). As for surfaces of revolution, it is clear that the parameter lines that match the revolved planar curve ( $x^{1}$-curves, or meridians) and the circles traced by individual points of the planar curve ( $x^{2}$-curves, or parallels of latitude) are mutually orthogonal. By previous computations, $g_{12}=0$ for both the helicoid and for the general surface of revolution.\n\n10.16 Show that under a change of coordinates $x^{1}=x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}=x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$ in the plane, the surface metric $\\left(g_{i j}\\right)$ transforms as a second-order covariant tensor.\n\nWe have by substitution $\\mathbf{r}\\left(x^{1}, x^{2}\\right)=\\mathbf{r}\\left(x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)\\right) \\equiv \\overline{\\mathbf{r}}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$, the latter being the \"new\" parameterization for $\\mathscr{S}$. To compute the metric under this parameterization, write (by the chain rule for partial derivatives and the bilinearity of the inner product)\n\n$$\n\\begin{aligned}\n\\bar{g}_{i j} & =\\overline{\\mathbf{r}}_{i} \\overline{\\mathbf{r}}_{j} \\equiv \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{j}}=\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{p}} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{q}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right)=\\left(\\mathbf{r}_{p} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\mathbf{r}_{q} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right) \\\\\n& =\\mathbf{r}_{p} \\mathbf{r}_{q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}} \\equiv g_{p q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\n\\end{aligned}\n$$\n\nwhich is the correct formula for tensor character.\n\n\\section*{GEODESICS}\n10.17 (a) Find the Christoffel symbols of the second kind for the sphere of radius $a$. (b) Verify that the great circles passing through the north and south poles (i.e., the $x^{1}$-curves) are geodesics.\\\\\n(a) The FFF for the sphere of radius $a$ may be calculated from Problem 10.12:\n\n$$\ng_{11}=a^{2} \\quad g_{12}=0=g_{21} \\quad g_{22}=a^{2} \\sin ^{2} x^{1}\n$$\n\nThe formulas from Problem 6.4 can be used, since $\\left(g_{i j}\\right)$ is diagonal; the nonzero Christoffel symbols are found to be:\n\n$$\n\\Gamma_{22}^{1}=-\\sin x^{1} \\cos x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\cot x^{1}\n$$\n\n(b) We want to show that the family of curves $x^{1}=t, x^{2}=d=$ const. are integral curves of the differential system (7.11)-(7.12), which may be conveniently written as\n\n$$\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2} \\frac{d x^{i}}{d t}\\left[\\frac{d}{d t} \\ln \\left(g_{j k} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}\\right)\\right]\n$$\n\nor, for the given metric,\n\n$$\n\\begin{array}{ll}\ni=1 & \\frac{d^{2} x^{1}}{d t^{2}}-\\left(\\sin x^{1} \\cos x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\frac{1}{2} \\frac{d x^{1}}{d t}\\left[\\frac{d}{d t} \\ln \\left(a^{2}\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(a^{2} \\sin ^{2} x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right)\\right] \\\\\ni=2 & \\frac{d^{2} x^{2}}{d t^{2}}+\\left(2 \\cot x^{1}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=\\frac{1}{2} \\frac{d x^{2}}{d t}\\left[\\frac{d}{d t} \\ln \\mathrm{I}\\right.\n\\end{array}\n$$\n\nSince $d x^{1} / d t=1$ and $d x^{2} / d t=0$, both equations reduce to $0=0$, and the verification is complete.\n\n10.18 Prove Theorem 10.6: A curve on a regular surface is a geodesic if and only if, by proper choice of the principal normal, $\\mathbf{N}=\\mathbf{n}$.\n\nLet any curve on the surface be given by $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(s), x^{2}(s)\\right)$, where $s=\\operatorname{arc}$ length, Then,\n\n$$\n\\mathbf{T}=\\mathbf{r}_{i} \\frac{d x^{i}}{d s}\n$$\n\nand the first formula (10.9) gives\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s}\\left(\\frac{\\partial \\mathbf{r}_{i}}{\\partial x^{j}} \\frac{d x^{j}}{d s}\\right) \\equiv \\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\tag{1}\n\\end{equation*}\n\n\nDot both sides of (1) by the vector $\\mathbf{r}_{k}$ and use the resuit of Problem 10.48:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i} \\mathbf{r}_{k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\mathbf{r}_{k} \\equiv \\frac{d^{2} x^{i}}{d s^{2}} g_{i k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\Gamma_{i j k} \\tag{2}\n\\end{equation*}\n\n\nMultiply both sides of (2) by $g^{k l}$ and sum on $k$ :\n\n\n\\begin{equation*}\ng^{k l} \\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\delta_{i}^{l}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} g^{k l} \\Gamma_{i j k}=\\frac{d^{2} x^{l}}{d s^{2}}+\\Gamma_{i j}^{l} \\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\tag{3}\n\\end{equation*}\n\n\nNow if $\\mathscr{C}$ is a geodesic, the right side of (3) vanishes, and this implies that $\\kappa \\mathbf{r}_{k} \\mathbf{N}=0$ for $k=1,2$. If $\\kappa \\neq 0$, then $\\mathbf{r}_{1} \\mathbf{N}=0=\\mathbf{r}_{2} \\mathbf{N}$; so that $\\mathbf{N}$ is orthogonal to both $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ (thus to the tangent plane). Therefore, but for orientation, $\\mathbf{N}=\\mathbf{n}$. If $\\kappa=0$ at some point $P$ and there is a sequence of points along the curve approaching $P$ for which $\\kappa \\neq 0$, then, by continuity, $\\mathbf{N}=\\mathbf{n}$. Otherwise, $\\kappa=0$ on an interval and the curve is a straight line on that interval, in which case its principal normal $\\mathbf{N}$ can be chosen to agree with $\\mathbf{n}$. Conversely, if the curve has the property that $\\mathbf{N}=\\mathbf{n}$ at all points, then $\\mathbf{r}_{k} \\mathbf{N}=\\mathbf{r}_{k} \\mathbf{n}=0$ and the left side of (3) vanishes, showing that the pre-image of $\\mathscr{C}$ satisfies the differential equations for a geodesic.\n\n10.19 Apply Theorem 10.6 to the plane sections of a torus.\n\nIn Fig. 10-15 is shown a torus and various examples of plane sections. In (a), an elliptical-shaped vertical section, the section cannot be a geodesic because the surface normal at $P$ does not lie in the plane of the curve (which contains the curve normal). In (b), a horizontal section that is a circle, again the surface normals do not lie in the plane, and this section is not a geodesic. The circles shown in $(c)$ and $(d)$ are geodesics, since the surface normal will coincide with a correctly chosen principal normal of the curve.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157}\n\\end{center}\n\nFig. 10-15\n\n10.20 Demonstrate that the intrinsic curvature $\\tilde{\\kappa}$ of a curve on a surface can be different from its curvature $\\kappa$ as a curve in $\\mathbf{E}^{3}$.\n\nOne example is a circle on a sphere of radius $a$. If the circle also has radius $a$, it is a great circle and, hence, a geodesic with zero intrinsic curvature. But its curvature as a (planar) curve in $\\mathbf{E}^{3}$ is $1 / a$. Another example is the circular helix: its curvature is nonzero as a curve in $\\mathbf{E}^{3}$, but as a geodesic on a circular cylinder its intrinsic curvature is zero.\n\n\\section*{SECOND FUNDAMENTAL FORM}\n10.21 (a) Find the SFF for the right circular cone of Problem 10.12. (b) At the point $P(1,0, a)$, calculate the curvature of the normal section having the direction $\\mathbf{j}$ at $P$ (see Fig. 10-16).\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157(1)}\n\\end{center}\n\nFig. 10-16\\\\\n(a) From $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, a x^{1}\\right) \\quad\\left(x^{1}>0\\right)$, we obtain:\n\n$$\n\\begin{array}{ccc}\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, a\\right) & \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, 0\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) & \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) & \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{array}\n$$\n\nand by Problem 10.10, $\\mathbf{n}=\\left(a^{2}+1\\right)^{-1 / 2}\\left(-a \\cos x^{2},-a \\sin x^{2}, 1\\right)$. The coefficients in II are thus $f_{12} \\equiv \\mathbf{n r}_{11}=0, f_{12}=f_{21} \\equiv \\mathbf{n} \\mathbf{r}_{12}=0, f_{22} \\equiv \\mathbf{n r}_{22}=\\left(a^{2}+1\\right)^{-1 / 2} a x^{1}$.\n\n(b) The direction $\\mathbf{j}$ at $P$ corresponds to the direction $\\left(u^{1}, u^{2}\\right)$ at $P^{\\prime}=(1,0)$ in the parameter plane, where\n\n$$\n\\begin{aligned}\n\\mathbf{j} & =u^{1} \\mathbf{r}_{1}(P)+u^{2} \\mathbf{r}_{2}(P) \\\\\n(0,1,0) & =u^{1}(1,0, a)+u^{2}(0,1,0) \\\\\n(0,1,0) & =\\left(u^{1}, u^{2}, a u^{1}\\right)\n\\end{aligned}\n$$\n\nThus, $u^{1}=0$ and $u^{2}=1$. Appropriating I from Problem 10.12, we have\n\n$$\n\\kappa_{g f}=\\frac{\\mathrm{II}\\left(u^{1}, u^{2}\\right)}{\\mathrm{I}\\left(u^{1}, u^{2}\\right)}=\\frac{f_{22}(P)\\left(u^{2}\\right)^{2}}{\\left(a^{2}+1\\right)\\left(u^{1}\\right)^{2}+(1)^{2}\\left(u^{2}\\right)^{2}}=f_{22}(P)=\\frac{a}{\\sqrt{a^{2}+1}}\n$$\n\n10.22 Develop geometrically a notion of \"parallel transport\" of a vector along a curve $\\mathscr{C}$ on a regular surface $\\mathscr{S}$.\n\nImagine $\\mathscr{C}$, and with it $\\mathscr{S}$, as being rolled without slipping onto a fixed plane $\\mathscr{F}$, in such fashion that the point of contact is aways on $\\mathscr{C}$ and the tangent plane to $\\mathscr{S}$ at the point of contacts always coincides with $\\mathscr{F}$. This maps $\\mathscr{C}$ to a (planar) curve $\\mathscr{C}^{*}$ in $\\mathscr{F}$ that has the same arc-length parameter and the same tangent vector. Then, any vector in $\\mathscr{F}$ that is attached to the point of contact and that remains parallel to itself (in the ordinary Euclidean sense) as the contact point describes $\\mathscr{C}^{*}$ may-under the inverse mapping $\\mathscr{C}^{*} \\rightarrow \\mathscr{C}$-be considered as undergoing parallel transport along $\\mathscr{C}$. In general, parallel transport of a given vector around a closed curve on a surface does not reproduce the initial vector.\n\n10.23 Prove (10.23).\n\nStart with the formula for the unit tangent vector of any curve $\\mathscr{C}$ on $\\mathscr{S}$ :\n\n$$\n\\mathbf{T}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{k l} u^{k} u^{i}}} \\quad\\left(u^{i} \\equiv \\frac{d x^{i}}{d t}\\right)\n$$\n\nThen\n\n\n\\begin{equation*}\n\\dot{\\mathbf{T}}=\\frac{d}{d t}\\left(\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}}\\right) \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\dot{\\mathbf{r}}_{i}=Q^{i} \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i j} u^{j} \\tag{1}\n\\end{equation*}\n\n\nwhere $Q^{i}$ is an abbreviation for the scalar coefficient of $\\mathbf{r}_{i}$. Now the Frenet formula gives $\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=$ $\\dot{\\mathbf{T}} / \\sqrt{g_{k l} u^{k} u^{\\prime}}$; together with (1), this yields:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\frac{Q_{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i}+\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\tag{2}\n\\end{equation*}\n\n\nDot both sides of (2) with $\\mathbf{n}$ (the surface normal) and use the fact that $\\mathbf{r}_{i} \\mathbf{n}=0$ for each $i$ :\n\n\n\\begin{equation*}\n\\kappa \\mathbf{n} \\mathbf{N}=\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\mathbf{n} \\equiv \\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} f_{i j} \\tag{3}\n\\end{equation*}\n\n\nIf $\\mathscr{C}$ is a normal section $\\mathscr{C}_{\\mathscr{F}}$ at $P$, and $\\kappa, \\mathbf{N}$, and the right side of (3) are all evaluated at $P$, then $\\kappa=\\kappa_{\\mathscr{F}}$, $\\mathbf{n N}=\\mathbf{n}^{2}=1$, and (3) becomes the desired expression\n\n$$\n\\kappa_{\\mathscr{F}}=\\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}\n$$\n\n\\section*{CURVATURE OF SURFACES}\n10.24 Show that the maximum and minimum values of the function\n\n$$\nF(\\mathbf{u})=\\frac{a_{i j} u^{i} u^{j}}{b_{k l} u^{k} u^{l}}=\\frac{\\mathbf{u}^{T} A \\mathbf{u}}{\\mathbf{u}^{T} B \\mathbf{u}}\n$$\n\nwhere $A=\\left[a_{i j}\\right]_{22}, B=\\left[b_{i j}\\right]_{22}, \\mathbf{u}=\\left(u^{1}, u^{2}\\right) \\neq(0,0)$, with $B$ positive definite, are the two roots of the quadratic equation in $\\lambda$\n\n\\[\n\\operatorname{det}(A-\\lambda B) \\equiv\\left|\\begin{array}{ll}\na_{11}-\\lambda b_{11} & a_{12}-\\lambda b_{12}  \\tag{1}\\\\\na_{21}-\\lambda b_{21} & a_{22}-\\lambda b_{22}\n\\end{array}\\right|=0\n\\]\n\n(hence, eigenvalues of $B^{-1} A$ ), and that the extreme values of $F$ occur for vectors $\\mathbf{u}$ satisfying $(A-x B) \\mathbf{u}=\\mathbf{0}$, where $x$ takes on the two eigenvalues of $B^{-1} A$ (hence, eigenvectors of $B^{-1} A$ ).\n\nWe may assume without loss of generality that $A$ and $B$ are symmetric. Let $\\mathscr{G}$ be any simple closed curve in the $u^{1} u^{2}$-plane having the origin in its interior. The Weierstrass theorem guarantees that $F(\\mathbf{u})$ asumes a largest value on $\\mathscr{G}$; say, $F(\\mathbf{w})=M$. Because $F$ is constant on rays emanating from the origin $(F(\\lambda \\mathbf{u})=F(\\mathbf{u})$ for any $\\lambda \\neq 0)$, the absolute maximum on $\\mathscr{G}$ is both an absolute and a relative maximum in the $u^{1} u^{2}$-plane; hence, the gradient of $F$ must vanish at $\\mathbf{w}$. We have:\n\n$$\n\\begin{gathered}\n\\frac{\\partial F(\\mathbf{u})}{\\partial u^{P}}=\\frac{\\left(b_{k l} u^{k} u^{l}\\right)\\left(2 a_{p j} u^{j}\\right)-\\left(a_{i j} u^{i} u^{j}\\right)\\left(2 b_{p l} u^{l}\\right)}{\\left(b_{k l} u^{k} u^{l}\\right)^{2}}=\\frac{2}{b_{k l} u^{k} u^{l}}\\left[a_{p j} u^{j}-F(\\mathbf{u})\\left(b_{p l} u^{l}\\right)\\right] \\\\\n\\nabla F(\\mathbf{u})=\\frac{2}{\\mathbf{u}^{T} B \\mathbf{u}}[A \\mathbf{u}-F(\\mathbf{u}) B \\mathbf{u}]\n\\end{gathered}\n$$\n\nor\n\nTherefore, $A \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0}$, which shows (i) that $M$ is an eigenvalue of $B^{-1} A$ and thus is a root of the characteristic equation (1); (ii) that $\\mathbf{w}$ is an eigenvector belonging to $M$.\n\nA like consideration of the minimum value, $m$, of $F$ on $\\mathscr{G}$ leads to the other eigenvalue and associated eigenvector.\n\n10.25 Prove that the extreme normal curvatures $\\kappa_{1}, \\kappa_{2}$ are the two roots of the quadratic equation $(10.25)$.\n\nIn Problem 10.24, take $a_{i j}=f_{i j}$ and $b_{k l}=g_{k l}$; expand (1) to obtain (10.25).\n\n10.26 Prove that the two normal section curves through $P$ on $\\mathscr{S}$ giving rise to $\\max \\kappa_{\\mathscr{F}}=\\kappa_{1}$ and $\\min \\kappa_{\\mathscr{F}}=\\kappa_{2}$ are orthogonal when $\\kappa_{1} \\neq \\kappa_{2}$ (that is, when $P$ is not an umbilical point on $\\mathscr{S}$ ).\n\nLet us prove the general result, in the notation of Problem 10.24. We have:\n\n$$\nA \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0} \\quad A \\mathbf{v}-m B \\mathbf{v}=\\mathbf{0}\n$$\n\nWith the inner product of column vectors defined as $\\mathbf{p} \\cdot \\mathbf{q} \\equiv \\mathbf{p}^{T} B \\mathbf{q}$, multiply the first equation by $\\mathbf{v}^{T}$ and the second by $\\mathbf{w}^{T}$, and subtract:\n\n$$\n(m-M) \\mathbf{v} \\cdot \\mathbf{w}=0\n$$\n\nHence, if $m \\neq M, \\mathbf{v}$ and $\\mathbf{w}$ are orthogonal.\n\n10.27 Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for the right helicoid. Show that as $x^{1} \\rightarrow \\infty, \\mathrm{K}$ tends to zero (the surface becomes \"flatter\" as the distance from its axis increases without bound).\n\nFrom Problem 10.11 and Example 10.4,\n\n$$\n\\begin{gathered}\n\\mathbf{n}=\\frac{1}{\\sqrt{\\left(x^{1}\\right)^{2}+a^{2}}}\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) \\quad \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) \\quad \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{gathered}\n$$\n\nso that\n\nand\n\n$$\n\\begin{gathered}\nf_{11}=\\mathbf{n r}_{11}=0 \\quad f_{12}=f_{21}=-a / \\sqrt{\\left(x^{1}\\right)^{2}+a^{2}} \\quad f_{22}=0 \\\\\n\\mathrm{~K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0-a^{2} /\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}=-\\frac{a^{2}}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]^{2}} \\rightarrow 0 \\quad \\text { as } x^{1} \\rightarrow \\infty \\\\\n\\mathrm{H}=\\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0+0-2(0)}{g}=0\n\\end{gathered}\n$$\n\n\\section*{STRUCTURE FORMULAS; ISOMETRIES}\n10.28 Complete the proof of $(10.27 a)$.\n\nThe relations $\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}$ and $\\mathbf{n}_{j} \\mathbf{r}_{i}=-f_{i j}$ imply\n\n$$\n-f_{i j}=u_{j}^{k} g_{k i}\n$$\n\nMultiply both sides by $g^{i s}$ and sum over $i$, obtaining $u_{j}^{s}=-g^{l s} f_{l j}$. Hence,\n\n$$\n\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}=-g^{l k} f_{l i} \\mathbf{r}_{k}=-g^{l k} f_{i l} \\mathbf{r}_{k}\n$$\n\n10.29 Prove (10.28).\n\nLet the equation $\\mathbf{r}_{i j}=u_{i j}^{1} \\mathbf{r}_{1}+u_{i j}^{2} \\mathbf{r}_{2}+u_{i j}^{3} \\mathbf{n}$ be rewritten as $\\mathbf{r}_{i j}=u_{i j}^{s} \\mathbf{r}_{s}+u_{i j}^{3} \\mathbf{n}$. Dot with $\\mathbf{n}$, obtaining $u_{i j}^{3}=f_{i j}$; therefore,\n\n\n\\begin{equation*}\n\\mathbf{r}_{i j}=u_{i j}^{s} \\mathbf{r}_{s}+f_{i j} \\mathbf{n} \\tag{1}\n\\end{equation*}\n\n\nDot (1) with $\\mathbf{r}_{k}$ and use Problem 10.48:\n\n$$\n\\mathbf{r}_{i j} \\mathbf{r}_{k}=u_{i j}^{s} \\mathbf{r}_{s} \\mathbf{r}_{k}+0 \\quad \\text { or } \\quad \\Gamma_{i j k}=u_{i j}^{s} g_{s k}\n$$\n\nSolve for $u_{i j}^{s}:$\n\n$$\ng^{k t} \\Gamma_{i j k}=u_{i j}^{s} g^{k t} g_{s k} \\quad \\text { or } \\quad \\Gamma_{i j}^{t}=u_{i j}^{s} \\delta_{s}^{t}=u_{i j}^{t}\n$$\n\nSubstitute back into (1):\n\n$$\n\\mathbf{r}_{i j}=\\Gamma_{i j}^{t} \\mathbf{r}_{t}+f_{i j} \\mathbf{n}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n10.30 (a) Describe geometrically the curve whose parametric vector equation is\n\n$$\n\\mathbf{r}=\\left(\\cos t, \\sin t,(1-t)^{-1}\\right) \\quad(0 \\leqq t<1)\n$$\n\nWhat happens as $t \\rightarrow 1$ ? (b) Use a programmable calculator and Simpson's rule to find the arc length for $0 \\leqq t \\leqq 1 / 2$ accurate to 6 places.\n\n10.31 Find the exact length of the space curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(-1 \\leqq t \\leqq 1)$.\n\n10.32 (a) Using the arc-length parameterization of the right circular helix,\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c \\equiv \\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nfind the coordinate equations of the tangent line to the helix at any point $P \\equiv \\mathbf{r}(s)$. (b) Show that the tangent line intersects the $x y$-plane at a point $Q \\equiv \\mathbf{r}^{*}(s)$ such that $P Q=s$. (c) By thinking of a string wound along the helix, interpret the result of $(b)$.\n\n10.33 Show that for the curve $y=x^{5}$ in the $x y$-plane, parameterized as $\\mathbf{r}=\\left(t, t^{5}, 0\\right)$, the vector $\\mathbf{T}^{\\prime} /\\left\\|\\mathbf{T}^{\\prime}\\right\\|$ has an essential point of discontinuity at $t=0$.\n\n10.34 Find the curvature and the torsion of the curve $\\mathbf{r}=\\left(t, t^{5}+a, t^{5}-a\\right)$.\n\n10.35 Prove that a curve is planar if and only if its torsion vanishes.\n\n10.36 Prove that a planar curve with constant nonzero curvature $\\kappa$ is a circle. [Hint: $\\mathbf{T}=(\\cos \\theta, \\sin \\theta, 0)$ and $\\mathbf{N}=(-\\sin \\theta, \\cos \\theta, 0)$ imply $\\kappa=\\theta^{\\prime}$ or $\\theta=\\kappa s+a$; show that the radius is $\\left.1 / \\kappa.\\right]$\n\n10.37 Verify the Serret-Frenet formulas for the circular helix.\n\n10.38 Show that if included in the range of the map $\\mathbf{r}\\left(x^{1}, x^{2}\\right)$, the vertex of the right circular cone is a singular point.\n\n10.39 Calculate the unit normal for the catenoid as parameterized in Fig. 10-14 and show that the surface is regular.\n\n10.40 Find the length of the curve on the right helicoid (Example 10.4) given by $x^{1}=t^{2}, x^{2}=\\ln t$, with $1 \\leqq t \\leqq 2$, in the special case when the parameter $a=1$.\n\n10.41 Find the two possible directions for a curve $\\mathscr{C}$ ' in the parameter plane whose image on the paraboloid of Fig. 10-17 meets the circle $x^{2}+y^{2}=4, z=4$ at $P(0,2,4)$ at an angle of $\\pi / 3$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-161}\n\\end{center}\n\nFig. 10-17\n\n10.42 Use Theorem 10.6 to show that an elliptical helix is not in general a geodesic on an elliptical cylinder.\n\n10.43 Calculate the Christoffel symbols of the second kind for the right helicoid (Example 10.4). Show that circular helices on the surface are geodesics.\n\n10.44 Exhibit the SFF for the general surface of revolution (Problem 10.10).\n\n10.45 Establish the formulas below for any surface of revolution, with $G \\equiv g^{\\prime} / f^{\\prime}$ (see Problems 10.12 and $10.44)$ :\n\n$$\n\\mathrm{K}=\\frac{G G^{\\prime}}{f f^{\\prime}\\left(1+G^{2}\\right)^{2}} \\quad \\text { and } \\quad \\mathrm{H}=\\frac{f G^{\\prime}+g^{\\prime}\\left(1+G^{2}\\right)}{f\\left|f^{\\prime}\\right|\\left(1+G^{2}\\right)^{3 / 2}}\n$$\n\nUse these formulas to verify that a sphere of radius $a$ has Gaussian curvature $1 / a^{2}$ and mean curvature $-2 / a$.\n\n10.46 (a) Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for two different parameterizations of the paraboloid $z=a\\left(x^{2}+y^{2}\\right)$ : (i) as the surface of revolution for which $f=x^{1}, g=a\\left(x^{1}\\right)^{2}$; (ii) as the surface $\\mathbf{r}=\\left(\\bar{x}^{1}, \\bar{x}^{2}, a\\left(\\bar{x}^{1}\\right)^{2}+a\\left(\\bar{x}^{2}\\right)^{2}\\right) .(b)$ Interpret the results of $(a)$.\n\n10.47 Infer from Problem 10.45 that $\\mathrm{H} \\equiv 0$ for any catenoid. [A surface with $\\mathrm{H}=0$ at all points is called a minimal surface. Among minimal surfaces are those that solve \"soap-bubble\" problems, which require a minimum in surface area.]\n\n10.48 Prove that $\\Gamma_{i j k}=\\mathbf{r}_{i j} \\mathbf{r}_{k}$. Hint: $\\left.\\quad\\left(\\mathbf{r}_{i} \\mathbf{r}_{j}\\right)_{k}=\\mathbf{r}_{i k} \\mathbf{r}_{j}+\\mathbf{r}_{i} \\mathbf{r}_{j k}.\\right]$\n\n10.49 Surfaces for which the Gaussian curvature is a negative constant are very rare. One such surface can be constructed as follows. (a) A tractrix is the involute of a catenary (see Problem 10.32(c)). Write the vector equation for the involute of the catenary $\\mathbf{r}=\\left(a \\cosh x^{1}, 0, a x^{1}\\right)$ (see Fig. 10-14). (b) Using Problem 10.45, show that $\\mathrm{K}=-1 / a^{2}$ for the tractroid generated by revolving the tractrix of $(a)$ about the $z$-axis.\n\n10.50 Prove that the catenoid,\n\n$$\nI=\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2}\n$$\n\nand the right helicoid,\n\n$$\n\\mathrm{I}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left[\\left(\\bar{x}^{1}\\right)^{2}+a^{2}\\right]\\left(d \\bar{x}^{2}\\right)^{2}\n$$\n\nare locally isometric.\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 11}", "\\section*{Tensors in Classical Mechanics}\n\\subsection*{11.1 INTRODUCTION}\nClassical mechanics originated with the work of Galileo and was developed extensively by Newton (it is often called Newtonian mechanics). It deals with the motion of particles in a fixed frame of reference (rectangular coordinate system). The basic premise of Newtonian mechanics is the concept of absolute time measurement between two reference frames at constant velocity relative to each other (called Galilean frames). Within those frames, other coordinate systems may be used so long as the metric remains Euclidean. This means that some of the theory of tensors can be brought to bear on this study.\n\n\\subsection*{11.2 PARTICLE KINEMATICS IN RECTANGULAR COORDINATES}\nLet $P$ be a particle whose path in $\\mathbf{E}^{3}$ is given by\n\n\n\\begin{equation*}\n\\mathscr{C}: \\mathbf{x}=\\left(x^{i}(t)\\right) \\tag{11.1}\n\\end{equation*}\n\n\nwhere $t$ represents time. The velocity vector of $P$ is defined as\n\n\n\\begin{equation*}\n\\mathbf{v} \\equiv \\frac{d \\mathbf{x}}{d t} \\equiv\\left(\\frac{d x^{1}}{d t}, \\frac{d x^{2}}{d t}, \\frac{d x^{3}}{d t}\\right) \\equiv\\left(\\dot{x}^{1}, \\dot{x}^{2}, \\dot{x}^{3}\\right) \\tag{11.2}\n\\end{equation*}\n\n\nand the (instantaneous) velocity or speed as the scalar\n\n\n\\begin{equation*}\nv \\equiv\\|\\mathbf{v}\\| \\equiv \\sqrt{\\left(\\dot{x}^{1}\\right)^{2}+\\left(\\dot{x}^{2}\\right)^{2}+\\left(\\dot{x}^{3}\\right)^{2}} \\tag{11.3}\n\\end{equation*}\n\n\nFurther, define the acceleration vector as\n\n\n\\begin{equation*}\n\\mathbf{a} \\equiv \\frac{d \\mathbf{v}}{d t} \\equiv\\left(\\frac{d^{2} x^{1}}{d t^{2}}, \\frac{d^{2} x^{2}}{d t^{2}}, \\frac{d^{2} x^{3}}{d t^{2}}\\right) \\equiv\\left(\\ddot{x}^{1}, \\ddot{x}^{2}, \\ddot{x}^{3}\\right) \\tag{11.4}\n\\end{equation*}\n\n\nand the acceleration as\n\n\n\\begin{equation*}\na=\\|\\mathbf{a}\\| \\equiv \\sqrt{\\left(\\ddot{x}^{1}\\right)^{2}+\\left(\\ddot{x}^{2}\\right)^{2}+\\left(\\ddot{x}^{3}\\right)^{2}} \\tag{11.5}\n\\end{equation*}\n\n\nIf $\\mathbf{v}=\\left(v^{i}\\right)$ and $\\mathbf{a}=\\left(a^{i}\\right)$, the preceding formulas have the component forms\n\n\n\\begin{equation*}\nv^{i}=\\frac{d x^{i}}{d t} \\quad v=\\sqrt{v^{i} v^{i}} \\quad a^{i}=\\frac{d^{2} x^{i}}{d t^{2}} \\quad a=\\sqrt{a^{i} a^{i}} \\tag{11.6}\n\\end{equation*}\n\n\nIn terms of the geometry of the curve $\\mathscr{C}$ we have $\\mathbf{v}=v \\mathbf{T}$, with $v=d s / d t$. Hence,\n\n$$\n\\mathbf{a}=\\frac{d}{d t}(v \\mathbf{T})=\\dot{v} \\mathbf{T}+v \\dot{\\mathbf{T}}=\\dot{v} \\mathbf{T}+v \\frac{d \\mathbf{T}}{d s} \\frac{d s}{d t}=\\dot{v} \\mathbf{T}+v^{2} \\mathbf{T}^{\\prime}\n$$\n\nBut $\\mathbf{T}^{\\prime}=\\kappa \\mathbf{N}$, and so\n\n\n\\begin{equation*}\n\\mathbf{a}=\\dot{v} \\mathbf{T}+\\kappa v^{2} \\mathbf{N} \\tag{11.7}\n\\end{equation*}\n\n\nVia the Pythagorean theorem, (11.7) implies\n\n\n\\begin{equation*}\na=\\sqrt{\\dot{v}^{2}+\\kappa^{2} v^{4}} \\tag{11.8}\n\\end{equation*}\n\n\nEXAMPLE 11.1 Formula (11.7) serves to define the tangential and normal accelerations of $P$ as\n\n$$\n\\dot{v}=\\frac{d^{2} s}{d t^{2}} \\quad \\text { and } \\quad \\kappa v^{2}=\\frac{v^{2}}{\\rho} \\quad(\\rho \\equiv \\text { radius of curvature })\n$$\n\nrespectively. For a particle with constant velocity, $a=\\left\\|\\kappa v^{2} \\mathbf{N}\\right\\|=|\\kappa| v^{2}$; i.e., the acceleration is proportional to the absolute curvature.\n\n\\subsection*{11.3 PARTICLE KINEMATICS IN CURVILINEAR COORDINATES}\nThere emerges the problem of expressing the preceding formulas in nonrectangular coordinate systems. This is not merely an academic consideration, for there are important situations in which the differential equations of motion can be solved only in polar or spherical coordinates (cf. Example 11.3), not to mention applications in relativistic mechanics.\n\nLet us start with the definitions of the velocity and acceleration vectors in a barred rectangular system:\n\n$$\n\\bar{v}^{i}=\\frac{d \\bar{x}^{i}}{d t} \\quad \\text { and } \\quad \\bar{a}^{i}=\\frac{d \\bar{v}^{i}}{d t}\n$$\n\nBecause the tangent field of $\\mathscr{C}$ is a tensor, the velocity components in an arbitrary coordinate system $\\left(x^{i}\\right)$ are just $v^{i}=d x^{i} / d t$. However, as we found in Chapter 6 , the acceleration components must be written as absolute derivatives along $\\mathscr{C}: a^{i}=\\delta v^{i} / \\delta t$. Hence, in an arbitrary coordinate system $\\left(x^{i}\\right)$, with $\\left(g_{i j}\\right)$ representing the Euclidean metric, we have:\n\n\n\\begin{equation*}\nv^{i}=\\frac{d x^{i}}{d t} \\quad a^{i}=\\frac{d v^{i}}{d t}+\\Gamma_{r s}^{i} v^{r} v^{s}=\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t} \\tag{11.9}\n\\end{equation*}\n\n\nand the speed and acceleration scalars are the invariants\n\n\n\\begin{equation*}\nv=\\sqrt{g_{i j} v^{i} v^{j}} \\quad a=\\sqrt{g_{i j} a^{i} a^{j}} \\tag{11.10}\n\\end{equation*}\n\n\nEXAMPLE 11.2 Formulas (11.9) give the contravariant components of velocity and acceleration. These are not the components used in classical physics and vector analysis. There, the metric for an orthogonal curvilinear coordinate system is written as\n\n$$\nd s^{2}=h_{1}^{2}\\left(d x_{1}\\right)^{2}+h_{2}^{2}\\left(d x_{2}\\right)^{2}+h_{3}^{2}\\left(d x_{3}\\right)^{2}\n$$\n\nand one defines the physical components of the velocity vector as\n\n$$\nv_{(1)}=h_{1} \\frac{d x_{1}}{d t} \\quad v_{(2)}=h_{2} \\frac{d x_{2}}{d t} \\quad v_{(3)}=h_{3} \\frac{d x_{3}}{d t}\n$$\n\nThus, the physical components are related to the contravariant components via\n\n\n\\begin{equation*}\nv_{(\\alpha)}=\\sqrt{g_{\\alpha \\alpha}} v^{\\alpha} \\quad(\\alpha=1,2,3 ; \\text { no summation }) \\tag{1}\n\\end{equation*}\n\n\nLikewise for acceleration and force vectors. (See Problem 11.24.)\n\nLet us illustrate the distinction by calculating the components of acceleration in cylindrical coordinates, $\\left(x^{1}, x^{2}, x^{3}\\right)=\\left(x_{1}, x_{2}, x_{3}\\right)=(r, \\theta, z)$, for which $g_{11}=1, g_{22}=\\left(x^{1}\\right)^{2}, g_{33}=1$. By Problem 6.26,\n\n$$\n\\left.\\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1 / x^{1} \\quad \\text { (all others zero }\\right)\n$$\n\nso that (11.9) gives for the contravariant components\n\n\n\\begin{equation*}\na^{1}=\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2} \\quad a^{2}=\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{r} \\frac{d r}{d t} \\frac{d \\theta}{d t} \\quad a^{3}=\\frac{d^{2} z}{d t^{2}} \\tag{2}\n\\end{equation*}\n\n\nThe physical components are then obtained from (1) as\n\n\n\\begin{equation*}\na_{(r)}=\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2} \\quad a_{(\\theta)}=r \\frac{d^{2} \\theta}{d t^{2}}+2 \\frac{d r}{d t} \\frac{d \\theta}{d t} \\quad a_{(z)}=\\frac{d^{2} z}{d t^{2}} \\tag{3}\n\\end{equation*}\n\n\nOnly the $\\theta$-component differs between (2) and (3); but the difference is significant. For instance, the coriolis acceleration of a particle is $2 \\dot{r} \\dot{\\theta}$, as in (3).\n\n\\subsection*{11.4 NEWTON'S SECOND LAW IN CURVILINEAR COORDINATES}\nThe momentum vector of a particle of mass $m$ is defined as $\\mathbf{M}=m \\mathbf{v}$. Relative to a rectangular coordinate system (with the property of being an inertial frame), Newton's second law of motion effectively defines the force vector acting on a particle as $\\mathbf{F}=d \\mathbf{M} / d t$. Accordingly, in curvilinear coordinates $\\left(x^{i}\\right)$, the law reads:\n\n\n\\begin{equation*}\n\\mathbf{F}=\\frac{\\delta \\mathbf{M}}{\\delta t}=m \\frac{\\delta \\mathbf{v}}{\\delta t}=m \\mathbf{a} \\tag{11.11}\n\\end{equation*}\n\n\nassuming a constant mass. Therefore, the contravariant components of force are given by\n\n\n\\begin{equation*}\nF^{i}=m a^{i}=m\\left(\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right) \\tag{11.12a}\n\\end{equation*}\n\n\nand the covariant components by\n\n\n\\begin{equation*}\nF_{i}=g_{i r} F^{r}=m\\left(g_{i r} \\frac{d^{2} x^{r}}{d t^{2}}+\\Gamma_{r s i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right) \\tag{11.12b}\n\\end{equation*}\n\n\nBy introducing a scalar invariant called the kinetic energy of the particle,\n\n$$\nT \\equiv \\frac{1}{2} m v^{2}=\\frac{1}{2} m g_{i j} v^{i} v^{j}\n$$\n\none can (see Problem 11.5) put (11.12b) into the equivalent Lagrangian form\n\n\n\\begin{equation*}\nF_{i}=\\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial v^{i}}\\right)-\\frac{\\partial T}{\\partial x^{i}} \\tag{11.13}\n\\end{equation*}\n\n\nThe partial derivatives in (11.13) are taken with $T$ considered as a function of six independent variables, the $x^{i}$ (via the $g_{i j}$ ) and the $v^{i}$.\n\nEXAMPLE 11.3 (Motion under a Central Force) (a) Obtain the differential equation for the trajectory of a particle acted on by a force that is always directed from (or toward) a fixed point $O$. (b) Solve the equation of (a) when the central force is gravitational, thus determining the orbit of a satellite.\n\n(a) By Problem 11.18, the motion will be confined to a plane through $O$. Take $O$ as the origin of polar coordinates $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$ in the plane; the force field then has the form $\\mathbf{F}=\\left(F^{1}, 0\\right)$. Taking the acceleration components from (2) of Example 11.2, we have as the equations of motion:\n\n$$\n\\begin{aligned}\n& F^{1}=m a^{1}=m\\left[\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2}\\right] \\\\\n& 0=m a^{2}=m\\left[\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{r} \\frac{d r}{d t} \\frac{d \\theta}{d t}\\right] \\equiv \\frac{m}{r^{2}} \\frac{d}{d t}\\left(r^{2} \\frac{d \\theta}{d t}\\right)\n\\end{aligned}\n$$\n\nThe $\\theta$-equation has the first integral\n\n$$\nr^{2} \\frac{d \\theta}{d t}=q=\\text { const. }\n$$\n\n(conservation of angular momentum), which can be used to change the parameter of the trajectory from $t$ to $\\theta$. Thus, writing $u=1 / r$, we have:\n\n$$\n\\begin{aligned}\n& \\frac{d r}{d t}=\\frac{d \\theta}{d t} \\frac{d u^{-1}}{d \\theta}=\\left(q u^{2}\\right)\\left(-u^{-2} \\frac{d u}{d \\theta}\\right)=-q \\frac{d u}{d \\theta} \\\\\n& \\frac{d^{2} r}{d t^{2}}=-q \\frac{d^{2} u}{d \\theta^{2}}\\left(q u^{2}\\right)=-q^{2} u^{2} \\frac{d^{2} u}{d \\theta^{2}}\n\\end{aligned}\n$$\n\nand the $r$-equation becomes\n\n\n\\begin{equation*}\n\\frac{d^{2} u}{d \\theta^{2}}+u=g(u, \\theta) \\tag{1}\n\\end{equation*}\n\n\nin which $g(u, \\theta)=-F^{1}\\left(u^{-1}, \\theta\\right) / m q^{2} u^{2}$.\\\\\n(b) For the gravitational field, $F^{1}=-k / r^{2}=-k u^{2}$ ( $k>0$; attractive force), so that $g(u, \\theta)=Q=$ const. and the solution of (1) is $u=P \\cos \\theta+Q$, or\n\n\n\\begin{equation*}\nr=\\frac{1 / Q}{1+e \\cos \\theta} \\tag{2}\n\\end{equation*}\n\n\nwhich is a conic having eccentricity $e=P / Q$ and focus at $O$, the classical result.\n\n\\subsection*{11.5 DIVERGENCE, LAPLACIAN, CURL}\nThe divergence of a contravariant vector $\\mathbf{u}=\\left(u^{i}\\right)$ on $\\mathbf{E}^{3}$ is defined by (9.11), with use of Problem 8.14:\n\n$$\n\\operatorname{div} \\mathbf{u}=u_{, i}^{i}=\\frac{\\partial u^{i}}{\\partial x^{i}}+\\Gamma_{r i}^{i} u^{r}=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{r} \\frac{\\partial}{\\partial x^{r}}(\\ln \\sqrt{g})=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}(\\sqrt{g})\n$$\n\nThe product rule for partial differentiation yields the compact form\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{u}=\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} u^{i}\\right) \\tag{11.14}\n\\end{equation*}\n\n\nAnother notation for the divergence is $\\boldsymbol{\\nabla} \\cdot \\mathbf{u}$.\n\nThe Laplacian of a scalar field $f$ is given by $\\nabla^{2} f \\equiv \\operatorname{div}(\\operatorname{grad} f)$. Since in general coordinates divergence is defined for contravariant tensors only, while grad $f=\\left(\\partial f / \\partial x^{i}\\right)$ is a covariant tensor (Example 3.5), we first raise the subscript and then find the divergence by (11.14):\n\n\n\\begin{equation*}\n\\nabla^{2} f=\\operatorname{div}\\left(g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right)=\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right) \\tag{11.15}\n\\end{equation*}\n\n\nThe Laplacian figures importantly in electromagnetic theory via the scalar wave equation,\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} f}{\\partial t^{2}}=k^{2} \\nabla^{2} f \\quad(k=\\text { const. }=\\text { wave speed }) \\tag{11.16a}\n\\end{equation*}\n\n\nIn cartesian coordinates only, one defines the Laplacian of a vector field as $\\nabla^{2} \\mathbf{u} \\equiv\\left(\\nabla^{2} u^{i}\\right)$, where $\\nabla^{2} u^{i}=u_{x x}^{i}+u_{y y}^{i}+u_{z z}^{i}$, and writes the vector wave equation,\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\mathbf{u}}{\\partial t^{2}}=k^{2} \\nabla^{2} \\mathbf{u} \\tag{11.16b}\n\\end{equation*}\n\n\nas an abbreviation for three scalar wave equations.\n\nEXAMPLE 11.4 Write the Laplacian for cvlindrical coordinates.\n\nUsing $g^{11}=1, g^{22}=1 /\\left(x^{1}\\right)^{2}, g^{33}=1$, and $g=\\left(x^{1}\\right)^{2}$ in (11.15),\n\n$$\n\\begin{aligned}\n\\nabla^{2} f & =\\frac{1}{x^{1}}\\left[\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\frac{\\partial f}{\\partial x^{1}}\\right)+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{1}{x^{1}} \\frac{\\partial f}{\\partial x^{2}}\\right)+\\frac{\\partial}{\\partial x^{3}}\\left(x^{1} \\frac{\\partial f}{\\partial x^{3}}\\right)\\right] \\quad\\left(x^{1}>0\\right) \\\\\n& =f_{11}+\\frac{1}{\\left(x^{1}\\right)^{2}} f_{22}+f_{33}+\\frac{1}{x^{1}} f_{1}\n\\end{aligned}\n$$\n\nthe last line employing subscript notation for the partial derivatives.\n\nThe curl of a vector field $\\mathbf{u}=\\left(u^{i}\\right)-$ symbolized curl $\\mathbf{u}, \\boldsymbol{\\nabla} \\times \\mathbf{u}$, or rot $\\mathbf{u}-$ is given in a rectangular coordinate system $\\left(x^{i}\\right)$ by\n\n\n\\begin{equation*}\n\\operatorname{curl} \\mathbf{u} \\equiv\\left(e_{i j k} \\frac{\\partial u^{k}}{\\partial x^{j}}\\right) \\tag{11.17a}\n\\end{equation*}\n\n\nwhere $e_{i j k}$ is the permutation symbol (Chapter 3). The definition may be rewritten as a determinantal operator:\n\n\\[\n\\operatorname{curl} \\mathbf{u} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{e}_{1} & \\mathbf{e}_{2} & \\mathbf{e}_{3}  \\tag{11.17b}\\\\\n\\frac{\\partial}{\\partial x^{1}} & \\frac{\\partial}{\\partial x^{2}} & \\frac{\\partial}{\\partial x^{3}} \\\\\nu^{1} & u^{2} & u^{3}\n\\end{array}\\right|\n\\]\n\nin which $\\left(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right)=(\\mathbf{i}, \\mathbf{j}, \\mathbf{k})$ is the standard orthonormal basis. Unlike the gradient and the divergence, the curl cannot be extended to curvilinear coordinate systems by a tensor formula.\n\nRemark 1: Not everything in mathematical physics is a tensor. Problem 11.11 shows that (11.17) defines a direct cartesian tensor, but that is all. This is not to say that the curl operator cannot be formulated and used in curvilinear coordinates (see any text in vector analysis). It is only that the curl in spherical coordinates (say) and the curl in rectangular coordinates are not related tensorwise.\n\n\\section*{Nonrelativistic Maxwell's Equations}\nLet $\\quad \\mathbf{E}=$ electric field strength\n\n$\\mathbf{D}=$ electric displacement\n\n$\\mathbf{H}=$ magnetic field strength\n\n$\\mathbf{B}=$ magnetic induction\n\n$\\mathbf{J}=$ current density\n\n$\\rho=$ charge density\n\n$\\epsilon=$ dielectric constant\n\n$\\mu=$ magnetic permeability\n\n$c=$ velocity of light\n\nThen the famous Maxwell's equations may be written as follows:\n\n\\[\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}+\\frac{1}{c} \\frac{\\partial \\mathbf{B}}{\\partial t}=0 & \\operatorname{div} \\mathbf{B}=0  \\tag{11.18}\\\\\n\\operatorname{curl} \\mathbf{H}-\\frac{1}{c} \\frac{\\partial \\mathbf{D}}{\\partial t}=\\frac{1}{c} \\mathbf{J} & \\operatorname{div} \\mathbf{D}=\\rho\n\\end{array}\n\\]\n\nFrom standard formulas in electromagnetic theory, $\\mathbf{D}=\\epsilon \\mathbf{E}, \\mathbf{B}=\\mu \\mathbf{H}$, and $\\mathbf{J}=\\rho \\mathbf{u}$, where $\\mathbf{u}$ denotes the velocity field of the charge distribution; (11.18) becomes\n\n$$\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}=-\\frac{\\mu}{c} \\frac{\\partial \\mathbf{H}}{\\partial t} & \\operatorname{div} \\mathbf{H}=0 \\\\\n\\operatorname{curl} \\mathbf{H}=\\frac{\\epsilon}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}+\\frac{\\rho}{c} \\mathbf{u} & \\operatorname{div} \\mathbf{E}=\\frac{\\rho}{\\epsilon}\n\\end{array}\n$$\n\nIf the charge distribution is in free space $\\left(\\epsilon=\\epsilon_{0}, \\mu=\\mu_{0}\\right)$, a proper choice of units brings the equations into the form\n\n\\[\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}=-\\frac{1}{c} \\frac{\\partial \\mathbf{H}}{\\partial t} & \\operatorname{div} \\mathbf{H}=0  \\tag{11.19}\\\\\n\\operatorname{curl} \\mathbf{H}=\\frac{1}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}+\\frac{\\rho}{c} \\mathbf{u} & \\operatorname{div} \\mathbf{E}=\\rho\n\\end{array}\n\\]\n\nWork with Maxwell's equations requires the vector identities listed below (see Problems 11.10 and 11.21).\n\n\n\\begin{align*}\n& \\boldsymbol{\\nabla} \\cdot(\\boldsymbol{\\nabla} \\times \\mathbf{u})=0 \\quad \\text { (for any } \\mathbf{u})  \\tag{11.20}\\\\\n& \\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\nabla(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})-\\nabla^{2} \\mathbf{u}  \\tag{11.21}\\\\\n& \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})=\\boldsymbol{\\nabla} \\cdot \\frac{\\partial \\mathbf{u}}{\\partial t}  \\tag{11.22}\\\\\n& \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\boldsymbol{\\nabla} \\times \\frac{\\partial \\mathbf{u}}{\\partial t} \\tag{11.23}\n\\end{align*}\n\n\n\\section*{Solved Problems}\n\\section*{VELOCITY AND ACCELERATION}\n11.1 Find the velocity and acceleration vectors and the scalars $v$ and $a$ for a particle whose equation of motion (along a twisted cubic) is $\\mathbf{x}=\\left(t, t^{2}, t^{3}\\right) \\quad(-1 \\leqq t \\leqq 1)$. Determine the extreme values of $v$ and $a$, and where they are assumed.\n\n$$\n\\begin{array}{lll}\n\\mathbf{v}=\\left(1,2 t, 3 t^{2}\\right) & \\text { and } & v=\\sqrt{1+4 t^{2}+9 t^{4}} \\\\\n\\mathbf{a}=(0,2,6 t) & \\text { and } & a=\\sqrt{4+36 t^{2}}\n\\end{array}\n$$\n\nHence $v$ and $a$ have maxima at $t= \\pm 1$, where $v=\\sqrt{14}$ and $a=\\sqrt{40}$. They have minima at $t=0$, where $v=1$ and $a=2$.\n\n\\section*{PARTICLE DYNAMICS}\n11.2 A particle travels at constant speed $v$ on a curve with positive curvature. Show that its acceleration is greatest where the curvature is greatest.\n\nBy (11.8) with $\\dot{v}=0, a=\\kappa v^{2}$ or $a / \\kappa=$ const.\n\n11.3 Compute the contravariant acceleration components in a coordinate system $\\left(x^{i}\\right)$ connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ by $\\bar{x}^{1}=\\left(x^{1}\\right)^{2}, \\bar{x}^{2}=x^{2}, \\bar{x}^{3}=x^{3}$.\n\nUse (5.7):\n\n$$\nG=J^{T} J=\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n4\\left(x^{1}\\right)^{2} & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nHence, the Christoffel symbols are given by\n\n$$\n\\Gamma_{11}^{1}=\\frac{\\partial}{\\partial x^{1}}\\left[\\frac{1}{2} \\ln 4\\left(x^{1}\\right)^{2}\\right]=\\frac{1}{x^{1}} \\quad \\text { (all others zero) }\n$$\n\nand (11.9) gives\n\n$$\na^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\frac{1}{x^{1}}\\left(\\frac{d x^{1}}{d t}\\right)^{2} \\quad a^{2}=\\frac{d^{2} x^{2}}{d t^{2}} \\quad a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}\n$$\n\n\\section*{NEWTON'S SECOND LAW}\n11.4 Show that Newton's second law is consistent with Newton's first law: A particle that is not acted upon by an outside force is at rest or is in motion along a straight line at constant velocity. Assume a rectangular coordinate system.\n\n$\\mathbf{F}=\\mathbf{0}$ implies $d \\mathbf{v} / d t=\\mathbf{0}$, or $\\mathbf{v}=\\mathbf{d}$ (constant). Then,\n\n$$\n\\frac{d \\mathbf{x}}{d t}=\\mathbf{d} \\quad \\text { or } \\quad \\mathbf{x}=t \\mathbf{d}+\\mathbf{x}_{0}\n$$\n\nwhich is the parametric equation for a point (if $\\mathbf{d}=\\mathbf{0}$ ) or for a straight line (if $\\mathbf{d} \\neq \\mathbf{0}$ ), along which $v=\\|\\mathbf{d}\\|=$ const.\n\n11.5 Prove the equivalence of (11.13) and (11.12b).\n\nFor simplicity, take $m=1$ in (11.13). By the chain rule and the symmetry of $\\left(g_{i j}\\right)$,\n\n$$\n\\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial v^{i}}\\right)-\\frac{\\partial T}{\\partial x^{i}}=\\frac{d}{d t}\\left(g_{i r} v^{r}\\right)-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}+\\frac{d g_{i r}}{d t} v^{r}\n$$\n\n$$\n\\begin{aligned}\n& =g_{i r} \\frac{d v^{r}}{d t}-g_{r s i}\\left(\\frac{1}{2} v^{r} v^{s}\\right)+\\frac{\\partial g_{i r}}{\\partial x^{s}} \\frac{d x^{s}}{d t} v^{r}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+g_{i r s} v^{s} v^{r} \\\\\n& =g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+\\frac{1}{2} g_{s i r} v^{s} v^{r}+\\frac{1}{2} g_{i r s} v^{s} v^{r}=g_{i r} \\frac{d v^{r}}{d t}+\\Gamma_{r s i} v^{r} v^{s}\n\\end{aligned}\n$$\n\nThe final expression is exactly the right-hand side of $(11.12 b)$ (for $m=1$ ).\n\n11.6 Solve (1) of Example 11.3 when the force field is of the form\n\n$$\ng(u, \\theta)=A u+h(\\theta)\n$$\n\nwhere $A$ is a constant and $h(\\theta)$ is periodic of period $2 \\pi$.\n\nWith primes denoting $\\theta$-derivatives, we must solve\n\n$$\nu^{\\prime \\prime}+u=A u+h(\\theta) \\quad \\text { or } \\quad u^{\\prime \\prime}+(1-A) u=h(\\theta)\n$$\n\nThe general solution to the homogeneous equation is\n\n$$\nu= \\begin{cases}P \\cos (\\sqrt{1-A} \\theta+\\alpha) & A<1 \\\\ \\alpha \\theta+\\beta & A=1 \\\\ Q \\exp (\\sqrt{A-1} \\theta)+R \\exp (-\\sqrt{A-1} \\theta) & A>1\\end{cases}\n$$\n\nA particular solution of the nonhomogeneous equation may be obtained in the form $u=u_{H} w$, where $u_{H}$ is any particular solution of the homogeneous equation. In fact, substitution in the differential equation yields\n\n$$\n2 u_{H}^{\\prime} w^{\\prime}+u_{H} w^{\\prime \\prime}=h \\quad \\text { or } \\quad\\left(u_{H}^{2} w^{\\prime}\\right)^{\\prime}=u_{H} h\n$$\n\nand this last equation can be solved by two quadratures:\n\n$$\nw^{\\prime}(\\theta)=\\frac{1}{u_{H}^{2}(\\theta)} \\int_{0}^{\\theta} u_{H}(\\phi) h(\\phi) d \\phi \\quad \\text { and } \\quad w(\\theta)=\\int_{0}^{\\theta} \\frac{d \\psi}{u_{H}^{2}(\\psi)} \\int_{0}^{\\psi} u_{H}(\\phi) h(\\phi) d \\phi\n$$\n\nThe integrals are easily evaluated when $h(\\phi)$ is represented as a Fourier series.\n\n11.7 If $h(\\theta)=0$ in Problem 11.6, identify the orbits corresponding to (a) $A=0,(b) A=1,(c)$ $A=5 / 4$.\n\n(a) The curve $1 / r=P \\cos (\\theta+\\alpha)$, or $r \\cos (\\theta+\\alpha)=1 / P$, is a straight line (Fig. 11-1).\n\n(b) The curve $1 / r=\\alpha \\theta+\\beta$ is a hyperbolic spiral that degenerates into a circle for $\\alpha=0$.\n\n(c) The curve $1 / r=Q e^{\\theta / 2}+R e^{-\\theta / 2}$ is a complex spiral which, in the case $Q=0, R=1$, reduces to the simple logarithmic spiral $r=e^{\\theta / 2}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-169}\n\\end{center}\n\nFig. 11-1\n\n\\section*{DIFFERENTIAL OPERATORS}\n11.8 Calculate the Laplacian for spherical coordinates by the tensor formula. (The calculation is very tedious by other methods.)\n\nWe have\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right] \\quad G^{-1}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{-2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{-2}\n\\end{array}\\right]\n$$\n\nand $g=\\left(x^{1}\\right)^{4} \\sin ^{2} x^{2}$, so that in (11.15),\n\nTherefore,\n\n$$\n\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right)\\left(g^{i 1} \\frac{\\partial f}{\\partial x^{1}}+g^{i 2} \\frac{\\partial f}{\\partial x^{2}}+g^{i 3} \\frac{\\partial f}{\\partial x^{3}}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\sqrt{g} g^{1 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}} \\\\\n& \\sqrt{g} g^{2 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1}\\right)^{2}} \\frac{\\partial f}{\\partial x^{2}}=\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}} \\\\\n& \\sqrt{g} g^{3 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1} \\sin x^{2}\\right)^{2}} \\frac{\\partial f}{\\partial x^{3}}=\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\n\\end{aligned}\n$$\n\nand so\n\n$$\n\\begin{aligned}\n& \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right]=\\frac{\\partial}{\\partial x^{1}} {\\left[\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}\\right]+\\frac{\\partial}{\\partial x^{2}}\\left[\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}\\right]+\\frac{\\partial}{\\partial x^{3}}\\left[\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\\right] } \\\\\n&=2 x^{1}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}+\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{1}\\right)^{2}}+\\left(\\cos x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}+\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{2}\\right)^{2}} \\\\\n&+\\left(\\csc x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{3}\\right)^{2}}\n\\end{aligned}\n$$\n\nIn writing the final steps we convert to $\\rho=x^{1}, \\varphi=x^{2}$, and $\\theta=x^{3}$ :\n\n$$\n\\begin{aligned}\n\\nabla^{2} f & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right] \\\\\n& =\\frac{1}{\\rho^{2} \\sin \\varphi}\\left[(2 \\rho \\sin \\varphi) \\frac{\\partial f}{\\partial \\rho}+\\left(\\rho^{2} \\sin \\varphi\\right) \\frac{\\partial^{2} f}{\\partial \\rho^{2}}+(\\cos \\varphi) \\frac{\\partial f}{\\partial \\varphi}+(\\sin \\varphi) \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+(\\csc \\varphi) \\frac{\\partial^{2} f}{\\partial \\theta^{2}}\\right] \\\\\n& =\\frac{\\partial^{2} f}{\\partial \\rho^{2}}+\\frac{1}{\\rho^{2}} \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+\\frac{1}{\\rho^{2} \\sin ^{2} \\varphi} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{2}{\\rho} \\frac{\\partial f}{\\partial \\rho}+\\frac{\\cot \\varphi}{\\rho^{2}} \\frac{\\partial f}{\\partial \\varphi}\n\\end{aligned}\n$$\n\n11.9 Calculate the divergence in spherical coordinates $(\\rho, \\varphi, \\theta)$ of $(a)$ a contravariant vector, $\\mathbf{u}=\\left(u^{i}\\right) ;(b)$ a vector specified by its physical components, $\\mathbf{u}=u_{(1)} \\mathbf{e}_{1}+u_{(2)} \\mathbf{e}_{2}+u_{(3)} \\mathbf{e}_{3}$.\n\n(a) We plug into the formula (11.14):\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} u^{i}\\right)=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}(\\sqrt{g}) \\\\\n& =\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\left(x^{1}\\right)^{2} \\sin x^{2}} \\frac{\\partial}{\\partial x^{i}}\\left[\\left(x^{1}\\right)^{2} \\sin x^{2}\\right] \\\\\n& =\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{1}\\left(\\frac{2}{x^{1}}\\right)+u^{2}\\left(\\frac{\\cos x^{2}}{\\sin x^{2}}\\right)+u^{3}(0)\n\\end{aligned}\n$$\n\nThus\n\n$$\n\\operatorname{div} \\mathbf{u}=\\frac{\\partial u^{1}}{\\partial \\rho}+\\frac{\\partial u^{2}}{\\partial \\varphi}+\\frac{\\partial u^{3}}{\\partial \\theta}+\\frac{2}{\\rho} u^{1}+(\\cot \\varphi) u^{2}\n$$\n\n(b) By Example 11.2, we apply (11.4) to the contravariant vector having components\n\n$$\nu^{1}=\\frac{u_{(1)}}{1} \\quad u^{2}=\\frac{u_{(2)}}{x^{1}} \\quad u^{3}=\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\n$$\n\nHence, from (a),\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{\\partial}{\\partial x^{1}} u_{(1)}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{(2)}}{x^{1}}\\right)+\\frac{\\partial}{\\partial x^{3}}\\left(\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\\right)+u_{(1)}\\left(\\frac{2}{x^{1}}\\right)+\\frac{u_{(2)}}{x^{1}}\\left(\\cot x^{2}\\right) \\\\\n& =\\frac{\\partial u_{(\\rho)}}{\\partial \\rho}+\\frac{1}{\\rho} \\frac{\\partial u_{(\\varphi)}}{\\partial \\varphi}+\\frac{1}{\\rho \\sin \\varphi} \\frac{\\partial u_{(\\theta)}}{\\partial \\theta}+\\frac{2}{\\rho} u_{(\\rho)}+\\frac{\\cot \\varphi}{\\rho} u_{(\\varphi)}\n\\end{aligned}\n$$\n\nIt is in this last form that \"the divergence in spherical coordinates\" is generally encountered in reference books.\n\n11.10 Establish in rectangular coordinates the identity\n\n\n\\begin{equation*}\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\nabla(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})-\\nabla^{2} \\mathbf{u} \\tag{1}\n\\end{equation*}\n\n\n(\"curl curl equals grad div minus del-square\").\n\nBoth sides of (1) are (cartesian) vectors; we shall show that they are componentwise equal.\n\nBy (11.7), the $i$ th component of curl $\\mathbf{u}$ is $e_{i j k}\\left(\\partial u^{k} / \\partial x^{j}\\right)$. Therefore, the $i$ th component of curl (curl $\\mathbf{u}$ ) is [use (3.23)]:\n\n$$\n\\begin{aligned}\ne_{i r s} \\frac{\\partial}{\\partial x^{r}}\\left(e_{s j k} \\frac{\\partial u^{k}}{\\partial x^{i}}\\right) & =e_{i r s} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=e_{s i r} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}} \\\\\n& =\\left(\\delta_{i j} \\delta_{r k}-\\delta_{i k} \\delta_{r j}\\right) \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=\\frac{\\partial^{2} u^{r}}{\\partial x^{r} \\partial x^{i}}-\\frac{\\partial^{2} u^{i}}{\\partial x^{r} \\partial x^{r}} \\\\\n& \\equiv \\frac{\\partial}{\\partial x^{i}}(\\operatorname{div} \\mathbf{u})-\\nabla^{2} u^{i}\n\\end{aligned}\n$$\n\nThe first term on the right is recognized as the $i$ th component of grad (div $\\mathbf{u}$ ), and the second term is (by definition) the $i$ th component of the Laplacian of the vector $\\mathbf{u}$. QED\n\n11.11 Prove that the array represented in rectangular coordinates $\\left(x^{i}\\right)$ by\n\n$$\n\\operatorname{curl} \\mathbf{u}=\\left(e_{i j k} \\frac{\\partial u^{k}}{\\partial x^{j}}\\right)\n$$\n\nis a direct cartesian tensor.\n\nIt suffices to show that $\\left(e_{i j k}\\right)$ is a direct cartesian tensor, since $\\left(\\partial u^{i} / \\partial x^{j}\\right)$ is known to be a (direct) cartesian tensor. Therefore, given the orthogonal transformation $\\bar{x}^{i}=a_{j}^{i} x^{j}$, with $\\left|a_{j}^{i}\\right|=+1$, define the $3^{3}=27$ quantities\n\n$$\n\\tau_{i j k} \\equiv e_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=e_{r s t} a_{r}^{i} a_{s}^{j} a_{t}^{k}\n$$\n\nWe observe that:\n\n(i) $\\tau_{i j k}=0$ when two subscripts have the same value; e.g.,\n\n\n\\begin{align*}\n\\tau_{i 22}=e_{r s t} a_{r}^{i} a_{s}^{2} a_{t}^{2} & =-e_{r t s} a_{r}^{i} a_{s}^{2} a_{t}^{2}=-e_{r s t} a_{r}^{i} a_{t}^{2} a_{s}^{2}=-\\tau_{i 22} \\\\\n\\tau_{123} & =e_{r s t} a_{r}^{1} a_{s}^{2} a_{t}^{3}=\\left|a_{j}^{i}\\right|=+1 \\tag{ii}\n\\end{align*}\n\n\n(iii) $\\tau_{i j k}$ changes sign when any two subscripts are interchanged; e.g.\n\n$$\n\\tau_{k j i}=e_{r s t} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-e_{t s r} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-\\tau_{i j k}\n$$\n\nBut these three properties identify $\\tau_{i j k}$ with $\\bar{e}_{i j k}$, and the proof is complete.\n\n11.12 Show that in a vacuum with zero charge $(\\rho=0)$, the electric field $\\mathbf{E}$ satisfies the vector wave equation\n\n$$\n\\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}=c^{2} \\nabla^{2} \\mathbf{E}\n$$\n\nFrom Maxwell's equations (11.19), along with the identity (11.23),\n\n$$\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\frac{1}{c} \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\times \\mathbf{H})=-\\frac{1}{c}\\left(\\frac{1}{c} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\\right)=-\\frac{1}{c^{2}} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\n$$\n\nBut $\\boldsymbol{\\nabla} \\cdot \\mathbf{E}=0$ and Problem 11.10 imply $\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\nabla^{2} \\mathbf{E}$, and the wave equation follows.\n\n\\section*{Supplementary Problems}\n11.13 Show that if $v$ is constant, a particle describes equal lengths of arc in equal periods of time.\n\n11.14 (a) Show that a particle whose path is given by $\\mathbf{x}=(\\cos t, \\sin t, \\cot t)$, for $\\pi / 4 \\leqq t<\\pi / 2$, has velocity decreasing to $\\sqrt{2}$ as $t \\rightarrow \\pi / 2$. (b) What is the behavior of the acceleration as $t \\rightarrow \\pi / 2$ ? (c) Find the extreme values of $v$ and $a$ for this particle.\n\n11.15 For what kind of motion, if any, is $a=d v / d t$ ?\n\n11.16 Develop a formula for \u00e0 for a particle that has constant speed $v$.\n\n11.17 Calculate the acceleration components (contravariant) in spherical coordinates $(\\rho, \\varphi, \\theta)$.\n\n11.18 Prove that motion under a central force is planar.\n\n11.19 Calculate the Laplacian for cylindrical coordinates $(r, \\theta, z)$.\n\n11.20 Show that $\\nabla^{2} f=g^{i j} f_{, i j}$. [Hint: Write (11.15) at the origin of Riemannian coordinates.]\n\n11.21 Prove (11.22) and (11.23).\n\n11.22 Prove that $\\operatorname{curl}(\\operatorname{grad} f)=\\mathbf{0}$ for any $C^{2}$ scalar field $f$.\n\n11.23 Show that in a charge-free vacuum, $\\mathbf{H}$ also satisfies the vector wave equation.\n\n11.24 Show that, relative to an orthogonal curvilinear coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$, an arbitrary contravariant vector $\\mathbf{v}=\\left(v^{i}\\right)$ has the representation\n\n$$\n\\mathbf{v}=v_{(1)} \\mathbf{e}_{1}+v_{(2)} \\mathbf{e}_{2}+v_{(3)} \\mathbf{e}_{3}\n$$\n\nwhere $v_{(\\alpha)}$ is the physical component and $\\mathbf{e}_{\\alpha}$ is the unit normal to the surface $x^{\\alpha}=$ const. [Hint: Use Problems 5.19 and 5.20].\n\n"], "lesson": "\\section*{Chapter 11}\n\\section*{Tensors in Classical Mechanics}\n\\subsection*{11.1 INTRODUCTION}\nClassical mechanics originated with the work of Galileo and was developed extensively by Newton (it is often called Newtonian mechanics). It deals with the motion of particles in a fixed frame of reference (rectangular coordinate system). The basic premise of Newtonian mechanics is the concept of absolute time measurement between two reference frames at constant velocity relative to each other (called Galilean frames). Within those frames, other coordinate systems may be used so long as the metric remains Euclidean. This means that some of the theory of tensors can be brought to bear on this study.\n\n\\subsection*{11.2 PARTICLE KINEMATICS IN RECTANGULAR COORDINATES}\nLet $P$ be a particle whose path in $\\mathbf{E}^{3}$ is given by\n\n\n\\begin{equation*}\n\\mathscr{C}: \\mathbf{x}=\\left(x^{i}(t)\\right) \\tag{11.1}\n\\end{equation*}\n\n\nwhere $t$ represents time. The velocity vector of $P$ is defined as\n\n\n\\begin{equation*}\n\\mathbf{v} \\equiv \\frac{d \\mathbf{x}}{d t} \\equiv\\left(\\frac{d x^{1}}{d t}, \\frac{d x^{2}}{d t}, \\frac{d x^{3}}{d t}\\right) \\equiv\\left(\\dot{x}^{1}, \\dot{x}^{2}, \\dot{x}^{3}\\right) \\tag{11.2}\n\\end{equation*}\n\n\nand the (instantaneous) velocity or speed as the scalar\n\n\n\\begin{equation*}\nv \\equiv\\|\\mathbf{v}\\| \\equiv \\sqrt{\\left(\\dot{x}^{1}\\right)^{2}+\\left(\\dot{x}^{2}\\right)^{2}+\\left(\\dot{x}^{3}\\right)^{2}} \\tag{11.3}\n\\end{equation*}\n\n\nFurther, define the acceleration vector as\n\n\n\\begin{equation*}\n\\mathbf{a} \\equiv \\frac{d \\mathbf{v}}{d t} \\equiv\\left(\\frac{d^{2} x^{1}}{d t^{2}}, \\frac{d^{2} x^{2}}{d t^{2}}, \\frac{d^{2} x^{3}}{d t^{2}}\\right) \\equiv\\left(\\ddot{x}^{1}, \\ddot{x}^{2}, \\ddot{x}^{3}\\right) \\tag{11.4}\n\\end{equation*}\n\n\nand the acceleration as\n\n\n\\begin{equation*}\na=\\|\\mathbf{a}\\| \\equiv \\sqrt{\\left(\\ddot{x}^{1}\\right)^{2}+\\left(\\ddot{x}^{2}\\right)^{2}+\\left(\\ddot{x}^{3}\\right)^{2}} \\tag{11.5}\n\\end{equation*}\n\n\nIf $\\mathbf{v}=\\left(v^{i}\\right)$ and $\\mathbf{a}=\\left(a^{i}\\right)$, the preceding formulas have the component forms\n\n\n\\begin{equation*}\nv^{i}=\\frac{d x^{i}}{d t} \\quad v=\\sqrt{v^{i} v^{i}} \\quad a^{i}=\\frac{d^{2} x^{i}}{d t^{2}} \\quad a=\\sqrt{a^{i} a^{i}} \\tag{11.6}\n\\end{equation*}\n\n\nIn terms of the geometry of the curve $\\mathscr{C}$ we have $\\mathbf{v}=v \\mathbf{T}$, with $v=d s / d t$. Hence,\n\n$$\n\\mathbf{a}=\\frac{d}{d t}(v \\mathbf{T})=\\dot{v} \\mathbf{T}+v \\dot{\\mathbf{T}}=\\dot{v} \\mathbf{T}+v \\frac{d \\mathbf{T}}{d s} \\frac{d s}{d t}=\\dot{v} \\mathbf{T}+v^{2} \\mathbf{T}^{\\prime}\n$$\n\nBut $\\mathbf{T}^{\\prime}=\\kappa \\mathbf{N}$, and so\n\n\n\\begin{equation*}\n\\mathbf{a}=\\dot{v} \\mathbf{T}+\\kappa v^{2} \\mathbf{N} \\tag{11.7}\n\\end{equation*}\n\n\nVia the Pythagorean theorem, (11.7) implies\n\n\n\\begin{equation*}\na=\\sqrt{\\dot{v}^{2}+\\kappa^{2} v^{4}} \\tag{11.8}\n\\end{equation*}\n\n\nEXAMPLE 11.1 Formula (11.7) serves to define the tangential and normal accelerations of $P$ as\n\n$$\n\\dot{v}=\\frac{d^{2} s}{d t^{2}} \\quad \\text { and } \\quad \\kappa v^{2}=\\frac{v^{2}}{\\rho} \\quad(\\rho \\equiv \\text { radius of curvature })\n$$\n\nrespectively. For a particle with constant velocity, $a=\\left\\|\\kappa v^{2} \\mathbf{N}\\right\\|=|\\kappa| v^{2}$; i.e., the acceleration is proportional to the absolute curvature.\n\n\\subsection*{11.3 PARTICLE KINEMATICS IN CURVILINEAR COORDINATES}\nThere emerges the problem of expressing the preceding formulas in nonrectangular coordinate systems. This is not merely an academic consideration, for there are important situations in which the differential equations of motion can be solved only in polar or spherical coordinates (cf. Example 11.3), not to mention applications in relativistic mechanics.\n\nLet us start with the definitions of the velocity and acceleration vectors in a barred rectangular system:\n\n$$\n\\bar{v}^{i}=\\frac{d \\bar{x}^{i}}{d t} \\quad \\text { and } \\quad \\bar{a}^{i}=\\frac{d \\bar{v}^{i}}{d t}\n$$\n\nBecause the tangent field of $\\mathscr{C}$ is a tensor, the velocity components in an arbitrary coordinate system $\\left(x^{i}\\right)$ are just $v^{i}=d x^{i} / d t$. However, as we found in Chapter 6 , the acceleration components must be written as absolute derivatives along $\\mathscr{C}: a^{i}=\\delta v^{i} / \\delta t$. Hence, in an arbitrary coordinate system $\\left(x^{i}\\right)$, with $\\left(g_{i j}\\right)$ representing the Euclidean metric, we have:\n\n\n\\begin{equation*}\nv^{i}=\\frac{d x^{i}}{d t} \\quad a^{i}=\\frac{d v^{i}}{d t}+\\Gamma_{r s}^{i} v^{r} v^{s}=\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t} \\tag{11.9}\n\\end{equation*}\n\n\nand the speed and acceleration scalars are the invariants\n\n\n\\begin{equation*}\nv=\\sqrt{g_{i j} v^{i} v^{j}} \\quad a=\\sqrt{g_{i j} a^{i} a^{j}} \\tag{11.10}\n\\end{equation*}\n\n\nEXAMPLE 11.2 Formulas (11.9) give the contravariant components of velocity and acceleration. These are not the components used in classical physics and vector analysis. There, the metric for an orthogonal curvilinear coordinate system is written as\n\n$$\nd s^{2}=h_{1}^{2}\\left(d x_{1}\\right)^{2}+h_{2}^{2}\\left(d x_{2}\\right)^{2}+h_{3}^{2}\\left(d x_{3}\\right)^{2}\n$$\n\nand one defines the physical components of the velocity vector as\n\n$$\nv_{(1)}=h_{1} \\frac{d x_{1}}{d t} \\quad v_{(2)}=h_{2} \\frac{d x_{2}}{d t} \\quad v_{(3)}=h_{3} \\frac{d x_{3}}{d t}\n$$\n\nThus, the physical components are related to the contravariant components via\n\n\n\\begin{equation*}\nv_{(\\alpha)}=\\sqrt{g_{\\alpha \\alpha}} v^{\\alpha} \\quad(\\alpha=1,2,3 ; \\text { no summation }) \\tag{1}\n\\end{equation*}\n\n\nLikewise for acceleration and force vectors. (See Problem 11.24.)\n\nLet us illustrate the distinction by calculating the components of acceleration in cylindrical coordinates, $\\left(x^{1}, x^{2}, x^{3}\\right)=\\left(x_{1}, x_{2}, x_{3}\\right)=(r, \\theta, z)$, for which $g_{11}=1, g_{22}=\\left(x^{1}\\right)^{2}, g_{33}=1$. By Problem 6.26,\n\n$$\n\\left.\\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1 / x^{1} \\quad \\text { (all others zero }\\right)\n$$\n\nso that (11.9) gives for the contravariant components\n\n\n\\begin{equation*}\na^{1}=\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2} \\quad a^{2}=\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{r} \\frac{d r}{d t} \\frac{d \\theta}{d t} \\quad a^{3}=\\frac{d^{2} z}{d t^{2}} \\tag{2}\n\\end{equation*}\n\n\nThe physical components are then obtained from (1) as\n\n\n\\begin{equation*}\na_{(r)}=\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2} \\quad a_{(\\theta)}=r \\frac{d^{2} \\theta}{d t^{2}}+2 \\frac{d r}{d t} \\frac{d \\theta}{d t} \\quad a_{(z)}=\\frac{d^{2} z}{d t^{2}} \\tag{3}\n\\end{equation*}\n\n\nOnly the $\\theta$-component differs between (2) and (3); but the difference is significant. For instance, the coriolis acceleration of a particle is $2 \\dot{r} \\dot{\\theta}$, as in (3).\n\n\\subsection*{11.4 NEWTON'S SECOND LAW IN CURVILINEAR COORDINATES}\nThe momentum vector of a particle of mass $m$ is defined as $\\mathbf{M}=m \\mathbf{v}$. Relative to a rectangular coordinate system (with the property of being an inertial frame), Newton's second law of motion effectively defines the force vector acting on a particle as $\\mathbf{F}=d \\mathbf{M} / d t$. Accordingly, in curvilinear coordinates $\\left(x^{i}\\right)$, the law reads:\n\n\n\\begin{equation*}\n\\mathbf{F}=\\frac{\\delta \\mathbf{M}}{\\delta t}=m \\frac{\\delta \\mathbf{v}}{\\delta t}=m \\mathbf{a} \\tag{11.11}\n\\end{equation*}\n\n\nassuming a constant mass. Therefore, the contravariant components of force are given by\n\n\n\\begin{equation*}\nF^{i}=m a^{i}=m\\left(\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{r s}^{i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right) \\tag{11.12a}\n\\end{equation*}\n\n\nand the covariant components by\n\n\n\\begin{equation*}\nF_{i}=g_{i r} F^{r}=m\\left(g_{i r} \\frac{d^{2} x^{r}}{d t^{2}}+\\Gamma_{r s i} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}\\right) \\tag{11.12b}\n\\end{equation*}\n\n\nBy introducing a scalar invariant called the kinetic energy of the particle,\n\n$$\nT \\equiv \\frac{1}{2} m v^{2}=\\frac{1}{2} m g_{i j} v^{i} v^{j}\n$$\n\none can (see Problem 11.5) put (11.12b) into the equivalent Lagrangian form\n\n\n\\begin{equation*}\nF_{i}=\\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial v^{i}}\\right)-\\frac{\\partial T}{\\partial x^{i}} \\tag{11.13}\n\\end{equation*}\n\n\nThe partial derivatives in (11.13) are taken with $T$ considered as a function of six independent variables, the $x^{i}$ (via the $g_{i j}$ ) and the $v^{i}$.\n\nEXAMPLE 11.3 (Motion under a Central Force) (a) Obtain the differential equation for the trajectory of a particle acted on by a force that is always directed from (or toward) a fixed point $O$. (b) Solve the equation of (a) when the central force is gravitational, thus determining the orbit of a satellite.\n\n(a) By Problem 11.18, the motion will be confined to a plane through $O$. Take $O$ as the origin of polar coordinates $\\left(x^{1}, x^{2}\\right)=(r, \\theta)$ in the plane; the force field then has the form $\\mathbf{F}=\\left(F^{1}, 0\\right)$. Taking the acceleration components from (2) of Example 11.2, we have as the equations of motion:\n\n$$\n\\begin{aligned}\n& F^{1}=m a^{1}=m\\left[\\frac{d^{2} r}{d t^{2}}-r\\left(\\frac{d \\theta}{d t}\\right)^{2}\\right] \\\\\n& 0=m a^{2}=m\\left[\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{r} \\frac{d r}{d t} \\frac{d \\theta}{d t}\\right] \\equiv \\frac{m}{r^{2}} \\frac{d}{d t}\\left(r^{2} \\frac{d \\theta}{d t}\\right)\n\\end{aligned}\n$$\n\nThe $\\theta$-equation has the first integral\n\n$$\nr^{2} \\frac{d \\theta}{d t}=q=\\text { const. }\n$$\n\n(conservation of angular momentum), which can be used to change the parameter of the trajectory from $t$ to $\\theta$. Thus, writing $u=1 / r$, we have:\n\n$$\n\\begin{aligned}\n& \\frac{d r}{d t}=\\frac{d \\theta}{d t} \\frac{d u^{-1}}{d \\theta}=\\left(q u^{2}\\right)\\left(-u^{-2} \\frac{d u}{d \\theta}\\right)=-q \\frac{d u}{d \\theta} \\\\\n& \\frac{d^{2} r}{d t^{2}}=-q \\frac{d^{2} u}{d \\theta^{2}}\\left(q u^{2}\\right)=-q^{2} u^{2} \\frac{d^{2} u}{d \\theta^{2}}\n\\end{aligned}\n$$\n\nand the $r$-equation becomes\n\n\n\\begin{equation*}\n\\frac{d^{2} u}{d \\theta^{2}}+u=g(u, \\theta) \\tag{1}\n\\end{equation*}\n\n\nin which $g(u, \\theta)=-F^{1}\\left(u^{-1}, \\theta\\right) / m q^{2} u^{2}$.\\\\\n(b) For the gravitational field, $F^{1}=-k / r^{2}=-k u^{2}$ ( $k>0$; attractive force), so that $g(u, \\theta)=Q=$ const. and the solution of (1) is $u=P \\cos \\theta+Q$, or\n\n\n\\begin{equation*}\nr=\\frac{1 / Q}{1+e \\cos \\theta} \\tag{2}\n\\end{equation*}\n\n\nwhich is a conic having eccentricity $e=P / Q$ and focus at $O$, the classical result.\n\n\\subsection*{11.5 DIVERGENCE, LAPLACIAN, CURL}\nThe divergence of a contravariant vector $\\mathbf{u}=\\left(u^{i}\\right)$ on $\\mathbf{E}^{3}$ is defined by (9.11), with use of Problem 8.14:\n\n$$\n\\operatorname{div} \\mathbf{u}=u_{, i}^{i}=\\frac{\\partial u^{i}}{\\partial x^{i}}+\\Gamma_{r i}^{i} u^{r}=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{r} \\frac{\\partial}{\\partial x^{r}}(\\ln \\sqrt{g})=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}(\\sqrt{g})\n$$\n\nThe product rule for partial differentiation yields the compact form\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{u}=\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} u^{i}\\right) \\tag{11.14}\n\\end{equation*}\n\n\nAnother notation for the divergence is $\\boldsymbol{\\nabla} \\cdot \\mathbf{u}$.\n\nThe Laplacian of a scalar field $f$ is given by $\\nabla^{2} f \\equiv \\operatorname{div}(\\operatorname{grad} f)$. Since in general coordinates divergence is defined for contravariant tensors only, while grad $f=\\left(\\partial f / \\partial x^{i}\\right)$ is a covariant tensor (Example 3.5), we first raise the subscript and then find the divergence by (11.14):\n\n\n\\begin{equation*}\n\\nabla^{2} f=\\operatorname{div}\\left(g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right)=\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right) \\tag{11.15}\n\\end{equation*}\n\n\nThe Laplacian figures importantly in electromagnetic theory via the scalar wave equation,\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} f}{\\partial t^{2}}=k^{2} \\nabla^{2} f \\quad(k=\\text { const. }=\\text { wave speed }) \\tag{11.16a}\n\\end{equation*}\n\n\nIn cartesian coordinates only, one defines the Laplacian of a vector field as $\\nabla^{2} \\mathbf{u} \\equiv\\left(\\nabla^{2} u^{i}\\right)$, where $\\nabla^{2} u^{i}=u_{x x}^{i}+u_{y y}^{i}+u_{z z}^{i}$, and writes the vector wave equation,\n\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\mathbf{u}}{\\partial t^{2}}=k^{2} \\nabla^{2} \\mathbf{u} \\tag{11.16b}\n\\end{equation*}\n\n\nas an abbreviation for three scalar wave equations.\n\nEXAMPLE 11.4 Write the Laplacian for cvlindrical coordinates.\n\nUsing $g^{11}=1, g^{22}=1 /\\left(x^{1}\\right)^{2}, g^{33}=1$, and $g=\\left(x^{1}\\right)^{2}$ in (11.15),\n\n$$\n\\begin{aligned}\n\\nabla^{2} f & =\\frac{1}{x^{1}}\\left[\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\frac{\\partial f}{\\partial x^{1}}\\right)+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{1}{x^{1}} \\frac{\\partial f}{\\partial x^{2}}\\right)+\\frac{\\partial}{\\partial x^{3}}\\left(x^{1} \\frac{\\partial f}{\\partial x^{3}}\\right)\\right] \\quad\\left(x^{1}>0\\right) \\\\\n& =f_{11}+\\frac{1}{\\left(x^{1}\\right)^{2}} f_{22}+f_{33}+\\frac{1}{x^{1}} f_{1}\n\\end{aligned}\n$$\n\nthe last line employing subscript notation for the partial derivatives.\n\nThe curl of a vector field $\\mathbf{u}=\\left(u^{i}\\right)-$ symbolized curl $\\mathbf{u}, \\boldsymbol{\\nabla} \\times \\mathbf{u}$, or rot $\\mathbf{u}-$ is given in a rectangular coordinate system $\\left(x^{i}\\right)$ by\n\n\n\\begin{equation*}\n\\operatorname{curl} \\mathbf{u} \\equiv\\left(e_{i j k} \\frac{\\partial u^{k}}{\\partial x^{j}}\\right) \\tag{11.17a}\n\\end{equation*}\n\n\nwhere $e_{i j k}$ is the permutation symbol (Chapter 3). The definition may be rewritten as a determinantal operator:\n\n\\[\n\\operatorname{curl} \\mathbf{u} \\equiv\\left|\\begin{array}{ccc}\n\\mathbf{e}_{1} & \\mathbf{e}_{2} & \\mathbf{e}_{3}  \\tag{11.17b}\\\\\n\\frac{\\partial}{\\partial x^{1}} & \\frac{\\partial}{\\partial x^{2}} & \\frac{\\partial}{\\partial x^{3}} \\\\\nu^{1} & u^{2} & u^{3}\n\\end{array}\\right|\n\\]\n\nin which $\\left(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right)=(\\mathbf{i}, \\mathbf{j}, \\mathbf{k})$ is the standard orthonormal basis. Unlike the gradient and the divergence, the curl cannot be extended to curvilinear coordinate systems by a tensor formula.\n\nRemark 1: Not everything in mathematical physics is a tensor. Problem 11.11 shows that (11.17) defines a direct cartesian tensor, but that is all. This is not to say that the curl operator cannot be formulated and used in curvilinear coordinates (see any text in vector analysis). It is only that the curl in spherical coordinates (say) and the curl in rectangular coordinates are not related tensorwise.\n\n\\section*{Nonrelativistic Maxwell's Equations}\nLet $\\quad \\mathbf{E}=$ electric field strength\n\n$\\mathbf{D}=$ electric displacement\n\n$\\mathbf{H}=$ magnetic field strength\n\n$\\mathbf{B}=$ magnetic induction\n\n$\\mathbf{J}=$ current density\n\n$\\rho=$ charge density\n\n$\\epsilon=$ dielectric constant\n\n$\\mu=$ magnetic permeability\n\n$c=$ velocity of light\n\nThen the famous Maxwell's equations may be written as follows:\n\n\\[\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}+\\frac{1}{c} \\frac{\\partial \\mathbf{B}}{\\partial t}=0 & \\operatorname{div} \\mathbf{B}=0  \\tag{11.18}\\\\\n\\operatorname{curl} \\mathbf{H}-\\frac{1}{c} \\frac{\\partial \\mathbf{D}}{\\partial t}=\\frac{1}{c} \\mathbf{J} & \\operatorname{div} \\mathbf{D}=\\rho\n\\end{array}\n\\]\n\nFrom standard formulas in electromagnetic theory, $\\mathbf{D}=\\epsilon \\mathbf{E}, \\mathbf{B}=\\mu \\mathbf{H}$, and $\\mathbf{J}=\\rho \\mathbf{u}$, where $\\mathbf{u}$ denotes the velocity field of the charge distribution; (11.18) becomes\n\n$$\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}=-\\frac{\\mu}{c} \\frac{\\partial \\mathbf{H}}{\\partial t} & \\operatorname{div} \\mathbf{H}=0 \\\\\n\\operatorname{curl} \\mathbf{H}=\\frac{\\epsilon}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}+\\frac{\\rho}{c} \\mathbf{u} & \\operatorname{div} \\mathbf{E}=\\frac{\\rho}{\\epsilon}\n\\end{array}\n$$\n\nIf the charge distribution is in free space $\\left(\\epsilon=\\epsilon_{0}, \\mu=\\mu_{0}\\right)$, a proper choice of units brings the equations into the form\n\n\\[\n\\begin{array}{ll}\n\\operatorname{curl} \\mathbf{E}=-\\frac{1}{c} \\frac{\\partial \\mathbf{H}}{\\partial t} & \\operatorname{div} \\mathbf{H}=0  \\tag{11.19}\\\\\n\\operatorname{curl} \\mathbf{H}=\\frac{1}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}+\\frac{\\rho}{c} \\mathbf{u} & \\operatorname{div} \\mathbf{E}=\\rho\n\\end{array}\n\\]\n\nWork with Maxwell's equations requires the vector identities listed below (see Problems 11.10 and 11.21).\n\n\n\\begin{align*}\n& \\boldsymbol{\\nabla} \\cdot(\\boldsymbol{\\nabla} \\times \\mathbf{u})=0 \\quad \\text { (for any } \\mathbf{u})  \\tag{11.20}\\\\\n& \\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\nabla(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})-\\nabla^{2} \\mathbf{u}  \\tag{11.21}\\\\\n& \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})=\\boldsymbol{\\nabla} \\cdot \\frac{\\partial \\mathbf{u}}{\\partial t}  \\tag{11.22}\\\\\n& \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\boldsymbol{\\nabla} \\times \\frac{\\partial \\mathbf{u}}{\\partial t} \\tag{11.23}\n\\end{align*}\n\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{VELOCITY AND ACCELERATION}\n11.1 Find the velocity and acceleration vectors and the scalars $v$ and $a$ for a particle whose equation of motion (along a twisted cubic) is $\\mathbf{x}=\\left(t, t^{2}, t^{3}\\right) \\quad(-1 \\leqq t \\leqq 1)$. Determine the extreme values of $v$ and $a$, and where they are assumed.\n\n$$\n\\begin{array}{lll}\n\\mathbf{v}=\\left(1,2 t, 3 t^{2}\\right) & \\text { and } & v=\\sqrt{1+4 t^{2}+9 t^{4}} \\\\\n\\mathbf{a}=(0,2,6 t) & \\text { and } & a=\\sqrt{4+36 t^{2}}\n\\end{array}\n$$\n\nHence $v$ and $a$ have maxima at $t= \\pm 1$, where $v=\\sqrt{14}$ and $a=\\sqrt{40}$. They have minima at $t=0$, where $v=1$ and $a=2$.\n\n\\section*{PARTICLE DYNAMICS}\n11.2 A particle travels at constant speed $v$ on a curve with positive curvature. Show that its acceleration is greatest where the curvature is greatest.\n\nBy (11.8) with $\\dot{v}=0, a=\\kappa v^{2}$ or $a / \\kappa=$ const.\n\n11.3 Compute the contravariant acceleration components in a coordinate system $\\left(x^{i}\\right)$ connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ by $\\bar{x}^{1}=\\left(x^{1}\\right)^{2}, \\bar{x}^{2}=x^{2}, \\bar{x}^{3}=x^{3}$.\n\nUse (5.7):\n\n$$\nG=J^{T} J=\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n4\\left(x^{1}\\right)^{2} & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nHence, the Christoffel symbols are given by\n\n$$\n\\Gamma_{11}^{1}=\\frac{\\partial}{\\partial x^{1}}\\left[\\frac{1}{2} \\ln 4\\left(x^{1}\\right)^{2}\\right]=\\frac{1}{x^{1}} \\quad \\text { (all others zero) }\n$$\n\nand (11.9) gives\n\n$$\na^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\frac{1}{x^{1}}\\left(\\frac{d x^{1}}{d t}\\right)^{2} \\quad a^{2}=\\frac{d^{2} x^{2}}{d t^{2}} \\quad a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}\n$$\n\n\\section*{NEWTON'S SECOND LAW}\n11.4 Show that Newton's second law is consistent with Newton's first law: A particle that is not acted upon by an outside force is at rest or is in motion along a straight line at constant velocity. Assume a rectangular coordinate system.\n\n$\\mathbf{F}=\\mathbf{0}$ implies $d \\mathbf{v} / d t=\\mathbf{0}$, or $\\mathbf{v}=\\mathbf{d}$ (constant). Then,\n\n$$\n\\frac{d \\mathbf{x}}{d t}=\\mathbf{d} \\quad \\text { or } \\quad \\mathbf{x}=t \\mathbf{d}+\\mathbf{x}_{0}\n$$\n\nwhich is the parametric equation for a point (if $\\mathbf{d}=\\mathbf{0}$ ) or for a straight line (if $\\mathbf{d} \\neq \\mathbf{0}$ ), along which $v=\\|\\mathbf{d}\\|=$ const.\n\n11.5 Prove the equivalence of (11.13) and (11.12b).\n\nFor simplicity, take $m=1$ in (11.13). By the chain rule and the symmetry of $\\left(g_{i j}\\right)$,\n\n$$\n\\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial v^{i}}\\right)-\\frac{\\partial T}{\\partial x^{i}}=\\frac{d}{d t}\\left(g_{i r} v^{r}\\right)-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}+\\frac{d g_{i r}}{d t} v^{r}\n$$\n\n$$\n\\begin{aligned}\n& =g_{i r} \\frac{d v^{r}}{d t}-g_{r s i}\\left(\\frac{1}{2} v^{r} v^{s}\\right)+\\frac{\\partial g_{i r}}{\\partial x^{s}} \\frac{d x^{s}}{d t} v^{r}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+g_{i r s} v^{s} v^{r} \\\\\n& =g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+\\frac{1}{2} g_{s i r} v^{s} v^{r}+\\frac{1}{2} g_{i r s} v^{s} v^{r}=g_{i r} \\frac{d v^{r}}{d t}+\\Gamma_{r s i} v^{r} v^{s}\n\\end{aligned}\n$$\n\nThe final expression is exactly the right-hand side of $(11.12 b)$ (for $m=1$ ).\n\n11.6 Solve (1) of Example 11.3 when the force field is of the form\n\n$$\ng(u, \\theta)=A u+h(\\theta)\n$$\n\nwhere $A$ is a constant and $h(\\theta)$ is periodic of period $2 \\pi$.\n\nWith primes denoting $\\theta$-derivatives, we must solve\n\n$$\nu^{\\prime \\prime}+u=A u+h(\\theta) \\quad \\text { or } \\quad u^{\\prime \\prime}+(1-A) u=h(\\theta)\n$$\n\nThe general solution to the homogeneous equation is\n\n$$\nu= \\begin{cases}P \\cos (\\sqrt{1-A} \\theta+\\alpha) & A<1 \\\\ \\alpha \\theta+\\beta & A=1 \\\\ Q \\exp (\\sqrt{A-1} \\theta)+R \\exp (-\\sqrt{A-1} \\theta) & A>1\\end{cases}\n$$\n\nA particular solution of the nonhomogeneous equation may be obtained in the form $u=u_{H} w$, where $u_{H}$ is any particular solution of the homogeneous equation. In fact, substitution in the differential equation yields\n\n$$\n2 u_{H}^{\\prime} w^{\\prime}+u_{H} w^{\\prime \\prime}=h \\quad \\text { or } \\quad\\left(u_{H}^{2} w^{\\prime}\\right)^{\\prime}=u_{H} h\n$$\n\nand this last equation can be solved by two quadratures:\n\n$$\nw^{\\prime}(\\theta)=\\frac{1}{u_{H}^{2}(\\theta)} \\int_{0}^{\\theta} u_{H}(\\phi) h(\\phi) d \\phi \\quad \\text { and } \\quad w(\\theta)=\\int_{0}^{\\theta} \\frac{d \\psi}{u_{H}^{2}(\\psi)} \\int_{0}^{\\psi} u_{H}(\\phi) h(\\phi) d \\phi\n$$\n\nThe integrals are easily evaluated when $h(\\phi)$ is represented as a Fourier series.\n\n11.7 If $h(\\theta)=0$ in Problem 11.6, identify the orbits corresponding to (a) $A=0,(b) A=1,(c)$ $A=5 / 4$.\n\n(a) The curve $1 / r=P \\cos (\\theta+\\alpha)$, or $r \\cos (\\theta+\\alpha)=1 / P$, is a straight line (Fig. 11-1).\n\n(b) The curve $1 / r=\\alpha \\theta+\\beta$ is a hyperbolic spiral that degenerates into a circle for $\\alpha=0$.\n\n(c) The curve $1 / r=Q e^{\\theta / 2}+R e^{-\\theta / 2}$ is a complex spiral which, in the case $Q=0, R=1$, reduces to the simple logarithmic spiral $r=e^{\\theta / 2}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-169}\n\\end{center}\n\nFig. 11-1\n\n\\section*{DIFFERENTIAL OPERATORS}\n11.8 Calculate the Laplacian for spherical coordinates by the tensor formula. (The calculation is very tedious by other methods.)\n\nWe have\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right] \\quad G^{-1}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{-2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{-2}\n\\end{array}\\right]\n$$\n\nand $g=\\left(x^{1}\\right)^{4} \\sin ^{2} x^{2}$, so that in (11.15),\n\nTherefore,\n\n$$\n\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right)\\left(g^{i 1} \\frac{\\partial f}{\\partial x^{1}}+g^{i 2} \\frac{\\partial f}{\\partial x^{2}}+g^{i 3} \\frac{\\partial f}{\\partial x^{3}}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\sqrt{g} g^{1 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}} \\\\\n& \\sqrt{g} g^{2 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1}\\right)^{2}} \\frac{\\partial f}{\\partial x^{2}}=\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}} \\\\\n& \\sqrt{g} g^{3 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1} \\sin x^{2}\\right)^{2}} \\frac{\\partial f}{\\partial x^{3}}=\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\n\\end{aligned}\n$$\n\nand so\n\n$$\n\\begin{aligned}\n& \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right]=\\frac{\\partial}{\\partial x^{1}} {\\left[\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}\\right]+\\frac{\\partial}{\\partial x^{2}}\\left[\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}\\right]+\\frac{\\partial}{\\partial x^{3}}\\left[\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\\right] } \\\\\n&=2 x^{1}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}+\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{1}\\right)^{2}}+\\left(\\cos x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}+\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{2}\\right)^{2}} \\\\\n&+\\left(\\csc x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{3}\\right)^{2}}\n\\end{aligned}\n$$\n\nIn writing the final steps we convert to $\\rho=x^{1}, \\varphi=x^{2}$, and $\\theta=x^{3}$ :\n\n$$\n\\begin{aligned}\n\\nabla^{2} f & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right] \\\\\n& =\\frac{1}{\\rho^{2} \\sin \\varphi}\\left[(2 \\rho \\sin \\varphi) \\frac{\\partial f}{\\partial \\rho}+\\left(\\rho^{2} \\sin \\varphi\\right) \\frac{\\partial^{2} f}{\\partial \\rho^{2}}+(\\cos \\varphi) \\frac{\\partial f}{\\partial \\varphi}+(\\sin \\varphi) \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+(\\csc \\varphi) \\frac{\\partial^{2} f}{\\partial \\theta^{2}}\\right] \\\\\n& =\\frac{\\partial^{2} f}{\\partial \\rho^{2}}+\\frac{1}{\\rho^{2}} \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+\\frac{1}{\\rho^{2} \\sin ^{2} \\varphi} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{2}{\\rho} \\frac{\\partial f}{\\partial \\rho}+\\frac{\\cot \\varphi}{\\rho^{2}} \\frac{\\partial f}{\\partial \\varphi}\n\\end{aligned}\n$$\n\n11.9 Calculate the divergence in spherical coordinates $(\\rho, \\varphi, \\theta)$ of $(a)$ a contravariant vector, $\\mathbf{u}=\\left(u^{i}\\right) ;(b)$ a vector specified by its physical components, $\\mathbf{u}=u_{(1)} \\mathbf{e}_{1}+u_{(2)} \\mathbf{e}_{2}+u_{(3)} \\mathbf{e}_{3}$.\n\n(a) We plug into the formula (11.14):\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} u^{i}\\right)=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}(\\sqrt{g}) \\\\\n& =\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\left(x^{1}\\right)^{2} \\sin x^{2}} \\frac{\\partial}{\\partial x^{i}}\\left[\\left(x^{1}\\right)^{2} \\sin x^{2}\\right] \\\\\n& =\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{1}\\left(\\frac{2}{x^{1}}\\right)+u^{2}\\left(\\frac{\\cos x^{2}}{\\sin x^{2}}\\right)+u^{3}(0)\n\\end{aligned}\n$$\n\nThus\n\n$$\n\\operatorname{div} \\mathbf{u}=\\frac{\\partial u^{1}}{\\partial \\rho}+\\frac{\\partial u^{2}}{\\partial \\varphi}+\\frac{\\partial u^{3}}{\\partial \\theta}+\\frac{2}{\\rho} u^{1}+(\\cot \\varphi) u^{2}\n$$\n\n(b) By Example 11.2, we apply (11.4) to the contravariant vector having components\n\n$$\nu^{1}=\\frac{u_{(1)}}{1} \\quad u^{2}=\\frac{u_{(2)}}{x^{1}} \\quad u^{3}=\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\n$$\n\nHence, from (a),\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{\\partial}{\\partial x^{1}} u_{(1)}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{(2)}}{x^{1}}\\right)+\\frac{\\partial}{\\partial x^{3}}\\left(\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\\right)+u_{(1)}\\left(\\frac{2}{x^{1}}\\right)+\\frac{u_{(2)}}{x^{1}}\\left(\\cot x^{2}\\right) \\\\\n& =\\frac{\\partial u_{(\\rho)}}{\\partial \\rho}+\\frac{1}{\\rho} \\frac{\\partial u_{(\\varphi)}}{\\partial \\varphi}+\\frac{1}{\\rho \\sin \\varphi} \\frac{\\partial u_{(\\theta)}}{\\partial \\theta}+\\frac{2}{\\rho} u_{(\\rho)}+\\frac{\\cot \\varphi}{\\rho} u_{(\\varphi)}\n\\end{aligned}\n$$\n\nIt is in this last form that \"the divergence in spherical coordinates\" is generally encountered in reference books.\n\n11.10 Establish in rectangular coordinates the identity\n\n\n\\begin{equation*}\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\nabla(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})-\\nabla^{2} \\mathbf{u} \\tag{1}\n\\end{equation*}\n\n\n(\"curl curl equals grad div minus del-square\").\n\nBoth sides of (1) are (cartesian) vectors; we shall show that they are componentwise equal.\n\nBy (11.7), the $i$ th component of curl $\\mathbf{u}$ is $e_{i j k}\\left(\\partial u^{k} / \\partial x^{j}\\right)$. Therefore, the $i$ th component of curl (curl $\\mathbf{u}$ ) is [use (3.23)]:\n\n$$\n\\begin{aligned}\ne_{i r s} \\frac{\\partial}{\\partial x^{r}}\\left(e_{s j k} \\frac{\\partial u^{k}}{\\partial x^{i}}\\right) & =e_{i r s} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=e_{s i r} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}} \\\\\n& =\\left(\\delta_{i j} \\delta_{r k}-\\delta_{i k} \\delta_{r j}\\right) \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=\\frac{\\partial^{2} u^{r}}{\\partial x^{r} \\partial x^{i}}-\\frac{\\partial^{2} u^{i}}{\\partial x^{r} \\partial x^{r}} \\\\\n& \\equiv \\frac{\\partial}{\\partial x^{i}}(\\operatorname{div} \\mathbf{u})-\\nabla^{2} u^{i}\n\\end{aligned}\n$$\n\nThe first term on the right is recognized as the $i$ th component of grad (div $\\mathbf{u}$ ), and the second term is (by definition) the $i$ th component of the Laplacian of the vector $\\mathbf{u}$. QED\n\n11.11 Prove that the array represented in rectangular coordinates $\\left(x^{i}\\right)$ by\n\n$$\n\\operatorname{curl} \\mathbf{u}=\\left(e_{i j k} \\frac{\\partial u^{k}}{\\partial x^{j}}\\right)\n$$\n\nis a direct cartesian tensor.\n\nIt suffices to show that $\\left(e_{i j k}\\right)$ is a direct cartesian tensor, since $\\left(\\partial u^{i} / \\partial x^{j}\\right)$ is known to be a (direct) cartesian tensor. Therefore, given the orthogonal transformation $\\bar{x}^{i}=a_{j}^{i} x^{j}$, with $\\left|a_{j}^{i}\\right|=+1$, define the $3^{3}=27$ quantities\n\n$$\n\\tau_{i j k} \\equiv e_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=e_{r s t} a_{r}^{i} a_{s}^{j} a_{t}^{k}\n$$\n\nWe observe that:\n\n(i) $\\tau_{i j k}=0$ when two subscripts have the same value; e.g.,\n\n\n\\begin{align*}\n\\tau_{i 22}=e_{r s t} a_{r}^{i} a_{s}^{2} a_{t}^{2} & =-e_{r t s} a_{r}^{i} a_{s}^{2} a_{t}^{2}=-e_{r s t} a_{r}^{i} a_{t}^{2} a_{s}^{2}=-\\tau_{i 22} \\\\\n\\tau_{123} & =e_{r s t} a_{r}^{1} a_{s}^{2} a_{t}^{3}=\\left|a_{j}^{i}\\right|=+1 \\tag{ii}\n\\end{align*}\n\n\n(iii) $\\tau_{i j k}$ changes sign when any two subscripts are interchanged; e.g.\n\n$$\n\\tau_{k j i}=e_{r s t} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-e_{t s r} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-\\tau_{i j k}\n$$\n\nBut these three properties identify $\\tau_{i j k}$ with $\\bar{e}_{i j k}$, and the proof is complete.\n\n11.12 Show that in a vacuum with zero charge $(\\rho=0)$, the electric field $\\mathbf{E}$ satisfies the vector wave equation\n\n$$\n\\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}=c^{2} \\nabla^{2} \\mathbf{E}\n$$\n\nFrom Maxwell's equations (11.19), along with the identity (11.23),\n\n$$\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\frac{1}{c} \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\times \\mathbf{H})=-\\frac{1}{c}\\left(\\frac{1}{c} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\\right)=-\\frac{1}{c^{2}} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\n$$\n\nBut $\\boldsymbol{\\nabla} \\cdot \\mathbf{E}=0$ and Problem 11.10 imply $\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\nabla^{2} \\mathbf{E}$, and the wave equation follows.\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n11.13 Show that if $v$ is constant, a particle describes equal lengths of arc in equal periods of time.\n\n11.14 (a) Show that a particle whose path is given by $\\mathbf{x}=(\\cos t, \\sin t, \\cot t)$, for $\\pi / 4 \\leqq t<\\pi / 2$, has velocity decreasing to $\\sqrt{2}$ as $t \\rightarrow \\pi / 2$. (b) What is the behavior of the acceleration as $t \\rightarrow \\pi / 2$ ? (c) Find the extreme values of $v$ and $a$ for this particle.\n\n11.15 For what kind of motion, if any, is $a=d v / d t$ ?\n\n11.16 Develop a formula for \u00e0 for a particle that has constant speed $v$.\n\n11.17 Calculate the acceleration components (contravariant) in spherical coordinates $(\\rho, \\varphi, \\theta)$.\n\n11.18 Prove that motion under a central force is planar.\n\n11.19 Calculate the Laplacian for cylindrical coordinates $(r, \\theta, z)$.\n\n11.20 Show that $\\nabla^{2} f=g^{i j} f_{, i j}$. [Hint: Write (11.15) at the origin of Riemannian coordinates.]\n\n11.21 Prove (11.22) and (11.23).\n\n11.22 Prove that $\\operatorname{curl}(\\operatorname{grad} f)=\\mathbf{0}$ for any $C^{2}$ scalar field $f$.\n\n11.23 Show that in a charge-free vacuum, $\\mathbf{H}$ also satisfies the vector wave equation.\n\n11.24 Show that, relative to an orthogonal curvilinear coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$, an arbitrary contravariant vector $\\mathbf{v}=\\left(v^{i}\\right)$ has the representation\n\n$$\n\\mathbf{v}=v_{(1)} \\mathbf{e}_{1}+v_{(2)} \\mathbf{e}_{2}+v_{(3)} \\mathbf{e}_{3}\n$$\n\nwhere $v_{(\\alpha)}$ is the physical component and $\\mathbf{e}_{\\alpha}$ is the unit normal to the surface $x^{\\alpha}=$ const. [Hint: Use Problems 5.19 and 5.20].\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 12}", "\\section*{Tensors in Special Relativity}\n\\subsection*{12.1 INTRODUCTION}\nIf the motion of a light pulse were an ordinary phenomenon, its velocity $c$ to one observer would appear to a second observer, moving at velocity $v$ relative to the first, to have the value $c-v$. This hypothetical property of light depends on the concept of absolute time measurement for all observers. However, beginning with the landmark Michelson-Morley experiment in 1880, all experimental data force us to abandon this reasonable hypothesis and to accept instead the now undisputed fact that the velocity of light, rather than the measurement of time, is an absolute of nature. Light is observed to have a single velocity, $c=2.9979 \\times 10^{8} \\mathrm{~m} / \\mathrm{s}$, independent of the observer's motion away from or towards its source. This calls for adjustments to the equations of Newtonian mechanics which become major when high-velocity particles are involved.\n\n\\subsection*{12.2 EVENT SPACE}\nIt is first necessary to wed the concepts of time and space. Thus, each event (atomic collision, flash of lightning, etc.) is assigned four coordinates $(t, x, y, z)$, where $t$ is the time (in seconds) of the event and $(x, y, z)$ is the location (in meters) of the event in ordinary rectangular coordinates. Such coordinates are called space-time coordinates.\n\nDefinition 1: An event space is an $\\mathbf{R}^{4}$ whose points are events, coordinatized by $\\left(x^{i}\\right)=\\left(x^{0}, x^{1}, x^{2}, x^{3}\\right)$, where $x^{0}=c t$ is the temporal coordinate, and $\\left(x^{1}, x^{2}, x^{3}\\right)=(x, y, z)$ the rectangular positional coordinates, of an event. Two events $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ are identical if $x_{1}^{i}=x_{2}^{i}$ for all $i$; simultaneous, if $x_{1}^{0}=x_{2}^{0}$; and copositional, if $x_{1}^{i}=x_{2}^{i}$ for $i=1,2,3$. The spatial distance between $E_{1}$ and $E_{2}$ is the number\n\n\n\\begin{equation*}\nd=\\sqrt{\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}} \\tag{12.1}\n\\end{equation*}\n\n\nwhere $\\Delta x^{i} \\equiv x_{2}^{i}-x_{1}^{i}$ for $i=1,2,3$.\n\n\\section*{Inertial Reference Frames}\nThe general setting for Einstein's Special Theory of Relativity (henceforth abbreviated SR) consists of two or more observers $O, \\bar{O}, \\overline{\\bar{O}}, \\ldots$, moving at constant velocities relative to each other, who set up space-time coordinate systems $\\left(x^{i}\\right),\\left(\\bar{x}^{i}\\right),\\left(\\bar{x}^{i}\\right), \\ldots$ to record events and make calculations for experiments they conduct. Such coordinate systems in uniform relative motion are called inertial frames provided Newton's first law is valid in each system. All the systems are assumed to have a common origin at some instant, which is taken as $t=\\bar{t}=\\overline{\\bar{t}}=\\cdots=0$.\n\n\\section*{Light Cone}\nA flash of light at position $(0,0,0)$ and time $t=0$ sends out an expanding spherical wave front, with equation $x^{2}+y^{2}+z^{2}=c^{2} t^{2}$, or\n\n\n\\begin{equation*}\n\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2}=0 \\tag{12.2}\n\\end{equation*}\n\n\n(12.2) is the equation of the light cone in event space, relative to the inertial frame $\\left(x^{i}\\right)$. Figure 12-1 shows the projection of the light cone onto the hyperplane $x^{3}=0$. In any other inertial frame, $\\left(\\bar{x}^{i}\\right)$, the equation of the light cone is exactly the same (since all observers measure the velocity of light as c):\n\n$$\n\\left(\\bar{x}^{0}\\right)^{2}-\\left(\\bar{x}^{1}\\right)^{2}-\\left(\\bar{x}^{2}\\right)^{2}-\\left(\\bar{x}^{3}\\right)^{2}=0\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-174}\n\\end{center}\n\nFig. 12-1\n\nIn Example 7.6 (using slightly different notation), we identified the light cone with the null geodesics of $\\mathbf{R}^{4}$ under the metric of SR.\n\n\\section*{Relativistic Length}\nFor an arbitrary event $E(\\mathbf{x})$, the quantity $\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2}$ may be positive, negative, or zero. The relativistic distance from $E(\\mathbf{x})$ to the origin $E_{0}(\\mathbf{0})$ is the real number $s \\geqq 0$ such that\n\n$$\n\\varepsilon s^{2}=\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2} \\quad(\\varepsilon= \\pm 1)\n$$\n\nMore generally, the length of interval or relativistic distance between $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ is the unique real number $\\Delta s \\geqq 0$ such that\n\n\n\\begin{equation*}\n\\varepsilon(\\Delta s)^{2}=\\left(\\Delta x^{0}\\right)^{2}-\\left(\\Delta x^{1}\\right)^{2}-\\left(\\Delta x^{2}\\right)^{2}-\\left(\\Delta x^{3}\\right)^{2} \\quad(\\varepsilon= \\pm 1) \\tag{12.3}\n\\end{equation*}\n\n\nwhere $\\Delta x^{i} \\equiv x_{2}^{i}-x_{1}^{i}$ for $i=0,1,2,3$. The chief significance of this length-concept is to be found in Theorem 12.1: Relativistic distance is an invariant across all inertial frames.\n\nFor a proof, see Problem 12.6.\n\n\\section*{Interval Types}\nThe interval between $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ is\n\n(1) spacelike if $\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}>\\left(\\Delta x^{0}\\right)^{2} \\quad$ (or $\\varepsilon=-1$; predominance of distance over time);\n\n(2) lightlike if $\\left(\\Delta x^{0}\\right)^{2}=\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2} \\quad$ (equality of time and distance);\n\n(3) timelike if $\\left(\\Delta x^{0}\\right)^{2}>\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2} \\quad$ (or $\\varepsilon=+1$; predominance of time over distance).\n\nBy Theorem 12.1, the categorization is independent of the particular inertial frame.\n\n\\subsection*{12.3 THE LORENTZ GROUP AND THE METRIC OF SR}\nImagine two observers, $\\dot{O}$ and $\\bar{O}$, in uniform relative motion at speed $v$. They approach each other during negative time, coincide at zero time, and then recede from each other during positive time (Fig. 12-2(a)). Let $O$ and $\\bar{O}$ set up independent reference frames $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ by means of identical but separate clocks, with $t=\\bar{t}=0$ when $O=\\bar{O}$, and identical metersticks. Newton's first law will be assumed to hold in both frames, making them inertial frames.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175(1)}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175}\n\\end{center}\n\n(b)\n\nFig. 12-2\n\nCommon observation of events sets up a correspondence\n\n\n\\begin{equation*}\n\\mathscr{T}: \\bar{x}^{i}=F^{i}\\left(x^{0}, x^{1}, x^{2}, x^{3}\\right) \\tag{12.4}\n\\end{equation*}\n\n\nthat is bijective, since each event is assigned a unique set of coordinates. In most of what follows, it will be assumed that $O$ and $\\bar{O}$ perform a simplifying maneuver at the instant of coincidence, whereby their $x$-axes point in the same direction along the line of motion and their $y$-axes and their $z$-axes coincide. In the ensuing translation, the $y$-axes and the $z$-axes remain parallel (Fig. 12-2(b)).\n\n\\section*{Postulates of SR}\n(1) Principle of Relativity: The laws of physics are the same in all inertial frames.\n\n(2) Invariance of Uniform Motion: A particle with constant velocity in one inertial frame will have constant velocity in all inertial frames.\n\n(3) Invariance of Light Speed: The speed of light is invariant across all inertial frames.\n\nPostulate 2 requires that the bijective transformation (12.4) be such as to map straight lines into straight lines. Consequently, each $F^{i}$ must be a linear function. Since $F^{i}(\\mathbf{0})=\\mathbf{0}$, constants $a_{j}^{i}$ exist such that\n\n\n\\begin{equation*}\nT: \\bar{x}^{i}=a_{j}^{i} x^{j} \\tag{12.5}\n\\end{equation*}\n\n\n\\section*{Lorentz Matrices and Transformations}\nThe invariance of the equation of the light cone (in consequence of Postulate 3) may be expressed as\n\n\n\\begin{equation*}\ng_{i j} x^{i} x^{j}=0=g_{i j} \\bar{x}^{i} \\bar{x}^{j} \\tag{12.6}\n\\end{equation*}\n\n\nwhere $g_{00}=1, g_{11}=g_{22}=g_{33}=-1$, and $g_{i j}=0$ for $i \\neq j$. Substitution of (12.5) in (12.6) yields (Problem 12.4):\n\n\n\\begin{equation*}\ng_{i j} a_{r}^{i} a_{s}^{j}=g_{r s} \\tag{12.7a}\n\\end{equation*}\n\n\nor, in matrix form,\n\nor, written out,\n\n\n\\begin{equation*}\nA^{T} G A=G \\tag{12.7b}\n\\end{equation*}\n\n\n\\[\n\\begin{array}{rlrl}\n\\left(a_{0}^{0}\\right)^{2}-\\left(a_{0}^{1}\\right)^{2}-\\left(a_{0}^{2}\\right)^{2}-\\left(a_{0}^{3}\\right)^{2} & =1 & \\\\\n\\left(a_{j}^{0}\\right)^{2}-\\left(a_{j}^{1}\\right)^{2}-\\left(a_{j}^{2}\\right)^{2}-\\left(a_{j}^{3}\\right)^{2}=-1 & & (j=1,2,3)  \\tag{12.7c}\\\\\na_{i}^{0} a_{j}^{0}-a_{i}^{1} a_{j}^{1}-a_{i}^{2} a_{j}^{2}-a_{i}^{3} a_{j}^{3} & =0 & & (i \\neq j)\n\\end{array}\n\\]\n\nIt is easy to see (Problem 12.8) that requiring $g_{i j} x^{i} x^{j}=0$ to be invariant is tantamount to requiring the invariance of $g_{i j} x^{i} x^{j}=q$ for every value of $q$. Thus, (12.7) is a criterion for the quadratic form $g_{i j} x^{i} x^{j}$ to be invariant.\n\nDefinition 2: Any $4 \\times 4$ matrix (or corresponding linear transformation) that preserves the quadratic form $\\mathbf{x}^{T} G \\mathbf{x}$ is called Lorentz.\n\nIn Problem 12.10 it is shown that the set of Lorentz matrices constitutes a group (the Lorentz group) under matrix multiplication.\n\n\\section*{Metric of SR}\nIf the terms $\\bar{g}_{i j} \\equiv g_{i j}$ are defined for the $\\left(\\bar{x}^{i}\\right)$-system, then (12.7a) becomes $g_{r s}=\\bar{g}_{i j} a_{r}^{i} a_{s}^{j}$, which makes $\\left(g_{i j}\\right)$ a covariant tensor of the second order under Lorentz transformations of coordinates. Accordingly, the metric for $\\mathbf{R}^{4}$ is chosen as\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \\equiv\\left(d x^{0}\\right)^{2}-\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2} \\tag{12.8}\n\\end{equation*}\n\n\n\\subsection*{12.4 SIMPLE LORENTZ MATRICES}\nLet us suppose that $O$ and $\\bar{O}$ have performed an alignment of their $x y z$-axes. Then any right circular cylinder with axis along the line of relative motion must have the same equation in the two systems; i.e., $\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}$ is invariant. It follows (see Problem 12.11) that the Lorentz transformation for this situation has the form\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{0}=a_{0}^{0} x^{0}+a_{1}^{0} x^{1} \\equiv a x^{0}+b x^{1}  \\tag{12.9}\\\\\n\\bar{x}^{1}=a_{0}^{1} x^{0}+a_{1}^{1} x^{1} \\equiv d x^{0}+e x^{1} \\\\\n\\bar{x}^{2}=x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array}\\right.\n\\]\n\nBy (12.7),\n\n\n\\begin{equation*}\na^{2}-d^{2}=1 \\quad b^{2}-e^{2}=-1 \\quad a b-d e=0 \\tag{12.10}\n\\end{equation*}\n\n\nBy considering the coordinates which $O$ and $\\bar{O}$ would assign to each other's origin (see Problem 12.12), we find that\n\n\n\\begin{equation*}\nd=-(v / c) a \\equiv-\\beta a \\quad \\text { and } \\quad a=e \\tag{12.11}\n\\end{equation*}\n\n\n(The notation $\\beta=v / c$ is standard in SR.) From (12.10) and the fact that $a>0$ (since both clocks can be assumed to run in the same sense), it follows that\n\n\n\\begin{equation*}\na=\\left(1-\\beta^{2}\\right)^{-1 / 2}=e \\quad b=-\\beta\\left(1-\\beta^{2}\\right)^{-1 / 2}=d \\tag{12.12}\n\\end{equation*}\n\n\nTherefore, the coordinate transformation takes on the simplified form\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{0}=\\frac{x^{0}-\\beta x^{1}}{\\sqrt{1-\\beta^{2}}}  \\tag{12.13}\\\\\n\\bar{x}^{1}=\\frac{-\\beta x^{0}+x^{1}}{\\sqrt{1-\\beta^{2}}} \\\\\n\\bar{x}^{2}=x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array} \\quad \\text { or } \\quad A=\\left[\\begin{array}{cccc}\n\\frac{1}{\\sqrt{1-\\beta^{2}}} & \\frac{-\\beta}{\\sqrt{1-\\beta^{2}}} & 0 & 0 \\\\\n\\frac{-\\beta}{\\sqrt{1-\\beta^{2}}} & \\frac{1}{\\sqrt{1-\\beta^{2}}} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\right.\n\\]\n\nAny $4 \\times 4$ matrix (linear transformation) of the form\n\n$$\nA=\\left[\\begin{array}{llll}\na & b & 0 & 0 \\\\\nb & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhere $a^{2}-b^{2}=1$, will be termed simple Lorentz. The relative velocity in the physical situation modeled by $A$ is recovered as $\\beta=-b / a$.\n\nEXAMPLE 12.1 By Problem 12.9, the inverse of a simple Lorentz matrix is\n\n$$\nA^{-1}=\\left[\\begin{array}{cccc}\na & -b & 0 & 0 \\\\\n-b & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhich is itself a simple Lorentz matrix, corresponding to a reversal in sign of $\\beta$. [If the velocity of $\\bar{O}$ with respect to $O$ is $v$, then the velocity of $O$ with respect to $\\bar{O}$ is $-v$.]\n\n\\section*{A Decomposition Theorem}\nThe possibility of simplifying an arbitrary Lorentz matrix by a suitable rotation of axes can be expressed in purely mathematical terms. (See Problems 12.14 and 12.15.)\n\nTheorem 12.2: An arbitrary Lorentz matrix $L=\\left(a_{j}^{i}\\right)$ has the representation\n\n$$\nL=R_{1} L^{*} R_{2}\n$$\n\nwhere $L^{*}$ is a simple Lorentz matrix with parameters $a=\\left|a_{0}^{0}\\right|=\\varepsilon a_{0}^{0}$ and $b=$ $-\\sqrt{\\left(a_{0}^{0}\\right)^{2}-1}$, and $R_{1}$ and $R_{2}$ are orthogonal Lorentz matrices defined by\n\n\\[\nR_{1}=L R_{2}^{T}\\left(L^{*}\\right)^{-1} \\quad \\text { and } \\quad R_{2}=\\left[\\begin{array}{llll}\n\\mathbf{e}_{1} & \\mathbf{r}^{\\prime} & \\mathbf{s}^{\\prime} & \\mathbf{t}^{\\prime} \\tag{12.14}\n\\end{array}\\right]^{T}\n\\]\n\nHere, $\\mathbf{e}_{1}=(1,0,0,0), \\mathbf{r}^{\\prime}=(\\varepsilon / b)\\left(0, a_{1}^{0}, a_{2}^{0}, a_{3}^{0}\\right) \\equiv(0, \\mathbf{r}), \\mathbf{s}^{\\prime}=(0, \\mathbf{s}), \\mathbf{t}^{\\prime}=(0, \\mathbf{t})$, with $\\mathbf{s}$ and $\\mathbf{t}$ chosen to complete a $3 \\times 3$ orthogonal matrix $\\left[\\begin{array}{lll}\\mathbf{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$.\n\nCorollary 12.3: If $L=\\left(a_{j}^{i}\\right)$ connects two inertial frames, then the relative velocity between the frames is\n\n\n\\begin{equation*}\nv=c \\sqrt{1-\\left(a_{0}^{0}\\right)^{-2}} \\tag{12.15}\n\\end{equation*}\n\n\n\\subsection*{12.5 PHYSICAL IMPLICATIONS OF THE SIMPLE LORENTZ TRANSFORMATION}\n\\section*{Length Contraction}\nFor any fixed $x^{0},(12.13)$ gives\n\n$$\n\\Delta \\bar{x}^{1}=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta x^{1} \\quad \\text { or } \\quad \\Delta x^{1}=\\sqrt{1-\\beta^{2}} \\Delta \\bar{x}^{1}<\\Delta \\bar{x}^{1}\n$$\n\nIf the frame $\\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, distances in $\\bar{O}$ appear to observer $O$ to be foreshortened in the direction of the motion by the factor $\\sqrt{1-\\beta^{2}}$.\n\n\\section*{Time Dilation}\nFor any fixed $\\bar{x}^{1},(12.13)$, inverted, gives\n\n$$\n\\Delta x^{0}=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta \\bar{x}^{0} \\quad \\text { or } \\quad \\Delta t=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta \\bar{t}>\\Delta \\bar{t}\n$$\n\nIf the frame $\\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, the clock of observer $\\bar{O}$ appears to observer $O$ to run slow by the factor $\\sqrt{1-\\beta^{2}}$.\n\n\\section*{Composition of Velocities}\nIf $\\bar{O}$ has velocity $v_{1}$ relative to $O$ and $\\overline{\\bar{O}}$ has velocity $v_{2}$ relative to $\\bar{O}$, then the Newtonian composition of velocities predicts that $\\bar{O}$ has velocity $v_{3}=v_{1}+v_{2}$ relative to $O$. Although the error does not show up unless $v_{1}$ and $v_{2}$ are substantial fractions of the velocity of light, SR shows the Newtonian theory to be incorrect. The correct formula (Problem 12.20) is\n\n\n\\begin{equation*}\nv_{3}=\\frac{v_{1}+v_{2}}{1+v_{1} v_{2} / c^{2}} \\tag{12.16}\n\\end{equation*}\n\n\n\\subsection*{12.6 RELATIVISTIC KINEMATICS}\n\\section*{4-Vectors}\nWe begin with ordinary velocity and acceleration of a particle within a single inertial frame $\\left(x^{i}\\right)$. By introducing the concept of proper time, we shall be able to obtain velocity and acceleration as contravariant vectors with respect to Lorentz transformations (to be called 4-vectors from now on). In general, $\\left(V^{i}\\right)$ is a 4-vector if it transforms according to the law $\\bar{V}^{i}=a_{j}^{i} V^{j}$, where $\\left(a_{j}^{i}\\right)$ is the Lorentz matrix of (12.5). It is customary to use the following notation for 4-vectors:\n\n$$\n\\left(V^{i}\\right) \\equiv\\left(V^{0}, \\mathbf{V}\\right) \\quad \\text { where } \\quad V^{0} \\equiv V_{t} \\quad \\text { and } \\quad \\mathbf{V} \\equiv\\left(V^{1}, V^{2}, V^{3}\\right) \\equiv\\left(V_{x}, V_{y}, V_{z}\\right)\n$$\n\n$V^{0}$ is referred to as the time component of the vector and $\\left(V_{x}, V_{y}, V_{z}\\right)$ are the usual space components. All indices are understood to range over the values $0,1,2,3$, unless specifically noted otherwise.\n\n\\section*{Nonrelativistic Velocity and Acceleration}\nIn the inertial frame $\\left(x^{i}\\right)=(x, y, z)$ let a particle describe the class $C^{2}$ curve\n\n$$\n\\mathscr{K}:\\left(x^{i}\\right)=(c t, \\mathbf{r}(t))=(c t, x(t), y(t), z(t))\n$$\n\nThen we have the classical formulas\n\n\n\\begin{equation*}\n\\left(v_{i}\\right)=\\left(\\frac{d x^{i}}{d t}\\right) \\equiv(c, \\mathbf{v}) \\tag{12.17}\n\\end{equation*}\n\n\nwhere $\\mathbf{v}=d \\mathbf{r} / d t$ and $\\hat{v} \\equiv\\|\\mathbf{v}\\|=\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}$;\n\n\n\\begin{equation*}\n\\left(a_{i}\\right)=\\left(\\frac{d^{2} x^{i}}{d t^{2}}\\right) \\equiv(0, \\mathbf{a}) \\tag{12.18}\n\\end{equation*}\n\n\nwhere $\\mathbf{a}=d \\mathbf{v} / d t$ and $\\hat{a}=\\|\\mathbf{a}\\|=\\sqrt{a_{x}^{2}+a_{y}^{2}+a_{z}^{2}}$.\n\nAs defined, neither the velocity nor the acceleration is a tensor under Lorentz transformations. In fact (Problem 12.22), if $\\left(\\bar{v}_{i}\\right)$ and $\\left(\\bar{a}_{i}\\right)$ are the like quantities in $\\left(\\bar{x}^{i}\\right)$, then (12.13) yields the relations\n\n\\[\n\\begin{array}{cc}\n\\bar{v}_{0}=c=v_{0} & \\bar{v}_{x}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\quad \\bar{v}_{y}=\\frac{v_{y} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\quad \\bar{v}_{z}=\\frac{v_{z} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\\\\n\\bar{a}_{0}=0=a_{0} \\quad \\bar{a}_{x}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\quad \\bar{a}_{y}=\\frac{\\left[a_{y}+\\left(v_{y} a_{x}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)\\right]\\left(1-\\beta^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\\\\n\\bar{a}_{z}=\\frac{\\left[a_{z}+\\left(v_{z} a_{x}-v_{x} a_{z}\\right)\\left(v / c^{2}\\right)\\right]\\left(1-\\beta^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\tag{12.20}\n\\end{array}\n\\]\n\nThe inverse relations can be quickly obtained by replacing $v$ by $-v$ and interchanging barred and unbarred terms throughout. For example, the second formula in (12.19) inverts to\n\n$$\nv_{x}=\\frac{\\bar{v}_{x}+v}{1+\\bar{v}_{x} v / c^{2}}\n$$\n\nwhich is just (12.16) as applied to $v_{1}=\\bar{v}_{x}$ and $v_{2}=v$.\n\n\\section*{Proper Time; Velocity and Acceleration 4-Vectors}\nLet us reparameterize the curve $\\mathscr{K}$, choosing now the quantity\n\n\n\\begin{equation*}\n\\tau=\\frac{s}{c}=\\frac{1}{c} \\int_{t_{0}}^{t} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}} d u \\quad \\text { or } \\quad \\frac{d \\tau}{d t}=\\sqrt{1-\\hat{v}^{2} / c^{2}} \\tag{12.21}\n\\end{equation*}\n\n\nwhere, as always, $\\hat{v}<c$. The new parameter $\\tau$ (a distance divided by a velocity) is known as the proper time for the particle; by Problem 12.23, a clock attached to the particle (and thus accelerating and decelerating along with it) reads $\\tau$.\n\nWhen $\\tau$-derivatives replace $t$-derivatives, velocity and acceleration become tensorial; i.e., the components\n\n\n\\begin{equation*}\nu^{i} \\equiv \\frac{d x^{i}}{d \\tau} \\quad b^{i} \\equiv \\frac{d u^{i}}{d \\tau}=\\frac{d^{2} x^{i}}{d \\tau^{2}} \\tag{12.22}\n\\end{equation*}\n\n\nare taken by (12.13) into\n\n\\[\n\\begin{array}{llll}\n\\bar{u}^{0}=\\frac{u^{0}-\\beta u^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{u}^{1}=\\frac{-\\beta u^{0}+u^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{u}^{2}=u^{2} & \\bar{u}^{3}=u^{3} \\\\\n\\bar{b}^{0}=\\frac{b^{0}-\\beta b^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{b}^{1}=\\frac{-\\beta b^{0}+b^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{b}^{2}=b^{2} & \\bar{b}^{3}=b^{3} \\tag{12.24}\n\\end{array}\n\\]\n\nThe important identities\n\n\n\\begin{equation*}\nu_{i} u^{i}=c^{2} \\quad u_{i} b^{i}=0 \\tag{12.25}\n\\end{equation*}\n\n\nare proved in Problem 12.24, and Problem 12.25 establishes the following connecting formulas between the numerical values of the relativistic and the nonrelativistic components:\n\n\n\\begin{equation*}\nu^{i}=\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\quad b^{i}=\\frac{a_{i}}{1-\\hat{v}^{2} / c^{2}}+\\frac{(\\mathbf{v a}) v_{i}}{c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}} \\tag{12.26}\n\\end{equation*}\n\n\n\\section*{Instantaneous Rest Frame}\nAt time $t=t_{1}$, the particle moving along $\\mathscr{K}$ has instantaneous position $P_{1}=\\mathbf{r}\\left(t_{1}\\right)$ and instantaneous speed $\\hat{v}\\left(t_{1}\\right)$. An instantaneous rest frame for the particle is an inertial frame that translates at speed $\\hat{v}\\left(t_{1}\\right)$ along the tangent to $\\mathscr{K}$ at $P_{1}$, in such manner that its origin coincides with $P_{1}$ at $t=t_{1}$. See Fig. 12-3.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-180}\n\\end{center}\n\nFig. 12-3\n\nWe shall say that a particle's motion (with respect to frame $O$ ) is uniformly accelerated if its spatial acceleration relative to an instantaneous rest frame $\\bar{O}$,\n\n$$\n\\alpha=\\hat{\\bar{a}}=\\sqrt{\\bar{a}_{x}^{2}+\\bar{a}_{y}^{2}+\\bar{a}_{z}^{2}}\n$$\n\ndoes not vary along the trajectory $\\mathscr{K}$.\n\nEXAMPLE 12.2 An electron fired at right angles into a uniform magnetic field undergoes uniformly accelerated motion (in a circle).\n\n\\subsection*{12.7 RELATIVISTIC MASS, FORCE, AND ENERGY}\nThe appropriate SR version of Newton's second law depends on the concept of mass to be adopted.\n\n\\section*{Rest Mass and Relativistic Mass}\nThe rest mass of a particle is its mass as measured or as inferred from Newtonian mechanics in any instantaneous rest frame for that particle.\n\nThe relativistic mass (in $O$ ) of a particle with spatial velocity $\\mathbf{v}$ (with respect to $O$ ) is\n\n\n\\begin{equation*}\n\\hat{m}=\\frac{m}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\tag{12.27}\n\\end{equation*}\n\n\nwhere $m$ is the rest mass of the particle. As is shown in Problem 12.27, (12.27) is a necessary consequence of conservation of momentum.\n\n\\section*{Relativistic Momentum and Force}\nThe 4-momentum of SR is defined by\n\n\n\\begin{equation*}\n\\left(p^{i}\\right) \\equiv\\left(p^{0}, \\mathbf{p}\\right)=\\left(m u^{i}\\right)=\\left(\\hat{m} v_{i}\\right) \\tag{12.28}\n\\end{equation*}\n\n\nand the (nontensorial) Lorentz force $\\left(F_{0}, \\mathbf{F}\\right)$ is defined as the time derivative of the 4-momentum:\n\n\n\\begin{equation*}\nF_{0} \\equiv \\frac{d p^{0}}{d t}=\\frac{d}{d t}\\left(\\frac{m c}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\quad \\mathbf{F} \\equiv \\frac{d}{d t}\\left(\\frac{m \\mathbf{v}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\tag{12.29}\n\\end{equation*}\n\n\nLike velocity, force becomes tensorial when proper time is introduced. Thus, the 4-force (Minkowski force) of SR is defined as\n\nFrom (12.21), we have the connecting formula\n\n\n\\begin{equation*}\n\\left(K^{i}\\right) \\equiv\\left(\\frac{d p^{i}}{d \\tau}\\right) \\tag{12.30}\n\\end{equation*}\n\n\n\n\\begin{equation*}\nK^{i}=\\frac{F_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\tag{12.31}\n\\end{equation*}\n\n\nThe following identities for the Lorentz and Minkowski forces are proved in Problem 12.29:\n\n\n\\begin{equation*}\nu_{i} K^{i}=0 \\quad K^{0}=\\frac{1}{c} \\mathbf{v K} \\quad F_{0}=\\frac{1}{c} \\mathbf{v F} \\quad \\mathbf{v F}=\\frac{d}{d t}\\left(\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\tag{12.32}\n\\end{equation*}\n\n\n\\section*{Relativistic Energy}\nAccording to the classical work-energy theorem, the rate at which work is performed on a particle, (vF), equals the rate of increase of the particle's kinetic energy. Thus, the last identity (12.32) suggests the definition for $\\mathrm{SR}$\n\n\n\\begin{equation*}\n\\hat{E}=\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\equiv \\hat{m} c^{2} \\tag{12.33}\n\\end{equation*}\n\n\nfor the energy of a particle moving at speed $\\hat{v}$. In the limit as $\\hat{v} \\rightarrow 0,(12.33)$ becomes\n\n\n\\begin{equation*}\nE=m c^{2} \\tag{12.34}\n\\end{equation*}\n\n\nThis is Einstein's famous formula for the rest energy $E$ of a particle with rest mass $m$.\n\n\\subsection*{12.8 MAXWELL'S EQUATIONS IN SR}\nIt is helpful to look briefly at the way the metric for Special Relativity affects the formulas for the divergence and the Laplacian, and to consider a new kind of matrix that will be useful in formulating Maxwell's equations.\n\n\\section*{Vector Calculus and Lorentz Transformations}\nFor the metric of SR, all Christoffel symbols vanish, so that\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{u}=\\frac{\\partial u^{i}}{\\partial x^{i}} \\tag{12.35}\n\\end{equation*}\n\n\nand\n\n\n\\begin{align*}\n\\operatorname{div}(\\operatorname{grad} f) \\equiv \\square f & =\\frac{\\partial}{\\partial x^{i}}\\left(g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right)=g^{i j} \\frac{\\partial^{2} f}{\\partial x^{i} \\partial x^{j}} \\\\\n& =\\frac{\\partial^{2} f}{\\left(\\partial x^{0}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{1}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{2}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{3}\\right)^{2}} \\\\\n& =\\frac{1}{c^{2}} \\frac{\\partial^{2} f}{\\partial t^{2}}-\\nabla^{2} f \\tag{12.36}\n\\end{align*}\n\n\nNote that in SR the Laplacian operator is notated $\\square$, with $\\nabla^{2}$ reserved for its spatial part. It is verified in Problem 12.31 that $\\square f$ is an invariant under Lorentz transformations, which means that the scalar wave equation has the same form, $\\square f=0$, in every inertial frame.\n\nIf we introduce the differential operator\n\n\n\\begin{equation*}\n\\partial^{i} \\equiv g^{i j} \\frac{\\partial}{\\partial x^{j}} \\tag{12.37}\n\\end{equation*}\n\n\nthen we can express the equation of continuity for a vector $\\left(w_{i}\\right) \\equiv\\left(w_{0}, \\mathbf{w}\\right)$ as\n\n\n\\begin{equation*}\n\\partial^{i} w_{i}=0 \\tag{12.38}\n\\end{equation*}\n\n\n(12.38) is equivalent to $\\partial w_{0} / \\partial t=c \\operatorname{div} \\mathbf{w}$.\n\n\\section*{Maxwell's Equations in Event Space}\nFirst, introduce for any two 3-vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ the two antisymmetric matrices\n\n\\[\n\\left[f^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & -V^{1} & -V^{2} & -V^{3}  \\tag{12.39a}\\\\\nV^{1} & 0 & U^{3} & -U^{2} \\\\\nV^{2} & -U^{3} & 0 & U^{1} \\\\\nV^{3} & U^{2} & -U^{1} & 0\n\\end{array}\\right] \\quad\\left[\\tilde{f}_{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & U^{1} & U^{2} & U^{3} \\\\\n-U^{1} & 0 & V^{3} & -V^{2} \\\\\n-U^{2} & -V^{3} & 0 & V^{1} \\\\\n-U^{3} & V^{2} & -V^{1} & 0\n\\end{array}\\right]\n\\]\n\nThe second matrix may be obtained from the first by making the replacements $\\mathbf{V} \\rightarrow-\\mathbf{U}$ and $\\mathbf{U} \\rightarrow \\mathbf{V}$; because these replacements constitute an anti-involution-i.e., $\\tilde{f}_{i j}=-f^{i j}-$ the two matrices are said to be dual to each other. In terms of individual components $\\left(e_{p q r}\\right.$ denotes the permutation symbol of order 3):\n\n\\[\n\\begin{array}{lll}\nf^{i j}=-f^{j i} & f^{0 q}=-V^{q} & f^{p q}=e_{p q r} U^{r}  \\tag{12.39b}\\\\\n\\tilde{f}_{i j}=-\\tilde{f}_{j i} & \\tilde{f}_{0 q}=U^{q} & \\tilde{f}_{p q}=e_{p q r} V^{r}\n\\end{array}\n\\]\n\nin which $i, j \\geqq 0$ and $p, q \\geqq 1$.\n\nBy their concoction, these matrices turn out to be tensors under Lorentz transformations (provided the row-divergences vanish in all inertial frames); a proof is given in Problem 12.32. Moreover, these tensors have the properties (Problem 12.33)\n\n\n\\begin{equation*}\n\\frac{\\partial f^{0 j}}{\\partial x^{j}}=-\\operatorname{div} \\mathbf{V} \\quad \\frac{\\partial \\tilde{f}_{0 j}}{\\partial x^{j}}=\\operatorname{div} \\mathbf{U} \\tag{12.40}\n\\end{equation*}\n\n\nand\n\n\n\\begin{equation*}\n\\left(\\frac{\\partial f^{1 j}}{\\partial x^{j}}, \\frac{\\partial f^{2 j}}{\\partial x^{j}}, \\frac{\\partial f^{3 j}}{\\partial x^{j}}\\right)=\\operatorname{curl} \\mathbf{U}+\\frac{1}{c} \\frac{\\partial \\mathbf{V}}{\\partial t} \\quad\\left(\\frac{\\partial \\tilde{f}_{1 j}}{\\partial x^{j}}, \\frac{\\partial \\tilde{f}_{2 j}}{\\partial x^{j}}, \\frac{\\partial \\tilde{f}_{3 j}}{\\partial x^{j}}\\right)=\\operatorname{curl} \\mathbf{V}-\\frac{1}{c} \\frac{\\partial \\mathbf{U}}{\\partial t} \\tag{12.41}\n\\end{equation*}\n\n\nWe now show how Maxwell's equations in vacuum, (11.19), may be extended to space-time via dual tensors of this sort. The equations are:\n\n\\[\n\\begin{array}{ll}\n\\operatorname{div} \\mathbf{H}=0 & \\operatorname{curl} \\mathbf{E}+\\frac{1}{c} \\frac{\\partial \\mathbf{H}}{\\partial t}=\\mathbf{0} \\\\\n\\operatorname{div} \\mathbf{E}=\\rho & \\operatorname{curl} \\mathbf{H}-\\frac{1}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}=\\frac{\\rho}{c} \\mathbf{v} \\tag{12.43}\n\\end{array}\n\\]\n\nin the last of which $\\mathbf{v}$ is the classical spatial velocity (12.17) of the charge-cloud $\\rho$. Define per (12.39) the tensors\n\n\\[\n\\mathscr{F}=\\left[F^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & -H_{1} & -H_{2} & -H_{3}  \\tag{12.44}\\\\\nH_{1} & 0 & E_{3} & -E_{2} \\\\\nH_{2} & -E_{3} & 0 & E_{1} \\\\\nH_{3} & E_{2} & -E_{1} & 0\n\\end{array}\\right] \\quad \\tilde{\\mathscr{F}}=\\left[\\tilde{F}^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{ccccc}\n0 & E_{1} & E_{2} & E_{3} \\\\\n-E_{1} & 0 & H_{3} & -H_{2} \\\\\n-E_{2} & -H_{3} & 0 & H_{1} \\\\\n-E_{3} & H_{2} & -H_{1} & 0\n\\end{array}\\right]\n\\]\n\n(with $\\mathbf{U}=\\mathbf{E}$ and $\\mathbf{V}=\\mathbf{H}$ ). In view of the first equations (12.40) and (12.41), (12.42) may be written as\n\n\n\\begin{equation*}\n\\frac{\\partial F^{i j}}{\\partial x^{j}}=0 \\tag{12.45a}\n\\end{equation*}\n\n\nSimilarly, if we make a 4-vector out of $\\mathbf{v}$ and $\\rho$ by the prescription\n\n\n\\begin{equation*}\n\\left(s^{i}\\right) \\equiv\\left(\\rho, \\frac{\\rho}{c} \\mathbf{v}\\right) \\tag{12.46}\n\\end{equation*}\n\n\n(see Problem 12.52), then the remaining Maxwell's equations, (12.43), are rendered tensorial as\n\n\n\\begin{equation*}\n\\frac{\\partial \\tilde{F}^{i j}}{\\partial x^{j}}=s^{i} \\tag{12.45b}\n\\end{equation*}\n\n\nEquations (12.45) are the relativistic Maxwell's equations, valid in every inertial frame. Because $\\tilde{\\mathbf{F}}$ is antisymmetric, we have from $(12.45 \\mathrm{~b})$ :\n\n$$\n\\frac{\\partial s^{i}}{\\partial x^{i}}=\\frac{\\partial^{2} \\tilde{F}^{i j}}{\\partial x^{i} \\partial x^{j}}=0 \\quad \\text { or } \\quad \\frac{\\partial}{\\partial x^{i}}\\left(g^{i j} s_{j}\\right)=\\left(g^{j i} \\frac{\\partial}{\\partial x^{i}}\\right) s_{j} \\equiv \\partial^{j} s_{j}=0\n$$\n\nso that the covariant vector $\\left(s_{j}\\right)$ obeys the equation of continuity (12.38).\n\n\\section*{Solved Problems}\n\\section*{EVENT SPACE}\n12.1 Calculate $\\varepsilon$ and $\\Delta s$ for the event pairs: (a) $E_{1}(5,1,-2,0)$ and $E_{2}(0,3,1,-3),(b)$ $E_{1}(5,1,3,3)$ and $E_{2}(2,-1,1,1),(c) E_{1}(7,2,4,4)$ and $E_{2}(4,1,2,6),(d) E_{1} \\equiv$ flash of light in Chicago at 7 p.m. and $E_{2} \\equiv$ flash of light in St. Louis ( 400 miles away) at 7.00000061 p.m. (e) Determine the interval type in each case.\n\n(a) $\\varepsilon(\\Delta s)^{2}=5^{2}-(-2)^{2}-(-3)^{2}-3^{2}=25-4-9-9=3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=1$.\n\n(b) $\\varepsilon(\\Delta s)^{2}=9-4-4-4=-3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=-1$.\n\n(c) $\\varepsilon(\\Delta s)^{2}=9-1-4-4=0$, or $\\Delta s=0$ and $\\varepsilon=1$.\n\n(d) With $c=186300 \\mathrm{mi} / \\mathrm{sec}, \\varepsilon(\\Delta s)^{2}=(0.002196 c)^{2}-(400)^{2} \\approx 7375 \\mathrm{mi}^{2}$, or $\\Delta s \\approx 85.8 \\mathrm{mi}$ and $\\varepsilon=1$.\n\n(e) Timelike, spacelike, lightlike, and timelike, respectively.\n\n12.2 Show that (a) simultaneous events have a spacelike interval; (b) copositional events have a timelike interval; $(c)$ the interval between two light flashes is lightlike if they are simultaneous to an observer who is present at the site of one of the flashes.\n\n\n\\begin{gather*}\n\\varepsilon(\\Delta s)^{2}=0^{2}-\\left(\\Delta x^{1}\\right)^{2}-\\left(\\Delta x^{2}\\right)^{2}-\\left(\\Delta x^{3}\\right)^{2}<0  \\tag{a}\\\\\n\\varepsilon(\\Delta s)^{2}=\\left(\\Delta x^{0}\\right)^{2}-0>0 \\tag{b}\n\\end{gather*}\n\n\n$$\n\\varepsilon(\\Delta s)-(\\Delta x)-0<0\n$$\n\n(c) Let the observer measure the proximate flash as $E_{1}(0,0,0,0)$. The distant flash $E_{2}\\left(c \\Delta t, \\Delta x^{1}, \\Delta x^{2}, \\Delta x^{3}\\right)$ will be registered simultaneously, at $x^{0}=0$, if\n\n$$\n\\Delta t=-\\frac{\\sqrt{\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}}}{c}\n$$\n\nBut then $\\varepsilon(\\Delta s)^{2}=0$ and the interval is lightlike. (Note that the (negative) time coordinate of $E_{2}$ is calculated, not measured.)\n\n\\section*{THE LORENTZ GROUP}\n12.3 Prove the following lemma involving the metric of SR, $g_{i j}$, as given by (12.6).\n\nLemma 12.4: If $C=\\left(c_{i j}\\right)$ is a symmetric $4 \\times 4$ matrix such that $c_{i j} x^{i} x^{j}=0$ for all $\\left(x^{i}\\right)$ such that $g_{i j} x^{i} x^{j}=0$, there exists a fixed real number $\\lambda$ for which $c_{i j}=\\lambda g_{i j}(C=\\lambda G)$.\n\nObserve that the vector $(1, \\pm 1,0,0)$ satisfies $g_{i j} x^{i} x^{j}=0$. Hence, substituting these components into the equation $c_{i j} x^{i} x^{j}=0$ yields\n\n$$\nc_{00} \\pm c_{01} \\pm c_{10}+c_{11}=0 \\quad \\text { or } \\quad c_{00}+c_{11}=0=c_{01}=c_{10}\n$$\n\n(by symmetry of $C$ ). Similarly, using the vectors $(1,0, \\pm 1,0)$ and $(1,0,0, \\pm 1)$, we get\n\n$$\nc_{00}=-c_{11}=-c_{22}=-c_{33}=\\lambda \\quad c_{i j}=0 \\quad(i=0 \\text { or } j=0)\n$$\n\nFinally, employing the vectors $(\\sqrt{2}, 1,1,0),(\\sqrt{2}, 1,0,1)$, and $(\\sqrt{2}, 0,1,1)$, we obtain $c_{12}=c_{13}=c_{23}=$ 0 .\n\n12.4 Establish the transformation (12.7) between inertial frames under the postulates for SR.\n\nFrom (12.6) and (12.5),\n\n$$\ng_{i j} x^{i} x^{j}=0=g_{i j} \\bar{x}^{i} \\bar{x}^{j}=g_{i j}\\left(a_{r}^{i} x^{r}\\right)\\left(a_{s}^{j} x^{s}\\right)=g_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}\n$$\n\nthat is,\n\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}=0 \\quad \\text { whenever } \\quad g_{i j} x^{i} x^{j}=0 \\tag{1}\n\\end{equation*}\n\n\nNow apply Lemma 12.4 to (1), with $g_{r s} a_{i}^{r} a_{j}^{s}=c_{i j}$, where $C=\\left(c_{i j}\\right)=A^{T} G A$ is symmetric. We obtain\n\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s}=\\lambda g_{i j} \\quad \\text { or } \\quad A^{T} G A=\\lambda G \\tag{2}\n\\end{equation*}\n\n\nIt remains to show that $\\lambda=1$. Since $G^{2}=I$, multiplication of (2) by the matrix $\\lambda^{-1} G$ gives $\\left(G\\left(\\lambda^{-1} A^{T}\\right) G\\right) A=I$, which shows that the inverse of $A$ is\n\n\\[\nB=\\frac{1}{\\lambda} G A^{T} G=\\left[\\begin{array}{rrrr}\na_{0}^{0} / \\lambda & -a_{0}^{1} / \\lambda & -a_{0}^{2} / \\lambda & -a_{0}^{3} / \\lambda  \\tag{3}\\\\\n-a_{1}^{0} / \\lambda & a_{1}^{1} / \\lambda & a_{1}^{2} / \\lambda & a_{1}^{3} / \\lambda \\\\\n-a_{2}^{0} / \\lambda & a_{2}^{1} / \\lambda & a_{2}^{2} / \\lambda & a_{2}^{3} / \\lambda \\\\\n-a_{3}^{0} / \\lambda & a_{3}^{1} / \\lambda & a_{3}^{2} / \\lambda & a_{3}^{3} / \\lambda\n\\end{array}\\right] \\equiv\\left[b_{j}^{i}\\right]_{44}\n\\]\n\nIn particular, $b_{0}^{0}=a_{0}^{0} / \\lambda$. Now since observers $O$ and $\\bar{O}$ are receding from each other at constant velocity $v$ and are using identical measuring devices, it is clear that each views the other in the sam. way. It follows that $a_{0}^{0}=b_{0}^{0}$ and $\\lambda=a_{0}^{0} / b_{0}^{0}=1$ (see Problem 12.5).\n\n12.5 With reference to Problem 12.4, give a \"thought-experiment\" which leads to the conclusion that $a_{0}^{0}=b_{0}^{0}$.\n\nConsider the motion of $O$ in $\\bar{O}$ 's frame: Transform the point $(c t, 0,0,0)$ under $\\mathscr{T}$ to get\n\n$$\n\\bar{x}^{0}=c \\bar{t}=a_{0}^{0} c t \\quad \\text { or } \\quad \\bar{t}=a_{0}^{0} t\n$$\n\nThus, 1 second on $O$ 's clock is $a_{0}^{0}$ seconds on $\\bar{O}$ 's; reciprocally, 1 second on $\\bar{O}$ 's clock is $b_{0}^{0}$ seconds on $O$ 's. Thus, $a_{0}^{0}=b_{0}^{0}$.\n\n12.6 Prove Theorem 12.1 from (12.7). [Note that Problem 12.4 did not make use of Theorem 12.1, so the proof will be logically correct.]\n\nBy (12.7), $\\left(g_{i j}\\right)$ is a covariant tensor under Lorentz transformations, so that $g_{i j} \\Delta x^{i} \\Delta x^{j}$ is an invariant (under Lorentz transformations).\n\n12.7 Verify that the following matrix is Lorentz:\n\n$$\n\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\frac{\\sqrt{6}}{2} & \\frac{1}{2} & \\frac{1}{2} \\\\\n1 & \\frac{\\sqrt{6}}{2} & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n\\end{array}\\right]\n$$\n\nWe verify directly the conditions $(12.7 c)$ :\n\n$$\n\\begin{array}{cc}\n(\\sqrt{3})^{2}-1^{2}-1^{2}-0^{2}=3-2=1 & (\\sqrt{2})^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-0^{2}=2-\\frac{3}{2}-\\frac{3}{2}=-1 \\\\\n0^{2}-\\left(\\frac{1}{2}\\right)^{2}-\\left(-\\frac{1}{2}\\right)^{2}-\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)^{2}=-1 & (\\sqrt{3})(\\sqrt{2})-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-0=0 \\\\\n(\\sqrt{3})(0)-(1)\\left(\\frac{1}{2}\\right)-(1)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)=0 & (\\sqrt{2})(0)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(\\frac{1}{2}\\right)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{1}{2}\\right)=0 \\\\\n0-\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(-\\frac{1}{2}\\right)+\\left(\\frac{\\sqrt{2}}{2}\\right)\\left(\\frac{\\sqrt{2}}{2}\\right)=0-\\frac{1}{4}-\\frac{1}{4}+\\frac{1}{2}=0\n\\end{array}\n$$\n\n12.8 Show that a matrix $A$ which preserves $\\mathbf{x}^{T} G \\mathbf{x}=0$ necessarily preserves $\\mathbf{x}^{T} G \\mathbf{x}=q$.\n\nThis is really Problem 12.6 in another guise. By Problem 12.4, $A$ must satisfy $A^{T} G A=G$. But then\n\n$$\n(A \\mathbf{x})^{T} G(A \\mathbf{x})=\\mathbf{x}^{T}\\left(A^{T} G A\\right) \\mathbf{x}=\\mathbf{x}^{T} G \\mathbf{x}=q\n$$\n\n12.9 (a) Exhibit the inverse, $B$, of a given Lorentz matrix, $A$. (b) If we define a matrix $A$ to be pseudo-orthogonal when there exists a matrix $J$ whose square is the identity and $A^{T} J A=J$, show that all Lorentz matrices are pseudo-orthogonal.\n\n(a) Set $\\lambda=1$ in (3) of Problem 12.4.\n\n(b) If $A$ is a Lorentz matrix, then $G$ clearly fills the role of $J$ in the definition of pseudo-orthogonal matrix.\n\n12.10 Prove that the Lorentz matrices compose a group under matrix multiplication.\n\nWe are required to show that (a) the product of two Lorentz matrices is Lorentz, $(b)$ the inverse of a Lorentz matrix is Lorentz.\n\n(a)\n\n$$\n(P Q)^{T} G(P Q)=Q^{T}\\left(P^{T} G P\\right) Q=Q^{T} G Q=G\n$$\n\n(b) Using Problem 12.4 with $\\lambda=1, B=A^{-1}=G A^{T} G$, and\n\n$$\nB^{T} G B=\\left(G A^{T} G\\right)^{T} G B=G A G^{2} B=G A B=G\n$$\n\n\\section*{SIMPLE LORENTZ MATRICES}\n12.11 Derive the simple form (12.9) of the transformation equations for SR by considering how observers $O$ and $\\bar{O}$ will view events occurring on a circular cylinder about their common $x$-axis.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-186}\n\\end{center}\n\nFig. 12-4\n\nAt any time $t$, let $E_{1}$ and $E_{2}$ be two events taking place at the points of space $(q, 1,0)$ and $(q, 0,1)$, respectively, which lie on a unit cylinder about $O$ 's $x$-axis (Fig. 12-4). Thus, with $p=c t$, we have space-time coordinates $E_{1}(p, q, 1,0)$ and $E_{2}(p, q, 0,1)$. Since the axes of $O$ are not turning with respect to $\\bar{O}$ 's, these two events will be viewed by observer $\\bar{O}$ as $E_{1}(\\bar{p}, \\bar{q}, 1,0)$ and $E_{2}\\left(\\bar{p}^{*}, \\bar{q}^{*}, 0,1\\right)$, respectively. The transformation equations (12.5) give:\n\n$$\n\\text { (I) }\\left\\{\\begin{array} { l } \n{ \\overline { p } = a _ { 0 } ^ { 0 } p + a _ { 1 } ^ { 0 } q + a _ { 2 } ^ { 0 } } \\\\\n{ \\overline { q } = a _ { 0 } ^ { 1 } p + a _ { 1 } ^ { 1 } q + a _ { 2 } ^ { 1 } } \\\\\n{ 1 = a _ { 0 } ^ { 2 } p + a _ { 1 } ^ { 2 } q + a _ { 2 } ^ { 2 } } \\\\\n{ 0 = a _ { 0 } ^ { 3 } p + a _ { 1 } ^ { 3 } q + a _ { 2 } ^ { 3 } }\n\\end{array} \\quad \\text { (II) } \\left\\{\\begin{array}{l}\n\\bar{p}^{*}=a_{0}^{0} p+a_{1}^{0} q+a_{3}^{0} \\\\\n\\bar{q}^{*}=a_{0}^{1} p+a_{1}^{1} q+a_{3}^{1} \\\\\n0=a_{0}^{2} p+a_{1}^{2} q+a_{3}^{2} \\\\\n1=a_{0}^{3} p+a_{2}^{3} q+a_{3}^{3}\n\\end{array}\\right.\\right.\n$$\n\nObserving just the last equation of (I) and the third equation of (II), we may, since $p$ and $q$ are arbitrary, take $p=q=0$, then $p=1, q=0$, and $p=0, q=1$. It follows that all six of the coefficients vanish: $a_{0}^{2}=a_{1}^{2}=a_{3}^{2}=a_{0}^{3}=a_{1}^{3}=a_{2}^{3}=0$. Using the third equation of (I) and the last equation of (II), we find that $a_{2}^{2}=a_{3}^{3}=1$. It follows that the last two equations of $\\mathscr{T}$ reduce to $\\bar{x}^{2}=x^{2}$ and $\\bar{x}^{3}=x^{3}$. Now to concentrate on the first two: If $p=q=0$, then event $E_{1}$ is $(0,0,1,0)$-occurring when $t=\\bar{t}=0$ at $x^{1}=0$, the instant when $\\bar{x}^{1}=0$. That is, $\\bar{p}=\\bar{q}=0$, with the result $a_{2}^{0}=a_{2}^{1}=0$. Similarly, using $E_{2}$, $p=q=0$ implies $\\bar{p}^{*}=\\bar{q}^{*}=0$ and $a_{3}^{0}=a_{3}^{1}=0$.\n\n12.12 Consider event $E_{1}$, a lightning flash at the point $(v, 0,0)$ at time $t=1 \\mathrm{~s}$ in $O$ 's frame, and event $E_{2}$, a lightning flash at $(-v, 0,0)$ at time $\\bar{t}=1 \\mathrm{~s}$ in $\\bar{O}$ 's frame. By determining the corresponding events in the opposing frames of reference, deduce (12.11).\n\nSince at $t=1$ observer $\\bar{O}$ has reached the point $(v, 0,0)$, the lightning strikes $\\bar{O}$ 's origin at time $\\bar{t}$. Hence, $E_{1}$ has coordinates $(c, v, 0,0)$ in $O$ and $(c \\bar{t}, 0,0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ we obtain\n\n$$\nc \\bar{t}=a c+b v \\quad 0=d c+e v\n$$\n\nThe second equation gives $d=-\\beta e$.\n\nSince $O$ has progressed backwards to the point $(-v, 0,0)$ in $\\bar{O}$ at the time $\\bar{t}=1$ at which $E_{2}$ occurs, this event has coordinates $(c t, 0,0,0)$ in $O$ and $(c,-v, 0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ yields\n\n$$\nc=a c t+b(0) \\quad-v=d c t+e(0)\n$$\n\nwhich upon division give $d=-\\beta a$. Hence, $a=e$.\n\n12.13 Show that a $4 \\times 4$ matrix is both Lorentz and orthogonal if and only if it has the form\n\n\\[\nR=\\left[\\begin{array}{rrrr} \n\\pm 1 & 0 & 0 & 0  \\tag{1}\\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\n\\]\n\nwhere the $3 \\times 3$ matrix $\\left[\\begin{array}{lll}\\mathbf{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal.\n\nA Lorentz matrix $A=\\left(a_{j}^{i}\\right)$ is also orthogonal if and only if its inverse $B$, as obtained in Problem 12.4 (with $\\lambda=1$ ), is equal to $A^{T}$ and is itself orthogonal. This observation immediately yields the form (1).\n\n\\subsection*{12.14 Prove Theorem 12.2.}\nSince $\\|\\mathbf{r}\\|^{2}=b^{-2}\\left[\\left(a_{1}^{0}\\right)^{2}+\\left(a_{2}^{0}\\right)^{2}+\\left(a_{3}^{0}\\right)^{2}\\right]=b^{-2}\\left[\\left(a_{0}^{0}\\right)^{2}-1\\right]=1$ (using Problem 12.36), the matrix $\\left[\\begin{array}{lll}\\mathrm{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal and $R_{2}^{T}$ has the form of the matrix in Problem 12.13, making it Lorentz and orthogonal. It follows that $R_{2}$ is orthogonal (and Lorentz), with $R_{2}^{-1}=R_{2}^{T}$; hence, $L=R_{1} L^{*} R_{2}$.\n\nNow, as the product of Lorentz matrices, $R_{1}$ is Lorentz; to show it is orthogonal, consider $L R_{2}^{T}\\left(L^{*}\\right)^{-1}$, which may be written as\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{llll}\na_{0} & b_{0} & c_{0} & d_{0} \\\\\na_{1} & b_{1} & c_{1} & d_{1} \\\\\na_{2} & b_{2} & c_{2} & d_{2} \\\\\na_{3} & b_{3} & c_{3} & d_{3}\n\\end{array}\\right]\\left[\\begin{array}{llll}\n1 & 0 & 0 & 0 \\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\na & -b & 0 & 0 \\\\\n-b & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-187}\n\\end{center}\n\n[The omitted rows have the form $\\left(a_{i}, b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}, b_{i} s_{1}+c_{i} s_{2}+d_{i} s_{3}, b_{i} t_{1}+c_{i} t_{2}+d_{i} t_{3}\\right.$ ), with $i=1,2,3$.] We first concentrate on proving that the top row and first column of this product are $( \\pm 1,0,0,0)$. The 00 -element of the product is\n\n$$\na_{0} a+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right)(-b)=\\varepsilon a_{0}^{2}+\\frac{\\varepsilon}{b}\\left(b_{0}^{2}+c_{0}^{2}+d_{0}^{2}\\right)(-b)=\\varepsilon\\left(a_{0}^{2}-b_{0}^{2}-c_{0}^{2}-d_{0}^{2}\\right)=\\varepsilon\n$$\n\nagain using the fact that the transpose of a Lorentz matrix is Lorentz. The next element in the top row of the product is\n\n$$\n-a_{0} b+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right) a=-a_{0} b+\\frac{b}{\\varepsilon}\\left(r^{2}\\right) \\varepsilon a_{0}=-a_{0} b+b a_{0}=0\n$$\n\nFor the third and fourth elements,\n\n$$\nb_{0} s_{1}+c_{0} s_{2}+d_{0} s_{3}=\\frac{b}{\\varepsilon} \\mathbf{r s}=0 \\quad \\text { and } \\quad b_{0} t_{1}+c_{0} t_{2}+d_{0} t_{3}=\\frac{b}{\\varepsilon} \\mathbf{r t}=0\n$$\n\nNow for the first column of the product; its elements, beginning with the second, are (for $i=1,2,3$ )\n\n$$\na_{i} a+\\left(b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}\\right)(-b)=\\varepsilon a_{i} a_{0}-\\varepsilon\\left(b_{i} b_{0}+c_{i} c_{0}+d_{i} d_{0}\\right)=0\n$$\n\nHence, the product matrix becomes\n\n$$\nR_{1}=\\left[\\begin{array}{llll}\n\\varepsilon & 0 & 0 & 0 \\\\\n0 & & & \\\\\n0 & & R & \\\\\n0 & & &\n\\end{array}\\right]\n$$\n\nand the $3 \\times 3$ matrix $R$ must be orthogonal, since $R_{1}$ is Lorentz.\n\n12.15 Apply Theorem 12.2 to the Lorentz matrix of Problem 12.7, and demonstrate the physical significance of this matrix by computing the velocity $v$ between the two observers involved.\n\nWe proceed to calculate $a, b$, and the vectors $\\mathbf{r}, \\mathbf{s}$, and $\\mathbf{t}$ :\n\n$$\na=\\sqrt{3} \\quad \\varepsilon=1 \\quad b=-\\sqrt{3-1}=-\\sqrt{2} \\quad \\mathbf{r}=-\\frac{1}{\\sqrt{2}}(\\sqrt{2}, 0,0)=(-1,0,0)\n$$\n\nHence, we may take $\\mathbf{s}=(0,1,0)$ and $\\mathbf{t}=(0,0,1)$, yielding\n\n$$\nR_{2}=\\left[\\begin{array}{rrrr}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand\n\n$$\n\\begin{aligned}\nR_{1} & =\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & \\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cccc}\n\\sqrt{3} & -\\sqrt{2} & 0 & 0 \\\\\n1 & -\\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & -\\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -\\sqrt{2} / 2 & 1 / 2 & 1 / 2 \\\\\n0 & -\\sqrt{2} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nBy Corollary 12.3,\n\n$$\nv=c \\sqrt{1-(\\sqrt{3})^{-2}}=\\sqrt{\\frac{2}{3}} c\n$$\n\n\\section*{LENGTH CONTRACTION, TIME DILATION}\n12.16 A pole-vaulter runs at the rate $(\\sqrt{3} / 2) c$ (in $\\mathrm{m} / \\mathrm{s}$ ) and carries a pole that is $20 \\mathrm{~m}$ long in his reference frame [the rest length of the pole is $20 \\mathrm{~m}$ ]. He approaches a barn that is open at both ends and is $10 \\mathrm{~m}$ long, as measured by a ground observer. To the ground observer, will the pole fit inside the barn? What is the pole-vaulter's conclusion?\n\nTo the ground observer, the pole undergoes length contraction with the factor $\\sqrt{1-\\beta^{2}}$, where $\\beta=\\sqrt{3} / 2$. Hence, the length of the pole in the frame of the ground observer is\n\n$$\n20 \\sqrt{1-(\\sqrt{3} / 2)^{2}}=10 \\mathrm{~m}\n$$\n\nand so, for her, the pole exactly fits inside the barn (instantaneously). To the runner, however, the barn is $10(1 / 2)=5 \\mathrm{~m}$ long, so that the $20-\\mathrm{m}$ pole does not fit.\n\nThis example shows that order relations are not preserved under the Lorentz transformation.\n\n12.17 (the Twin Paradox) One of a pair of twins embarks on a journey into outer space, taking one year (earth time) to accelerate to $(3 / 4) c$, then spends the next 20 years cruising to reach a galaxy 15 light-years away. An additional year is spend in decelerating in order to explore one of its solar systems. After one year of exploration $(\\beta=0)$, the twin returns to earth by the same schedule-one year of acceleration, 20 years of cruising, and one year of deceleration. Estimate the difference in the ages of the twins after the journey has ended.\n\nIn order to apply SR, replace the four periods of acceleration or deceleration by four periods of uniform motion at speed (3/8)c (the time-average speed under constant acceleration). These account for 4 years by the earth clock; but to the space twin, who measures proper (shortest) time intervals, the time lapse is $(\\beta=3 / 8)$\n\n$$\n4 \\sqrt{1-(3 / 8)^{2}} \\approx 3.71 \\text { years }\n$$\n\nSimilarly, the 40 earth-years of cruising at $\\beta=3 / 4$ corresponds to a proper-time interval of\n\n$$\n40 \\sqrt{1-(3 / 4)^{2}} \\approx 26.46 \\text { years }\n$$\n\nThus, the space twin has aged $3.71+26.46+1 \\approx 31$ years while the earth twin has aged $4+40+1=$ 45 years.\n\nThe space twin returns biologically younger by some 14 years. While the accelerations and decelerations between the two twins were reciprocal, the forces in the situation acted on the space twin alone.\n\n12.18 Prove the basic integrity of (12.16) by solving algebraically for $v_{2}$ as a function of $v_{1}$ and $v_{3}$ to verify that $v_{2}$ follows the correct format for composition of velocities.\n\nSolving,\n\n$$\nv_{2}=\\frac{-v_{1}+v_{3}}{1-v_{1} v_{3} / c^{2}}\n$$\n\nwhich is precisely (12.16) under the substitution $\\left(v_{1}, v_{2}, v_{3}\\right) \\rightarrow\\left(-v_{1}, v_{3}, v_{2}\\right)$.\n\n12.19 A light source at $O$ sends a spherical wavefront (Fig. 12-5(a)) advancing in all directions at velocity $c$; it reaches the ends of a diameter $A B$ centered at $O$ simultaneously, as determined by $O$. But as far as $\\bar{O}$ is concerned, the spherical wave, centered at $\\bar{O}$, moves with him (invariance of the light cone) and therefore reaches point $B$ before it reaches point $A$. Calculate the time difference on $\\bar{O}$ 's clock for these two events (light reaching $B$ and light reaching $A$ ) if $\\beta=1 / 2$ and if $A B=6 \\mathrm{~m}$.\n\nSince $A B=6 \\mathrm{~m}$ and $O$ is the midpoint of segment $A B, O$ assigns spatial coordinates $B(3,0,0)$ and $A(-3,0,0)$ to the endpoints. It takes $3 / c$ seconds for light to reach $A$ and $B$, so $O$ calculates the time coordinate as $x^{0}=c(3 / c)=3 \\mathrm{~m}$. The space-time coordinates of the two events are thus\n\n$$\nE_{B}(3,3,0,0) \\quad \\text { and } \\quad E_{A}(3,-3,0,0)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189(1)}\n\\end{center}\n\n(b)\n\nFig. 12-5\n\nSubstitute these values and $\\beta=1 / 2$ into the first equation (12.13) to obtain $\\bar{t}_{B}=\\sqrt{3} / c, \\bar{t}_{A}=3 \\sqrt{3} / c$. Hence, $\\Delta \\bar{t}=2 \\sqrt{3} / c$ (in s), while $\\Delta t=0$.\n\nIt is seen that simultaneity is not an invariant of Lorentz transformations.\n\n12.20 Derive the composition of velocities formula, (12.16).\n\nAccording to Section 12.4, we must have $v_{i}=-b_{i} c / a_{i}$ for $i=1,2,3$. Composing the simple Lorentz transformations, we have\n\n$$\n\\left[\\begin{array}{cccc}\na_{1} & b_{1} & 0 & 0 \\\\\nb_{1} & a_{1} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\na_{2} & b_{2} & 0 & 0 \\\\\nb_{2} & a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\na_{1} a_{2}+b_{1} b_{2} & a_{1} b_{2}+a_{2} b_{1} & 0 & 0 \\\\\na_{2} b_{1}+a_{1} b_{2} & b_{1} b_{2}+a_{1} a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhence $a_{3}=a_{1} a_{2}+b_{1} b_{2}, b_{3}=a_{1} b_{2}+a_{2} b_{1}$, and\n\n$$\nv_{3}=-\\frac{\\left(a_{1} b_{2}+a_{2} b_{1}\\right) c}{a_{1} a_{2}+b_{1} b_{2}}=\\frac{-\\frac{a_{1} b_{2} c}{a_{1} a_{2}}-\\frac{a_{2} b_{1} c}{a_{1} a_{2}}}{\\frac{a_{1} a_{2}}{a_{1} a_{2}}+\\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\\frac{-\\frac{b_{2} c}{a_{2}}-\\frac{b_{1} c}{a_{1}}}{1+\\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\\frac{v_{2}+v_{1}}{1+v_{1} v_{2} / c^{2}}\n$$\n\n12.21 A physicist wants to compose two equal velocities $v=v_{1}=v_{2}$ to produce a resultant velocity that is $90 \\%$ of the velocity of light. What velocity must he use?\n\nFrom (12.16),\n\n$$\n0.90 c=\\frac{2 v}{1+v^{2} / c^{2}} \\quad \\text { or } \\quad 0.90=\\frac{2 \\beta}{1+\\beta^{2}}\n$$\n\nSolving the quadratic, $\\beta \\approx 0.627$ (as compared to the Newtonian value 0.45 ).\n\n\\section*{VELOCITY AND ACCELERATION IN RELATIVITY}\n12.22 Establish (12.19) and (12.20), the Lorentz transformations of velocity and acceleration, that define how $\\bar{O}$ tracks the motion of a particle in $O$ 's frame.\n\nTo simplify notation, let $\\gamma \\equiv\\left(1-\\beta^{2}\\right)^{-1 / 2}$. Then $\\mathscr{T}$ is\n\n$$\nc \\bar{t}=\\gamma(c t-\\beta x) \\quad \\bar{x}=\\gamma(-\\beta c t+x) \\quad \\bar{y}=y \\quad \\bar{z}=z\n$$\n\nDifferentiate the first equation with respect to $\\bar{t}$ and use the chain rule:\n\n$$\nc=\\gamma\\left(c-\\beta v_{x}\\right) \\frac{d t}{d \\bar{t}} \\quad \\text { or } \\quad \\frac{d t}{d \\bar{t}}=\\frac{1}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}\n$$\n\nNow differentiate the last three equations:\n\n$$\n\\begin{aligned}\n& \\bar{v}_{x}=\\gamma\\left(-\\beta c+v_{x}\\right) \\frac{d t}{d \\bar{t}}=\\frac{\\gamma\\left(-v+v_{x}\\right)}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{y}=v_{y} \\frac{d t}{d \\bar{t}}=\\frac{v_{y}}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{y} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{z}=v_{z} \\frac{d t}{d \\bar{t}}=\\frac{v_{z} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}}\n\\end{aligned}\n$$\n\nBy differentiation of the velocity components just found,\n\n$$\n\\begin{aligned}\n\\bar{a}_{x} & =\\frac{d \\bar{v}_{x}}{d t} \\frac{d t}{d \\bar{t}}=\\frac{\\left(a_{x}-0\\right)\\left(1-v_{x} v / c^{2}\\right)-\\left(v_{x}-v\\right)\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1}{\\gamma\\left(1-v_{x} v / c^{2}\\right)} \\\\\n& =\\frac{a_{x}-a_{x} v_{x} v / c^{2}+v_{x} a_{x} v / c^{2}-a_{x} v^{2} / c^{2}}{\\gamma\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}\n\\end{aligned}\n$$\n\n$$\n\\bar{a}_{y}=\\frac{a_{y}\\left(1-v_{x} v / c^{2}\\right)-v_{y}\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1-\\beta^{2}}{1-v v_{x} / c^{2}}=\\frac{a_{y}+\\left(a_{x} v_{y}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)}{\\left(1-v_{x} / c^{2}\\right)^{3}}\\left(1-\\beta^{2}\\right)\n$$\n\nThe formula for $\\bar{a}_{z}$ is derived as that for $\\bar{a}_{y}$, with $z$ replacing $y$ throughout.\n\n12.23 Show that if the curve of motion in $O$ 's frame is the path of $\\bar{O}$ itself, the clock in $\\bar{O}$ 's frame (the clock moving with the particle) measures proper time.\n\nBy (3) of Problem 12.4,\n\n$$\n\\begin{aligned}\nx^{0} & =a_{0}^{0} \\bar{x}^{0}-a_{0}^{1} \\bar{x}^{1}-a_{0}^{2} \\bar{x}^{2}-a_{0}^{3} \\bar{x}^{3} \\\\\nx^{i} & =-a_{i}^{0} \\bar{x}^{0}+a_{i}^{1} \\bar{x}^{1}+a_{i}^{2} \\bar{x}^{2}+a_{i}^{3} \\bar{x}^{3} \\quad(i=1,2,3)\n\\end{aligned}\n$$\n\nNow the motion of $\\bar{O}$ relative to itself is obviously $\\bar{x}^{1}=\\bar{x}^{2}=\\bar{x}^{3}=0$. Hence,\n\n$$\nx^{0}=a_{0}^{0} c u \\quad x^{1}=-a_{1}^{0} c u \\quad x^{2}=-a_{2}^{0} c u \\quad x^{3}=-a_{3}^{0} c u\n$$\n\ngive the trajectory of $\\bar{O}$ in $O$ 's frame, with parameter $u=\\bar{t}$. Therefore, the tangent field to the trajectory is\n\n$$\n\\left(\\frac{d x^{i}}{d u}\\right)=\\left(a_{0}^{0} c,-a_{1}^{0} c,-a_{2}^{0} c,-a_{3}^{0} c\\right)\n$$\n\nso that the proper time parameter for this curve is defined as\n\n$$\n\\tau=\\frac{1}{c} \\int_{0}^{\\bar{t}} \\sqrt{\\left|\\left(a_{0}^{0} c\\right)^{2}-\\left(a_{1}^{0} c\\right)^{2}-\\left(a_{2}^{0} c\\right)^{2}-\\left(a_{3}^{0} c\\right)^{2}\\right|} d u=\\sqrt{\\left|\\left(a_{0}^{0}\\right)^{2}-\\left(a_{1}^{0}\\right)^{2}-\\left(a_{2}^{0}\\right)^{2}-\\left(a_{3}^{0}\\right)^{2}\\right|} \\int_{0}^{\\bar{t}} d u\n$$\n\nBecause the inverse transformation is Lorentz, the factor in front of the integral sign equals 1 , and so $\\tau=\\bar{t}$.\n\n12.24 Derive the identities (12.25).\n\nBy (12.21),\n\n$$\n\\begin{aligned}\nu_{i} u^{i} & =g_{i j} u^{i} u^{j} \\equiv\\left(u^{0}\\right)^{2}-\\left(u^{1}\\right)^{2}-\\left(u^{2}\\right)^{2}-\\left(u^{3}\\right)^{2} \\\\\n& =\\left[\\left(v_{t}\\right)^{2}-\\left(v_{x}\\right)^{2}-\\left(v_{y}\\right)^{2}-\\left(v_{z}\\right)^{2}\\right]\\left(\\frac{d t}{d \\tau}\\right)^{2}=\\left[c^{2}-\\hat{v}^{2}\\right] \\frac{1}{1-\\hat{v}^{2} / c^{2}}=c^{2}\n\\end{aligned}\n$$\n\nand from this,\n\n$$\n0=\\frac{d}{d \\tau}\\left(c^{2}\\right)=\\frac{d}{d \\tau}\\left(u_{i} u^{i}\\right)=2 u_{i} b^{i}\n$$\n\n12.25 Establish the formulas in (12.26).\n\n$$\n\\begin{aligned}\nu^{i} & =\\frac{d x^{i}}{d \\tau}=\\frac{d x^{i}}{d t} \\frac{d t}{d \\tau}=\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\\\\nb^{i} & =\\frac{d u^{i}}{d \\tau}=\\left[\\frac{d}{d t}\\left(\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\\right] \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{1 / 2}-v_{i}(1 / 2)\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}\\left(-2 a_{x} v_{x}-2 a_{y} v_{y}-2 a_{z} v_{z}\\right) / c^{2}}{1-\\hat{v}^{2} / c^{2}} \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)+v_{i}\\left(a_{x} v_{x}+a_{y} v_{y}+a_{z} v_{z}\\right) / c^{2}}{\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}=\\frac{a_{i}}{1-\\hat{v}^{2} / c^{2}}+\\frac{(\\mathbf{v a}) v_{i}}{c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}\n\\end{aligned}\n$$\n\n12.26 Derive the equation for uniformly accelerated motion along the $x$-axis of an inertial frame:\n\n$$\nx^{2}-c^{2} t^{2}=\\frac{c^{4}}{\\alpha^{2}}\n$$\n\nLet $\\bar{O}$ be an instantaneous rest frame at some point $t_{1}$, and let $O$ be a given (stationary) frame in which the motion curve is traced. Since the motion is along the $x$-axis of $O$,\n\n$$\nv_{y}=v_{z}=a_{y}=a_{z}=0 \\quad \\text { and } \\quad \\bar{v}_{y}=\\bar{v}_{z}=\\bar{a}_{y}=\\bar{a}_{z}=0\n$$\n\nwhereby $\\bar{a}_{x}=\\alpha=$ const. (assuming $\\bar{a}_{x}>0$ ). At $t=t_{1}, v=v_{x}$ (the constant velocity of $\\bar{O}$ is by definition equal to the instantaneous velocity of the particle); thus, from (12.20),\n\n\n\\begin{equation*}\n\\alpha=\\frac{a_{x}\\left(1-v^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3}}=\\frac{a_{x}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}} \\tag{1}\n\\end{equation*}\n\n\nSince $t_{1}$ is arbitrary, (1) must hold for all $t$. Writing $\\dot{x}, \\ddot{x}$ for the derivatives of $x(t)$, we have from (1):\n\n\n\\begin{equation*}\nc^{3} \\ddot{x}=\\alpha\\left(c^{2}-\\dot{x}^{2}\\right)^{3 / 2} \\tag{2}\n\\end{equation*}\n\n\nMake the substitution $y=\\dot{x}$ and (2) becomes\n\n\n\\begin{equation*}\nc^{3} \\frac{d y}{d t}=\\alpha\\left(c^{2}-y^{2}\\right)^{3 / 2} \\quad \\text { or } \\quad \\int \\frac{c^{3} d y}{\\left(c^{2}-y^{2}\\right)^{3 / 2}}=\\int \\alpha d t \\tag{3}\n\\end{equation*}\n\n\nStandard techniques of integration yield the first integral\n\n\n\\begin{equation*}\n\\frac{c y}{\\sqrt{c^{2}-y^{2}}}=\\alpha t \\tag{4}\n\\end{equation*}\n\n\n(where we have taken the initial velocity to be zero). Solving (4) for $y$ (assumed positive for positive $t$ ) and then integrating the equation $\\dot{x}=y(t)$, we obtain\n\n$$\nx=c \\sqrt{c^{2}+\\alpha^{2} t^{2}} / \\alpha \\quad \\text { or } \\quad x^{2}-c^{2} t^{2}=c^{4} / \\alpha^{2}\n$$\n\n(where we also take the initial position as zero). This is the desired equation, which represents a hyperbola in the $x t$ plane. By contrast, the Newtonian equation is the parabola $x=\\frac{1}{2} \\alpha t^{2}$.\n\n\\section*{RELATIVISTIC MASS, FORCE, AND ENERGY}\n12.27 Show that the observed mass of a particle with rest mass $m$, moving at velocity $v$, is $\\hat{m}=m\\left(1-v^{2} / c^{2}\\right)^{-1 / 2}$, by considering the following experiment. Let each observer $O$ and $\\bar{O}$ carry a ball with rest mass $m$ near his origin and so situated as to collide obliquely at $t=\\bar{t}=0$ (when their origins coincide). See Fig. 12-6. Suppose this collision imparts reciprocal velocities of $\\varepsilon$ in the positive $x$-direction and negative $\\bar{x}$-direction. Calculate the momentum of the system before and after collision (which is preserved), and what each observer sees based on the equations of SR; then take the limit as $\\varepsilon \\rightarrow 0$.\n\nThe velocity vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ of balls $B_{1}$ and $B_{2}$ before collision are, as seen by $O,(0,0,0)=\\mathbf{0}$ and $(v, 0,0)=v$ i. Observer $\\bar{O}$ calculates these vectors as $\\overline{\\mathbf{v}}_{1}=(-v, 0,0)$ and $\\overline{\\mathbf{v}}_{2}=(0,0,0)$ (either by\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192}\n\\end{center}\n\n(a) Before impact\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192(1)}\n\\end{center}\n\n(b) After impact\n\nFig. 12-6\\\\\nreciprocity or by use of (12.19). After collision, observer $O$ calculates the velocity of $B_{1}$ as $\\mathbf{v}_{1}=$ $(\\varepsilon, \\delta, 0)=\\varepsilon \\mathbf{i}+\\delta \\mathbf{j}$, assuming $B_{1}$ has the proper alignment with $B_{2}$. Reciprocally, observer $\\bar{O}$ calculates the velocity of $B_{2}$ as $\\overline{\\mathbf{v}}_{2}=(-\\varepsilon,-\\delta, 0)$. To find $\\mathbf{v}_{2}$, use the inverse of (12.19), with $\\bar{v}_{x}=-\\varepsilon$ and $\\bar{v}_{y}=-\\delta$ :\n\n$$\n\\mathbf{v}_{2}=v_{x} \\mathbf{i}+v_{y} \\mathbf{j}=\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n$$\n\nThus, observer $O$ calculates the net momentum vector of the system as follows, using the rest mass $m$ of $B_{1}$ for $m_{1}$ and the \"perceived mass\" $\\hat{m}$ of $B_{2}$ for $m_{2}$ :\n\n$$\n\\text { before impact } \\quad \\begin{aligned}\nm_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m_{1}(\\mathbf{0})+m_{2}(v \\mathbf{i})=\\hat{m} v \\mathbf{i} \\\\\n\\text { after impact } \\quad m_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m(\\varepsilon \\mathbf{i}+\\delta \\mathbf{j})+\\hat{m}\\left[\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\\right] \\\\\n& =\\left(m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(m \\delta-\\hat{m} \\frac{\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n\\end{aligned}\n$$\n\nSince $O$ is using the universal laws of physics as they apply to his frame (Postulate 1 of SR), the two momentum vectors above must be the same. Hence,\n\n$$\n\\hat{m} v=m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}} \\quad \\text { and } \\quad 0=m-\\hat{m} \\frac{\\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\n$$\n\n(after division by $\\delta$ ). Now take the limit as $\\varepsilon \\rightarrow 0$ :\n\n$$\n\\hat{m} v=\\hat{m} v \\quad \\text { and } \\quad 0=m-\\hat{m} \\sqrt{1-\\beta^{2}}\n$$\n\nThe right-hand equation is the connection between $m$ and $\\hat{m}$.\n\n12.28 Show that the Minkowski force is a 4-vector.\n\nWe must show that $\\bar{K}^{i}=a_{j}^{i} K^{j}$, if $\\bar{x}^{i}=a_{j}^{i} x^{j}$, where $\\left(a_{j}^{i}\\right)$ is any Lorentz matrix. Since $\\tau$ is invariant and $a_{i}^{i}=$ const. we may differentiate the coordinate transformation with respect to $\\tau$ across the equal sign:\n\n$$\n\\frac{d}{d \\tau}\\left(\\bar{x}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} x^{j}\\right) \\quad \\text { or } \\quad \\bar{u}^{i}=a_{j}^{i} u^{j}\n$$\n\n(proving that $\\left(u^{i}\\right)$ is a 4-vector). Multiply both sides by $m$ and differentiate again, using the fact that the rest mass of a particle is invariant:\n\n$$\n\\bar{K}^{i}=\\frac{d}{d \\tau}\\left(\\bar{m} \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(m \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} m u^{j}\\right)=a_{j}^{i} \\frac{d}{d \\tau}\\left(m u^{j}\\right)=a_{j}^{i} K^{j}\n$$\n\n\\subsection*{12.29 Establish (12.32).}\nDefinition (12.30), $K^{i}=d\\left(m u^{i}\\right) / d \\tau=m b^{i}$, along with the second identity (12.25), gives at once $u_{i} K^{i}=0$.\n\nFrom $u_{i} K^{i}=g_{i j} u^{i} K^{j}=u^{0} K^{0}-u^{q} K^{q}=0$ and the first formula (12.26), we have\n\n$$\n\\frac{v_{0} K^{0}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}-\\frac{v_{q} K^{q}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}=0 \\quad \\text { or } \\quad c K^{0}=\\mathbf{v K}\n$$\n\nBy (12.31) and $c K^{0}=\\mathbf{v K}$,\n\n$$\n\\frac{1}{c} \\mathbf{v F}=\\frac{1}{c} \\mathbf{v K} \\sqrt{1-\\hat{v}^{2} / c^{2}}=K^{0} \\sqrt{1-\\hat{v}^{2} / c^{2}}=F_{0}\n$$\n\nUsing the first definition (12.29),\n\n$$\n\\mathbf{v F}=c F_{0}=\\frac{d}{d t}\\left(\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\n$$\n\n12.30 Show that as $\\hat{v} \\rightarrow 0, \\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+O\\left(\\hat{v}^{4} / c^{2}\\right)$. Interpret this result.\n\nThe expression for relativistic energy, $\\hat{E}=m c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}$ may be expanded by the binomial theorem:\n\n$$\n(1+x)^{\\alpha}=1+\\alpha x+\\frac{\\alpha(\\alpha-1)}{2 !} x^{2}+\\cdots \\quad(-1<x<1)\n$$\n\nThe result is\n\n$$\n\\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+\\frac{3 m \\hat{v}^{4}}{8 c^{2}}+\\cdots\n$$\n\nThus, at low speeds, the total energy of particle is very nearly the sum of its rest energy (which includes all sorts of potential energy) and its classical kinetic energy.\n\n\\section*{MAXWELL'S EQUATIONS IN SR}\n12.31 Prove that $\\bar{\\square} \\bar{f}=\\square f$.\n\nAs the $g_{i j}$ are constants, $\\square f \\equiv g^{i j} f_{, i j}=$ invariant.\n\n12.32 Prove that if $\\left(F^{i j}\\right)$ is any matrix of functions of the 3-vectors $\\mathbf{U}$ and $\\mathbf{V}$ such that $\\partial F^{i j} / \\partial x^{j}=$ $0 \\quad(i=0,1,2,3)$ for all inertial frames and $F^{i j}(\\mathbf{0}, \\mathbf{0})=0$ for all $i, j$, where $\\mathbf{0}=(0,0,0)$, then $\\left(F^{i j}\\right)$ is a second-order contravariant tensor under Lorentz transformations.\n\nLet $\\left(u_{i}\\right)$ be any constant, covariant vector under Lorentz transformations [hence, $\\left(\\bar{u}_{i}\\right)=\\left(b_{i}^{k} u_{k}\\right)$ is also constant]. Define\n\n$$\nS^{i} \\equiv u_{k} F^{k i} \\quad \\bar{S}^{i} \\equiv \\bar{u}_{k} \\bar{F}^{k i}\n$$\n\nBy the given conditions $\\partial \\bar{F}^{i j} / \\partial \\bar{x}^{j}=0$,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=\\bar{u}_{k} \\frac{\\partial \\bar{F}^{k i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial S^{i}}{\\partial x^{i}}\n$$\n\nSuppose that at some point $\\left(x_{0}^{i}\\right), \\bar{S}^{i}=h^{i}\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$; then,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial h^{i}}{\\partial S^{j}} \\frac{\\partial S^{j}}{\\partial x^{k}} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad\\left(b_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}\\right) \\frac{\\partial S^{j}}{\\partial x^{k}}=0\n$$\n\nfor an arbitrary matrix $\\left(\\partial S^{j} / \\partial x^{k}\\right)$ having $\\partial S^{i} / \\partial x^{i}=0$. By a well-known lemma (Problem 12.57), there exists a real number $\\lambda=\\lambda\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$ such that\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda \\delta_{j}^{k} \\tag{1}\n\\end{equation*}\n\n\nNow differentiate both sides of (1) with respect to $S^{l}$ :\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial^{2} h^{i}}{\\partial S^{j} \\partial S^{l}}=\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k} \\tag{2}\n\\end{equation*}\n\n\nwhich is symmetrical in $j$ and $l$; therefore,\n\n\n\\begin{equation*}\n\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k}=\\frac{\\partial \\lambda}{\\partial S^{j}} \\delta_{l}^{k} \\tag{3}\n\\end{equation*}\n\n\nfor all $j, k, l$. Let $k=l \\neq j$ in (3):\n\n$$\n\\frac{\\partial \\lambda}{\\partial S^{k}} \\cdot 0=\\frac{\\partial \\lambda}{\\partial S^{j}} \\cdot 1 \\quad \\text { or } \\quad \\frac{\\partial \\lambda}{\\partial S^{j}}=0\n$$\n\nHence $\\lambda$ is constant with respect to the $S^{i}$ and (1) inverts to give\n\n\n\\begin{equation*}\n\\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda a_{j}^{i} \\tag{4}\n\\end{equation*}\n\n\nIntegrating (4),\n\n\n\\begin{equation*}\nh^{i} \\equiv \\bar{S}^{i}=\\lambda a_{j}^{i} S^{j}+T^{i} \\tag{5}\n\\end{equation*}\n\n\nFor the special assignment $\\mathbf{U}=\\mathbf{V}=\\mathbf{0}$, we have (since $\\overline{\\mathbf{0}}=\\mathbf{0}$ ):\n\n\n\\begin{align*}\n& S^{i}=\\left(u_{1}\\right)(0)+\\left(u_{2}\\right)(0)+\\left(u_{3}\\right)(0)+\\left(u_{4}\\right)(0)=0  \\tag{6}\\\\\n& \\bar{S}^{i}=\\left(\\bar{u}_{1}\\right)(0)+\\left(\\bar{u}_{2}\\right)(0)+\\left(\\bar{u}_{3}\\right)(0)+\\left(\\bar{u}_{4}\\right)(0)=0\n\\end{align*}\n\n\nTogether, (5) and (6) imply $T^{i}=0 \\quad(i=0,1,2,3)$; consequently,\n\n\n\\begin{equation*}\n\\bar{S}^{i}=\\lambda a_{j}^{i} S^{j} \\tag{7}\n\\end{equation*}\n\n\nSimilarly, there exists a real number $\\mu$ such that\n\n\n\\begin{equation*}\nS^{i}=\\mu b_{j}^{i} \\bar{S}^{j} \\tag{8}\n\\end{equation*}\n\n\nIt follows that $\\bar{S}^{i}=\\lambda a_{i}^{i} \\mu b_{k}^{j} \\bar{S}^{k}=\\lambda \\mu \\bar{S}^{i}$, or $\\lambda \\mu=1$. But we can exploit the reciprocal relationship between observers $O$ and $\\bar{O}$, as in Problem 12.4; to show that $\\lambda=\\mu$. Therefore, $\\lambda=\\mu=1$ and (7) or (8) becomes the transformation law of a (contravariant) 4-vector. Finally, we conclude from the Quotient Theorem that if $F^{k i} u_{k} \\equiv S^{i}$ is a tensor for an arbitrary covariant vector $\\left(u_{i}\\right),\\left(F^{i j}\\right)$ is a second-order contravariant tensor.\n\n12.33 Prove the relations (12.40) and (12.41).\n\nBy (12.39) and the constancy of the $g_{i j}$,\n\n$$\n\\frac{\\partial f^{0 j}}{\\partial x^{j}}=\\frac{\\partial f^{00}}{\\partial x^{0}}+\\frac{\\partial f^{0 q}}{\\partial x^{q}}=\\frac{\\partial}{\\partial x^{q}}\\left(-V^{q}\\right)=-\\frac{\\partial V^{q}}{\\partial x^{q}}=-\\operatorname{div} \\mathbf{V}\n$$\n\nand, for $p=1,2,3$,\n\n$$\n\\frac{\\partial f^{p j}}{\\partial x^{j}}=\\frac{\\partial f^{p 0}}{\\partial x^{0}}+\\frac{\\partial f^{p q}}{\\partial x^{q}}=-\\frac{\\partial f^{0 p}}{\\partial x^{0}}+\\frac{\\partial}{\\partial x^{q}}\\left(\\varepsilon_{p q r} U^{r}\\right)=\\frac{\\partial V^{p}}{\\partial x^{0}}-\\varepsilon_{p r q} \\frac{\\partial U^{r}}{\\partial x^{q}}=\\left(\\frac{1}{c} \\frac{\\partial \\mathbf{V}}{\\partial t}+\\operatorname{curl} \\mathbf{U}\\right)_{p}\n$$\n\nThe other two formulas are derived from these by replacing $\\mathbf{U}, \\mathbf{V}$ by $\\mathbf{V},-\\mathbf{U}$.\n\n\\section*{Supplementary Problems}\n12.34 Suppose two events consist of light signals, and an observer sends one of the signals himself. Classify the space-time interval between the events if the observer sees the distant light signal $(a)$ before he sends his own signal, (b) after he sends his own signal, (c) at the same time he sends his own signal.\n\n12.35 Assuming that any velocity less than $c$ is attainable, suppose that a concert in Los Angeles begins at 8:0508 p.m. and one in New York City, 3000 miles away (consider this the accurate distance), begins at 8:0506 p.m. could a person physically attend both events (opening measures only)? Is the pair of events timelike or spacelike?\n\n12.36 Show that the transpose of a Lorentz matrix is Lorentz.\n\n12.37 Verify the expressions (12.12).\n\n12.38 An event occurs at $\\bar{O}$ 's origin at some time $\\bar{t}$. (a) How does $O$ view this event? (b) What is the significance of $a_{0}^{0}>0$ ?\n\n12.39 Write out the simple Lorentz transformation connecting inertial frames $O$ and $\\bar{O}$ that move apart at $80 \\%$ of the velocity of light.\n\n12.40 (a) Confirm that a photon (a particle with the velocity of light in some inertial frame) will be viewed as having the velocity of light in all other inertial frames. (b) What must be the rest mass of such a particle?\n\n12.41 Show that the following matrix is Lorentz, and use Theorem 12.2 to find the matrices $L^{*}, R_{1}$, and $R_{2}$, and the velocity $v$ between the two observers.\n\n$$\nL=\\left[\\begin{array}{crcr}\n5 / 4 & 1 / 2 & 1 / 4 & -1 / 2 \\\\\n-3 / 4 & -5 / 6 & -5 / 12 & 5 / 6 \\\\\n0 & 2 / 3 & 2 / 15 & 11 / 15 \\\\\n0 & -1 / 3 & 14 / 15 & 2 / 15\n\\end{array}\\right]\n$$\n\n12.42 Verify that the following matrix is Lorentz and calculate the velocity between the two observers without finding the simple Lorentz matrix $L^{*}$.\n\n$$\nL=\\left[\\begin{array}{cccc}\n3 / \\sqrt{3} & 1 / \\sqrt{3} & 2 / \\sqrt{3} & -1 / \\sqrt{3} \\\\\n1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & -1 \\\\\n0 & 1 / \\sqrt{3} & -1 / \\sqrt{3} & -1 / \\sqrt{3}\n\\end{array}\\right]\n$$\n\n12.43 Show that by definition the proper-time parameter $\\tau$ is an invariant with respect to all Lorentz transformations\n\n12.44 Verify the formula for the composition of velocities by (i) multiplying the two simple Lorentz matrices below; (ii) calculating from (12.15) the velocities belonging to the two matrices and to their product; (iii) showing that the three velocities obey (12.16).\n\n$$\nL_{1}=\\left[\\begin{array}{cccc}\n13 / 12 & 5 / 12 & 0 & 0 \\\\\n5 / 12 & 13 / 12 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\quad L_{2}=\\left[\\begin{array}{cccc}\n17 / 8 & -15 / 8 & 0 & 0 \\\\\n-15 / 8 & 17 / 8 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\n12.45 An electron gun shoots particles in opposite directions at one-half the velocity of light. At what relative velocity are the particles receding from each other?\n\n12.46 Show that the composition of two velocities less than $c$ is also less than $c$.\n\n12.47 How slow would your watch run relative to a stationary clock if you were moving at $2 / 3$ the velocity of light?\n\n12.48 At the age of 20 , an astronaut left her twin brother on earth to go exploring in outer space. The first two years the spaceship gradually accelerated to a cruising speed 95 percent of the velocity of light. Traveling at that speed for 25 years, it reached a distant galaxy (23.75 light years away) and then decelerated for two years. Two years were spent exploring the galaxy before the journey back home, which followed the schedule of the trip outward. How old is the astronaut when she rejoins her 80-year-old brother? (Use an average rate for clock-retardation during the 8 years in acceleration/deceleration.)\n\n12.49 How fast would a pole-vaulter have to run for his 20 -foot pole to fit (instantaneously) inside a barn, in the judgment of a ground observer for whom the barn is 19 feet 11 inches long?\n\n12.50 An alternate definition of uniformly accelerated motion is motion under a constant Lorentz force. Verify that the two definitions are equivalent for one-dimensional motion.\n\n12.51 Show that $g^{r s} a_{r}^{i} a_{s}^{j}=g^{i j}$.\n\n12.52 Prove that the array (12.46) is a 4-vector.\n\n12.53 Show that the matrices $\\tilde{\\mathscr{F}}$ and $\\mathscr{F}$ of (12.44) are connected via $\\tilde{F}^{i j}=\\frac{1}{2} e_{i j k l} g_{k r} g_{l s} F^{r s}$. [Hint: First evaluate the matrix product $G F G \\equiv P$.]\n\n12.54 Define Faraday's two-form by\n\n$$\n\\Phi \\equiv G \\tilde{\\mathscr{F}} G=\\left[\\begin{array}{cccc}\n0 & -E_{1} & -E_{2} & -E_{3} \\\\\nE_{1} & 0 & H_{3} & -H_{2} \\\\\nE_{2} & -H_{3} & 0 & H_{1} \\\\\nE_{3} & H_{2} & -H_{1} & 0\n\\end{array}\\right]=\\left[\\Phi_{i j}\\right]_{44}\n$$\n\nor, inversely, $\\tilde{\\mathscr{F}}=G \\Phi G$. Show that $(a) \\mathscr{F}$ is related to $\\Phi$ through $F^{i j}=-\\frac{1}{2} e_{i j k l} \\Phi_{k l} ;(b)$ Maxwell's equations can be written in terms of the single matrix $\\Phi$ as\n\n$$\n\\frac{\\partial \\Phi_{i j}}{\\partial x^{k}}+\\frac{\\partial \\Phi_{k i}}{\\partial x^{j}}+\\frac{\\partial \\Phi_{j k}}{\\partial x^{i}}=0 \\quad g_{i k} g_{i l} \\frac{\\partial \\Phi_{k l}}{\\partial x^{j}}=s^{i}\n$$\n\n12.55 The energy flux in an electromagnetic field is specified by the Poynting vector, $\\mathbf{p}=\\mathbf{E} \\times \\mathbf{H}$. By direct matrix multiplication or otherwise, derive the formula\n\n$$\n\\frac{1}{2}(\\tilde{\\mathscr{F}} \\mathscr{F}-\\tilde{F} \\tilde{\\mathscr{F}})=\\left[\\begin{array}{cccc}\n0 & 0 & 0 & 0 \\\\\n* & 0 & p_{3} & -p_{2} \\\\\n* & * & 0 & p_{1} \\\\\n* & * & * & 0\n\\end{array}\\right] \\text { (antisymmetric matrix) }\n$$\n\n12.56 Verify that for simple Lorentz matrices $A$ (hence, no rotation of axes allowed): (a) $\\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V})=$ $G \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) G ;(b) \\overline{\\mathscr{F}}(\\overline{\\mathbf{V}}, \\overline{\\mathbf{U}})=B^{T} \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) B$, where $B=A^{-1} ;$ and $(c) \\tilde{\\mathscr{F}}(\\overline{\\mathbf{U}}, \\overline{\\mathbf{V}})=A \\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V}) A^{T}$ (thereby proving that $\\left(F^{i j}\\right)$ is a contravariant tensor under simple Lorentz transformations).\n\n12.57 Prove that if $A \\equiv\\left[A_{i j}\\right]_{n n}$ satisfies $A_{i j} B_{i j}=0$ for every $B \\equiv\\left[B_{i j}\\right]_{n n}$ that has zero trace $\\left(B_{i i}=0\\right)$, then $A=\\lambda I$, for some real $\\lambda$. [Hint: First take $B$ as having all elements zero, except for one off-diagonal element. Then choose $B_{\\alpha \\alpha}=-B_{\\beta \\beta}=1 \\quad(\\alpha \\neq \\beta ;$ no summation $)$, with all other $B_{i j}$ zero.]\n\n"], "lesson": "\\section*{Chapter 12}\n\\section*{Tensors in Special Relativity}\n\\subsection*{12.1 INTRODUCTION}\nIf the motion of a light pulse were an ordinary phenomenon, its velocity $c$ to one observer would appear to a second observer, moving at velocity $v$ relative to the first, to have the value $c-v$. This hypothetical property of light depends on the concept of absolute time measurement for all observers. However, beginning with the landmark Michelson-Morley experiment in 1880, all experimental data force us to abandon this reasonable hypothesis and to accept instead the now undisputed fact that the velocity of light, rather than the measurement of time, is an absolute of nature. Light is observed to have a single velocity, $c=2.9979 \\times 10^{8} \\mathrm{~m} / \\mathrm{s}$, independent of the observer's motion away from or towards its source. This calls for adjustments to the equations of Newtonian mechanics which become major when high-velocity particles are involved.\n\n\\subsection*{12.2 EVENT SPACE}\nIt is first necessary to wed the concepts of time and space. Thus, each event (atomic collision, flash of lightning, etc.) is assigned four coordinates $(t, x, y, z)$, where $t$ is the time (in seconds) of the event and $(x, y, z)$ is the location (in meters) of the event in ordinary rectangular coordinates. Such coordinates are called space-time coordinates.\n\nDefinition 1: An event space is an $\\mathbf{R}^{4}$ whose points are events, coordinatized by $\\left(x^{i}\\right)=\\left(x^{0}, x^{1}, x^{2}, x^{3}\\right)$, where $x^{0}=c t$ is the temporal coordinate, and $\\left(x^{1}, x^{2}, x^{3}\\right)=(x, y, z)$ the rectangular positional coordinates, of an event. Two events $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ are identical if $x_{1}^{i}=x_{2}^{i}$ for all $i$; simultaneous, if $x_{1}^{0}=x_{2}^{0}$; and copositional, if $x_{1}^{i}=x_{2}^{i}$ for $i=1,2,3$. The spatial distance between $E_{1}$ and $E_{2}$ is the number\n\n\n\\begin{equation*}\nd=\\sqrt{\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}} \\tag{12.1}\n\\end{equation*}\n\n\nwhere $\\Delta x^{i} \\equiv x_{2}^{i}-x_{1}^{i}$ for $i=1,2,3$.\n\n\\section*{Inertial Reference Frames}\nThe general setting for Einstein's Special Theory of Relativity (henceforth abbreviated SR) consists of two or more observers $O, \\bar{O}, \\overline{\\bar{O}}, \\ldots$, moving at constant velocities relative to each other, who set up space-time coordinate systems $\\left(x^{i}\\right),\\left(\\bar{x}^{i}\\right),\\left(\\bar{x}^{i}\\right), \\ldots$ to record events and make calculations for experiments they conduct. Such coordinate systems in uniform relative motion are called inertial frames provided Newton's first law is valid in each system. All the systems are assumed to have a common origin at some instant, which is taken as $t=\\bar{t}=\\overline{\\bar{t}}=\\cdots=0$.\n\n\\section*{Light Cone}\nA flash of light at position $(0,0,0)$ and time $t=0$ sends out an expanding spherical wave front, with equation $x^{2}+y^{2}+z^{2}=c^{2} t^{2}$, or\n\n\n\\begin{equation*}\n\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2}=0 \\tag{12.2}\n\\end{equation*}\n\n\n(12.2) is the equation of the light cone in event space, relative to the inertial frame $\\left(x^{i}\\right)$. Figure 12-1 shows the projection of the light cone onto the hyperplane $x^{3}=0$. In any other inertial frame, $\\left(\\bar{x}^{i}\\right)$, the equation of the light cone is exactly the same (since all observers measure the velocity of light as c):\n\n$$\n\\left(\\bar{x}^{0}\\right)^{2}-\\left(\\bar{x}^{1}\\right)^{2}-\\left(\\bar{x}^{2}\\right)^{2}-\\left(\\bar{x}^{3}\\right)^{2}=0\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-174}\n\\end{center}\n\nFig. 12-1\n\nIn Example 7.6 (using slightly different notation), we identified the light cone with the null geodesics of $\\mathbf{R}^{4}$ under the metric of SR.\n\n\\section*{Relativistic Length}\nFor an arbitrary event $E(\\mathbf{x})$, the quantity $\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2}$ may be positive, negative, or zero. The relativistic distance from $E(\\mathbf{x})$ to the origin $E_{0}(\\mathbf{0})$ is the real number $s \\geqq 0$ such that\n\n$$\n\\varepsilon s^{2}=\\left(x^{0}\\right)^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\left(x^{3}\\right)^{2} \\quad(\\varepsilon= \\pm 1)\n$$\n\nMore generally, the length of interval or relativistic distance between $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ is the unique real number $\\Delta s \\geqq 0$ such that\n\n\n\\begin{equation*}\n\\varepsilon(\\Delta s)^{2}=\\left(\\Delta x^{0}\\right)^{2}-\\left(\\Delta x^{1}\\right)^{2}-\\left(\\Delta x^{2}\\right)^{2}-\\left(\\Delta x^{3}\\right)^{2} \\quad(\\varepsilon= \\pm 1) \\tag{12.3}\n\\end{equation*}\n\n\nwhere $\\Delta x^{i} \\equiv x_{2}^{i}-x_{1}^{i}$ for $i=0,1,2,3$. The chief significance of this length-concept is to be found in Theorem 12.1: Relativistic distance is an invariant across all inertial frames.\n\nFor a proof, see Problem 12.6.\n\n\\section*{Interval Types}\nThe interval between $E_{1}\\left(\\mathbf{x}_{1}\\right)$ and $E_{2}\\left(\\mathbf{x}_{2}\\right)$ is\n\n(1) spacelike if $\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}>\\left(\\Delta x^{0}\\right)^{2} \\quad$ (or $\\varepsilon=-1$; predominance of distance over time);\n\n(2) lightlike if $\\left(\\Delta x^{0}\\right)^{2}=\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2} \\quad$ (equality of time and distance);\n\n(3) timelike if $\\left(\\Delta x^{0}\\right)^{2}>\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2} \\quad$ (or $\\varepsilon=+1$; predominance of time over distance).\n\nBy Theorem 12.1, the categorization is independent of the particular inertial frame.\n\n\\subsection*{12.3 THE LORENTZ GROUP AND THE METRIC OF SR}\nImagine two observers, $\\dot{O}$ and $\\bar{O}$, in uniform relative motion at speed $v$. They approach each other during negative time, coincide at zero time, and then recede from each other during positive time (Fig. 12-2(a)). Let $O$ and $\\bar{O}$ set up independent reference frames $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ by means of identical but separate clocks, with $t=\\bar{t}=0$ when $O=\\bar{O}$, and identical metersticks. Newton's first law will be assumed to hold in both frames, making them inertial frames.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175(1)}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175}\n\\end{center}\n\n(b)\n\nFig. 12-2\n\nCommon observation of events sets up a correspondence\n\n\n\\begin{equation*}\n\\mathscr{T}: \\bar{x}^{i}=F^{i}\\left(x^{0}, x^{1}, x^{2}, x^{3}\\right) \\tag{12.4}\n\\end{equation*}\n\n\nthat is bijective, since each event is assigned a unique set of coordinates. In most of what follows, it will be assumed that $O$ and $\\bar{O}$ perform a simplifying maneuver at the instant of coincidence, whereby their $x$-axes point in the same direction along the line of motion and their $y$-axes and their $z$-axes coincide. In the ensuing translation, the $y$-axes and the $z$-axes remain parallel (Fig. 12-2(b)).\n\n\\section*{Postulates of SR}\n(1) Principle of Relativity: The laws of physics are the same in all inertial frames.\n\n(2) Invariance of Uniform Motion: A particle with constant velocity in one inertial frame will have constant velocity in all inertial frames.\n\n(3) Invariance of Light Speed: The speed of light is invariant across all inertial frames.\n\nPostulate 2 requires that the bijective transformation (12.4) be such as to map straight lines into straight lines. Consequently, each $F^{i}$ must be a linear function. Since $F^{i}(\\mathbf{0})=\\mathbf{0}$, constants $a_{j}^{i}$ exist such that\n\n\n\\begin{equation*}\nT: \\bar{x}^{i}=a_{j}^{i} x^{j} \\tag{12.5}\n\\end{equation*}\n\n\n\\section*{Lorentz Matrices and Transformations}\nThe invariance of the equation of the light cone (in consequence of Postulate 3) may be expressed as\n\n\n\\begin{equation*}\ng_{i j} x^{i} x^{j}=0=g_{i j} \\bar{x}^{i} \\bar{x}^{j} \\tag{12.6}\n\\end{equation*}\n\n\nwhere $g_{00}=1, g_{11}=g_{22}=g_{33}=-1$, and $g_{i j}=0$ for $i \\neq j$. Substitution of (12.5) in (12.6) yields (Problem 12.4):\n\n\n\\begin{equation*}\ng_{i j} a_{r}^{i} a_{s}^{j}=g_{r s} \\tag{12.7a}\n\\end{equation*}\n\n\nor, in matrix form,\n\nor, written out,\n\n\n\\begin{equation*}\nA^{T} G A=G \\tag{12.7b}\n\\end{equation*}\n\n\n\\[\n\\begin{array}{rlrl}\n\\left(a_{0}^{0}\\right)^{2}-\\left(a_{0}^{1}\\right)^{2}-\\left(a_{0}^{2}\\right)^{2}-\\left(a_{0}^{3}\\right)^{2} & =1 & \\\\\n\\left(a_{j}^{0}\\right)^{2}-\\left(a_{j}^{1}\\right)^{2}-\\left(a_{j}^{2}\\right)^{2}-\\left(a_{j}^{3}\\right)^{2}=-1 & & (j=1,2,3)  \\tag{12.7c}\\\\\na_{i}^{0} a_{j}^{0}-a_{i}^{1} a_{j}^{1}-a_{i}^{2} a_{j}^{2}-a_{i}^{3} a_{j}^{3} & =0 & & (i \\neq j)\n\\end{array}\n\\]\n\nIt is easy to see (Problem 12.8) that requiring $g_{i j} x^{i} x^{j}=0$ to be invariant is tantamount to requiring the invariance of $g_{i j} x^{i} x^{j}=q$ for every value of $q$. Thus, (12.7) is a criterion for the quadratic form $g_{i j} x^{i} x^{j}$ to be invariant.\n\nDefinition 2: Any $4 \\times 4$ matrix (or corresponding linear transformation) that preserves the quadratic form $\\mathbf{x}^{T} G \\mathbf{x}$ is called Lorentz.\n\nIn Problem 12.10 it is shown that the set of Lorentz matrices constitutes a group (the Lorentz group) under matrix multiplication.\n\n\\section*{Metric of SR}\nIf the terms $\\bar{g}_{i j} \\equiv g_{i j}$ are defined for the $\\left(\\bar{x}^{i}\\right)$-system, then (12.7a) becomes $g_{r s}=\\bar{g}_{i j} a_{r}^{i} a_{s}^{j}$, which makes $\\left(g_{i j}\\right)$ a covariant tensor of the second order under Lorentz transformations of coordinates. Accordingly, the metric for $\\mathbf{R}^{4}$ is chosen as\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \\equiv\\left(d x^{0}\\right)^{2}-\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2} \\tag{12.8}\n\\end{equation*}\n\n\n\\subsection*{12.4 SIMPLE LORENTZ MATRICES}\nLet us suppose that $O$ and $\\bar{O}$ have performed an alignment of their $x y z$-axes. Then any right circular cylinder with axis along the line of relative motion must have the same equation in the two systems; i.e., $\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}$ is invariant. It follows (see Problem 12.11) that the Lorentz transformation for this situation has the form\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{0}=a_{0}^{0} x^{0}+a_{1}^{0} x^{1} \\equiv a x^{0}+b x^{1}  \\tag{12.9}\\\\\n\\bar{x}^{1}=a_{0}^{1} x^{0}+a_{1}^{1} x^{1} \\equiv d x^{0}+e x^{1} \\\\\n\\bar{x}^{2}=x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array}\\right.\n\\]\n\nBy (12.7),\n\n\n\\begin{equation*}\na^{2}-d^{2}=1 \\quad b^{2}-e^{2}=-1 \\quad a b-d e=0 \\tag{12.10}\n\\end{equation*}\n\n\nBy considering the coordinates which $O$ and $\\bar{O}$ would assign to each other's origin (see Problem 12.12), we find that\n\n\n\\begin{equation*}\nd=-(v / c) a \\equiv-\\beta a \\quad \\text { and } \\quad a=e \\tag{12.11}\n\\end{equation*}\n\n\n(The notation $\\beta=v / c$ is standard in SR.) From (12.10) and the fact that $a>0$ (since both clocks can be assumed to run in the same sense), it follows that\n\n\n\\begin{equation*}\na=\\left(1-\\beta^{2}\\right)^{-1 / 2}=e \\quad b=-\\beta\\left(1-\\beta^{2}\\right)^{-1 / 2}=d \\tag{12.12}\n\\end{equation*}\n\n\nTherefore, the coordinate transformation takes on the simplified form\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{0}=\\frac{x^{0}-\\beta x^{1}}{\\sqrt{1-\\beta^{2}}}  \\tag{12.13}\\\\\n\\bar{x}^{1}=\\frac{-\\beta x^{0}+x^{1}}{\\sqrt{1-\\beta^{2}}} \\\\\n\\bar{x}^{2}=x^{2} \\\\\n\\bar{x}^{3}=x^{3}\n\\end{array} \\quad \\text { or } \\quad A=\\left[\\begin{array}{cccc}\n\\frac{1}{\\sqrt{1-\\beta^{2}}} & \\frac{-\\beta}{\\sqrt{1-\\beta^{2}}} & 0 & 0 \\\\\n\\frac{-\\beta}{\\sqrt{1-\\beta^{2}}} & \\frac{1}{\\sqrt{1-\\beta^{2}}} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\right.\n\\]\n\nAny $4 \\times 4$ matrix (linear transformation) of the form\n\n$$\nA=\\left[\\begin{array}{llll}\na & b & 0 & 0 \\\\\nb & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhere $a^{2}-b^{2}=1$, will be termed simple Lorentz. The relative velocity in the physical situation modeled by $A$ is recovered as $\\beta=-b / a$.\n\nEXAMPLE 12.1 By Problem 12.9, the inverse of a simple Lorentz matrix is\n\n$$\nA^{-1}=\\left[\\begin{array}{cccc}\na & -b & 0 & 0 \\\\\n-b & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhich is itself a simple Lorentz matrix, corresponding to a reversal in sign of $\\beta$. [If the velocity of $\\bar{O}$ with respect to $O$ is $v$, then the velocity of $O$ with respect to $\\bar{O}$ is $-v$.]\n\n\\section*{A Decomposition Theorem}\nThe possibility of simplifying an arbitrary Lorentz matrix by a suitable rotation of axes can be expressed in purely mathematical terms. (See Problems 12.14 and 12.15.)\n\nTheorem 12.2: An arbitrary Lorentz matrix $L=\\left(a_{j}^{i}\\right)$ has the representation\n\n$$\nL=R_{1} L^{*} R_{2}\n$$\n\nwhere $L^{*}$ is a simple Lorentz matrix with parameters $a=\\left|a_{0}^{0}\\right|=\\varepsilon a_{0}^{0}$ and $b=$ $-\\sqrt{\\left(a_{0}^{0}\\right)^{2}-1}$, and $R_{1}$ and $R_{2}$ are orthogonal Lorentz matrices defined by\n\n\\[\nR_{1}=L R_{2}^{T}\\left(L^{*}\\right)^{-1} \\quad \\text { and } \\quad R_{2}=\\left[\\begin{array}{llll}\n\\mathbf{e}_{1} & \\mathbf{r}^{\\prime} & \\mathbf{s}^{\\prime} & \\mathbf{t}^{\\prime} \\tag{12.14}\n\\end{array}\\right]^{T}\n\\]\n\nHere, $\\mathbf{e}_{1}=(1,0,0,0), \\mathbf{r}^{\\prime}=(\\varepsilon / b)\\left(0, a_{1}^{0}, a_{2}^{0}, a_{3}^{0}\\right) \\equiv(0, \\mathbf{r}), \\mathbf{s}^{\\prime}=(0, \\mathbf{s}), \\mathbf{t}^{\\prime}=(0, \\mathbf{t})$, with $\\mathbf{s}$ and $\\mathbf{t}$ chosen to complete a $3 \\times 3$ orthogonal matrix $\\left[\\begin{array}{lll}\\mathbf{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$.\n\nCorollary 12.3: If $L=\\left(a_{j}^{i}\\right)$ connects two inertial frames, then the relative velocity between the frames is\n\n\n\\begin{equation*}\nv=c \\sqrt{1-\\left(a_{0}^{0}\\right)^{-2}} \\tag{12.15}\n\\end{equation*}\n\n\n\\subsection*{12.5 PHYSICAL IMPLICATIONS OF THE SIMPLE LORENTZ TRANSFORMATION}\n\\section*{Length Contraction}\nFor any fixed $x^{0},(12.13)$ gives\n\n$$\n\\Delta \\bar{x}^{1}=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta x^{1} \\quad \\text { or } \\quad \\Delta x^{1}=\\sqrt{1-\\beta^{2}} \\Delta \\bar{x}^{1}<\\Delta \\bar{x}^{1}\n$$\n\nIf the frame $\\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, distances in $\\bar{O}$ appear to observer $O$ to be foreshortened in the direction of the motion by the factor $\\sqrt{1-\\beta^{2}}$.\n\n\\section*{Time Dilation}\nFor any fixed $\\bar{x}^{1},(12.13)$, inverted, gives\n\n$$\n\\Delta x^{0}=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta \\bar{x}^{0} \\quad \\text { or } \\quad \\Delta t=\\frac{1}{\\sqrt{1-\\beta^{2}}} \\Delta \\bar{t}>\\Delta \\bar{t}\n$$\n\nIf the frame $\\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, the clock of observer $\\bar{O}$ appears to observer $O$ to run slow by the factor $\\sqrt{1-\\beta^{2}}$.\n\n\\section*{Composition of Velocities}\nIf $\\bar{O}$ has velocity $v_{1}$ relative to $O$ and $\\overline{\\bar{O}}$ has velocity $v_{2}$ relative to $\\bar{O}$, then the Newtonian composition of velocities predicts that $\\bar{O}$ has velocity $v_{3}=v_{1}+v_{2}$ relative to $O$. Although the error does not show up unless $v_{1}$ and $v_{2}$ are substantial fractions of the velocity of light, SR shows the Newtonian theory to be incorrect. The correct formula (Problem 12.20) is\n\n\n\\begin{equation*}\nv_{3}=\\frac{v_{1}+v_{2}}{1+v_{1} v_{2} / c^{2}} \\tag{12.16}\n\\end{equation*}\n\n\n\\subsection*{12.6 RELATIVISTIC KINEMATICS}\n\\section*{4-Vectors}\nWe begin with ordinary velocity and acceleration of a particle within a single inertial frame $\\left(x^{i}\\right)$. By introducing the concept of proper time, we shall be able to obtain velocity and acceleration as contravariant vectors with respect to Lorentz transformations (to be called 4-vectors from now on). In general, $\\left(V^{i}\\right)$ is a 4-vector if it transforms according to the law $\\bar{V}^{i}=a_{j}^{i} V^{j}$, where $\\left(a_{j}^{i}\\right)$ is the Lorentz matrix of (12.5). It is customary to use the following notation for 4-vectors:\n\n$$\n\\left(V^{i}\\right) \\equiv\\left(V^{0}, \\mathbf{V}\\right) \\quad \\text { where } \\quad V^{0} \\equiv V_{t} \\quad \\text { and } \\quad \\mathbf{V} \\equiv\\left(V^{1}, V^{2}, V^{3}\\right) \\equiv\\left(V_{x}, V_{y}, V_{z}\\right)\n$$\n\n$V^{0}$ is referred to as the time component of the vector and $\\left(V_{x}, V_{y}, V_{z}\\right)$ are the usual space components. All indices are understood to range over the values $0,1,2,3$, unless specifically noted otherwise.\n\n\\section*{Nonrelativistic Velocity and Acceleration}\nIn the inertial frame $\\left(x^{i}\\right)=(x, y, z)$ let a particle describe the class $C^{2}$ curve\n\n$$\n\\mathscr{K}:\\left(x^{i}\\right)=(c t, \\mathbf{r}(t))=(c t, x(t), y(t), z(t))\n$$\n\nThen we have the classical formulas\n\n\n\\begin{equation*}\n\\left(v_{i}\\right)=\\left(\\frac{d x^{i}}{d t}\\right) \\equiv(c, \\mathbf{v}) \\tag{12.17}\n\\end{equation*}\n\n\nwhere $\\mathbf{v}=d \\mathbf{r} / d t$ and $\\hat{v} \\equiv\\|\\mathbf{v}\\|=\\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}$;\n\n\n\\begin{equation*}\n\\left(a_{i}\\right)=\\left(\\frac{d^{2} x^{i}}{d t^{2}}\\right) \\equiv(0, \\mathbf{a}) \\tag{12.18}\n\\end{equation*}\n\n\nwhere $\\mathbf{a}=d \\mathbf{v} / d t$ and $\\hat{a}=\\|\\mathbf{a}\\|=\\sqrt{a_{x}^{2}+a_{y}^{2}+a_{z}^{2}}$.\n\nAs defined, neither the velocity nor the acceleration is a tensor under Lorentz transformations. In fact (Problem 12.22), if $\\left(\\bar{v}_{i}\\right)$ and $\\left(\\bar{a}_{i}\\right)$ are the like quantities in $\\left(\\bar{x}^{i}\\right)$, then (12.13) yields the relations\n\n\\[\n\\begin{array}{cc}\n\\bar{v}_{0}=c=v_{0} & \\bar{v}_{x}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\quad \\bar{v}_{y}=\\frac{v_{y} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\quad \\bar{v}_{z}=\\frac{v_{z} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\\\\n\\bar{a}_{0}=0=a_{0} \\quad \\bar{a}_{x}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\quad \\bar{a}_{y}=\\frac{\\left[a_{y}+\\left(v_{y} a_{x}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)\\right]\\left(1-\\beta^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\\\\n\\bar{a}_{z}=\\frac{\\left[a_{z}+\\left(v_{z} a_{x}-v_{x} a_{z}\\right)\\left(v / c^{2}\\right)\\right]\\left(1-\\beta^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{3}} \\tag{12.20}\n\\end{array}\n\\]\n\nThe inverse relations can be quickly obtained by replacing $v$ by $-v$ and interchanging barred and unbarred terms throughout. For example, the second formula in (12.19) inverts to\n\n$$\nv_{x}=\\frac{\\bar{v}_{x}+v}{1+\\bar{v}_{x} v / c^{2}}\n$$\n\nwhich is just (12.16) as applied to $v_{1}=\\bar{v}_{x}$ and $v_{2}=v$.\n\n\\section*{Proper Time; Velocity and Acceleration 4-Vectors}\nLet us reparameterize the curve $\\mathscr{K}$, choosing now the quantity\n\n\n\\begin{equation*}\n\\tau=\\frac{s}{c}=\\frac{1}{c} \\int_{t_{0}}^{t} \\sqrt{\\varepsilon g_{i j} \\frac{d x^{i}}{d u} \\frac{d x^{j}}{d u}} d u \\quad \\text { or } \\quad \\frac{d \\tau}{d t}=\\sqrt{1-\\hat{v}^{2} / c^{2}} \\tag{12.21}\n\\end{equation*}\n\n\nwhere, as always, $\\hat{v}<c$. The new parameter $\\tau$ (a distance divided by a velocity) is known as the proper time for the particle; by Problem 12.23, a clock attached to the particle (and thus accelerating and decelerating along with it) reads $\\tau$.\n\nWhen $\\tau$-derivatives replace $t$-derivatives, velocity and acceleration become tensorial; i.e., the components\n\n\n\\begin{equation*}\nu^{i} \\equiv \\frac{d x^{i}}{d \\tau} \\quad b^{i} \\equiv \\frac{d u^{i}}{d \\tau}=\\frac{d^{2} x^{i}}{d \\tau^{2}} \\tag{12.22}\n\\end{equation*}\n\n\nare taken by (12.13) into\n\n\\[\n\\begin{array}{llll}\n\\bar{u}^{0}=\\frac{u^{0}-\\beta u^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{u}^{1}=\\frac{-\\beta u^{0}+u^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{u}^{2}=u^{2} & \\bar{u}^{3}=u^{3} \\\\\n\\bar{b}^{0}=\\frac{b^{0}-\\beta b^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{b}^{1}=\\frac{-\\beta b^{0}+b^{1}}{\\sqrt{1-\\beta^{2}}} & \\bar{b}^{2}=b^{2} & \\bar{b}^{3}=b^{3} \\tag{12.24}\n\\end{array}\n\\]\n\nThe important identities\n\n\n\\begin{equation*}\nu_{i} u^{i}=c^{2} \\quad u_{i} b^{i}=0 \\tag{12.25}\n\\end{equation*}\n\n\nare proved in Problem 12.24, and Problem 12.25 establishes the following connecting formulas between the numerical values of the relativistic and the nonrelativistic components:\n\n\n\\begin{equation*}\nu^{i}=\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\quad b^{i}=\\frac{a_{i}}{1-\\hat{v}^{2} / c^{2}}+\\frac{(\\mathbf{v a}) v_{i}}{c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}} \\tag{12.26}\n\\end{equation*}\n\n\n\\section*{Instantaneous Rest Frame}\nAt time $t=t_{1}$, the particle moving along $\\mathscr{K}$ has instantaneous position $P_{1}=\\mathbf{r}\\left(t_{1}\\right)$ and instantaneous speed $\\hat{v}\\left(t_{1}\\right)$. An instantaneous rest frame for the particle is an inertial frame that translates at speed $\\hat{v}\\left(t_{1}\\right)$ along the tangent to $\\mathscr{K}$ at $P_{1}$, in such manner that its origin coincides with $P_{1}$ at $t=t_{1}$. See Fig. 12-3.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-180}\n\\end{center}\n\nFig. 12-3\n\nWe shall say that a particle's motion (with respect to frame $O$ ) is uniformly accelerated if its spatial acceleration relative to an instantaneous rest frame $\\bar{O}$,\n\n$$\n\\alpha=\\hat{\\bar{a}}=\\sqrt{\\bar{a}_{x}^{2}+\\bar{a}_{y}^{2}+\\bar{a}_{z}^{2}}\n$$\n\ndoes not vary along the trajectory $\\mathscr{K}$.\n\nEXAMPLE 12.2 An electron fired at right angles into a uniform magnetic field undergoes uniformly accelerated motion (in a circle).\n\n\\subsection*{12.7 RELATIVISTIC MASS, FORCE, AND ENERGY}\nThe appropriate SR version of Newton's second law depends on the concept of mass to be adopted.\n\n\\section*{Rest Mass and Relativistic Mass}\nThe rest mass of a particle is its mass as measured or as inferred from Newtonian mechanics in any instantaneous rest frame for that particle.\n\nThe relativistic mass (in $O$ ) of a particle with spatial velocity $\\mathbf{v}$ (with respect to $O$ ) is\n\n\n\\begin{equation*}\n\\hat{m}=\\frac{m}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\tag{12.27}\n\\end{equation*}\n\n\nwhere $m$ is the rest mass of the particle. As is shown in Problem 12.27, (12.27) is a necessary consequence of conservation of momentum.\n\n\\section*{Relativistic Momentum and Force}\nThe 4-momentum of SR is defined by\n\n\n\\begin{equation*}\n\\left(p^{i}\\right) \\equiv\\left(p^{0}, \\mathbf{p}\\right)=\\left(m u^{i}\\right)=\\left(\\hat{m} v_{i}\\right) \\tag{12.28}\n\\end{equation*}\n\n\nand the (nontensorial) Lorentz force $\\left(F_{0}, \\mathbf{F}\\right)$ is defined as the time derivative of the 4-momentum:\n\n\n\\begin{equation*}\nF_{0} \\equiv \\frac{d p^{0}}{d t}=\\frac{d}{d t}\\left(\\frac{m c}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\quad \\mathbf{F} \\equiv \\frac{d}{d t}\\left(\\frac{m \\mathbf{v}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\tag{12.29}\n\\end{equation*}\n\n\nLike velocity, force becomes tensorial when proper time is introduced. Thus, the 4-force (Minkowski force) of SR is defined as\n\nFrom (12.21), we have the connecting formula\n\n\n\\begin{equation*}\n\\left(K^{i}\\right) \\equiv\\left(\\frac{d p^{i}}{d \\tau}\\right) \\tag{12.30}\n\\end{equation*}\n\n\n\n\\begin{equation*}\nK^{i}=\\frac{F_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\tag{12.31}\n\\end{equation*}\n\n\nThe following identities for the Lorentz and Minkowski forces are proved in Problem 12.29:\n\n\n\\begin{equation*}\nu_{i} K^{i}=0 \\quad K^{0}=\\frac{1}{c} \\mathbf{v K} \\quad F_{0}=\\frac{1}{c} \\mathbf{v F} \\quad \\mathbf{v F}=\\frac{d}{d t}\\left(\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right) \\tag{12.32}\n\\end{equation*}\n\n\n\\section*{Relativistic Energy}\nAccording to the classical work-energy theorem, the rate at which work is performed on a particle, (vF), equals the rate of increase of the particle's kinetic energy. Thus, the last identity (12.32) suggests the definition for $\\mathrm{SR}$\n\n\n\\begin{equation*}\n\\hat{E}=\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\equiv \\hat{m} c^{2} \\tag{12.33}\n\\end{equation*}\n\n\nfor the energy of a particle moving at speed $\\hat{v}$. In the limit as $\\hat{v} \\rightarrow 0,(12.33)$ becomes\n\n\n\\begin{equation*}\nE=m c^{2} \\tag{12.34}\n\\end{equation*}\n\n\nThis is Einstein's famous formula for the rest energy $E$ of a particle with rest mass $m$.\n\n\\subsection*{12.8 MAXWELL'S EQUATIONS IN SR}\nIt is helpful to look briefly at the way the metric for Special Relativity affects the formulas for the divergence and the Laplacian, and to consider a new kind of matrix that will be useful in formulating Maxwell's equations.\n\n\\section*{Vector Calculus and Lorentz Transformations}\nFor the metric of SR, all Christoffel symbols vanish, so that\n\n\n\\begin{equation*}\n\\operatorname{div} \\mathbf{u}=\\frac{\\partial u^{i}}{\\partial x^{i}} \\tag{12.35}\n\\end{equation*}\n\n\nand\n\n\n\\begin{align*}\n\\operatorname{div}(\\operatorname{grad} f) \\equiv \\square f & =\\frac{\\partial}{\\partial x^{i}}\\left(g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right)=g^{i j} \\frac{\\partial^{2} f}{\\partial x^{i} \\partial x^{j}} \\\\\n& =\\frac{\\partial^{2} f}{\\left(\\partial x^{0}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{1}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{2}\\right)^{2}}-\\frac{\\partial^{2} f}{\\left(\\partial x^{3}\\right)^{2}} \\\\\n& =\\frac{1}{c^{2}} \\frac{\\partial^{2} f}{\\partial t^{2}}-\\nabla^{2} f \\tag{12.36}\n\\end{align*}\n\n\nNote that in SR the Laplacian operator is notated $\\square$, with $\\nabla^{2}$ reserved for its spatial part. It is verified in Problem 12.31 that $\\square f$ is an invariant under Lorentz transformations, which means that the scalar wave equation has the same form, $\\square f=0$, in every inertial frame.\n\nIf we introduce the differential operator\n\n\n\\begin{equation*}\n\\partial^{i} \\equiv g^{i j} \\frac{\\partial}{\\partial x^{j}} \\tag{12.37}\n\\end{equation*}\n\n\nthen we can express the equation of continuity for a vector $\\left(w_{i}\\right) \\equiv\\left(w_{0}, \\mathbf{w}\\right)$ as\n\n\n\\begin{equation*}\n\\partial^{i} w_{i}=0 \\tag{12.38}\n\\end{equation*}\n\n\n(12.38) is equivalent to $\\partial w_{0} / \\partial t=c \\operatorname{div} \\mathbf{w}$.\n\n\\section*{Maxwell's Equations in Event Space}\nFirst, introduce for any two 3-vectors $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ the two antisymmetric matrices\n\n\\[\n\\left[f^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & -V^{1} & -V^{2} & -V^{3}  \\tag{12.39a}\\\\\nV^{1} & 0 & U^{3} & -U^{2} \\\\\nV^{2} & -U^{3} & 0 & U^{1} \\\\\nV^{3} & U^{2} & -U^{1} & 0\n\\end{array}\\right] \\quad\\left[\\tilde{f}_{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & U^{1} & U^{2} & U^{3} \\\\\n-U^{1} & 0 & V^{3} & -V^{2} \\\\\n-U^{2} & -V^{3} & 0 & V^{1} \\\\\n-U^{3} & V^{2} & -V^{1} & 0\n\\end{array}\\right]\n\\]\n\nThe second matrix may be obtained from the first by making the replacements $\\mathbf{V} \\rightarrow-\\mathbf{U}$ and $\\mathbf{U} \\rightarrow \\mathbf{V}$; because these replacements constitute an anti-involution-i.e., $\\tilde{f}_{i j}=-f^{i j}-$ the two matrices are said to be dual to each other. In terms of individual components $\\left(e_{p q r}\\right.$ denotes the permutation symbol of order 3):\n\n\\[\n\\begin{array}{lll}\nf^{i j}=-f^{j i} & f^{0 q}=-V^{q} & f^{p q}=e_{p q r} U^{r}  \\tag{12.39b}\\\\\n\\tilde{f}_{i j}=-\\tilde{f}_{j i} & \\tilde{f}_{0 q}=U^{q} & \\tilde{f}_{p q}=e_{p q r} V^{r}\n\\end{array}\n\\]\n\nin which $i, j \\geqq 0$ and $p, q \\geqq 1$.\n\nBy their concoction, these matrices turn out to be tensors under Lorentz transformations (provided the row-divergences vanish in all inertial frames); a proof is given in Problem 12.32. Moreover, these tensors have the properties (Problem 12.33)\n\n\n\\begin{equation*}\n\\frac{\\partial f^{0 j}}{\\partial x^{j}}=-\\operatorname{div} \\mathbf{V} \\quad \\frac{\\partial \\tilde{f}_{0 j}}{\\partial x^{j}}=\\operatorname{div} \\mathbf{U} \\tag{12.40}\n\\end{equation*}\n\n\nand\n\n\n\\begin{equation*}\n\\left(\\frac{\\partial f^{1 j}}{\\partial x^{j}}, \\frac{\\partial f^{2 j}}{\\partial x^{j}}, \\frac{\\partial f^{3 j}}{\\partial x^{j}}\\right)=\\operatorname{curl} \\mathbf{U}+\\frac{1}{c} \\frac{\\partial \\mathbf{V}}{\\partial t} \\quad\\left(\\frac{\\partial \\tilde{f}_{1 j}}{\\partial x^{j}}, \\frac{\\partial \\tilde{f}_{2 j}}{\\partial x^{j}}, \\frac{\\partial \\tilde{f}_{3 j}}{\\partial x^{j}}\\right)=\\operatorname{curl} \\mathbf{V}-\\frac{1}{c} \\frac{\\partial \\mathbf{U}}{\\partial t} \\tag{12.41}\n\\end{equation*}\n\n\nWe now show how Maxwell's equations in vacuum, (11.19), may be extended to space-time via dual tensors of this sort. The equations are:\n\n\\[\n\\begin{array}{ll}\n\\operatorname{div} \\mathbf{H}=0 & \\operatorname{curl} \\mathbf{E}+\\frac{1}{c} \\frac{\\partial \\mathbf{H}}{\\partial t}=\\mathbf{0} \\\\\n\\operatorname{div} \\mathbf{E}=\\rho & \\operatorname{curl} \\mathbf{H}-\\frac{1}{c} \\frac{\\partial \\mathbf{E}}{\\partial t}=\\frac{\\rho}{c} \\mathbf{v} \\tag{12.43}\n\\end{array}\n\\]\n\nin the last of which $\\mathbf{v}$ is the classical spatial velocity (12.17) of the charge-cloud $\\rho$. Define per (12.39) the tensors\n\n\\[\n\\mathscr{F}=\\left[F^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{cccc}\n0 & -H_{1} & -H_{2} & -H_{3}  \\tag{12.44}\\\\\nH_{1} & 0 & E_{3} & -E_{2} \\\\\nH_{2} & -E_{3} & 0 & E_{1} \\\\\nH_{3} & E_{2} & -E_{1} & 0\n\\end{array}\\right] \\quad \\tilde{\\mathscr{F}}=\\left[\\tilde{F}^{i j}\\right]_{44} \\equiv\\left[\\begin{array}{ccccc}\n0 & E_{1} & E_{2} & E_{3} \\\\\n-E_{1} & 0 & H_{3} & -H_{2} \\\\\n-E_{2} & -H_{3} & 0 & H_{1} \\\\\n-E_{3} & H_{2} & -H_{1} & 0\n\\end{array}\\right]\n\\]\n\n(with $\\mathbf{U}=\\mathbf{E}$ and $\\mathbf{V}=\\mathbf{H}$ ). In view of the first equations (12.40) and (12.41), (12.42) may be written as\n\n\n\\begin{equation*}\n\\frac{\\partial F^{i j}}{\\partial x^{j}}=0 \\tag{12.45a}\n\\end{equation*}\n\n\nSimilarly, if we make a 4-vector out of $\\mathbf{v}$ and $\\rho$ by the prescription\n\n\n\\begin{equation*}\n\\left(s^{i}\\right) \\equiv\\left(\\rho, \\frac{\\rho}{c} \\mathbf{v}\\right) \\tag{12.46}\n\\end{equation*}\n\n\n(see Problem 12.52), then the remaining Maxwell's equations, (12.43), are rendered tensorial as\n\n\n\\begin{equation*}\n\\frac{\\partial \\tilde{F}^{i j}}{\\partial x^{j}}=s^{i} \\tag{12.45b}\n\\end{equation*}\n\n\nEquations (12.45) are the relativistic Maxwell's equations, valid in every inertial frame. Because $\\tilde{\\mathbf{F}}$ is antisymmetric, we have from $(12.45 \\mathrm{~b})$ :\n\n$$\n\\frac{\\partial s^{i}}{\\partial x^{i}}=\\frac{\\partial^{2} \\tilde{F}^{i j}}{\\partial x^{i} \\partial x^{j}}=0 \\quad \\text { or } \\quad \\frac{\\partial}{\\partial x^{i}}\\left(g^{i j} s_{j}\\right)=\\left(g^{j i} \\frac{\\partial}{\\partial x^{i}}\\right) s_{j} \\equiv \\partial^{j} s_{j}=0\n$$\n\nso that the covariant vector $\\left(s_{j}\\right)$ obeys the equation of continuity (12.38).\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{EVENT SPACE}\n12.1 Calculate $\\varepsilon$ and $\\Delta s$ for the event pairs: (a) $E_{1}(5,1,-2,0)$ and $E_{2}(0,3,1,-3),(b)$ $E_{1}(5,1,3,3)$ and $E_{2}(2,-1,1,1),(c) E_{1}(7,2,4,4)$ and $E_{2}(4,1,2,6),(d) E_{1} \\equiv$ flash of light in Chicago at 7 p.m. and $E_{2} \\equiv$ flash of light in St. Louis ( 400 miles away) at 7.00000061 p.m. (e) Determine the interval type in each case.\n\n(a) $\\varepsilon(\\Delta s)^{2}=5^{2}-(-2)^{2}-(-3)^{2}-3^{2}=25-4-9-9=3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=1$.\n\n(b) $\\varepsilon(\\Delta s)^{2}=9-4-4-4=-3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=-1$.\n\n(c) $\\varepsilon(\\Delta s)^{2}=9-1-4-4=0$, or $\\Delta s=0$ and $\\varepsilon=1$.\n\n(d) With $c=186300 \\mathrm{mi} / \\mathrm{sec}, \\varepsilon(\\Delta s)^{2}=(0.002196 c)^{2}-(400)^{2} \\approx 7375 \\mathrm{mi}^{2}$, or $\\Delta s \\approx 85.8 \\mathrm{mi}$ and $\\varepsilon=1$.\n\n(e) Timelike, spacelike, lightlike, and timelike, respectively.\n\n12.2 Show that (a) simultaneous events have a spacelike interval; (b) copositional events have a timelike interval; $(c)$ the interval between two light flashes is lightlike if they are simultaneous to an observer who is present at the site of one of the flashes.\n\n\n\\begin{gather*}\n\\varepsilon(\\Delta s)^{2}=0^{2}-\\left(\\Delta x^{1}\\right)^{2}-\\left(\\Delta x^{2}\\right)^{2}-\\left(\\Delta x^{3}\\right)^{2}<0  \\tag{a}\\\\\n\\varepsilon(\\Delta s)^{2}=\\left(\\Delta x^{0}\\right)^{2}-0>0 \\tag{b}\n\\end{gather*}\n\n\n$$\n\\varepsilon(\\Delta s)-(\\Delta x)-0<0\n$$\n\n(c) Let the observer measure the proximate flash as $E_{1}(0,0,0,0)$. The distant flash $E_{2}\\left(c \\Delta t, \\Delta x^{1}, \\Delta x^{2}, \\Delta x^{3}\\right)$ will be registered simultaneously, at $x^{0}=0$, if\n\n$$\n\\Delta t=-\\frac{\\sqrt{\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}}}{c}\n$$\n\nBut then $\\varepsilon(\\Delta s)^{2}=0$ and the interval is lightlike. (Note that the (negative) time coordinate of $E_{2}$ is calculated, not measured.)\n\n\\section*{THE LORENTZ GROUP}\n12.3 Prove the following lemma involving the metric of SR, $g_{i j}$, as given by (12.6).\n\nLemma 12.4: If $C=\\left(c_{i j}\\right)$ is a symmetric $4 \\times 4$ matrix such that $c_{i j} x^{i} x^{j}=0$ for all $\\left(x^{i}\\right)$ such that $g_{i j} x^{i} x^{j}=0$, there exists a fixed real number $\\lambda$ for which $c_{i j}=\\lambda g_{i j}(C=\\lambda G)$.\n\nObserve that the vector $(1, \\pm 1,0,0)$ satisfies $g_{i j} x^{i} x^{j}=0$. Hence, substituting these components into the equation $c_{i j} x^{i} x^{j}=0$ yields\n\n$$\nc_{00} \\pm c_{01} \\pm c_{10}+c_{11}=0 \\quad \\text { or } \\quad c_{00}+c_{11}=0=c_{01}=c_{10}\n$$\n\n(by symmetry of $C$ ). Similarly, using the vectors $(1,0, \\pm 1,0)$ and $(1,0,0, \\pm 1)$, we get\n\n$$\nc_{00}=-c_{11}=-c_{22}=-c_{33}=\\lambda \\quad c_{i j}=0 \\quad(i=0 \\text { or } j=0)\n$$\n\nFinally, employing the vectors $(\\sqrt{2}, 1,1,0),(\\sqrt{2}, 1,0,1)$, and $(\\sqrt{2}, 0,1,1)$, we obtain $c_{12}=c_{13}=c_{23}=$ 0 .\n\n12.4 Establish the transformation (12.7) between inertial frames under the postulates for SR.\n\nFrom (12.6) and (12.5),\n\n$$\ng_{i j} x^{i} x^{j}=0=g_{i j} \\bar{x}^{i} \\bar{x}^{j}=g_{i j}\\left(a_{r}^{i} x^{r}\\right)\\left(a_{s}^{j} x^{s}\\right)=g_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}\n$$\n\nthat is,\n\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}=0 \\quad \\text { whenever } \\quad g_{i j} x^{i} x^{j}=0 \\tag{1}\n\\end{equation*}\n\n\nNow apply Lemma 12.4 to (1), with $g_{r s} a_{i}^{r} a_{j}^{s}=c_{i j}$, where $C=\\left(c_{i j}\\right)=A^{T} G A$ is symmetric. We obtain\n\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s}=\\lambda g_{i j} \\quad \\text { or } \\quad A^{T} G A=\\lambda G \\tag{2}\n\\end{equation*}\n\n\nIt remains to show that $\\lambda=1$. Since $G^{2}=I$, multiplication of (2) by the matrix $\\lambda^{-1} G$ gives $\\left(G\\left(\\lambda^{-1} A^{T}\\right) G\\right) A=I$, which shows that the inverse of $A$ is\n\n\\[\nB=\\frac{1}{\\lambda} G A^{T} G=\\left[\\begin{array}{rrrr}\na_{0}^{0} / \\lambda & -a_{0}^{1} / \\lambda & -a_{0}^{2} / \\lambda & -a_{0}^{3} / \\lambda  \\tag{3}\\\\\n-a_{1}^{0} / \\lambda & a_{1}^{1} / \\lambda & a_{1}^{2} / \\lambda & a_{1}^{3} / \\lambda \\\\\n-a_{2}^{0} / \\lambda & a_{2}^{1} / \\lambda & a_{2}^{2} / \\lambda & a_{2}^{3} / \\lambda \\\\\n-a_{3}^{0} / \\lambda & a_{3}^{1} / \\lambda & a_{3}^{2} / \\lambda & a_{3}^{3} / \\lambda\n\\end{array}\\right] \\equiv\\left[b_{j}^{i}\\right]_{44}\n\\]\n\nIn particular, $b_{0}^{0}=a_{0}^{0} / \\lambda$. Now since observers $O$ and $\\bar{O}$ are receding from each other at constant velocity $v$ and are using identical measuring devices, it is clear that each views the other in the sam. way. It follows that $a_{0}^{0}=b_{0}^{0}$ and $\\lambda=a_{0}^{0} / b_{0}^{0}=1$ (see Problem 12.5).\n\n12.5 With reference to Problem 12.4, give a \"thought-experiment\" which leads to the conclusion that $a_{0}^{0}=b_{0}^{0}$.\n\nConsider the motion of $O$ in $\\bar{O}$ 's frame: Transform the point $(c t, 0,0,0)$ under $\\mathscr{T}$ to get\n\n$$\n\\bar{x}^{0}=c \\bar{t}=a_{0}^{0} c t \\quad \\text { or } \\quad \\bar{t}=a_{0}^{0} t\n$$\n\nThus, 1 second on $O$ 's clock is $a_{0}^{0}$ seconds on $\\bar{O}$ 's; reciprocally, 1 second on $\\bar{O}$ 's clock is $b_{0}^{0}$ seconds on $O$ 's. Thus, $a_{0}^{0}=b_{0}^{0}$.\n\n12.6 Prove Theorem 12.1 from (12.7). [Note that Problem 12.4 did not make use of Theorem 12.1, so the proof will be logically correct.]\n\nBy (12.7), $\\left(g_{i j}\\right)$ is a covariant tensor under Lorentz transformations, so that $g_{i j} \\Delta x^{i} \\Delta x^{j}$ is an invariant (under Lorentz transformations).\n\n12.7 Verify that the following matrix is Lorentz:\n\n$$\n\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\frac{\\sqrt{6}}{2} & \\frac{1}{2} & \\frac{1}{2} \\\\\n1 & \\frac{\\sqrt{6}}{2} & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n\\end{array}\\right]\n$$\n\nWe verify directly the conditions $(12.7 c)$ :\n\n$$\n\\begin{array}{cc}\n(\\sqrt{3})^{2}-1^{2}-1^{2}-0^{2}=3-2=1 & (\\sqrt{2})^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-0^{2}=2-\\frac{3}{2}-\\frac{3}{2}=-1 \\\\\n0^{2}-\\left(\\frac{1}{2}\\right)^{2}-\\left(-\\frac{1}{2}\\right)^{2}-\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)^{2}=-1 & (\\sqrt{3})(\\sqrt{2})-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-0=0 \\\\\n(\\sqrt{3})(0)-(1)\\left(\\frac{1}{2}\\right)-(1)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)=0 & (\\sqrt{2})(0)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(\\frac{1}{2}\\right)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{1}{2}\\right)=0 \\\\\n0-\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(-\\frac{1}{2}\\right)+\\left(\\frac{\\sqrt{2}}{2}\\right)\\left(\\frac{\\sqrt{2}}{2}\\right)=0-\\frac{1}{4}-\\frac{1}{4}+\\frac{1}{2}=0\n\\end{array}\n$$\n\n12.8 Show that a matrix $A$ which preserves $\\mathbf{x}^{T} G \\mathbf{x}=0$ necessarily preserves $\\mathbf{x}^{T} G \\mathbf{x}=q$.\n\nThis is really Problem 12.6 in another guise. By Problem 12.4, $A$ must satisfy $A^{T} G A=G$. But then\n\n$$\n(A \\mathbf{x})^{T} G(A \\mathbf{x})=\\mathbf{x}^{T}\\left(A^{T} G A\\right) \\mathbf{x}=\\mathbf{x}^{T} G \\mathbf{x}=q\n$$\n\n12.9 (a) Exhibit the inverse, $B$, of a given Lorentz matrix, $A$. (b) If we define a matrix $A$ to be pseudo-orthogonal when there exists a matrix $J$ whose square is the identity and $A^{T} J A=J$, show that all Lorentz matrices are pseudo-orthogonal.\n\n(a) Set $\\lambda=1$ in (3) of Problem 12.4.\n\n(b) If $A$ is a Lorentz matrix, then $G$ clearly fills the role of $J$ in the definition of pseudo-orthogonal matrix.\n\n12.10 Prove that the Lorentz matrices compose a group under matrix multiplication.\n\nWe are required to show that (a) the product of two Lorentz matrices is Lorentz, $(b)$ the inverse of a Lorentz matrix is Lorentz.\n\n(a)\n\n$$\n(P Q)^{T} G(P Q)=Q^{T}\\left(P^{T} G P\\right) Q=Q^{T} G Q=G\n$$\n\n(b) Using Problem 12.4 with $\\lambda=1, B=A^{-1}=G A^{T} G$, and\n\n$$\nB^{T} G B=\\left(G A^{T} G\\right)^{T} G B=G A G^{2} B=G A B=G\n$$\n\n\\section*{SIMPLE LORENTZ MATRICES}\n12.11 Derive the simple form (12.9) of the transformation equations for SR by considering how observers $O$ and $\\bar{O}$ will view events occurring on a circular cylinder about their common $x$-axis.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-186}\n\\end{center}\n\nFig. 12-4\n\nAt any time $t$, let $E_{1}$ and $E_{2}$ be two events taking place at the points of space $(q, 1,0)$ and $(q, 0,1)$, respectively, which lie on a unit cylinder about $O$ 's $x$-axis (Fig. 12-4). Thus, with $p=c t$, we have space-time coordinates $E_{1}(p, q, 1,0)$ and $E_{2}(p, q, 0,1)$. Since the axes of $O$ are not turning with respect to $\\bar{O}$ 's, these two events will be viewed by observer $\\bar{O}$ as $E_{1}(\\bar{p}, \\bar{q}, 1,0)$ and $E_{2}\\left(\\bar{p}^{*}, \\bar{q}^{*}, 0,1\\right)$, respectively. The transformation equations (12.5) give:\n\n$$\n\\text { (I) }\\left\\{\\begin{array} { l } \n{ \\overline { p } = a _ { 0 } ^ { 0 } p + a _ { 1 } ^ { 0 } q + a _ { 2 } ^ { 0 } } \\\\\n{ \\overline { q } = a _ { 0 } ^ { 1 } p + a _ { 1 } ^ { 1 } q + a _ { 2 } ^ { 1 } } \\\\\n{ 1 = a _ { 0 } ^ { 2 } p + a _ { 1 } ^ { 2 } q + a _ { 2 } ^ { 2 } } \\\\\n{ 0 = a _ { 0 } ^ { 3 } p + a _ { 1 } ^ { 3 } q + a _ { 2 } ^ { 3 } }\n\\end{array} \\quad \\text { (II) } \\left\\{\\begin{array}{l}\n\\bar{p}^{*}=a_{0}^{0} p+a_{1}^{0} q+a_{3}^{0} \\\\\n\\bar{q}^{*}=a_{0}^{1} p+a_{1}^{1} q+a_{3}^{1} \\\\\n0=a_{0}^{2} p+a_{1}^{2} q+a_{3}^{2} \\\\\n1=a_{0}^{3} p+a_{2}^{3} q+a_{3}^{3}\n\\end{array}\\right.\\right.\n$$\n\nObserving just the last equation of (I) and the third equation of (II), we may, since $p$ and $q$ are arbitrary, take $p=q=0$, then $p=1, q=0$, and $p=0, q=1$. It follows that all six of the coefficients vanish: $a_{0}^{2}=a_{1}^{2}=a_{3}^{2}=a_{0}^{3}=a_{1}^{3}=a_{2}^{3}=0$. Using the third equation of (I) and the last equation of (II), we find that $a_{2}^{2}=a_{3}^{3}=1$. It follows that the last two equations of $\\mathscr{T}$ reduce to $\\bar{x}^{2}=x^{2}$ and $\\bar{x}^{3}=x^{3}$. Now to concentrate on the first two: If $p=q=0$, then event $E_{1}$ is $(0,0,1,0)$-occurring when $t=\\bar{t}=0$ at $x^{1}=0$, the instant when $\\bar{x}^{1}=0$. That is, $\\bar{p}=\\bar{q}=0$, with the result $a_{2}^{0}=a_{2}^{1}=0$. Similarly, using $E_{2}$, $p=q=0$ implies $\\bar{p}^{*}=\\bar{q}^{*}=0$ and $a_{3}^{0}=a_{3}^{1}=0$.\n\n12.12 Consider event $E_{1}$, a lightning flash at the point $(v, 0,0)$ at time $t=1 \\mathrm{~s}$ in $O$ 's frame, and event $E_{2}$, a lightning flash at $(-v, 0,0)$ at time $\\bar{t}=1 \\mathrm{~s}$ in $\\bar{O}$ 's frame. By determining the corresponding events in the opposing frames of reference, deduce (12.11).\n\nSince at $t=1$ observer $\\bar{O}$ has reached the point $(v, 0,0)$, the lightning strikes $\\bar{O}$ 's origin at time $\\bar{t}$. Hence, $E_{1}$ has coordinates $(c, v, 0,0)$ in $O$ and $(c \\bar{t}, 0,0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ we obtain\n\n$$\nc \\bar{t}=a c+b v \\quad 0=d c+e v\n$$\n\nThe second equation gives $d=-\\beta e$.\n\nSince $O$ has progressed backwards to the point $(-v, 0,0)$ in $\\bar{O}$ at the time $\\bar{t}=1$ at which $E_{2}$ occurs, this event has coordinates $(c t, 0,0,0)$ in $O$ and $(c,-v, 0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ yields\n\n$$\nc=a c t+b(0) \\quad-v=d c t+e(0)\n$$\n\nwhich upon division give $d=-\\beta a$. Hence, $a=e$.\n\n12.13 Show that a $4 \\times 4$ matrix is both Lorentz and orthogonal if and only if it has the form\n\n\\[\nR=\\left[\\begin{array}{rrrr} \n\\pm 1 & 0 & 0 & 0  \\tag{1}\\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\n\\]\n\nwhere the $3 \\times 3$ matrix $\\left[\\begin{array}{lll}\\mathbf{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal.\n\nA Lorentz matrix $A=\\left(a_{j}^{i}\\right)$ is also orthogonal if and only if its inverse $B$, as obtained in Problem 12.4 (with $\\lambda=1$ ), is equal to $A^{T}$ and is itself orthogonal. This observation immediately yields the form (1).\n\n\\subsection*{12.14 Prove Theorem 12.2.}\nSince $\\|\\mathbf{r}\\|^{2}=b^{-2}\\left[\\left(a_{1}^{0}\\right)^{2}+\\left(a_{2}^{0}\\right)^{2}+\\left(a_{3}^{0}\\right)^{2}\\right]=b^{-2}\\left[\\left(a_{0}^{0}\\right)^{2}-1\\right]=1$ (using Problem 12.36), the matrix $\\left[\\begin{array}{lll}\\mathrm{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal and $R_{2}^{T}$ has the form of the matrix in Problem 12.13, making it Lorentz and orthogonal. It follows that $R_{2}$ is orthogonal (and Lorentz), with $R_{2}^{-1}=R_{2}^{T}$; hence, $L=R_{1} L^{*} R_{2}$.\n\nNow, as the product of Lorentz matrices, $R_{1}$ is Lorentz; to show it is orthogonal, consider $L R_{2}^{T}\\left(L^{*}\\right)^{-1}$, which may be written as\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{llll}\na_{0} & b_{0} & c_{0} & d_{0} \\\\\na_{1} & b_{1} & c_{1} & d_{1} \\\\\na_{2} & b_{2} & c_{2} & d_{2} \\\\\na_{3} & b_{3} & c_{3} & d_{3}\n\\end{array}\\right]\\left[\\begin{array}{llll}\n1 & 0 & 0 & 0 \\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\na & -b & 0 & 0 \\\\\n-b & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-187}\n\\end{center}\n\n[The omitted rows have the form $\\left(a_{i}, b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}, b_{i} s_{1}+c_{i} s_{2}+d_{i} s_{3}, b_{i} t_{1}+c_{i} t_{2}+d_{i} t_{3}\\right.$ ), with $i=1,2,3$.] We first concentrate on proving that the top row and first column of this product are $( \\pm 1,0,0,0)$. The 00 -element of the product is\n\n$$\na_{0} a+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right)(-b)=\\varepsilon a_{0}^{2}+\\frac{\\varepsilon}{b}\\left(b_{0}^{2}+c_{0}^{2}+d_{0}^{2}\\right)(-b)=\\varepsilon\\left(a_{0}^{2}-b_{0}^{2}-c_{0}^{2}-d_{0}^{2}\\right)=\\varepsilon\n$$\n\nagain using the fact that the transpose of a Lorentz matrix is Lorentz. The next element in the top row of the product is\n\n$$\n-a_{0} b+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right) a=-a_{0} b+\\frac{b}{\\varepsilon}\\left(r^{2}\\right) \\varepsilon a_{0}=-a_{0} b+b a_{0}=0\n$$\n\nFor the third and fourth elements,\n\n$$\nb_{0} s_{1}+c_{0} s_{2}+d_{0} s_{3}=\\frac{b}{\\varepsilon} \\mathbf{r s}=0 \\quad \\text { and } \\quad b_{0} t_{1}+c_{0} t_{2}+d_{0} t_{3}=\\frac{b}{\\varepsilon} \\mathbf{r t}=0\n$$\n\nNow for the first column of the product; its elements, beginning with the second, are (for $i=1,2,3$ )\n\n$$\na_{i} a+\\left(b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}\\right)(-b)=\\varepsilon a_{i} a_{0}-\\varepsilon\\left(b_{i} b_{0}+c_{i} c_{0}+d_{i} d_{0}\\right)=0\n$$\n\nHence, the product matrix becomes\n\n$$\nR_{1}=\\left[\\begin{array}{llll}\n\\varepsilon & 0 & 0 & 0 \\\\\n0 & & & \\\\\n0 & & R & \\\\\n0 & & &\n\\end{array}\\right]\n$$\n\nand the $3 \\times 3$ matrix $R$ must be orthogonal, since $R_{1}$ is Lorentz.\n\n12.15 Apply Theorem 12.2 to the Lorentz matrix of Problem 12.7, and demonstrate the physical significance of this matrix by computing the velocity $v$ between the two observers involved.\n\nWe proceed to calculate $a, b$, and the vectors $\\mathbf{r}, \\mathbf{s}$, and $\\mathbf{t}$ :\n\n$$\na=\\sqrt{3} \\quad \\varepsilon=1 \\quad b=-\\sqrt{3-1}=-\\sqrt{2} \\quad \\mathbf{r}=-\\frac{1}{\\sqrt{2}}(\\sqrt{2}, 0,0)=(-1,0,0)\n$$\n\nHence, we may take $\\mathbf{s}=(0,1,0)$ and $\\mathbf{t}=(0,0,1)$, yielding\n\n$$\nR_{2}=\\left[\\begin{array}{rrrr}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand\n\n$$\n\\begin{aligned}\nR_{1} & =\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & \\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cccc}\n\\sqrt{3} & -\\sqrt{2} & 0 & 0 \\\\\n1 & -\\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & -\\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -\\sqrt{2} / 2 & 1 / 2 & 1 / 2 \\\\\n0 & -\\sqrt{2} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nBy Corollary 12.3,\n\n$$\nv=c \\sqrt{1-(\\sqrt{3})^{-2}}=\\sqrt{\\frac{2}{3}} c\n$$\n\n\\section*{LENGTH CONTRACTION, TIME DILATION}\n12.16 A pole-vaulter runs at the rate $(\\sqrt{3} / 2) c$ (in $\\mathrm{m} / \\mathrm{s}$ ) and carries a pole that is $20 \\mathrm{~m}$ long in his reference frame [the rest length of the pole is $20 \\mathrm{~m}$ ]. He approaches a barn that is open at both ends and is $10 \\mathrm{~m}$ long, as measured by a ground observer. To the ground observer, will the pole fit inside the barn? What is the pole-vaulter's conclusion?\n\nTo the ground observer, the pole undergoes length contraction with the factor $\\sqrt{1-\\beta^{2}}$, where $\\beta=\\sqrt{3} / 2$. Hence, the length of the pole in the frame of the ground observer is\n\n$$\n20 \\sqrt{1-(\\sqrt{3} / 2)^{2}}=10 \\mathrm{~m}\n$$\n\nand so, for her, the pole exactly fits inside the barn (instantaneously). To the runner, however, the barn is $10(1 / 2)=5 \\mathrm{~m}$ long, so that the $20-\\mathrm{m}$ pole does not fit.\n\nThis example shows that order relations are not preserved under the Lorentz transformation.\n\n12.17 (the Twin Paradox) One of a pair of twins embarks on a journey into outer space, taking one year (earth time) to accelerate to $(3 / 4) c$, then spends the next 20 years cruising to reach a galaxy 15 light-years away. An additional year is spend in decelerating in order to explore one of its solar systems. After one year of exploration $(\\beta=0)$, the twin returns to earth by the same schedule-one year of acceleration, 20 years of cruising, and one year of deceleration. Estimate the difference in the ages of the twins after the journey has ended.\n\nIn order to apply SR, replace the four periods of acceleration or deceleration by four periods of uniform motion at speed (3/8)c (the time-average speed under constant acceleration). These account for 4 years by the earth clock; but to the space twin, who measures proper (shortest) time intervals, the time lapse is $(\\beta=3 / 8)$\n\n$$\n4 \\sqrt{1-(3 / 8)^{2}} \\approx 3.71 \\text { years }\n$$\n\nSimilarly, the 40 earth-years of cruising at $\\beta=3 / 4$ corresponds to a proper-time interval of\n\n$$\n40 \\sqrt{1-(3 / 4)^{2}} \\approx 26.46 \\text { years }\n$$\n\nThus, the space twin has aged $3.71+26.46+1 \\approx 31$ years while the earth twin has aged $4+40+1=$ 45 years.\n\nThe space twin returns biologically younger by some 14 years. While the accelerations and decelerations between the two twins were reciprocal, the forces in the situation acted on the space twin alone.\n\n12.18 Prove the basic integrity of (12.16) by solving algebraically for $v_{2}$ as a function of $v_{1}$ and $v_{3}$ to verify that $v_{2}$ follows the correct format for composition of velocities.\n\nSolving,\n\n$$\nv_{2}=\\frac{-v_{1}+v_{3}}{1-v_{1} v_{3} / c^{2}}\n$$\n\nwhich is precisely (12.16) under the substitution $\\left(v_{1}, v_{2}, v_{3}\\right) \\rightarrow\\left(-v_{1}, v_{3}, v_{2}\\right)$.\n\n12.19 A light source at $O$ sends a spherical wavefront (Fig. 12-5(a)) advancing in all directions at velocity $c$; it reaches the ends of a diameter $A B$ centered at $O$ simultaneously, as determined by $O$. But as far as $\\bar{O}$ is concerned, the spherical wave, centered at $\\bar{O}$, moves with him (invariance of the light cone) and therefore reaches point $B$ before it reaches point $A$. Calculate the time difference on $\\bar{O}$ 's clock for these two events (light reaching $B$ and light reaching $A$ ) if $\\beta=1 / 2$ and if $A B=6 \\mathrm{~m}$.\n\nSince $A B=6 \\mathrm{~m}$ and $O$ is the midpoint of segment $A B, O$ assigns spatial coordinates $B(3,0,0)$ and $A(-3,0,0)$ to the endpoints. It takes $3 / c$ seconds for light to reach $A$ and $B$, so $O$ calculates the time coordinate as $x^{0}=c(3 / c)=3 \\mathrm{~m}$. The space-time coordinates of the two events are thus\n\n$$\nE_{B}(3,3,0,0) \\quad \\text { and } \\quad E_{A}(3,-3,0,0)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189(1)}\n\\end{center}\n\n(b)\n\nFig. 12-5\n\nSubstitute these values and $\\beta=1 / 2$ into the first equation (12.13) to obtain $\\bar{t}_{B}=\\sqrt{3} / c, \\bar{t}_{A}=3 \\sqrt{3} / c$. Hence, $\\Delta \\bar{t}=2 \\sqrt{3} / c$ (in s), while $\\Delta t=0$.\n\nIt is seen that simultaneity is not an invariant of Lorentz transformations.\n\n12.20 Derive the composition of velocities formula, (12.16).\n\nAccording to Section 12.4, we must have $v_{i}=-b_{i} c / a_{i}$ for $i=1,2,3$. Composing the simple Lorentz transformations, we have\n\n$$\n\\left[\\begin{array}{cccc}\na_{1} & b_{1} & 0 & 0 \\\\\nb_{1} & a_{1} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\na_{2} & b_{2} & 0 & 0 \\\\\nb_{2} & a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\na_{1} a_{2}+b_{1} b_{2} & a_{1} b_{2}+a_{2} b_{1} & 0 & 0 \\\\\na_{2} b_{1}+a_{1} b_{2} & b_{1} b_{2}+a_{1} a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhence $a_{3}=a_{1} a_{2}+b_{1} b_{2}, b_{3}=a_{1} b_{2}+a_{2} b_{1}$, and\n\n$$\nv_{3}=-\\frac{\\left(a_{1} b_{2}+a_{2} b_{1}\\right) c}{a_{1} a_{2}+b_{1} b_{2}}=\\frac{-\\frac{a_{1} b_{2} c}{a_{1} a_{2}}-\\frac{a_{2} b_{1} c}{a_{1} a_{2}}}{\\frac{a_{1} a_{2}}{a_{1} a_{2}}+\\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\\frac{-\\frac{b_{2} c}{a_{2}}-\\frac{b_{1} c}{a_{1}}}{1+\\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\\frac{v_{2}+v_{1}}{1+v_{1} v_{2} / c^{2}}\n$$\n\n12.21 A physicist wants to compose two equal velocities $v=v_{1}=v_{2}$ to produce a resultant velocity that is $90 \\%$ of the velocity of light. What velocity must he use?\n\nFrom (12.16),\n\n$$\n0.90 c=\\frac{2 v}{1+v^{2} / c^{2}} \\quad \\text { or } \\quad 0.90=\\frac{2 \\beta}{1+\\beta^{2}}\n$$\n\nSolving the quadratic, $\\beta \\approx 0.627$ (as compared to the Newtonian value 0.45 ).\n\n\\section*{VELOCITY AND ACCELERATION IN RELATIVITY}\n12.22 Establish (12.19) and (12.20), the Lorentz transformations of velocity and acceleration, that define how $\\bar{O}$ tracks the motion of a particle in $O$ 's frame.\n\nTo simplify notation, let $\\gamma \\equiv\\left(1-\\beta^{2}\\right)^{-1 / 2}$. Then $\\mathscr{T}$ is\n\n$$\nc \\bar{t}=\\gamma(c t-\\beta x) \\quad \\bar{x}=\\gamma(-\\beta c t+x) \\quad \\bar{y}=y \\quad \\bar{z}=z\n$$\n\nDifferentiate the first equation with respect to $\\bar{t}$ and use the chain rule:\n\n$$\nc=\\gamma\\left(c-\\beta v_{x}\\right) \\frac{d t}{d \\bar{t}} \\quad \\text { or } \\quad \\frac{d t}{d \\bar{t}}=\\frac{1}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}\n$$\n\nNow differentiate the last three equations:\n\n$$\n\\begin{aligned}\n& \\bar{v}_{x}=\\gamma\\left(-\\beta c+v_{x}\\right) \\frac{d t}{d \\bar{t}}=\\frac{\\gamma\\left(-v+v_{x}\\right)}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{y}=v_{y} \\frac{d t}{d \\bar{t}}=\\frac{v_{y}}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{y} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{z}=v_{z} \\frac{d t}{d \\bar{t}}=\\frac{v_{z} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}}\n\\end{aligned}\n$$\n\nBy differentiation of the velocity components just found,\n\n$$\n\\begin{aligned}\n\\bar{a}_{x} & =\\frac{d \\bar{v}_{x}}{d t} \\frac{d t}{d \\bar{t}}=\\frac{\\left(a_{x}-0\\right)\\left(1-v_{x} v / c^{2}\\right)-\\left(v_{x}-v\\right)\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1}{\\gamma\\left(1-v_{x} v / c^{2}\\right)} \\\\\n& =\\frac{a_{x}-a_{x} v_{x} v / c^{2}+v_{x} a_{x} v / c^{2}-a_{x} v^{2} / c^{2}}{\\gamma\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}\n\\end{aligned}\n$$\n\n$$\n\\bar{a}_{y}=\\frac{a_{y}\\left(1-v_{x} v / c^{2}\\right)-v_{y}\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1-\\beta^{2}}{1-v v_{x} / c^{2}}=\\frac{a_{y}+\\left(a_{x} v_{y}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)}{\\left(1-v_{x} / c^{2}\\right)^{3}}\\left(1-\\beta^{2}\\right)\n$$\n\nThe formula for $\\bar{a}_{z}$ is derived as that for $\\bar{a}_{y}$, with $z$ replacing $y$ throughout.\n\n12.23 Show that if the curve of motion in $O$ 's frame is the path of $\\bar{O}$ itself, the clock in $\\bar{O}$ 's frame (the clock moving with the particle) measures proper time.\n\nBy (3) of Problem 12.4,\n\n$$\n\\begin{aligned}\nx^{0} & =a_{0}^{0} \\bar{x}^{0}-a_{0}^{1} \\bar{x}^{1}-a_{0}^{2} \\bar{x}^{2}-a_{0}^{3} \\bar{x}^{3} \\\\\nx^{i} & =-a_{i}^{0} \\bar{x}^{0}+a_{i}^{1} \\bar{x}^{1}+a_{i}^{2} \\bar{x}^{2}+a_{i}^{3} \\bar{x}^{3} \\quad(i=1,2,3)\n\\end{aligned}\n$$\n\nNow the motion of $\\bar{O}$ relative to itself is obviously $\\bar{x}^{1}=\\bar{x}^{2}=\\bar{x}^{3}=0$. Hence,\n\n$$\nx^{0}=a_{0}^{0} c u \\quad x^{1}=-a_{1}^{0} c u \\quad x^{2}=-a_{2}^{0} c u \\quad x^{3}=-a_{3}^{0} c u\n$$\n\ngive the trajectory of $\\bar{O}$ in $O$ 's frame, with parameter $u=\\bar{t}$. Therefore, the tangent field to the trajectory is\n\n$$\n\\left(\\frac{d x^{i}}{d u}\\right)=\\left(a_{0}^{0} c,-a_{1}^{0} c,-a_{2}^{0} c,-a_{3}^{0} c\\right)\n$$\n\nso that the proper time parameter for this curve is defined as\n\n$$\n\\tau=\\frac{1}{c} \\int_{0}^{\\bar{t}} \\sqrt{\\left|\\left(a_{0}^{0} c\\right)^{2}-\\left(a_{1}^{0} c\\right)^{2}-\\left(a_{2}^{0} c\\right)^{2}-\\left(a_{3}^{0} c\\right)^{2}\\right|} d u=\\sqrt{\\left|\\left(a_{0}^{0}\\right)^{2}-\\left(a_{1}^{0}\\right)^{2}-\\left(a_{2}^{0}\\right)^{2}-\\left(a_{3}^{0}\\right)^{2}\\right|} \\int_{0}^{\\bar{t}} d u\n$$\n\nBecause the inverse transformation is Lorentz, the factor in front of the integral sign equals 1 , and so $\\tau=\\bar{t}$.\n\n12.24 Derive the identities (12.25).\n\nBy (12.21),\n\n$$\n\\begin{aligned}\nu_{i} u^{i} & =g_{i j} u^{i} u^{j} \\equiv\\left(u^{0}\\right)^{2}-\\left(u^{1}\\right)^{2}-\\left(u^{2}\\right)^{2}-\\left(u^{3}\\right)^{2} \\\\\n& =\\left[\\left(v_{t}\\right)^{2}-\\left(v_{x}\\right)^{2}-\\left(v_{y}\\right)^{2}-\\left(v_{z}\\right)^{2}\\right]\\left(\\frac{d t}{d \\tau}\\right)^{2}=\\left[c^{2}-\\hat{v}^{2}\\right] \\frac{1}{1-\\hat{v}^{2} / c^{2}}=c^{2}\n\\end{aligned}\n$$\n\nand from this,\n\n$$\n0=\\frac{d}{d \\tau}\\left(c^{2}\\right)=\\frac{d}{d \\tau}\\left(u_{i} u^{i}\\right)=2 u_{i} b^{i}\n$$\n\n12.25 Establish the formulas in (12.26).\n\n$$\n\\begin{aligned}\nu^{i} & =\\frac{d x^{i}}{d \\tau}=\\frac{d x^{i}}{d t} \\frac{d t}{d \\tau}=\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\\\\nb^{i} & =\\frac{d u^{i}}{d \\tau}=\\left[\\frac{d}{d t}\\left(\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\\right] \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{1 / 2}-v_{i}(1 / 2)\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}\\left(-2 a_{x} v_{x}-2 a_{y} v_{y}-2 a_{z} v_{z}\\right) / c^{2}}{1-\\hat{v}^{2} / c^{2}} \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)+v_{i}\\left(a_{x} v_{x}+a_{y} v_{y}+a_{z} v_{z}\\right) / c^{2}}{\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}=\\frac{a_{i}}{1-\\hat{v}^{2} / c^{2}}+\\frac{(\\mathbf{v a}) v_{i}}{c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}\n\\end{aligned}\n$$\n\n12.26 Derive the equation for uniformly accelerated motion along the $x$-axis of an inertial frame:\n\n$$\nx^{2}-c^{2} t^{2}=\\frac{c^{4}}{\\alpha^{2}}\n$$\n\nLet $\\bar{O}$ be an instantaneous rest frame at some point $t_{1}$, and let $O$ be a given (stationary) frame in which the motion curve is traced. Since the motion is along the $x$-axis of $O$,\n\n$$\nv_{y}=v_{z}=a_{y}=a_{z}=0 \\quad \\text { and } \\quad \\bar{v}_{y}=\\bar{v}_{z}=\\bar{a}_{y}=\\bar{a}_{z}=0\n$$\n\nwhereby $\\bar{a}_{x}=\\alpha=$ const. (assuming $\\bar{a}_{x}>0$ ). At $t=t_{1}, v=v_{x}$ (the constant velocity of $\\bar{O}$ is by definition equal to the instantaneous velocity of the particle); thus, from (12.20),\n\n\n\\begin{equation*}\n\\alpha=\\frac{a_{x}\\left(1-v^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3}}=\\frac{a_{x}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}} \\tag{1}\n\\end{equation*}\n\n\nSince $t_{1}$ is arbitrary, (1) must hold for all $t$. Writing $\\dot{x}, \\ddot{x}$ for the derivatives of $x(t)$, we have from (1):\n\n\n\\begin{equation*}\nc^{3} \\ddot{x}=\\alpha\\left(c^{2}-\\dot{x}^{2}\\right)^{3 / 2} \\tag{2}\n\\end{equation*}\n\n\nMake the substitution $y=\\dot{x}$ and (2) becomes\n\n\n\\begin{equation*}\nc^{3} \\frac{d y}{d t}=\\alpha\\left(c^{2}-y^{2}\\right)^{3 / 2} \\quad \\text { or } \\quad \\int \\frac{c^{3} d y}{\\left(c^{2}-y^{2}\\right)^{3 / 2}}=\\int \\alpha d t \\tag{3}\n\\end{equation*}\n\n\nStandard techniques of integration yield the first integral\n\n\n\\begin{equation*}\n\\frac{c y}{\\sqrt{c^{2}-y^{2}}}=\\alpha t \\tag{4}\n\\end{equation*}\n\n\n(where we have taken the initial velocity to be zero). Solving (4) for $y$ (assumed positive for positive $t$ ) and then integrating the equation $\\dot{x}=y(t)$, we obtain\n\n$$\nx=c \\sqrt{c^{2}+\\alpha^{2} t^{2}} / \\alpha \\quad \\text { or } \\quad x^{2}-c^{2} t^{2}=c^{4} / \\alpha^{2}\n$$\n\n(where we also take the initial position as zero). This is the desired equation, which represents a hyperbola in the $x t$ plane. By contrast, the Newtonian equation is the parabola $x=\\frac{1}{2} \\alpha t^{2}$.\n\n\\section*{RELATIVISTIC MASS, FORCE, AND ENERGY}\n12.27 Show that the observed mass of a particle with rest mass $m$, moving at velocity $v$, is $\\hat{m}=m\\left(1-v^{2} / c^{2}\\right)^{-1 / 2}$, by considering the following experiment. Let each observer $O$ and $\\bar{O}$ carry a ball with rest mass $m$ near his origin and so situated as to collide obliquely at $t=\\bar{t}=0$ (when their origins coincide). See Fig. 12-6. Suppose this collision imparts reciprocal velocities of $\\varepsilon$ in the positive $x$-direction and negative $\\bar{x}$-direction. Calculate the momentum of the system before and after collision (which is preserved), and what each observer sees based on the equations of SR; then take the limit as $\\varepsilon \\rightarrow 0$.\n\nThe velocity vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ of balls $B_{1}$ and $B_{2}$ before collision are, as seen by $O,(0,0,0)=\\mathbf{0}$ and $(v, 0,0)=v$ i. Observer $\\bar{O}$ calculates these vectors as $\\overline{\\mathbf{v}}_{1}=(-v, 0,0)$ and $\\overline{\\mathbf{v}}_{2}=(0,0,0)$ (either by\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192}\n\\end{center}\n\n(a) Before impact\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192(1)}\n\\end{center}\n\n(b) After impact\n\nFig. 12-6\\\\\nreciprocity or by use of (12.19). After collision, observer $O$ calculates the velocity of $B_{1}$ as $\\mathbf{v}_{1}=$ $(\\varepsilon, \\delta, 0)=\\varepsilon \\mathbf{i}+\\delta \\mathbf{j}$, assuming $B_{1}$ has the proper alignment with $B_{2}$. Reciprocally, observer $\\bar{O}$ calculates the velocity of $B_{2}$ as $\\overline{\\mathbf{v}}_{2}=(-\\varepsilon,-\\delta, 0)$. To find $\\mathbf{v}_{2}$, use the inverse of (12.19), with $\\bar{v}_{x}=-\\varepsilon$ and $\\bar{v}_{y}=-\\delta$ :\n\n$$\n\\mathbf{v}_{2}=v_{x} \\mathbf{i}+v_{y} \\mathbf{j}=\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n$$\n\nThus, observer $O$ calculates the net momentum vector of the system as follows, using the rest mass $m$ of $B_{1}$ for $m_{1}$ and the \"perceived mass\" $\\hat{m}$ of $B_{2}$ for $m_{2}$ :\n\n$$\n\\text { before impact } \\quad \\begin{aligned}\nm_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m_{1}(\\mathbf{0})+m_{2}(v \\mathbf{i})=\\hat{m} v \\mathbf{i} \\\\\n\\text { after impact } \\quad m_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m(\\varepsilon \\mathbf{i}+\\delta \\mathbf{j})+\\hat{m}\\left[\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\\right] \\\\\n& =\\left(m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(m \\delta-\\hat{m} \\frac{\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n\\end{aligned}\n$$\n\nSince $O$ is using the universal laws of physics as they apply to his frame (Postulate 1 of SR), the two momentum vectors above must be the same. Hence,\n\n$$\n\\hat{m} v=m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}} \\quad \\text { and } \\quad 0=m-\\hat{m} \\frac{\\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\n$$\n\n(after division by $\\delta$ ). Now take the limit as $\\varepsilon \\rightarrow 0$ :\n\n$$\n\\hat{m} v=\\hat{m} v \\quad \\text { and } \\quad 0=m-\\hat{m} \\sqrt{1-\\beta^{2}}\n$$\n\nThe right-hand equation is the connection between $m$ and $\\hat{m}$.\n\n12.28 Show that the Minkowski force is a 4-vector.\n\nWe must show that $\\bar{K}^{i}=a_{j}^{i} K^{j}$, if $\\bar{x}^{i}=a_{j}^{i} x^{j}$, where $\\left(a_{j}^{i}\\right)$ is any Lorentz matrix. Since $\\tau$ is invariant and $a_{i}^{i}=$ const. we may differentiate the coordinate transformation with respect to $\\tau$ across the equal sign:\n\n$$\n\\frac{d}{d \\tau}\\left(\\bar{x}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} x^{j}\\right) \\quad \\text { or } \\quad \\bar{u}^{i}=a_{j}^{i} u^{j}\n$$\n\n(proving that $\\left(u^{i}\\right)$ is a 4-vector). Multiply both sides by $m$ and differentiate again, using the fact that the rest mass of a particle is invariant:\n\n$$\n\\bar{K}^{i}=\\frac{d}{d \\tau}\\left(\\bar{m} \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(m \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} m u^{j}\\right)=a_{j}^{i} \\frac{d}{d \\tau}\\left(m u^{j}\\right)=a_{j}^{i} K^{j}\n$$\n\n\\subsection*{12.29 Establish (12.32).}\nDefinition (12.30), $K^{i}=d\\left(m u^{i}\\right) / d \\tau=m b^{i}$, along with the second identity (12.25), gives at once $u_{i} K^{i}=0$.\n\nFrom $u_{i} K^{i}=g_{i j} u^{i} K^{j}=u^{0} K^{0}-u^{q} K^{q}=0$ and the first formula (12.26), we have\n\n$$\n\\frac{v_{0} K^{0}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}-\\frac{v_{q} K^{q}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}=0 \\quad \\text { or } \\quad c K^{0}=\\mathbf{v K}\n$$\n\nBy (12.31) and $c K^{0}=\\mathbf{v K}$,\n\n$$\n\\frac{1}{c} \\mathbf{v F}=\\frac{1}{c} \\mathbf{v K} \\sqrt{1-\\hat{v}^{2} / c^{2}}=K^{0} \\sqrt{1-\\hat{v}^{2} / c^{2}}=F_{0}\n$$\n\nUsing the first definition (12.29),\n\n$$\n\\mathbf{v F}=c F_{0}=\\frac{d}{d t}\\left(\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\n$$\n\n12.30 Show that as $\\hat{v} \\rightarrow 0, \\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+O\\left(\\hat{v}^{4} / c^{2}\\right)$. Interpret this result.\n\nThe expression for relativistic energy, $\\hat{E}=m c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}$ may be expanded by the binomial theorem:\n\n$$\n(1+x)^{\\alpha}=1+\\alpha x+\\frac{\\alpha(\\alpha-1)}{2 !} x^{2}+\\cdots \\quad(-1<x<1)\n$$\n\nThe result is\n\n$$\n\\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+\\frac{3 m \\hat{v}^{4}}{8 c^{2}}+\\cdots\n$$\n\nThus, at low speeds, the total energy of particle is very nearly the sum of its rest energy (which includes all sorts of potential energy) and its classical kinetic energy.\n\n\\section*{MAXWELL'S EQUATIONS IN SR}\n12.31 Prove that $\\bar{\\square} \\bar{f}=\\square f$.\n\nAs the $g_{i j}$ are constants, $\\square f \\equiv g^{i j} f_{, i j}=$ invariant.\n\n12.32 Prove that if $\\left(F^{i j}\\right)$ is any matrix of functions of the 3-vectors $\\mathbf{U}$ and $\\mathbf{V}$ such that $\\partial F^{i j} / \\partial x^{j}=$ $0 \\quad(i=0,1,2,3)$ for all inertial frames and $F^{i j}(\\mathbf{0}, \\mathbf{0})=0$ for all $i, j$, where $\\mathbf{0}=(0,0,0)$, then $\\left(F^{i j}\\right)$ is a second-order contravariant tensor under Lorentz transformations.\n\nLet $\\left(u_{i}\\right)$ be any constant, covariant vector under Lorentz transformations [hence, $\\left(\\bar{u}_{i}\\right)=\\left(b_{i}^{k} u_{k}\\right)$ is also constant]. Define\n\n$$\nS^{i} \\equiv u_{k} F^{k i} \\quad \\bar{S}^{i} \\equiv \\bar{u}_{k} \\bar{F}^{k i}\n$$\n\nBy the given conditions $\\partial \\bar{F}^{i j} / \\partial \\bar{x}^{j}=0$,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=\\bar{u}_{k} \\frac{\\partial \\bar{F}^{k i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial S^{i}}{\\partial x^{i}}\n$$\n\nSuppose that at some point $\\left(x_{0}^{i}\\right), \\bar{S}^{i}=h^{i}\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$; then,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial h^{i}}{\\partial S^{j}} \\frac{\\partial S^{j}}{\\partial x^{k}} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad\\left(b_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}\\right) \\frac{\\partial S^{j}}{\\partial x^{k}}=0\n$$\n\nfor an arbitrary matrix $\\left(\\partial S^{j} / \\partial x^{k}\\right)$ having $\\partial S^{i} / \\partial x^{i}=0$. By a well-known lemma (Problem 12.57), there exists a real number $\\lambda=\\lambda\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$ such that\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda \\delta_{j}^{k} \\tag{1}\n\\end{equation*}\n\n\nNow differentiate both sides of (1) with respect to $S^{l}$ :\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial^{2} h^{i}}{\\partial S^{j} \\partial S^{l}}=\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k} \\tag{2}\n\\end{equation*}\n\n\nwhich is symmetrical in $j$ and $l$; therefore,\n\n\n\\begin{equation*}\n\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k}=\\frac{\\partial \\lambda}{\\partial S^{j}} \\delta_{l}^{k} \\tag{3}\n\\end{equation*}\n\n\nfor all $j, k, l$. Let $k=l \\neq j$ in (3):\n\n$$\n\\frac{\\partial \\lambda}{\\partial S^{k}} \\cdot 0=\\frac{\\partial \\lambda}{\\partial S^{j}} \\cdot 1 \\quad \\text { or } \\quad \\frac{\\partial \\lambda}{\\partial S^{j}}=0\n$$\n\nHence $\\lambda$ is constant with respect to the $S^{i}$ and (1) inverts to give\n\n\n\\begin{equation*}\n\\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda a_{j}^{i} \\tag{4}\n\\end{equation*}\n\n\nIntegrating (4),\n\n\n\\begin{equation*}\nh^{i} \\equiv \\bar{S}^{i}=\\lambda a_{j}^{i} S^{j}+T^{i} \\tag{5}\n\\end{equation*}\n\n\nFor the special assignment $\\mathbf{U}=\\mathbf{V}=\\mathbf{0}$, we have (since $\\overline{\\mathbf{0}}=\\mathbf{0}$ ):\n\n\n\\begin{align*}\n& S^{i}=\\left(u_{1}\\right)(0)+\\left(u_{2}\\right)(0)+\\left(u_{3}\\right)(0)+\\left(u_{4}\\right)(0)=0  \\tag{6}\\\\\n& \\bar{S}^{i}=\\left(\\bar{u}_{1}\\right)(0)+\\left(\\bar{u}_{2}\\right)(0)+\\left(\\bar{u}_{3}\\right)(0)+\\left(\\bar{u}_{4}\\right)(0)=0\n\\end{align*}\n\n\nTogether, (5) and (6) imply $T^{i}=0 \\quad(i=0,1,2,3)$; consequently,\n\n\n\\begin{equation*}\n\\bar{S}^{i}=\\lambda a_{j}^{i} S^{j} \\tag{7}\n\\end{equation*}\n\n\nSimilarly, there exists a real number $\\mu$ such that\n\n\n\\begin{equation*}\nS^{i}=\\mu b_{j}^{i} \\bar{S}^{j} \\tag{8}\n\\end{equation*}\n\n\nIt follows that $\\bar{S}^{i}=\\lambda a_{i}^{i} \\mu b_{k}^{j} \\bar{S}^{k}=\\lambda \\mu \\bar{S}^{i}$, or $\\lambda \\mu=1$. But we can exploit the reciprocal relationship between observers $O$ and $\\bar{O}$, as in Problem 12.4; to show that $\\lambda=\\mu$. Therefore, $\\lambda=\\mu=1$ and (7) or (8) becomes the transformation law of a (contravariant) 4-vector. Finally, we conclude from the Quotient Theorem that if $F^{k i} u_{k} \\equiv S^{i}$ is a tensor for an arbitrary covariant vector $\\left(u_{i}\\right),\\left(F^{i j}\\right)$ is a second-order contravariant tensor.\n\n12.33 Prove the relations (12.40) and (12.41).\n\nBy (12.39) and the constancy of the $g_{i j}$,\n\n$$\n\\frac{\\partial f^{0 j}}{\\partial x^{j}}=\\frac{\\partial f^{00}}{\\partial x^{0}}+\\frac{\\partial f^{0 q}}{\\partial x^{q}}=\\frac{\\partial}{\\partial x^{q}}\\left(-V^{q}\\right)=-\\frac{\\partial V^{q}}{\\partial x^{q}}=-\\operatorname{div} \\mathbf{V}\n$$\n\nand, for $p=1,2,3$,\n\n$$\n\\frac{\\partial f^{p j}}{\\partial x^{j}}=\\frac{\\partial f^{p 0}}{\\partial x^{0}}+\\frac{\\partial f^{p q}}{\\partial x^{q}}=-\\frac{\\partial f^{0 p}}{\\partial x^{0}}+\\frac{\\partial}{\\partial x^{q}}\\left(\\varepsilon_{p q r} U^{r}\\right)=\\frac{\\partial V^{p}}{\\partial x^{0}}-\\varepsilon_{p r q} \\frac{\\partial U^{r}}{\\partial x^{q}}=\\left(\\frac{1}{c} \\frac{\\partial \\mathbf{V}}{\\partial t}+\\operatorname{curl} \\mathbf{U}\\right)_{p}\n$$\n\nThe other two formulas are derived from these by replacing $\\mathbf{U}, \\mathbf{V}$ by $\\mathbf{V},-\\mathbf{U}$.\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n12.34 Suppose two events consist of light signals, and an observer sends one of the signals himself. Classify the space-time interval between the events if the observer sees the distant light signal $(a)$ before he sends his own signal, (b) after he sends his own signal, (c) at the same time he sends his own signal.\n\n12.35 Assuming that any velocity less than $c$ is attainable, suppose that a concert in Los Angeles begins at 8:0508 p.m. and one in New York City, 3000 miles away (consider this the accurate distance), begins at 8:0506 p.m. could a person physically attend both events (opening measures only)? Is the pair of events timelike or spacelike?\n\n12.36 Show that the transpose of a Lorentz matrix is Lorentz.\n\n12.37 Verify the expressions (12.12).\n\n12.38 An event occurs at $\\bar{O}$ 's origin at some time $\\bar{t}$. (a) How does $O$ view this event? (b) What is the significance of $a_{0}^{0}>0$ ?\n\n12.39 Write out the simple Lorentz transformation connecting inertial frames $O$ and $\\bar{O}$ that move apart at $80 \\%$ of the velocity of light.\n\n12.40 (a) Confirm that a photon (a particle with the velocity of light in some inertial frame) will be viewed as having the velocity of light in all other inertial frames. (b) What must be the rest mass of such a particle?\n\n12.41 Show that the following matrix is Lorentz, and use Theorem 12.2 to find the matrices $L^{*}, R_{1}$, and $R_{2}$, and the velocity $v$ between the two observers.\n\n$$\nL=\\left[\\begin{array}{crcr}\n5 / 4 & 1 / 2 & 1 / 4 & -1 / 2 \\\\\n-3 / 4 & -5 / 6 & -5 / 12 & 5 / 6 \\\\\n0 & 2 / 3 & 2 / 15 & 11 / 15 \\\\\n0 & -1 / 3 & 14 / 15 & 2 / 15\n\\end{array}\\right]\n$$\n\n12.42 Verify that the following matrix is Lorentz and calculate the velocity between the two observers without finding the simple Lorentz matrix $L^{*}$.\n\n$$\nL=\\left[\\begin{array}{cccc}\n3 / \\sqrt{3} & 1 / \\sqrt{3} & 2 / \\sqrt{3} & -1 / \\sqrt{3} \\\\\n1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & -1 \\\\\n0 & 1 / \\sqrt{3} & -1 / \\sqrt{3} & -1 / \\sqrt{3}\n\\end{array}\\right]\n$$\n\n12.43 Show that by definition the proper-time parameter $\\tau$ is an invariant with respect to all Lorentz transformations\n\n12.44 Verify the formula for the composition of velocities by (i) multiplying the two simple Lorentz matrices below; (ii) calculating from (12.15) the velocities belonging to the two matrices and to their product; (iii) showing that the three velocities obey (12.16).\n\n$$\nL_{1}=\\left[\\begin{array}{cccc}\n13 / 12 & 5 / 12 & 0 & 0 \\\\\n5 / 12 & 13 / 12 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\quad L_{2}=\\left[\\begin{array}{cccc}\n17 / 8 & -15 / 8 & 0 & 0 \\\\\n-15 / 8 & 17 / 8 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\n12.45 An electron gun shoots particles in opposite directions at one-half the velocity of light. At what relative velocity are the particles receding from each other?\n\n12.46 Show that the composition of two velocities less than $c$ is also less than $c$.\n\n12.47 How slow would your watch run relative to a stationary clock if you were moving at $2 / 3$ the velocity of light?\n\n12.48 At the age of 20 , an astronaut left her twin brother on earth to go exploring in outer space. The first two years the spaceship gradually accelerated to a cruising speed 95 percent of the velocity of light. Traveling at that speed for 25 years, it reached a distant galaxy (23.75 light years away) and then decelerated for two years. Two years were spent exploring the galaxy before the journey back home, which followed the schedule of the trip outward. How old is the astronaut when she rejoins her 80-year-old brother? (Use an average rate for clock-retardation during the 8 years in acceleration/deceleration.)\n\n12.49 How fast would a pole-vaulter have to run for his 20 -foot pole to fit (instantaneously) inside a barn, in the judgment of a ground observer for whom the barn is 19 feet 11 inches long?\n\n12.50 An alternate definition of uniformly accelerated motion is motion under a constant Lorentz force. Verify that the two definitions are equivalent for one-dimensional motion.\n\n12.51 Show that $g^{r s} a_{r}^{i} a_{s}^{j}=g^{i j}$.\n\n12.52 Prove that the array (12.46) is a 4-vector.\n\n12.53 Show that the matrices $\\tilde{\\mathscr{F}}$ and $\\mathscr{F}$ of (12.44) are connected via $\\tilde{F}^{i j}=\\frac{1}{2} e_{i j k l} g_{k r} g_{l s} F^{r s}$. [Hint: First evaluate the matrix product $G F G \\equiv P$.]\n\n12.54 Define Faraday's two-form by\n\n$$\n\\Phi \\equiv G \\tilde{\\mathscr{F}} G=\\left[\\begin{array}{cccc}\n0 & -E_{1} & -E_{2} & -E_{3} \\\\\nE_{1} & 0 & H_{3} & -H_{2} \\\\\nE_{2} & -H_{3} & 0 & H_{1} \\\\\nE_{3} & H_{2} & -H_{1} & 0\n\\end{array}\\right]=\\left[\\Phi_{i j}\\right]_{44}\n$$\n\nor, inversely, $\\tilde{\\mathscr{F}}=G \\Phi G$. Show that $(a) \\mathscr{F}$ is related to $\\Phi$ through $F^{i j}=-\\frac{1}{2} e_{i j k l} \\Phi_{k l} ;(b)$ Maxwell's equations can be written in terms of the single matrix $\\Phi$ as\n\n$$\n\\frac{\\partial \\Phi_{i j}}{\\partial x^{k}}+\\frac{\\partial \\Phi_{k i}}{\\partial x^{j}}+\\frac{\\partial \\Phi_{j k}}{\\partial x^{i}}=0 \\quad g_{i k} g_{i l} \\frac{\\partial \\Phi_{k l}}{\\partial x^{j}}=s^{i}\n$$\n\n12.55 The energy flux in an electromagnetic field is specified by the Poynting vector, $\\mathbf{p}=\\mathbf{E} \\times \\mathbf{H}$. By direct matrix multiplication or otherwise, derive the formula\n\n$$\n\\frac{1}{2}(\\tilde{\\mathscr{F}} \\mathscr{F}-\\tilde{F} \\tilde{\\mathscr{F}})=\\left[\\begin{array}{cccc}\n0 & 0 & 0 & 0 \\\\\n* & 0 & p_{3} & -p_{2} \\\\\n* & * & 0 & p_{1} \\\\\n* & * & * & 0\n\\end{array}\\right] \\text { (antisymmetric matrix) }\n$$\n\n12.56 Verify that for simple Lorentz matrices $A$ (hence, no rotation of axes allowed): (a) $\\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V})=$ $G \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) G ;(b) \\overline{\\mathscr{F}}(\\overline{\\mathbf{V}}, \\overline{\\mathbf{U}})=B^{T} \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) B$, where $B=A^{-1} ;$ and $(c) \\tilde{\\mathscr{F}}(\\overline{\\mathbf{U}}, \\overline{\\mathbf{V}})=A \\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V}) A^{T}$ (thereby proving that $\\left(F^{i j}\\right)$ is a contravariant tensor under simple Lorentz transformations).\n\n12.57 Prove that if $A \\equiv\\left[A_{i j}\\right]_{n n}$ satisfies $A_{i j} B_{i j}=0$ for every $B \\equiv\\left[B_{i j}\\right]_{n n}$ that has zero trace $\\left(B_{i i}=0\\right)$, then $A=\\lambda I$, for some real $\\lambda$. [Hint: First take $B$ as having all elements zero, except for one off-diagonal element. Then choose $B_{\\alpha \\alpha}=-B_{\\beta \\beta}=1 \\quad(\\alpha \\neq \\beta ;$ no summation $)$, with all other $B_{i j}$ zero.]\n\n", "answers_to_supplementary_problems": ""}, {"all": ["\\section*{Chapter 13}", "\\section*{Tensor Fields on Manifolds}\n\\subsection*{13.1 INTRODUCTION}\nThe modern, noncoordinate approach to tensors will be introduced as an important alternative to the coordinate-component approach employed exclusively in the previous chapters. This will entail somewhat more sophisticated mathematics.\n\n\\subsection*{13.2 ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}\nLinear algebra provides a means of systematically studying the algebraic interplay between real numbers (scalars) and a wide variety of different types of objects (vectors). Vectors can be matrices, $n$-tuples of real numbers, functions, differential operators, etc. In this chapter, we shall adopt the convention of using uppercase boldface characters for sets (of points, of real numbers, of elements of a group, etc.), and lowercase boldface for vectors (as in the preceding chapters). However, the latter will be gradually phased out in favor of light uppercase characters, not only for easier reading, but also in conformity with notation used in many standard textbooks.\n\nThe concept of vector spaces requires a careful distinction between the scalars $a, b, c, \\ldots$, and the objects of study (vectors), $\\mathbf{u}, \\mathbf{v}, \\mathbf{w}, \\ldots$ We shall always identify the scalars with the field of real numbers, although any field could serve for the construction of an abstract vector space.\n\n\\section*{Algebraic Properties of a Vector Space}\nIn terms of two binary operations, the axioms for a vector space are as follows.\n\n\\section*{Addition Axioms}\n\\begin{enumerate}\n  \\item $\\mathbf{u}+\\mathbf{v}$ is always a vector\n\n  \\item $\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}$\n\n  \\item $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n\n  \\item There is a vector $\\mathbf{0}$ such that $\\mathbf{u}+\\mathbf{0}=\\mathbf{u}$.\n\n  \\item For each $\\mathbf{u}$ there is a vector $-\\mathbf{u}$ such that $\\mathbf{u}+(-\\mathbf{u})=\\mathbf{0}$.\n\n\\end{enumerate}\n\n\\section*{Scalar Multiplication Axioms}\n\\begin{enumerate}\n  \\setcounter{enumi}{5}\n  \\item $a \\cdot \\mathbf{u} \\equiv a \\mathbf{u}$ is always a vector\n  \\item $a(\\mathbf{u}+\\mathbf{v})=a \\mathbf{u}+a \\mathbf{v}$\n  \\item $(a+b) \\mathbf{u}=a \\mathbf{u}+b \\mathbf{u}$\n  \\item $(a b) \\mathbf{u}=a(b \\mathbf{u})$\n  \\item $1 \\mathbf{u}=\\mathbf{u}$\n\\end{enumerate}\n\nEXAMPLE 13.1 We give notation for four familiar vector spaces.\n\n(a) $\\mathbf{R}^{n} \\equiv$ the $n$-tuples of reals under componentwise addition and scalar multiplication.\n\n(b) $\\quad \\mathbf{P}^{n} \\equiv$ the polynomials (in a variable $t$ ) of degree $n$ or less. If $p(t) \\equiv a_{i} t^{i}, q(t) \\equiv b_{i} t^{i}$, let $p(t)+q(t)=$ $\\left(a_{i}+b_{i}\\right) t^{i}$ and $r \\cdot p(t)=\\left(r a_{i}\\right) t^{i}$.\n\n(c) $\\quad C^{k}(\\mathbf{R}) \\equiv$ the continuously $k$-times differentiable functions (of $t$ ), $f \\vdots \\mathbf{R} \\rightarrow \\mathbf{R}$ (mapping the reals into the reals). To define + and $\\cdot$, write $f(t)+g(t)=(f+g)(t)$ and $r \\cdot f(t)=(r f)(t)$.\n\n(d) $\\mathbf{M}^{n}(\\mathbf{R}) \\equiv$ the $n \\times n$ matrices over $\\mathbf{R}$. If $A=\\left(a_{i j}\\right)$ and $B=\\left(b_{i j}\\right)$, addition and scalar multiplication are defined by $A+B=\\left(a_{i j}+b_{i j}\\right)$ and $r A=\\left(r a_{i j}\\right)$.\n\n\\section*{Algebraic Properties of a Group}\nAxioms 1-5 make a vector space an abelian (commutative) group under addition. In the general definition of a group, the binary operation is designated as \"multiplication\" and the commutative requirement is dropped.\n\n\\section*{Multiplication Axioms}\n\\begin{enumerate}\n  \\item $u v$ belongs to the group.\n\n  \\item $(u v) w=u(v w)$.\n\n  \\item There is an identity element $e$ such that $e u=u e=u$.\n\n  \\item For each $u$ there is an inverse element $u^{-1}$ such that $u u^{-1}=u^{-1} u=e$.\n\n\\end{enumerate}\n\nEXAMPLE 13.2 Some frequently encountered groups follow.\n\n(a) The reals $\\mathbf{R}$ over ordinary addition; the reals over ordinary multiplication if 0 is removed from the set.\n\n(b) The cube roots of unity, $\\mathbf{C}^{3}=\\left\\{1, \\omega, \\omega^{2}\\right\\}$, over ordinary multiplication of complex numbers, where $\\omega=\\frac{1}{2}\\left(-1+i \\sqrt{3}\\right.$ ). Groups of this type are called cyclic and are generally denoted by $\\mathbf{C}^{k}$ (the cyclic group of order $k$ ). $\\mathbf{C}^{k}$ is necessarily abelian.\n\n(c) The 4-group $\\{e, u, s, b\\}$, under the rules $u^{2}=s^{2}=b^{2}=e, b=u s$, and the associative law of multiplication. The 4-group is abelian, but it is not equivalent to the cyclic group on four elements, $\\mathbf{C}^{4}$.\n\n(d) $\\mathbf{M}^{n}(\\mathbf{R})$, under matrix addition.\n\n(e) $\\mathbf{G L}(n, \\mathbf{R}) \\equiv$ the real, nonsingular $n \\times n$ matrices under matrix multiplication; this is the general linear group (nonabelian). GL $(n, \\mathbf{R})$ contains many very important smaller groups (called subgroups). Some of these are: $\\mathbf{S L}(n, \\mathbf{R}) \\equiv$ the real $n \\times n$ matrices with determinant $+1 ; \\mathbf{S O}(n) \\equiv$ the $n \\times n$ orthogonal matrices; and $\\mathbf{L}(n) \\equiv$ the $n \\times n$ Lorentz matrices [see the definition of $\\mathbf{L}(4)$ in Section 12.3].\n\n(f) $\\mathbf{G L}(n, \\mathbf{C}) \\equiv$ the complex, nonsingular $n \\times n$ matrices under matrix multiplication. An important subgroup is the unitary group, $\\mathbf{U}(n)$, consisting of all $n \\times n$ Hermitian matrices (such that $A^{-1}=\\bar{A}^{T}$, where the bar denotes complex conjugation).\n\n\\subsection*{13.3 IMPORTANT CONCEPTS FOR VECTOR SPACES}\nBasis\n\nA basis for a vector space is a maximal, linearly independent set of vectors $\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots$ If this set is finite, possessing $n$ elements, the vector space is finite-dimensional, of dimension $n$. Otherwise, the space is said to be infinite-dimensional.\n\nEXAMPLE 13.3 (1) It is obvious that a basis for $\\mathbf{R}^{n}$ is the set of vectors\n\n$$\n\\mathbf{e}_{1}=(1,0,0, \\ldots, 0), \\quad \\mathbf{e}_{2}=(0,1,0, \\ldots, 0), \\ldots, \\quad \\mathbf{e}_{n}=(0,0, \\ldots, 0,1)\n$$\n\ncalled the standard basis. (2) $\\mathbf{P}^{n}$ is finite-dimensional, of dimension $n+1$; one basis is $\\left\\{t^{i}\\right\\}, 0 \\leqq i \\leqq n$. (3) The vector space of all polynomials is infinite-dimensional, as is the vector space $C^{k}(\\mathbf{R})$. See Problem 13.4.\n\n\\section*{Isomorphisms, Linear Mappings}\nTwo mathematical systems of the same type (such as two vector spaces or two groups) are called isomorphic if they are structurally identical and differ only in nomenclature. In the case of two vector spaces, an isomorphism is a one-to-one (bijective) linear mapping $\\varphi$ from one space to the other, where the term linear refers to the properties (for all vectors $\\mathbf{u}, \\mathbf{v}$, and scalars $a$ ):\n\n\n\\begin{equation*}\n\\varphi(\\mathbf{u}+\\mathbf{v})=\\varphi(\\mathbf{u})+\\varphi(\\mathbf{v}) \\quad \\text { and } \\quad \\varphi(a \\mathbf{u})=a \\varphi(\\mathbf{u}) \\tag{13.1}\n\\end{equation*}\n\n\nFor groups, an isomorphism would be a bijection $\\psi$ with the property $\\psi(u v)=\\psi(u) \\psi(v)$, for all elements of the group. A more general mapping that is important for groups is a homomorphism, which merely requires that $\\psi(u v)=\\psi(u) \\psi(v)$ for all $u$ and $v$, without necessarily requiring one-tooneness.\n\n\\section*{Product of Vector Spaces}\nIf $\\mathbf{U}$ and $\\mathbf{V}$ are any two vector spaces, the ordinary cartesian product $\\mathbf{U} \\times \\mathbf{V}$, the set of ordered pairs $(\\mathbf{u}, \\mathbf{v})$ with $\\mathbf{u}$ in $\\mathbf{U}$ and $\\mathbf{v}$ in $\\mathbf{V}$, can be made into a vector space by defining addition and scalar multiplication of pairs via\n\n$$\n(\\mathbf{p}, \\mathbf{q})+(\\mathbf{r}, \\mathbf{s})=(\\mathbf{p}+\\mathbf{r}, \\mathbf{q}+\\mathbf{s}) \\quad \\text { and } \\quad a(\\mathbf{p}, \\mathbf{q})=(a \\mathbf{p}, a \\mathbf{q})\n$$\n\nSuch a product space is denoted $\\mathbf{U} \\otimes \\mathbf{V}$; if $\\mathbf{U}=\\mathbf{V}$, write $\\mathbf{U} \\otimes \\mathbf{V}$ as $\\mathbf{U}^{2}$. More generally, the product of any number of vector spaces $\\mathbf{V}_{1}, \\mathbf{V}_{2}, \\ldots, \\mathbf{V}_{k}$ may be easily defined as above; this product is denoted $\\mathbf{V}_{1} \\otimes \\mathbf{V}_{2} \\otimes \\mathbf{V}_{3} \\otimes \\cdots \\otimes \\mathbf{V}_{k}$. If $\\mathbf{V}_{1}=\\mathbf{V}_{2}=\\cdots=\\mathbf{V}_{k}=\\mathbf{V}$, the product space is written $\\mathbf{V}^{k}$. (This notation is also often used for the tensor product of two vector spaces, a concept which will not be treated here.)\n\n\\subsection*{13.4 THE ALGEBRAIC DUAL OF A VECTOR SPACE}\nIf a vector space $\\mathbf{V}$ be mapped linearly into the reals $\\mathbf{R}$, satisfying (13.1), the mapping is called a linear functional, or one-form. As in Example 13.1(c), we can make the set of all linear functionals on $\\mathbf{V}$ into a vector space itself, with the zero functional as that mapping which takes every vector in $\\mathbf{V}$ into 0 in $\\mathbf{R}$.\n\nDefinition 1: The algebraic dual of a vector space $\\mathbf{V}$ is the set $\\mathbf{V}^{*}$ of all linear functionals made into a vector space under ordinary pointwise addition and scalar multiplication:\n\n$$\n(f+g)(\\mathbf{v})=f(\\mathbf{v})+g(\\mathbf{v}) \\quad(\\lambda f)(\\mathbf{v})=\\lambda f(\\mathbf{v})\n$$\n\nSince any linear functional on $\\mathbf{R}^{n}$ can be expressed as a linear function of the coordinates,\n\n$$\n\\mathbf{v}=v^{1} \\mathbf{e}_{1}+v^{2} \\mathbf{e}_{2}+\\cdots+v^{n} \\mathbf{e}_{n} \\rightarrow f(\\mathbf{v})=a_{1} v^{1}+a_{2} v^{2}+\\cdots+a_{n} v^{n}\n$$\n\nwhere $a_{i}=f\\left(\\mathbf{e}_{i}\\right)$ for each $i$, the functional is completely determined by the $n$-tuple $\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right)$.\n\n\\section*{Differential Notation: One-Forms}\nThus, different functionals correspond to different $n$-tuples, as\n\n$$\nf \\leftrightarrow\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right) \\quad g \\leftrightarrow\\left(b_{1}, b_{2}, \\ldots, b_{n}\\right) \\quad \\ldots\n$$\n\nand it has become customary to represent linear functionals by the compact notation of one-forms:\n\n$$\n\\boldsymbol{\\omega}=a_{1} d x^{1}+a_{2} d x^{2}+\\cdots+a_{n} d x^{n} \\quad \\boldsymbol{\\sigma}=b_{1} d x^{1}+b^{2} d x^{2}+\\cdots+b_{n} d x^{n} \\quad \\cdots\n$$\n\nBut why $d x^{i}$ for the coordinates? The motivation comes from differential geometry. Recall that any class $C^{1}$ multivariate function $F\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ on $\\mathbf{R}^{n}$ has the gradient $\\nabla F=\\left(\\partial F / \\partial x^{i}\\right)$ and the directional derivative (in the direction $\\left(d x^{1}, d x^{2}, \\ldots, d x^{n}\\right)$ )\n\n$$\nd F=\\frac{\\partial F}{\\partial x^{1}} d x^{1}+\\frac{\\partial F}{\\partial x^{2}} d x^{2}+\\cdots+\\frac{\\partial F}{\\partial x^{n}} d x^{n}\n$$\n\nwhich, at a specific point in space, is a one-form that defines a linear functional on $\\mathbf{R}^{n}$ (i.e., the set of all directions). Recall too that, just as in ordinary one-dimensional calculus,\n\n$$\nd x=\\Delta x \\equiv \\text { an unspecified real number }\n$$\n\nnot necessarily small.\n\nEXAMPLE 13.4 (a) In $\\mathbf{R}^{3}$, find the image of $\\mathbf{v}=(1,3,5)$ under the one-forms (linear functionals)\n\n$$\n\\boldsymbol{\\omega}=4 d x^{1}-d x^{2} \\quad \\boldsymbol{\\sigma}=2 d x^{1}+3 d x^{2}-d x^{3} \\quad \\boldsymbol{\\omega}+\\boldsymbol{\\sigma}=6 d x^{1}+2 d x^{2}-d x^{3}\n$$\n\n(b) What is the relationship among $\\boldsymbol{\\omega}(\\mathbf{v}), \\boldsymbol{\\sigma}(\\mathbf{v})$, and $(\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v})$ ?\\\\\n(a)\n\n\n\\begin{align*}\n& \\boldsymbol{\\omega}(\\mathbf{v})=4 \\cdot 1-1 \\cdot 3+0 \\cdot 5=4-3+0=1 \\\\\n& \\boldsymbol{\\sigma}(\\mathbf{v})=2 \\cdot 1+3 \\cdot 3-1 \\cdot 5=2+9-5=6 \\\\\n& (\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v})=6 \\cdot 1+2 \\cdot 3-1 \\cdot 5=6+6-5=7 \\\\\n& \\quad \\boldsymbol{\\omega}(\\mathbf{v})+\\boldsymbol{\\sigma}(\\mathbf{v})=1+6=7=(\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v}) \\tag{b}\n\\end{align*}\n\n\nFor vector spaces different from $\\mathbf{R}^{n}$ we agree to use the procedure of Example 13.4 on the components of vectors relative to an arbitrary basis. That is, to evaluate the image of $\\mathbf{v}=$ $v^{1} \\mathbf{b}_{1}+v^{2} \\mathbf{b}_{2}+\\cdots+v^{n} \\mathbf{b}_{n} \\equiv v^{i} \\mathbf{b}_{i}$ under the one-form $\\boldsymbol{\\omega}=a_{i} d x^{i}$, simply write\n\n\n\\begin{equation*}\n\\boldsymbol{\\omega}(\\mathbf{v})=\\boldsymbol{\\omega}\\left(v^{j} \\mathbf{b}_{j}\\right) \\equiv\\left(a_{i} d x^{i}\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=a_{i} v^{i} \\tag{13.2}\n\\end{equation*}\n\n\nA dual reading of (13.2) gives a better understanding of the relationship between $\\mathbf{V}$ and $\\mathbf{V}^{*}$ (between vectors and one-forms). If we regard the $a_{i}$ as fixed (tantamount to fixing a basis in $\\mathbf{V}$ ) while the $v^{i}$ vary-the \"normal\" situation-then a linear map from $\\mathbf{V}$ to $\\mathbf{R}$ is uniquely defined. If, on the other hand, the vector components $v^{i}$ are held fixed and the coefficients $a_{i}$ are allowed to vary (this amounts to fixing a basis in $\\mathbf{V}^{*}$ ), a linear map from $\\mathbf{V}^{*}$ to $\\mathbf{R}$ is defined (the latter map is actually an element of the space $\\mathrm{V}^{* *}$ ). The expression $a_{i} v^{i}$ is bilinear in the two vector variables $\\mathrm{v}$ and $\\omega$.\n\nTheorem 13.1: If $\\mathbf{V}$ is a finite-dimensional vector space, then $\\mathrm{V}^{*}$ is finite-dimensional, of the same dimension, and is isomorphic to $\\mathbf{V}$.\n\nA proof is given in Problem 13.6.\n\n\\section*{Dual Basis}\n$A$ basis $\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots, \\mathbf{b}_{n}$ for $\\mathbf{V}$ determines one for the dual space $\\mathbf{V}^{*}$ in a very natural way. Each $\\mathbf{v}$ in $\\mathbf{V}$ has a representation $\\mathbf{v}=v^{j} \\mathbf{b}_{j}$ and thus defines a linear functional\n\n\n\\begin{equation*}\n\\varphi(\\mathbf{v})=v^{1} d x^{1}+v^{2} d x^{2}+\\cdots+v^{n} d x^{n} \\tag{13.3}\n\\end{equation*}\n\n\nThen the $n$ linear functionals (vectors in $\\mathbf{V}^{*}$ ) defined by\n\n\n\\begin{equation*}\n\\varphi\\left(\\mathbf{b}_{i}\\right) \\equiv \\boldsymbol{\\beta}^{i} \\quad(i=1,2, \\ldots, n) \\tag{13.4a}\n\\end{equation*}\n\n\nform a basis for $\\mathbf{V}^{*}$ (see Problem 13.6); we say that the basis $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ in $\\mathbf{V}^{*}$ is the dual of the basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ in $\\mathbf{V}$. The evaluation rule (13.2) provides a simpler characterization of the dual basis:\n\n\n\\begin{equation*}\n\\boldsymbol{\\beta}^{i}(\\mathbf{v})=\\left(\\varphi\\left(\\mathbf{b}_{i}\\right)\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=\\left(0 \\cdot d x^{1}+0 \\cdot d x^{2}+\\cdots+1 \\cdot d x^{i}+\\cdots+0 \\cdot d x^{n}\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=v^{i} \\tag{13.4b}\n\\end{equation*}\n\n\nThus, $\\boldsymbol{\\beta}^{i}=d x^{i}$ is the linear functional that picks out the $i$ th component relative to $\\left\\{\\mathbf{b}_{k}\\right\\}$ of any vector in V. A special application of (13.4b) gives\n\n\n\\begin{equation*}\n\\boldsymbol{\\beta}^{i}\\left(\\mathbf{b}_{j}\\right)=\\delta_{j}^{i} \\tag{13.5}\n\\end{equation*}\n\n\nfor all $i, j$.\n\nEXAMPLE 13.5 The standard basis $\\mathbf{e}=\\left\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\right\\}$ for $\\mathbf{R}^{n}$ generates the standard basis for $\\left(\\mathbf{R}^{n}\\right)^{*}$, given in terms of one-forms as\n\n$$\n\\boldsymbol{\\beta}^{1}(\\mathbf{e})=d x^{1} \\quad \\boldsymbol{\\beta}^{2}(\\mathbf{e})=d x^{2} \\quad \\ldots \\quad \\boldsymbol{\\beta}^{n}(\\mathbf{e})=d x^{n}\n$$\n\nSuppose, then, that $\\mathbf{R}^{3}$ is referred to the (nonstandard) basis\n\n$$\nb_{1}=(1,1,0) \\quad b_{2}=(1,0,1) \\quad b_{3}=(0,1,1)\n$$\n\nThis may be written in terms of the standard basis $\\left\\{\\mathbf{e}_{i}\\right\\}$ through a formal matrix multiplication:\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{b}_{1} \\\\\n\\mathbf{b}_{2} \\\\\n\\mathbf{b}_{3}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathbf{e}_{1} \\\\\n\\mathbf{e}_{2} \\\\\n\\mathbf{e}_{3}\n\\end{array}\\right]\n$$\n\nExpress the dual basis $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ for $\\left(\\mathbf{R}^{3}\\right)^{*}$ in terms of its standard basis $\\left(d x^{i}\\right)$ as a similar matrix product.\n\nLet $\\boldsymbol{\\beta}^{i}=a_{1}^{i} d x^{1}+a_{2}^{i} d x^{2}+a_{3}^{i} d x^{3}$; we must solve for the components $a_{j}^{i}$. For $i=1$, we have from (13.5):\n\n$$\n\\begin{array}{r}\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{1}\\right)=\\boldsymbol{\\beta}^{1}(1,1,0)=a_{1}^{1} \\cdot 1+a_{2}^{1} \\cdot 1+a_{3}^{1} \\cdot 0=a_{1}^{1}+a_{2}^{1} \\equiv x+y=1 \\\\\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{2}\\right)=\\boldsymbol{\\beta}^{1}(1,0,1)=x \\cdot 1+y \\cdot 0+z \\cdot 1=x+z=0 \\\\\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{3}\\right)=\\boldsymbol{\\beta}^{1}(0,1,1)=x \\cdot 0+y \\cdot 1+z \\cdot 1=y+z=0\n\\end{array}\n$$\n\n(where $x \\equiv a_{1}^{1}, y \\equiv a_{2}^{1}, z \\equiv a_{3}^{1}$ ). Solving, $x=\\frac{1}{2}=y, z=-\\frac{1}{2}$. A similar analysis may be used to determine the $a_{j}^{2}$ and $a_{j}^{3}$. The final result is\n\n$$\n\\begin{aligned}\n& \\boldsymbol{\\beta}^{1}=\\frac{1}{2} d x^{1}+\\frac{1}{2} d x^{2}-\\frac{1}{2} d x^{3} \\\\\n& \\boldsymbol{\\beta}^{2}=\\frac{1}{2} d x^{1}-\\frac{1}{2} d x^{2}+\\frac{1}{2} d x^{3} \\\\\n& \\boldsymbol{\\beta}^{3}=-\\frac{1}{2} d x^{1}+\\frac{1}{2} d x^{2}+\\frac{1}{2} d x^{3}\n\\end{aligned} \\quad \\text { or } \\quad\\left[\\begin{array}{l}\n\\boldsymbol{\\beta}^{1} \\\\\n\\boldsymbol{\\beta}^{2} \\\\\n\\boldsymbol{\\beta}^{3}\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n\\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nd x^{1} \\\\\nd x^{2} \\\\\nd x^{3}\n\\end{array}\\right]\n$$\n\nObserve that the two basis-connecting matrices are formal inverses of each other.\n\n\\section*{Change of Basis in $\\mathbf{V}$ and $\\mathbf{V}^{*}$}\nThe result of Example 13.5 can be generalized. Let $\\left\\{\\mathbf{b}_{i}\\right\\}$ and $\\left\\{\\overline{\\mathbf{b}}_{i}\\right\\}$ be two bases for $\\mathbf{V}$, and let $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ and $\\left\\{\\overline{\\boldsymbol{\\beta}}^{i}\\right\\}$ be the respective dual bases for $\\mathbf{V}^{*}$. Then\n\n\n\\begin{equation*}\n\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j} \\rightarrow \\overline{\\boldsymbol{\\beta}}^{i}=\\bar{A}_{j}^{i} \\boldsymbol{\\beta}^{j} \\quad \\text { with } \\quad\\left(\\bar{A}_{j}^{i}\\right)=\\left(A_{j}^{i}\\right)^{-1} \\tag{13.6}\n\\end{equation*}\n\n\nthe arrow denoting implication. (See Problem 13.7.)\n\n\\subsection*{13.5 TENSORS ON VECTOR SPACES}\nThe concept of a multilinear functional is needed: If $f\\left(\\mathbf{v}^{1}, \\mathbf{v}^{2}, \\ldots, \\mathbf{v}^{m}\\right)$ represents a mapping of $m$ vector variables into the reals such that the restricted mapping obtained by holding all but one of the variables fixed is a linear functional, then $f$ is said to be multilinear in all its variables.\n\nDefinition 2: A type- $\\left(\\begin{array}{c}p \\\\ q\\end{array}\\right)$ tensor is any multilinear functional $T:\\left(\\mathbf{V}^{*}\\right)^{p} \\otimes \\mathbf{V}^{q} \\rightarrow \\mathbf{R}$ mapping $p$ one-forms and $q$ vectors into the reals; the real image is denoted\n\n$$\nT\\left(\\boldsymbol{\\omega}^{1}, \\ldots, \\boldsymbol{\\omega}^{p} ; \\mathbf{v}^{1}, \\ldots, \\mathbf{v}^{q}\\right)\n$$\n\nEXAMPLE 13.6 Let $T$ represent a linear functional in what follows. A type-(1) tensor takes on real values $T(\\boldsymbol{\\omega})$ for all one-forms $\\boldsymbol{\\omega}$ as argument. As we shall see later, such a tensor can be identified with a contravariant vector. A type- $\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$ tensor takes on real values $T(\\mathbf{v})$ for all vectors $\\mathbf{v}$ as argument; it can be shown to correspond to a covariant vector. A type- $\\left(\\begin{array}{c}1 \\\\ 1\\end{array}\\right)$ tensor takes on real values $U(\\boldsymbol{\\omega} ; \\mathbf{v})$ for all ordered pairs in $\\mathbf{V}^{*} \\otimes \\mathbf{V}$ as argument, with $U$ a bilinear functional.\n\nEXAMPLE 13.7 For $n$-dimensional vectors, the ordinary scalar product $\\mathbf{u} \\cdot \\mathbf{v} \\equiv \\mathbf{u v}$ defines a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor, in the form $G(\\mathbf{u}, \\mathbf{v})=\\mathbf{u v}$, since the elementary properties of the scalar product make $G$ a bilinear mapping of a vector pair into the reals. More generally, an inner product defined arbitrarily by the quadratic form\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=\\mathbf{u}^{T} E \\mathbf{v}\n$$\n\nwhere $E$ is an $n \\times n$ matrix, defines a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor.\n\nDefinition 3: A type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor $G(\\mathbf{u}, \\mathbf{v})$ is (i) symmetric if, for every $\\mathbf{u}$ and $\\mathbf{v}$,\n\n(ii) nonsingular if\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=G(\\mathbf{v}, \\mathbf{u})\n$$\n\n$$\n[G(\\mathbf{u}, \\mathbf{v})=0, \\text { identically in } \\mathbf{u}] \\rightarrow \\mathbf{v}=\\mathbf{0}\n$$\n\nand (iii) positive definite if, for any nonzero vector $\\mathbf{u}$,\n\n$$\nG(\\mathbf{u}, \\mathbf{u})>0\n$$\n\nA type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor that is symmetric and nonsingular is called a metric tensor. (A positive definite tensor is necessarily nonsingular.)\n\nEXAMPLE 13.8 Let $C=\\left[C_{j}^{i}\\right]_{n n}$ be a square matrix and let $\\left(a_{i}\\right)$ and $\\left(v^{i}\\right)$ be the respective components of $\\omega$ and $\\mathbf{v}$ relative to the standard basis in $\\mathbf{R}^{n}$ and its dual. Then the matrix product\n\n$$\nT(\\boldsymbol{\\omega} ; \\mathbf{v})=\\boldsymbol{\\omega} C \\mathbf{v} \\equiv a_{i} C_{j}^{i} v^{j} \\quad \\text { (a bilinear form) }\n$$\n\ndefines a type- $\\left(\\begin{array}{l}1 \\\\ 1\\end{array}\\right)$ tensor over the vector space $\\mathbf{R}^{n}$.\n\n\\section*{Tensor Components}\nIn the three types of tensors considered in Examples 13.6-13.8, we may define tensor components in the following manner, which may be generalized to arbitrary tensors in an obvious way. Let $\\mathbf{b}_{1}, \\ldots, \\mathbf{b}_{n}$ be a basis for $\\mathbf{V}$, and $\\boldsymbol{\\beta}^{1}, \\ldots, \\boldsymbol{\\beta}^{n}$ its dual in $\\mathbf{V}^{*}$. Then, for each $i$, write\n\n$$\n\\begin{array}{ll}\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n\\mathbf{0}\n\\end{array}\\right) & T^{i}=T\\left(\\boldsymbol{\\beta}^{i}\\right) \\\\\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{0} \\\\\n1\n\\end{array}\\right) & T_{i}=T\\left(\\mathbf{b}_{i}\\right) \\\\\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n1\n\\end{array}\\right) & T_{j}^{i}=T\\left(\\boldsymbol{\\beta}^{i} ; \\mathbf{b}_{j}\\right)\n\\end{array}\n$$\n\nEXAMPLE 13.9 Find the components, relative to the standard basis for $\\mathbf{V}=\\mathbf{R}^{n}$, of a type- $\\left(\\begin{array}{l}1 \\\\ 1\\end{array}\\right)$ tensor on $\\mathbf{V}$ constructed by the recipe of Example 13.8.\n\nBy construction, $T(\\boldsymbol{\\omega} ; \\mathbf{v})=a_{i} C_{j}^{i} v^{j}$, for all $\\boldsymbol{\\omega}$ and $\\mathbf{v}$. Substituting $\\boldsymbol{\\omega}=\\boldsymbol{\\beta}^{p}=d x^{p}$ and $\\mathbf{v}=\\mathbf{b}=\\mathbf{e}_{q}$, we find\n\n$$\nT_{q}^{p} \\equiv T\\left(d x^{p} ; \\mathbf{e}_{q}\\right)=\\delta_{i}^{p} C_{j}^{i} \\delta_{q}^{j}=C_{q}^{p}\n$$\n\nThus, the components of $T$ are independent of those of the arguments, $\\boldsymbol{\\omega}$ and $\\mathbf{v}$, and depend only on the components of the matrix $C$.\n\n\\section*{Effect of Change of Basis on Tensor Components}\nUnder a change of basis, (13.6),\n\n$$\n\\begin{aligned}\n& \\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n\\mathbf{0}\n\\end{array}\\right) \\quad \\bar{T}^{i}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}\\right)=T\\left(\\bar{A}_{r}^{i} \\boldsymbol{\\beta}^{r}\\right)=\\bar{A}_{r}^{i} T\\left(\\boldsymbol{\\beta}^{r}\\right)=T^{r} \\bar{A}_{r}^{i} \\\\\n& \\text { type }\\left(\\begin{array}{l}\n0 \\\\\n\\mathbf{1}\n\\end{array}\\right) \\quad \\bar{T}_{i}=T\\left(\\overline{\\mathbf{b}}_{i}\\right)=T\\left(A_{i}^{r} \\mathbf{b}_{r}\\right)=A_{i}^{r} T\\left(\\mathbf{b}_{r}\\right)=T_{r} A_{i}^{r}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-203}\n\\end{center}\n\nEXAMPLE 13.10 If $\\mathbf{V}$ is Euclidean $\\mathbf{R}^{n}$, the change of basis $\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j}$ induces the change of coordinates $x^{i}=A_{j}^{i} \\bar{x}^{j}$, for which\n\n$$\n\\bar{J} \\equiv\\left(\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right)=A \\quad \\text { and so } \\quad J=\\bar{J}^{-1}=\\bar{A}\n$$\n\nThe above transformation formulas then reduce to the classical laws for affine tensors-compare (3.21).\n\n\\subsection*{13.6 THEORY OF MANIFOLDS}\nA manifold is the natural extension of a surface to higher dimensions, and also to spaces more general than $\\mathbf{R}^{n}$. It is helpful at first to think of a manifold as just a hypersurface in $\\mathbf{R}^{n}$.\n\nBy the term neighborhood of a point we shall understand either the set of all points in $\\mathbf{R}^{n}$ within some fixed distance from the given point, or any set containing these points. A neighborhood of $p$ will be denoted $\\mathbf{U}_{p}$. If the concept used for distance in $\\mathbf{R}^{m}$ is Euclidean, then every neighborhood $\\mathbf{U}_{p}$ contains a solid, spherical ball (or \"hyperball,\" if $n>3$ ), having some positive radius and centered at $p$. A neighborhood of a point $p$ in a set is the intersection of a neighborhood $\\mathbf{U}_{p}$ and the set. A set is open if each of its points has a neighborhood completely composed of points of the set. An open neighborhood is simply a neighborhood that is also an open set (in the case of a solid-ball neighborhood, the outer boundary of the ball would have to be removed in order to make it an open neighborhood).\n\n\\section*{Descriptive Definition of a Manifold}\nA manifold is a set which has the property that each point can serve as the origin of local coordinates that are valid in an open neighborhood of the point, which neighborhood is an exact \"copy\" of an open neighborhood of a point in $\\mathbf{R}^{n}$. Though such a definition allows the manifold to lie in a metric space, topological space, Banach space, or other abstract mathematical system, it is best that we begin with manifolds in a simpler space, like $\\mathbf{R}^{m}$. Accordingly:\n\nDefinition 4: A manifold is any set $\\mathbf{M}$ in $\\mathbf{R}^{m}$ which has the property that for each point $p$ in the manifold there exists an open neighborhood $\\mathbf{U}_{p}$ in $\\mathbf{M}$ and a mapping $\\varphi_{p}$ which carries $\\mathbf{U}_{p}$ into a neighborhood in $\\mathbf{R}^{n}$. The mapping is required to be a homeomorphism; i.e.\n\n(1) $\\varphi_{p}$ is continuous.\n\n(2) $\\varphi_{p}$ is bijective from $\\mathbf{U}_{p}$ onto its range, $\\varphi_{p}\\left(\\mathbf{U}_{p}\\right)$.\n\n(3) $\\varphi_{p}^{-1}$ is continuous.\n\nSee Fig. 13-1.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-204}\n\\end{center}\n\nFig. 13-1\n\n\\section*{Coordinate Patches, Atlas}\nThe neighborhoods $\\mathbf{U}_{p}$ for $p$ in $\\mathbf{M}$ provide a means for locally ascribing coordinates to $\\mathbf{M}$ which have the correct dimension (e.g., a plane lying in 3-space is actually 2-dimensional and, as a manifold, has a coordinatization by pairs of reals instead of triples, as in Section 10.4). For any point $p$ in $\\mathbf{M}$, the pair $\\left(\\mathbf{U}_{p}, \\varphi_{p}\\right)$ is called a coordinate patch (also chart, or local coordinatization) for $\\mathbf{M}$, while any collection of such pairs for which the neighborhoods $\\mathbf{U}_{p}$ together cover $\\mathbf{M}$ is called an atlas for $\\mathbf{M}$. Since the coordinate patches make $\\mathbf{M} n$-dimensional at each point, $\\mathbf{M}$ is sometimes referred to as an $n$-manifold.\n\nOften a finite number of charts is sufficient for an atlas (Example 13.11). It can be proved that if a manifold in $\\mathbf{R}^{m}$ is closed, and bounded in terms of the distance in $\\mathbf{R}^{m}$, a finite number of charts will always be sufficient.\n\n\\section*{EXAMPLE 13.11}\n(a) The 2-sphere, denoted $\\mathbf{S}^{2}$, is the ordinary sphere in 3-dimensional space $\\left(y^{i}\\right)$, centered at $(0,0,0)$ having radius $a$. It may be coordinatized by an atlas of only two charts, as follows. [Note that the usual spherical coordinates $(\\varphi, \\theta)$ fail to give a one-one mapping at the poles, where $\\theta$ is indeterminate.]\n\n$$\n\\begin{aligned}\n& y^{1}=\\frac{2 a^{2} x^{1}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\\\\n& y^{2}=\\frac{2 a^{2} x^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\\\\n& y^{3}=\\varepsilon a \\frac{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}-a^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\quad(\\varepsilon= \\pm 1)\n\\end{aligned}\n$$\n\nAs illustrated in Fig. 13-2, the chart corresponding to $\\varepsilon=+1$ has $\\mathbf{U}_{p}$ centered on the south pole (whose coordinates are $x^{1}=x^{2}=0$ ) and including every point of the sphere except the north pole. The other chart $(\\varepsilon=-1)$ is the mirror image of the first chart in the equatorial plane. For a derivation of this atlas, see Problem 13.15.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205}\n\\end{center}\n\nFig. 13-2\n\n(b) The $n$-sphere $\\mathbf{S}^{n}$ in $\\mathbf{R}^{n+1}$ may be defined as the set of points $\\left(y^{i}\\right)$ in $\\mathbf{R}^{n+1}$ such that\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}+\\cdots+\\left(y^{n+1}\\right)^{2}=a^{2}\n$$\n\n(centered at $(0,0,0, \\ldots, 0)$, radius $a)$. A coordinate patch for a neighborhood of $(0,0, \\ldots, a)$ is:\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad \\cdots \\quad y^{n}=x^{n} \\quad y^{n+1}=\\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\cdots-\\left(x^{n}\\right)^{2}}\n$$\n\nwhere the mapping is into the $n$-dimensional neighborhood $\\left(x^{1}\\right)^{2}+\\cdots+\\left(x^{n}\\right)<a^{2} \\quad$ (the interior of $\\mathbf{S}^{n-1}$ ). Establishing the analogous patches around the other \"diametrically opposite\" endpoints, we obtain an atlas of $2 n+2$ charts. (A smaller atlas requires a more clever approach.)\n\n\\section*{Differentiable Manifolds}\nInevitably, there will exist pairs $\\left(\\mathbf{U}_{p}, \\varphi_{p}\\right)$ and $\\left(\\mathbf{U}_{q}, \\varphi_{q}\\right)$ whose neighborhoods overlap in $\\mathbf{M}$ (Fig. 13-3); so the common region $\\mathbf{U}_{p} \\cap \\mathbf{U}_{q} \\equiv \\mathbf{W}$, called an overlapping set, generates a map $\\varphi$ between the images of $\\mathbf{W}$ under $\\varphi_{p}$ and $\\varphi_{q}$. Explicitly (trace the circuit in Fig. 13-3), $\\varphi=\\varphi_{q}{ }^{\\circ} \\varphi_{p}^{-1}$.\n\nIt is clear that $\\varphi$ and $\\varphi^{-1}$ are both continuous. If $\\varphi$ and $\\varphi^{-1}$ are of class $C^{k}$ (have continuous partial derivatives of order $k$ at each point) then the overlapping set $\\mathbf{W}$ is said to be of class $C^{k}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205(1)}\n\\end{center}\n\nFig. 13-3\n\nDefinition 5: A differentiable manifold is a manifold which possesses an atlas such that all overlapping sets are of class $C^{1}$. A $C^{k}\\left(C^{\\infty}\\right.$ or $\\left.C^{\\omega}\\right)$ manifold has an atlas whose overlapping sets are of class $C^{k}\\left(C^{\\infty}\\right.$ or $\\left.C^{\\omega}\\right)$.\n\nRemark 1: Recall the distinction between infinitely differentiable $\\left(C^{\\infty}\\right)$ and analytic $\\left(C^{\\omega}\\right)$.\n\nOne way to ensure that a manifold be $C^{k}$ in the present context is to demand that each $\\varphi_{p}$ and $\\varphi_{p}^{-1}$ be class $C^{k}$. As a matter of convenience, we assume from now on that all manifolds are $C^{\\infty}$ manifolds.\n\nEXAMPLE 13.12 In the case of the spherical manifolds of Example 13.11, the mapping functions $\\varphi_{P}^{-1}$ are either rational with nonvanishing denominators or square roots of positive polynomials. These are certainly $C^{\\infty}$ manifolds (in fact, $C^{\\omega}$ ).\n\nTo bring the notation closer to that of differential geometry (cf. Section 10.4), we now redesignate the maps $\\varphi_{p}^{-1}$ linking $\\mathbf{M}$ with coordinates $\\left(x^{i}\\right)$ : let\n\n$$\n\\varphi_{p}^{-1}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv \\mathbf{r}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv\\left(y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)\\right)\n$$\n\nfor $1 \\leqq j \\leqq m$. (See Fig. 13-4.)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-206}\n\\end{center}\n\nFig. 13-4\n\n\\subsection*{13.7 TANGENT SPACE; VECTOR FIELDS ON MANIFOLDS}\nIntuitively expressed, a vector field $V$ on a manifold $\\mathbf{M}$ is simply a tangent vector to $\\mathbf{M}$ which varies in some continuous (and differentiable) manner from point to point (Fig. 13-5). More precisely, it is a rule that gives a tangent vector at every point of $\\mathbf{M}$. One way to obtain a vector field (if we are in $\\mathbf{R}^{3}$ ) is to take the variable normal vector $\\mathbf{n}$ and cross it with some fixed vector $\\mathbf{a}$; thus $V=\\mathbf{n} \\times \\mathbf{a}$ is a differentiable vector field. But this definition takes us outside the manifold (is extrinsic). We seek a way to remain on the manifold itself (which is immediately applicable to abstract manifolds not imbeddable in a familiar space); such endeavors are called intrinsic methods.\n\nThe clue is to consider some curve on $\\mathbf{M}$ and to define $V$ as the tangent field of that curve. If the curve is defined through a coordinate patch by\n\n$$\n\\mathbf{c}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t), \\ldots, x^{n}(t)\\right)=\\left(y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)\\right) \\quad(1 \\leqq j \\leqq m)\n$$\n\nthen the chain rule gives\n\n$$\n\\frac{d \\mathbf{c}}{d t}=\\frac{d \\mathbf{r}}{d t}=\\frac{\\partial \\mathbf{r}}{\\partial x^{i}} \\frac{d x^{i}}{d t}=V \\quad \\text { or } \\quad V=V^{i} \\mathbf{r}_{i}\n$$\n\nwhere the vectors $\\mathbf{r}_{i} \\equiv \\partial \\mathbf{r} / \\partial x^{i}$ are themselves tangent to $\\mathbf{M}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-207}\n\\end{center}\n\nFig. 13-5\n\nDefinition 6: For any (differentible) manifold $\\mathbf{M}$ having coordinate patch $\\mathbf{U}_{p}$ at any point $p$, the span of the vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ [evaluated at $\\varphi_{p}(p)$ ] is the tangent space at $p$, denoted $T_{p}(\\mathbf{M})$. The union of all tangent spaces $T_{p}(\\mathbf{M})$, for ail $p$ in $\\mathbf{M}$, is called the tangent bundle of $\\mathbf{M}$, denoted $T(\\mathbf{M})$.\n\nAlthough each $T_{p}(\\mathbf{M})$ is a vector space, this does not guarantee that $T(\\mathbf{M})$ is a vector space. For example, the sum of a vector in $T_{p}(\\mathbf{M})$ and a vector in $T_{q}(\\mathbf{M})$ will not generally be tangent to $\\mathbf{M}$.\n\nDefinition 7: A vector field $V$ on a manifold $\\mathbf{M}$ is any $C^{\\infty}$ function that maps $\\mathbf{M}$ to its tangent bundle $T(\\mathbf{M})$. That is, for each point $p$ in $\\mathbf{M}$, the image $\\mathbf{V}(p)=\\mathbf{V}_{p}$ is a vector belonging to the tangent space $T_{p}(\\mathbf{M})$ at $p$. Explicitly, for certain scalar functions $V^{i}$,\n\n\n\\begin{equation*}\nV=V^{i} \\mathbf{r}_{i}=\\left(V^{i} \\frac{\\partial y^{j}}{\\partial x^{i}}\\right) \\quad(j=1,2, \\ldots, m) \\tag{13.7a}\n\\end{equation*}\n\n\nA fundamental theorem regarding vector fields on manifolds may be proved from the basic theory of systems of ordinary differential equations.\n\nTheorem 13.2: Every vector field on a manifold $\\mathbf{M}$ possesses a system of flow curves or integral curves on $\\mathbf{M}$, defined as curves for which the tangent vector at each point coincides with the given vector field at that point.\n\n\\section*{Notation}\nTo de-emphasize the particular choice of coordinatization map $\\varphi_{p}: \\mathbf{U}_{p} \\rightarrow \\mathbf{R}^{n}$, it is customary to omit the vector $\\mathbf{r}$ from the above description of the basis for $T_{p}(\\mathbf{M})$, and to write\n\n$$\n\\frac{\\partial}{\\partial x^{1}}, \\frac{\\partial}{\\partial x^{2}}, \\ldots, \\frac{\\partial}{\\partial x^{n}} \\text { in place of } \\frac{\\partial \\mathbf{r}}{\\partial x^{1}}, \\frac{\\partial \\mathbf{r}}{\\partial x^{2}}, \\ldots, \\frac{\\partial \\mathbf{r}}{\\partial x^{n}}\n$$\n\nor, even more cursorily, $\\partial_{1}, \\partial_{2}, \\ldots, \\partial_{n}$. Many textbooks use this last notation exclusively, and write $(13.7 a)$ as\n\n\n\\begin{equation*}\nV=V^{i} \\partial_{i} \\tag{13.7b}\n\\end{equation*}\n\n\nSuch shorthand is particularly convenient when the coordinate maps $\\varphi_{p}^{-1}$ for a particular manifold are unspecified (for example, when the manifold is defined by an equation $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$ for some real-valued function $F$ ). In this situation, since $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ are not explicitly defined, we use the notation $E_{1}, E_{2}, \\ldots, E_{n}$ to denote the coordinate frame on $\\mathbf{M}$, whose restriction to $T_{p}(\\mathbf{M})$, for each $p$ in $\\mathbf{M}$, is a basis for $T_{p}(\\mathbf{M})$. We make the identifications $E_{i} \\equiv \\partial_{i} \\quad(i=1,2, \\ldots, n)$, giving\n\n\n\\begin{equation*}\nV=V^{i} E_{i} \\tag{13.7c}\n\\end{equation*}\n\n\n\\section*{Extrinsic Representation of Vector Fields}\nIt is possible to represent a vector field $V$ on a manifold without reference to coordinate patches (which often have the disadvantage of being complicated or difficult to construct); one can stay entirely in the $\\left(y^{i}\\right)$ system, which we assume to be rectangular. Suppose $\\mathbf{M}$ is given by a single equation $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$, for some $C^{k}$ function $F$. A one-form $\\sigma=\\omega_{i} d y^{i}$, where $\\omega_{i}=$ $\\omega_{i}\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)$, is said to be restricted to $\\mathbf{M}$ if the point $\\left(y^{i}\\right)$ is required to lie on $\\mathbf{M}$; that is, $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$. As is well known from multidimensional calculus, the gradient $\\nabla F=\\left(\\partial F / \\partial y^{i}\\right)$ is normal to $\\mathbf{M}$, so that if we further require that the restriction of $\\sigma$ map $\\nabla F$ into zero,\n\n$$\n\\omega_{i} \\frac{\\partial F}{\\partial y^{i}}=0\n$$\n\nthen $V=\\sigma$ is a vector field on $\\mathbf{M}$ (having components $d y^{i}$ ).\n\nEXAMPLE 13.13 Consider the paraboloid $\\mathbf{P}$ in $\\mathbf{R}^{3}$ given by\n\n$$\nF\\left(y^{1}, y^{2}, y^{3}\\right)=\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}-y^{3}=0\n$$\n\nShow that the restriction of $\\sigma=y^{1} y^{2} d y^{1}+\\left(y^{2}\\right)^{2} d y^{2}+2 y^{2} y^{3} d y^{3}$ to $\\mathbf{P}$ is a vector field on $\\mathbf{P}$.\n\nWe must show that the scalar product of $\\left(y^{1} y^{2},\\left(y^{2}\\right)^{2}, 2 y^{2} y^{3}\\right)$ and $\\nabla F=\\left(2 y^{1}, 2 y^{2},-1\\right)$ is zero:\n\n$$\n\\left(y^{1} y^{2}\\right)\\left(2 y^{1}\\right)+\\left(y^{2}\\right)^{2}\\left(2 y^{2}\\right)+\\left(2 y^{2} y^{3}\\right)(-1)=\\left[\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}-y^{3}\\right] 2 y^{2}=0\n$$\n\n\\subsection*{13.8 TENSOR FIELDS ON MANIFOLDS}\n\\section*{Dual Tangent Bundle}\nAt each $p$ in $\\mathbf{M}$, let $T_{p}^{*}(\\mathbf{M})$ denote the dual of the vector space $T_{p}(\\mathbf{M})$, and denote by $T^{*}(\\mathbf{M})$ the union of all spaces $T_{p}^{*}(\\mathbf{M})$. The set $T^{*}(\\mathbf{M})$, called the dual tangent bundle of $\\mathbf{M}$, is not necessarily a vector space (just as $T(\\mathbf{M})$ was not).\n\nWe need to make explicit certain elements of $T^{*}(\\mathbf{M})$.\n\n\\section*{Differentials on $M$}\nThe differential of a function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ is rigorously defined as a two-vector function $d f:\\left(\\mathbf{R}^{n}\\right)^{2} \\rightarrow \\mathbf{R}$ which maps each pair $(\\mathbf{x}, \\mathbf{v})$ - where $\\mathbf{x}$ is a point in $\\mathbf{R}^{n}$ and $\\mathbf{v}=\\left(d x^{1}, d x^{2}, \\ldots, d x^{n}\\right)$ is a direction in $\\mathbf{R}^{n}$-to the real number\n\n\n\\begin{equation*}\nd f(\\mathbf{x}, \\mathbf{v}) \\equiv \\frac{\\partial f}{\\partial x^{1}} d x^{1}+\\frac{\\partial f}{\\partial x^{2}} d x^{2}+\\cdots+\\frac{\\partial f}{\\partial x^{n}} d x^{n}=f_{i} d x^{i} \\tag{13.8}\n\\end{equation*}\n\n\nHere, the $f_{i}$ are evaluated at $\\mathbf{x}$. If $f$ is any real-valued $C^{k}$ function on $\\mathbf{M}$, then the differential of\n\n$$\nf\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv f\\left(y^{1}\\left(x^{1}, \\ldots, x^{n}\\right), y^{2}\\left(x^{1}, \\ldots, x^{n}\\right), \\ldots, y^{m}\\left(x^{1}, \\ldots, x^{n}\\right)\\right)\n$$\n\ncalled a differential field on $\\mathbf{M}$, is\n\n$$\nd f=\\frac{\\partial}{\\partial x^{i}}(f(\\mathbf{r}(x))) d x^{i}=\\frac{\\partial f}{\\partial y^{k}} \\frac{\\partial y^{k}}{\\partial x^{i}} d x^{i}=\\left(\\nabla f \\cdot \\mathbf{r}_{i}\\right) d x^{i}\n$$\n\n\\begin{itemize}\n  \\item a one-form. Thus, $d f$ may be thought of as a mapping from $\\mathbf{M}$ to $T^{*}(\\mathbf{M})$ : we agree that the evaluation of $d f$ at $p$ is the one-form $\\left(\\nabla f(p) \\cdot r_{i}(p)\\right) d x^{i}$ in $T_{p}^{*}(\\mathbf{M})$.\n\\end{itemize}\n\nLet us now compare the two kinds of fields on $\\mathbf{M}$.\n\nvector field\n\ndifferential field\n\n$$\n\\begin{aligned}\n& V: \\frac{\\text { Mapping }}{} \\quad \\frac{\\text { Restricted to } \\mathbf{U}_{P}}{V=V^{i} E_{i}} \\\\\n& d f: \\mathbf{M} \\rightarrow T^{*}(\\mathbf{M}) \\quad \\omega=\\left(\\nabla f \\cdot E_{i}\\right) d x^{i}\n\\end{aligned}\n$$\n\nDefinition 8: A tensor field of type $\\left(\\begin{array}{c}r \\\\ s\\end{array}\\right)$ on a manifold $\\mathbf{M}$ is a mapping $T:\\left[T^{*}(\\mathbf{M})\\right]^{r} \\otimes[T(\\mathbf{M})]^{s} \\rightarrow C^{k}\\left(\\mathbf{R}^{m}\\right)$ taking $r$ differential fields and $s$ vector fields on $\\mathbf{M}$ to real-valued $C^{k}$-functions $f$ on $\\mathbf{R}^{m}$. It is assumed that the evaluation of $T$ at a point $p$ on $\\mathbf{M}$ is given by\n\n$$\nT_{p}\\left(\\omega^{1}, \\ldots, \\omega^{r} ; V_{1}, \\ldots, V_{s}\\right)=T\\left(\\omega_{p}^{1}, \\ldots, \\omega_{p}^{r} ; V_{1 p}, \\ldots, V_{s p}\\right) \\equiv f(p)\n$$\n\nand that each map $T_{p}$ is multilinear.\n\nEXAMPLE 13.14 (a) At each fixed $p$ on M, the mapping $T_{p}$ is a tensor [on the vector space $\\left[T_{p}^{*}(\\mathbf{M})\\right]^{r} \\otimes\\left[T_{p}(\\mathbf{M})\\right]^{s}$, of type $\\left.\\left(\\begin{array}{c}r \\\\ s\\end{array}\\right)\\right]$, per Definition 2. (b) Any vector field $V$ on $\\mathbf{M}$ can be interpreted as a type- $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$ tensor field via a mapping $T(\\omega)=\\omega(V)$; compare Problem 13.20.\n\n\\section*{Solved Problems}\n\\section*{ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}\n13.1 (a) Show that the set of polynomials\n\n$$\np_{1}(t)=1+t \\quad p_{2}(t)=t+t^{2} \\quad p_{3}(t)=t^{2}+t^{3} \\quad p_{4}(t)=t^{3}-1\n$$\n\nis a basis for the vector space $\\mathbf{P}^{3}$ (polynomials of degree $\\leqq 3$ ). (b) Find the components of the polynomial $p(t)=t^{3}$ relative to this basis.\n\n(a) Since the dimension of $\\mathbf{P}^{3}$ is 4 and there are 4 vectors, it suffices to show they are linearly independent. Suppose that, for all $t$,\n\nor\n\n$$\n\\begin{gathered}\n\\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=0 \\\\\n\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}\\right) t^{3}=0\n\\end{gathered}\n$$\n\nSince this is an identity, we must have\n\n$$\n0=\\lambda^{1}-\\lambda^{4}=\\lambda^{1}+\\lambda^{2}=\\lambda^{2}+\\lambda^{3}=\\lambda^{3}+\\lambda^{4}\n$$\n\nThus $\\lambda^{1}=\\lambda^{4}, \\lambda^{1}=-\\lambda^{2}=\\lambda^{3}$; so the last equation gives $\\lambda^{1}+\\lambda^{1}=0$ or $\\lambda^{1}=0$, and all $\\lambda^{i}$ vanish, thus proving linear independence.\n\n(b) To find the linear combination yielding $p(t)=t^{3}$, write\n\n$$\n\\begin{aligned}\n& \\qquad \\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=t^{3} \\\\\n& \\text { i.e. } \\quad\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}-1\\right) t^{3}=0 \\\\\n& \\text { or } \\quad \\lambda^{1}=\\lambda^{4} \\quad \\lambda^{1}=-\\lambda^{2}=\\lambda^{3} \\quad \\lambda^{1}+\\lambda^{1}-1=0\n\\end{aligned}\n$$\n\nHence, $\\lambda^{1}=-\\lambda^{2}=\\lambda^{3}=\\lambda^{4}=1 / 2$.\n\n13.2 (a) Model the 4-group by manipulating an ordinary $8 \\frac{1}{2}$ by 11 sheet of paper, in the following way: Let $s$ be the operation of turning the sheet over sideways (as in a book) and setting it on its original location; $u$, the operation of turning the sheet upside down (end-for-end); $b$, both operations ( $s$ followed by $u$, resulting in a $180^{\\circ}$ rotation of the page, face up); and $e$, doing nothing (identity). Interpret the group operation (multiplication) as one operation followed by another (thus, for example, by the above definitions, $b=s u$, reading from left to right). $(b)$ Show that the 4-group cannot be isomorphic to the cyclic group on four elements, $\\mathbf{C}^{4}$.\\\\\n(a) This is one of those problems in mathematics that is best handled without formulas or equations. By simple observation, the operation $u s$ also results in a $180^{\\circ}$ rotation; hence, $b=s u=u s$. It is also clear that if we apply $s$ twice, or $u$ twice, the sheet is left in its original state; $s^{2}=u^{2}=e$. Next, observe that the associative law is valid, so long as we keep the order of the operations intact. It follows that\n\n$$\nb^{2}=(s u)(u s)=s(u)^{2} s=s^{2}=e\n$$\n\nWhen we multiply all the group elements by $b$, we obtain:\n\n$$\nb e=b \\quad b b=e \\quad b u=(s u) u=s u^{2}=s \\quad b s=(s u) s=(u s) s=u\n$$\n\nHence, the multiplication table for this group may be displayed and may be seen to coincide with that for the 4-group:\n\n\\begin{center}\n\\begin{tabular}{c|cccc}\n$\\cdot$ & $e$ & $s$ & $u$ & $b$ \\\\\n\\hline\n$e$ & $e$ & $s$ & $u$ & $b$ \\\\\n$s$ & $s$ & $e$ & $b$ & $u$ \\\\\n$u$ & $u$ & $b$ & $e$ & $s$ \\\\\n$b$ & $b$ & $u$ & $s$ & $e$ \\\\\n\\end{tabular}\n\\end{center}\n\n(b) For the cyclic group, $\\left\\{e, z, z^{2}, z^{3}\\right\\}$, with $z^{4}=e$ for some $z$, we could not have $z^{2}=e$, which is the characteristic property of all elements of the 4-group.\n\n13.3 The simple Lorentz group can be studied by compressing the $4 \\times 4$ matrices down to $2 \\times 2$ matrices:\n\n$$\n\\left[\\begin{array}{llll}\na & b & 0 & 0 \\\\\nb & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad\\left(a^{2}-b^{2}=1\\right)\n$$\n\nShow explicitly that all real $2 \\times 2$ matrices of the above form constitute an abelian group (the group $\\mathbf{L}(2)$ ) under matrix multiplication, and that $\\mathbf{L}(2)$ is a subgroup of the following two larger groups:\n\n$$\n\\begin{aligned}\n& \\mathbf{G L}(2, \\mathbf{R}) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d \\neq b c \\\\\n& \\mathbf{S U}(2) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d-b c=1\n\\end{aligned}\n$$\n\nSince for a matrix in $\\mathbf{L}(2)$,\n\n$$\na d-b c=a^{2}-b^{2}=1\n$$\n\nall such matrices belong to $\\mathbf{S U}(2)$, which, in turn, is a $\\operatorname{subgroup}$ of $\\mathbf{G L}(2, \\mathbf{R})$. Now verify the group properties:\n\n(1) $u v$ belongs to the group for all $u, v$.\n\nIf\n\n$$\n\\begin{gathered}\nA=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right] \\\\\nA B=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right]=\\left[\\begin{array}{ll}\na c+b d & a d+b c \\\\\nb c+a d & b d+a c\n\\end{array}\\right] \\equiv\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right] \\\\\nx^{2}-y^{2}=\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)=(1)(1)=1\n\\end{gathered}\n$$\n\n$$\n\\begin{aligned}\n& \\text { then } \\\\\n& \\text { and }\n\\end{aligned}\n$$\n\n(2) $(u v) w=u(v w)$. Yes: matrix multiplication is associative.\n\n(3) For some $e$ and all $u$, $e u=u e=u$. Yes: the identity matrix has $1^{2}-0^{2}=1$, so is a member of $\\mathbf{L}(2)$.\n\n(4) Given $u, u^{-1} u=u u^{-1}=e$ for some $u^{-1}$.\n\n$$\n\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]^{-1}=\\frac{1}{a^{2}-b^{2}}\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]=\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]\n$$\n\nwhich is in $\\mathbf{L}(2)$.\\\\\n$u v=v u \\quad$ (abelian group).\n\n\\[\nB A=\\left[\\begin{array}{ll}\nc & d  \\tag{5}\\\\\nd & c\n\\end{array}\\right]\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nc a+d b & c b+d a \\\\\nd a+c b & d b+c a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right]=A B\n\\]\n\n\\section*{VECTOR SPACE CONCEPTS}\n13.4 (a) Show that the space $\\mathbf{P}$ of all real-valued polynomials in a real variable $x$ is infinitedimensional. (b) Conclude that $C^{k}(\\mathbf{R})$ is infinite-dimensional.\n\n(a) Suppose that $\\mathbf{P}$ had the finite basis $\\left\\{p_{1}, p_{2}, \\ldots, p_{n}\\right\\}$. Then, for any real polynomial $p(x)$, there exist constants $a_{1}, \\ldots, a_{n}$ such that\n\n\n\\begin{equation*}\na_{1} p_{1}(x)+a_{2}(x)+\\cdots+a_{n} p_{n}(x)=p(x) \\tag{1}\n\\end{equation*}\n\n\nWrite (1) for the $n+1$ values $x_{1}<x_{2}<\\cdots<x_{n+1}$ as a matrix equation:\n\n\\[\na_{1}\\left[\\begin{array}{c}\np_{1}\\left(x_{1}\\right)  \\tag{2}\\\\\np_{1}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{1}\\left(x_{n+1}\\right)\n\\end{array}\\right]+a_{2}\\left[\\begin{array}{c}\np_{2}\\left(x_{1}\\right) \\\\\np_{2}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{2}\\left(x_{n+1}\\right)\n\\end{array}\\right]+\\cdots+a_{n}\\left[\\begin{array}{c}\np_{n}\\left(x_{1}\\right) \\\\\np_{n}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{n}\\left(x_{n+1}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{c}\np\\left(x_{1}\\right) \\\\\np\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np\\left(x_{n+1}\\right)\n\\end{array}\\right]\n\\]\n\nThe column vectors on the left are elements of $\\mathbf{R}^{n+1}$, and as there are $n$ of them, they do not span $\\mathbf{R}^{n+1}$ (see Problem 13.5). To finish the proof, we have only to choose a vector on the right of (2) that is not in the span of those on the left-say, $\\left(z_{1}, z_{2}, \\ldots, z_{n+1}\\right)$-and then to exhibit a polynomial $p$ that takes on those values at $x_{1}, x_{2}, \\ldots, x_{n+1}$. The polynomial provided by Lagrange's interpolation formula does the job.\n\n(b) For any $k$, the vector space $C^{k}(\\mathbf{R})$ contains the infinite-dimensional subspace $\\mathbf{P}$; thus, it too is infinite-dimensional.\n\n13.5 The set $\\mathbf{S}$ of all linear combinations of a fixed set of $n$ vectors, $\\left\\{\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots, \\mathbf{b}_{n}\\right\\}$, is called the span of the given vectors; it is obviously a vector space. Prove that this space has dimension $m \\leqq n$, with equality if and only if the given vectors are linearly independent.\n\nFirst we show that any $n+1$ vectors in $\\mathbf{S}$ are linearly dependent. Suppose, on the contrary, that $\\left\\{\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n+1}\\right\\}$ are linearly independent. Then, because the sequence of vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nis necessarily dependent, the well-known exchange lemma tells us that a sequence\n\n$$\n\\begin{array}{lllllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\ldots & \\mathbf{b}_{j-1} & \\mathbf{b}_{j+1} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nalso spans $\\mathbf{S}$. Repeating the argument $n-1$ times, we arrive at the result that the vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{n} & \\mathbf{u}_{n-1} & \\ldots & \\mathbf{u}_{2} & \\mathbf{u}_{1}\n\\end{array}\n$$\n\nspan $\\mathbf{S}$, making $\\mathbf{u}_{n+1}$ dependent on them-a contradiction.\n\nIf, therefore, $\\left\\{\\mathbf{b}_{i}\\right\\}$ is linearly independent, it constitutes a basis for $\\mathbf{S}$, and $m=n$. On the other hand, if only $m<n$ of the $\\mathbf{b}_{i}$ are linearly independent, the above argument shows that any basis consists of exactly $m$ vectors.\n\n\\section*{DUAL SPACE}\n\\subsection*{13.6 Prove Theorem 13.1.}\nIt is almost trivial that any two vector spaces of dimension $n$ are isomorphic [if $\\left\\{\\mathbf{b}_{i}^{(1)}\\right\\}$ and $\\left\\{\\mathbf{b}_{i}^{(2)}\\right\\}$ are bases, set up the correspondence $\\left.v^{i} \\mathbf{b}_{i}^{(1)} \\leftrightarrow v^{i} \\mathbf{b}_{i}^{(2)}\\right]$. Thus it is necessary to prove only that $\\mathbf{V}^{*}$ is $n$-dimensional if $\\mathbf{V}$ is; in other words, to prove that the set of vectors $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ defined by (13.4) (i) is linearly independent and (ii) has $\\mathbf{V}^{*}$ as its span. (Problem 13.5 will then immediately yield Theorem 13.1.)\n\nProof of (i): By (13.5), for $j=1,2, \\ldots, n$,\n\n$$\n\\lambda_{i} \\boldsymbol{\\beta}^{i}(\\mathbf{v})=0 \\quad \\rightarrow \\quad \\lambda_{i} \\boldsymbol{\\beta}^{i}\\left(\\mathbf{b}_{j}\\right)=0 \\rightarrow \\lambda_{i} \\delta_{j}^{i}=0 \\quad \\rightarrow \\quad \\lambda_{j}=0\n$$\n\nProof of (ii): If $\\boldsymbol{\\beta}(\\mathbf{v})$ is an arbitrary element of $\\mathbf{V}^{*}$, then, by $(13.4 b)$,\n\n$$\n\\boldsymbol{\\beta}(\\mathbf{v})=\\boldsymbol{\\beta}\\left(v^{i} \\mathbf{b}_{i}\\right)=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) v^{i}=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) \\boldsymbol{\\beta}^{i}(\\mathbf{v})\n$$\n\nthat is, $\\boldsymbol{\\beta}$ is a linear combination of the $\\boldsymbol{\\beta}^{i}$.\n\n13.7 Prove the inverse relation between the matrices $A$ and $\\bar{A}$ of (13.6).\n\nBy definition $\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j}$ and $\\overline{\\boldsymbol{\\beta}}^{j}=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}$, so look at (13.5): $\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\delta_{i}^{j}$. By the algebra of mappings and the fact that each $\\overline{\\boldsymbol{\\beta}}^{j}$ and $\\boldsymbol{\\beta}^{k}$ is linear, we have\n\n$$\n\\begin{aligned}\n\\delta_{i}^{j}=\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right) & =\\left(\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\right)\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(A_{i}^{r} \\mathbf{b}_{r}\\right) \\\\\n& =\\bar{A}_{k}^{j}{ }^{j} A_{i}^{r} \\boldsymbol{\\beta}^{k}\\left(\\mathbf{b}_{r}\\right)=\\bar{A}_{k}^{j} A_{i}^{r} \\delta_{r}^{k}=\\bar{A}_{k}^{j} A_{i}^{k}\n\\end{aligned}\n$$\n\nthat is, $\\bar{A} A=I$.\n\n\\section*{TENSORS ON VECTOR SPACES}\n13.8 Which of the following represent linear mappings of $\\left(\\mathbf{R}^{3}\\right)^{*}$ (taking the one-forms on $\\mathbf{R}^{3}$ into the reals), and so constitute (contravariant) tensors of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$ ?\\\\\n(a) $T\\left(a_{1} d x^{1}+a_{2} d x^{2}+a_{3} d x^{3}\\right)=a_{1} a_{2} a_{3}$\\\\\n(b) $T\\left(a_{i} d x^{i}\\right)=a_{1}-a_{3}$\\\\\n(c) $T\\left(a_{i} d x^{i}\\right)=1$\\\\\n(d) $T\\left(a_{i} d x^{i}\\right)=0$\n\n(b) and (d)-the only linear mappings.\n\n13.9 Associated with a particular basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ of a vector space of dimension $n$, we are given some set of numbers $\\left\\{C_{k}^{i j} ; \\quad i, j, k=1, \\ldots, n\\right\\}$. Then we define another set of numbers (and assume a similar definition for all changes of bases), $\\left\\{\\bar{C}_{k}^{i j} ; i, j, k=1, \\ldots, n\\right\\}$, such that\n\n$$\n\\bar{C}_{k}^{i j}=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s}\n$$\n\nand call these numbers the components of the \"tensor\" $C$ on the new basis $\\left\\{\\overline{\\mathbf{b}}_{i}\\right\\}$. Show that this \"tensor\" is indeed a tensor per Definition 2.\n\nWe have only to define the functional\n\n$$\nT\\left(\\boldsymbol{\\omega}_{1}, \\boldsymbol{\\omega}_{2} ; \\mathbf{v}\\right)=T\\left(a_{i} \\boldsymbol{\\beta}^{i}, b_{j} \\boldsymbol{\\beta}^{j} ; v^{k} \\mathbf{b}_{k}\\right)=a_{i} b_{j} C_{k}^{i j} v^{k}\n$$\n\nwhich, by inspection, is a type $\\left(\\begin{array}{l}2 \\\\ 1\\end{array}\\right)$ tensor. We have:\n\n$$\n\\begin{aligned}\n& T_{k}^{i j}=T\\left(\\boldsymbol{\\beta}^{i}, \\boldsymbol{\\beta}^{j} ; \\mathbf{b}_{k}\\right)=T\\left(\\delta_{r}^{i} \\boldsymbol{\\beta}^{r}, \\delta_{s}^{j} \\boldsymbol{\\beta}^{s} ; \\delta_{k}^{t} \\mathbf{b}_{t}\\right)=\\delta_{r}^{i} \\delta_{s}^{j} C_{t}^{r s} \\delta_{k}^{t}=C_{k}^{i j} \\\\\n& \\bar{T}_{k}^{i j}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\boldsymbol{\\beta}}^{j} ; \\overline{\\mathbf{b}}_{k}\\right)=T\\left(\\bar{A}_{r}^{i} \\boldsymbol{\\beta}^{r}, \\bar{A}_{s}^{j} \\overline{\\boldsymbol{\\beta}}^{s} ; A_{k}^{t} \\mathbf{b}_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} T\\left(\\beta^{r}, \\beta^{s} ; b_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s} \\equiv \\bar{C}_{k}^{i j}\n\\end{aligned}\n$$\n\nwhich show that $T$ and $C$ coincide in all coordinate systems.\n\n13.10 In terms of the components $g_{i j}$ of a metric tensor $G(\\mathbf{u}, \\mathbf{v})$, show that:\n\n(a) $G$ is symmetric if and only if $g_{i j}=g_{j i}$ for all $i, j$.\n\n(b) $G$ is nonsingular if and only if $\\left|g_{i j}\\right| \\neq 0$.\n\n(c) $G$ is positive definite if, for all vectors $\\left(u^{i}\\right) \\neq \\mathbf{0}, g_{i j} u^{i} u^{j} \\neq 0$ and $g_{11}>0$.\n\nBy Section 13.5, $g_{i j}=G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)$ where $\\left\\{\\mathbf{b}_{i}\\right\\}$ is some basis for $\\mathbf{V}$. Then, if $\\mathbf{u}=u^{i} \\mathbf{b}_{i}$ and $\\mathbf{v}=v^{i} \\mathbf{b}_{i}$ are any two vectors in $\\mathbf{V}$,\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=u^{i} v^{j} G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)=g_{i j} u^{i} v^{j}\n$$\n\n(a) $G(\\mathbf{u}, \\mathbf{v})=G(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$, if and only if\n\n$$\ng_{i j} u^{i} v^{j}=g_{i j} v^{i} u^{j}=g_{j i} u^{i} v^{j} \\quad \\text { or } \\quad\\left(g_{i j}-g_{j i}\\right) u^{i} v^{j}=0\n$$\n\nfor all real $u^{i}, v^{j}$, which is true if and only if $g_{i j}=g_{j i}$.\n\n(b) In matrix form, the nonsingularity criterion reads:\n\n$$\n\\left[u^{T} G v=0, \\text { for all } u\\right] \\rightarrow v=0\n$$\n\nBut $u^{T} G v$ vanishes for all $u$ if and only if $G v$ is the zero vector. Hence the criterion takes the form\n\n$$\nG v=0 \\rightarrow v=0\n$$\n\nwhich defines $G$ as a nonsingular matrix (a matrix with nonvanishing determinant).\n\n(c) For each fixed $\\mathbf{u}$ and a scalar parameter $\\lambda$, we have\n\n$$\ng_{i j}\\left(u^{i}+\\lambda \\mathbf{b}_{1}^{i}\\right)\\left(u^{j}+\\lambda \\mathbf{b}_{1}^{j}\\right)=g_{i j}\\left(u^{i}+\\lambda \\delta_{1}^{i}\\right)\\left(u^{j}+\\lambda \\delta_{1}^{j}\\right)=G(\\mathbf{u}, \\mathbf{u})+b \\lambda+g_{11} \\lambda^{2} \\equiv P(\\lambda)\n$$\n\nwhere $b \\equiv\\left(g_{1 j}+g_{j 1}\\right) u^{j}$. If $\\mathbf{u}$ is not in the span of $\\mathbf{b}_{1}$, the quadratic form is, by hypothesis, nonzero. Hence, the discriminant of $P(\\lambda)$ is negative:\n\n$$\nb^{2}-4 g_{11} G(\\mathbf{u}, \\mathbf{u})<0 \\quad \\text { or } \\quad G(\\mathbf{u}, \\mathbf{u})>\\frac{b^{2}}{4 g_{11}} \\geqq 0\n$$\n\nIt only remains to note that if $\\mathbf{u}=\\kappa \\mathbf{b}_{1} \\quad(\\kappa \\neq 0)$, then $G(\\mathbf{u}, \\mathbf{u})=\\kappa^{2} g_{11}$, which is again positive.\n\n13.11 Show that positive-definiteness of a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor $G$ implies its nonsingularity.\n\nIf $G(\\mathbf{u}, \\mathbf{v})=0$ for all $\\mathbf{u}$ and some $\\mathbf{v}$, then $G(\\mathbf{v}, \\mathbf{v})=0$; and so, by positive-definiteness, $\\mathbf{v}=0$.\n\n13.12 A covariant tensor $A(\\mathbf{u}, \\mathbf{v})$ is antisymmetric if and only if $A(\\mathbf{u}, \\mathbf{v})--A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$. Show that a criterion for antisymmetry is:\n\n$$\nA(\\mathbf{u}, \\mathbf{u})=0 \\quad(\\text { all } \\mathbf{u})\n$$\n\nBy bilinearity,\n\n$$\nA(\\mathbf{u}+\\mathbf{v}, \\mathbf{u}+\\mathbf{v})=A(\\mathbf{u}, \\mathbf{u})+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+A(\\mathbf{v}, \\mathbf{v})\n$$\n\nThus, if $A(\\mathbf{u}, \\mathbf{u})=0$ for all $\\mathbf{u}$,\n\n$$\n0=0+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+0 \\quad \\text { or } \\quad A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})\n$$\n\nConversely, suppose $A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}$ and $\\mathbf{v}$. Then, with $\\mathbf{u}=\\mathbf{v}$, we have $A(\\mathbf{u}, \\mathbf{u})=-A(\\mathbf{u}, \\mathbf{u})$, or $A(\\mathbf{u}, \\mathbf{u})=0$.\n\n\\section*{MANIFOLDS}\n13.13 (a) Show that the 1 -sphere $\\mathbf{S}^{1}$ (a circle in $\\mathbf{R}^{2}$ ) can be made into a $C^{\\infty} 1$-manifold by constructing an atlas with two charts. (b) Show that a one-chart atlas does not exist [thus, a circle is not homeomorphic to a line or interval].\n\n(a) The standard parameterization of the circle,\n\n$$\n\\varphi^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos \\theta \\\\\ny^{2}=a \\sin \\theta\n\\end{array} \\quad(0 \\leq \\theta<2 \\pi)\\right.\n$$\n\nis insufficient, since the inverse map $\\varphi$ is discontinuous at point $p$ (Fig. 13-6). But if we define\n\n$$\n\\varphi_{p}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(-\\pi<x^{1}<\\pi\\right) \\quad \\varphi_{q}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(0<x^{1}<2 \\pi\\right)\\right.\\right.\n$$\n\nthen $\\left(\\mathbf{S}^{1}-q, \\varphi_{p}\\right)$ and $\\left(\\mathbf{S}^{1}-p, \\varphi_{q}\\right)$ will constitute an atlas. Since there are no 'singular' points involved, it is clear that $\\varphi_{p}, \\varphi_{q}$ and their inverses are $C^{\\infty}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-214}\n\\end{center}\n\nFig. 13-6\n\n(b) Suppose (U, $\\phi$ ) covered $\\mathbf{S}^{1}\\left(\\mathbf{U}=\\mathbf{S}^{1}\\right)$ and $\\phi$ mapped $\\mathbf{S}^{1}$ to the real line $\\left(x^{1}\\right)$, with both $\\phi$ and $\\boldsymbol{\\phi}^{-1}$ continuous. It is not too difficult to see that $\\phi$ maps the circle to a closed interval I: for continuous maps take bounded, closed sets to bounded, closed sets, and connected sets to connected sets; and the only bounded, closed, connected subsets of the real line are closed finite intervals. For any point $P$ on the circle let $P^{\\prime}$ be its diametrically opposite point. The map $g(t) \\equiv \\phi\\left[\\left(\\phi^{-1}(t)\\right)^{\\prime}\\right]$ takes a real number $t$ in $\\mathbf{I}$, maps it to a unique point $P$ on $\\mathbf{S}^{1}$, goes to the (unique) diametrically opposite point $P^{\\prime}$, and returns to a unique real number $t^{\\prime}$ in $\\mathbf{I}$; it is thus a continuous map from $\\mathbf{I}$ to $\\mathbf{I}$. As such, it must (by a familiar theorem of analysis) have a fixed point:\n\n$$\ng\\left(t_{0}\\right)=t_{0} \\text { for some } t_{0} \\text { in } \\mathbf{I}\n$$\n\nBut this means that $\\phi$ sends some pair of diametrically opposite points on $\\mathbf{S}^{1}$ to the same real number, denying one-oneness of $\\phi$.\n\n13.14 A manifold in $\\mathbf{R}^{4}$ is defined by the charts $\\left(k=\\ldots,-2,-1,0,1,2, \\ldots ; x^{1}>0\\right)$\n\n$$\n\\mathbf{r}_{(k)}:\\left\\{\\begin{array}{l}\ny^{1}=x^{1} \\cos x^{2} \\cos x^{3} \\\\\ny^{2}=x^{1} \\cos x^{2} \\sin x^{3} \\\\\ny^{3}=x^{1} \\sin x^{2} \\\\\ny^{4}=a\\left(x^{2}+x^{3}\\right)\n\\end{array} \\quad\\left((k-1) \\frac{\\pi}{2}<x^{3}<(k+1) \\frac{\\pi}{2}\\right)\\right.\n$$\n\n(a) Show that on each coordinate patch, the mapping $\\mathbf{r}_{(k)}$ is one-to-one; hence, $\\varphi_{(k)}=$ $\\mathbf{r}_{(k)}^{-1}: U_{(k)} \\rightarrow \\mathbf{R}^{3}$ exists. (b) Show that both $\\varphi_{(k)}$ and $\\varphi_{(k)}^{-1}$ are continuous. (c) Show that the manifold is generated by a line in $\\mathbf{R}^{4}$ moving along an axis orthogonal to it, with the axis, in turn, orthogonal to the hyperplane $y^{4}=0$ (use vector geometry in $\\mathbf{R}^{4}$ ). Verify that the parameter $x^{1}$ measures the distance from a given point on the manifold to the axis. (d) Show that the parametric section $x^{3}=0$ is a right helicoid (Example 10.4), lying in the hyperplane $y^{2}=0$ ( $\\mathbf{R}^{3}$ coordinatized by $\\left.y^{1}, y^{3}, y^{4}\\right)$.\n\n(a) Assume that $\\mathbf{r}_{(k)}\\left(x^{i}\\right)=\\mathbf{r}_{(k)}\\left(u^{i}\\right)$; we want to show that $\\left(x^{1}, x^{2}, x^{3}\\right)=\\left(u^{1}, u^{2}, u^{3}\\right)$. Now,\n\n$$\n\\left.\\begin{array}{c}\nx^{1} \\cos x^{2} \\cos x^{3}=u^{1} \\cos u^{2} \\cos u^{2} \\\\\nx^{1} \\cos x^{2} \\sin x^{3}=u^{1} \\cos u^{2} \\sin u^{3}\n\\end{array}\\right\\} \\rightarrow \\tan x^{3}=\\tan u^{3}\n$$\n\nBut, for $\\mathrm{U}_{(k)}$, the argument of the tangent function is restricted to a range of $\\pi$ units; so $x^{3}=u^{3}$. It follows that\n\n$$\na\\left(x^{2}+x^{3}\\right)=a\\left(u^{2}+u^{3}\\right) \\rightarrow x^{2}=u^{2}\n$$\n\nFinally, from $x^{1} \\sin x^{2}=u^{1} \\sin u^{2}$, we obtain $x^{1}=u^{1}$.\\\\\n(b) From the form of $\\varphi_{(k)}^{-1} \\equiv \\mathbf{r}_{(k)}$, this function is $C^{\\infty}$. To solve for $\\left(x^{i}\\right)$ in terms of $\\left(y^{i}\\right)$ (to find $\\left.\\varphi_{(k)}\\right)$, write\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(x^{1}\\right)^{2}\\left(\\cos ^{2} x^{2}\\right)\\left(\\cos ^{2} x^{3}+\\sin ^{2} x^{3}\\right)+\\left(x^{1}\\right)^{2}\\left(\\sin ^{2} x^{2}\\right)=\\left(x^{1}\\right)^{2}\n$$\n\nor $x^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\quad\\left(\\right.$ since $\\left.x^{1}>0\\right)$. Then\n\n$$\n\\sin x^{2}=\\frac{y^{3}}{x^{1}}=\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\n$$\n\nor, for a suitable branch of the function $\\sin ^{-1}$,\n\n$$\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n$$\n\nIt is seen that\n\n$$\n\\varphi_{(k)}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\\\\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right) \\\\\nx^{3}=\\frac{y^{4}}{a}-\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n\\end{array}\\right.\n$$\n\nis continuous (in fact, $C^{\\infty}$ ).\n\n(c) The axis orthogonal to $y^{4}=0$ is the vector $\\mathbf{e}_{4}$ in $\\mathbf{R}^{4}$. At the point $y^{4}=a\\left(x^{2}+x^{3}\\right)=$ const. on the manifold, we have (with $x^{2}, x^{3}$ constants and $x^{1}=t$ )\n\n$$\ny^{1}=t \\cos x^{2} \\cos x^{3} \\quad y^{2}=t \\cos x^{2} \\sin x^{3} \\quad y^{3}=t \\sin x^{2} \\quad y^{4}=\\text { const } .\n$$\n\n-a straight line with direction vector orthogonal to $\\mathbf{e}_{4}$. A previous calculation gives the distance from $\\left(y^{1}, y^{2}, y^{3}, y^{4}\\right)$ on $\\mathbf{M}$ to $\\left(0,0,0, a\\left(x^{2}+x^{3}\\right)\\right)$ as\n\n$$\n\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}=x^{1}\n$$\n\n(d) Set $x^{3}=0$ and the map reduces to\n\n$$\ny^{1}=x^{1} \\cos x^{2} \\quad y^{2}=0 \\quad y^{3}=x^{1} \\sin x^{2} \\quad y^{4}=a x^{2}\n$$\n\n13.15 Derive the charts of Example 13.11(a), using stereographic projection (Fig. 13-7).\n\nAs $P$ is a \"convex\" combination of $Q$ and $\\mathrm{N}$,\n\n\n\\begin{equation*}\n\\left(y^{1}, y^{2}, y^{3}\\right)=\\lambda\\left(x^{1}, x^{2}, 0\\right)+(1-\\lambda)(0,0, a) \\tag{1}\n\\end{equation*}\n\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-215}\n\\end{center}\n\nFig. 13-7\n\nTo determine $\\lambda(\\lambda>0)$, write\n\n$$\na^{2}=\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(\\lambda x^{1}\\right)^{2}+\\left(\\lambda x^{2}\\right)^{2}+[(1-\\lambda) a]^{2}\n$$\n\nand solve, obtaining\n\n\n\\begin{equation*}\n\\lambda=\\frac{2 a^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\tag{2}\n\\end{equation*}\n\n\n[Note that $\\lambda$ is less than or greater than 1 according as $P$ lies in the northern or southern hemisphere; $\\lambda \\neq 0$, so this patch omits the north pole.] Together, (1) and (2) yield the chart $\\varepsilon=+1$; the chart $\\varepsilon=-1$ is obtained by changing $a$ to $-a$ in the above (stereographic projection from the south pole).\n\n\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.16 The hyperboloid of one sheet $4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}-\\left(y^{3}\\right)^{2}=16$ is a $C^{\\infty} 2$-manifold $\\mathbf{M}$, by the coordinatization $(k=1,2)$\n\n$$\n\\varphi_{(k)}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=2 \\cos x^{1} \\cosh x^{2} \\\\\ny^{2}=2 \\sin x^{1} \\cosh x^{2} \\\\\ny^{3}=4 \\sinh x^{2}\n\\end{array} \\quad\\left((k-2) \\pi<x^{1}<k \\pi\\right)\\right.\n$$\n\nwith $\\mathbf{U}_{(1)}=\\mathbf{U}_{p}$ and $p=(2,0,0), \\mathbf{U}_{(2)}=\\mathbf{U}_{q}$ and $q=(-2,0,0)$. Represent the vector field on $\\mathbf{M}$ given by\n\n$$\n\\left(V^{i}\\right)=\\left(4 \\sinh x^{2}, 4 \\cosh x^{2}\\right)\n$$\n\nin terms of $(a)$ a vector basis for the tangent space $T_{p}(\\mathbf{M})$, and $(b)$ extrinsically. (c) Describe this field geometrically.\n\n(a) By the usual tools of surface theory (Section 10.5):\n\n$$\n\\begin{aligned}\n& \\mathbf{r}=\\left(2 \\cos x^{1} \\cosh x^{2}, 2 \\sin x^{1} \\cosh x^{2}, 4 \\sinh x^{2}\\right) \\\\\n& E_{1}=\\mathbf{r}_{1}=\\left(-2 \\sin x^{1} \\cosh x^{2}, 2 \\cos x^{1} \\cosh x^{2}, 0\\right) \\\\\n& E_{2}=\\mathbf{r}_{2}=\\left(2 \\cos x^{1} \\sinh x^{2}, 2 \\sin x^{1} \\sinh x^{2}, 4 \\cosh x^{2}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 0\\right) \\\\\n& \\quad+\\left(8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 16 \\cosh ^{2} x^{2}\\right) \\\\\n& \\quad=\\left(4\\left(\\cos x^{1}-\\sin x^{1}\\right) \\sinh 2 x^{2}, 4\\left(\\cos x^{1}+\\sin x^{1}\\right) \\sinh 2 x^{2}, 16 \\cosh ^{2} x^{2}\\right)\n\\end{aligned}\n$$\n\n(b) From the equations for $y^{1}, y^{2}, y^{3}$ we may calculate:\n\n$$\n\\begin{array}{ll}\n\\cosh x^{2}=\\frac{1}{2} \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}} & \\sinh x^{2}=\\frac{1}{4} y^{3} \\\\\n\\cos x^{1}=\\frac{y^{1}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}} & \\sin x^{1}=\\frac{y^{2}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\n& E_{1}=\\left(-y^{2}, y^{1}, 0\\right) \\\\\n& E_{2}=\\left(\\frac{y^{1} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, \\frac{y^{2} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& \\left(V^{i}\\right)=\\left(y^{3}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-y^{2} y^{3}, y^{1} y^{3}, 0\\right)+\\left(y^{1} y^{3}, y^{2} y^{3}, 4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}\\right) \\\\\n& \\quad=\\left(y^{3}\\left(y^{1}-y^{2}\\right), y^{3}\\left(y^{1}+y^{2}\\right),\\left(y^{3}\\right)^{2}+16\\right)\n\\end{aligned}\n$$\n\n(using the equation of the hyperboloid). Hence, in terms of the coordinates $\\left(y^{i}\\right)$,\n\n$$\nV=\\sigma=y^{3}\\left(y^{1}-y^{2}\\right) d y^{1}+y^{3}\\left(y^{1}+y^{2}\\right) d y^{2}+\\left[\\left(y^{3}\\right)^{2}+16\\right] d y^{3}\n$$\n\n(c) See Fig. 13-8 and note that the first component is zero in the plane $y^{1}=y^{2}$. Hence, along the curve of intersection, the field is always parallel to the $y^{2} y^{3}$-plane. Similarly, along $y^{1}=-y^{2}$, the field is parallel to the $y^{1} y^{3}$-plane. On the circle $y^{3}=0$ the field is $(0,0,16)$, or vertical. Since the third component is $\\geqq 16$, there is always a vertical component.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217}\n\\end{center}\n\nFig. 13-8\n\n13.17 Show that the restrictions of (a) $\\sigma_{1}=y^{1} d y^{2}-y^{2} d y^{1}$ and (b) $\\sigma_{2}=\\left(y^{2}-y^{3}\\right) d y^{1}-\\left(y^{1}+\\right.$ $\\left.y^{3}\\right) d y^{2}+\\left(y^{1}+y^{2}\\right) d y^{3}$ to the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}$ are vector fields. (See Fig. 13-9 for a graph of selected values of $\\sigma_{1}$.) By the well-known \"Hairy-Ball Theorem\" (every head of hair has a cowlick), every continuous vector field on $\\mathbf{S}^{2}$ (and also on $\\mathbf{S}^{n}$, for all even integers $n$ ) is zero at some point on the sphere. In fact, the field must vanish at some point of an arbitrarily selected, open hemisphere. (c) Find the zero points explicitly.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217(1)}\n\\end{center}\n\nFig. 13-9\\\\\n(a) The normal vector to $\\mathbf{S}^{2}$ is $\\omega=2 y^{1} d y^{1}+2 y^{2} d y^{2}+2 y^{3} d y^{3}$ and\n\n\n\\begin{gather*}\n\\sigma_{1} \\cdot \\frac{1}{2} \\omega=\\left(-y^{2}\\right)\\left(y^{1}\\right)+\\left(y^{1}\\right)\\left(y^{2}\\right)+(0)\\left(y^{3}\\right)=0 \\\\\n\\sigma_{2} \\cdot \\frac{1}{2} \\omega=\\left(y^{2}-y^{3}\\right)\\left(y^{1}\\right)-\\left(y^{1}+y^{3}\\right)\\left(y^{2}\\right)+\\left(y^{1}+y^{2}\\right)\\left(y^{3}\\right)  \\tag{b}\\\\\n=y^{1} y^{2}-y^{1} y^{3}-y^{1} y^{2}-y^{2} y^{3}+y^{1} y^{3}+y^{2} y^{3}=0\n\\end{gather*}\n\n\n(c) If $\\sigma_{1}=0,-y^{2}=y^{1}=0$ and $0^{2}=0^{2}+\\left(y^{3}\\right)^{2}=a^{2}$, or $y^{3}= \\pm a$. Thus, the zero points are $(0,0, \\pm a)$. For $\\sigma_{2}=0$,\n\n$$\ny^{2}-y^{3}=y^{1}+y^{3}=y^{1}+y^{2}=0 \\quad \\rightarrow \\quad y^{2}=y^{3}=-y^{1}\n$$\n\nand $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}=3\\left(y^{1}\\right)^{2}$, or $y^{1}= \\pm a / \\sqrt{3}$. Hence, the zero points are $\\pm(a / \\sqrt{3}$, $-a / \\sqrt{3},-a / \\sqrt{3})$.\n\n13.18 Consider a manifold whose coordinatization is not easily determined ( $\\mathbf{S O}(n)$, of Example 13.2(e), is such a manifold in $\\mathbf{R}^{n^{2}}$ ) for which, therefore, base vectors $\\mathbf{r}_{i}=E_{i}$ for $T_{p}(\\mathbf{M})$ are unavailable. Develop a reasonable definition of $T_{p}(\\mathbf{M})$ in this situation, which possesses the salient properties of a \"tangent space\" at point $p$.\n\nTo get an idea of what may be desirable, examine the case when the vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ are available. Each tangent vector has the form $V=V^{i} \\mathbf{r}_{i}$, and when $V$ is the tangent vector of a curve $\\mathscr{C}$ on M-the image of $\\mathscr{C}^{\\prime}: x^{i}=x^{i}(t)$ in the coordinate space $\\mathbf{R}^{n}-$ then\n\n$$\nV=\\frac{d x^{i}}{d t} \\mathbf{r}_{i} \\quad \\text { or } \\quad V^{i}=\\frac{d x^{i}}{d t}\n$$\n\nThus $\\left(V^{i}\\right)$ is a direction vector. Recall that\n\n$$\n\\begin{gathered}\n\\mathbf{r}=\\mathbf{r}\\left(x^{1}, \\ldots, x^{n}\\right) \\equiv \\mathbf{r}\\left(y^{1}\\left(x^{1}, \\ldots, x^{n}\\right), \\dot{y}^{2}\\left(x^{1}, \\ldots, x^{n}\\right), \\ldots, y^{m}\\left(x^{1}, \\ldots, x^{n}\\right)\\right) \\\\\n\\mathbf{r}_{i}=\\left(\\frac{\\partial y^{1}}{\\partial x^{i}}, \\frac{\\partial y^{2}}{\\partial x^{i}}, \\ldots, \\frac{\\partial y^{m}}{\\partial x^{i}}\\right)\n\\end{gathered}\n$$\n\nwhence\n\nThus when we write $V^{i} \\mathbf{r}_{i}$ we are actually indicating the $m$ directional derivatives\n\n$$\nV^{i} \\frac{\\partial y^{j}}{\\partial x^{i}} \\equiv \\nabla y^{j} \\cdot V \\quad(1 \\leqq j \\leqq m)\n$$\n\nwith each $y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ a $C^{\\infty}$ real-valued function (defined on $\\mathbf{M}$ if we identify the points of $\\mathbf{M}$ with their coordinates $\\left(x^{i}\\right)$ in $\\mathbf{R}^{n}$ ). It is customary to let the directional derivative of a function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ in the direction $V$ be denoted\n\n$$\nV(f) \\equiv \\nabla f \\cdot V\n$$\n\nThus, each vector $V$ maps a differentiable, real-valued function $f$ to its directional derivative in the direction $V$. The properties of this mapping are immediate: If $f$ and $g$ denote any two differentiable functions from $\\mathbf{M}$ to $\\mathbf{R}$, with $f g$ denoting the ordinary product of two functions, and if $a$ and $b$ are two scalar constants, then\n\n$$\n\\begin{aligned}\n\\text { linearity } & V(a f+b g)=a V(f)+b V(g) \\\\\n\\text { Leibniz' rule } & V(f g)=V(f) g+f V(g)\n\\end{aligned}\n$$\n\nWith this in mind, and armed with the knowledge that the directional derivatives of all functions on $\\mathbf{M}$ would be enough information to construct the basis $\\left\\{\\mathbf{r}_{i}\\right\\}$ when $\\mathbf{r}$ is known, we frame\n\nDefinition 9: By $\\mathbf{C}^{\\infty}(p)$ will be understood the real-valued $C^{\\infty}$ functions on $\\mathbf{U}_{p}$, such that any two functions that agree on some neighborhood of $p$ are identified.\n\nDefinition 10: The tangent space $T_{p}(\\mathbf{M})$ at $p$ is the set of all mappings $V_{p}: \\mathbf{C}^{\\infty}(p) \\rightarrow \\mathbf{R}$ that satisfy for all $a, b$ in $\\mathbf{R}$ and $f, g$ in $\\mathbf{C}^{\\infty}(p)$ the two conditions\\\\\n(i) $V_{p}(a f+b g)=a V_{p}(f)+b V_{p}(g)$\n\n(ii) $V_{p}(f g)=V_{p}(f) g+f V_{p}(g)$\n\nwith the vector-space operations in $T_{p}(\\mathbf{M})$ defined by\n\n$$\n\\begin{aligned}\n\\left(U_{p}+V_{p}\\right)(f) & \\equiv U_{p}(f)+V_{p}(f) \\\\\n\\left(a V_{p}\\right)(f) & \\equiv a V_{p}(f)\n\\end{aligned}\n$$\n\nAny $V_{p}$ in $T_{p}(\\mathbf{M})$ will be called a tangent vector to $\\mathbf{M}$ at $p$. This definition has the advantage not only of dispensing with coordinates, but of enabling one to extend naturally a mapping $F: \\mathbf{M} \\rightarrow \\mathbf{N}$ (from one manifold to another) to a mapping $F_{*}: T_{p}(\\mathbf{M}) \\rightarrow T_{p^{\\prime}}(\\mathbf{N})$ at each point $p$ in $\\mathbf{M}$, where $p^{\\prime}=F(p)$. Such an extension cannot be accomplished using the more elementary definition.\n\nRemark 2: The vectors of $T_{p}(\\mathbf{M})$ as originally defined, if regarded as mappings on $\\mathbf{C}^{\\infty}(p)$, are members of the abstract $T_{p}(\\mathbf{M})$ (Definition 10). In more advanced treatments it is shown that the reverse is true and that $\\operatorname{dim} T_{p}(\\mathbf{M})=\\operatorname{dim} \\mathbf{M}=n$. Hence, the two approaches to tangent spaces are equivalent.\n\n\\section*{TENSOR FIELDS}\n13.19 Show that tensor fields always have the property of being bilinear with respect to scalar functions (as well as to scalar constants), unlike differential operators.\n\nWe must show that for any scalar function $f$ on $\\mathbf{M}$ and any tensor $T$ of type $\\left(\\begin{array}{c}0 \\\\ r\\end{array}\\right)$,\n\n$$\nT\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right)=f T\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n$$\n\nThis is true, since it is true at each point $p$ of $\\mathbf{M}$ :\n\n$$\n\\begin{aligned}\nT_{p}\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right) & \\equiv T\\left(V_{1 p}, \\ldots, f(p) V_{i p}, \\ldots, V_{r p}\\right)=f(p) T\\left(V_{1 p}, \\ldots, V_{i p}, \\ldots, V_{r p}\\right) \\\\\n& =f T_{p}\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n\\end{aligned}\n$$\n\n13.20 Show how to interpret the tangent vector to a curve on a surface $S$ as a (contravariant) tensor of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$.\n\nLet $\\mathbf{c}=\\mathbf{c}(t)$ be a given curve on $\\mathbf{M}=\\mathbf{S}$, with\n\n$$\n\\mathbf{c}_{*}(t)=\\frac{d \\mathbf{c}}{d t}=\\frac{\\partial \\mathbf{y}}{\\partial x^{i}} \\frac{d x^{i}}{d t}\n$$\n\nDefine for any one-form $\\omega=a_{i} d z^{i}$ the linear mapping from $T^{*}(\\mathbf{M})$ to $\\mathbf{R}$ :\n\n$$\nT(\\omega)=a_{i} \\frac{d x^{i}}{d t} \\equiv \\omega\\left(\\frac{d \\mathbf{x}}{d t}\\right)\n$$\n\nUnder the standard basis $\\left\\{d z^{1}, d z^{2}, \\ldots, d z^{n}\\right\\}$ of $T_{p}^{*}(\\mathbf{M})$, and with $\\omega=d z^{i} \\equiv \\delta_{j}^{i} d z^{i}$,\n\n$$\nT^{i}=T\\left(d z^{i}\\right)=\\delta_{j}^{i} \\frac{d x^{j}}{d t}=\\frac{d x^{i}}{d t}\n$$\n\n(We saw earlier that the $d x^{i} / d t$ were contravariant components.)\n\n13.21 Show how to interpret the gradient of a function as a tensor of type $\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$.\n\nLet $f$ have gradient $\\nabla f \\equiv\\left(\\partial f / \\partial x^{i}\\right)$. Define the linear mapping\n\n$$\nT(V)=V^{i} \\frac{\\partial f}{\\partial x^{i}} \\quad\\left(\\frac{\\partial f}{\\partial x^{i}} \\text { fixed }\\right)\n$$\n\nUse the basis $\\left\\{E_{1}, E_{2}, \\ldots, E_{n}\\right\\}$ for $T_{p}(\\mathbf{M})$; then with $V=E_{i} \\equiv \\delta_{i}^{j} E_{j}$,\n\n$$\nT_{i}=T\\left(E_{i}\\right)=\\delta_{i}^{j} \\frac{\\partial f}{\\partial x^{j}}=\\frac{\\partial f}{\\partial x^{i}}\n$$\n\n\\section*{Supplementary Problems}\n13.22 The set of all $2 \\times 2$ matrices of the form\n\n$$\n\\left[\\begin{array}{rr} \n\\pm 1 & 0 \\\\\n0 & \\pm 1\n\\end{array}\\right]\n$$\n\nwhere all possible combinations of signs are taken, forms a four-element subset of $\\mathbf{G L}(2, \\mathbf{R})$. Is it a subgroup?\n\n13.23 Prove that $\\mathrm{SU}(n)$, the set of all $n \\times n$ matrices over the complex numbers having determinant +1 , is a subgroup of $\\mathbf{G L}(n, \\mathbf{C})$. [Hint: $\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)$ holds for complex matrices.]\n\n13.24 Show that the operator $L(f)=\\int_{0}^{1} f(x) d x$ is a linear functional over the set of continuous, real-valued functions on $[0,1]$.\n\n13.25 In terms of the standard basis $\\left\\{d x^{i}\\right\\}$ of $\\left(\\mathbf{R}^{3}\\right)^{*}$, a new basis is defined by\n\n$$\n\\boldsymbol{\\beta}^{1}=d x^{1}-2 d x^{3} \\quad \\boldsymbol{\\beta}^{2}=2 d x^{1}+d x^{2} \\quad \\boldsymbol{\\beta}^{3}=d x^{1}+d x^{3}\n$$\n\nFind the corresponding dual basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ for $\\mathbf{R}^{3}$ in terms of $\\left(\\mathbf{e}_{i}\\right)$, using (13.6). Check your answer by making several calculations of the form $\\omega(v)=\\bar{\\omega}(\\bar{v})$ (a change of basis does not affect the value a linear functional assigns to a vector).\n\n13.26 Consider a tensor $T(\\boldsymbol{\\omega} ; \\mathbf{v})$ over a vector space of dimension $n$ and its dual, with components $T_{j}^{i}$. (a) Show that the trace $\\tau(T) \\equiv T_{i}^{i}$ is invariant under changes of bases. (b) Find $\\tau(T)$ for the tensor defined by $T(\\boldsymbol{\\omega} ; \\mathbf{v})=\\boldsymbol{\\omega}(\\mathbf{v})$.\n\n13.27 Show that every metric tensor $G$ induces a one-to-one mapping (which is an isomorphism, since it is linear) $\\hat{G}: \\mathbf{V} \\rightarrow \\mathbf{V}^{*}$ from a vector space to its dual, under the definition: For each fixed $\\mathbf{u}$ in $\\mathbf{V}$, let $\\hat{G}(\\mathbf{u})$ be the linear functional $\\hat{G}(\\mathbf{u})(\\mathbf{v})=G(\\mathbf{u}, \\mathbf{v})$, for all $\\mathbf{v}$ in $\\mathbf{V}$. This proves for vector spaces of arbitrary dimension:\n\nTheorem 13.3: If $\\mathbf{V}$ possesses a metric tensor, then $\\mathbf{V}$ is isomorphic to its dual $\\mathbf{V}^{*}$.\n\n13.28 Find a convenient atlas showing that the set in $\\mathbf{R}^{4}$ given by the equation\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}-\\left(y^{4}\\right)^{2}=a^{2}\n$$\n\ncan be made into a $C^{\\infty} 3$-manifold. [Hint: Use radicals, as in Example 13.11(b); here, 6 charts will suffice.]\n\n13.29 Show that the restriction of $\\sigma=y^{1} d y^{2}-y^{2} d y^{1}+y^{3} d y^{4}-y^{4} d y^{3}$ on $\\mathbf{R}^{4}$ to the sphere $\\mathbf{S}^{3}$ is a nonzero vector field on $\\mathbf{S}^{3}$.\n\n13.30 Extend Problem 13.29 to the sphere $\\mathbf{S}^{2 k-1}(k \\geqq 2)$.\n\n13.31 Show that if there are only two points, $p_{1}$ and $p_{2}$, on $\\mathbf{S}^{2}$ where a vector field is zero, those points must be antipodal (endpoints of a diameter).\n\n13.32 Show, by geometric reasoning, that there exists a continuous, nonzero vector field on the torus.\n\n13.33 Show that the restrictions of the following one-forms to $\\mathbf{S}^{4},\\left(y^{1}\\right)^{2}+\\cdots+\\left(y^{5}\\right)^{2}=1$, are vector fields on $\\mathbf{S}^{4}$, and find the points where they are zero:\n\n(a) $\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}$\n\n(b) $\\sigma=\\left(y^{2}-y^{3}-y^{4}\\right) d y^{1}+\\left(y^{3}-y^{1}\\right) d y^{2}+\\left(y^{1}-y^{2}+y^{5}\\right) d y^{3}+y^{1} d y^{4}-y^{3} d y^{5}$\n\nB3.3 Although no nonvanishing continuous vector field exists on the 2-sphere $\\mathbf{S}^{2}$, there are three, mutually orthogonal, unit vector fields on $\\mathbf{S}^{3} \\subset \\mathbf{R}^{4}$. These are, in the extrinsic representation of $\\mathbf{S}^{3}$,\n\n$$\n\\begin{aligned}\n& \\sigma_{1}=-y^{1} d y^{1}+y^{2} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4} \\\\\n& \\sigma_{2}=-y^{3} d y^{1}-y^{4} d y^{2}+y^{1} d y^{3}+y^{2} d y^{4} \\\\\n& \\sigma_{3}=-y^{4} d y^{1}+y^{3} d y^{2}-y^{2} d y^{3}+y^{1} d y^{4}\n\\end{aligned}\n$$\n\nShow this. [Note: Manifolds with such vector-field bases are called parallelizable. The manifolds $\\mathbf{S}^{1}, \\mathbf{S}^{3}$, $\\mathbf{S}^{7}$ - and no other $n$-spheres-and the torus are examples.]\n\n13.35 Without resorting to coordinate patches, express extrinsically the collection of tangent spaces $T(\\mathbf{M})$, if $\\mathbf{M}$ is the hyperboloid of one sheet $\\left(y^{1}\\right)^{2}-4\\left(y^{2}\\right)^{2}+4\\left(y^{3}\\right)^{2}=4$.\n\n13.36 For the manifold $\\mathbf{M}$ of Problem 13.35, consider the coordinate patch\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=\\sqrt{1-\\left(x^{1} / 2\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n\nvalid for $y^{3}>0$. Find an expression for an arbitrary vector in $T_{p}(\\mathbf{M})$.\n\n13.37 One way to show that two surfaces meet at right angles is to show that along the curve of intersection the normal vector to one lies in the tangent space of the other. Illustrate this idea for the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=16$ and the cone $\\left(y^{3}\\right)^{2}=9\\left(y^{1}\\right)^{2}+9\\left(y^{2}\\right)^{2}$, the latter coordinatized by\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=3 \\sqrt{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n\n\\section*{Answers to Supplementary Problems}\n\\section*{CHAPTER 1}\n$1.15 a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}+a_{4} b_{4}+a_{5} b_{5}+a_{6} b_{6}$\n\n1.16 $R_{j k 1}^{1}+R_{j k 2}^{2}+R_{j k 3}^{3}+R_{j k 4}^{4}$. The index $i$ is a dummy index, while $j$ and $k$ are free indices; there are 16 summations.\n\n$1.17 x_{j}$\n\n1.18 (a) $n$; (b) $\\delta_{i j} \\delta_{i j}=\\delta_{i i}=n ;(c) \\delta_{i j} c_{i j}=c_{i i}=c_{11}+c_{22}+c_{33}+\\cdots+c_{n n}$\n\n$1.19 a_{i 3} b_{i 3} \\quad(n=3)$\n\n$1.20 \\quad a_{i j} x_{i} x_{j} \\quad(n=3)$\n\n$1.21 y_{i}=c_{i j} x_{j} \\quad(n=2)$\n\n$1.22 a_{1 k} \\quad(k=2,3)$\n\n$1.23 \\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)=a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{j}\\right)=a_{i j} \\delta_{j k}=a_{i k}$\n\n$1.24 a_{i k}\\left[\\left(x_{i}\\right)^{2}+2 x_{i} x_{k}\\right]$ [not summed on $k$ ]\n\n$1.25\\left(a_{l i j}+a_{i l j}+a_{i j l}\\right) x_{i} x_{j}$\n\n$1.26 a_{k l}+a_{l k}$\n\n1.27 (a) $b_{j}^{i} T_{i}^{r r}$; (b) $a_{i j} b_{j r} x_{r}$; (c) $a_{i j k} b_{i r} b_{j s} b_{k t} x_{r} x_{s} x_{t}$\n\n1.28 (c) $a_{i j}\\left(x_{i}+x_{j}\\right)=a_{i j}\\left(\\varepsilon_{j} x_{i}+\\varepsilon_{i} x_{j}\\right)=a_{i j} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=a_{j i} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=2 a_{i j} \\varepsilon_{i} x_{j}$\n\n\\section*{CHAPTER 2}\n(a) and $(b)\\left[\\begin{array}{lllll}u^{11} & u^{12} & u^{13} & u^{14} & u^{15} \\\\ u^{21} & u^{22} & u^{23} & u^{24} & u^{25} \\\\ u^{31} & u^{32} & u^{33} & u^{34} & u^{35}\\end{array}\\right]$\n\n(c) $\\left[\\begin{array}{lll}u^{11} & u^{12} & u^{13} \\\\ u^{21} & u^{22} & u^{23} \\\\ u^{31} & u^{32} & u^{33} \\\\ u^{41} & u^{42} & u^{43} \\\\ u^{51} & u^{52} & u^{53}\\end{array}\\right]$\n\n(d) $\\left[\\begin{array}{llllll}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0\\end{array}\\right]$\n\n2.25\n\n(a) $\\left[\\begin{array}{l}5 \\\\ 0 \\\\ 5\\end{array}\\right]$\n\n(b) $\\left[\\begin{array}{lll}1 & 2 & -4 \\\\ 2 & 2 & -2\\end{array}\\right]$\n\n2.29 (a) $17 ;(b) 0 ;(c)-1$\n\n2.30 (a) $-a_{12} a_{21} a_{33} a_{44}+a_{12} a_{21} a_{34} a_{43}+a_{12} a_{23} a_{31} a_{44}-a_{12} a_{23} a_{34} a_{41}-a_{12} a_{24} a_{31} a_{43}+a_{12} a_{24} a_{33} a_{41}$\n\n(b) $-a_{12}\\left|\\begin{array}{lll}a_{21} & a_{23} & a_{24} \\\\ a_{31} & a_{33} & a_{34} \\\\ a_{41} & a_{43} & a_{44}\\end{array}\\right| \\equiv a_{12} A_{12}$\n\n2.32 (a) $\\left[\\begin{array}{rr}2 & -1 \\\\ -5 & 3\\end{array}\\right] \\quad$ (b) $\\frac{1}{7}\\left[\\begin{array}{rrr}1 & 3 & 2 \\\\ 1 & -4 & 2 \\\\ 3 & 2 & -1\\end{array}\\right]$\n\n2.33 One need verify only that (i) interchanging a pair of consecutive indices changes the sign of a single factor in the product; (ii)\n\n$$\n\\prod_{p>q} \\frac{p-q}{|p-q|}=\\prod 1=1\n$$\n\n$2.342 \\pi / 3$\n\n2.35 One pair are $(2,3,0)$ and $(-3,-2,5)$.\n\n$2.36\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]=\\left[\\begin{array}{r}-1 \\\\ 5\\end{array}\\right]$\n\n2.37 $Q=x_{1}^{2}+2 x_{2}^{2}-x_{3}^{2}+8 x_{1} x_{2}+6 x_{1} x_{3}$\n\n$2.38 \\quad A=\\left[\\begin{array}{rrrr}-3 & -\\frac{1}{2} & -\\frac{1}{2} & 3 \\\\ -\\frac{1}{2} & -1 & 0 & 0 \\\\ -\\frac{1}{2} & 0 & 1 & 0 \\\\ 3 & 0 & 0 & 0\\end{array}\\right]$\n\n$2.39 \\bar{c}_{i}=c_{r} b_{r i}$, where $\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}$.\n\n$2.40 \\quad g_{11}=13 / 49, g_{12}=g_{21}=4 / 49, g_{22}=5 / 49$\n\n$2.41 d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=3=d(\\mathbf{x}, \\mathbf{y})$\n\n\\section*{CHAPTER 3}\n3.23 (a) $\\mathscr{I}=-2 \\exp \\left(2 x^{1}\\right)<0$\n\n(b) $\\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}x^{1}=\\frac{1}{2} \\ln \\left(\\bar{x}^{-1} \\bar{x}^{2}\\right) \\\\ x^{2}=\\frac{1}{2} \\ln \\left(\\bar{x}^{1} / \\bar{x}^{2}\\right)\\end{array} \\quad\\left(\\bar{x}^{1}, \\bar{x}^{2}>0\\right)\\right.$\n\n(c) $\\bar{J}=\\left[\\begin{array}{rr}1 / 2 \\bar{x}^{1} & 1 / 2 \\bar{x}^{2} \\\\ 1 / 2 \\bar{x}^{1} & -1 / 2 \\bar{x}^{2}\\end{array}\\right]=\\left[\\begin{array}{rr}\\exp \\left(x^{1}+x^{2}\\right) & \\exp \\left(x^{1}+x^{2}\\right) \\\\ \\exp \\left(x^{1}-x^{2}\\right) & -\\exp \\left(x^{1}-x^{2}\\right)\\end{array}\\right]^{-1}$\n\n$3.26 \\frac{\\partial \\bar{f}}{\\partial \\theta}=0$, so that $f(x, y)=\\bar{f}(r)=\\bar{f}\\left(\\sqrt{x^{2}+y^{2}}\\right)=g\\left(x^{2}+y^{2}\\right)$.\n\n$3.29 \\delta_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}=\\delta^{i}{ }_{j}=\\bar{\\delta}^{i}{ }_{j}$\n\n3.30 The inverse Jacobian matrix at $(1,2)$ is\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\bar{x}^{2} & \\bar{x}^{1} \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nBy Problem 3.14(a), covariance of the matrix\n\nwould imply the matrix equation\n\n$$\nE \\equiv\\left[e_{i j}\\right]_{22}=\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 0 \\\\\n1 & 1\n\\end{array}\\right]\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nor\n\nwhich is patently false.\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0 & 2 \\\\\n-2 & 0\n\\end{array}\\right]\n$$\n\n3.32 (a) $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ represents a tensor if and only if\n\n$$\n\\left(T_{s}^{r}+T_{r}^{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T_{r}^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nwhich requires that $J T \\bar{J}=\\bar{J}^{T} T J^{T}$. This last relation, in turn, generally requires that $\\bar{J}=J^{T}$; i.e., $J$ must be an orthogonal matrix.\n\n(b) $\\bar{T}=J T \\bar{J}$, so that $\\bar{T}^{T}=\\bar{T}$ if $\\bar{J}=J^{T}$.\n\n3.35 As $\\mathbf{T}$ is a tensor (Example 3.4), it is an affine tensor: $\\bar{T}^{i}=a_{r}^{i} T^{r}$. Thus,\n\n$$\n\\frac{d \\bar{T}^{i}}{d t}=a_{r}^{i} \\frac{d T^{r}}{d t}\n$$\n\nshowing $d \\mathbf{T} / d t$ also to be an affine tensor. Any affine tensor is a fortiori a cartesian tensor.\n\n\n\\begin{equation*}\n\\bar{u}_{i} \\bar{u}_{i}=\\left(a_{i r} u_{r}\\right)\\left(a_{i s} u_{s}\\right)=a_{i r} a_{i s} u_{r} u_{s}=\\delta_{r s} u_{r} u_{s}=u_{r} u_{r} \\tag{a}\n\\end{equation*}\n\n\n(b) No, because distance and angles are not preserved under arbitrary linear transformations. Specifically, consider $\\bar{x}^{1}=3 x^{1}, \\bar{x}^{2}=x^{1}+x^{2}$. A scalar product in $\\left(\\bar{x}^{i}\\right)$ is\n\n$$\n\\bar{u}_{i} \\bar{v}_{i}=\\left(3 u_{1}, u_{1}+u_{2}\\right) \\cdot\\left(3 v_{1}, v_{1}+v_{2}\\right)=10 u_{1} v_{1}+u_{1} v_{2}+u_{2} v_{1}+u_{2} v_{2}\n$$\n\nThis clearly will not coincide with $u_{1} v_{1}+u_{2} v_{2}$.\n\n\\section*{CHAPTER 4}\n4.19 Write $[\\mathbf{S T}]=\\left(U_{l m n}^{i j k}\\right)$. There are $\\left(\\begin{array}{c}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations for the contraction indices $u$ and $v$ among the contravariant indices, and, for each of these, $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations among the covariant indices. A given quartet of locations can be filled in 2 inequivalent ways. Thus, the desired number is\n\n$$\n\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot 2=18\n$$\n\n4.23 First, use the device of Problem 4.11 to establish that $T_{j k l}^{i} U^{k} V^{l}$ are tensor components for all $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$; then apply the Quotient Theorem twice.\n\n\\section*{CHAPTER 5}\n5.21 $L=a \\pi$; semicircle of radius $a$.\n\n5.22 No: $Q(1,0,3)=-1$.\n\n5.23 $L=2+e$\n\n5.24 The true distance formula, $\\overline{P_{1} P_{2}}=\\sqrt{\\left(x_{1}^{1}-x_{2}^{1}\\right)^{2}+\\left(x_{1}^{2}-x_{2}^{2}\\right)^{2}-0.2021125\\left(x_{1}^{1}-x_{2}^{1}\\right)\\left(x_{1}^{2}-x_{2}^{2}\\right)}$, yields 4.751. for an error of +0.249 .\n\n5.25\n\n$$\nG=\\left[\\begin{array}{ccc}\n\\left(x^{2}\\right)^{2} & x^{1} x^{2} & 0 \\\\\nx^{1} x^{2} & 1+\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\n$5.26\\left(U_{i}\\right)=(0,1,0),\\left(V_{i}\\right)=\\left(x^{2}, x^{1}, 0\\right)$\n\n5.27 (a) $\\|\\mathbf{U}+\\mathbf{V}\\|^{2}=(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U V}=\\|\\mathbf{U}\\|^{2}+\\left\\|\\mathbf{V}^{2}\\right\\|+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\| \\cos \\theta$\n\n(b) Take $\\theta=\\pi / 2$ in (a).\n\n5.28 (a) $x^{2}=C \\exp \\left(-2 b x^{3} / a^{2}\\right)$ (a one-parameter family of spirals on the cylinder $x^{1}=a$ )\\\\\n(b) No: the curves of (a) have tangent field $\\mathbf{V}$ all along their length; but, for orthogonality, it is necessary only that the tangent at intersections with the pseudo-helix be V. For example, the curve $x^{2}=x^{3}$ on $x^{1}=a$ is also orthogonal to the pseudo-helix at the point $x^{2}=-a^{2} / 2 b, x^{3}=a^{4} / 4 b$.\n\n$5.29 \\quad x^{1}=d \\exp \\left(-\\left(x^{2}\\right)^{2} / 2\\right) \\quad(d=$ const. $)$\n\n$5.30 f^{\\prime}\\left(\\theta_{0}\\right) g^{\\prime}\\left(\\theta_{0}\\right)=-a^{2}$ at intersection points.\n\n5.32 (a) $g^{i \\alpha}=\\lambda(\\alpha) \\delta_{\\alpha}^{i}$, which is tantamount to $g^{i j}=g_{i j}=0$ for $i \\neq j$.\n\n$5.33\\|\\mathbf{V}\\|=1, L=\\pi / 2$\n\n$5.34 x^{1}=a, x^{3}=b \\cot x^{2}+c(c=$ const. $)$\n\n\\section*{CHAPTER 6}\n6.19 $\\bar{x}^{i}=\\frac{1}{2} a_{r s}^{i} x^{r} x^{s}+b_{r}^{i} x^{r}+c^{i} \\quad$ (the $b_{j}^{i}$ and $c^{i}$ constants)\n\n$6.20(a)$\n\n$$\nG=\\left[\\begin{array}{cc}\n16\\left(x^{1}\\right)^{2}+1 & 4 x^{1}-3 \\\\\n4 x^{1}-3 & 10\n\\end{array}\\right]\n$$\n\n(b) $\\Gamma_{111}=16 x^{1}, \\Gamma_{112}=4$, all others 0\n\n6.22 The values, in $\\left(x^{i}\\right)$, of the $\\partial x^{i} / \\partial \\bar{x}^{j}$ are easiest found by inverting $J \\equiv\\left(\\partial \\bar{x}^{i} / \\partial x^{j}\\right)$. Final results are:\n\n$$\n\\Gamma_{11}^{1}=\\Gamma_{22}^{1}-\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1\n$$\n\n6.23 From Problem 6.21(b), $\\Gamma_{j k}^{i}=0$ for $j \\neq k$; while, for $j=k=\\alpha$ (no summation on $\\alpha$ ),\n\n$$\n\\Gamma_{\\alpha \\alpha}^{i}=\\frac{\\partial}{\\partial x^{\\alpha}}\\left(\\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=\\left(d_{\\alpha} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=d_{\\alpha} \\delta_{\\alpha}^{i}\n$$\n\n$6.26-\\Gamma_{221}=\\Gamma_{212}=\\Gamma_{122}=x^{1} ; \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=1 / x^{1}, \\Gamma_{22}^{1}=-x^{1}$\n\n6.27\n\n$$\n\\overline{\\mathscr{J}}=\\left|\\begin{array}{lll}\na_{1}^{1} \\exp \\bar{x}^{1} & 2 a_{2}^{1} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{1} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{2} \\exp \\bar{x}^{-1} & 2 a_{2}^{2} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{2} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{3} \\exp \\bar{x}^{1} & 2 a_{2}^{3} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{3} \\exp 3 \\bar{x}^{3}\n\\end{array}\\right|=\\left[6 \\exp \\left(\\bar{x}^{1}+2 \\bar{x}^{2}+3 \\bar{x}^{3}\\right)\\right] \\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0\n$$\n\nHence the condition is $\\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0$.\n\n6.29 $\\bar{x}^{i}=A^{i} x^{1} \\sin x^{2}+B^{i} x^{1} \\cos x^{2}+C^{i} \\quad(i=1,2)$, with\n\n$$\nx^{1}\\left|\\begin{array}{ll}\nA^{1} & B^{1} \\\\\nA^{2} & B^{2}\n\\end{array}\\right| \\neq 0\n$$\n\nfor a bijection.\n\n6.30 No, because of the presence of Christoffel symbols in (6.7).\n\n$6.31 \\quad T_{j r s, k}^{i}=\\frac{\\partial T_{j r s}^{i}}{\\partial x^{k}}+\\Gamma_{u k}^{i} T_{j r s}^{u}-\\Gamma_{j k}^{u} T_{u r s}^{i}-\\Gamma_{r k}^{u} T_{j u s}^{i}-\\Gamma_{s k}^{u} T_{j r u}^{i}$\n\n$6.36 \\kappa=1 / b$\n\n6.37 (a) $\\frac{d^{2} u}{d s^{2}}=\\frac{d^{2} v}{d s^{2}}=0$\n\n(b) $x^{2}=p\\left(x^{1}\\right)^{2}+q$ (a two-parameter family of \"parabolas\")\\\\\n$6.38(a)$\n\n$$\n\\begin{aligned}\n& \\frac{d^{2} x^{2}}{d s^{2}}-\\left(\\sin x^{2} \\cos x^{2}\\right)\\left(\\frac{d x^{3}}{d s}\\right)^{2}=0 \\\\\n& \\frac{d^{2} x^{3}}{d s^{2}}+\\left(2 \\cot x^{2}\\right) \\frac{d x^{2}}{d s} \\frac{d x^{3}}{d s}=0\n\\end{aligned}\n$$\n\n(b) $x^{2}=\\frac{1}{a} s \\quad x^{3}=0$\n\n(c) The solution (b) represents an arc of a particular great circle $\\left(x^{2}+z^{2}=a^{2}\\right.$, in the usual cartesian coordinates) on the sphere. By symmetry of the sphere, all great-circular arcs, and only these, will be geodesics.\n\n\\section*{CHAPTER 7}\n$7.24 \\varepsilon= \\begin{cases}+1 & 0<|t| \\leqq 1 / 2 \\\\ -1 & |t|>1 / 2\\end{cases}$\n\n$7.25 t=0,1$\n\n$7.26 L=8 \\sqrt{2} / 3$\n\n$7.27 t=\\sqrt{5} / 3\\left[t=0\\right.$, which makes $\\gamma \\equiv\\left|g_{i j}\\right|=0$, is disallowed $]$\n\n$7.28 \\quad L=(64+11 \\sqrt{11}) / 216 \\approx 0.465$\n\n$7.29 \\quad \\theta=i \\ln 2$ at $(0,2,0) ; \\theta=\\cos ^{-1}(7 / 4 \\sqrt{11})$ at $(5,2,3)$\n\n7.30 (a) $L=8(1+3 \\sqrt{3}) \\approx 49.57$\n\n(b) $x^{1}=3\\left(\\sigma s^{2 / 3}+4\\right), x^{2}=\\left(\\sigma s^{2 / 3}+4\\right)^{3 / 2}$, where\n\n$$\n\\sigma=\\left\\{\\begin{array}{lr}\n+1 & -8 \\leqq s>0 \\\\\n-1 & 0 \\leqq s \\leqq 24 \\sqrt{3}\n\\end{array}\\right.\n$$\n\n(c) The null points are $t=0(s=-8)$ and $t=1(s=0)$.\n\n7.31 $L=8(5 \\sqrt{5}-1) \\approx 81.44$\n\n7.32 For $s \\neq 0, \\mathbf{T}=\\left(2|s|^{-1 / 3}, \\sqrt{\\sigma+4 s^{-2 / 3}}\\right)$ and $\\|\\mathbf{T}\\|^{2}=|-\\sigma|=+1$\n\n$7.33 \\quad N^{1}=T^{2}, N^{2}=T^{1}$\n\n7.34 For $s \\neq-8,0$,\n\n7.36\n\n$$\n\\begin{gathered}\n\\kappa=\\frac{-2}{3 s \\sqrt{\\sigma s^{2 / 3}+4}} \\quad \\kappa_{0}=|\\kappa| \\\\\n\\kappa_{0}=|\\kappa|=\\frac{2}{3|s|\\left(s^{2 / 3}-4\\right)^{1 / 2}} \\quad(s \\neq 8)\n\\end{gathered}\n$$\n\nAt the null point $(0,0)$, both Euclidean and Riemannian absolute curvatures become infinite; but at the null point $(12,8)$, only the Riemannian curvature becomes infinite.\n\n$7.37 \\mathbf{T}=|1-4 t|^{-1 / 2}(1,2 t), \\mathbf{N}=|1-4 t|^{-1 / 2}(1,1-2 t), \\kappa=2|1-4 t|^{-3 / 2}$\n\n$7.38 \\quad$ (a) $L=a$. (b) $L=3 a / 2$. (c) Riemannian: $\\mathbf{T}=|\\cos 2 t|^{-1 / 2}(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a)(\\csc 2 t)|\\cos 2 t|^{-3 / 2}$; Euclidean: $\\mathbf{T}=(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a) \\csc 2 t$\n\n7.39 (a) $\\Gamma_{11}^{1}=1 / 2 x^{1}, \\Gamma_{22}^{2}=1 / 2 x^{2}$, others zero\n\n\\section*{CHAPTER 8}\n8.16 By Problem 6.34 and $(8: 1)$,\n\n$$\n\\begin{aligned}\nV_{, k l}^{i}-V_{, l k}^{i} & =g^{i r}\\left(V_{r, k l}-V_{r, l k}\\right)=g^{i r} R_{r k l}^{s} V_{s} \\\\\n& =g^{i r}\\left(g_{s t} R_{r k l}^{s}\\right) V^{t}=\\left(g^{i r} R_{t r k l}\\right) V^{t}=-R_{t k l}^{i} V^{t}\n\\end{aligned}\n$$\n\n8.22 $\\mathrm{K}=1 / 4\\left(x^{1}\\right)^{2}$\n\n8.24 (a) and (b) $\\mathrm{K}=\\frac{x^{1}+x^{2}}{4\\left(x^{1}\\right)^{2} x^{2}\\left(1+2 x^{2}\\right)}$\n\n(c) $\\mathbf{U}_{(2)}=-\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}, \\mathbf{V}_{(2)}=\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}$\n\n8.25 $\\mathrm{K}=1 / a^{2}$\n\n8.26 Basic sets of nonvanishing terms are:\n\n$$\n\\text { (A) } R_{1212}=-\\frac{1}{4}\\left(2 f^{\\prime \\prime}-\\frac{f^{\\prime 2}}{f}-\\frac{f^{\\prime} g^{\\prime}}{g}\\right), R_{1313}=-\\frac{1}{4} \\frac{f^{\\prime} h^{\\prime}}{g}, R_{2323}=-\\frac{1}{4}\\left(2 h^{\\prime \\prime}-\\frac{h^{\\prime 2}}{h}-\\frac{h^{\\prime} g^{\\prime}}{g}\\right)\n$$\n\nand\n\n(A) $G_{1212}=f g, G_{1313}=f h, G_{2323}=g h$\n\nso that\n\n(a) $\\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right)=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{f g W_{1212}+f h W_{1313}+g h W_{2323}}$\n\n(b) $\\quad R=-\\frac{2}{f g h}\\left(h R_{1212}+g R_{1313}+f R_{2323}\\right)$\n\n8.27\n\n(a) $\\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right)=\\frac{-2(\\ln |f|)^{\\prime \\prime}\\left(W_{1212}+W_{2323}\\right)-(\\ln |f|)^{\\prime 2} W_{1313}}{4 f\\left(W_{1212}+W_{1313}+W_{2323}\\right)}$\n\n(b) $R=\\frac{4 f^{\\prime \\prime} f-3 f^{\\prime 2}}{2 f^{3}}$\n\n8.28 Isotropic points compose the surface $x^{2}=e^{-3 / 2}$, over which $\\mathrm{K}=2 e^{3 / 27}$.\n\n8.29 $\\mathrm{K}=-1 / 4$\n\n8.32 $R_{11}=-1, R_{12}=R_{21}=0, R_{22}=\\sin ^{2} x^{1} ; R_{1}^{1}=-1 / a^{2}=R_{2}^{2}, R_{2}^{1}=0=R_{1}^{2} ; R=-2 / a^{2}$\n\n8.33 $\\quad R_{11}=R_{22}=R_{33}=2 /\\left(x^{1}\\right)^{2}$, others $0 ; R_{1}^{1}=R_{2}^{2}=R_{3}^{3}=2$, others $0 ; R=6$\n\n8.35 $g_{i j}=\\left(x^{1}\\right)^{4} \\delta_{i j}$ has $R=0, \\mathrm{~K} \\neq 0$ (use Problem 8.27).\n\n8.36 No implication either way.\n\n\\section*{CHAPTER 9}\n9.17 (a) $u_{0}= \\pm \\sqrt{x^{1} x^{2}+a} \\quad(a=$ const. $) ;(b)$ incompatible\n\n9.20 flat, non-Euclidean\n\n9.21 Euclidean\n\n$9.22(++-)$\n\n9.26 With the notation $f_{i} \\equiv \\partial f / \\partial x^{i}$, for any function $f$ :\n\n$$\n\\begin{aligned}\n& G_{1}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[-\\frac{\\psi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\\\\n& G_{2}^{2}=e^{-\\varphi}\\left(-\\frac{\\psi_{11}}{2}-\\frac{\\psi_{1}^{2}}{4}+\\frac{\\varphi_{1} \\psi_{1}}{4}+\\frac{\\varphi_{1}}{2 x^{1}}-\\frac{\\psi_{i}}{2 x^{1}}\\right)+e^{-\\psi}\\left(\\frac{\\varphi_{44}}{2}+\\frac{\\varphi_{4}^{2}}{4}-\\frac{\\varphi_{4} \\psi_{4}}{4}\\right)=G_{3}^{3} \\\\\n& G_{4}^{4}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[\\frac{\\varphi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\quad G_{4}^{1}=-\\varphi_{4} e^{-\\varphi / x^{1}} \\quad G_{1}^{4}=\\varphi_{4} e^{-\\psi / x^{1}}\n\\end{aligned}\n$$\n\n\\section*{CHAPTER 10}\n10.30 (a) The curve lies on a right circular cylinder of unit radius, beginning at the point $(1,0,1)$ and rising in helix fashion, approaching $\\infty$ asymptotic to the vertical line $x=\\cos 1, y=\\sin 1$, as $t \\rightarrow 1$.\n\n\n\\begin{equation*}\nL=\\int_{0}^{1 / 2} \\frac{\\sqrt{(1-t)^{4}+1}}{(1-t)^{2}} d t \\approx 1.13209039 \\tag{b}\n\\end{equation*}\n\n\n$10.3116 / 3$\n\n10.32 (a) $\\mathbf{T}=(-(a / c) \\sin (s / c),(a / c) \\cos (s / c), b / c)$. Hence the tangent line, $\\mathbf{r}(t) \\equiv \\mathbf{r}+t \\mathbf{T}$, has the coordinate equations\n\n$$\nx=a \\cos \\frac{s}{c}-\\frac{a t}{c} \\sin \\frac{s}{c} \\quad y=a \\sin \\frac{s}{c}+\\frac{a t}{c} \\cos \\frac{s}{c} \\quad z=\\frac{b s}{c}+\\frac{b t}{c}\n$$\n\n(b) $Q$ corresponds to $t=-s$ and $P Q=\\|-s \\mathbf{T}\\|=s$.\n\n(c) The interpretation is that $Q$ can be thought of as the free end of the taut string as it is unwound from the helix. [The locus of $Q, \\mathbf{r}^{*}=\\mathbf{r}(s)-s \\mathbf{r}^{\\prime}(s)$, is called an involute of the helix.]\n\n10.33\n\n$$\n\\begin{gathered}\n\\frac{\\mathbf{T}^{\\prime}}{\\left\\|\\mathbf{T}^{\\prime}\\right\\|}=\\frac{t /|t|}{\\left(1+25 t^{8}\\right)^{1 / 2}}\\left(-5 t^{4}, 1,0\\right) \\\\\n\\kappa=\\frac{20 t^{3} \\sqrt{2}}{\\left(1+50 t^{8}\\right)^{3 / 2}} \\quad \\tau=0\n\\end{gathered}\n$$\n\n10.35 Let the curve $\\mathbf{r}=\\mathbf{r}(s)$ lie in the plane $\\mathbf{b r}=$ const., where $\\mathbf{b}=$ const. and $\\|\\mathbf{b}\\|=1$. Differentiate twice with respect to $s: \\mathbf{b T}=0$ and $\\mathbf{b T}^{\\prime}=0$; hence, $\\mathbf{b}(\\kappa \\mathbf{N})=0$ or $\\mathbf{b N}=0$. It follows that $\\mathbf{b}=\\mathbf{B}$, the binormal vector, so that $\\mathbf{B}^{\\prime}=\\mathbf{0}$ and $\\tau=-\\mathbf{B}^{\\prime} \\mathbf{N}=0$. Conversely, if $\\tau=0$ for a curve $\\mathbf{r}=\\mathbf{r}(s)$, then $\\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}=\\mathbf{0}$ and $\\mathbf{B}$ is a constant unit vector. Define the function $Q(s) \\equiv \\mathbf{B} \\cdot(\\mathbf{r}(s)-\\mathbf{r}(0))$; we have\n\n$$\nQ^{\\prime}=\\mathbf{B r}^{\\prime}=\\mathbf{B} \\mathbf{T}=0\n$$\n\nwhence $Q=$ const. $=Q(0)=0$. Therefore, the curve lies in the plane\n\n$$\n\\mathbf{B r}=\\mathbf{B r}(0)=\\text { const. }\n$$\n\n10.38 $E=\\left|x^{1}\\right| \\sqrt{a^{2}+1}=0$ at $x^{1}=0$.\n\n10.39 $E=a^{2} \\cosh ^{2} x^{1}>0, \\mathbf{n}=\\frac{1}{\\cosh x^{1}}\\left(-\\cos x^{2},-\\sin x^{2}, \\sinh x^{1}\\right)$\n\n10.40\n\n$$\nL=\\int_{1}^{2} \\frac{\\sqrt{5 t^{4}+1}}{t} d t=\\frac{1}{2}\\left[9-\\sqrt{6}+\\ln \\frac{2}{5}(\\sqrt{6}+1)\\right]\n$$\n\n$10.41\\left(v^{1}, v^{2}\\right)=(\\sqrt{12 / 29}, \\sqrt{17 / 29})$ or $(-\\sqrt{12 / 29}, \\sqrt{17 / 29})$\n\n10.43 $\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{2 x^{1}}{\\left(x^{1}\\right)^{2}+a^{2}} ;$ all others zero\n\n10.44\n\n$$\n\\mathrm{II}=\\frac{f^{\\prime} g^{\\prime \\prime}-f^{\\prime \\prime} g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\left(d x^{1}\\right)^{2}+\\frac{f g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\left(d x^{2}\\right)^{2}\n$$\n\n$10.46(a)$\\\\\n(i) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{2}}$\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(x^{1}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{3 / 2}}$\\\\\n(ii) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{2}}$,\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+2 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{3 / 2}}$\n\n(b) Consistent with the invariance of $\\mathrm{K}$ and $\\mathrm{H}$, the change of parameters $\\bar{x}^{1}=x^{1} \\cos x^{2}, \\bar{x}^{2}=$ $x^{1} \\sin x^{2}-$ i.e., the transformation from polar to rectangular coordinates in the parameter planetakes the forms (i) into the forms (ii).\n\n10.49 (a) $\\mathbf{r}^{*}=\\left(a \\operatorname{sech} x^{1}, 0, a x^{1}-a \\tanh x^{1}\\right)$\n\n10.50 The two FFFs correspond under the mapping $\\bar{x}^{1}=a \\sinh x^{1}, \\bar{x}^{2}=x^{2}$.\n\n\\section*{CHAPTER 11}\n\\includegraphics[max width=\\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-229}\\\\\n(b) $\\quad a=\\sqrt{1+4 \\csc ^{4} t \\cot ^{2} t} \\rightarrow 1$;\\\\\n(c) $\\max v=\\sqrt{5}, \\quad \\max a=\\sqrt{17}$\n\n11.15 Rectilinear motion [use (11.8) to show that $\\kappa$ must vanish].\n\n11.16 From (11.7) and (10.9), $\\dot{\\mathbf{a}}=-\\kappa^{2} v^{3} \\mathbf{T}+\\dot{\\kappa} v^{2} \\mathbf{N}+\\kappa \\tau v^{3} \\mathbf{B}$.\n\n$11.17 \\quad a^{1}=\\frac{d^{2} \\rho}{d t^{2}}-\\left(\\rho \\sin ^{2} \\varphi\\right)\\left(\\frac{d \\theta}{d t}\\right)^{2}-\\rho\\left(\\frac{d \\varphi}{d t}\\right)^{2}, \\quad a^{2}=\\frac{d^{2} \\varphi}{d t^{2}}+\\frac{2}{\\rho} \\frac{d \\rho}{d t} \\frac{d \\varphi}{d t}-(\\sin \\varphi \\cos \\varphi)\\left(\\frac{d \\theta}{d t}\\right)^{2}$, $a^{3}=\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{\\rho} \\frac{d \\rho}{d t} \\frac{d \\theta}{d t}+(2 \\cot \\varphi) \\frac{d \\theta}{d t} \\frac{d \\varphi}{d t}$\n\n11.18 Let the center of force be the origin of rectangular coordinates for $\\mathbf{E}^{3}$, with the particle's path given by $\\mathbf{r}=\\mathbf{r}(t)$. By Newton's second law, $f \\mathbf{r}=m \\ddot{\\mathbf{r}}$, so that\n\n$$\n\\frac{d}{d t}(\\mathbf{r} \\times \\dot{\\mathbf{r}})=\\mathbf{r} \\times \\ddot{\\mathbf{r}}=\\mathbf{r} \\times\\left(\\frac{f}{m} \\mathbf{r}\\right)=\\mathbf{0}\n$$\n\nand $\\mathbf{r} \\times \\dot{\\mathbf{r}}=\\mathbf{p}=$ const. It follows that $\\mathbf{p} \\cdot \\mathbf{r}=0$.\n\n$11.19 \\nabla^{2} f=\\frac{\\partial^{2} f}{\\partial r^{2}}+\\frac{1}{r^{2}} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{\\partial^{2} f}{\\partial z^{2}}+\\frac{1}{r} \\frac{\\partial f}{\\partial r}$\n\n\\section*{CHAPTER 12}\n12.34 (a) timelike; (b) spacelike; (c) lightlike\n\n12.35 Yes: travel at $4167 \\mathrm{mi} / \\mathrm{sec} \\ll c$. Timelike interval.\n\n12.36 Premultiply $A^{T} G A=G$ by $A G$, and postmultiply by $A^{-1} G$. 12.38 (a) $t=a_{0}^{0} \\bar{t}, x^{1}=-a_{0}^{1} c \\bar{t}, x^{2}=-a_{0}^{2} c \\bar{c}, x^{3}=-a_{0}^{3} c \\bar{c}$. (b) $a_{0}^{0}>0$ if $t$ and $\\bar{t}$ have the same sign; that is, if the\\\\\nclocks of the two observers are both turning clockwise or both counterclockwise.\n\n$$\n\\bar{x}^{0}=\\frac{5}{3} x^{0}-\\frac{4}{3} x^{1} \\quad \\bar{x}^{1}=-\\frac{4}{3} x^{0}+\\frac{5}{3} x^{1} \\quad \\bar{x}^{2}=x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\n12.40\n\n(b) zero\n\n12.41\n\n$$\n\\begin{aligned}\nL^{*} & =\\left[\\begin{array}{cccc}\n5 / 4 & -3 / 4 & 0 & 0 \\\\\n-3 / 4 & 5 / 4 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\nv & =(3 / 5) c\n\\end{aligned}\n$$\n\n$12.42 v=\\sqrt{\\frac{2}{3}} c$\n\n$12.45 v=(4 / 5) c$\n\n12.47 Approximately $25 \\%$ slow.\n\n12.48 About 45 years old.\n\n$12.49 \\approx 17000 \\mathrm{mi} / \\mathrm{sec}$\n\n12.50 For constants $\\hat{F}$ and $\\hat{\\bar{a}} \\equiv \\hat{F} / m$, and with $\\mathbf{F}=(\\hat{F}, 0,0)$ and $\\mathbf{v}=\\left(v_{x}, 0,0\\right),(12.29)$ becomes identical with (1) of Problem 12.26.\n\n12.52 Since $\\partial \\bar{s}^{i} / \\partial \\bar{x}^{i}=0=\\partial s^{i} / \\partial x^{i}$ (the equation of continuity), $\\left(s^{i}\\right)$ may be identified with the vector $\\left(S^{i}\\right)$ of Problem 12.32.\n\n12.54 (a) By analogy with the evaluation of $\\frac{1}{2} e_{i j k l} P_{k l}$ in Problem 12.53,\n\n$$\n\\left[\\frac{1}{2} e_{i j k l}\\left(-\\Phi_{k l}\\right)\\right]_{44}=\\left[\\begin{array}{cccc}\n0 & -\\Phi_{23} & \\Phi_{13} & -\\Phi_{12} \\\\\n* & 0 & -\\Phi_{03} & \\Phi_{02} \\\\\n* & * & 0 & -\\Phi_{01} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n0 & -H_{1} & -H_{2} & -H_{3} \\\\\n* & 0 & E_{3} & -E_{2} \\\\\n* & * & 0 & E_{1} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[F^{i j}\\right]_{44}\n$$\n\n(b) Let $(a b c d)$ denote a permutation of (0123). Then $\\Phi_{a b}=-e_{a b c d} F^{c d} \\quad$ (no summation) and\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\Phi_{a b}}{\\partial x^{c}}+\\frac{\\partial \\Phi_{c a}}{\\partial x^{b}}+\\frac{\\partial \\Phi_{b c}}{\\partial x^{a}} & =-e_{a b c d} \\frac{\\partial F^{c d}}{\\partial x^{c}}-e_{c a b d} \\frac{\\partial F^{b d}}{\\partial x^{b}}-e_{b c a d} \\frac{\\partial F^{a d}}{\\partial x^{a}} \\\\\n& =-e_{a b c d}\\left(\\frac{\\partial F^{c d}}{\\partial x^{c}}+\\frac{\\partial F^{b d}}{\\partial x^{b}}+\\frac{\\partial F^{a d}}{\\partial x^{d}}\\right)= \\pm \\frac{\\partial F^{j d}}{\\partial x^{j}}=0\n\\end{aligned}\n$$\n\nThe second set of equations is derivable directly from $(12.45 b)$, the definition of $\\Phi$, and the fact that the $g_{i j}$ are constants.\n\n\\section*{CHAPTER 13}\n13.22 Yes; it is isomorphic to the 4-group.\n\n13.26 (a) By (13.6),\n\n$$\n\\bar{T}_{i}^{i}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\mathbf{b}}_{i}\\right)=T\\left(\\bar{A}_{j}^{i} \\boldsymbol{\\beta}^{j}, A_{i}^{k} \\mathbf{b}_{k}\\right)=\\bar{A}_{j}^{i} A_{i}^{k} T\\left(\\boldsymbol{\\beta}^{j}, \\mathbf{b}_{k}\\right)=\\delta_{j}^{k} T_{k}^{j}=T_{j}^{j}\n$$\n\n(b) $\\tau(T)=n$\n\n13.27 Suppose that $\\hat{G}\\left(\\mathbf{u}_{1}\\right)=\\hat{G}\\left(\\mathbf{u}_{2}\\right)$. Then $G\\left(\\mathbf{u}_{1}, \\mathbf{v}\\right)=G\\left(\\mathbf{u}_{2}, \\mathbf{v}\\right)$, or by symmetry, $G\\left(\\mathbf{v}, \\mathbf{u}_{1}\\right)=G\\left(\\mathbf{v}, \\mathbf{u}_{2}\\right)$ for all $\\mathbf{v}$ By nonsingularity, $\\mathbf{u}_{1}=\\mathbf{u}_{2}$.\n\n13.28\n\n$$\n\\begin{aligned}\n& \\varphi_{p}^{-1}: \\begin{cases}y^{1}=\\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{2}=x^{1} & \\\\\ny^{3}=x^{2} & \\mathbf{U}_{p}: y^{1}>0 \\\\\ny^{4}=x^{3} & p=(a, 0,0,0)\\end{cases}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231}\n\\end{center}\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(2)}\n\\end{center}\n\n$$\n\\begin{aligned}\n& \\varphi_{ \\pm r}^{-1}:\\left\\{\\begin{array}{l|l}\ny^{1}=x^{1} & \\mathbf{U}_{r}: y^{3}>0 \\\\\ny^{2}=x^{2} & \\begin{array}{c}\n\\mathbf{U}_{-r}: y^{3}<0 \\\\\nr=(0,0, a, 0)\n\\end{array} \\\\\ny^{3}= \\pm \\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{4}=x^{3}\n\\end{array}\\right.\n\\end{aligned}\n$$\n\n13.30\n\n$$\n\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}+y^{6} d y^{5}-y^{5} d y^{6}+\\cdots+y^{2 k} d y^{2 k-1}-y^{2 k-1} d y^{2 k}\n$$\n\n13.31 If $p_{1}$ and $p_{2}$ are not antipodal, there exists a closed hemisphere containing neither one, on which the given (continuous) vector field is nonzero-an impossibility by Problem 13.17(b).\n\n13.32 As shown in Fig. 13-10, let a unit tangent vector be constructed to a generating circle; as the circle is revolved to generate the torus, the tangent vector is obviously propagated continuously to all points of the torus.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(1)}\n\\end{center}\n\nFig. 13-10\n\n13.33 Zero points are: $(a)(0,0,0,0, \\pm a) ;(b) \\pm(0, a / \\sqrt{3}, 0, a / \\sqrt{3}, a / \\sqrt{3})$.\n\n13.35 With $\\omega \\equiv 2\\left(y^{1} d y^{1}-4 y^{2} d y^{2}+4 y^{3} d y^{3}\\right), \\sigma=f d y^{1}+g d y^{2}+h d y^{3}$ must be orthogonal to $\\omega$, for $C^{\\infty}$ functions $f, g, h$. Hence,\n\n$$\ny^{1} f-4 y^{2} g+4 y^{3} h=0\n$$\n\nReplace $f, g$ by $4 y^{3} F, y^{3} G$, and solve for $h$. Similarly, replace $g$ by $y^{1} G$ and $h$ by $y_{1} H$ and solve for $f$; etc. All possible tangent vectors are given by one of three distinct types $\\left(F, G, H\\right.$ denote arbitrary $C^{\\infty}$ functions of $y^{1}, y^{2}, y^{3}$ ):\n\n(1) $\\sigma=4 y^{3} F d y^{1}+y^{3} G d y^{2}+\\left(y^{2} G-y^{1} F\\right) d y^{3}$\n\n(2) $\\sigma=\\left(4 y^{2} G-4 y^{3} H\\right) d y^{1}+y^{1} G d y^{2}+y^{1} H d y^{3}$\n\n(3) $\\sigma=4 y^{2} F d y^{1}+\\left(y^{1} F+y^{3} H\\right) d y^{2}+y^{2} H d y^{3}$\n\n13.36\n\n$$\nU=U_{1}^{i} \\mathbf{r}_{i} \\equiv\\left(2 U^{1} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}}, 2 U^{2} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}},-x^{1} U^{1}+4 x^{2} U^{2}\\right)\n$$\n\nfor any two $C^{\\infty}$ functions $U^{1}, U^{2}$ on $\\left(x^{1}, x^{2}\\right)$.\n\n13.37 The normal vector to the sphere is represented by $\\sigma=y^{1} d y^{1}+y^{2} d y^{2}+y^{3} d y^{3}$; the tangent space of the cone is given by\n\n$$\n\\left(U_{1}, U_{2},\\left(3 x^{1} U^{1}+3 x^{2} U^{2}\\right)\\left(\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right)^{-1 / 2}\\right)\n$$\n\nSet $U^{1}=y^{1}$ and $U^{2}=y^{2}$.\n\n\n\\end{document}"], "lesson": "\\section*{Chapter 13}\n\\section*{Tensor Fields on Manifolds}\n\\subsection*{13.1 INTRODUCTION}\nThe modern, noncoordinate approach to tensors will be introduced as an important alternative to the coordinate-component approach employed exclusively in the previous chapters. This will entail somewhat more sophisticated mathematics.\n\n\\subsection*{13.2 ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}\nLinear algebra provides a means of systematically studying the algebraic interplay between real numbers (scalars) and a wide variety of different types of objects (vectors). Vectors can be matrices, $n$-tuples of real numbers, functions, differential operators, etc. In this chapter, we shall adopt the convention of using uppercase boldface characters for sets (of points, of real numbers, of elements of a group, etc.), and lowercase boldface for vectors (as in the preceding chapters). However, the latter will be gradually phased out in favor of light uppercase characters, not only for easier reading, but also in conformity with notation used in many standard textbooks.\n\nThe concept of vector spaces requires a careful distinction between the scalars $a, b, c, \\ldots$, and the objects of study (vectors), $\\mathbf{u}, \\mathbf{v}, \\mathbf{w}, \\ldots$ We shall always identify the scalars with the field of real numbers, although any field could serve for the construction of an abstract vector space.\n\n\\section*{Algebraic Properties of a Vector Space}\nIn terms of two binary operations, the axioms for a vector space are as follows.\n\n\\section*{Addition Axioms}\n\\begin{enumerate}\n  \\item $\\mathbf{u}+\\mathbf{v}$ is always a vector\n\n  \\item $\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}$\n\n  \\item $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n\n  \\item There is a vector $\\mathbf{0}$ such that $\\mathbf{u}+\\mathbf{0}=\\mathbf{u}$.\n\n  \\item For each $\\mathbf{u}$ there is a vector $-\\mathbf{u}$ such that $\\mathbf{u}+(-\\mathbf{u})=\\mathbf{0}$.\n\n\\end{enumerate}\n\n\\section*{Scalar Multiplication Axioms}\n\\begin{enumerate}\n  \\setcounter{enumi}{5}\n  \\item $a \\cdot \\mathbf{u} \\equiv a \\mathbf{u}$ is always a vector\n  \\item $a(\\mathbf{u}+\\mathbf{v})=a \\mathbf{u}+a \\mathbf{v}$\n  \\item $(a+b) \\mathbf{u}=a \\mathbf{u}+b \\mathbf{u}$\n  \\item $(a b) \\mathbf{u}=a(b \\mathbf{u})$\n  \\item $1 \\mathbf{u}=\\mathbf{u}$\n\\end{enumerate}\n\nEXAMPLE 13.1 We give notation for four familiar vector spaces.\n\n(a) $\\mathbf{R}^{n} \\equiv$ the $n$-tuples of reals under componentwise addition and scalar multiplication.\n\n(b) $\\quad \\mathbf{P}^{n} \\equiv$ the polynomials (in a variable $t$ ) of degree $n$ or less. If $p(t) \\equiv a_{i} t^{i}, q(t) \\equiv b_{i} t^{i}$, let $p(t)+q(t)=$ $\\left(a_{i}+b_{i}\\right) t^{i}$ and $r \\cdot p(t)=\\left(r a_{i}\\right) t^{i}$.\n\n(c) $\\quad C^{k}(\\mathbf{R}) \\equiv$ the continuously $k$-times differentiable functions (of $t$ ), $f \\vdots \\mathbf{R} \\rightarrow \\mathbf{R}$ (mapping the reals into the reals). To define + and $\\cdot$, write $f(t)+g(t)=(f+g)(t)$ and $r \\cdot f(t)=(r f)(t)$.\n\n(d) $\\mathbf{M}^{n}(\\mathbf{R}) \\equiv$ the $n \\times n$ matrices over $\\mathbf{R}$. If $A=\\left(a_{i j}\\right)$ and $B=\\left(b_{i j}\\right)$, addition and scalar multiplication are defined by $A+B=\\left(a_{i j}+b_{i j}\\right)$ and $r A=\\left(r a_{i j}\\right)$.\n\n\\section*{Algebraic Properties of a Group}\nAxioms 1-5 make a vector space an abelian (commutative) group under addition. In the general definition of a group, the binary operation is designated as \"multiplication\" and the commutative requirement is dropped.\n\n\\section*{Multiplication Axioms}\n\\begin{enumerate}\n  \\item $u v$ belongs to the group.\n\n  \\item $(u v) w=u(v w)$.\n\n  \\item There is an identity element $e$ such that $e u=u e=u$.\n\n  \\item For each $u$ there is an inverse element $u^{-1}$ such that $u u^{-1}=u^{-1} u=e$.\n\n\\end{enumerate}\n\nEXAMPLE 13.2 Some frequently encountered groups follow.\n\n(a) The reals $\\mathbf{R}$ over ordinary addition; the reals over ordinary multiplication if 0 is removed from the set.\n\n(b) The cube roots of unity, $\\mathbf{C}^{3}=\\left\\{1, \\omega, \\omega^{2}\\right\\}$, over ordinary multiplication of complex numbers, where $\\omega=\\frac{1}{2}\\left(-1+i \\sqrt{3}\\right.$ ). Groups of this type are called cyclic and are generally denoted by $\\mathbf{C}^{k}$ (the cyclic group of order $k$ ). $\\mathbf{C}^{k}$ is necessarily abelian.\n\n(c) The 4-group $\\{e, u, s, b\\}$, under the rules $u^{2}=s^{2}=b^{2}=e, b=u s$, and the associative law of multiplication. The 4-group is abelian, but it is not equivalent to the cyclic group on four elements, $\\mathbf{C}^{4}$.\n\n(d) $\\mathbf{M}^{n}(\\mathbf{R})$, under matrix addition.\n\n(e) $\\mathbf{G L}(n, \\mathbf{R}) \\equiv$ the real, nonsingular $n \\times n$ matrices under matrix multiplication; this is the general linear group (nonabelian). GL $(n, \\mathbf{R})$ contains many very important smaller groups (called subgroups). Some of these are: $\\mathbf{S L}(n, \\mathbf{R}) \\equiv$ the real $n \\times n$ matrices with determinant $+1 ; \\mathbf{S O}(n) \\equiv$ the $n \\times n$ orthogonal matrices; and $\\mathbf{L}(n) \\equiv$ the $n \\times n$ Lorentz matrices [see the definition of $\\mathbf{L}(4)$ in Section 12.3].\n\n(f) $\\mathbf{G L}(n, \\mathbf{C}) \\equiv$ the complex, nonsingular $n \\times n$ matrices under matrix multiplication. An important subgroup is the unitary group, $\\mathbf{U}(n)$, consisting of all $n \\times n$ Hermitian matrices (such that $A^{-1}=\\bar{A}^{T}$, where the bar denotes complex conjugation).\n\n\\subsection*{13.3 IMPORTANT CONCEPTS FOR VECTOR SPACES}\nBasis\n\nA basis for a vector space is a maximal, linearly independent set of vectors $\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots$ If this set is finite, possessing $n$ elements, the vector space is finite-dimensional, of dimension $n$. Otherwise, the space is said to be infinite-dimensional.\n\nEXAMPLE 13.3 (1) It is obvious that a basis for $\\mathbf{R}^{n}$ is the set of vectors\n\n$$\n\\mathbf{e}_{1}=(1,0,0, \\ldots, 0), \\quad \\mathbf{e}_{2}=(0,1,0, \\ldots, 0), \\ldots, \\quad \\mathbf{e}_{n}=(0,0, \\ldots, 0,1)\n$$\n\ncalled the standard basis. (2) $\\mathbf{P}^{n}$ is finite-dimensional, of dimension $n+1$; one basis is $\\left\\{t^{i}\\right\\}, 0 \\leqq i \\leqq n$. (3) The vector space of all polynomials is infinite-dimensional, as is the vector space $C^{k}(\\mathbf{R})$. See Problem 13.4.\n\n\\section*{Isomorphisms, Linear Mappings}\nTwo mathematical systems of the same type (such as two vector spaces or two groups) are called isomorphic if they are structurally identical and differ only in nomenclature. In the case of two vector spaces, an isomorphism is a one-to-one (bijective) linear mapping $\\varphi$ from one space to the other, where the term linear refers to the properties (for all vectors $\\mathbf{u}, \\mathbf{v}$, and scalars $a$ ):\n\n\n\\begin{equation*}\n\\varphi(\\mathbf{u}+\\mathbf{v})=\\varphi(\\mathbf{u})+\\varphi(\\mathbf{v}) \\quad \\text { and } \\quad \\varphi(a \\mathbf{u})=a \\varphi(\\mathbf{u}) \\tag{13.1}\n\\end{equation*}\n\n\nFor groups, an isomorphism would be a bijection $\\psi$ with the property $\\psi(u v)=\\psi(u) \\psi(v)$, for all elements of the group. A more general mapping that is important for groups is a homomorphism, which merely requires that $\\psi(u v)=\\psi(u) \\psi(v)$ for all $u$ and $v$, without necessarily requiring one-tooneness.\n\n\\section*{Product of Vector Spaces}\nIf $\\mathbf{U}$ and $\\mathbf{V}$ are any two vector spaces, the ordinary cartesian product $\\mathbf{U} \\times \\mathbf{V}$, the set of ordered pairs $(\\mathbf{u}, \\mathbf{v})$ with $\\mathbf{u}$ in $\\mathbf{U}$ and $\\mathbf{v}$ in $\\mathbf{V}$, can be made into a vector space by defining addition and scalar multiplication of pairs via\n\n$$\n(\\mathbf{p}, \\mathbf{q})+(\\mathbf{r}, \\mathbf{s})=(\\mathbf{p}+\\mathbf{r}, \\mathbf{q}+\\mathbf{s}) \\quad \\text { and } \\quad a(\\mathbf{p}, \\mathbf{q})=(a \\mathbf{p}, a \\mathbf{q})\n$$\n\nSuch a product space is denoted $\\mathbf{U} \\otimes \\mathbf{V}$; if $\\mathbf{U}=\\mathbf{V}$, write $\\mathbf{U} \\otimes \\mathbf{V}$ as $\\mathbf{U}^{2}$. More generally, the product of any number of vector spaces $\\mathbf{V}_{1}, \\mathbf{V}_{2}, \\ldots, \\mathbf{V}_{k}$ may be easily defined as above; this product is denoted $\\mathbf{V}_{1} \\otimes \\mathbf{V}_{2} \\otimes \\mathbf{V}_{3} \\otimes \\cdots \\otimes \\mathbf{V}_{k}$. If $\\mathbf{V}_{1}=\\mathbf{V}_{2}=\\cdots=\\mathbf{V}_{k}=\\mathbf{V}$, the product space is written $\\mathbf{V}^{k}$. (This notation is also often used for the tensor product of two vector spaces, a concept which will not be treated here.)\n\n\\subsection*{13.4 THE ALGEBRAIC DUAL OF A VECTOR SPACE}\nIf a vector space $\\mathbf{V}$ be mapped linearly into the reals $\\mathbf{R}$, satisfying (13.1), the mapping is called a linear functional, or one-form. As in Example 13.1(c), we can make the set of all linear functionals on $\\mathbf{V}$ into a vector space itself, with the zero functional as that mapping which takes every vector in $\\mathbf{V}$ into 0 in $\\mathbf{R}$.\n\nDefinition 1: The algebraic dual of a vector space $\\mathbf{V}$ is the set $\\mathbf{V}^{*}$ of all linear functionals made into a vector space under ordinary pointwise addition and scalar multiplication:\n\n$$\n(f+g)(\\mathbf{v})=f(\\mathbf{v})+g(\\mathbf{v}) \\quad(\\lambda f)(\\mathbf{v})=\\lambda f(\\mathbf{v})\n$$\n\nSince any linear functional on $\\mathbf{R}^{n}$ can be expressed as a linear function of the coordinates,\n\n$$\n\\mathbf{v}=v^{1} \\mathbf{e}_{1}+v^{2} \\mathbf{e}_{2}+\\cdots+v^{n} \\mathbf{e}_{n} \\rightarrow f(\\mathbf{v})=a_{1} v^{1}+a_{2} v^{2}+\\cdots+a_{n} v^{n}\n$$\n\nwhere $a_{i}=f\\left(\\mathbf{e}_{i}\\right)$ for each $i$, the functional is completely determined by the $n$-tuple $\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right)$.\n\n\\section*{Differential Notation: One-Forms}\nThus, different functionals correspond to different $n$-tuples, as\n\n$$\nf \\leftrightarrow\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right) \\quad g \\leftrightarrow\\left(b_{1}, b_{2}, \\ldots, b_{n}\\right) \\quad \\ldots\n$$\n\nand it has become customary to represent linear functionals by the compact notation of one-forms:\n\n$$\n\\boldsymbol{\\omega}=a_{1} d x^{1}+a_{2} d x^{2}+\\cdots+a_{n} d x^{n} \\quad \\boldsymbol{\\sigma}=b_{1} d x^{1}+b^{2} d x^{2}+\\cdots+b_{n} d x^{n} \\quad \\cdots\n$$\n\nBut why $d x^{i}$ for the coordinates? The motivation comes from differential geometry. Recall that any class $C^{1}$ multivariate function $F\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ on $\\mathbf{R}^{n}$ has the gradient $\\nabla F=\\left(\\partial F / \\partial x^{i}\\right)$ and the directional derivative (in the direction $\\left(d x^{1}, d x^{2}, \\ldots, d x^{n}\\right)$ )\n\n$$\nd F=\\frac{\\partial F}{\\partial x^{1}} d x^{1}+\\frac{\\partial F}{\\partial x^{2}} d x^{2}+\\cdots+\\frac{\\partial F}{\\partial x^{n}} d x^{n}\n$$\n\nwhich, at a specific point in space, is a one-form that defines a linear functional on $\\mathbf{R}^{n}$ (i.e., the set of all directions). Recall too that, just as in ordinary one-dimensional calculus,\n\n$$\nd x=\\Delta x \\equiv \\text { an unspecified real number }\n$$\n\nnot necessarily small.\n\nEXAMPLE 13.4 (a) In $\\mathbf{R}^{3}$, find the image of $\\mathbf{v}=(1,3,5)$ under the one-forms (linear functionals)\n\n$$\n\\boldsymbol{\\omega}=4 d x^{1}-d x^{2} \\quad \\boldsymbol{\\sigma}=2 d x^{1}+3 d x^{2}-d x^{3} \\quad \\boldsymbol{\\omega}+\\boldsymbol{\\sigma}=6 d x^{1}+2 d x^{2}-d x^{3}\n$$\n\n(b) What is the relationship among $\\boldsymbol{\\omega}(\\mathbf{v}), \\boldsymbol{\\sigma}(\\mathbf{v})$, and $(\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v})$ ?\\\\\n(a)\n\n\n\\begin{align*}\n& \\boldsymbol{\\omega}(\\mathbf{v})=4 \\cdot 1-1 \\cdot 3+0 \\cdot 5=4-3+0=1 \\\\\n& \\boldsymbol{\\sigma}(\\mathbf{v})=2 \\cdot 1+3 \\cdot 3-1 \\cdot 5=2+9-5=6 \\\\\n& (\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v})=6 \\cdot 1+2 \\cdot 3-1 \\cdot 5=6+6-5=7 \\\\\n& \\quad \\boldsymbol{\\omega}(\\mathbf{v})+\\boldsymbol{\\sigma}(\\mathbf{v})=1+6=7=(\\boldsymbol{\\omega}+\\boldsymbol{\\sigma})(\\mathbf{v}) \\tag{b}\n\\end{align*}\n\n\nFor vector spaces different from $\\mathbf{R}^{n}$ we agree to use the procedure of Example 13.4 on the components of vectors relative to an arbitrary basis. That is, to evaluate the image of $\\mathbf{v}=$ $v^{1} \\mathbf{b}_{1}+v^{2} \\mathbf{b}_{2}+\\cdots+v^{n} \\mathbf{b}_{n} \\equiv v^{i} \\mathbf{b}_{i}$ under the one-form $\\boldsymbol{\\omega}=a_{i} d x^{i}$, simply write\n\n\n\\begin{equation*}\n\\boldsymbol{\\omega}(\\mathbf{v})=\\boldsymbol{\\omega}\\left(v^{j} \\mathbf{b}_{j}\\right) \\equiv\\left(a_{i} d x^{i}\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=a_{i} v^{i} \\tag{13.2}\n\\end{equation*}\n\n\nA dual reading of (13.2) gives a better understanding of the relationship between $\\mathbf{V}$ and $\\mathbf{V}^{*}$ (between vectors and one-forms). If we regard the $a_{i}$ as fixed (tantamount to fixing a basis in $\\mathbf{V}$ ) while the $v^{i}$ vary-the \"normal\" situation-then a linear map from $\\mathbf{V}$ to $\\mathbf{R}$ is uniquely defined. If, on the other hand, the vector components $v^{i}$ are held fixed and the coefficients $a_{i}$ are allowed to vary (this amounts to fixing a basis in $\\mathbf{V}^{*}$ ), a linear map from $\\mathbf{V}^{*}$ to $\\mathbf{R}$ is defined (the latter map is actually an element of the space $\\mathrm{V}^{* *}$ ). The expression $a_{i} v^{i}$ is bilinear in the two vector variables $\\mathrm{v}$ and $\\omega$.\n\nTheorem 13.1: If $\\mathbf{V}$ is a finite-dimensional vector space, then $\\mathrm{V}^{*}$ is finite-dimensional, of the same dimension, and is isomorphic to $\\mathbf{V}$.\n\nA proof is given in Problem 13.6.\n\n\\section*{Dual Basis}\n$A$ basis $\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots, \\mathbf{b}_{n}$ for $\\mathbf{V}$ determines one for the dual space $\\mathbf{V}^{*}$ in a very natural way. Each $\\mathbf{v}$ in $\\mathbf{V}$ has a representation $\\mathbf{v}=v^{j} \\mathbf{b}_{j}$ and thus defines a linear functional\n\n\n\\begin{equation*}\n\\varphi(\\mathbf{v})=v^{1} d x^{1}+v^{2} d x^{2}+\\cdots+v^{n} d x^{n} \\tag{13.3}\n\\end{equation*}\n\n\nThen the $n$ linear functionals (vectors in $\\mathbf{V}^{*}$ ) defined by\n\n\n\\begin{equation*}\n\\varphi\\left(\\mathbf{b}_{i}\\right) \\equiv \\boldsymbol{\\beta}^{i} \\quad(i=1,2, \\ldots, n) \\tag{13.4a}\n\\end{equation*}\n\n\nform a basis for $\\mathbf{V}^{*}$ (see Problem 13.6); we say that the basis $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ in $\\mathbf{V}^{*}$ is the dual of the basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ in $\\mathbf{V}$. The evaluation rule (13.2) provides a simpler characterization of the dual basis:\n\n\n\\begin{equation*}\n\\boldsymbol{\\beta}^{i}(\\mathbf{v})=\\left(\\varphi\\left(\\mathbf{b}_{i}\\right)\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=\\left(0 \\cdot d x^{1}+0 \\cdot d x^{2}+\\cdots+1 \\cdot d x^{i}+\\cdots+0 \\cdot d x^{n}\\right)\\left(v^{j} \\mathbf{b}_{j}\\right)=v^{i} \\tag{13.4b}\n\\end{equation*}\n\n\nThus, $\\boldsymbol{\\beta}^{i}=d x^{i}$ is the linear functional that picks out the $i$ th component relative to $\\left\\{\\mathbf{b}_{k}\\right\\}$ of any vector in V. A special application of (13.4b) gives\n\n\n\\begin{equation*}\n\\boldsymbol{\\beta}^{i}\\left(\\mathbf{b}_{j}\\right)=\\delta_{j}^{i} \\tag{13.5}\n\\end{equation*}\n\n\nfor all $i, j$.\n\nEXAMPLE 13.5 The standard basis $\\mathbf{e}=\\left\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{n}\\right\\}$ for $\\mathbf{R}^{n}$ generates the standard basis for $\\left(\\mathbf{R}^{n}\\right)^{*}$, given in terms of one-forms as\n\n$$\n\\boldsymbol{\\beta}^{1}(\\mathbf{e})=d x^{1} \\quad \\boldsymbol{\\beta}^{2}(\\mathbf{e})=d x^{2} \\quad \\ldots \\quad \\boldsymbol{\\beta}^{n}(\\mathbf{e})=d x^{n}\n$$\n\nSuppose, then, that $\\mathbf{R}^{3}$ is referred to the (nonstandard) basis\n\n$$\nb_{1}=(1,1,0) \\quad b_{2}=(1,0,1) \\quad b_{3}=(0,1,1)\n$$\n\nThis may be written in terms of the standard basis $\\left\\{\\mathbf{e}_{i}\\right\\}$ through a formal matrix multiplication:\n\n$$\n\\left[\\begin{array}{l}\n\\mathbf{b}_{1} \\\\\n\\mathbf{b}_{2} \\\\\n\\mathbf{b}_{3}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathbf{e}_{1} \\\\\n\\mathbf{e}_{2} \\\\\n\\mathbf{e}_{3}\n\\end{array}\\right]\n$$\n\nExpress the dual basis $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ for $\\left(\\mathbf{R}^{3}\\right)^{*}$ in terms of its standard basis $\\left(d x^{i}\\right)$ as a similar matrix product.\n\nLet $\\boldsymbol{\\beta}^{i}=a_{1}^{i} d x^{1}+a_{2}^{i} d x^{2}+a_{3}^{i} d x^{3}$; we must solve for the components $a_{j}^{i}$. For $i=1$, we have from (13.5):\n\n$$\n\\begin{array}{r}\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{1}\\right)=\\boldsymbol{\\beta}^{1}(1,1,0)=a_{1}^{1} \\cdot 1+a_{2}^{1} \\cdot 1+a_{3}^{1} \\cdot 0=a_{1}^{1}+a_{2}^{1} \\equiv x+y=1 \\\\\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{2}\\right)=\\boldsymbol{\\beta}^{1}(1,0,1)=x \\cdot 1+y \\cdot 0+z \\cdot 1=x+z=0 \\\\\n\\boldsymbol{\\beta}^{1}\\left(\\mathbf{b}_{3}\\right)=\\boldsymbol{\\beta}^{1}(0,1,1)=x \\cdot 0+y \\cdot 1+z \\cdot 1=y+z=0\n\\end{array}\n$$\n\n(where $x \\equiv a_{1}^{1}, y \\equiv a_{2}^{1}, z \\equiv a_{3}^{1}$ ). Solving, $x=\\frac{1}{2}=y, z=-\\frac{1}{2}$. A similar analysis may be used to determine the $a_{j}^{2}$ and $a_{j}^{3}$. The final result is\n\n$$\n\\begin{aligned}\n& \\boldsymbol{\\beta}^{1}=\\frac{1}{2} d x^{1}+\\frac{1}{2} d x^{2}-\\frac{1}{2} d x^{3} \\\\\n& \\boldsymbol{\\beta}^{2}=\\frac{1}{2} d x^{1}-\\frac{1}{2} d x^{2}+\\frac{1}{2} d x^{3} \\\\\n& \\boldsymbol{\\beta}^{3}=-\\frac{1}{2} d x^{1}+\\frac{1}{2} d x^{2}+\\frac{1}{2} d x^{3}\n\\end{aligned} \\quad \\text { or } \\quad\\left[\\begin{array}{l}\n\\boldsymbol{\\beta}^{1} \\\\\n\\boldsymbol{\\beta}^{2} \\\\\n\\boldsymbol{\\beta}^{3}\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n\\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nd x^{1} \\\\\nd x^{2} \\\\\nd x^{3}\n\\end{array}\\right]\n$$\n\nObserve that the two basis-connecting matrices are formal inverses of each other.\n\n\\section*{Change of Basis in $\\mathbf{V}$ and $\\mathbf{V}^{*}$}\nThe result of Example 13.5 can be generalized. Let $\\left\\{\\mathbf{b}_{i}\\right\\}$ and $\\left\\{\\overline{\\mathbf{b}}_{i}\\right\\}$ be two bases for $\\mathbf{V}$, and let $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ and $\\left\\{\\overline{\\boldsymbol{\\beta}}^{i}\\right\\}$ be the respective dual bases for $\\mathbf{V}^{*}$. Then\n\n\n\\begin{equation*}\n\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j} \\rightarrow \\overline{\\boldsymbol{\\beta}}^{i}=\\bar{A}_{j}^{i} \\boldsymbol{\\beta}^{j} \\quad \\text { with } \\quad\\left(\\bar{A}_{j}^{i}\\right)=\\left(A_{j}^{i}\\right)^{-1} \\tag{13.6}\n\\end{equation*}\n\n\nthe arrow denoting implication. (See Problem 13.7.)\n\n\\subsection*{13.5 TENSORS ON VECTOR SPACES}\nThe concept of a multilinear functional is needed: If $f\\left(\\mathbf{v}^{1}, \\mathbf{v}^{2}, \\ldots, \\mathbf{v}^{m}\\right)$ represents a mapping of $m$ vector variables into the reals such that the restricted mapping obtained by holding all but one of the variables fixed is a linear functional, then $f$ is said to be multilinear in all its variables.\n\nDefinition 2: A type- $\\left(\\begin{array}{c}p \\\\ q\\end{array}\\right)$ tensor is any multilinear functional $T:\\left(\\mathbf{V}^{*}\\right)^{p} \\otimes \\mathbf{V}^{q} \\rightarrow \\mathbf{R}$ mapping $p$ one-forms and $q$ vectors into the reals; the real image is denoted\n\n$$\nT\\left(\\boldsymbol{\\omega}^{1}, \\ldots, \\boldsymbol{\\omega}^{p} ; \\mathbf{v}^{1}, \\ldots, \\mathbf{v}^{q}\\right)\n$$\n\nEXAMPLE 13.6 Let $T$ represent a linear functional in what follows. A type-(1) tensor takes on real values $T(\\boldsymbol{\\omega})$ for all one-forms $\\boldsymbol{\\omega}$ as argument. As we shall see later, such a tensor can be identified with a contravariant vector. A type- $\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$ tensor takes on real values $T(\\mathbf{v})$ for all vectors $\\mathbf{v}$ as argument; it can be shown to correspond to a covariant vector. A type- $\\left(\\begin{array}{c}1 \\\\ 1\\end{array}\\right)$ tensor takes on real values $U(\\boldsymbol{\\omega} ; \\mathbf{v})$ for all ordered pairs in $\\mathbf{V}^{*} \\otimes \\mathbf{V}$ as argument, with $U$ a bilinear functional.\n\nEXAMPLE 13.7 For $n$-dimensional vectors, the ordinary scalar product $\\mathbf{u} \\cdot \\mathbf{v} \\equiv \\mathbf{u v}$ defines a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor, in the form $G(\\mathbf{u}, \\mathbf{v})=\\mathbf{u v}$, since the elementary properties of the scalar product make $G$ a bilinear mapping of a vector pair into the reals. More generally, an inner product defined arbitrarily by the quadratic form\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=\\mathbf{u}^{T} E \\mathbf{v}\n$$\n\nwhere $E$ is an $n \\times n$ matrix, defines a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor.\n\nDefinition 3: A type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor $G(\\mathbf{u}, \\mathbf{v})$ is (i) symmetric if, for every $\\mathbf{u}$ and $\\mathbf{v}$,\n\n(ii) nonsingular if\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=G(\\mathbf{v}, \\mathbf{u})\n$$\n\n$$\n[G(\\mathbf{u}, \\mathbf{v})=0, \\text { identically in } \\mathbf{u}] \\rightarrow \\mathbf{v}=\\mathbf{0}\n$$\n\nand (iii) positive definite if, for any nonzero vector $\\mathbf{u}$,\n\n$$\nG(\\mathbf{u}, \\mathbf{u})>0\n$$\n\nA type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor that is symmetric and nonsingular is called a metric tensor. (A positive definite tensor is necessarily nonsingular.)\n\nEXAMPLE 13.8 Let $C=\\left[C_{j}^{i}\\right]_{n n}$ be a square matrix and let $\\left(a_{i}\\right)$ and $\\left(v^{i}\\right)$ be the respective components of $\\omega$ and $\\mathbf{v}$ relative to the standard basis in $\\mathbf{R}^{n}$ and its dual. Then the matrix product\n\n$$\nT(\\boldsymbol{\\omega} ; \\mathbf{v})=\\boldsymbol{\\omega} C \\mathbf{v} \\equiv a_{i} C_{j}^{i} v^{j} \\quad \\text { (a bilinear form) }\n$$\n\ndefines a type- $\\left(\\begin{array}{l}1 \\\\ 1\\end{array}\\right)$ tensor over the vector space $\\mathbf{R}^{n}$.\n\n\\section*{Tensor Components}\nIn the three types of tensors considered in Examples 13.6-13.8, we may define tensor components in the following manner, which may be generalized to arbitrary tensors in an obvious way. Let $\\mathbf{b}_{1}, \\ldots, \\mathbf{b}_{n}$ be a basis for $\\mathbf{V}$, and $\\boldsymbol{\\beta}^{1}, \\ldots, \\boldsymbol{\\beta}^{n}$ its dual in $\\mathbf{V}^{*}$. Then, for each $i$, write\n\n$$\n\\begin{array}{ll}\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n\\mathbf{0}\n\\end{array}\\right) & T^{i}=T\\left(\\boldsymbol{\\beta}^{i}\\right) \\\\\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{0} \\\\\n1\n\\end{array}\\right) & T_{i}=T\\left(\\mathbf{b}_{i}\\right) \\\\\n\\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n1\n\\end{array}\\right) & T_{j}^{i}=T\\left(\\boldsymbol{\\beta}^{i} ; \\mathbf{b}_{j}\\right)\n\\end{array}\n$$\n\nEXAMPLE 13.9 Find the components, relative to the standard basis for $\\mathbf{V}=\\mathbf{R}^{n}$, of a type- $\\left(\\begin{array}{l}1 \\\\ 1\\end{array}\\right)$ tensor on $\\mathbf{V}$ constructed by the recipe of Example 13.8.\n\nBy construction, $T(\\boldsymbol{\\omega} ; \\mathbf{v})=a_{i} C_{j}^{i} v^{j}$, for all $\\boldsymbol{\\omega}$ and $\\mathbf{v}$. Substituting $\\boldsymbol{\\omega}=\\boldsymbol{\\beta}^{p}=d x^{p}$ and $\\mathbf{v}=\\mathbf{b}=\\mathbf{e}_{q}$, we find\n\n$$\nT_{q}^{p} \\equiv T\\left(d x^{p} ; \\mathbf{e}_{q}\\right)=\\delta_{i}^{p} C_{j}^{i} \\delta_{q}^{j}=C_{q}^{p}\n$$\n\nThus, the components of $T$ are independent of those of the arguments, $\\boldsymbol{\\omega}$ and $\\mathbf{v}$, and depend only on the components of the matrix $C$.\n\n\\section*{Effect of Change of Basis on Tensor Components}\nUnder a change of basis, (13.6),\n\n$$\n\\begin{aligned}\n& \\text { type }\\left(\\begin{array}{l}\n\\mathbf{1} \\\\\n\\mathbf{0}\n\\end{array}\\right) \\quad \\bar{T}^{i}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}\\right)=T\\left(\\bar{A}_{r}^{i} \\boldsymbol{\\beta}^{r}\\right)=\\bar{A}_{r}^{i} T\\left(\\boldsymbol{\\beta}^{r}\\right)=T^{r} \\bar{A}_{r}^{i} \\\\\n& \\text { type }\\left(\\begin{array}{l}\n0 \\\\\n\\mathbf{1}\n\\end{array}\\right) \\quad \\bar{T}_{i}=T\\left(\\overline{\\mathbf{b}}_{i}\\right)=T\\left(A_{i}^{r} \\mathbf{b}_{r}\\right)=A_{i}^{r} T\\left(\\mathbf{b}_{r}\\right)=T_{r} A_{i}^{r}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-203}\n\\end{center}\n\nEXAMPLE 13.10 If $\\mathbf{V}$ is Euclidean $\\mathbf{R}^{n}$, the change of basis $\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j}$ induces the change of coordinates $x^{i}=A_{j}^{i} \\bar{x}^{j}$, for which\n\n$$\n\\bar{J} \\equiv\\left(\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right)=A \\quad \\text { and so } \\quad J=\\bar{J}^{-1}=\\bar{A}\n$$\n\nThe above transformation formulas then reduce to the classical laws for affine tensors-compare (3.21).\n\n\\subsection*{13.6 THEORY OF MANIFOLDS}\nA manifold is the natural extension of a surface to higher dimensions, and also to spaces more general than $\\mathbf{R}^{n}$. It is helpful at first to think of a manifold as just a hypersurface in $\\mathbf{R}^{n}$.\n\nBy the term neighborhood of a point we shall understand either the set of all points in $\\mathbf{R}^{n}$ within some fixed distance from the given point, or any set containing these points. A neighborhood of $p$ will be denoted $\\mathbf{U}_{p}$. If the concept used for distance in $\\mathbf{R}^{m}$ is Euclidean, then every neighborhood $\\mathbf{U}_{p}$ contains a solid, spherical ball (or \"hyperball,\" if $n>3$ ), having some positive radius and centered at $p$. A neighborhood of a point $p$ in a set is the intersection of a neighborhood $\\mathbf{U}_{p}$ and the set. A set is open if each of its points has a neighborhood completely composed of points of the set. An open neighborhood is simply a neighborhood that is also an open set (in the case of a solid-ball neighborhood, the outer boundary of the ball would have to be removed in order to make it an open neighborhood).\n\n\\section*{Descriptive Definition of a Manifold}\nA manifold is a set which has the property that each point can serve as the origin of local coordinates that are valid in an open neighborhood of the point, which neighborhood is an exact \"copy\" of an open neighborhood of a point in $\\mathbf{R}^{n}$. Though such a definition allows the manifold to lie in a metric space, topological space, Banach space, or other abstract mathematical system, it is best that we begin with manifolds in a simpler space, like $\\mathbf{R}^{m}$. Accordingly:\n\nDefinition 4: A manifold is any set $\\mathbf{M}$ in $\\mathbf{R}^{m}$ which has the property that for each point $p$ in the manifold there exists an open neighborhood $\\mathbf{U}_{p}$ in $\\mathbf{M}$ and a mapping $\\varphi_{p}$ which carries $\\mathbf{U}_{p}$ into a neighborhood in $\\mathbf{R}^{n}$. The mapping is required to be a homeomorphism; i.e.\n\n(1) $\\varphi_{p}$ is continuous.\n\n(2) $\\varphi_{p}$ is bijective from $\\mathbf{U}_{p}$ onto its range, $\\varphi_{p}\\left(\\mathbf{U}_{p}\\right)$.\n\n(3) $\\varphi_{p}^{-1}$ is continuous.\n\nSee Fig. 13-1.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-204}\n\\end{center}\n\nFig. 13-1\n\n\\section*{Coordinate Patches, Atlas}\nThe neighborhoods $\\mathbf{U}_{p}$ for $p$ in $\\mathbf{M}$ provide a means for locally ascribing coordinates to $\\mathbf{M}$ which have the correct dimension (e.g., a plane lying in 3-space is actually 2-dimensional and, as a manifold, has a coordinatization by pairs of reals instead of triples, as in Section 10.4). For any point $p$ in $\\mathbf{M}$, the pair $\\left(\\mathbf{U}_{p}, \\varphi_{p}\\right)$ is called a coordinate patch (also chart, or local coordinatization) for $\\mathbf{M}$, while any collection of such pairs for which the neighborhoods $\\mathbf{U}_{p}$ together cover $\\mathbf{M}$ is called an atlas for $\\mathbf{M}$. Since the coordinate patches make $\\mathbf{M} n$-dimensional at each point, $\\mathbf{M}$ is sometimes referred to as an $n$-manifold.\n\nOften a finite number of charts is sufficient for an atlas (Example 13.11). It can be proved that if a manifold in $\\mathbf{R}^{m}$ is closed, and bounded in terms of the distance in $\\mathbf{R}^{m}$, a finite number of charts will always be sufficient.\n\n\\section*{EXAMPLE 13.11}\n(a) The 2-sphere, denoted $\\mathbf{S}^{2}$, is the ordinary sphere in 3-dimensional space $\\left(y^{i}\\right)$, centered at $(0,0,0)$ having radius $a$. It may be coordinatized by an atlas of only two charts, as follows. [Note that the usual spherical coordinates $(\\varphi, \\theta)$ fail to give a one-one mapping at the poles, where $\\theta$ is indeterminate.]\n\n$$\n\\begin{aligned}\n& y^{1}=\\frac{2 a^{2} x^{1}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\\\\n& y^{2}=\\frac{2 a^{2} x^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\\\\n& y^{3}=\\varepsilon a \\frac{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}-a^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\quad(\\varepsilon= \\pm 1)\n\\end{aligned}\n$$\n\nAs illustrated in Fig. 13-2, the chart corresponding to $\\varepsilon=+1$ has $\\mathbf{U}_{p}$ centered on the south pole (whose coordinates are $x^{1}=x^{2}=0$ ) and including every point of the sphere except the north pole. The other chart $(\\varepsilon=-1)$ is the mirror image of the first chart in the equatorial plane. For a derivation of this atlas, see Problem 13.15.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205}\n\\end{center}\n\nFig. 13-2\n\n(b) The $n$-sphere $\\mathbf{S}^{n}$ in $\\mathbf{R}^{n+1}$ may be defined as the set of points $\\left(y^{i}\\right)$ in $\\mathbf{R}^{n+1}$ such that\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}+\\cdots+\\left(y^{n+1}\\right)^{2}=a^{2}\n$$\n\n(centered at $(0,0,0, \\ldots, 0)$, radius $a)$. A coordinate patch for a neighborhood of $(0,0, \\ldots, a)$ is:\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad \\cdots \\quad y^{n}=x^{n} \\quad y^{n+1}=\\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}-\\cdots-\\left(x^{n}\\right)^{2}}\n$$\n\nwhere the mapping is into the $n$-dimensional neighborhood $\\left(x^{1}\\right)^{2}+\\cdots+\\left(x^{n}\\right)<a^{2} \\quad$ (the interior of $\\mathbf{S}^{n-1}$ ). Establishing the analogous patches around the other \"diametrically opposite\" endpoints, we obtain an atlas of $2 n+2$ charts. (A smaller atlas requires a more clever approach.)\n\n\\section*{Differentiable Manifolds}\nInevitably, there will exist pairs $\\left(\\mathbf{U}_{p}, \\varphi_{p}\\right)$ and $\\left(\\mathbf{U}_{q}, \\varphi_{q}\\right)$ whose neighborhoods overlap in $\\mathbf{M}$ (Fig. 13-3); so the common region $\\mathbf{U}_{p} \\cap \\mathbf{U}_{q} \\equiv \\mathbf{W}$, called an overlapping set, generates a map $\\varphi$ between the images of $\\mathbf{W}$ under $\\varphi_{p}$ and $\\varphi_{q}$. Explicitly (trace the circuit in Fig. 13-3), $\\varphi=\\varphi_{q}{ }^{\\circ} \\varphi_{p}^{-1}$.\n\nIt is clear that $\\varphi$ and $\\varphi^{-1}$ are both continuous. If $\\varphi$ and $\\varphi^{-1}$ are of class $C^{k}$ (have continuous partial derivatives of order $k$ at each point) then the overlapping set $\\mathbf{W}$ is said to be of class $C^{k}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205(1)}\n\\end{center}\n\nFig. 13-3\n\nDefinition 5: A differentiable manifold is a manifold which possesses an atlas such that all overlapping sets are of class $C^{1}$. A $C^{k}\\left(C^{\\infty}\\right.$ or $\\left.C^{\\omega}\\right)$ manifold has an atlas whose overlapping sets are of class $C^{k}\\left(C^{\\infty}\\right.$ or $\\left.C^{\\omega}\\right)$.\n\nRemark 1: Recall the distinction between infinitely differentiable $\\left(C^{\\infty}\\right)$ and analytic $\\left(C^{\\omega}\\right)$.\n\nOne way to ensure that a manifold be $C^{k}$ in the present context is to demand that each $\\varphi_{p}$ and $\\varphi_{p}^{-1}$ be class $C^{k}$. As a matter of convenience, we assume from now on that all manifolds are $C^{\\infty}$ manifolds.\n\nEXAMPLE 13.12 In the case of the spherical manifolds of Example 13.11, the mapping functions $\\varphi_{P}^{-1}$ are either rational with nonvanishing denominators or square roots of positive polynomials. These are certainly $C^{\\infty}$ manifolds (in fact, $C^{\\omega}$ ).\n\nTo bring the notation closer to that of differential geometry (cf. Section 10.4), we now redesignate the maps $\\varphi_{p}^{-1}$ linking $\\mathbf{M}$ with coordinates $\\left(x^{i}\\right)$ : let\n\n$$\n\\varphi_{p}^{-1}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv \\mathbf{r}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv\\left(y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)\\right)\n$$\n\nfor $1 \\leqq j \\leqq m$. (See Fig. 13-4.)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-206}\n\\end{center}\n\nFig. 13-4\n\n\\subsection*{13.7 TANGENT SPACE; VECTOR FIELDS ON MANIFOLDS}\nIntuitively expressed, a vector field $V$ on a manifold $\\mathbf{M}$ is simply a tangent vector to $\\mathbf{M}$ which varies in some continuous (and differentiable) manner from point to point (Fig. 13-5). More precisely, it is a rule that gives a tangent vector at every point of $\\mathbf{M}$. One way to obtain a vector field (if we are in $\\mathbf{R}^{3}$ ) is to take the variable normal vector $\\mathbf{n}$ and cross it with some fixed vector $\\mathbf{a}$; thus $V=\\mathbf{n} \\times \\mathbf{a}$ is a differentiable vector field. But this definition takes us outside the manifold (is extrinsic). We seek a way to remain on the manifold itself (which is immediately applicable to abstract manifolds not imbeddable in a familiar space); such endeavors are called intrinsic methods.\n\nThe clue is to consider some curve on $\\mathbf{M}$ and to define $V$ as the tangent field of that curve. If the curve is defined through a coordinate patch by\n\n$$\n\\mathbf{c}=\\mathbf{r}\\left(x^{1}(t), x^{2}(t), \\ldots, x^{n}(t)\\right)=\\left(y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)\\right) \\quad(1 \\leqq j \\leqq m)\n$$\n\nthen the chain rule gives\n\n$$\n\\frac{d \\mathbf{c}}{d t}=\\frac{d \\mathbf{r}}{d t}=\\frac{\\partial \\mathbf{r}}{\\partial x^{i}} \\frac{d x^{i}}{d t}=V \\quad \\text { or } \\quad V=V^{i} \\mathbf{r}_{i}\n$$\n\nwhere the vectors $\\mathbf{r}_{i} \\equiv \\partial \\mathbf{r} / \\partial x^{i}$ are themselves tangent to $\\mathbf{M}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-207}\n\\end{center}\n\nFig. 13-5\n\nDefinition 6: For any (differentible) manifold $\\mathbf{M}$ having coordinate patch $\\mathbf{U}_{p}$ at any point $p$, the span of the vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ [evaluated at $\\varphi_{p}(p)$ ] is the tangent space at $p$, denoted $T_{p}(\\mathbf{M})$. The union of all tangent spaces $T_{p}(\\mathbf{M})$, for ail $p$ in $\\mathbf{M}$, is called the tangent bundle of $\\mathbf{M}$, denoted $T(\\mathbf{M})$.\n\nAlthough each $T_{p}(\\mathbf{M})$ is a vector space, this does not guarantee that $T(\\mathbf{M})$ is a vector space. For example, the sum of a vector in $T_{p}(\\mathbf{M})$ and a vector in $T_{q}(\\mathbf{M})$ will not generally be tangent to $\\mathbf{M}$.\n\nDefinition 7: A vector field $V$ on a manifold $\\mathbf{M}$ is any $C^{\\infty}$ function that maps $\\mathbf{M}$ to its tangent bundle $T(\\mathbf{M})$. That is, for each point $p$ in $\\mathbf{M}$, the image $\\mathbf{V}(p)=\\mathbf{V}_{p}$ is a vector belonging to the tangent space $T_{p}(\\mathbf{M})$ at $p$. Explicitly, for certain scalar functions $V^{i}$,\n\n\n\\begin{equation*}\nV=V^{i} \\mathbf{r}_{i}=\\left(V^{i} \\frac{\\partial y^{j}}{\\partial x^{i}}\\right) \\quad(j=1,2, \\ldots, m) \\tag{13.7a}\n\\end{equation*}\n\n\nA fundamental theorem regarding vector fields on manifolds may be proved from the basic theory of systems of ordinary differential equations.\n\nTheorem 13.2: Every vector field on a manifold $\\mathbf{M}$ possesses a system of flow curves or integral curves on $\\mathbf{M}$, defined as curves for which the tangent vector at each point coincides with the given vector field at that point.\n\n\\section*{Notation}\nTo de-emphasize the particular choice of coordinatization map $\\varphi_{p}: \\mathbf{U}_{p} \\rightarrow \\mathbf{R}^{n}$, it is customary to omit the vector $\\mathbf{r}$ from the above description of the basis for $T_{p}(\\mathbf{M})$, and to write\n\n$$\n\\frac{\\partial}{\\partial x^{1}}, \\frac{\\partial}{\\partial x^{2}}, \\ldots, \\frac{\\partial}{\\partial x^{n}} \\text { in place of } \\frac{\\partial \\mathbf{r}}{\\partial x^{1}}, \\frac{\\partial \\mathbf{r}}{\\partial x^{2}}, \\ldots, \\frac{\\partial \\mathbf{r}}{\\partial x^{n}}\n$$\n\nor, even more cursorily, $\\partial_{1}, \\partial_{2}, \\ldots, \\partial_{n}$. Many textbooks use this last notation exclusively, and write $(13.7 a)$ as\n\n\n\\begin{equation*}\nV=V^{i} \\partial_{i} \\tag{13.7b}\n\\end{equation*}\n\n\nSuch shorthand is particularly convenient when the coordinate maps $\\varphi_{p}^{-1}$ for a particular manifold are unspecified (for example, when the manifold is defined by an equation $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$ for some real-valued function $F$ ). In this situation, since $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ are not explicitly defined, we use the notation $E_{1}, E_{2}, \\ldots, E_{n}$ to denote the coordinate frame on $\\mathbf{M}$, whose restriction to $T_{p}(\\mathbf{M})$, for each $p$ in $\\mathbf{M}$, is a basis for $T_{p}(\\mathbf{M})$. We make the identifications $E_{i} \\equiv \\partial_{i} \\quad(i=1,2, \\ldots, n)$, giving\n\n\n\\begin{equation*}\nV=V^{i} E_{i} \\tag{13.7c}\n\\end{equation*}\n\n\n\\section*{Extrinsic Representation of Vector Fields}\nIt is possible to represent a vector field $V$ on a manifold without reference to coordinate patches (which often have the disadvantage of being complicated or difficult to construct); one can stay entirely in the $\\left(y^{i}\\right)$ system, which we assume to be rectangular. Suppose $\\mathbf{M}$ is given by a single equation $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$, for some $C^{k}$ function $F$. A one-form $\\sigma=\\omega_{i} d y^{i}$, where $\\omega_{i}=$ $\\omega_{i}\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)$, is said to be restricted to $\\mathbf{M}$ if the point $\\left(y^{i}\\right)$ is required to lie on $\\mathbf{M}$; that is, $F\\left(y^{1}, y^{2}, \\ldots, y^{m}\\right)=0$. As is well known from multidimensional calculus, the gradient $\\nabla F=\\left(\\partial F / \\partial y^{i}\\right)$ is normal to $\\mathbf{M}$, so that if we further require that the restriction of $\\sigma$ map $\\nabla F$ into zero,\n\n$$\n\\omega_{i} \\frac{\\partial F}{\\partial y^{i}}=0\n$$\n\nthen $V=\\sigma$ is a vector field on $\\mathbf{M}$ (having components $d y^{i}$ ).\n\nEXAMPLE 13.13 Consider the paraboloid $\\mathbf{P}$ in $\\mathbf{R}^{3}$ given by\n\n$$\nF\\left(y^{1}, y^{2}, y^{3}\\right)=\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}-y^{3}=0\n$$\n\nShow that the restriction of $\\sigma=y^{1} y^{2} d y^{1}+\\left(y^{2}\\right)^{2} d y^{2}+2 y^{2} y^{3} d y^{3}$ to $\\mathbf{P}$ is a vector field on $\\mathbf{P}$.\n\nWe must show that the scalar product of $\\left(y^{1} y^{2},\\left(y^{2}\\right)^{2}, 2 y^{2} y^{3}\\right)$ and $\\nabla F=\\left(2 y^{1}, 2 y^{2},-1\\right)$ is zero:\n\n$$\n\\left(y^{1} y^{2}\\right)\\left(2 y^{1}\\right)+\\left(y^{2}\\right)^{2}\\left(2 y^{2}\\right)+\\left(2 y^{2} y^{3}\\right)(-1)=\\left[\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}-y^{3}\\right] 2 y^{2}=0\n$$\n\n\\subsection*{13.8 TENSOR FIELDS ON MANIFOLDS}\n\\section*{Dual Tangent Bundle}\nAt each $p$ in $\\mathbf{M}$, let $T_{p}^{*}(\\mathbf{M})$ denote the dual of the vector space $T_{p}(\\mathbf{M})$, and denote by $T^{*}(\\mathbf{M})$ the union of all spaces $T_{p}^{*}(\\mathbf{M})$. The set $T^{*}(\\mathbf{M})$, called the dual tangent bundle of $\\mathbf{M}$, is not necessarily a vector space (just as $T(\\mathbf{M})$ was not).\n\nWe need to make explicit certain elements of $T^{*}(\\mathbf{M})$.\n\n\\section*{Differentials on $M$}\nThe differential of a function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ is rigorously defined as a two-vector function $d f:\\left(\\mathbf{R}^{n}\\right)^{2} \\rightarrow \\mathbf{R}$ which maps each pair $(\\mathbf{x}, \\mathbf{v})$ - where $\\mathbf{x}$ is a point in $\\mathbf{R}^{n}$ and $\\mathbf{v}=\\left(d x^{1}, d x^{2}, \\ldots, d x^{n}\\right)$ is a direction in $\\mathbf{R}^{n}$-to the real number\n\n\n\\begin{equation*}\nd f(\\mathbf{x}, \\mathbf{v}) \\equiv \\frac{\\partial f}{\\partial x^{1}} d x^{1}+\\frac{\\partial f}{\\partial x^{2}} d x^{2}+\\cdots+\\frac{\\partial f}{\\partial x^{n}} d x^{n}=f_{i} d x^{i} \\tag{13.8}\n\\end{equation*}\n\n\nHere, the $f_{i}$ are evaluated at $\\mathbf{x}$. If $f$ is any real-valued $C^{k}$ function on $\\mathbf{M}$, then the differential of\n\n$$\nf\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right) \\equiv f\\left(y^{1}\\left(x^{1}, \\ldots, x^{n}\\right), y^{2}\\left(x^{1}, \\ldots, x^{n}\\right), \\ldots, y^{m}\\left(x^{1}, \\ldots, x^{n}\\right)\\right)\n$$\n\ncalled a differential field on $\\mathbf{M}$, is\n\n$$\nd f=\\frac{\\partial}{\\partial x^{i}}(f(\\mathbf{r}(x))) d x^{i}=\\frac{\\partial f}{\\partial y^{k}} \\frac{\\partial y^{k}}{\\partial x^{i}} d x^{i}=\\left(\\nabla f \\cdot \\mathbf{r}_{i}\\right) d x^{i}\n$$\n\n\\begin{itemize}\n  \\item a one-form. Thus, $d f$ may be thought of as a mapping from $\\mathbf{M}$ to $T^{*}(\\mathbf{M})$ : we agree that the evaluation of $d f$ at $p$ is the one-form $\\left(\\nabla f(p) \\cdot r_{i}(p)\\right) d x^{i}$ in $T_{p}^{*}(\\mathbf{M})$.\n\\end{itemize}\n\nLet us now compare the two kinds of fields on $\\mathbf{M}$.\n\nvector field\n\ndifferential field\n\n$$\n\\begin{aligned}\n& V: \\frac{\\text { Mapping }}{} \\quad \\frac{\\text { Restricted to } \\mathbf{U}_{P}}{V=V^{i} E_{i}} \\\\\n& d f: \\mathbf{M} \\rightarrow T^{*}(\\mathbf{M}) \\quad \\omega=\\left(\\nabla f \\cdot E_{i}\\right) d x^{i}\n\\end{aligned}\n$$\n\nDefinition 8: A tensor field of type $\\left(\\begin{array}{c}r \\\\ s\\end{array}\\right)$ on a manifold $\\mathbf{M}$ is a mapping $T:\\left[T^{*}(\\mathbf{M})\\right]^{r} \\otimes[T(\\mathbf{M})]^{s} \\rightarrow C^{k}\\left(\\mathbf{R}^{m}\\right)$ taking $r$ differential fields and $s$ vector fields on $\\mathbf{M}$ to real-valued $C^{k}$-functions $f$ on $\\mathbf{R}^{m}$. It is assumed that the evaluation of $T$ at a point $p$ on $\\mathbf{M}$ is given by\n\n$$\nT_{p}\\left(\\omega^{1}, \\ldots, \\omega^{r} ; V_{1}, \\ldots, V_{s}\\right)=T\\left(\\omega_{p}^{1}, \\ldots, \\omega_{p}^{r} ; V_{1 p}, \\ldots, V_{s p}\\right) \\equiv f(p)\n$$\n\nand that each map $T_{p}$ is multilinear.\n\nEXAMPLE 13.14 (a) At each fixed $p$ on M, the mapping $T_{p}$ is a tensor [on the vector space $\\left[T_{p}^{*}(\\mathbf{M})\\right]^{r} \\otimes\\left[T_{p}(\\mathbf{M})\\right]^{s}$, of type $\\left.\\left(\\begin{array}{c}r \\\\ s\\end{array}\\right)\\right]$, per Definition 2. (b) Any vector field $V$ on $\\mathbf{M}$ can be interpreted as a type- $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$ tensor field via a mapping $T(\\omega)=\\omega(V)$; compare Problem 13.20.\n\n\\section*{", "solved_problems": "Solved Problems}\n\\section*{ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}\n13.1 (a) Show that the set of polynomials\n\n$$\np_{1}(t)=1+t \\quad p_{2}(t)=t+t^{2} \\quad p_{3}(t)=t^{2}+t^{3} \\quad p_{4}(t)=t^{3}-1\n$$\n\nis a basis for the vector space $\\mathbf{P}^{3}$ (polynomials of degree $\\leqq 3$ ). (b) Find the components of the polynomial $p(t)=t^{3}$ relative to this basis.\n\n(a) Since the dimension of $\\mathbf{P}^{3}$ is 4 and there are 4 vectors, it suffices to show they are linearly independent. Suppose that, for all $t$,\n\nor\n\n$$\n\\begin{gathered}\n\\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=0 \\\\\n\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}\\right) t^{3}=0\n\\end{gathered}\n$$\n\nSince this is an identity, we must have\n\n$$\n0=\\lambda^{1}-\\lambda^{4}=\\lambda^{1}+\\lambda^{2}=\\lambda^{2}+\\lambda^{3}=\\lambda^{3}+\\lambda^{4}\n$$\n\nThus $\\lambda^{1}=\\lambda^{4}, \\lambda^{1}=-\\lambda^{2}=\\lambda^{3}$; so the last equation gives $\\lambda^{1}+\\lambda^{1}=0$ or $\\lambda^{1}=0$, and all $\\lambda^{i}$ vanish, thus proving linear independence.\n\n(b) To find the linear combination yielding $p(t)=t^{3}$, write\n\n$$\n\\begin{aligned}\n& \\qquad \\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=t^{3} \\\\\n& \\text { i.e. } \\quad\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}-1\\right) t^{3}=0 \\\\\n& \\text { or } \\quad \\lambda^{1}=\\lambda^{4} \\quad \\lambda^{1}=-\\lambda^{2}=\\lambda^{3} \\quad \\lambda^{1}+\\lambda^{1}-1=0\n\\end{aligned}\n$$\n\nHence, $\\lambda^{1}=-\\lambda^{2}=\\lambda^{3}=\\lambda^{4}=1 / 2$.\n\n13.2 (a) Model the 4-group by manipulating an ordinary $8 \\frac{1}{2}$ by 11 sheet of paper, in the following way: Let $s$ be the operation of turning the sheet over sideways (as in a book) and setting it on its original location; $u$, the operation of turning the sheet upside down (end-for-end); $b$, both operations ( $s$ followed by $u$, resulting in a $180^{\\circ}$ rotation of the page, face up); and $e$, doing nothing (identity). Interpret the group operation (multiplication) as one operation followed by another (thus, for example, by the above definitions, $b=s u$, reading from left to right). $(b)$ Show that the 4-group cannot be isomorphic to the cyclic group on four elements, $\\mathbf{C}^{4}$.\\\\\n(a) This is one of those problems in mathematics that is best handled without formulas or equations. By simple observation, the operation $u s$ also results in a $180^{\\circ}$ rotation; hence, $b=s u=u s$. It is also clear that if we apply $s$ twice, or $u$ twice, the sheet is left in its original state; $s^{2}=u^{2}=e$. Next, observe that the associative law is valid, so long as we keep the order of the operations intact. It follows that\n\n$$\nb^{2}=(s u)(u s)=s(u)^{2} s=s^{2}=e\n$$\n\nWhen we multiply all the group elements by $b$, we obtain:\n\n$$\nb e=b \\quad b b=e \\quad b u=(s u) u=s u^{2}=s \\quad b s=(s u) s=(u s) s=u\n$$\n\nHence, the multiplication table for this group may be displayed and may be seen to coincide with that for the 4-group:\n\n\\begin{center}\n\\begin{tabular}{c|cccc}\n$\\cdot$ & $e$ & $s$ & $u$ & $b$ \\\\\n\\hline\n$e$ & $e$ & $s$ & $u$ & $b$ \\\\\n$s$ & $s$ & $e$ & $b$ & $u$ \\\\\n$u$ & $u$ & $b$ & $e$ & $s$ \\\\\n$b$ & $b$ & $u$ & $s$ & $e$ \\\\\n\\end{tabular}\n\\end{center}\n\n(b) For the cyclic group, $\\left\\{e, z, z^{2}, z^{3}\\right\\}$, with $z^{4}=e$ for some $z$, we could not have $z^{2}=e$, which is the characteristic property of all elements of the 4-group.\n\n13.3 The simple Lorentz group can be studied by compressing the $4 \\times 4$ matrices down to $2 \\times 2$ matrices:\n\n$$\n\\left[\\begin{array}{llll}\na & b & 0 & 0 \\\\\nb & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad\\left(a^{2}-b^{2}=1\\right)\n$$\n\nShow explicitly that all real $2 \\times 2$ matrices of the above form constitute an abelian group (the group $\\mathbf{L}(2)$ ) under matrix multiplication, and that $\\mathbf{L}(2)$ is a subgroup of the following two larger groups:\n\n$$\n\\begin{aligned}\n& \\mathbf{G L}(2, \\mathbf{R}) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d \\neq b c \\\\\n& \\mathbf{S U}(2) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d-b c=1\n\\end{aligned}\n$$\n\nSince for a matrix in $\\mathbf{L}(2)$,\n\n$$\na d-b c=a^{2}-b^{2}=1\n$$\n\nall such matrices belong to $\\mathbf{S U}(2)$, which, in turn, is a $\\operatorname{subgroup}$ of $\\mathbf{G L}(2, \\mathbf{R})$. Now verify the group properties:\n\n(1) $u v$ belongs to the group for all $u, v$.\n\nIf\n\n$$\n\\begin{gathered}\nA=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right] \\\\\nA B=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right]=\\left[\\begin{array}{ll}\na c+b d & a d+b c \\\\\nb c+a d & b d+a c\n\\end{array}\\right] \\equiv\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right] \\\\\nx^{2}-y^{2}=\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)=(1)(1)=1\n\\end{gathered}\n$$\n\n$$\n\\begin{aligned}\n& \\text { then } \\\\\n& \\text { and }\n\\end{aligned}\n$$\n\n(2) $(u v) w=u(v w)$. Yes: matrix multiplication is associative.\n\n(3) For some $e$ and all $u$, $e u=u e=u$. Yes: the identity matrix has $1^{2}-0^{2}=1$, so is a member of $\\mathbf{L}(2)$.\n\n(4) Given $u, u^{-1} u=u u^{-1}=e$ for some $u^{-1}$.\n\n$$\n\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]^{-1}=\\frac{1}{a^{2}-b^{2}}\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]=\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]\n$$\n\nwhich is in $\\mathbf{L}(2)$.\\\\\n$u v=v u \\quad$ (abelian group).\n\n\\[\nB A=\\left[\\begin{array}{ll}\nc & d  \\tag{5}\\\\\nd & c\n\\end{array}\\right]\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nc a+d b & c b+d a \\\\\nd a+c b & d b+c a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right]=A B\n\\]\n\n\\section*{VECTOR SPACE CONCEPTS}\n13.4 (a) Show that the space $\\mathbf{P}$ of all real-valued polynomials in a real variable $x$ is infinitedimensional. (b) Conclude that $C^{k}(\\mathbf{R})$ is infinite-dimensional.\n\n(a) Suppose that $\\mathbf{P}$ had the finite basis $\\left\\{p_{1}, p_{2}, \\ldots, p_{n}\\right\\}$. Then, for any real polynomial $p(x)$, there exist constants $a_{1}, \\ldots, a_{n}$ such that\n\n\n\\begin{equation*}\na_{1} p_{1}(x)+a_{2}(x)+\\cdots+a_{n} p_{n}(x)=p(x) \\tag{1}\n\\end{equation*}\n\n\nWrite (1) for the $n+1$ values $x_{1}<x_{2}<\\cdots<x_{n+1}$ as a matrix equation:\n\n\\[\na_{1}\\left[\\begin{array}{c}\np_{1}\\left(x_{1}\\right)  \\tag{2}\\\\\np_{1}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{1}\\left(x_{n+1}\\right)\n\\end{array}\\right]+a_{2}\\left[\\begin{array}{c}\np_{2}\\left(x_{1}\\right) \\\\\np_{2}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{2}\\left(x_{n+1}\\right)\n\\end{array}\\right]+\\cdots+a_{n}\\left[\\begin{array}{c}\np_{n}\\left(x_{1}\\right) \\\\\np_{n}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{n}\\left(x_{n+1}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{c}\np\\left(x_{1}\\right) \\\\\np\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np\\left(x_{n+1}\\right)\n\\end{array}\\right]\n\\]\n\nThe column vectors on the left are elements of $\\mathbf{R}^{n+1}$, and as there are $n$ of them, they do not span $\\mathbf{R}^{n+1}$ (see Problem 13.5). To finish the proof, we have only to choose a vector on the right of (2) that is not in the span of those on the left-say, $\\left(z_{1}, z_{2}, \\ldots, z_{n+1}\\right)$-and then to exhibit a polynomial $p$ that takes on those values at $x_{1}, x_{2}, \\ldots, x_{n+1}$. The polynomial provided by Lagrange's interpolation formula does the job.\n\n(b) For any $k$, the vector space $C^{k}(\\mathbf{R})$ contains the infinite-dimensional subspace $\\mathbf{P}$; thus, it too is infinite-dimensional.\n\n13.5 The set $\\mathbf{S}$ of all linear combinations of a fixed set of $n$ vectors, $\\left\\{\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots, \\mathbf{b}_{n}\\right\\}$, is called the span of the given vectors; it is obviously a vector space. Prove that this space has dimension $m \\leqq n$, with equality if and only if the given vectors are linearly independent.\n\nFirst we show that any $n+1$ vectors in $\\mathbf{S}$ are linearly dependent. Suppose, on the contrary, that $\\left\\{\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n+1}\\right\\}$ are linearly independent. Then, because the sequence of vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nis necessarily dependent, the well-known exchange lemma tells us that a sequence\n\n$$\n\\begin{array}{lllllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\ldots & \\mathbf{b}_{j-1} & \\mathbf{b}_{j+1} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nalso spans $\\mathbf{S}$. Repeating the argument $n-1$ times, we arrive at the result that the vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{n} & \\mathbf{u}_{n-1} & \\ldots & \\mathbf{u}_{2} & \\mathbf{u}_{1}\n\\end{array}\n$$\n\nspan $\\mathbf{S}$, making $\\mathbf{u}_{n+1}$ dependent on them-a contradiction.\n\nIf, therefore, $\\left\\{\\mathbf{b}_{i}\\right\\}$ is linearly independent, it constitutes a basis for $\\mathbf{S}$, and $m=n$. On the other hand, if only $m<n$ of the $\\mathbf{b}_{i}$ are linearly independent, the above argument shows that any basis consists of exactly $m$ vectors.\n\n\\section*{DUAL SPACE}\n\\subsection*{13.6 Prove Theorem 13.1.}\nIt is almost trivial that any two vector spaces of dimension $n$ are isomorphic [if $\\left\\{\\mathbf{b}_{i}^{(1)}\\right\\}$ and $\\left\\{\\mathbf{b}_{i}^{(2)}\\right\\}$ are bases, set up the correspondence $\\left.v^{i} \\mathbf{b}_{i}^{(1)} \\leftrightarrow v^{i} \\mathbf{b}_{i}^{(2)}\\right]$. Thus it is necessary to prove only that $\\mathbf{V}^{*}$ is $n$-dimensional if $\\mathbf{V}$ is; in other words, to prove that the set of vectors $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ defined by (13.4) (i) is linearly independent and (ii) has $\\mathbf{V}^{*}$ as its span. (Problem 13.5 will then immediately yield Theorem 13.1.)\n\nProof of (i): By (13.5), for $j=1,2, \\ldots, n$,\n\n$$\n\\lambda_{i} \\boldsymbol{\\beta}^{i}(\\mathbf{v})=0 \\quad \\rightarrow \\quad \\lambda_{i} \\boldsymbol{\\beta}^{i}\\left(\\mathbf{b}_{j}\\right)=0 \\rightarrow \\lambda_{i} \\delta_{j}^{i}=0 \\quad \\rightarrow \\quad \\lambda_{j}=0\n$$\n\nProof of (ii): If $\\boldsymbol{\\beta}(\\mathbf{v})$ is an arbitrary element of $\\mathbf{V}^{*}$, then, by $(13.4 b)$,\n\n$$\n\\boldsymbol{\\beta}(\\mathbf{v})=\\boldsymbol{\\beta}\\left(v^{i} \\mathbf{b}_{i}\\right)=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) v^{i}=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) \\boldsymbol{\\beta}^{i}(\\mathbf{v})\n$$\n\nthat is, $\\boldsymbol{\\beta}$ is a linear combination of the $\\boldsymbol{\\beta}^{i}$.\n\n13.7 Prove the inverse relation between the matrices $A$ and $\\bar{A}$ of (13.6).\n\nBy definition $\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j}$ and $\\overline{\\boldsymbol{\\beta}}^{j}=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}$, so look at (13.5): $\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\delta_{i}^{j}$. By the algebra of mappings and the fact that each $\\overline{\\boldsymbol{\\beta}}^{j}$ and $\\boldsymbol{\\beta}^{k}$ is linear, we have\n\n$$\n\\begin{aligned}\n\\delta_{i}^{j}=\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right) & =\\left(\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\right)\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(A_{i}^{r} \\mathbf{b}_{r}\\right) \\\\\n& =\\bar{A}_{k}^{j}{ }^{j} A_{i}^{r} \\boldsymbol{\\beta}^{k}\\left(\\mathbf{b}_{r}\\right)=\\bar{A}_{k}^{j} A_{i}^{r} \\delta_{r}^{k}=\\bar{A}_{k}^{j} A_{i}^{k}\n\\end{aligned}\n$$\n\nthat is, $\\bar{A} A=I$.\n\n\\section*{TENSORS ON VECTOR SPACES}\n13.8 Which of the following represent linear mappings of $\\left(\\mathbf{R}^{3}\\right)^{*}$ (taking the one-forms on $\\mathbf{R}^{3}$ into the reals), and so constitute (contravariant) tensors of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$ ?\\\\\n(a) $T\\left(a_{1} d x^{1}+a_{2} d x^{2}+a_{3} d x^{3}\\right)=a_{1} a_{2} a_{3}$\\\\\n(b) $T\\left(a_{i} d x^{i}\\right)=a_{1}-a_{3}$\\\\\n(c) $T\\left(a_{i} d x^{i}\\right)=1$\\\\\n(d) $T\\left(a_{i} d x^{i}\\right)=0$\n\n(b) and (d)-the only linear mappings.\n\n13.9 Associated with a particular basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ of a vector space of dimension $n$, we are given some set of numbers $\\left\\{C_{k}^{i j} ; \\quad i, j, k=1, \\ldots, n\\right\\}$. Then we define another set of numbers (and assume a similar definition for all changes of bases), $\\left\\{\\bar{C}_{k}^{i j} ; i, j, k=1, \\ldots, n\\right\\}$, such that\n\n$$\n\\bar{C}_{k}^{i j}=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s}\n$$\n\nand call these numbers the components of the \"tensor\" $C$ on the new basis $\\left\\{\\overline{\\mathbf{b}}_{i}\\right\\}$. Show that this \"tensor\" is indeed a tensor per Definition 2.\n\nWe have only to define the functional\n\n$$\nT\\left(\\boldsymbol{\\omega}_{1}, \\boldsymbol{\\omega}_{2} ; \\mathbf{v}\\right)=T\\left(a_{i} \\boldsymbol{\\beta}^{i}, b_{j} \\boldsymbol{\\beta}^{j} ; v^{k} \\mathbf{b}_{k}\\right)=a_{i} b_{j} C_{k}^{i j} v^{k}\n$$\n\nwhich, by inspection, is a type $\\left(\\begin{array}{l}2 \\\\ 1\\end{array}\\right)$ tensor. We have:\n\n$$\n\\begin{aligned}\n& T_{k}^{i j}=T\\left(\\boldsymbol{\\beta}^{i}, \\boldsymbol{\\beta}^{j} ; \\mathbf{b}_{k}\\right)=T\\left(\\delta_{r}^{i} \\boldsymbol{\\beta}^{r}, \\delta_{s}^{j} \\boldsymbol{\\beta}^{s} ; \\delta_{k}^{t} \\mathbf{b}_{t}\\right)=\\delta_{r}^{i} \\delta_{s}^{j} C_{t}^{r s} \\delta_{k}^{t}=C_{k}^{i j} \\\\\n& \\bar{T}_{k}^{i j}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\boldsymbol{\\beta}}^{j} ; \\overline{\\mathbf{b}}_{k}\\right)=T\\left(\\bar{A}_{r}^{i} \\boldsymbol{\\beta}^{r}, \\bar{A}_{s}^{j} \\overline{\\boldsymbol{\\beta}}^{s} ; A_{k}^{t} \\mathbf{b}_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} T\\left(\\beta^{r}, \\beta^{s} ; b_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s} \\equiv \\bar{C}_{k}^{i j}\n\\end{aligned}\n$$\n\nwhich show that $T$ and $C$ coincide in all coordinate systems.\n\n13.10 In terms of the components $g_{i j}$ of a metric tensor $G(\\mathbf{u}, \\mathbf{v})$, show that:\n\n(a) $G$ is symmetric if and only if $g_{i j}=g_{j i}$ for all $i, j$.\n\n(b) $G$ is nonsingular if and only if $\\left|g_{i j}\\right| \\neq 0$.\n\n(c) $G$ is positive definite if, for all vectors $\\left(u^{i}\\right) \\neq \\mathbf{0}, g_{i j} u^{i} u^{j} \\neq 0$ and $g_{11}>0$.\n\nBy Section 13.5, $g_{i j}=G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)$ where $\\left\\{\\mathbf{b}_{i}\\right\\}$ is some basis for $\\mathbf{V}$. Then, if $\\mathbf{u}=u^{i} \\mathbf{b}_{i}$ and $\\mathbf{v}=v^{i} \\mathbf{b}_{i}$ are any two vectors in $\\mathbf{V}$,\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=u^{i} v^{j} G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)=g_{i j} u^{i} v^{j}\n$$\n\n(a) $G(\\mathbf{u}, \\mathbf{v})=G(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$, if and only if\n\n$$\ng_{i j} u^{i} v^{j}=g_{i j} v^{i} u^{j}=g_{j i} u^{i} v^{j} \\quad \\text { or } \\quad\\left(g_{i j}-g_{j i}\\right) u^{i} v^{j}=0\n$$\n\nfor all real $u^{i}, v^{j}$, which is true if and only if $g_{i j}=g_{j i}$.\n\n(b) In matrix form, the nonsingularity criterion reads:\n\n$$\n\\left[u^{T} G v=0, \\text { for all } u\\right] \\rightarrow v=0\n$$\n\nBut $u^{T} G v$ vanishes for all $u$ if and only if $G v$ is the zero vector. Hence the criterion takes the form\n\n$$\nG v=0 \\rightarrow v=0\n$$\n\nwhich defines $G$ as a nonsingular matrix (a matrix with nonvanishing determinant).\n\n(c) For each fixed $\\mathbf{u}$ and a scalar parameter $\\lambda$, we have\n\n$$\ng_{i j}\\left(u^{i}+\\lambda \\mathbf{b}_{1}^{i}\\right)\\left(u^{j}+\\lambda \\mathbf{b}_{1}^{j}\\right)=g_{i j}\\left(u^{i}+\\lambda \\delta_{1}^{i}\\right)\\left(u^{j}+\\lambda \\delta_{1}^{j}\\right)=G(\\mathbf{u}, \\mathbf{u})+b \\lambda+g_{11} \\lambda^{2} \\equiv P(\\lambda)\n$$\n\nwhere $b \\equiv\\left(g_{1 j}+g_{j 1}\\right) u^{j}$. If $\\mathbf{u}$ is not in the span of $\\mathbf{b}_{1}$, the quadratic form is, by hypothesis, nonzero. Hence, the discriminant of $P(\\lambda)$ is negative:\n\n$$\nb^{2}-4 g_{11} G(\\mathbf{u}, \\mathbf{u})<0 \\quad \\text { or } \\quad G(\\mathbf{u}, \\mathbf{u})>\\frac{b^{2}}{4 g_{11}} \\geqq 0\n$$\n\nIt only remains to note that if $\\mathbf{u}=\\kappa \\mathbf{b}_{1} \\quad(\\kappa \\neq 0)$, then $G(\\mathbf{u}, \\mathbf{u})=\\kappa^{2} g_{11}$, which is again positive.\n\n13.11 Show that positive-definiteness of a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor $G$ implies its nonsingularity.\n\nIf $G(\\mathbf{u}, \\mathbf{v})=0$ for all $\\mathbf{u}$ and some $\\mathbf{v}$, then $G(\\mathbf{v}, \\mathbf{v})=0$; and so, by positive-definiteness, $\\mathbf{v}=0$.\n\n13.12 A covariant tensor $A(\\mathbf{u}, \\mathbf{v})$ is antisymmetric if and only if $A(\\mathbf{u}, \\mathbf{v})--A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$. Show that a criterion for antisymmetry is:\n\n$$\nA(\\mathbf{u}, \\mathbf{u})=0 \\quad(\\text { all } \\mathbf{u})\n$$\n\nBy bilinearity,\n\n$$\nA(\\mathbf{u}+\\mathbf{v}, \\mathbf{u}+\\mathbf{v})=A(\\mathbf{u}, \\mathbf{u})+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+A(\\mathbf{v}, \\mathbf{v})\n$$\n\nThus, if $A(\\mathbf{u}, \\mathbf{u})=0$ for all $\\mathbf{u}$,\n\n$$\n0=0+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+0 \\quad \\text { or } \\quad A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})\n$$\n\nConversely, suppose $A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}$ and $\\mathbf{v}$. Then, with $\\mathbf{u}=\\mathbf{v}$, we have $A(\\mathbf{u}, \\mathbf{u})=-A(\\mathbf{u}, \\mathbf{u})$, or $A(\\mathbf{u}, \\mathbf{u})=0$.\n\n\\section*{MANIFOLDS}\n13.13 (a) Show that the 1 -sphere $\\mathbf{S}^{1}$ (a circle in $\\mathbf{R}^{2}$ ) can be made into a $C^{\\infty} 1$-manifold by constructing an atlas with two charts. (b) Show that a one-chart atlas does not exist [thus, a circle is not homeomorphic to a line or interval].\n\n(a) The standard parameterization of the circle,\n\n$$\n\\varphi^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos \\theta \\\\\ny^{2}=a \\sin \\theta\n\\end{array} \\quad(0 \\leq \\theta<2 \\pi)\\right.\n$$\n\nis insufficient, since the inverse map $\\varphi$ is discontinuous at point $p$ (Fig. 13-6). But if we define\n\n$$\n\\varphi_{p}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(-\\pi<x^{1}<\\pi\\right) \\quad \\varphi_{q}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(0<x^{1}<2 \\pi\\right)\\right.\\right.\n$$\n\nthen $\\left(\\mathbf{S}^{1}-q, \\varphi_{p}\\right)$ and $\\left(\\mathbf{S}^{1}-p, \\varphi_{q}\\right)$ will constitute an atlas. Since there are no 'singular' points involved, it is clear that $\\varphi_{p}, \\varphi_{q}$ and their inverses are $C^{\\infty}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-214}\n\\end{center}\n\nFig. 13-6\n\n(b) Suppose (U, $\\phi$ ) covered $\\mathbf{S}^{1}\\left(\\mathbf{U}=\\mathbf{S}^{1}\\right)$ and $\\phi$ mapped $\\mathbf{S}^{1}$ to the real line $\\left(x^{1}\\right)$, with both $\\phi$ and $\\boldsymbol{\\phi}^{-1}$ continuous. It is not too difficult to see that $\\phi$ maps the circle to a closed interval I: for continuous maps take bounded, closed sets to bounded, closed sets, and connected sets to connected sets; and the only bounded, closed, connected subsets of the real line are closed finite intervals. For any point $P$ on the circle let $P^{\\prime}$ be its diametrically opposite point. The map $g(t) \\equiv \\phi\\left[\\left(\\phi^{-1}(t)\\right)^{\\prime}\\right]$ takes a real number $t$ in $\\mathbf{I}$, maps it to a unique point $P$ on $\\mathbf{S}^{1}$, goes to the (unique) diametrically opposite point $P^{\\prime}$, and returns to a unique real number $t^{\\prime}$ in $\\mathbf{I}$; it is thus a continuous map from $\\mathbf{I}$ to $\\mathbf{I}$. As such, it must (by a familiar theorem of analysis) have a fixed point:\n\n$$\ng\\left(t_{0}\\right)=t_{0} \\text { for some } t_{0} \\text { in } \\mathbf{I}\n$$\n\nBut this means that $\\phi$ sends some pair of diametrically opposite points on $\\mathbf{S}^{1}$ to the same real number, denying one-oneness of $\\phi$.\n\n13.14 A manifold in $\\mathbf{R}^{4}$ is defined by the charts $\\left(k=\\ldots,-2,-1,0,1,2, \\ldots ; x^{1}>0\\right)$\n\n$$\n\\mathbf{r}_{(k)}:\\left\\{\\begin{array}{l}\ny^{1}=x^{1} \\cos x^{2} \\cos x^{3} \\\\\ny^{2}=x^{1} \\cos x^{2} \\sin x^{3} \\\\\ny^{3}=x^{1} \\sin x^{2} \\\\\ny^{4}=a\\left(x^{2}+x^{3}\\right)\n\\end{array} \\quad\\left((k-1) \\frac{\\pi}{2}<x^{3}<(k+1) \\frac{\\pi}{2}\\right)\\right.\n$$\n\n(a) Show that on each coordinate patch, the mapping $\\mathbf{r}_{(k)}$ is one-to-one; hence, $\\varphi_{(k)}=$ $\\mathbf{r}_{(k)}^{-1}: U_{(k)} \\rightarrow \\mathbf{R}^{3}$ exists. (b) Show that both $\\varphi_{(k)}$ and $\\varphi_{(k)}^{-1}$ are continuous. (c) Show that the manifold is generated by a line in $\\mathbf{R}^{4}$ moving along an axis orthogonal to it, with the axis, in turn, orthogonal to the hyperplane $y^{4}=0$ (use vector geometry in $\\mathbf{R}^{4}$ ). Verify that the parameter $x^{1}$ measures the distance from a given point on the manifold to the axis. (d) Show that the parametric section $x^{3}=0$ is a right helicoid (Example 10.4), lying in the hyperplane $y^{2}=0$ ( $\\mathbf{R}^{3}$ coordinatized by $\\left.y^{1}, y^{3}, y^{4}\\right)$.\n\n(a) Assume that $\\mathbf{r}_{(k)}\\left(x^{i}\\right)=\\mathbf{r}_{(k)}\\left(u^{i}\\right)$; we want to show that $\\left(x^{1}, x^{2}, x^{3}\\right)=\\left(u^{1}, u^{2}, u^{3}\\right)$. Now,\n\n$$\n\\left.\\begin{array}{c}\nx^{1} \\cos x^{2} \\cos x^{3}=u^{1} \\cos u^{2} \\cos u^{2} \\\\\nx^{1} \\cos x^{2} \\sin x^{3}=u^{1} \\cos u^{2} \\sin u^{3}\n\\end{array}\\right\\} \\rightarrow \\tan x^{3}=\\tan u^{3}\n$$\n\nBut, for $\\mathrm{U}_{(k)}$, the argument of the tangent function is restricted to a range of $\\pi$ units; so $x^{3}=u^{3}$. It follows that\n\n$$\na\\left(x^{2}+x^{3}\\right)=a\\left(u^{2}+u^{3}\\right) \\rightarrow x^{2}=u^{2}\n$$\n\nFinally, from $x^{1} \\sin x^{2}=u^{1} \\sin u^{2}$, we obtain $x^{1}=u^{1}$.\\\\\n(b) From the form of $\\varphi_{(k)}^{-1} \\equiv \\mathbf{r}_{(k)}$, this function is $C^{\\infty}$. To solve for $\\left(x^{i}\\right)$ in terms of $\\left(y^{i}\\right)$ (to find $\\left.\\varphi_{(k)}\\right)$, write\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(x^{1}\\right)^{2}\\left(\\cos ^{2} x^{2}\\right)\\left(\\cos ^{2} x^{3}+\\sin ^{2} x^{3}\\right)+\\left(x^{1}\\right)^{2}\\left(\\sin ^{2} x^{2}\\right)=\\left(x^{1}\\right)^{2}\n$$\n\nor $x^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\quad\\left(\\right.$ since $\\left.x^{1}>0\\right)$. Then\n\n$$\n\\sin x^{2}=\\frac{y^{3}}{x^{1}}=\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\n$$\n\nor, for a suitable branch of the function $\\sin ^{-1}$,\n\n$$\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n$$\n\nIt is seen that\n\n$$\n\\varphi_{(k)}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\\\\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right) \\\\\nx^{3}=\\frac{y^{4}}{a}-\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n\\end{array}\\right.\n$$\n\nis continuous (in fact, $C^{\\infty}$ ).\n\n(c) The axis orthogonal to $y^{4}=0$ is the vector $\\mathbf{e}_{4}$ in $\\mathbf{R}^{4}$. At the point $y^{4}=a\\left(x^{2}+x^{3}\\right)=$ const. on the manifold, we have (with $x^{2}, x^{3}$ constants and $x^{1}=t$ )\n\n$$\ny^{1}=t \\cos x^{2} \\cos x^{3} \\quad y^{2}=t \\cos x^{2} \\sin x^{3} \\quad y^{3}=t \\sin x^{2} \\quad y^{4}=\\text { const } .\n$$\n\n-a straight line with direction vector orthogonal to $\\mathbf{e}_{4}$. A previous calculation gives the distance from $\\left(y^{1}, y^{2}, y^{3}, y^{4}\\right)$ on $\\mathbf{M}$ to $\\left(0,0,0, a\\left(x^{2}+x^{3}\\right)\\right)$ as\n\n$$\n\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}=x^{1}\n$$\n\n(d) Set $x^{3}=0$ and the map reduces to\n\n$$\ny^{1}=x^{1} \\cos x^{2} \\quad y^{2}=0 \\quad y^{3}=x^{1} \\sin x^{2} \\quad y^{4}=a x^{2}\n$$\n\n13.15 Derive the charts of Example 13.11(a), using stereographic projection (Fig. 13-7).\n\nAs $P$ is a \"convex\" combination of $Q$ and $\\mathrm{N}$,\n\n\n\\begin{equation*}\n\\left(y^{1}, y^{2}, y^{3}\\right)=\\lambda\\left(x^{1}, x^{2}, 0\\right)+(1-\\lambda)(0,0, a) \\tag{1}\n\\end{equation*}\n\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-215}\n\\end{center}\n\nFig. 13-7\n\nTo determine $\\lambda(\\lambda>0)$, write\n\n$$\na^{2}=\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(\\lambda x^{1}\\right)^{2}+\\left(\\lambda x^{2}\\right)^{2}+[(1-\\lambda) a]^{2}\n$$\n\nand solve, obtaining\n\n\n\\begin{equation*}\n\\lambda=\\frac{2 a^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\tag{2}\n\\end{equation*}\n\n\n[Note that $\\lambda$ is less than or greater than 1 according as $P$ lies in the northern or southern hemisphere; $\\lambda \\neq 0$, so this patch omits the north pole.] Together, (1) and (2) yield the chart $\\varepsilon=+1$; the chart $\\varepsilon=-1$ is obtained by changing $a$ to $-a$ in the above (stereographic projection from the south pole).\n\n\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.16 The hyperboloid of one sheet $4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}-\\left(y^{3}\\right)^{2}=16$ is a $C^{\\infty} 2$-manifold $\\mathbf{M}$, by the coordinatization $(k=1,2)$\n\n$$\n\\varphi_{(k)}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=2 \\cos x^{1} \\cosh x^{2} \\\\\ny^{2}=2 \\sin x^{1} \\cosh x^{2} \\\\\ny^{3}=4 \\sinh x^{2}\n\\end{array} \\quad\\left((k-2) \\pi<x^{1}<k \\pi\\right)\\right.\n$$\n\nwith $\\mathbf{U}_{(1)}=\\mathbf{U}_{p}$ and $p=(2,0,0), \\mathbf{U}_{(2)}=\\mathbf{U}_{q}$ and $q=(-2,0,0)$. Represent the vector field on $\\mathbf{M}$ given by\n\n$$\n\\left(V^{i}\\right)=\\left(4 \\sinh x^{2}, 4 \\cosh x^{2}\\right)\n$$\n\nin terms of $(a)$ a vector basis for the tangent space $T_{p}(\\mathbf{M})$, and $(b)$ extrinsically. (c) Describe this field geometrically.\n\n(a) By the usual tools of surface theory (Section 10.5):\n\n$$\n\\begin{aligned}\n& \\mathbf{r}=\\left(2 \\cos x^{1} \\cosh x^{2}, 2 \\sin x^{1} \\cosh x^{2}, 4 \\sinh x^{2}\\right) \\\\\n& E_{1}=\\mathbf{r}_{1}=\\left(-2 \\sin x^{1} \\cosh x^{2}, 2 \\cos x^{1} \\cosh x^{2}, 0\\right) \\\\\n& E_{2}=\\mathbf{r}_{2}=\\left(2 \\cos x^{1} \\sinh x^{2}, 2 \\sin x^{1} \\sinh x^{2}, 4 \\cosh x^{2}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 0\\right) \\\\\n& \\quad+\\left(8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 16 \\cosh ^{2} x^{2}\\right) \\\\\n& \\quad=\\left(4\\left(\\cos x^{1}-\\sin x^{1}\\right) \\sinh 2 x^{2}, 4\\left(\\cos x^{1}+\\sin x^{1}\\right) \\sinh 2 x^{2}, 16 \\cosh ^{2} x^{2}\\right)\n\\end{aligned}\n$$\n\n(b) From the equations for $y^{1}, y^{2}, y^{3}$ we may calculate:\n\n$$\n\\begin{array}{ll}\n\\cosh x^{2}=\\frac{1}{2} \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}} & \\sinh x^{2}=\\frac{1}{4} y^{3} \\\\\n\\cos x^{1}=\\frac{y^{1}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}} & \\sin x^{1}=\\frac{y^{2}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\n& E_{1}=\\left(-y^{2}, y^{1}, 0\\right) \\\\\n& E_{2}=\\left(\\frac{y^{1} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, \\frac{y^{2} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& \\left(V^{i}\\right)=\\left(y^{3}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-y^{2} y^{3}, y^{1} y^{3}, 0\\right)+\\left(y^{1} y^{3}, y^{2} y^{3}, 4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}\\right) \\\\\n& \\quad=\\left(y^{3}\\left(y^{1}-y^{2}\\right), y^{3}\\left(y^{1}+y^{2}\\right),\\left(y^{3}\\right)^{2}+16\\right)\n\\end{aligned}\n$$\n\n(using the equation of the hyperboloid). Hence, in terms of the coordinates $\\left(y^{i}\\right)$,\n\n$$\nV=\\sigma=y^{3}\\left(y^{1}-y^{2}\\right) d y^{1}+y^{3}\\left(y^{1}+y^{2}\\right) d y^{2}+\\left[\\left(y^{3}\\right)^{2}+16\\right] d y^{3}\n$$\n\n(c) See Fig. 13-8 and note that the first component is zero in the plane $y^{1}=y^{2}$. Hence, along the curve of intersection, the field is always parallel to the $y^{2} y^{3}$-plane. Similarly, along $y^{1}=-y^{2}$, the field is parallel to the $y^{1} y^{3}$-plane. On the circle $y^{3}=0$ the field is $(0,0,16)$, or vertical. Since the third component is $\\geqq 16$, there is always a vertical component.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217}\n\\end{center}\n\nFig. 13-8\n\n13.17 Show that the restrictions of (a) $\\sigma_{1}=y^{1} d y^{2}-y^{2} d y^{1}$ and (b) $\\sigma_{2}=\\left(y^{2}-y^{3}\\right) d y^{1}-\\left(y^{1}+\\right.$ $\\left.y^{3}\\right) d y^{2}+\\left(y^{1}+y^{2}\\right) d y^{3}$ to the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}$ are vector fields. (See Fig. 13-9 for a graph of selected values of $\\sigma_{1}$.) By the well-known \"Hairy-Ball Theorem\" (every head of hair has a cowlick), every continuous vector field on $\\mathbf{S}^{2}$ (and also on $\\mathbf{S}^{n}$, for all even integers $n$ ) is zero at some point on the sphere. In fact, the field must vanish at some point of an arbitrarily selected, open hemisphere. (c) Find the zero points explicitly.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217(1)}\n\\end{center}\n\nFig. 13-9\\\\\n(a) The normal vector to $\\mathbf{S}^{2}$ is $\\omega=2 y^{1} d y^{1}+2 y^{2} d y^{2}+2 y^{3} d y^{3}$ and\n\n\n\\begin{gather*}\n\\sigma_{1} \\cdot \\frac{1}{2} \\omega=\\left(-y^{2}\\right)\\left(y^{1}\\right)+\\left(y^{1}\\right)\\left(y^{2}\\right)+(0)\\left(y^{3}\\right)=0 \\\\\n\\sigma_{2} \\cdot \\frac{1}{2} \\omega=\\left(y^{2}-y^{3}\\right)\\left(y^{1}\\right)-\\left(y^{1}+y^{3}\\right)\\left(y^{2}\\right)+\\left(y^{1}+y^{2}\\right)\\left(y^{3}\\right)  \\tag{b}\\\\\n=y^{1} y^{2}-y^{1} y^{3}-y^{1} y^{2}-y^{2} y^{3}+y^{1} y^{3}+y^{2} y^{3}=0\n\\end{gather*}\n\n\n(c) If $\\sigma_{1}=0,-y^{2}=y^{1}=0$ and $0^{2}=0^{2}+\\left(y^{3}\\right)^{2}=a^{2}$, or $y^{3}= \\pm a$. Thus, the zero points are $(0,0, \\pm a)$. For $\\sigma_{2}=0$,\n\n$$\ny^{2}-y^{3}=y^{1}+y^{3}=y^{1}+y^{2}=0 \\quad \\rightarrow \\quad y^{2}=y^{3}=-y^{1}\n$$\n\nand $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}=3\\left(y^{1}\\right)^{2}$, or $y^{1}= \\pm a / \\sqrt{3}$. Hence, the zero points are $\\pm(a / \\sqrt{3}$, $-a / \\sqrt{3},-a / \\sqrt{3})$.\n\n13.18 Consider a manifold whose coordinatization is not easily determined ( $\\mathbf{S O}(n)$, of Example 13.2(e), is such a manifold in $\\mathbf{R}^{n^{2}}$ ) for which, therefore, base vectors $\\mathbf{r}_{i}=E_{i}$ for $T_{p}(\\mathbf{M})$ are unavailable. Develop a reasonable definition of $T_{p}(\\mathbf{M})$ in this situation, which possesses the salient properties of a \"tangent space\" at point $p$.\n\nTo get an idea of what may be desirable, examine the case when the vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ are available. Each tangent vector has the form $V=V^{i} \\mathbf{r}_{i}$, and when $V$ is the tangent vector of a curve $\\mathscr{C}$ on M-the image of $\\mathscr{C}^{\\prime}: x^{i}=x^{i}(t)$ in the coordinate space $\\mathbf{R}^{n}-$ then\n\n$$\nV=\\frac{d x^{i}}{d t} \\mathbf{r}_{i} \\quad \\text { or } \\quad V^{i}=\\frac{d x^{i}}{d t}\n$$\n\nThus $\\left(V^{i}\\right)$ is a direction vector. Recall that\n\n$$\n\\begin{gathered}\n\\mathbf{r}=\\mathbf{r}\\left(x^{1}, \\ldots, x^{n}\\right) \\equiv \\mathbf{r}\\left(y^{1}\\left(x^{1}, \\ldots, x^{n}\\right), \\dot{y}^{2}\\left(x^{1}, \\ldots, x^{n}\\right), \\ldots, y^{m}\\left(x^{1}, \\ldots, x^{n}\\right)\\right) \\\\\n\\mathbf{r}_{i}=\\left(\\frac{\\partial y^{1}}{\\partial x^{i}}, \\frac{\\partial y^{2}}{\\partial x^{i}}, \\ldots, \\frac{\\partial y^{m}}{\\partial x^{i}}\\right)\n\\end{gathered}\n$$\n\nwhence\n\nThus when we write $V^{i} \\mathbf{r}_{i}$ we are actually indicating the $m$ directional derivatives\n\n$$\nV^{i} \\frac{\\partial y^{j}}{\\partial x^{i}} \\equiv \\nabla y^{j} \\cdot V \\quad(1 \\leqq j \\leqq m)\n$$\n\nwith each $y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ a $C^{\\infty}$ real-valued function (defined on $\\mathbf{M}$ if we identify the points of $\\mathbf{M}$ with their coordinates $\\left(x^{i}\\right)$ in $\\mathbf{R}^{n}$ ). It is customary to let the directional derivative of a function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ in the direction $V$ be denoted\n\n$$\nV(f) \\equiv \\nabla f \\cdot V\n$$\n\nThus, each vector $V$ maps a differentiable, real-valued function $f$ to its directional derivative in the direction $V$. The properties of this mapping are immediate: If $f$ and $g$ denote any two differentiable functions from $\\mathbf{M}$ to $\\mathbf{R}$, with $f g$ denoting the ordinary product of two functions, and if $a$ and $b$ are two scalar constants, then\n\n$$\n\\begin{aligned}\n\\text { linearity } & V(a f+b g)=a V(f)+b V(g) \\\\\n\\text { Leibniz' rule } & V(f g)=V(f) g+f V(g)\n\\end{aligned}\n$$\n\nWith this in mind, and armed with the knowledge that the directional derivatives of all functions on $\\mathbf{M}$ would be enough information to construct the basis $\\left\\{\\mathbf{r}_{i}\\right\\}$ when $\\mathbf{r}$ is known, we frame\n\nDefinition 9: By $\\mathbf{C}^{\\infty}(p)$ will be understood the real-valued $C^{\\infty}$ functions on $\\mathbf{U}_{p}$, such that any two functions that agree on some neighborhood of $p$ are identified.\n\nDefinition 10: The tangent space $T_{p}(\\mathbf{M})$ at $p$ is the set of all mappings $V_{p}: \\mathbf{C}^{\\infty}(p) \\rightarrow \\mathbf{R}$ that satisfy for all $a, b$ in $\\mathbf{R}$ and $f, g$ in $\\mathbf{C}^{\\infty}(p)$ the two conditions\\\\\n(i) $V_{p}(a f+b g)=a V_{p}(f)+b V_{p}(g)$\n\n(ii) $V_{p}(f g)=V_{p}(f) g+f V_{p}(g)$\n\nwith the vector-space operations in $T_{p}(\\mathbf{M})$ defined by\n\n$$\n\\begin{aligned}\n\\left(U_{p}+V_{p}\\right)(f) & \\equiv U_{p}(f)+V_{p}(f) \\\\\n\\left(a V_{p}\\right)(f) & \\equiv a V_{p}(f)\n\\end{aligned}\n$$\n\nAny $V_{p}$ in $T_{p}(\\mathbf{M})$ will be called a tangent vector to $\\mathbf{M}$ at $p$. This definition has the advantage not only of dispensing with coordinates, but of enabling one to extend naturally a mapping $F: \\mathbf{M} \\rightarrow \\mathbf{N}$ (from one manifold to another) to a mapping $F_{*}: T_{p}(\\mathbf{M}) \\rightarrow T_{p^{\\prime}}(\\mathbf{N})$ at each point $p$ in $\\mathbf{M}$, where $p^{\\prime}=F(p)$. Such an extension cannot be accomplished using the more elementary definition.\n\nRemark 2: The vectors of $T_{p}(\\mathbf{M})$ as originally defined, if regarded as mappings on $\\mathbf{C}^{\\infty}(p)$, are members of the abstract $T_{p}(\\mathbf{M})$ (Definition 10). In more advanced treatments it is shown that the reverse is true and that $\\operatorname{dim} T_{p}(\\mathbf{M})=\\operatorname{dim} \\mathbf{M}=n$. Hence, the two approaches to tangent spaces are equivalent.\n\n\\section*{TENSOR FIELDS}\n13.19 Show that tensor fields always have the property of being bilinear with respect to scalar functions (as well as to scalar constants), unlike differential operators.\n\nWe must show that for any scalar function $f$ on $\\mathbf{M}$ and any tensor $T$ of type $\\left(\\begin{array}{c}0 \\\\ r\\end{array}\\right)$,\n\n$$\nT\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right)=f T\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n$$\n\nThis is true, since it is true at each point $p$ of $\\mathbf{M}$ :\n\n$$\n\\begin{aligned}\nT_{p}\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right) & \\equiv T\\left(V_{1 p}, \\ldots, f(p) V_{i p}, \\ldots, V_{r p}\\right)=f(p) T\\left(V_{1 p}, \\ldots, V_{i p}, \\ldots, V_{r p}\\right) \\\\\n& =f T_{p}\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n\\end{aligned}\n$$\n\n13.20 Show how to interpret the tangent vector to a curve on a surface $S$ as a (contravariant) tensor of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$.\n\nLet $\\mathbf{c}=\\mathbf{c}(t)$ be a given curve on $\\mathbf{M}=\\mathbf{S}$, with\n\n$$\n\\mathbf{c}_{*}(t)=\\frac{d \\mathbf{c}}{d t}=\\frac{\\partial \\mathbf{y}}{\\partial x^{i}} \\frac{d x^{i}}{d t}\n$$\n\nDefine for any one-form $\\omega=a_{i} d z^{i}$ the linear mapping from $T^{*}(\\mathbf{M})$ to $\\mathbf{R}$ :\n\n$$\nT(\\omega)=a_{i} \\frac{d x^{i}}{d t} \\equiv \\omega\\left(\\frac{d \\mathbf{x}}{d t}\\right)\n$$\n\nUnder the standard basis $\\left\\{d z^{1}, d z^{2}, \\ldots, d z^{n}\\right\\}$ of $T_{p}^{*}(\\mathbf{M})$, and with $\\omega=d z^{i} \\equiv \\delta_{j}^{i} d z^{i}$,\n\n$$\nT^{i}=T\\left(d z^{i}\\right)=\\delta_{j}^{i} \\frac{d x^{j}}{d t}=\\frac{d x^{i}}{d t}\n$$\n\n(We saw earlier that the $d x^{i} / d t$ were contravariant components.)\n\n13.21 Show how to interpret the gradient of a function as a tensor of type $\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$.\n\nLet $f$ have gradient $\\nabla f \\equiv\\left(\\partial f / \\partial x^{i}\\right)$. Define the linear mapping\n\n$$\nT(V)=V^{i} \\frac{\\partial f}{\\partial x^{i}} \\quad\\left(\\frac{\\partial f}{\\partial x^{i}} \\text { fixed }\\right)\n$$\n\nUse the basis $\\left\\{E_{1}, E_{2}, \\ldots, E_{n}\\right\\}$ for $T_{p}(\\mathbf{M})$; then with $V=E_{i} \\equiv \\delta_{i}^{j} E_{j}$,\n\n$$\nT_{i}=T\\left(E_{i}\\right)=\\delta_{i}^{j} \\frac{\\partial f}{\\partial x^{j}}=\\frac{\\partial f}{\\partial x^{i}}\n$$\n\n\\section*{", "supplementary_problems": "Supplementary ProblemsSupplementary Problems}\n13.22 The set of all $2 \\times 2$ matrices of the form\n\n$$\n\\left[\\begin{array}{rr} \n\\pm 1 & 0 \\\\\n0 & \\pm 1\n\\end{array}\\right]\n$$\n\nwhere all possible combinations of signs are taken, forms a four-element subset of $\\mathbf{G L}(2, \\mathbf{R})$. Is it a subgroup?\n\n13.23 Prove that $\\mathrm{SU}(n)$, the set of all $n \\times n$ matrices over the complex numbers having determinant +1 , is a subgroup of $\\mathbf{G L}(n, \\mathbf{C})$. [Hint: $\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)$ holds for complex matrices.]\n\n13.24 Show that the operator $L(f)=\\int_{0}^{1} f(x) d x$ is a linear functional over the set of continuous, real-valued functions on $[0,1]$.\n\n13.25 In terms of the standard basis $\\left\\{d x^{i}\\right\\}$ of $\\left(\\mathbf{R}^{3}\\right)^{*}$, a new basis is defined by\n\n$$\n\\boldsymbol{\\beta}^{1}=d x^{1}-2 d x^{3} \\quad \\boldsymbol{\\beta}^{2}=2 d x^{1}+d x^{2} \\quad \\boldsymbol{\\beta}^{3}=d x^{1}+d x^{3}\n$$\n\nFind the corresponding dual basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ for $\\mathbf{R}^{3}$ in terms of $\\left(\\mathbf{e}_{i}\\right)$, using (13.6). Check your answer by making several calculations of the form $\\omega(v)=\\bar{\\omega}(\\bar{v})$ (a change of basis does not affect the value a linear functional assigns to a vector).\n\n13.26 Consider a tensor $T(\\boldsymbol{\\omega} ; \\mathbf{v})$ over a vector space of dimension $n$ and its dual, with components $T_{j}^{i}$. (a) Show that the trace $\\tau(T) \\equiv T_{i}^{i}$ is invariant under changes of bases. (b) Find $\\tau(T)$ for the tensor defined by $T(\\boldsymbol{\\omega} ; \\mathbf{v})=\\boldsymbol{\\omega}(\\mathbf{v})$.\n\n13.27 Show that every metric tensor $G$ induces a one-to-one mapping (which is an isomorphism, since it is linear) $\\hat{G}: \\mathbf{V} \\rightarrow \\mathbf{V}^{*}$ from a vector space to its dual, under the definition: For each fixed $\\mathbf{u}$ in $\\mathbf{V}$, let $\\hat{G}(\\mathbf{u})$ be the linear functional $\\hat{G}(\\mathbf{u})(\\mathbf{v})=G(\\mathbf{u}, \\mathbf{v})$, for all $\\mathbf{v}$ in $\\mathbf{V}$. This proves for vector spaces of arbitrary dimension:\n\nTheorem 13.3: If $\\mathbf{V}$ possesses a metric tensor, then $\\mathbf{V}$ is isomorphic to its dual $\\mathbf{V}^{*}$.\n\n13.28 Find a convenient atlas showing that the set in $\\mathbf{R}^{4}$ given by the equation\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}-\\left(y^{4}\\right)^{2}=a^{2}\n$$\n\ncan be made into a $C^{\\infty} 3$-manifold. [Hint: Use radicals, as in Example 13.11(b); here, 6 charts will suffice.]\n\n13.29 Show that the restriction of $\\sigma=y^{1} d y^{2}-y^{2} d y^{1}+y^{3} d y^{4}-y^{4} d y^{3}$ on $\\mathbf{R}^{4}$ to the sphere $\\mathbf{S}^{3}$ is a nonzero vector field on $\\mathbf{S}^{3}$.\n\n13.30 Extend Problem 13.29 to the sphere $\\mathbf{S}^{2 k-1}(k \\geqq 2)$.\n\n13.31 Show that if there are only two points, $p_{1}$ and $p_{2}$, on $\\mathbf{S}^{2}$ where a vector field is zero, those points must be antipodal (endpoints of a diameter).\n\n13.32 Show, by geometric reasoning, that there exists a continuous, nonzero vector field on the torus.\n\n13.33 Show that the restrictions of the following one-forms to $\\mathbf{S}^{4},\\left(y^{1}\\right)^{2}+\\cdots+\\left(y^{5}\\right)^{2}=1$, are vector fields on $\\mathbf{S}^{4}$, and find the points where they are zero:\n\n(a) $\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}$\n\n(b) $\\sigma=\\left(y^{2}-y^{3}-y^{4}\\right) d y^{1}+\\left(y^{3}-y^{1}\\right) d y^{2}+\\left(y^{1}-y^{2}+y^{5}\\right) d y^{3}+y^{1} d y^{4}-y^{3} d y^{5}$\n\nB3.3 Although no nonvanishing continuous vector field exists on the 2-sphere $\\mathbf{S}^{2}$, there are three, mutually orthogonal, unit vector fields on $\\mathbf{S}^{3} \\subset \\mathbf{R}^{4}$. These are, in the extrinsic representation of $\\mathbf{S}^{3}$,\n\n$$\n\\begin{aligned}\n& \\sigma_{1}=-y^{1} d y^{1}+y^{2} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4} \\\\\n& \\sigma_{2}=-y^{3} d y^{1}-y^{4} d y^{2}+y^{1} d y^{3}+y^{2} d y^{4} \\\\\n& \\sigma_{3}=-y^{4} d y^{1}+y^{3} d y^{2}-y^{2} d y^{3}+y^{1} d y^{4}\n\\end{aligned}\n$$\n\nShow this. [Note: Manifolds with such vector-field bases are called parallelizable. The manifolds $\\mathbf{S}^{1}, \\mathbf{S}^{3}$, $\\mathbf{S}^{7}$ - and no other $n$-spheres-and the torus are examples.]\n\n13.35 Without resorting to coordinate patches, express extrinsically the collection of tangent spaces $T(\\mathbf{M})$, if $\\mathbf{M}$ is the hyperboloid of one sheet $\\left(y^{1}\\right)^{2}-4\\left(y^{2}\\right)^{2}+4\\left(y^{3}\\right)^{2}=4$.\n\n13.36 For the manifold $\\mathbf{M}$ of Problem 13.35, consider the coordinate patch\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=\\sqrt{1-\\left(x^{1} / 2\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n\nvalid for $y^{3}>0$. Find an expression for an arbitrary vector in $T_{p}(\\mathbf{M})$.\n\n13.37 One way to show that two surfaces meet at right angles is to show that along the curve of intersection the normal vector to one lies in the tangent space of the other. Illustrate this idea for the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=16$ and the cone $\\left(y^{3}\\right)^{2}=9\\left(y^{1}\\right)^{2}+9\\left(y^{2}\\right)^{2}$, the latter coordinatized by\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=3 \\sqrt{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n\n\\section*{", "answers_to_supplementary_problems": "Answers to Supplementary Problems}\n\\section*{CHAPTER 1}\n$1.15 a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}+a_{4} b_{4}+a_{5} b_{5}+a_{6} b_{6}$\n\n1.16 $R_{j k 1}^{1}+R_{j k 2}^{2}+R_{j k 3}^{3}+R_{j k 4}^{4}$. The index $i$ is a dummy index, while $j$ and $k$ are free indices; there are 16 summations.\n\n$1.17 x_{j}$\n\n1.18 (a) $n$; (b) $\\delta_{i j} \\delta_{i j}=\\delta_{i i}=n ;(c) \\delta_{i j} c_{i j}=c_{i i}=c_{11}+c_{22}+c_{33}+\\cdots+c_{n n}$\n\n$1.19 a_{i 3} b_{i 3} \\quad(n=3)$\n\n$1.20 \\quad a_{i j} x_{i} x_{j} \\quad(n=3)$\n\n$1.21 y_{i}=c_{i j} x_{j} \\quad(n=2)$\n\n$1.22 a_{1 k} \\quad(k=2,3)$\n\n$1.23 \\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)=a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{j}\\right)=a_{i j} \\delta_{j k}=a_{i k}$\n\n$1.24 a_{i k}\\left[\\left(x_{i}\\right)^{2}+2 x_{i} x_{k}\\right]$ [not summed on $k$ ]\n\n$1.25\\left(a_{l i j}+a_{i l j}+a_{i j l}\\right) x_{i} x_{j}$\n\n$1.26 a_{k l}+a_{l k}$\n\n1.27 (a) $b_{j}^{i} T_{i}^{r r}$; (b) $a_{i j} b_{j r} x_{r}$; (c) $a_{i j k} b_{i r} b_{j s} b_{k t} x_{r} x_{s} x_{t}$\n\n1.28 (c) $a_{i j}\\left(x_{i}+x_{j}\\right)=a_{i j}\\left(\\varepsilon_{j} x_{i}+\\varepsilon_{i} x_{j}\\right)=a_{i j} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=a_{j i} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=2 a_{i j} \\varepsilon_{i} x_{j}$\n\n\\section*{CHAPTER 2}\n(a) and $(b)\\left[\\begin{array}{lllll}u^{11} & u^{12} & u^{13} & u^{14} & u^{15} \\\\ u^{21} & u^{22} & u^{23} & u^{24} & u^{25} \\\\ u^{31} & u^{32} & u^{33} & u^{34} & u^{35}\\end{array}\\right]$\n\n(c) $\\left[\\begin{array}{lll}u^{11} & u^{12} & u^{13} \\\\ u^{21} & u^{22} & u^{23} \\\\ u^{31} & u^{32} & u^{33} \\\\ u^{41} & u^{42} & u^{43} \\\\ u^{51} & u^{52} & u^{53}\\end{array}\\right]$\n\n(d) $\\left[\\begin{array}{llllll}1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0\\end{array}\\right]$\n\n2.25\n\n(a) $\\left[\\begin{array}{l}5 \\\\ 0 \\\\ 5\\end{array}\\right]$\n\n(b) $\\left[\\begin{array}{lll}1 & 2 & -4 \\\\ 2 & 2 & -2\\end{array}\\right]$\n\n2.29 (a) $17 ;(b) 0 ;(c)-1$\n\n2.30 (a) $-a_{12} a_{21} a_{33} a_{44}+a_{12} a_{21} a_{34} a_{43}+a_{12} a_{23} a_{31} a_{44}-a_{12} a_{23} a_{34} a_{41}-a_{12} a_{24} a_{31} a_{43}+a_{12} a_{24} a_{33} a_{41}$\n\n(b) $-a_{12}\\left|\\begin{array}{lll}a_{21} & a_{23} & a_{24} \\\\ a_{31} & a_{33} & a_{34} \\\\ a_{41} & a_{43} & a_{44}\\end{array}\\right| \\equiv a_{12} A_{12}$\n\n2.32 (a) $\\left[\\begin{array}{rr}2 & -1 \\\\ -5 & 3\\end{array}\\right] \\quad$ (b) $\\frac{1}{7}\\left[\\begin{array}{rrr}1 & 3 & 2 \\\\ 1 & -4 & 2 \\\\ 3 & 2 & -1\\end{array}\\right]$\n\n2.33 One need verify only that (i) interchanging a pair of consecutive indices changes the sign of a single factor in the product; (ii)\n\n$$\n\\prod_{p>q} \\frac{p-q}{|p-q|}=\\prod 1=1\n$$\n\n$2.342 \\pi / 3$\n\n2.35 One pair are $(2,3,0)$ and $(-3,-2,5)$.\n\n$2.36\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]=\\left[\\begin{array}{r}-1 \\\\ 5\\end{array}\\right]$\n\n2.37 $Q=x_{1}^{2}+2 x_{2}^{2}-x_{3}^{2}+8 x_{1} x_{2}+6 x_{1} x_{3}$\n\n$2.38 \\quad A=\\left[\\begin{array}{rrrr}-3 & -\\frac{1}{2} & -\\frac{1}{2} & 3 \\\\ -\\frac{1}{2} & -1 & 0 & 0 \\\\ -\\frac{1}{2} & 0 & 1 & 0 \\\\ 3 & 0 & 0 & 0\\end{array}\\right]$\n\n$2.39 \\bar{c}_{i}=c_{r} b_{r i}$, where $\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}$.\n\n$2.40 \\quad g_{11}=13 / 49, g_{12}=g_{21}=4 / 49, g_{22}=5 / 49$\n\n$2.41 d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=3=d(\\mathbf{x}, \\mathbf{y})$\n\n\\section*{CHAPTER 3}\n3.23 (a) $\\mathscr{I}=-2 \\exp \\left(2 x^{1}\\right)<0$\n\n(b) $\\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}x^{1}=\\frac{1}{2} \\ln \\left(\\bar{x}^{-1} \\bar{x}^{2}\\right) \\\\ x^{2}=\\frac{1}{2} \\ln \\left(\\bar{x}^{1} / \\bar{x}^{2}\\right)\\end{array} \\quad\\left(\\bar{x}^{1}, \\bar{x}^{2}>0\\right)\\right.$\n\n(c) $\\bar{J}=\\left[\\begin{array}{rr}1 / 2 \\bar{x}^{1} & 1 / 2 \\bar{x}^{2} \\\\ 1 / 2 \\bar{x}^{1} & -1 / 2 \\bar{x}^{2}\\end{array}\\right]=\\left[\\begin{array}{rr}\\exp \\left(x^{1}+x^{2}\\right) & \\exp \\left(x^{1}+x^{2}\\right) \\\\ \\exp \\left(x^{1}-x^{2}\\right) & -\\exp \\left(x^{1}-x^{2}\\right)\\end{array}\\right]^{-1}$\n\n$3.26 \\frac{\\partial \\bar{f}}{\\partial \\theta}=0$, so that $f(x, y)=\\bar{f}(r)=\\bar{f}\\left(\\sqrt{x^{2}+y^{2}}\\right)=g\\left(x^{2}+y^{2}\\right)$.\n\n$3.29 \\delta_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}=\\delta^{i}{ }_{j}=\\bar{\\delta}^{i}{ }_{j}$\n\n3.30 The inverse Jacobian matrix at $(1,2)$ is\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\bar{x}^{2} & \\bar{x}^{1} \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nBy Problem 3.14(a), covariance of the matrix\n\nwould imply the matrix equation\n\n$$\nE \\equiv\\left[e_{i j}\\right]_{22}=\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 0 \\\\\n1 & 1\n\\end{array}\\right]\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nor\n\nwhich is patently false.\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0 & 2 \\\\\n-2 & 0\n\\end{array}\\right]\n$$\n\n3.32 (a) $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ represents a tensor if and only if\n\n$$\n\\left(T_{s}^{r}+T_{r}^{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T_{r}^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nwhich requires that $J T \\bar{J}=\\bar{J}^{T} T J^{T}$. This last relation, in turn, generally requires that $\\bar{J}=J^{T}$; i.e., $J$ must be an orthogonal matrix.\n\n(b) $\\bar{T}=J T \\bar{J}$, so that $\\bar{T}^{T}=\\bar{T}$ if $\\bar{J}=J^{T}$.\n\n3.35 As $\\mathbf{T}$ is a tensor (Example 3.4), it is an affine tensor: $\\bar{T}^{i}=a_{r}^{i} T^{r}$. Thus,\n\n$$\n\\frac{d \\bar{T}^{i}}{d t}=a_{r}^{i} \\frac{d T^{r}}{d t}\n$$\n\nshowing $d \\mathbf{T} / d t$ also to be an affine tensor. Any affine tensor is a fortiori a cartesian tensor.\n\n\n\\begin{equation*}\n\\bar{u}_{i} \\bar{u}_{i}=\\left(a_{i r} u_{r}\\right)\\left(a_{i s} u_{s}\\right)=a_{i r} a_{i s} u_{r} u_{s}=\\delta_{r s} u_{r} u_{s}=u_{r} u_{r} \\tag{a}\n\\end{equation*}\n\n\n(b) No, because distance and angles are not preserved under arbitrary linear transformations. Specifically, consider $\\bar{x}^{1}=3 x^{1}, \\bar{x}^{2}=x^{1}+x^{2}$. A scalar product in $\\left(\\bar{x}^{i}\\right)$ is\n\n$$\n\\bar{u}_{i} \\bar{v}_{i}=\\left(3 u_{1}, u_{1}+u_{2}\\right) \\cdot\\left(3 v_{1}, v_{1}+v_{2}\\right)=10 u_{1} v_{1}+u_{1} v_{2}+u_{2} v_{1}+u_{2} v_{2}\n$$\n\nThis clearly will not coincide with $u_{1} v_{1}+u_{2} v_{2}$.\n\n\\section*{CHAPTER 4}\n4.19 Write $[\\mathbf{S T}]=\\left(U_{l m n}^{i j k}\\right)$. There are $\\left(\\begin{array}{c}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations for the contraction indices $u$ and $v$ among the contravariant indices, and, for each of these, $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations among the covariant indices. A given quartet of locations can be filled in 2 inequivalent ways. Thus, the desired number is\n\n$$\n\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot 2=18\n$$\n\n4.23 First, use the device of Problem 4.11 to establish that $T_{j k l}^{i} U^{k} V^{l}$ are tensor components for all $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$; then apply the Quotient Theorem twice.\n\n\\section*{CHAPTER 5}\n5.21 $L=a \\pi$; semicircle of radius $a$.\n\n5.22 No: $Q(1,0,3)=-1$.\n\n5.23 $L=2+e$\n\n5.24 The true distance formula, $\\overline{P_{1} P_{2}}=\\sqrt{\\left(x_{1}^{1}-x_{2}^{1}\\right)^{2}+\\left(x_{1}^{2}-x_{2}^{2}\\right)^{2}-0.2021125\\left(x_{1}^{1}-x_{2}^{1}\\right)\\left(x_{1}^{2}-x_{2}^{2}\\right)}$, yields 4.751. for an error of +0.249 .\n\n5.25\n\n$$\nG=\\left[\\begin{array}{ccc}\n\\left(x^{2}\\right)^{2} & x^{1} x^{2} & 0 \\\\\nx^{1} x^{2} & 1+\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\n$5.26\\left(U_{i}\\right)=(0,1,0),\\left(V_{i}\\right)=\\left(x^{2}, x^{1}, 0\\right)$\n\n5.27 (a) $\\|\\mathbf{U}+\\mathbf{V}\\|^{2}=(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U V}=\\|\\mathbf{U}\\|^{2}+\\left\\|\\mathbf{V}^{2}\\right\\|+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\| \\cos \\theta$\n\n(b) Take $\\theta=\\pi / 2$ in (a).\n\n5.28 (a) $x^{2}=C \\exp \\left(-2 b x^{3} / a^{2}\\right)$ (a one-parameter family of spirals on the cylinder $x^{1}=a$ )\\\\\n(b) No: the curves of (a) have tangent field $\\mathbf{V}$ all along their length; but, for orthogonality, it is necessary only that the tangent at intersections with the pseudo-helix be V. For example, the curve $x^{2}=x^{3}$ on $x^{1}=a$ is also orthogonal to the pseudo-helix at the point $x^{2}=-a^{2} / 2 b, x^{3}=a^{4} / 4 b$.\n\n$5.29 \\quad x^{1}=d \\exp \\left(-\\left(x^{2}\\right)^{2} / 2\\right) \\quad(d=$ const. $)$\n\n$5.30 f^{\\prime}\\left(\\theta_{0}\\right) g^{\\prime}\\left(\\theta_{0}\\right)=-a^{2}$ at intersection points.\n\n5.32 (a) $g^{i \\alpha}=\\lambda(\\alpha) \\delta_{\\alpha}^{i}$, which is tantamount to $g^{i j}=g_{i j}=0$ for $i \\neq j$.\n\n$5.33\\|\\mathbf{V}\\|=1, L=\\pi / 2$\n\n$5.34 x^{1}=a, x^{3}=b \\cot x^{2}+c(c=$ const. $)$\n\n\\section*{CHAPTER 6}\n6.19 $\\bar{x}^{i}=\\frac{1}{2} a_{r s}^{i} x^{r} x^{s}+b_{r}^{i} x^{r}+c^{i} \\quad$ (the $b_{j}^{i}$ and $c^{i}$ constants)\n\n$6.20(a)$\n\n$$\nG=\\left[\\begin{array}{cc}\n16\\left(x^{1}\\right)^{2}+1 & 4 x^{1}-3 \\\\\n4 x^{1}-3 & 10\n\\end{array}\\right]\n$$\n\n(b) $\\Gamma_{111}=16 x^{1}, \\Gamma_{112}=4$, all others 0\n\n6.22 The values, in $\\left(x^{i}\\right)$, of the $\\partial x^{i} / \\partial \\bar{x}^{j}$ are easiest found by inverting $J \\equiv\\left(\\partial \\bar{x}^{i} / \\partial x^{j}\\right)$. Final results are:\n\n$$\n\\Gamma_{11}^{1}=\\Gamma_{22}^{1}-\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1\n$$\n\n6.23 From Problem 6.21(b), $\\Gamma_{j k}^{i}=0$ for $j \\neq k$; while, for $j=k=\\alpha$ (no summation on $\\alpha$ ),\n\n$$\n\\Gamma_{\\alpha \\alpha}^{i}=\\frac{\\partial}{\\partial x^{\\alpha}}\\left(\\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=\\left(d_{\\alpha} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=d_{\\alpha} \\delta_{\\alpha}^{i}\n$$\n\n$6.26-\\Gamma_{221}=\\Gamma_{212}=\\Gamma_{122}=x^{1} ; \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=1 / x^{1}, \\Gamma_{22}^{1}=-x^{1}$\n\n6.27\n\n$$\n\\overline{\\mathscr{J}}=\\left|\\begin{array}{lll}\na_{1}^{1} \\exp \\bar{x}^{1} & 2 a_{2}^{1} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{1} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{2} \\exp \\bar{x}^{-1} & 2 a_{2}^{2} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{2} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{3} \\exp \\bar{x}^{1} & 2 a_{2}^{3} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{3} \\exp 3 \\bar{x}^{3}\n\\end{array}\\right|=\\left[6 \\exp \\left(\\bar{x}^{1}+2 \\bar{x}^{2}+3 \\bar{x}^{3}\\right)\\right] \\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0\n$$\n\nHence the condition is $\\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0$.\n\n6.29 $\\bar{x}^{i}=A^{i} x^{1} \\sin x^{2}+B^{i} x^{1} \\cos x^{2}+C^{i} \\quad(i=1,2)$, with\n\n$$\nx^{1}\\left|\\begin{array}{ll}\nA^{1} & B^{1} \\\\\nA^{2} & B^{2}\n\\end{array}\\right| \\neq 0\n$$\n\nfor a bijection.\n\n6.30 No, because of the presence of Christoffel symbols in (6.7).\n\n$6.31 \\quad T_{j r s, k}^{i}=\\frac{\\partial T_{j r s}^{i}}{\\partial x^{k}}+\\Gamma_{u k}^{i} T_{j r s}^{u}-\\Gamma_{j k}^{u} T_{u r s}^{i}-\\Gamma_{r k}^{u} T_{j u s}^{i}-\\Gamma_{s k}^{u} T_{j r u}^{i}$\n\n$6.36 \\kappa=1 / b$\n\n6.37 (a) $\\frac{d^{2} u}{d s^{2}}=\\frac{d^{2} v}{d s^{2}}=0$\n\n(b) $x^{2}=p\\left(x^{1}\\right)^{2}+q$ (a two-parameter family of \"parabolas\")\\\\\n$6.38(a)$\n\n$$\n\\begin{aligned}\n& \\frac{d^{2} x^{2}}{d s^{2}}-\\left(\\sin x^{2} \\cos x^{2}\\right)\\left(\\frac{d x^{3}}{d s}\\right)^{2}=0 \\\\\n& \\frac{d^{2} x^{3}}{d s^{2}}+\\left(2 \\cot x^{2}\\right) \\frac{d x^{2}}{d s} \\frac{d x^{3}}{d s}=0\n\\end{aligned}\n$$\n\n(b) $x^{2}=\\frac{1}{a} s \\quad x^{3}=0$\n\n(c) The solution (b) represents an arc of a particular great circle $\\left(x^{2}+z^{2}=a^{2}\\right.$, in the usual cartesian coordinates) on the sphere. By symmetry of the sphere, all great-circular arcs, and only these, will be geodesics.\n\n\\section*{CHAPTER 7}\n$7.24 \\varepsilon= \\begin{cases}+1 & 0<|t| \\leqq 1 / 2 \\\\ -1 & |t|>1 / 2\\end{cases}$\n\n$7.25 t=0,1$\n\n$7.26 L=8 \\sqrt{2} / 3$\n\n$7.27 t=\\sqrt{5} / 3\\left[t=0\\right.$, which makes $\\gamma \\equiv\\left|g_{i j}\\right|=0$, is disallowed $]$\n\n$7.28 \\quad L=(64+11 \\sqrt{11}) / 216 \\approx 0.465$\n\n$7.29 \\quad \\theta=i \\ln 2$ at $(0,2,0) ; \\theta=\\cos ^{-1}(7 / 4 \\sqrt{11})$ at $(5,2,3)$\n\n7.30 (a) $L=8(1+3 \\sqrt{3}) \\approx 49.57$\n\n(b) $x^{1}=3\\left(\\sigma s^{2 / 3}+4\\right), x^{2}=\\left(\\sigma s^{2 / 3}+4\\right)^{3 / 2}$, where\n\n$$\n\\sigma=\\left\\{\\begin{array}{lr}\n+1 & -8 \\leqq s>0 \\\\\n-1 & 0 \\leqq s \\leqq 24 \\sqrt{3}\n\\end{array}\\right.\n$$\n\n(c) The null points are $t=0(s=-8)$ and $t=1(s=0)$.\n\n7.31 $L=8(5 \\sqrt{5}-1) \\approx 81.44$\n\n7.32 For $s \\neq 0, \\mathbf{T}=\\left(2|s|^{-1 / 3}, \\sqrt{\\sigma+4 s^{-2 / 3}}\\right)$ and $\\|\\mathbf{T}\\|^{2}=|-\\sigma|=+1$\n\n$7.33 \\quad N^{1}=T^{2}, N^{2}=T^{1}$\n\n7.34 For $s \\neq-8,0$,\n\n7.36\n\n$$\n\\begin{gathered}\n\\kappa=\\frac{-2}{3 s \\sqrt{\\sigma s^{2 / 3}+4}} \\quad \\kappa_{0}=|\\kappa| \\\\\n\\kappa_{0}=|\\kappa|=\\frac{2}{3|s|\\left(s^{2 / 3}-4\\right)^{1 / 2}} \\quad(s \\neq 8)\n\\end{gathered}\n$$\n\nAt the null point $(0,0)$, both Euclidean and Riemannian absolute curvatures become infinite; but at the null point $(12,8)$, only the Riemannian curvature becomes infinite.\n\n$7.37 \\mathbf{T}=|1-4 t|^{-1 / 2}(1,2 t), \\mathbf{N}=|1-4 t|^{-1 / 2}(1,1-2 t), \\kappa=2|1-4 t|^{-3 / 2}$\n\n$7.38 \\quad$ (a) $L=a$. (b) $L=3 a / 2$. (c) Riemannian: $\\mathbf{T}=|\\cos 2 t|^{-1 / 2}(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a)(\\csc 2 t)|\\cos 2 t|^{-3 / 2}$; Euclidean: $\\mathbf{T}=(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a) \\csc 2 t$\n\n7.39 (a) $\\Gamma_{11}^{1}=1 / 2 x^{1}, \\Gamma_{22}^{2}=1 / 2 x^{2}$, others zero\n\n\\section*{CHAPTER 8}\n8.16 By Problem 6.34 and $(8: 1)$,\n\n$$\n\\begin{aligned}\nV_{, k l}^{i}-V_{, l k}^{i} & =g^{i r}\\left(V_{r, k l}-V_{r, l k}\\right)=g^{i r} R_{r k l}^{s} V_{s} \\\\\n& =g^{i r}\\left(g_{s t} R_{r k l}^{s}\\right) V^{t}=\\left(g^{i r} R_{t r k l}\\right) V^{t}=-R_{t k l}^{i} V^{t}\n\\end{aligned}\n$$\n\n8.22 $\\mathrm{K}=1 / 4\\left(x^{1}\\right)^{2}$\n\n8.24 (a) and (b) $\\mathrm{K}=\\frac{x^{1}+x^{2}}{4\\left(x^{1}\\right)^{2} x^{2}\\left(1+2 x^{2}\\right)}$\n\n(c) $\\mathbf{U}_{(2)}=-\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}, \\mathbf{V}_{(2)}=\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}$\n\n8.25 $\\mathrm{K}=1 / a^{2}$\n\n8.26 Basic sets of nonvanishing terms are:\n\n$$\n\\text { (A) } R_{1212}=-\\frac{1}{4}\\left(2 f^{\\prime \\prime}-\\frac{f^{\\prime 2}}{f}-\\frac{f^{\\prime} g^{\\prime}}{g}\\right), R_{1313}=-\\frac{1}{4} \\frac{f^{\\prime} h^{\\prime}}{g}, R_{2323}=-\\frac{1}{4}\\left(2 h^{\\prime \\prime}-\\frac{h^{\\prime 2}}{h}-\\frac{h^{\\prime} g^{\\prime}}{g}\\right)\n$$\n\nand\n\n(A) $G_{1212}=f g, G_{1313}=f h, G_{2323}=g h$\n\nso that\n\n(a) $\\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right)=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{f g W_{1212}+f h W_{1313}+g h W_{2323}}$\n\n(b) $\\quad R=-\\frac{2}{f g h}\\left(h R_{1212}+g R_{1313}+f R_{2323}\\right)$\n\n8.27\n\n(a) $\\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right)=\\frac{-2(\\ln |f|)^{\\prime \\prime}\\left(W_{1212}+W_{2323}\\right)-(\\ln |f|)^{\\prime 2} W_{1313}}{4 f\\left(W_{1212}+W_{1313}+W_{2323}\\right)}$\n\n(b) $R=\\frac{4 f^{\\prime \\prime} f-3 f^{\\prime 2}}{2 f^{3}}$\n\n8.28 Isotropic points compose the surface $x^{2}=e^{-3 / 2}$, over which $\\mathrm{K}=2 e^{3 / 27}$.\n\n8.29 $\\mathrm{K}=-1 / 4$\n\n8.32 $R_{11}=-1, R_{12}=R_{21}=0, R_{22}=\\sin ^{2} x^{1} ; R_{1}^{1}=-1 / a^{2}=R_{2}^{2}, R_{2}^{1}=0=R_{1}^{2} ; R=-2 / a^{2}$\n\n8.33 $\\quad R_{11}=R_{22}=R_{33}=2 /\\left(x^{1}\\right)^{2}$, others $0 ; R_{1}^{1}=R_{2}^{2}=R_{3}^{3}=2$, others $0 ; R=6$\n\n8.35 $g_{i j}=\\left(x^{1}\\right)^{4} \\delta_{i j}$ has $R=0, \\mathrm{~K} \\neq 0$ (use Problem 8.27).\n\n8.36 No implication either way.\n\n\\section*{CHAPTER 9}\n9.17 (a) $u_{0}= \\pm \\sqrt{x^{1} x^{2}+a} \\quad(a=$ const. $) ;(b)$ incompatible\n\n9.20 flat, non-Euclidean\n\n9.21 Euclidean\n\n$9.22(++-)$\n\n9.26 With the notation $f_{i} \\equiv \\partial f / \\partial x^{i}$, for any function $f$ :\n\n$$\n\\begin{aligned}\n& G_{1}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[-\\frac{\\psi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\\\\n& G_{2}^{2}=e^{-\\varphi}\\left(-\\frac{\\psi_{11}}{2}-\\frac{\\psi_{1}^{2}}{4}+\\frac{\\varphi_{1} \\psi_{1}}{4}+\\frac{\\varphi_{1}}{2 x^{1}}-\\frac{\\psi_{i}}{2 x^{1}}\\right)+e^{-\\psi}\\left(\\frac{\\varphi_{44}}{2}+\\frac{\\varphi_{4}^{2}}{4}-\\frac{\\varphi_{4} \\psi_{4}}{4}\\right)=G_{3}^{3} \\\\\n& G_{4}^{4}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[\\frac{\\varphi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\quad G_{4}^{1}=-\\varphi_{4} e^{-\\varphi / x^{1}} \\quad G_{1}^{4}=\\varphi_{4} e^{-\\psi / x^{1}}\n\\end{aligned}\n$$\n\n\\section*{CHAPTER 10}\n10.30 (a) The curve lies on a right circular cylinder of unit radius, beginning at the point $(1,0,1)$ and rising in helix fashion, approaching $\\infty$ asymptotic to the vertical line $x=\\cos 1, y=\\sin 1$, as $t \\rightarrow 1$.\n\n\n\\begin{equation*}\nL=\\int_{0}^{1 / 2} \\frac{\\sqrt{(1-t)^{4}+1}}{(1-t)^{2}} d t \\approx 1.13209039 \\tag{b}\n\\end{equation*}\n\n\n$10.3116 / 3$\n\n10.32 (a) $\\mathbf{T}=(-(a / c) \\sin (s / c),(a / c) \\cos (s / c), b / c)$. Hence the tangent line, $\\mathbf{r}(t) \\equiv \\mathbf{r}+t \\mathbf{T}$, has the coordinate equations\n\n$$\nx=a \\cos \\frac{s}{c}-\\frac{a t}{c} \\sin \\frac{s}{c} \\quad y=a \\sin \\frac{s}{c}+\\frac{a t}{c} \\cos \\frac{s}{c} \\quad z=\\frac{b s}{c}+\\frac{b t}{c}\n$$\n\n(b) $Q$ corresponds to $t=-s$ and $P Q=\\|-s \\mathbf{T}\\|=s$.\n\n(c) The interpretation is that $Q$ can be thought of as the free end of the taut string as it is unwound from the helix. [The locus of $Q, \\mathbf{r}^{*}=\\mathbf{r}(s)-s \\mathbf{r}^{\\prime}(s)$, is called an involute of the helix.]\n\n10.33\n\n$$\n\\begin{gathered}\n\\frac{\\mathbf{T}^{\\prime}}{\\left\\|\\mathbf{T}^{\\prime}\\right\\|}=\\frac{t /|t|}{\\left(1+25 t^{8}\\right)^{1 / 2}}\\left(-5 t^{4}, 1,0\\right) \\\\\n\\kappa=\\frac{20 t^{3} \\sqrt{2}}{\\left(1+50 t^{8}\\right)^{3 / 2}} \\quad \\tau=0\n\\end{gathered}\n$$\n\n10.35 Let the curve $\\mathbf{r}=\\mathbf{r}(s)$ lie in the plane $\\mathbf{b r}=$ const., where $\\mathbf{b}=$ const. and $\\|\\mathbf{b}\\|=1$. Differentiate twice with respect to $s: \\mathbf{b T}=0$ and $\\mathbf{b T}^{\\prime}=0$; hence, $\\mathbf{b}(\\kappa \\mathbf{N})=0$ or $\\mathbf{b N}=0$. It follows that $\\mathbf{b}=\\mathbf{B}$, the binormal vector, so that $\\mathbf{B}^{\\prime}=\\mathbf{0}$ and $\\tau=-\\mathbf{B}^{\\prime} \\mathbf{N}=0$. Conversely, if $\\tau=0$ for a curve $\\mathbf{r}=\\mathbf{r}(s)$, then $\\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}=\\mathbf{0}$ and $\\mathbf{B}$ is a constant unit vector. Define the function $Q(s) \\equiv \\mathbf{B} \\cdot(\\mathbf{r}(s)-\\mathbf{r}(0))$; we have\n\n$$\nQ^{\\prime}=\\mathbf{B r}^{\\prime}=\\mathbf{B} \\mathbf{T}=0\n$$\n\nwhence $Q=$ const. $=Q(0)=0$. Therefore, the curve lies in the plane\n\n$$\n\\mathbf{B r}=\\mathbf{B r}(0)=\\text { const. }\n$$\n\n10.38 $E=\\left|x^{1}\\right| \\sqrt{a^{2}+1}=0$ at $x^{1}=0$.\n\n10.39 $E=a^{2} \\cosh ^{2} x^{1}>0, \\mathbf{n}=\\frac{1}{\\cosh x^{1}}\\left(-\\cos x^{2},-\\sin x^{2}, \\sinh x^{1}\\right)$\n\n10.40\n\n$$\nL=\\int_{1}^{2} \\frac{\\sqrt{5 t^{4}+1}}{t} d t=\\frac{1}{2}\\left[9-\\sqrt{6}+\\ln \\frac{2}{5}(\\sqrt{6}+1)\\right]\n$$\n\n$10.41\\left(v^{1}, v^{2}\\right)=(\\sqrt{12 / 29}, \\sqrt{17 / 29})$ or $(-\\sqrt{12 / 29}, \\sqrt{17 / 29})$\n\n10.43 $\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{2 x^{1}}{\\left(x^{1}\\right)^{2}+a^{2}} ;$ all others zero\n\n10.44\n\n$$\n\\mathrm{II}=\\frac{f^{\\prime} g^{\\prime \\prime}-f^{\\prime \\prime} g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\left(d x^{1}\\right)^{2}+\\frac{f g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\left(d x^{2}\\right)^{2}\n$$\n\n$10.46(a)$\\\\\n(i) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{2}}$\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(x^{1}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{3 / 2}}$\\\\\n(ii) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{2}}$,\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+2 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{3 / 2}}$\n\n(b) Consistent with the invariance of $\\mathrm{K}$ and $\\mathrm{H}$, the change of parameters $\\bar{x}^{1}=x^{1} \\cos x^{2}, \\bar{x}^{2}=$ $x^{1} \\sin x^{2}-$ i.e., the transformation from polar to rectangular coordinates in the parameter planetakes the forms (i) into the forms (ii).\n\n10.49 (a) $\\mathbf{r}^{*}=\\left(a \\operatorname{sech} x^{1}, 0, a x^{1}-a \\tanh x^{1}\\right)$\n\n10.50 The two FFFs correspond under the mapping $\\bar{x}^{1}=a \\sinh x^{1}, \\bar{x}^{2}=x^{2}$.\n\n\\section*{CHAPTER 11}\n\\includegraphics[max width=\\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-229}\\\\\n(b) $\\quad a=\\sqrt{1+4 \\csc ^{4} t \\cot ^{2} t} \\rightarrow 1$;\\\\\n(c) $\\max v=\\sqrt{5}, \\quad \\max a=\\sqrt{17}$\n\n11.15 Rectilinear motion [use (11.8) to show that $\\kappa$ must vanish].\n\n11.16 From (11.7) and (10.9), $\\dot{\\mathbf{a}}=-\\kappa^{2} v^{3} \\mathbf{T}+\\dot{\\kappa} v^{2} \\mathbf{N}+\\kappa \\tau v^{3} \\mathbf{B}$.\n\n$11.17 \\quad a^{1}=\\frac{d^{2} \\rho}{d t^{2}}-\\left(\\rho \\sin ^{2} \\varphi\\right)\\left(\\frac{d \\theta}{d t}\\right)^{2}-\\rho\\left(\\frac{d \\varphi}{d t}\\right)^{2}, \\quad a^{2}=\\frac{d^{2} \\varphi}{d t^{2}}+\\frac{2}{\\rho} \\frac{d \\rho}{d t} \\frac{d \\varphi}{d t}-(\\sin \\varphi \\cos \\varphi)\\left(\\frac{d \\theta}{d t}\\right)^{2}$, $a^{3}=\\frac{d^{2} \\theta}{d t^{2}}+\\frac{2}{\\rho} \\frac{d \\rho}{d t} \\frac{d \\theta}{d t}+(2 \\cot \\varphi) \\frac{d \\theta}{d t} \\frac{d \\varphi}{d t}$\n\n11.18 Let the center of force be the origin of rectangular coordinates for $\\mathbf{E}^{3}$, with the particle's path given by $\\mathbf{r}=\\mathbf{r}(t)$. By Newton's second law, $f \\mathbf{r}=m \\ddot{\\mathbf{r}}$, so that\n\n$$\n\\frac{d}{d t}(\\mathbf{r} \\times \\dot{\\mathbf{r}})=\\mathbf{r} \\times \\ddot{\\mathbf{r}}=\\mathbf{r} \\times\\left(\\frac{f}{m} \\mathbf{r}\\right)=\\mathbf{0}\n$$\n\nand $\\mathbf{r} \\times \\dot{\\mathbf{r}}=\\mathbf{p}=$ const. It follows that $\\mathbf{p} \\cdot \\mathbf{r}=0$.\n\n$11.19 \\nabla^{2} f=\\frac{\\partial^{2} f}{\\partial r^{2}}+\\frac{1}{r^{2}} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{\\partial^{2} f}{\\partial z^{2}}+\\frac{1}{r} \\frac{\\partial f}{\\partial r}$\n\n\\section*{CHAPTER 12}\n12.34 (a) timelike; (b) spacelike; (c) lightlike\n\n12.35 Yes: travel at $4167 \\mathrm{mi} / \\mathrm{sec} \\ll c$. Timelike interval.\n\n12.36 Premultiply $A^{T} G A=G$ by $A G$, and postmultiply by $A^{-1} G$. 12.38 (a) $t=a_{0}^{0} \\bar{t}, x^{1}=-a_{0}^{1} c \\bar{t}, x^{2}=-a_{0}^{2} c \\bar{c}, x^{3}=-a_{0}^{3} c \\bar{c}$. (b) $a_{0}^{0}>0$ if $t$ and $\\bar{t}$ have the same sign; that is, if the\\\\\nclocks of the two observers are both turning clockwise or both counterclockwise.\n\n$$\n\\bar{x}^{0}=\\frac{5}{3} x^{0}-\\frac{4}{3} x^{1} \\quad \\bar{x}^{1}=-\\frac{4}{3} x^{0}+\\frac{5}{3} x^{1} \\quad \\bar{x}^{2}=x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\n12.40\n\n(b) zero\n\n12.41\n\n$$\n\\begin{aligned}\nL^{*} & =\\left[\\begin{array}{cccc}\n5 / 4 & -3 / 4 & 0 & 0 \\\\\n-3 / 4 & 5 / 4 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\nv & =(3 / 5) c\n\\end{aligned}\n$$\n\n$12.42 v=\\sqrt{\\frac{2}{3}} c$\n\n$12.45 v=(4 / 5) c$\n\n12.47 Approximately $25 \\%$ slow.\n\n12.48 About 45 years old.\n\n$12.49 \\approx 17000 \\mathrm{mi} / \\mathrm{sec}$\n\n12.50 For constants $\\hat{F}$ and $\\hat{\\bar{a}} \\equiv \\hat{F} / m$, and with $\\mathbf{F}=(\\hat{F}, 0,0)$ and $\\mathbf{v}=\\left(v_{x}, 0,0\\right),(12.29)$ becomes identical with (1) of Problem 12.26.\n\n12.52 Since $\\partial \\bar{s}^{i} / \\partial \\bar{x}^{i}=0=\\partial s^{i} / \\partial x^{i}$ (the equation of continuity), $\\left(s^{i}\\right)$ may be identified with the vector $\\left(S^{i}\\right)$ of Problem 12.32.\n\n12.54 (a) By analogy with the evaluation of $\\frac{1}{2} e_{i j k l} P_{k l}$ in Problem 12.53,\n\n$$\n\\left[\\frac{1}{2} e_{i j k l}\\left(-\\Phi_{k l}\\right)\\right]_{44}=\\left[\\begin{array}{cccc}\n0 & -\\Phi_{23} & \\Phi_{13} & -\\Phi_{12} \\\\\n* & 0 & -\\Phi_{03} & \\Phi_{02} \\\\\n* & * & 0 & -\\Phi_{01} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n0 & -H_{1} & -H_{2} & -H_{3} \\\\\n* & 0 & E_{3} & -E_{2} \\\\\n* & * & 0 & E_{1} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[F^{i j}\\right]_{44}\n$$\n\n(b) Let $(a b c d)$ denote a permutation of (0123). Then $\\Phi_{a b}=-e_{a b c d} F^{c d} \\quad$ (no summation) and\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\Phi_{a b}}{\\partial x^{c}}+\\frac{\\partial \\Phi_{c a}}{\\partial x^{b}}+\\frac{\\partial \\Phi_{b c}}{\\partial x^{a}} & =-e_{a b c d} \\frac{\\partial F^{c d}}{\\partial x^{c}}-e_{c a b d} \\frac{\\partial F^{b d}}{\\partial x^{b}}-e_{b c a d} \\frac{\\partial F^{a d}}{\\partial x^{a}} \\\\\n& =-e_{a b c d}\\left(\\frac{\\partial F^{c d}}{\\partial x^{c}}+\\frac{\\partial F^{b d}}{\\partial x^{b}}+\\frac{\\partial F^{a d}}{\\partial x^{d}}\\right)= \\pm \\frac{\\partial F^{j d}}{\\partial x^{j}}=0\n\\end{aligned}\n$$\n\nThe second set of equations is derivable directly from $(12.45 b)$, the definition of $\\Phi$, and the fact that the $g_{i j}$ are constants.\n\n\\section*{CHAPTER 13}\n13.22 Yes; it is isomorphic to the 4-group.\n\n13.26 (a) By (13.6),\n\n$$\n\\bar{T}_{i}^{i}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\mathbf{b}}_{i}\\right)=T\\left(\\bar{A}_{j}^{i} \\boldsymbol{\\beta}^{j}, A_{i}^{k} \\mathbf{b}_{k}\\right)=\\bar{A}_{j}^{i} A_{i}^{k} T\\left(\\boldsymbol{\\beta}^{j}, \\mathbf{b}_{k}\\right)=\\delta_{j}^{k} T_{k}^{j}=T_{j}^{j}\n$$\n\n(b) $\\tau(T)=n$\n\n13.27 Suppose that $\\hat{G}\\left(\\mathbf{u}_{1}\\right)=\\hat{G}\\left(\\mathbf{u}_{2}\\right)$. Then $G\\left(\\mathbf{u}_{1}, \\mathbf{v}\\right)=G\\left(\\mathbf{u}_{2}, \\mathbf{v}\\right)$, or by symmetry, $G\\left(\\mathbf{v}, \\mathbf{u}_{1}\\right)=G\\left(\\mathbf{v}, \\mathbf{u}_{2}\\right)$ for all $\\mathbf{v}$ By nonsingularity, $\\mathbf{u}_{1}=\\mathbf{u}_{2}$.\n\n13.28\n\n$$\n\\begin{aligned}\n& \\varphi_{p}^{-1}: \\begin{cases}y^{1}=\\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{2}=x^{1} & \\\\\ny^{3}=x^{2} & \\mathbf{U}_{p}: y^{1}>0 \\\\\ny^{4}=x^{3} & p=(a, 0,0,0)\\end{cases}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231}\n\\end{center}\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(2)}\n\\end{center}\n\n$$\n\\begin{aligned}\n& \\varphi_{ \\pm r}^{-1}:\\left\\{\\begin{array}{l|l}\ny^{1}=x^{1} & \\mathbf{U}_{r}: y^{3}>0 \\\\\ny^{2}=x^{2} & \\begin{array}{c}\n\\mathbf{U}_{-r}: y^{3}<0 \\\\\nr=(0,0, a, 0)\n\\end{array} \\\\\ny^{3}= \\pm \\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{4}=x^{3}\n\\end{array}\\right.\n\\end{aligned}\n$$\n\n13.30\n\n$$\n\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}+y^{6} d y^{5}-y^{5} d y^{6}+\\cdots+y^{2 k} d y^{2 k-1}-y^{2 k-1} d y^{2 k}\n$$\n\n13.31 If $p_{1}$ and $p_{2}$ are not antipodal, there exists a closed hemisphere containing neither one, on which the given (continuous) vector field is nonzero-an impossibility by Problem 13.17(b).\n\n13.32 As shown in Fig. 13-10, let a unit tangent vector be constructed to a generating circle; as the circle is revolved to generate the torus, the tangent vector is obviously propagated continuously to all points of the torus.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(1)}\n\\end{center}\n\nFig. 13-10\n\n13.33 Zero points are: $(a)(0,0,0,0, \\pm a) ;(b) \\pm(0, a / \\sqrt{3}, 0, a / \\sqrt{3}, a / \\sqrt{3})$.\n\n13.35 With $\\omega \\equiv 2\\left(y^{1} d y^{1}-4 y^{2} d y^{2}+4 y^{3} d y^{3}\\right), \\sigma=f d y^{1}+g d y^{2}+h d y^{3}$ must be orthogonal to $\\omega$, for $C^{\\infty}$ functions $f, g, h$. Hence,\n\n$$\ny^{1} f-4 y^{2} g+4 y^{3} h=0\n$$\n\nReplace $f, g$ by $4 y^{3} F, y^{3} G$, and solve for $h$. Similarly, replace $g$ by $y^{1} G$ and $h$ by $y_{1} H$ and solve for $f$; etc. All possible tangent vectors are given by one of three distinct types $\\left(F, G, H\\right.$ denote arbitrary $C^{\\infty}$ functions of $y^{1}, y^{2}, y^{3}$ ):\n\n(1) $\\sigma=4 y^{3} F d y^{1}+y^{3} G d y^{2}+\\left(y^{2} G-y^{1} F\\right) d y^{3}$\n\n(2) $\\sigma=\\left(4 y^{2} G-4 y^{3} H\\right) d y^{1}+y^{1} G d y^{2}+y^{1} H d y^{3}$\n\n(3) $\\sigma=4 y^{2} F d y^{1}+\\left(y^{1} F+y^{3} H\\right) d y^{2}+y^{2} H d y^{3}$\n\n13.36\n\n$$\nU=U_{1}^{i} \\mathbf{r}_{i} \\equiv\\left(2 U^{1} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}}, 2 U^{2} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}},-x^{1} U^{1}+4 x^{2} U^{2}\\right)\n$$\n\nfor any two $C^{\\infty}$ functions $U^{1}, U^{2}$ on $\\left(x^{1}, x^{2}\\right)$.\n\n13.37 The normal vector to the sphere is represented by $\\sigma=y^{1} d y^{1}+y^{2} d y^{2}+y^{3} d y^{3}$; the tangent space of the cone is given by\n\n$$\n\\left(U_{1}, U_{2},\\left(3 x^{1} U^{1}+3 x^{2} U^{2}\\right)\\left(\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right)^{-1 / 2}\\right)\n$$\n\nSet $U^{1}=y^{1}$ and $U^{2}=y^{2}$.\n\n\n\\end{document}"}], {"1.1": "\\section*{REPEATED INDICES}\n1.1 Use the summation convention to write the following, and assign the value of $n$ in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{41} b_{14}$\n\n(b) $a_{11} b_{11}+a_{12} b_{12}+a_{13} b_{13}+a_{14} b_{14}+a_{15} b_{15}+a_{16} b_{16}$\n\n(c) $c_{11}^{i}+c_{22}^{i}+c_{33}^{i}+c_{44}^{i}+c_{55}^{i}+c_{66}^{i}+c_{77}^{i}+c_{88}^{i} \\quad(1 \\leqq i \\leqq 8)$\n\n(a) $a_{i 1} b_{1 i} \\quad(n=4) ;(b) a_{1 i} b_{1 i} \\quad(n=6) ;(c) c_{j j}^{i} \\quad(n=8)$.", "1.2": "```\n1.2 Use the summation convention to write each of the following systems, state which indices are free and which are dummy indices, and fix the value of $n$ :\n\n$$\n\\begin{array}{lc}\nc_{11} x_{1}+c_{12} x_{2}+c_{13} x_{3}=2 & \\text { (b) } a_{j}^{1} x_{1}+a_{j}^{2} x_{2}+a_{j}^{3} x_{3}+a_{j}^{4} x_{4}=b_{j} \\\\\nc_{21} x_{1}+c_{22} x_{2}+c_{23} x_{3}=-3 & (j=1,2) \\\\\nc_{31} x_{1}+c_{32} x_{2}+c_{33} x_{3}=5 &\n\\end{array}\n$$\n\n(a) Set $d_{1}=2, d_{2}=-3$, and $d_{3}=5$. Then one can write the system as $c_{i j} x_{j}=d_{i} \\quad(n=3)$. The free index is $i$ and the dummy index is $j$.\n\n(b) Here, the range of the free index does not match that of the dummy index $(n=4)$, and this fact must be indicated:\n\n$$\na_{j}^{i} x_{i}=b_{j} \\quad(j=1,2)\n$$\n\nThe free index is $j$ and the dummy index is $i$.\n```", "1.3": "```\n1.3 Write out explicitly the summations\n\n$$\nc_{i}\\left(x_{i}+y_{i}\\right) \\quad c_{j} x_{j}+c_{k} y_{k}\n$$\n\nwhere $n=4$ for both, and compare the results.\n\n$$\n\\begin{aligned}\nc_{i}\\left(x_{i}+y_{i}\\right) & =c_{1}\\left(x_{1}+y_{1}\\right)+c_{2}\\left(x_{2}+y_{2}\\right)+c_{3}\\left(x_{3}+y_{3}\\right)+c_{4}\\left(x_{4}+y_{4}\\right) \\\\\n& =c_{1} x_{1}+c_{1} y_{1}+c_{2} x_{2}+c_{2} y_{2}+c_{3} x_{3}+c_{3} y_{3}+c_{4} x_{4}+c_{4} y_{4} \\\\\nc_{j} x_{j}+c_{k} y_{k} & =c_{1} x_{1}+c_{2} x_{2}+c_{3} x_{3}+c_{4} x_{4}+c_{1} y_{1}+c_{2} y_{2}+c_{3} y_{3}+c_{4} y_{4}\n\\end{aligned}\n$$\n\nThe two summations are identical except for the order in which the terms occur, constituting a special case of (1.2).\n```", "1.4": "\\section*{DOUBLE SUMS}\n1.4 If $n=3$, expand $Q=a^{i j} x_{i} x_{j}$.\n\n$$\n\\begin{aligned}\nQ & =a^{1 j} x_{1} x_{j}+a^{2 j} x_{2} x_{j}+a^{3 j} x_{3} x_{j} \\\\\n& =a^{11} x_{1} x_{1}+a^{12} x_{1} x_{2}+a^{13} x_{1} x_{3}+a^{21} x_{2} x_{1}+a^{22} x_{2} x_{2}+a^{23} x_{2} x_{3}+a^{31} x_{3} x_{1}+a^{32} x_{3} x_{2}+a^{33} x_{3} x_{3}\n\\end{aligned}\n$$", "1.5": "\\section*{DOUBLE SUMS}\n1.5 Use the summation convention to write the following, and state the value of $n$ necessary in each case:\n\n(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{12} b_{21}+a_{22} b_{22}+a_{32} b_{23}+a_{13} b_{31}+a_{23} b_{32}+a_{33} b_{33}$\n\n?\n\n(b) \" $g_{11}^{1}+g_{12}^{1}+\\dot{g}_{21}^{1}+g_{22}^{1}+g_{11}^{2}+g_{12}^{2}+g_{21}^{2}+g_{22}^{2}$\\\\\n(a) $a_{i 1} b_{1 i}+a_{i 2} b_{2 i}+a_{i 3} b_{3 i} \\equiv a_{i j} b_{j i} \\quad(n=3)$.\n\n(b) Set $c_{i}=1$ for each $i \\quad(n=2)$. Then the expression may be written\n\n$$\n\\begin{aligned}\ng_{11}^{i} c_{i}+g_{12}^{i} c_{i}+g_{21}^{i} c_{i}+g_{22}^{i} c_{i} & =\\left(g_{11}^{i}+g_{12}^{i}+g_{21}^{i}+g_{22}^{i}\\right) c_{i} \\\\\n& =\\left(g_{j k}^{i} c_{j} c_{k}\\right) c_{i}=g_{j k}^{i} c_{i} c_{j} c_{k}\n\\end{aligned}\n$$", "1.6": "```\n1.6 If $n=2$, write out explicitly the triple summation $c_{r s t} x^{r} y^{s} z^{t}$.\n\nAny expansion technique that yields all $2^{3}=8$ terms will do. In this case we shall interpret the triplet rst as a three-digit integer, and list the terms in increasing order of that integer:\n\n$$\n\\begin{aligned}\n& c_{r s t} x^{r} y^{s} z^{t}=c_{111} x^{1} y^{1} z^{1}+c_{112} x^{1} y^{1} z^{2}+c_{121} x^{1} y^{2} z^{1}+c_{122} x^{1} y^{2} z^{2} \\\\\n& +c_{211} x^{2} y^{1} z^{1}+c_{212} x^{2} y^{1} z^{2}+c_{221} x^{2} y^{2} z^{1}+c_{222} x^{2} y^{2} z^{2}\n\\end{aligned}\n$$\n```", "1.7": "```latex\n1.7 Show that $a_{i j} x_{i} x_{j}=0$ if $a_{i j} \\equiv i-j$.\n\nBecause, for all $i$ and $j, a_{i j}=-a_{j i}$ and $x_{i} x_{j}=x_{i} x_{i}$, the \"off-diagonal\" terms $a_{i j} x_{i} x_{j} \\quad(i<j$; no sum) and $a_{j i} x_{j} x_{i} \\quad\\left(j>i\\right.$; no sum) cancel in pairs, while the \"diagonal\" terms $a_{i i}\\left(x_{i}\\right)^{2}$ are zero to begin with. Thus the sum is zero.\n\nThe result also follows at once from (1.5).\n```", "1.8": "```\n\\section*{DOUBLE SUMS}\n1.8 If the $a_{i j}$ are constants, calculate the partial derivative\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nReverting to $\\Sigma$-notation, we have:\n\n$$\n\\begin{aligned}\n\\sum_{i, j} a_{i j} x_{i} x_{j} & =\\sum_{\\substack{i \\neq k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj \\neq k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i \\neq k \\\\\nj=k}} a_{i j} x_{i} x_{j}+\\sum_{\\substack{i=k \\\\\nj=k}} a_{i j} x_{i} x_{j} \\\\\n& =C+\\left(\\sum_{j \\neq k} a_{k j} x_{j}\\right) x_{k}+\\left(\\sum_{i \\neq k} a_{i k} x_{i}\\right) x_{k}+a_{k k}\\left(x_{k}\\right)^{2}\n\\end{aligned}\n$$\n\nwhere $C$ is independent of $x_{k}$. Differentiating with respect to $x_{k}$,\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(\\sum_{i, j} a_{i j} x_{i} x_{j}\\right) & =0+\\sum_{j \\neq k} a_{k j} x_{j}+\\sum_{i \\neq k} a_{i k} x_{i}+2 a_{k k} x_{k} \\\\\n& =\\sum_{j} a_{k j} x_{j}+\\sum_{i} a_{i k} x_{i}\n\\end{aligned}\n$$\n\nor, going back to the Einstein summation convention,\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right)=a_{k i} x_{i}+a_{i k} x_{i}=\\left(a_{i k}+a_{k i}\\right) x_{i}\n$$\n```", "1.9": "\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.9 Express $b^{i j} y_{i} y_{j}$ in terms of $x$-variables, if $y_{i}=c_{i j} x_{j}$ and $b^{i j} c_{i k}=\\delta_{k}^{j}$.\n\n$$\nb^{i j} y_{i} y_{j}=b^{i j}\\left(c_{i r} x_{r}\\right)\\left(c_{j s} x_{s}\\right)=\\left(b^{i j} c_{i r}\\right) x_{r} c_{j s} x_{s}=\\delta^{i}{ }_{r} x_{r} c_{j s} x_{s}=x_{j} c_{j s} x_{s}=c_{i j} x_{i} x_{j}\n$$", "1.10": "\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.10 Rework Problem 1.8 by use of the product rule for differentiation and the fact that\n\n$$\n\\frac{\\partial x_{p}}{\\partial x_{q}}=\\delta_{p q}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{i} x_{j}\\right) & =a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i} x_{j}\\right)=a_{i j}\\left(x_{j} \\frac{\\partial x_{i}}{\\partial x_{k}}+x_{i} \\frac{\\partial x_{j}}{\\partial x_{k}}\\right) \\\\\n& =a_{i j}\\left(x_{j} \\delta_{i k}+x_{i} \\delta_{j k}\\right)=a_{k j} x_{j}+a_{i k} x_{i} \\\\\n& =\\left(a_{i k}+a_{k i}\\right) x_{i}\n\\end{aligned}\n$$", "1.11": "```\n\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.11 If $a_{i j}=a_{j i}$ are constants, calculate\n\n$$\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\n$$\n\nUsing Problem 1.8, we have\n\n$$\n\\begin{aligned}\n\\frac{\\partial^{2}}{\\partial x_{k} \\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right) & =\\frac{\\partial}{\\partial x_{k}}\\left[\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j} x_{i} x_{j}\\right)\\right]=\\frac{\\partial}{\\partial x_{k}}\\left[\\left(a_{l j}+a_{j l}\\right) x_{j}\\right] \\\\\n& =\\frac{\\partial}{\\partial x_{k}}\\left(2 a_{i l} x_{i}\\right)=2 a_{i l} \\frac{\\partial}{\\partial x_{k}}\\left(x_{i}\\right)=2 a_{i l} \\delta_{k i}=2 a_{k l}\n\\end{aligned}\n$$\n```", "1.12": "```\n1.12 Consider a system of linear equations of the form $y^{i}=a^{i j} x_{j}$ and suppose that $\\left(b_{i j}\\right)$ is a matrix of numbers such that for all $i$ and $j, b_{i r} a^{r j}=\\delta_{i}^{j}$ [that is, the matrix $\\left(b_{i j}\\right)$ is the inverse of the matrix $\\left(a^{i j}\\right)$ ]. Solve the system for $x_{i}$ in terms of the $y^{j}$.\n\nMultiply both sides of the $i$ th equation by $b_{k i}$ and sum over $i$ :\n\n$$\nb_{k i} y^{i}=b_{k i} a^{i j} x_{j}=\\delta_{k}^{j} x_{j}=x_{k}\n$$\n\nor $x_{i}=b_{i j} y^{j}$.\n```", "1.13": "\\section*{SUBSTITUTIONS, KRONECKER DELTA}\n1.13 Show that, generally, $a_{i j k}\\left(x_{i}+y_{j}\\right) z_{k} \\neq a_{i j k} x_{i} z_{k}+a_{i j k} y_{j} z_{k}$.\n\nSimply observe that on the left side there are no free indices, but on the right, $j$ is free for the first term and $i$ is free for the second.", "1.14": "\\section*{DOUBLE SUMS}\n1.14 Show that $c_{i j}\\left(x_{i}+y_{i}\\right) z_{j} \\equiv c_{i j} x_{i} z_{j}+c_{i j} y_{i} z_{j}$.\n\nLet us prove (1.2); the desired identity will then follow upon setting $a_{i j} \\equiv c_{j i}$.\n\n$$\n\\begin{aligned}\na_{i j} x_{j}+a_{i j} y_{j} & \\equiv \\sum_{j} a_{i j} x_{j}+\\sum_{j} a_{i j} y_{j}=\\sum_{j}\\left(a_{i j} x_{j}+a_{i j} y_{j}\\right) \\\\\n& =\\sum_{j} a_{i j}\\left(x_{j}+y_{j}\\right) \\equiv a_{i j}\\left(x_{j}+y_{j}\\right)\n\\end{aligned}\n$$", "2.1": "\\section*{TENSOR NOTATION}\n2.1 Display explicitly the matrices $(a)\\left[b_{i}^{j}\\right]_{42},(b)\\left[b_{j}^{i}\\right]_{24},(c)\\left[\\delta^{i j}\\right]_{33}$.\n\n$$\n\\begin{gathered}\n\\text { (a) }\\left[b_{i}^{j}\\right]_{42}=\\left[\\begin{array}{ll}\nb_{1}^{1} & b_{2}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} \\\\\nb_{1}^{3} & b_{2}^{3} \\\\\nb_{1}^{4} & b_{2}^{4}\n\\end{array}\\right] \\quad \\text { (b) }\\left[b_{j}^{i}\\right]_{24}=\\left[\\begin{array}{llll}\nb_{1}^{1} & b_{2}^{1} & b_{3}^{1} & b_{4}^{1} \\\\\nb_{1}^{2} & b_{2}^{2} & b_{3}^{2} & b_{4}^{2}\n\\end{array}\\right] \\\\\n\\text { (c) }\\left[\\delta^{i j}\\right]_{33}=\\left[\\begin{array}{lll}\n\\delta^{11} & \\delta^{12} & \\delta^{13} \\\\\n\\delta^{21} & \\delta^{22} & \\delta^{23} \\\\\n\\delta^{31} & \\delta^{32} & \\delta^{33}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n\\end{gathered}\n$$\n\nFrom (a) and (b) it is evident that merely interchanging the indices $i$ and $j$ in a matrix $A \\equiv\\left[a_{i j}\\right]_{m n}$ does not necessarily yield the transpose, $A^{T}$.", "2.2": "\\subsection*{2.2 Given}\n$$\nA=\\left[\\begin{array}{ccc}\na & -a & -a \\\\\n2 b & b & -b \\\\\n4 c & 2 c & -2 c\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{rrr}\n2 & 4 & -6 \\\\\n-1 & -2 & 3 \\\\\n3 & 6 & -9\n\\end{array}\\right]\n$$\n\nverify that $A B \\neq B A$.\n\n$$\nA B=\\left[\\begin{array}{ccc}\n2 a+a-3 a & 4 a+2 a-6 a & -6 a-3 a+9 a \\\\\n4 b-b-3 b & 8 b-2 b-6 b & -12 b+3 b+9 b \\\\\n8 c-2 c-6 c & 16 c-4 c-12 c & -24 c+6 c+18 c\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right] \\equiv \\mathrm{O}\n$$\n\nbut $\\quad B A=\\left[\\begin{array}{ccc}2 a+8 b-24 c & -2 a+4 b-12 c & -2 a-4 b+12 c \\\\ -a-4 b+12 c & a-2 b+6 c & a+2 b-6 c \\\\ 3 a+12 b-36 c & -3 a+6 b-18 c & -3 a-6 b+18 c\\end{array}\\right] \\neq \\mathrm{O}$\n\nThus, the commutative law $(A B=B A)$ fails for matrices. Further, $A B=\\mathrm{O}$ does not imply that $A=\\mathrm{O}$ or $B=$ O.", "2.3": "2.3 Prove by use of tensor notation and the product rule for matrices that $(A B)^{T}=B^{T} A^{T}$, for any two conformable matrices $A$ and $B$.\n\nLet $A \\equiv\\left[a_{i j}\\right]_{m n}, B \\equiv\\left[b_{i j}\\right]_{n k}, A B \\equiv\\left[c_{i j}\\right]_{m k}$, and, for all $i$ and $j$,\n\n$$\na_{i j}^{\\prime}=a_{j i} \\quad b_{i j}^{\\prime}=b_{j i} \\quad c_{i j}^{\\prime}=c_{j i}\n$$\n\nHence, $A^{T}=\\left[a_{i j}^{\\prime}\\right]_{n m}, B^{T}=\\left[b_{i j}^{\\prime}\\right]_{k n}$, and $(A B)^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. We must show that $B^{T} A^{T}=\\left[c_{i j}^{\\prime}\\right]_{k m}$. By definition of matrix product, $B^{T} A^{T}=\\left[b_{i r}^{\\prime}{ }^{\\prime} a_{r j}^{\\prime}\\right]_{k m}$, and since\n\n$$\nb_{i r}^{\\prime} a_{r j}^{\\prime}=b_{r i} a_{j r}=a_{j r} b_{r i}=c_{j i}=c_{i j}^{\\prime}\n$$\n\nthe desired result follows.", "2.4": "2.4 Show that any matrix of the form $A=B^{T} B$ is symmetric.\n\nBy Problem 2.3 and the involutory nature of the transpose operation,\n\n$$\nA^{T}=\\left(B^{T} B\\right)^{T}=B^{T}\\left(B^{T}\\right)^{T}=B^{T} B=A\n$$", "2.5": "2.5 From the definition, (2.3), of a determinant of order 3, derive the Laplace expansion by cofactors of the first row.\n\nIn the case $n=3,(2.3)$ becomes\n\n$$\n\\left|\\begin{array}{lll}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{array}\\right| \\equiv\\left|a_{i j}\\right|=e_{i j k} a_{1 i} a_{2 j} a_{3 k}\n$$\n\nSince $e_{i j k}=0$ if any two subscripts coincide, we write only terms for which (ijk) is a permutation of (123):\n\n$$\n\\begin{aligned}\n\\left|a_{i j}\\right| & =e_{123} a_{11} a_{22} a_{33}+e_{132} a_{11} a_{23} a_{32}+e_{213} a_{12} a_{21} a_{33} \\\\\n& \\quad+e_{231} a_{12} a_{23} a_{31}+e_{312} a_{13} a_{21} a_{32}+e_{321} a_{13} a_{22} a_{31} \\\\\n& =a_{11} a_{22} a_{33}-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31} \\\\\n& =a_{11}\\left(a_{22} a_{33}-a_{23} a_{32}\\right)-a_{12}\\left(a_{21} a_{33}-a_{23} a_{31}\\right)+a_{13}\\left(a_{21} a_{32}-a_{22} a_{31}\\right)\n\\end{aligned}\n$$\n\nBut, for $n=2,(2.3)$ gives\n\n$$\n\\left|\\begin{array}{ll}\na_{22} & a_{23} \\\\\na_{32} & a_{33}\n\\end{array}\\right| \\equiv+A_{11}=e_{12} a_{22} a_{33}+e_{2}: a_{23} a_{32}=a_{22} a_{33}-a_{23} a_{32}\n$$\n\nand the analogous expansions of $-A_{12}$ and $+A_{13}$. Hence,\n\n$$\n\\left|a_{i j}\\right|=a_{11} A_{11}+a_{12} A_{12}+a_{13} A_{13}=a_{1 j} A_{1 j}\n$$\n\nas in $(2.6)$.", "2.6": "\\subsection*{2.6 Evaluate:}\n$$\n\\begin{aligned}\n& \\text { (a) }\\left|\\begin{array}{cc}\nb & -2 a \\\\\n-2 c & b\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| \\\\\n& \\left|\\begin{array}{cc}\nb \\cdot & -2 a \\\\\n-2 c & b\n\\end{array}\\right|=b \\cdot b-(-2 a)(-2 c)=b^{2}-4 a c\n\\end{aligned}\n$$\n\n(b) Because of the zeros in the second column, it is simplest to expand by that column:\n\n$$\n\\begin{aligned}\n\\left|\\begin{array}{rrr}\n5 & -2 & 15 \\\\\n-10 & 0 & 10 \\\\\n15 & 0 & 30\n\\end{array}\\right| & =-(-2)\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|+0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n15 & 30\n\\end{array}\\right|-0\\left|\\begin{array}{rr}\n5 & 15 \\\\\n-10 & 10\n\\end{array}\\right| \\\\\n& =2\\left|\\begin{array}{rr}\n-10 & 10 \\\\\n15 & 30\n\\end{array}\\right|=2(10)(15)\\left|\\begin{array}{rr}\n-1 & 1 \\\\\n1 & 2\n\\end{array}\\right|=300(-2-1)=-900\n\\end{aligned}\n$$", "2.7": "2.7 Calculate the angle between the following two vectors in $\\mathbf{R}^{5}$ :\n\n$$\n\\mathbf{x}=(1,0,-2,-1,0) \\quad \\text { and } \\quad \\mathbf{y}=(0,0,2,2,0)\n$$\n\nWe have:\n\n$$\n\\begin{aligned}\n& \\mathbf{x y}=(1)(0)+(0)(0)+(-2)(2)+(-1)(2)+(0)(0)=-6 \\\\\n& \\mathbf{x}^{2}=1^{2}+0^{2}+(-2)^{2}+(-1)^{2}+0^{2}=6 \\\\\n& \\mathbf{y}^{2}=0^{2}+0^{2}+2^{2}+2^{2}+0^{2}=8\n\\end{aligned}\n$$\n\nand (2.9) gives\n\n$$\n\\cos \\theta=\\frac{-6}{\\sqrt{6} \\cdot \\sqrt{8}}=-\\frac{\\sqrt{3}}{2} \\quad \\text { or } \\quad \\theta=\\frac{5 \\pi}{6}\n$$", "2.8": "2.8 Find three linearly independent vectors in $\\mathbf{R}^{4}$ which are orthogonal to the vector $(3,4,1,-2)$.\n\nIt is useful to choose vectors having as many zero components as possible. The components $(0,1,0,2)$ clearly work, and $(1,0,-3,0)$ also. Finally, $(0,0,2,1)$ is orthogonal to the given vector, and seems not to be dependent on the first two chosen. To check independence, suppose scalars $x, y$, and $z$ exist such that\n\n$$\nx\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n0 \\\\\n2\n\\end{array}\\right]+y\\left[\\begin{array}{r}\n1 \\\\\n0 \\\\\n-3 \\\\\n0\n\\end{array}\\right]+z\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n2 \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right] \\quad \\text { or } \\quad \\begin{aligned}\n& x(0)+y(1)+z(0)=0 \\\\\n& x(1)+y(0)+z(0)=0 \\\\\n& x(0)+y(-3)+z(2)=0 \\\\\n& x(2)+y(0)+z(1)=0\n\\end{aligned}\n$$\n\nThis system has the sole solution $x=y=z=0$, and the vectors are independent.", "2.9": "2.9 Prove that the vector product in $R^{3}$ is anticommutative: $\\mathbf{x} \\times \\mathbf{y}=-\\mathbf{y} \\times \\mathbf{x}$.\n\nBy $(2.10 b)$,\n\n$$\n\\mathbf{x} \\times \\mathbf{y}=\\left(e_{i j k} x_{j} y_{k}\\right) \\quad \\text { and } \\quad \\mathbf{y} \\times \\mathbf{x}=\\left(e_{i j k} y_{j} x_{k}\\right)\n$$\n\nBut $e_{i k j}=-e_{i j k}$, so that\n\n$$\ne_{i j k} y_{j} x_{k}=e_{i k j} y_{k} x_{j}=-e_{i j k} y_{k} x_{j}=-e_{i j k} x_{j} y_{k}\n$$", "2.10": "\\section*{INVERTING A MATRIX}\n2.10 Establish the generalized Laplace expansion theorem: $a_{r j} A_{s j}=|A| \\delta_{r s}$.\n\nConsider the matrix\n\n$$\nA^{*}=\\left[\\begin{array}{cccc}\na_{11} & a_{12} & \\ldots & a_{1 n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\cdots & \\ldots & \\cdots \\\\\na_{r 1} & a_{r 2} & \\ldots & a_{r n} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{n 1} & a_{n 2} & \\ldots & a_{n n}\n\\end{array}\\right] \\text { row } \\boldsymbol{r}\n$$\n\nwhich is obtained from matrix $A$ by replacing its $s$ th row by its $r$ th row $(r \\neq s)$. By (2.6), applied to row $r$ of $A^{*}$,\n\n$$\n\\operatorname{det} A^{*}=a_{r j} A_{r j}^{*} \\quad(\\text { not summed on } r)\n$$\n\nNow, because rows $r$ and $s$ are identical, we have for all $j$,\n\n$$\nA_{r j}^{*}=(-1)^{p} A_{s j}^{*}=(-1)^{p} A_{s j} \\quad(p \\equiv r-s)\n$$\n\nTherefore, $\\operatorname{det} A^{*}=(-1)^{p} a_{r j} A_{s j}$. But it is easy to see (Problem 2.31) that, with two rows the same, $\\operatorname{det} A^{*}=0$. We have thus proved that\n\n$$\na_{r j} A_{s j}=0 \\quad(r \\neq s)\n$$\n\nand this, together with (2.6) for the case $r=s$, yields the theorem.", "2.11": "2.11 Given a matrix $A \\equiv\\left[a_{i j}\\right]_{n n}$, with $|A| \\neq 0$, use Problem 2.10 to show that\n\n$$\nA B=I \\quad \\text { where } \\quad B=\\frac{1}{|A|}\\left[A_{i j}\\right]_{n n}^{T}\n$$\n\nSince the $(i, j)$-element of $B$ is $A_{j i} /|A|$,\n\n$$\nA B=\\left[a_{i k}\\left(A_{j k} /|A|\\right)\\right]_{n n}=\\frac{1}{|A|}\\left[|A| \\delta_{i j}\\right]_{n n}=\\frac{|A|}{|A|}\\left[\\delta_{i j}\\right]_{n n}=I\n$$\n\n[It follows from basic facts of linear algebra that also $B A=I$; therefore, $A^{-1}=B$, which establishes (2.11a).]", "2.12": "2.12 Invert the matrix\n\n$$\nA=\\left[\\begin{array}{rrr}\n-2 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n2 & -2 & 3\n\\end{array}\\right]\n$$\n\nrow:\n\nUse (2.11c). To evaluate $|A|$, add twice the third column to the first and then expand by the first\n\n$$\n|A|=\\left|\\begin{array}{rrr}\n0 & 0 & 1 \\\\\n3 & 1 & 0 \\\\\n8 & -2 & 3\n\\end{array}\\right|=1 \\cdot\\left|\\begin{array}{rr}\n3 & 1 \\\\\n8 & -2\n\\end{array}\\right|=-6-8=-14\n$$\n\nThen, computing cofactors as we go,\n\n$$\nA^{-1}=\\frac{1}{-14}\\left[\\begin{array}{rrr}\n3 & -2 & -1 \\\\\n-9 & -8 & 3 \\\\\n-8 & -4 & -2\n\\end{array}\\right]=\\left[\\begin{array}{rrr}\n-3 / 14 & 1 / 7 & 1 / 14 \\\\\n9 / 14 & 4 / 7 & -3 / 14 \\\\\n4 / 7 & 2 / 7 & 1 / 7\n\\end{array}\\right]\n$$", "2.13": "2.13 Let $A$ and $B$ be invertible matrices of the same order. Prove that $(a)\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$ (i.e., the operations of transposition and inversion commute); (b) $(A B)^{-1}=B^{-1} A^{-1}$.\n\n(a) Transpose the equations $A A^{-1}=A^{-1} A=I$, recalling Problem 2.3, to obtain\n\n$$\n\\left(A^{-1}\\right)^{T} A^{T}=A^{T}\\left(A^{-1}\\right)^{T}=I^{T}=I\n$$\n\nwhich show that $A^{T}$ is invertible, with inverse $\\left(A^{T}\\right)^{-1}=\\left(A^{-1}\\right)^{T}$.\n\n(b) By the associative law for matrix multiplication,\n\n$$\n(A B)\\left(B^{-1} A^{-1}\\right)=A\\left(B B^{-1}\\right) A^{-1}=A I A^{-1}=A A^{-1}=I\n$$\n\nand, similarly,\n\n$$\n\\left(B^{-1} A^{-1}\\right)(A B)=I\n$$\n\nHence, $(A B)^{-1}=B^{-1} A^{-1}$.", "2.14": "\\subsection*{2.14 Write the following system of equations in matrix form, then solve by using the inverse matrix:}\n\n$$\n\\begin{aligned}\n3 x-4 y & =-18 \\\\\n-5 x+8 y & =34\n\\end{aligned}\n$$\n\nThe matrix form of the system is\n\n\\[\n\\left[\\begin{array}{rr}\n3 & -4  \\tag{1}\\\\\n-5 & 8\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]\n\\]\n\nThe inverse of the $2 \\times 2$ coefficient matrix is:\n\n$$\n\\left[\\begin{array}{rr}\n3 & -4 \\\\\n-5 & 8\n\\end{array}\\right]^{-1}=\\frac{1}{24-(+20)}\\left[\\begin{array}{ll}\n8 & 4 \\\\\n5 & 3\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\n$$\n\nPremultiplying (1) by this matrix gives\n\n$$\nI\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 1 \\\\\n5 / 4 & 3 / 4\n\\end{array}\\right]\\left[\\begin{array}{r}\n-18 \\\\\n34\n\\end{array}\\right]=\\left[\\begin{array}{r}\n-2 \\\\\n3\n\\end{array}\\right]\n$$\n\nor $x=-2, y=3$.", "2.15": "\\subsection*{2.15 If $\\left[b^{i j}\\right]=\\left[a_{i j}\\right]^{-1}$, solve the $n \\times n$ system}\n\n\\begin{equation*}\ny_{i}=a_{i j} x_{j} \\tag{1}\n\\end{equation*}\n\nfor the $x_{j}$ in terms of the $y_{i}$.\n\nMultiply both sides of (1) by $b^{k i}$ and sum on $i$ :\n\n$$\nb^{k i} y_{i}=b^{k i} a_{i j} x_{j}=\\delta_{j}^{k} x_{j}=x_{k}\n$$\n\nTherefore, $x_{j}=b^{j i} y_{i}$.", "2.16": "\\subsection*{2.16 Write the quadratic form in $\\mathbf{R}^{4}$}\n\n$$\nQ=7 x_{1}^{2}-4 x_{1} x_{3}+3 x_{1} x_{4}-x_{2}^{2}+10 x_{2} x_{4}+x_{3}^{2}-6 x_{3} x_{4}+3 x_{4}^{2}\n$$\n\nin the matrix form $\\mathbf{x}^{T} A \\mathbf{x}$ with $A$ symmetric.\n\n$$\nQ=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -4 & 3 \\\\\n0 & -1 & 0 & 10 \\\\\n0 & 0 & 1 & -6 \\\\\n0 & 0 & 0 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]=\\left[\\begin{array}{llll}\nx_{1} & x_{2} & x_{3} & x_{4}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\n7 & 0 & -2 & 3 / 2 \\\\\n0 & -1 & 0 & 5 \\\\\n-2 & 0 & 1 & -3 \\\\\n3 / 2 & 5 & -3 & 3\n\\end{array}\\right]\\left[\\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4}\n\\end{array}\\right]\n$$", "2.17": "\\subsection*{2.17 Show that under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$, the quadric hypersurface $c_{i j} x_{i} x_{j}=1$ transforms to $\\bar{c}_{i j} \\bar{x}_{i} \\bar{x}_{j}=1$, where}\n\n$$\n\\bar{c}_{i j}=c_{r s} b_{r i} b_{s j} \\quad \\text { with } \\quad\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}\n$$\n\nThis will be worked using matrices, from which the component form can be easily deduced. The hypersurface has the equation $\\mathbf{x}^{T} C \\mathbf{x}=1$ in unbarred coordinates, and $\\overline{\\mathbf{x}}=A \\mathbf{x}$ defines a barred coordinate system. Substituting $\\mathbf{x}=B \\overline{\\mathbf{x}} \\quad\\left(B=A^{-1}\\right)$ into the equation of the quadric, we have\n\n$$\n(B \\overline{\\mathbf{x}})^{T} C(B \\overline{\\mathbf{x}})=1 \\quad \\text { or } \\quad \\overline{\\mathbf{x}}^{T} B^{T} C B \\overline{\\mathbf{x}}=1\n$$\n\nThus, in the barred coordinate system, the equation of the quadric is $\\overline{\\mathbf{x}}^{T} \\bar{C} \\overline{\\mathbf{x}}=1$, where $\\bar{C}=B^{T} C B$.", "2.18": "\\section*{DISTANCE IN A BARRED COORDINATE SYSTEM}\n2.18 Calculate the coefficients $g_{i j}$ in the distance formula (2.14) for the barred coordinate system in $\\mathbf{R}^{2}$ defined by $\\bar{x}_{i}=a_{i j} x_{j}$, where $a_{11}=a_{22}=1, a_{12}=0$, and $a_{21}=2$.\n\nWe have merely to calculate $G=\\left(A A^{T}\\right)^{-1}$, where $A=\\left(a_{i j}\\right)$ :\n\n$$\nA A^{T}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & 2 \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n1 \\cdot 1+0 & 1 \\cdot 2+0 \\\\\n2 \\cdot 1+0 & 2 \\cdot 2+1 \\cdot 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]\n$$\n\nBy $(2.11 b)$,\n\n$$\n\\left(A A^{T}\\right)^{-1}=\\left[\\begin{array}{ll}\n1 & 2 \\\\\n2 & 5\n\\end{array}\\right]^{-1}=\\frac{1}{5-4}\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n5 & -2 \\\\\n-2 & 1\n\\end{array}\\right]\n$$\n\nThus, $g_{11}=5, g_{12}=g_{21}=-2, g_{22}=1$.", "2.19": "\\subsection*{2.19 Test the distance formula obtained in Problem 2.18 by finding the distance between the aliases of $\\left(x_{i}\\right)=(1,-3)$ and $\\left(y_{i}\\right)=(0,-2)$, which points are a distance $\\sqrt{2}$ apart.}\n\nThe coordinates for the given points in the barred system are found to be\n\n$$\n\\overline{\\mathbf{x}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n1 \\\\\n-3\n\\end{array}\\right]=\\left[\\begin{array}{r}\n1 \\\\\n-1\n\\end{array}\\right] \\quad \\overline{\\mathbf{y}}=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n2 & 1\n\\end{array}\\right]\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]=\\left[\\begin{array}{r}\n0 \\\\\n-2\n\\end{array}\\right]\n$$\n\nor $\\left(\\bar{x}_{i}\\right)=(1,-1)$ and $\\left(\\bar{y}_{i}\\right)=(0,-2)$. Using the $g_{i j}$ calculated in Problem 2.18,\n\n$$\nd(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=\\sqrt{5(1-0)^{2}-2 \\cdot 2(1-0)(-1+2)+1(-1+2)^{2}}=\\sqrt{2}\n$$", "2.20": "\\subsection*{2.20 Prove formula (2.14).}\n\nIn unbarred coordinates, the distance formula has the matrix form\n\n$$\nd(\\mathbf{x}, \\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|=\\sqrt{(\\mathbf{x}-\\mathbf{y})^{T}(\\mathbf{x}-\\mathbf{y})}\n$$\n\nNow, $\\overline{\\mathbf{x}}=A \\mathbf{x}$ or $\\mathbf{x}=B \\overline{\\mathbf{x}}$, where $B=A^{-1}$; so we have by substitution,\n\n$$\n\\begin{aligned}\nd(\\mathbf{x}, \\mathbf{y}) & =\\sqrt{(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})^{T}(B \\overline{\\mathbf{x}}-B \\overline{\\mathbf{y}})}=\\sqrt{(B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}}))^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} B^{T} B(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})}=\\sqrt{(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})^{T} G(\\overline{\\mathbf{x}}-\\overline{\\mathbf{y}})} \\\\\n& =d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})\n\\end{aligned}\n$$\n\nwhere $G \\equiv B^{T} B=\\left(A^{-1}\\right)^{T} A^{-1}=\\left(A^{T}\\right)^{-1} A^{-1}=\\left(A A^{T}\\right)^{-1}$, the last two equalities following from Problem 2.13 .", "2.21": "\\section*{RECTANGULAR COORDINATES}\n2.21 Suppose that $\\left(x^{i}\\right)=(x, y, z)$ and $\\left(\\bar{x}^{i}\\right)=(\\bar{x}, \\bar{y}, \\bar{z})$ (the use of superscripts here anticipates future notation) denote two rectangular coordinate systems at $O$ and that the direction angles of the $\\bar{x}^{i}$-axis relative to the $x$-, $y$-, and $z$-axes are $\\left(\\alpha_{i}, \\beta_{i}, \\gamma_{i}\\right), i=1,2,3$. Show that the correspondence between the coordinate systems is given by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, where $\\mathbf{x}=(x, y, z)$, $\\overline{\\mathbf{x}}=(\\bar{x}, \\bar{y}, \\bar{z})$, and where the matrix\n\n$$\nA=\\left[\\begin{array}{lll}\n\\cos \\alpha_{1} & \\cos \\beta_{1} & \\cos \\gamma_{1} \\\\\n\\cos \\alpha_{2} & \\cos \\beta_{2} & \\cos \\gamma_{2} \\\\\n\\cos \\alpha_{3} & \\cos \\beta_{3} & \\cos \\gamma_{3}\n\\end{array}\\right]\n$$\n\nis orthogonal.\n\nLet the unit vectors along the $\\bar{x}$-, $\\bar{y}$, and $\\bar{z}$-axes be $\\overline{\\mathbf{i}}=\\overrightarrow{O P}, \\overline{\\mathbf{j}}=\\overrightarrow{O Q}$, and $\\overline{\\mathbf{k}}=\\overrightarrow{O R}$, respectively (see Fig. 2-1). If $\\overline{\\mathbf{x}}$ is the position vector of any point $W(x, y, z)$, then\n\n$$\n\\overline{\\mathbf{x}}=\\bar{x} \\overline{\\mathbf{i}}+\\bar{y} \\overline{\\mathbf{j}}+\\bar{z} \\overline{\\mathbf{k}}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-028}\n\\end{center}\n\nFig. 2-1\n\nWe know that the $(x, y, z)$-coordinates of $P$ are $\\left(\\cos \\alpha_{1}, \\cos \\beta_{1}, \\cos \\gamma_{1}\\right)$. Similar statements hold for the coordinates of $Q$ and $R$, respectively. Hence:\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{i}} & =\\left(\\cos \\alpha_{1}\\right) \\mathbf{i}+\\left(\\cos \\beta_{1}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{1}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{j}} & =\\left(\\cos \\alpha_{2}\\right) \\mathbf{i}+\\left(\\cos \\beta_{2}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{2}\\right) \\mathbf{k} \\\\\n\\overline{\\mathbf{k}} & =\\left(\\cos \\alpha_{3}\\right) \\mathbf{i}+\\left(\\cos \\beta_{3}\\right) \\mathbf{j}+\\left(\\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nSubstituting these into the expression for $\\overline{\\mathbf{x}}$ and collecting coefficients of $\\mathbf{i}, \\mathbf{j}$, and $\\mathbf{k}$ :\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{x}}=\\left(\\bar{x} \\cos \\alpha_{1}\\right. & \\left.+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\\right) \\mathbf{i} \\\\\n& +\\left(\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3}\\right) \\mathbf{j} \\\\\n& +\\left(\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\\right) \\mathbf{k}\n\\end{aligned}\n$$\n\nHence, the $x$-coordinate of $W$ is the coefficient of $\\mathbf{i}$, or\n\nSimilarly,\n\n$$\nx=\\bar{x} \\cos \\alpha_{1}+\\bar{y} \\cos \\alpha_{2}+\\bar{z} \\cos \\alpha_{3}\n$$\n\n$$\n\\begin{aligned}\n& y=\\bar{x} \\cos \\beta_{1}+\\bar{y} \\cos \\beta_{2}+\\bar{z} \\cos \\beta_{3} \\\\\n& z=\\bar{x} \\cos \\gamma_{1}+\\bar{y} \\cos \\gamma_{2}+\\bar{z} \\cos \\gamma_{3}\n\\end{aligned}\n$$\n\nIn terms of the matrix $A$ defined above, we can write these three equations in the matrix form\n\n\n\\begin{equation*}\n\\mathbf{x}=A^{T} \\overline{\\mathbf{x}} \\tag{1}\n\\end{equation*}\n\n\nNow, the $(i, j)$-element of the matrix $A A^{T}$ is\n\n$$\n\\cos \\alpha_{i} \\cos \\alpha_{j}+\\cos \\beta_{i} \\cos \\beta_{j}+\\cos \\gamma_{i} \\cos \\gamma_{j}\n$$\n\nfor $i, j=1,2,3$. Note that the diagonal elements,\n\n$$\n\\left(\\cos \\alpha_{i}\\right)^{2}+\\left(\\cos \\beta_{i}\\right)^{2}+\\left(\\cos \\gamma_{i}\\right)^{2} \\quad(i=1,2,3)\n$$\n\nare the three quantities $\\overrightarrow{O P} \\cdot \\overrightarrow{O P}, \\overrightarrow{O Q} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O R} \\cdot \\overrightarrow{O R}$; i.e., they are unity. If $i \\neq j$, then the corresponding element of $A A^{T}$ is either $\\overrightarrow{O P} \\cdot \\overrightarrow{O Q}, \\overrightarrow{O P} \\cdot \\overrightarrow{O R}$, or $\\overrightarrow{O Q} \\cdot \\overrightarrow{O R}$, and is therefore zero (since these vectors are mutually orthogonal). Hence, $A A^{T}=I$ (and also $A^{T} A=I$ ), and, from (1),\n\n$$\nA \\mathbf{x}=A A^{T} \\overrightarrow{\\mathbf{x}}=\\overline{\\mathbf{x}}\n$$", "2.22": "\\subsection*{2.22 A curvilinear coordinate system $(\\bar{x}, \\bar{y})$ is defined in terms of rectangular coordinates $(x, y)$ by}\n\n\\begin{align*}\n& \\bar{x}=x^{2}-x y  \\tag{1}\\\\\n& \\bar{y}=x y\n\\end{align*}\n\nShow that in the barred coordinate system the equation of the line $y=x-1$ is $\\bar{y}=\\bar{x}^{2}-\\bar{x}$. [In the alibi interpretation, (1) deforms the straight line into a parabola.]\n\nIt helps initially to parameterize the equation of the line as $x=t, y=t-1$. Substitution of $x=t$, $y=t-1$ in the change-of-coordinates formula gives the parametric equations of the line in the barred coordinate system:\n\n\\begin{align*}\n& \\bar{x}=t^{2}-t(t-1)=t \\\\\n& \\bar{y}=t(t-1)=t^{2}-t \\tag{2}\n\\end{align*}\n\nNow $t$ may be eliminated from (2) to give $\\bar{y}=\\bar{x}^{2}-\\bar{x}$.", "2.23": "\\subsection*{2.23 Suppose that under a change of coordinates, $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)(1 \\leqq i \\leqq n)$, the realvalued vector functions $\\left(\\bar{T}_{i}\\right)$ and $\\left(T_{i}\\right)$ are related by the formula}\n\n\\begin{equation*}\n\\bar{T}_{i}=T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}} \\tag{1}\n\\end{equation*}\n\n\\textit{Find the transformation rule for the partial derivatives of $\\left(T_{i}\\right)$-that is, express the $\\partial \\bar{T}_{i} / \\partial \\bar{x}_{j}$ in terms of the $\\partial T_{r} / \\partial x_{s}$-given that all second-order partial derivatives are zero.}\n\nBegin by taking the partial derivative with respect to $\\bar{x}_{j}$ of both sides of (1), using the product rule:\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{T_{r} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}=\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}+T_{r} \\frac{\\partial}{\\partial \\bar{x}_{j}}\\left\\{\\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\\right\\}\n$$\n\nBy assumption, the second term on the right is zero; and, by the chain rule,\n\n$$\n\\frac{\\partial T_{r}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}}\n$$\n\nConsequently, the desired transformation rule is\n\n$$\n\\frac{\\partial \\bar{T}_{i}}{\\partial \\bar{x}_{j}}=\\frac{\\partial T_{r}}{\\partial x_{s}} \\frac{\\partial x_{s}}{\\partial \\bar{x}_{j}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{i}}\n$$", "3.1": "\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{1} x^{2}  \\tag{1}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis a one-one mapping between the regions $x^{2}>0$ and $\\bar{x}^{2}>0$, and that\n\n\\[\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=x^{\\prime} x^{2}  \\tag{2}\\\\\n\\bar{x}^{2}=\\left(x^{2}\\right)^{2}\n\\end{array} \\quad \\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}\nx^{1}=-\\bar{x}^{1} / \\sqrt{\\bar{x}^{2}} \\\\\nx^{2}=-\\sqrt{\\bar{x}^{2}}\n\\end{array}\\right.\\right.\n\\]\n\nis one-one between $x^{2}<0$ and $\\bar{x}^{2}>0$. Note that the two regions of the $x^{1} x^{2}$-plane are separated by the line on which the Jacobian of $\\mathscr{T}$ vanishes.\n\n(b) From Example 3.2,\n\n$$\nJ=\\left[\\begin{array}{cc}\nx^{2} & x^{1} \\\\\n0 & 2 x^{2}\n\\end{array}\\right] \\quad \\text { and so } \\quad J^{-1}=\\frac{1}{2\\left(x^{2}\\right)^{2}}\\left[\\begin{array}{cc}\n2 x^{2} & -x^{1} \\\\\n0 & x^{2}\n\\end{array}\\right]\n$$\n\nvalid in both regions $x^{2}>0$ and $x^{2}<0$. Now, on $\\bar{x}^{2}>0$, differentiation of the inverse transformation (1), followed by a change back to unbarred coordinates, yields\n\n$$\n\\bar{J}=\\left[\\begin{array}{ll}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}} & \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(\\bar{x}^{2}\\right)^{-1 / 2} & -\\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & \\frac{1}{2}\\left(\\bar{x}^{2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & \\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\n$$\n\nIt is seen that on $x^{2}>0, \\bar{J}=J^{-1}$.\n\nSimilarly, from (2), with $x^{2}<0$,\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n-\\left(\\bar{x}^{2}\\right)^{-1 / 2} & \\frac{1}{2} \\bar{x}^{1}\\left(\\bar{x}^{2}\\right)^{-3 / 2} \\\\\n0 & -\\frac{1}{2}\\left(\\bar{x}^{-2}\\right)^{-1 / 2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n+\\left(x^{2}\\right)^{-1} & -\\frac{1}{2} x^{1}\\left(x^{2}\\right)^{-2} \\\\\n0 & +\\frac{1}{2}\\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=J^{-1}\n$$", "3.2": "\\[\n\\text{Solved Problems}\n\\]\n\n\\[\n\\section*{CHANGE OF COORDINATES}\n\\]\n\n\\[\n3.2 \\text{ For polar coordinates as defined by (3.2), (a) calculate the Jacobian matrix of } \\mathscr{T} \\text{ and infer the region over which } \\mathscr{T} \\text{ is bijective; } (b) \\text{ calculate the Jacobian matrix of } \\mathscr{T}^{-1} \\text{ for the region}\n\\]\n\n\\[\n\\{\n(r, \\theta) \\mid r>0,-\\pi / 2<\\theta<\\pi / 2\n\\}\n\\]\n\n\\[\n\\text{i.e., the right half-plane, and verify that it is the inverse of the matrix of } (a).\n\\]\n\n\\[\n(a)\n\\]\n\n\\[\nJ=\\left[\\begin{array}{cc}\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\cos x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\cos x^{2}\\right) \\\\\n\\frac{\\partial}{\\partial x^{1}}\\left(x^{1} \\sin x^{2}\\right) & \\frac{\\partial}{\\partial x^{2}}\\left(x^{1} \\sin x^{2}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\cos x^{2} & -x^{1} \\sin x^{2} \\\\\n\\sin x^{2} & x^{1} \\cos x^{2}\n\\end{array}\\right]\n\\]\n\n\\[\n\\text{whence } \\mathscr{J}=x^{1} \\equiv r. \\text{ Therefore, } \\mathscr{T} \\text{ is bijective on the open set } r>0, \\text{ which is the entire plane punctured at the origin.}\n\\]\n\n\\[\n(b) \\text{ For } \\mathscr{T}^{-1} \\text{ we have, over the right half-plane,}\n\\]\n\n\\[\n\\begin{gathered}\n\\frac{\\partial x^{1}}{\\partial \\bar{x}^{1}}=\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\quad \\frac{\\partial x^{1}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{\\partial x^{2}}{\\partial \\bar{x}^{1}}=\\frac{1}{1+\\left(\\bar{x}^{2} / \\bar{x}^{1}\\right)^{2}}\\left[-\\frac{\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}}\\right]=\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} \\quad \\frac{\\partial x^{2}}{\\partial \\bar{x}^{2}}=\\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{gathered}\n\\]\n\n\\[\n\\text{and so}\n\\]\n\n\\[\n\\bar{J}=\\left[\\begin{array}{cc}\n\\frac{\\bar{x}^{1}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} & \\frac{\\bar{x}^{2}}{\\sqrt{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}} \\\\\n\\frac{-\\bar{x}^{2}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}} & \\frac{\\bar{x}^{1}}{\\left(\\bar{x}^{1}\\right)^{2}+\\left(\\bar{x}^{2}\\right)^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]\n\\]\n\n\\[\n\\text{Now compute } J^{-1} :\n\\]\n\n\\[\nJ^{-1}=\\frac{1}{x^{1}}\\left[\\begin{array}{cc}\nx^{1} \\cos x^{2} & x^{1} \\sin x^{2} \\\\\n-\\sin x^{2} & \\cos x^{2}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\cos x^{2} & \\sin x^{2} \\\\\n-\\frac{\\sin x^{2}}{x^{1}} & \\frac{\\cos x^{2}}{x^{1}}\n\\end{array}\\right]=\\bar{J}\n\\]", "3.3": "\\section*{CONTRAVARIANT VECTORS}\n3.3 If $\\mathbf{V}=\\left(T^{i}\\right)$ is a contravariant vector, show that the partial derivatives $T_{j}^{i} \\equiv \\partial T^{i} / \\partial x^{j}$, defined in each coordinate system, transform according to the rule\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{r} \\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nDifferentiate both sides of\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $\\bar{x}^{j}$, using the product rule:\n\n\n\\begin{equation*}\n\\bar{T}_{j}^{i} \\equiv \\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{j}}=\\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy the chain rule for partial derivatives, $(2.15)$,\n\n$$\n\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\equiv T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { and } \\quad \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left[\\frac{\\partial}{\\partial x^{s}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\right] \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nSubstituting these expressions into (1) yields the desired formula.", "3.4": "\\section*{CONTRAVARIANT VECTORS}\n3.4 Suppose that $\\left(T^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{2}$ and that $\\left(T^{i}\\right)=\\left(x^{2}, x^{1}\\right)$ in the $\\left(x^{i}\\right)$-system.\n\nCalculate $\\left(\\bar{T}^{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system, under the change of coordinates\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{2}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\nBy definition of contravariance,\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}=T^{1} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{1}}+T^{2} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{2}}\n$$\n\nNote that the top row of the Jacobian matrix $J$ is needed for the case $i=1$, and the bottom row is needed for $i=2$.\n\n$$\nJ=\\left[\\begin{array}{ll}\n\\frac{\\partial \\bar{x}^{1}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{1}}{\\partial x^{2}} \\\\\n\\frac{\\partial \\bar{x}^{2}}{\\partial x^{1}} & \\frac{\\partial \\bar{x}^{2}}{\\partial x^{2}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 2 x^{2} \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\n$$\n\nThus,\n\n$$\n\\bar{T}^{1}=T^{1}(0)+T^{2}\\left(2 x^{2}\\right)=2 x^{1} x^{2} \\quad \\bar{T}^{2}=T^{1}\\left(x^{2}\\right)+T^{2}\\left(x^{1}\\right)=\\left(x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\n$$\n\nwhich, in terms of barred coordinates, are\n\n$$\n\\bar{T}^{1}=2 \\bar{x}^{2} \\quad \\bar{T}^{2}=\\bar{x}^{1}+\\frac{\\left(\\bar{x}^{2}\\right)^{2}}{\\bar{x}^{1}}\n$$", "3.5": "\\section*{CONTRAVARIANT VECTORS}\n3.5 Show that a contravariant vector can be constructed the components of which take on a given set of values $(a, b, c, \\ldots)$ in some particular coordinate system. (The prescribed values may be point functions.)\n\nLet $(a, b, c, \\ldots) \\equiv\\left(a^{i}\\right)$ be the given values to be assigned in the coordinate system $\\left(x^{i}\\right)$. Set $V^{i}=a^{i}$ for the values in $\\left(x^{i}\\right)$, and for any other admissible coordinate system $\\left(\\bar{x}^{i}\\right)$, set $\\bar{V}^{i}=a^{r}\\left(\\partial \\bar{x}^{i} / \\partial x^{r}\\right)$. To show that $\\left(V^{i}\\right)$ is a contravariant tensor, let $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ be any two admissible coordinate systems. Then, $y^{i}=f^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ and $\\bar{y}^{i}=g^{i}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$, and, by definition, the values of $\\left(V^{i}\\right)$ in $\\left(y^{i}\\right)$ and $\\left(\\bar{y}^{i}\\right)$ are, respectively, $T^{i}=a^{r}\\left(\\partial y^{i} / \\partial x^{r}\\right)$ and $\\bar{T}^{i}=a^{r}\\left(\\partial \\bar{y}^{i} / \\partial x^{r}\\right)$. But, by the chain rule,\n\n$$\n\\bar{T}^{i}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial x^{r}}=a^{r} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\frac{\\partial y^{s}}{\\partial x^{r}}=T^{s} \\frac{\\partial \\bar{y}^{i}}{\\partial y^{s}} \\quad \\text { QED }\n$$", "3.6": "\\section*{COVARIANT VECTORS}\n3.6 Calculate $\\left(\\bar{T}_{i}\\right)$ in the $\\left(\\bar{x}^{i}\\right)$-system if $\\mathbf{V}=\\left(T_{i}\\right) \\equiv\\left(x^{2}, x^{1}+2 x^{2}\\right)$ is a covariant vector under the coordinate transformation of Problem 3.4.\n\nTo avoid radicals, compute $J^{-1}$ in terms of $\\left(x^{i}\\right)$ :\n\n$$\nJ^{-1}=\\left[\\begin{array}{cc}\n\\frac{-x^{1}}{2\\left(x^{2}\\right)^{2}} & \\frac{1}{x^{2}} \\\\\n\\frac{1}{2 x^{2}} & 0\n\\end{array}\\right]\n$$\n\nBy covariance,\n\n$$\n\\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}=T_{1} \\frac{\\partial x^{1}}{\\partial \\bar{x}^{i}}+T_{2} \\frac{\\partial x^{2}}{\\partial \\bar{x}^{i}} \\quad(i=1,2)\n$$\n\nFor $i=1$, read off the partials from the first column of $J^{-1}$ :\n\n$$\n\\bar{T}_{1}=T_{1}\\left(-x^{1} / 2\\left(x^{2}\\right)^{2}\\right)+T_{2}\\left(1 / 2 x^{2}\\right)=-x^{1} / 2 x^{2}+x^{1} / 2 x^{2}+1=1\n$$\n\nSimilarly, for $i=2$, use the second column of $J^{-1}$ :\n\n$$\n\\bar{T}_{2}=T_{1}\\left(1 / x^{2}\\right)+T_{2}(0)=x^{2}\\left(1 / x^{2}\\right)=1\n$$\n\nHence, $\\left(\\bar{T}_{i}\\right)=(1,1)$ at all points in the $\\left(\\bar{x}^{i}\\right)$-system $\\left(\\bar{x}^{1}=0\\right.$ excluded $)$.", "3.7": "\\section*{COVARIANT VECTORS}\n3.7 Use the fact that $\\nabla f$ is a covariant vector (Example 3.5) to bring the partial differential equation\n\n\\begin{equation*}\nx \\frac{\\partial f}{\\partial x}=y \\frac{\\partial f}{\\partial y} \\tag{1}\n\\end{equation*}\n\ninto simpler form by the change of variables $\\bar{x}=x y, \\bar{y}=(y)^{2}$; then solve.\n\nWrite $\\nabla f=(\\partial f / \\partial x, \\partial f / \\partial y) \\equiv\\left(T_{i}\\right),\\left(x^{1}, x^{2}\\right)=(x, y),\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)=(\\bar{x}, \\bar{y})$, and\n\n$$\n\\bar{T}_{i} \\equiv \\frac{\\partial \\bar{f}}{\\partial \\bar{x}^{i}}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nAgain calculating $J$ first, then its inverse, we have\n\nso that\n\n$$\n\\left(\\frac{\\partial x^{i}}{\\partial \\bar{x}^{j}}\\right) \\equiv J^{-1}=\\left[\\begin{array}{cc}\ny & x \\\\\n0 & 2 y\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{cc}\n\\frac{1}{y} & \\frac{-x}{2(y)^{2}} \\\\\n0 & \\frac{1}{2 y}\n\\end{array}\\right]\n$$\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{x}} \\equiv \\bar{T}_{1}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{1}}=T_{1} \\cdot \\frac{1}{y}+T_{2} \\cdot 0=\\frac{1}{y} \\frac{\\partial f}{\\partial x} \\\\\n& \\frac{\\partial \\bar{f}}{\\partial \\bar{y}} \\equiv \\bar{T}_{2}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{2}}=T_{1} \\cdot \\frac{-x}{2(y)^{2}}+T_{2} \\cdot \\frac{1}{2 y}=-\\frac{x}{2(y)^{2}} \\frac{\\partial f}{\\partial x}+\\frac{1}{2 y} \\frac{\\partial f}{\\partial y}\n\\end{aligned}\n$$\n\nBut, by (1),\n\n$$\n\\frac{\\partial \\bar{f}}{\\partial \\bar{y}}=\\frac{1}{2(y)^{2}}\\left(-x \\frac{\\partial f}{\\partial x}+y \\frac{\\partial f}{\\partial y}\\right)=0\n$$\n\nwhich implies that $\\bar{f}=F(\\bar{x})$, a function of $\\bar{x}$ alone; therefore, $f=F(x y)$ is the general solution to (1).", "3.8": "\\subsection*{3.8 Prove Theorem 3.1.}\nWe must show that if $\\left(S^{i}\\right)$ and $\\left(T_{i}\\right)$ are tensors of the indicated types and order, then the quantity $E \\equiv S^{i} T_{i}$ is invariant with respect to coordinate changes; that is, $\\bar{E}=E$, where $\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}$. But observe that\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}_{i}=T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}\n$$\n\nso that, in view of $(3.7)$,\n\n$$\n\\bar{E}=\\bar{S}^{i} \\bar{T}_{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}}=S^{r} T_{s} \\delta_{r}^{s}=S^{r} T_{r}=E", "3.9": "\\subsection*{3.9 Show that under linear coordinate changes of $\\mathbf{R}^{n}, \\bar{x}^{i}=a_{j}^{i} x^{j} \\quad\\left(\\left|a_{j}^{i}\\right| \\neq 0\\right)$, the equation of a hyperplane $A_{i} x^{i}=1$ is invariant provided the normal vector $\\left(A_{i}\\right)$ is covariant.}\n\nIn view of Theorem 3.1, it suffices to show that $\\left(T^{i}\\right)=\\left(x^{i}\\right)$ is a contravariant affine tensor. But this is immediate:\n\n$$\n\\bar{T}^{i} \\equiv \\bar{x}^{i}=a_{j}^{i} x^{j} \\equiv a_{j}^{i} T^{j}\n$$\n\nwhich is the transformation law (3.21).", "3.10": "\\section*{SECOND-ORDER CONTRAVARIANT TENSORS}\n3.10 Suppose that the components of a contravariant tensor $T$ of order 2 in a coordinate system $\\left(x^{i}\\right)$ of $\\mathbf{R}^{2}$ are $T^{11}=1, T^{12}=1, T^{21}=-1$, and $T^{22}=2$. (a) Find the components $\\bar{T}^{j j}$ of $\\mathbf{T}$ in the $\\left(\\bar{x}^{i}\\right)$-system, connected to the $\\left(x^{i}\\right)$-system via\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\left(x^{1}\\right)^{2} \\neq 0 \\\\\n& \\bar{x}^{2}=x^{1} x^{2}\n\\end{aligned}\n$$\n\n(b) Compute the values of the $\\bar{T}^{i j}$ at the point which corresponds to $x^{1}=1, x^{2}=-2$.\n\nFor economy of effort, the problem will be worked using matrices.\n\n(a) Writing\n\n$$\nJ_{j}^{i} \\equiv J_{i}^{\\prime j} \\equiv \\frac{\\partial \\bar{x}^{i}}{\\partial x^{j}}\n$$\n\nwe have from $(2.1 b)$,\n\n$$\n\\bar{T}^{i j}=T^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{s}}=J_{r}^{i} T^{r s} J_{j}^{s}\n$$\n\nThat is,\n\n$$\n\\begin{aligned}\n\\bar{T} & =J T J^{T} \\\\\n& =\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\n2 x^{1} & x^{2} \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n(b) At the point $(1,-2)$,\n\n$$\n\\begin{array}{ll}\n\\bar{T}^{11}=4(1)^{2}=4 & \\bar{T}^{12}=2(1)(-2)+2(1)^{2}=-2 \\\\\n\\bar{T}^{21}=2(1)(-2)-2(1)^{2}=-6 & \\bar{T}^{22}=2(1)^{2}+(-2)^{2}=6\n\\end{array}\n$$", "3.11": "3.11 Show that if $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ are contravariant vectors on $\\mathbf{R}^{n}$, the matrix $\\left[U^{i j}\\right] \\equiv\\left[S^{i} T^{j}\\right]_{n n}$, defined in this manner for all coordinate systems, represents a contravariant tensor of order 2.\n\nMultiply\n\nto obtain\n\n$$\n\\bar{S}^{i}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\quad \\text { and } \\quad \\bar{T}^{j}=T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\n$$\n\\bar{U}^{i j}=\\bar{S}^{i} \\bar{T}^{j}=S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\cdot T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}=U^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the tensor law. (The notion of the \"outer product\" of two tensors will be further developed in Chapter 4.)", "3.12": "\\section*{SECOND-ORDER COVARIANT TENSORS}\n3.12 Show that if $T_{i}$ are the components of covariant vector $\\mathbf{T}$, then $S_{i j} \\equiv T_{i} T_{j}-T_{j} T_{i}$ are the components of a skew-symmetric covariant tensor $\\mathbf{S}$.\n\nThe skew-symmetry is obvious. From the transformation law for $\\mathbf{T}$,\n\nor\n\n$$\n\\begin{gathered}\n\\bar{T}_{i} \\bar{T}_{j}-\\bar{T}_{j} \\bar{T}_{i}=T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\cdot T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\cdot T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\\\\n=T_{r} T_{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-T_{s} T_{r} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\left(T_{r} T_{s}-T_{s} T_{r}\\right) \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\\\\n\\bar{S}_{i j}=S_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n\\end{gathered}\n$$\n\nwhich establishes the covariant tensor character of $\\mathbf{S}$.", "3.13": "\\section*{CONTRAVARIANT VECTORS}\n3.13 If $\\mathbf{V}=\\left(T^{i}\\right)$ is a contravariant vector, show that the partial derivatives $T_{j}^{i} \\equiv \\partial T^{i} / \\partial x^{j}$, defined in each coordinate system, transform according to the rule\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{r} \\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nDifferentiate both sides of\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\nwith respect to $\\bar{x}^{j}$, using the product rule:\n\n\\begin{equation*}\n\\bar{T}_{j}^{i} \\equiv \\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{j}}=\\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}+T^{r} \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\tag{1}\n\\end{equation*}\n\nBy the chain rule for partial derivatives, $(2.15)$,\n\n$$\n\\frac{\\partial T^{r}}{\\partial \\bar{x}^{j}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\equiv T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { and } \\quad \\frac{\\partial}{\\partial \\bar{x}^{j}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left[\\frac{\\partial}{\\partial x^{s}}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\right] \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nSubstituting these expressions into (1) yields the desired formula.", "3.14": "\\section*{SECOND-ORDER COVARIANT TENSORS}\n3.14 Let $\\mathbf{U}=\\left(U_{i j}\\right)$ be a covariant tensor of order 2 . Under the same coordinate change as in Problem 3.10, (a) calculate the components $\\bar{U}_{i j}$, if $U_{11}=x^{2}, U_{12}=U_{21}=0, U_{22}=x^{1} ;(b)$ verify that the quantity $T^{i j} U_{i j}=E$ is an invariant, where the $T^{i j}$ and $\\bar{T}^{i j}$ are obtained from Problem 3.10.\n\n(a) In terms of the inverse Jacobian matrix, the covariant transformation law is\n\n$$\n\\bar{U}_{i j}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} U_{r s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\bar{J}_{i}^{r} U_{r s} \\bar{J}_{j}^{s}=\\bar{J}_{r}^{\\prime i} U_{r s} \\bar{J}_{j}^{s} \\quad \\text { or } \\quad \\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nSubstituting\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n2 x^{1} & 0 \\\\\nx^{2} & x^{1}\n\\end{array}\\right]^{-1}=\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\quad U=\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\n$$\n\nwe find\n\n$$\n\\bar{U}=\\left[\\begin{array}{cc}\n\\frac{1}{2 x^{1}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n0 & \\frac{1}{x^{1}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]\\left[\\begin{array}{rr}\n\\frac{1}{2 x^{1}} & 0 \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right]\n$$\n\nfrom which the $\\bar{U}_{i j}$ may be read off.\n\n(b) Continuing in the matrix approach, we note that $E$ is the trace (sum of diagonal elements) of the matrix $T U^{T}$.\n\n$$\n\\begin{aligned}\n& T U^{T}=\\left[\\begin{array}{rr}\n1 & 1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{cc}\nx^{2} & 0 \\\\\n0 & x^{1}\n\\end{array}\\right]=\\left[\\begin{array}{rr}\nx^{2} & x^{1} \\\\\n-x^{2} & 2 x^{1}\n\\end{array}\\right] \\\\\n& E=x^{2}+2 x^{1}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\bar{T} \\bar{U}^{T}=\\left[\\begin{array}{cc}\n4\\left(x^{1}\\right)^{2} & 2 x^{1} x^{2}+2\\left(x^{1}\\right)^{2} \\\\\n2 x^{1} x^{2}-2\\left(x^{1}\\right)^{2} & 2\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{x^{1} x^{2}+\\left(x^{2}\\right)^{2}}{4\\left(x^{1}\\right)^{3}} & -\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} \\\\\n-\\frac{x^{2}}{2\\left(x^{1}\\right)^{2}} & \\frac{1}{x^{1}}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{cc}\n0 & 2 x^{1} \\\\\n-\\frac{3 x^{2}}{2} & x^{2}+2 x^{1}\n\\end{array}\\right] \\\\\n& \\bar{E}=x^{2}+2 x^{1}=E\n\\end{aligned}\n$$", "3.15": "\\section*{CONTRAVARIANT VECTORS}\n3.15 Prove Theorem 3.2.\n\nObserve first of all that if a covariant matrix (second-order tensor) $U$ has inverse $V$ in unbarred coordinates, then $\\bar{U}$ has inverse $\\bar{V}$ in barred coordinates; i.e., $(\\bar{U})^{-1}=\\overline{U^{-1}}$. Now, by Problem 3.14(a),\n\n$$\n\\bar{U}=\\bar{J}^{T} U \\bar{J}\n$$\n\nInverting both sides of this matrix equation, applying Problem 2.13 , and recalling that $J \\bar{J}=I$, we obtain\n\n$$\n\\overline{U^{-1}}=\\bar{J}^{-1} U^{-1}\\left(\\bar{J}^{T}\\right)^{-1}=J U^{-1} J^{T}\n$$\n\nwhich is the contravariant law for $U^{-1}$ [see Problem 3.10(a)].", "3.16": "\\section*{MIXED TENSORS}\n3.16 Compute the formulas for the tensor components $\\left(\\bar{T}_{j}^{i}\\right)$ in polar coordinates in terms of $\\left(T_{j}^{i}\\right)$ in rectangular coordinates, if the tensor is symmetric in rectangular coordinates. (In contrast to Section 3.1, it is now the curvilinear coordinates that are barred.)\n\nThe general formula calls for the calculations\n\n$$\n\\bar{T}_{j}^{i}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} T_{s}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad\\left(T_{j}^{i}=T_{i}^{j}\\right)\n$$\n\nUsing $(2.1 b)$, this may be written in matrix form as\n\n\n\\begin{equation*}\n\\bar{T}=J T J^{-1}=\\bar{J}^{-1} T \\bar{J} \\tag{1}\n\\end{equation*}\n\n\nwhere $T=\\left[T_{j}^{i}\\right]_{22}$ and where\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right]\n$$\n\nis the Jacobian matrix of the transformation from $(r, \\theta)$ to $(x, y)$. Thus,\n\n$$\n\\begin{aligned}\n& \\bar{T}=\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} & T_{2}^{1} \\\\\nT_{2}^{1} & T_{2}^{2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\cos \\theta & -r \\sin \\theta \\\\\n\\sin \\theta & r \\cos \\theta\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\cos \\theta & \\sin \\theta \\\\\n-\\frac{\\sin \\theta}{r} & \\frac{\\cos \\theta}{r}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos \\theta+T_{2}^{1} \\sin \\theta & -r T_{1}^{1} \\sin \\theta+r T_{2}^{1} \\cos \\theta \\\\\nT_{2}^{1} \\cos \\theta+T_{2}^{2} \\sin \\theta & -r T_{2}^{1} \\sin \\theta+r T_{2}^{2} \\cos \\theta\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nThe final matrix multiplication can be carried out routinely, simplifying by means of trigonometric identities:\n\n$$\n\\bar{T}=\\left[\\begin{array}{cc}\nT_{1}^{1} \\cos ^{2} \\theta+T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\sin ^{2} \\theta & -\\frac{r}{2} T_{1}^{1} \\sin 2 \\theta+r T_{2}^{1} \\cos 2 \\theta+\\frac{r}{2} T_{2}^{2} \\sin 2 \\theta \\\\\n-T_{1}^{1} \\frac{\\sin 2 \\theta}{2 r}+T_{2}^{1} \\frac{\\cos 2 \\theta}{r}+T_{2}^{2} \\frac{\\sin 2 \\theta}{2 r} & T_{1}^{1} \\sin ^{2} \\theta-T_{2}^{1} \\sin 2 \\theta+T_{2}^{2} \\cos ^{2} \\theta\n\\end{array}\\right]\n$$\n\nObserve that $\\bar{T}$ does not share the symmetry of $T: \\quad \\bar{T}_{1}^{2}=r^{-2} \\bar{T}_{2}^{1}$.", "3.17": "\\section*{MIXED TENSORS}\n3.17 Prove that the determinant of a mixed tensor of order two is invariant.\n\nBy (1) of Problem 3.16, we have-whether or not $T$ is symmetric-\n\n$$\n|\\bar{T}|=\\left|J T J^{-1}\\right|=|J||T|\\left|J^{-1}\\right|=\\mathscr{J}|T| \\mathscr{J}^{-1}=|T|\n$$", "3.18": "\\section*{GENERAL TENSORS}\n3.18 Display the transformation law for a third-order tensor that is contravariant of order two and covariant of order one.\n\nTake $p=2$ and $q=1$ in Definition 7 and, to avoid unnecessary subscripts, write $i, j, k, r, s, t$ in place of $i_{1}, i_{2}, j_{1}, r_{1}, r_{2}, s_{1}$. Then (3.14) gives\n\n$$\n\\bar{T}_{k}^{i j}=T_{t}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$", "3.19": "\\section*{GENERAL TENSORS}\n3.19 Let $\\mathbf{T}=\\left(T_{k l m}^{i j}\\right)$ denote a tensor of the order and type indicated by the indices. Prove that $\\mathbf{S}=\\left(T_{k}\\right) \\equiv\\left(T_{k i j}^{i j}\\right)$ is a covariant vector.\n\nThe transformation law (3.14) for $\\mathbf{T}$ is\n\n$$\n\\bar{T}_{k l m}^{i j}=T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{l}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{m}}\n$$\n\nSet $l=i, m=j$ and sum:\n\n$$\n\\begin{aligned}\n\\bar{T}_{k} \\equiv \\bar{T}_{k i j}^{i j} & =T_{t u v}^{r s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}=T_{t u v}^{r s}\\left(\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{u}}{\\partial \\bar{x}^{i}}\\right)\\left(\\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{v}}{\\partial \\bar{x}^{j}}\\right) \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{t u v}^{r s} \\delta_{r}^{u} \\delta_{s}^{v} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{t r s}^{r s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\equiv T_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n\\end{aligned}\n$$", "3.20": "\\section*{CARTESIAN TENSORS}\n3.20 Show that the permutation symbol $\\left(e_{i j}\\right)$ defines a direct cartesian tensor over $\\mathbf{R}^{2}$. Assume that $e_{i j}$ is defined the same way for all rectangular coordinate systems.\n\nIf the coordinate change is $\\bar{x}_{i}=a_{i j} x_{j}$, where $\\left(a_{i j}\\right)^{T}\\left(a_{k l}\\right)=\\left(\\delta_{p q}\\right)$ and\n\n$$\n\\left|a_{i j}\\right|=a_{11} a_{22}-a_{12} a_{21}=1\n$$\n\nwe must establish the cartesian tensor law (3.22):\n\n$$\n\\bar{e}_{i j}=e_{r s} a_{i r} a_{j s} \\quad(n=2)\n$$\n\nWe examine separately the four possible cases:\n\n$$\n\\begin{aligned}\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{1 r} a_{1 s}=a_{11} a_{12}-a_{12} a_{11}=0=\\bar{e}_{11} \\\\\n\\boldsymbol{i}=\\mathbf{1}, \\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{1 r} a_{2 s}=a_{11} a_{22}-a_{12} a_{21}=1=\\bar{e}_{12} \\\\\n\\boldsymbol{i}=\\mathbf{2}, \\boldsymbol{j}=\\mathbf{1} & e_{r s} a_{2 r} a_{1 s}=a_{21} a_{12}-a_{22} a_{11}=-1=\\bar{e}_{21} \\\\\n\\boldsymbol{i}=\\boldsymbol{j}=\\mathbf{2} & e_{r s} a_{2 r} a_{2 s}=a_{21} a_{22}-a_{22} a_{21}=0=\\bar{e}_{22}\n\\end{aligned}\n$$", "3.21": "\\section*{CARTESIAN TENSORS}\n3.21 Prove that (a) the coefficients $c_{i j}$ of the quadratic form $c_{i j} x^{i} x^{j}=1$ transform as an affine tensor and $(b)$ the trace $c_{i i}$ of $\\left(c_{i j}\\right)$ is a cartesian invariant.\n\n(a) If $\\bar{x}^{i}=a_{j}^{i} x^{j}$ and $x^{i}=b_{j}^{i} \\bar{x}^{j}$, where $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{-1}$, the quadratic form goes over into\n\n$$\n1=c_{i j}\\left(b_{r}^{i} \\bar{x}^{r}\\right)\\left(b_{s}^{j} \\bar{x}^{s}\\right) \\equiv \\bar{c}_{r s} \\bar{x}^{r} \\bar{x}^{s}\n$$\n\nwith $\\bar{c}_{r s}=b_{r}^{i} b_{s}^{j} c_{i j}$. But this formula is just (3.21) for a covariant affine tensor of order two.\n\n(b) Assuming an orthogonal transformation, $\\left(b_{j}^{i}\\right)=\\left(a_{j}^{i}\\right)^{T}$, we have\n\n$$\n\\bar{c}_{r s}=b_{r}^{i} a_{j}^{s} c_{i j}\n$$\n\nHence, $\\bar{c}_{r r}=\\left(b_{r}^{i} a_{j}^{r}\\right) c_{i j}=\\delta_{j}^{i} c_{i j}=c_{i i}$.", "3.22": "\\section*{GENERAL TENSORS}\n3.22 Establish the identity between the permutation symbol and the Kronecker delta:\n\n\n\\begin{equation*}\ne_{r i j} e_{r k l} \\equiv \\delta_{i k} \\delta_{j l}-\\delta_{i l} \\delta_{j k} \\tag{3.23}\n\\end{equation*}\n\n\nThe identity implies $n=3$, so that there are potentially $3^{4}=81$ separate cases to consider. However, this number can be quickly reduced to only 4 cases by the following reasoning: If either $i=j$ or $k=l$, then both sides vanish. For example, if $i=j$, then on the left $e_{r i j}=0$, and on the right,\n\n$$\n\\delta_{i k} \\delta_{j t}-\\delta_{j l} \\delta_{i k}=0\n$$\n\nHence, we need only consider the cases in which both $i \\neq j$ and $k \\neq l$. Upon writing out the sum on the left, two of the terms drop out, since $i \\neq j$ :\n\n$$\ne_{1 i j} e_{1 k l}+e_{2 i j} e_{2 k l}+e_{3 i j} e_{3 k l}=e_{1^{\\prime} 2^{\\prime} 3^{\\prime}} e_{1^{\\prime} k l} \\quad\\left(i=2^{\\prime}, j=3^{\\prime}\\right)\n$$\n\nwhere $\\left(1^{\\prime} 2^{\\prime} 3^{\\prime}\\right)$ denotes some permutation of (123). Thus, there are left only two cases, each with two subcases.\n\nCase 1: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l} \\neq 0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Here, either $k=2^{\\prime}$ and $l=3^{\\prime}$ or $k=3^{\\prime}$ and $l=2^{\\prime}$. If the former, then the left member of (3.23) is +1 , while the right member equals\n\n$$\n\\delta_{2^{\\prime} 2^{\\prime}} \\delta_{3^{\\prime} 3^{\\prime}}-\\delta_{2^{\\prime} 3^{\\prime}} \\delta_{3^{\\prime} 2^{\\prime}}=1-0=1\n$$\n\nIf the latter, then both members equal -1 , as can be easily verified.\n\nCase 2: $\\quad e_{1^{\\prime} 2^{\\prime} 3^{\\prime}}, e_{1^{\\prime} k l}=0$ (with $i=2^{\\prime}, j=3^{\\prime}$ ). Since $k \\neq l$, either $k=1^{\\prime}$ or $l=1^{\\prime}$. If $k=1^{\\prime}$, then the right member of (3.23) equals\n\n$$\n\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} l}-\\delta_{2^{\\prime} l^{\\prime}} \\delta_{3^{\\prime} 1^{\\prime}}=0-0=0\n$$\n\nIf $l=1^{\\prime}$, we have $\\delta_{2^{\\prime} k} \\delta_{3^{\\prime} 1^{\\prime}}-\\delta_{2^{\\prime} 1^{\\prime}} \\delta_{3^{\\prime} k}=0-0=0$.\n\nThis completes the examination of all cases, and the identity is established.", "4.1": "4.1 Show that if $\\lambda$ and $\\mu$ are invariants and $S^{i}$ and $T^{i}$ are components of contravariant vectors, the vector defined in all coordinate systems by $\\left(\\lambda S^{i}+\\mu T^{i}\\right)$ is a contravariant vector.\n\nSince $\\bar{\\lambda}=\\lambda$ and $\\bar{\\mu}=\\mu$,\n\nas desired.\n\n$$\n\\bar{\\lambda} \\bar{S}^{i}+\\bar{\\mu} \\bar{T}^{i}=\\lambda\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)+\\mu\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)=\\left(\\lambda S^{r}+\\mu T^{r}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$", "4.2": "4.2 Prove that (a) the array defined in each coordinate system by $\\left(T_{i j}-T_{j i}\\right)$, where $\\left(T_{i j}\\right)$ is a given covariant tensor, is a covariant tensor; $(b)$ the array defined in each coordinate system by $\\left(T_{j}^{i}-T_{i}^{j}\\right)$, where $\\left(T_{j}^{i}\\right)$ is a given mixed tensor, is not generally a tensor, but is a cartesian tensor.\n\n(a) By (4.2b), the array is a tensor if and only if $\\left(T_{i j}^{*}\\right) \\equiv\\left(T_{j i}\\right)$ is a covariant tensor. But the transformation law for $\\left(T_{i j}\\right)$ gives\n\n$$\n\\bar{T}_{j i}=T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{T}_{i j}^{*}=T_{s r}^{*} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich shows that $\\left(T_{i j}^{*}\\right)$ is indeed a covariant tensor.\n\n(b) We give a second proof [recall Problem 3.32(a)], based on (4.2b). The question is whether $\\left(U_{j}^{i}\\right) \\equiv\\left(T_{i}^{j}\\right)$ is a tensor. From the transformation law for $\\left(T_{j}^{i}\\right)$,\n\n$$\n\\bar{T}_{i}^{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad \\bar{U}_{j}^{i}=U_{r}^{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{r}}\n$$\n\nThus, $\\left(U_{i}^{i}\\right)$ does not obey a tensor law, unless, for all $p, q$,\n\n$$\n\\frac{\\partial \\bar{x}^{p}}{\\partial x^{q}}=\\frac{\\partial x^{q}}{\\partial \\bar{x}^{p}} \\quad \\text { or } \\quad J=\\left(J^{-1}\\right)^{T}\n$$\n\ni.e., unless the Jacobian matrix is orthogonal-as it is for orthogonal linear transformations (cartesian tensors).", "4.3": "\\section*{OUTER PRODUCT}\n4.3 Show that the outer product of two contravariant vectors is a contravariant tensor of order two.\n\nWith $\\left(S^{i}\\right)$ and $\\left(T^{i}\\right)$ as the given vectors,\n\n$$\n\\bar{S}^{i} \\bar{T}^{j}=\\left(S^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(T^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\\right)=S^{r} T^{s} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}}\n$$\n\nwhich is the correct transformation law for the outer product to be a contravariant tensor of order two.", "4.4": "\\section*{INNER PRODUCT}\n4.4 Prove that the inner product $\\left(T^{r} U_{i r}\\right)$ is a tensor if $\\left(T^{i}\\right)$ and $\\left(U_{i j}\\right)$ are tensors of the types indicated.\n\nWith $V_{j} \\equiv T^{i} U_{j i}$,\n\n$$\n\\bar{V}_{j}=\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(U_{s t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}\\right)=\\left(T^{r} U_{s t} \\delta_{r}^{t}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nwhich is the desired transformation law.", "4.5": "\\subsection*{4.5 Prove that if $\\mathbf{g}=\\left(g_{i j}\\right)$ is a covariant tensor of order two, and $\\mathbf{U}=\\left(U^{i}\\right)$ and $\\mathbf{V}=\\left(V^{i}\\right)$ are contravariant vectors, then the double inner product $\\mathbf{g U V}=g_{i j} U^{i} V^{j}$ is an invariant.}\n\nThe transformation laws are\n\n$$\n\\bar{g}_{i j}=g_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\bar{U}^{i}=U^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{i}} \\quad \\bar{V}^{j}=V^{u} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}\n$$\n\nMultiply, and sum over $i$ and $j$ :\n\n$$\n\\overline{\\mathbf{g}} \\overline{\\mathbf{U}} \\overline{\\mathbf{V}}=\\bar{g}_{i j} \\bar{U}^{i} \\bar{V}^{j}=g_{r s} U^{t} V^{u} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{u}}=g_{r s} U^{t} V^{u} \\delta_{t}^{r} \\delta_{u}^{s}=g_{r s} U^{r} V^{s}=\\mathbf{g} \\mathbf{U} \\mathbf{V}\n$$", "4.6": "\\subsection*{4.6 Assuming that contraction of a tensor yields a tensor, how many tensors may be created by repeated contraction of the tensor $\\mathbf{T}=\\left(T_{k l}^{i j}\\right)$ ?}\n\nSingle contraction produces the four mixed tensors\n\n$$\n\\left(T_{u l}^{u j}\\right) \\quad\\left(T_{k u}^{u j}\\right) \\quad\\left(T_{u l}^{i u}\\right) \\quad\\left(T_{k u}^{i u}\\right)\n$$\n\nand double contraction produces the two zero-order tensors (invariants) $T_{u v}^{u v}$ and $T_{v u}^{u v}$. Thus there are six tensors, in general all distinct.", "4.7": "\\subsection*{4.7 Show that any contraction of the tensor $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$ results in a covariant vector.}\n\nWe may contract on either $i=j$ or $i=k$. For $\\left(S_{k}\\right) \\equiv\\left(T_{i k}^{i}\\right)$, we have the transformation law\n\n$$\n\\bar{S}_{k} \\equiv \\bar{T}_{i k}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{s t}^{r} \\delta_{r}^{s} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=T_{r t}^{r} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=S_{t} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}\n$$\n\nand, for $\\left(U_{j}\\right) \\equiv\\left(T_{j i}^{i}\\right)$,\n\n$$\n\\bar{U}_{j} \\equiv \\bar{T}_{j i}^{i}=T_{s t}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}=T_{s t}^{r} \\delta_{r}^{t} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s r}^{r} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nIn either case, the transformation law is that of a covariant vector.", "4.8": "\\section*{COMBINED OPERATIONS}\n4.8 Suppose that $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ are tensors from which a contravariant vector $\\mathbf{V}=\\left(V^{i}\\right)$ is to be constructed using a combination of outer/inner products and contractions. (a) Show that there are six possibilities for $\\mathbf{V}$, which can all be distinct. (b) Verify that each possible $\\mathbf{V}$ is obtainable as a contraction of an inner product ST.\n\n(a) Writing $[\\mathbf{S T}] \\equiv \\mathbf{U}=\\left(U_{l m}^{i j k}\\right)$, we obtain the contravariant vectors as the double contractions of $\\mathbf{U}$ :\n\n$$\n\\begin{array}{llllll}\n\\left(U_{u v}^{u v k}\\right) & \\left(U_{v u}^{u v k}\\right) & \\left(U_{u v}^{u j v}\\right) & \\left(U_{v u}^{u j v}\\right) & \\left(U_{u v}^{i u v}\\right) & \\left(U_{v u}^{i u v}\\right)\n\\end{array}\n$$\n\n(b) The vector $\\left(U_{u v}^{u v k}\\right) \\equiv\\left(S_{u}^{u v} T_{v}^{k}\\right)$ may be obtained by first taking the inner product $\\left(S_{l}^{i v} T_{v}^{k}\\right)$ and then contracting on $i=u=l$. Likewise for the other five vectors of $(a)$.", "4.9": "\\subsection*{4.9 Prove criterion (2) of Section 4.2 without invoking the Quotient Theorem.}\n\nWe are to verify that $\\left(T_{i j}\\right)$ is a covariant tensor of order two if it is given that for every contravariant vector $\\left(V^{i}\\right), T_{i j} V^{i} \\equiv U_{j}$ are components of a covariant vector. Start out with the transformation law for $\\left(U_{j}\\right)$ [from $\\left(x^{i}\\right)$ to $\\left.\\left(\\bar{x}^{i}\\right)\\right]$ :\n\n$$\n\\bar{U}_{j}=U_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad \\bar{T}_{i j} \\bar{V}^{i}=T_{i s} V^{i} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nNow substitute the transformation law for $\\bar{V}^{i}$ [from $\\left(\\bar{x}^{i}\\right)$ to $\\left.\\left(x^{i}\\right)\\right]$ :\n\n$$\n\\bar{T}_{i j} \\bar{V}^{i}=T_{i s}\\left(\\overline{\\bar{V}}^{p} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{p}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\n$$\n\nReplace the dummy index $i$ by $p$ on the left and by $r$ on the right:\n\n$$\n\\bar{T}_{p j} \\bar{V}^{p}=T_{r s} \\bar{V}^{p} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\quad \\text { or } \\quad\\left(\\bar{T}_{p j}-T_{r s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{p}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right) \\bar{V}^{p}=0\n$$\n\nThe proof is concluded as in Example 4.4.", "4.10": "\\section*{TESTS FOR TENSOR CHARACTER}\n4.10 Prove criterion (3) of Section 4.2.\n\nHere we must show that $\\left(T_{i j}\\right)$ is a covariant tensor, assuming that $T_{i j} U^{i} V^{j}$ is invariant. Using criterion (1), we conclude that $\\left(T_{i j} U^{i}\\right)$ is a covariant vector. Using criterion (2), it follows that since $\\left(U^{i}\\right)$ is arbitrary, $\\left(T_{i j}\\right)$ is a covariant tensor of order two, the desired conclusion.", "4.11": "\\subsection*{4.11 Prove criterion (4) of Section 4.2.}\n\nWe wish to show that if $\\left(T_{i j}\\right)$ is a symmetric array such that $T_{i j} V^{i} V^{j}$ is an invariant for every contravariant vector $\\left(V^{i}\\right)$, then $\\left(T_{i j}\\right)$ is a (symmetric) covariant tensor of order two.\n\nLet $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ denote arbitrary contravariant vectors and let $\\left(W^{i}\\right) \\equiv\\left(U^{i}+V^{i}\\right)$, a contravariant vector by $(4.2 a)$. Then,\n\n$$\n\\begin{aligned}\nT_{i j} W^{i} W^{j} & \\equiv T_{i j}\\left(U^{i}+V^{i}\\right)\\left(U^{j}+V^{j}\\right) \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} U^{j}+T_{i j} U^{i} V^{j}+T_{i j} V^{i} V^{j} \\\\\n& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} V^{j}+2 T_{i j} U^{i} V^{j}\n\\end{aligned}\n$$\n\nwhere the symmetry of $\\left(T_{i j}\\right)$ has been used in the last step. Now, by hypothesis, the left-hand side and the first two terms of the right-hand side of the above identity are invariants. Therefore, $T_{i j} U^{i} V^{j}$ must be an invariant, and the desired conclusion follows from criterion (3).", "4.12": "\\subsection*{4.12 Use Lemma 4.1 to write a proof of the Quotient Theorem, Theorem 4.2.}\n\nIn the notation of the theorem and lemma, $S_{j_{1} j_{2} \\cdots j_{q}}^{i_{1} i_{2} \\ldots i_{p}} \\cdot U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}}$ is a tensor of order zero, or an invariant, for arbitrary $\\mathbf{U}^{(\\alpha)}$ and $\\mathbf{V}_{(\\beta)}$; that is,\n\n$$\nT_{j_{1} j_{2} \\cdots j_{q} k}^{i_{1} i_{2} \\cdots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \\cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \\cdots V_{(q)}^{j_{q}} V^{k}\n$$\n\nis an invariant, with $\\left(V^{k}\\right)$ also arbitrary. It then follows from Lemma 4.1 (with $q$ replaced by $q+1$ ) that $\\left(T_{j_{1} j_{2} \\ldots j_{q^{k}}}^{i_{1} i_{2} \\ldots i_{p}}\\right)$ is a tensor, contravariant of order $p$ and covariant of order $q+1$.\n\nFrom the above method of proof, it is clear that the Quotient Theorem is equally valid when the \"divisor\" is an arbitrary covariant vector. This form of the theorem will be used in Problem 4.13.", "4.13": "\\section*{TESTS FOR TENSOR CHARACTER}\n\n4.13 Use the Quotient Theorem to prove Theorem 3.2.\n\nIf $\\mathbf{U}=\\left(U^{i}\\right)$ is a contravariant vector, the inner product\n\n$$\n\\mathbf{V}=\\mathbf{T} \\mathbf{U} \\equiv\\left(T_{i j} U^{j}\\right)\n$$\n\nis a covariant vector. Moreover, because $\\left[T_{i j}\\right]_{n n}$ has an inverse, it follows that as $\\mathbf{U}$ runs through all contravariant vectors, $\\mathbf{V}$ runs through all covariant vectors. Thus,\n\n$$\n\\mathbf{U}=\\mathbf{T}^{-1} \\mathbf{V} \\equiv\\left(T^{i l} V_{j}\\right)\n$$\n\nis a tensor for an arbitrary $\\left(V_{i}\\right)$, making $\\left(T^{i j}\\right)$ a contravariant tensor of order two.", "4.14": "\\section*{TENSOR EQUATIONS}\n4.14 Prove that if $\\left(T_{j k l}^{i}\\right)$ is a tensor such that, in the $\\left(x^{i}\\right)$-system, $T_{j k l}^{i}=3 T_{l j k}^{i}$, then $T_{j k l}^{i}=3 T_{l j k}^{i}$ in all coordinate systems.\n\nWe must prove that $\\bar{T}_{j k l}^{i}=3 \\bar{T}_{l j k}^{i}$ in $\\left(\\bar{x}^{i}\\right)$. But\n\n$$\n\\begin{aligned}\n\\bar{T}_{j k l}^{i}-3 \\bar{T}_{l j k}^{i} & =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}} \\\\\n& =T_{r s t}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}}-3 T_{t r s}^{p} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\\\\n& =\\left(T_{r s t}^{p}-3 T_{t r s}^{p}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{p}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{l}}=0\n\\end{aligned}\n$$\n\nas desired.", "4.15": "\\subsection*{4.15 Prove Theorem 4.3.}\nBy Problem 3.14(a), the covariant transformation law has the matrix expression\n\n$$\n\\bar{T}=\\bar{J}^{T} T \\bar{J} \\quad \\text { whence } \\quad|\\bar{T}|=\\overline{\\mathscr{J}}^{2}|T|\n$$\n\nThus, $|T|=0$ implies $|\\bar{T}|=0$.", "4.16": "```latex\n\\section*{TENSOR EQUATIONS}\n\n4.16 Prove that if a mixed tensor $\\left(T_{j}^{i}\\right)$ can be expressed as the outer product of contravariant and covariant vectors $\\left(U^{i}\\right)$ and $\\left(V_{j}\\right)$ in one coordinate system, then $\\left(T_{j}^{i}\\right)$ is the outer product of those vectors in general.\n\nWe must prove that $\\bar{T}_{j}^{i}=\\bar{U}^{i} \\bar{V}_{j}$ for any admissible coordinate system $\\left(\\bar{x}^{i}\\right)$. But, by hypothesis,\n\n$$\n\\bar{T}_{j}^{i}-\\bar{U}^{i} \\bar{V}_{j}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}-\\left(U^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right)\\left(V_{s} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}\\right)=\\left(T_{s}^{r}-U^{r} V_{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=0\n$$\n```", "5.1": "5.1 A curve is given in spherical coordinates $\\left(x^{i}\\right)$ by\n\n$$\nx^{1}=t \\quad x^{2}=\\arcsin \\frac{1}{t} \\quad x^{3}=\\sqrt{t^{2}-1}\n$$\n\nFind the length of the arc $1 \\leqq t \\leqq 2$.\n\nBy $(5.4)$,\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}\n$$\n\nso we first calculate the $\\left(d x^{i} / d t\\right)^{2}$ :\n\n$$\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}=1 \\quad\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\left(\\frac{-1 / t^{2}}{\\sqrt{1-(1 / t)^{2}}}\\right)^{2}=\\frac{1}{t^{2}\\left(t^{2}-1\\right)} \\quad\\left(\\frac{d x^{3}}{d t}\\right)^{2}=\\left(\\frac{1}{2} \\frac{2 t}{\\sqrt{t^{2}-1}}\\right)^{2}=\\frac{t^{2}}{t^{2}-1}\n$$\n\nThen\n\n$$\n\\left(\\frac{d s}{d t}\\right)^{2}=1+t^{2} \\cdot \\frac{1}{t^{2}\\left(t^{2}-1\\right)}+\\left(t \\cdot \\frac{1}{t}\\right)^{2} \\cdot \\frac{t^{2}}{t^{2}-1}=\\frac{2 t^{2}}{t^{2}-1}\n$$\n\nand $(5.1 a)$ gives\n\n$$\n\\left.L=\\int_{1}^{2} \\frac{\\sqrt{2} t}{\\sqrt{t^{2}-1}} d t=\\sqrt{2\\left(t^{2}-1\\right)}\\right]_{1}^{2}=\\sqrt{6}\n$$", "5.2": "5.2 Find the length of the curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array}{l}\nx^{1}=1 \\\\\nx^{2}=t\n\\end{array} \\quad(1 \\leqq t \\leqq 2)\\right.\n$$\n\nif the metric is that of the hyperbolic plane $\\left(x^{2}>0\\right)$ :\n\n$$\ng_{11}=g_{22}=\\frac{1}{\\left(x^{2}\\right)^{2}} \\quad g_{12}=g_{21}=0\n$$\n\nSince $\\left(d x^{i} / d t\\right)=(0,1),(5.6 d)$ yields $(\\varepsilon=1)$\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{ll}\n0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\frac{1}{t^{2}} & 0 \\\\\n0 & \\frac{1}{t^{2}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1\n\\end{array}\\right]=\\frac{1}{t^{2}} \\\\\nL=\\int_{1}^{2} \\frac{1}{t} d t=\\ln 2\n\\end{gathered}\n$$", "5.3": "\\section*{GENERALIZED METRICS}\n5.3 Is the form $d x^{2}+3 d x d y+4 d y^{2}+d z^{2}$ positive definite?\n\nIt must be determined whether the polynomial $Q \\equiv\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+4\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}$ is positive unless $u^{1}=u^{2}=u^{3}=0$. By completing the square,\n\n$$\nQ=\\left(u^{1}\\right)^{2}+3 u^{1} u^{2}+\\frac{9}{4}\\left(u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}=\\left(u^{1}+\\frac{3}{2} u^{2}\\right)^{2}+\\frac{7}{4}\\left(u^{2}\\right)^{2}+\\left(u^{3}\\right)^{2}\n$$\n\nAll terms are perfect squares with positive coefficients; hence the form is indeed positive definite.", "5.4": "5.4 Show that the formula (5.1a) for arc length does not depend on the particular parameterization of the curve.\n\nGiven a curve $\\mathscr{C}: x^{i}=x^{i}(t) \\quad(a \\leqq t \\leqq b)$, suppose that $x^{i}=x^{i}(\\bar{t}) \\quad(\\bar{a} \\leqq \\bar{t} \\leqq \\bar{b})$ is a different parameterization, where $\\bar{t}=\\phi(t)$, with $\\phi^{\\prime}(t)>0$ and $\\bar{a}=\\phi(a), \\bar{b}=\\phi(b)$. Then, by the chain rule and substitution rule for integrals,\n\n$$\n\\begin{aligned}\nL & =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}\\right|} d t=\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\left(\\phi^{\\prime}(t)\\right)^{2}\\right|} d t \\\\\n& =\\int_{a}^{b} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} \\phi^{\\prime}(t) d t=\\int_{\\bar{a}}^{\\bar{b}} \\sqrt{\\left|g_{i j} \\frac{d x^{i}}{d \\bar{t}} \\frac{d x^{j}}{d \\bar{t}}\\right|} d \\bar{t}=\\bar{L}\n\\end{aligned}\n$$", "5.5": "\\section*{TENSOR PROPERTY OF THE METRIC}\n5.5 Find the Euclidean metric tensor (in matrix form) for spherical coordinates, using Theorem 5.2 .\n\nSince spherical coordinates $\\left(x^{i}\\right)$ are connected to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{-1}=x^{1} \\sin x^{2} \\cos x^{3} \\quad \\bar{x}^{2}=\\bar{x}^{-1} \\sin x^{2} \\sin x^{3} \\quad \\bar{x}^{3}=x^{1} \\cos x^{2}\n$$\n\nwe have\n\n$$\nJ^{T} J=\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & \\sin x^{2} \\sin x^{3} & \\cos x^{2} \\\\\nx^{1} \\cos x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & -x^{1} \\sin x^{2} \\\\\n-x^{1} \\sin x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} & 0\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\sin x^{2} \\cos x^{3} & x^{1} \\cos x^{2} \\cos x^{3} & -x^{1} \\sin x^{2} \\sin x^{3} \\\\\n\\sin x^{2} \\sin x^{3} & x^{1} \\cos x^{2} \\sin x^{3} & x^{1} \\sin x^{2} \\cos x^{3} \\\\\n\\cos x^{2} & -x^{1} \\sin x^{2} & 0\n\\end{array}\\right]\n$$\n\nSince $G=J^{T} J$ is known to be symmetric (see Problem 2.4), we need only compute the elements on or above the main diagonal:\n\n$$\nG=\\left[\\begin{array}{ccc}\n\\left(\\sin ^{2} x^{2}\\right)(1)+\\cos ^{2} x^{2} & \\left(x^{1} \\sin x^{2} \\cos x^{2}\\right)(1)-x^{1} \\sin x^{2} \\cos x^{2} & g_{13} \\\\\ng_{21} & \\left(\\left(x^{1}\\right)^{2} \\cos ^{2} x^{2}\\right)(1)+\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2} & g_{23} \\\\\ng_{31} & g_{32} & \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)(1)\n\\end{array}\\right]\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& g_{13}=\\left(x^{1} \\sin ^{2} x^{2}\\right)\\left(-\\sin x^{3} \\cos x^{3}+\\cos x^{3} \\sin x^{3}\\right)=0 \\\\\n& g_{23}=\\left(\\left(x^{1}\\right)^{2} \\sin x^{2} \\cos x^{2}\\right)\\left(-\\cos x^{3} \\sin x^{3}+\\sin x^{3} \\cos x^{3}\\right)=0\n\\end{aligned}\n$$\n\nHence\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right]\n$$", "5.6": "\\section*{GENERALIZED METRICS}\n5.6 Find the components $g_{i j}$ of the Euclidean metric tensor in the special coordinate system $\\left(x^{i}\\right)$ defined from rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ by $x^{1}=\\bar{x}^{1}, x^{2}=\\exp \\left(\\bar{x}^{2}-\\bar{x}^{1}\\right)$.\n\nWe must compute $J^{T} J$, where $J$ is the Jacobian matrix of the transformation $\\bar{x}^{i}=\\bar{x}^{i}\\left(x^{1}, x^{2}\\right)$. Thus, we solve the above equations for the $\\bar{x}^{i}$ :\n\nHence\n\n$$\n\\bar{x}^{1}=x^{1} \\quad \\bar{x}^{2}=x^{1}+\\ln x^{2}\n$$\n\n$$\n\\begin{aligned}\n& \\text { Hence } \\quad J=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right] \\\\\n& \\text { and } G=\\left[\\begin{array}{cc}\n1 & 1 \\\\\n0 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0 \\\\\n1 & \\left(x^{2}\\right)^{-1}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & \\left(x^{2}\\right)^{-1} \\\\\n\\left(x^{2}\\right)^{-1} & \\left(x^{2}\\right)^{-2}\n\\end{array}\\right] \\\\\n& \\text { or } g_{11}=2, g_{10}=g_{01}=\\left(x^{2}\\right)^{-1}, g_{23}=\\left(x^{2}\\right)^{-2} \\text {. }\n\\end{aligned}\n$$\n\nor $g_{11}=2, g_{12}=g_{21}=\\left(x^{2}\\right)^{-1}, g_{22}=\\left(x^{2}\\right)^{-2}$.", "5.7": "5.7 (a) Using the metric of Problem 5.6, calculate the length of the curve\n\n$$\n\\mathscr{C}: x^{1}=3 t, \\quad x^{2}=e^{t} \\quad(0 \\leqq t \\leqq 2)\n$$\n\n(b) Interpret geometrically.\n\n(a) First calculate the $d x^{i} / d t$ :\n\nThen\n\n$$\n\\frac{d x^{1}}{d t}=3 \\quad \\frac{d x^{2}}{d t}=e^{t}\n$$\n\nThen\n\n$$\n\\begin{gathered}\n\\left(\\frac{d s}{d t}\\right)^{2}=2\\left(\\frac{d x^{1}}{d t}\\right)^{2}+2\\left(x^{2}\\right)^{-1}\\left(\\frac{d x^{1}}{d t}\\right)\\left(\\frac{d x^{2}}{d t}\\right)+\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2} \\\\\n=2(9)+2 e^{-t}(3)\\left(e^{t}\\right)+e^{-2 t}\\left(e^{2 t}\\right)=25 \\\\\nL=\\int_{0}^{2} 5 d t=10\n\\end{gathered}\n$$\n\nand\n\n(b) From the transformation equations of Problem 5.6, the curve is described in rectangular coordinates by $\\bar{x}^{2}=\\frac{4}{3} \\bar{x}^{1}$; it is therefore a straight line joining the points which correspond to $t=0$ and $t=2$, or $(0,0)$ and $(6,8)$. The distance from $(0,0)$ to $(6,8)$ is\n\n$$\n\\sqrt{6^{2}+8^{2}}=10\n$$\n\nas found in $(a)$.", "5.8": "\\section*{TENSOR PROPERTY OF THE METRIC}\n5.8 Making use of the Euclidean metric for cylindrical coordinates, (5.3), calculate the length of arc along the circular helix\n\n$$\n\\bar{x}^{1}=a \\cos t \\quad \\bar{x}^{2}=a \\sin t \\quad \\bar{x}^{3}=b t\n$$\n\nwith $a$ and $b$ positive constants, from $t=0$ to $t=c>0$. See Fig. 5-3.\n\nIn cylindrical coordinates $\\left(x^{i}\\right)$, where\n\n$$\n\\bar{x}^{1}=x^{1} \\cos x^{2} \\quad \\bar{x}^{2}=x^{1} \\sin x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$\n\nthe helical arc is represented by the linear equations\n\n$$\n\\begin{gathered}\nx^{1}=a \\quad x^{2}=t \\quad x^{3}=b t \\quad(0 \\leqq t \\leqq c) \\\\\n\\left(\\frac{d s}{d t}\\right)^{2}=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\nb\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & b\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\na^{2} \\\\\nb\n\\end{array}\\right]=a^{2}+b^{2}\n\\end{gathered}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070(1)}\n\\end{center}\n\nFig. 5-3\n\nwhence\n\n$$\nL=\\int_{0}^{c} \\sqrt{a^{2}+b^{2}} d t=c \\sqrt{a^{2}+b^{2}}\n$$", "5.9": "5.9 (Affine Coordinates in $\\mathbf{R}^{3}$ ) Carpenters taking measurements in a room notice that at the corner they had used as reference point the angles were not true. If the actual measures of the angles are as given in Fig. 5-4, what correction in the usual metric formula,\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{\\sum_{i=1}^{3}\\left(x_{1}^{i}-x_{2}^{i}\\right)^{2}}\n$$\n\nshould be made to compensate for the errors?\n\nWe are asked, in effect, to display $\\mathbf{g}=\\left(g_{i j}\\right)$ for three-dimensional affine coordinates $\\left(x^{i}\\right)$. Instead of applying Theorem 5.2, it is much simpler to recall from Problem 3.9 that position vectors are contravariant affine vectors-in particular, the unit vectors\n\n$$\n\\mathbf{u}=\\left(\\delta_{1}^{i}\\right) \\quad \\mathbf{v}=\\left(\\delta_{2}^{i}\\right) \\quad \\mathbf{w}=\\left(\\delta_{3}^{i}\\right)\n$$\n\nalong the oblique axes (Fig. 5-4). We can now use (5.11) in inverse fashion, to obtain:\n\n$$\n\\cos \\alpha=\\frac{g_{i j} \\delta_{1}^{i} \\delta_{2}^{j}}{\\sqrt{g_{p q} \\delta_{1}^{p} \\delta_{1}^{q}} \\sqrt{g_{r s} \\delta_{2}^{r} \\delta_{2}^{s}}}=\\frac{g_{12}}{\\sqrt{g_{11}} \\sqrt{g_{22}}}=g_{12}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070}\n\\end{center}\n\nFig. 5-4\\\\\nsince, obviously, $g_{11}=g_{22}=g_{33}=1 \\quad\\left( \\pm d s=d x^{1}\\right.$ for motion parallel to $\\mathbf{u}$; etc. $)$. Likewise,\n\n$$\n\\cos \\beta=g_{13} \\quad \\cos \\gamma=g_{23}\n$$\n\nand the complete symmetric matrix is\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & \\cos \\alpha & \\cos \\beta \\\\\n\\cos \\alpha & 1 & \\cos \\gamma \\\\\n\\cos \\beta & \\cos \\gamma & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & -0.01745 & -0.00873 \\\\\n-0.01745 & 1 & 0.01745 \\\\\n-0.00873 & 0.01745 & 1\n\\end{array}\\right]\n$$\n\nIt follows that the carpenters must use as the corrected distance formula\n\n$$\n\\overline{P_{1} P_{2}}=\\sqrt{g_{i j}\\left(x_{1}^{i}-x_{2}^{i}\\right)\\left(x_{1}^{j}-x_{2}^{j}\\right)}\n$$\n\nwhere the $g_{i j}$ have the numerical values given above.", "5.10": "\\section*{RAISING AND LOWERING INDICES}\n5.10 Given that $\\left(V^{i}\\right)$ is a contravariant vector on $\\mathbf{R}^{3}$, find its associated covariant vector $\\left(V_{i}\\right)$ in cylindrical coordinates $\\left(x^{i}\\right)$ under the Euclidean metric.\n\nSince\n\n$$\n\\left[g_{i j}\\right]_{33}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand $V_{i}=g_{i r} V^{r}$, we have in matrix form,\n\n$$\n\\left[\\begin{array}{l}\nV_{1} \\\\\nV_{2} \\\\\nV_{3}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nV^{1} \\\\\nV^{2} \\\\\nV^{3}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nV^{1} \\\\\n\\left(x^{1}\\right)^{2} V^{2} \\\\\nV^{3}\n\\end{array}\\right]\n$$", "5.11": "5.11 Show that under orthogonal coordinate changes, starting with any particular system of rectangular coordinates, the raising and lowering of indices has no effect on tensors, consistent with the fact (Section 3.6) that there is no distinction between contravariant and covariant cartesian tensors.\n\nIt suffices to show merely that $g_{i j}=\\delta_{i j}=g^{i j}$ for any admissible coordinate system $\\left(x^{i}\\right)$, for then it will follow that\n\n$$\nT^{i}=\\delta^{i j} T_{j}=T_{i} \\quad T_{i j k}=\\delta_{i r} T_{j k}^{r}=T_{j k}^{i} \\quad T_{j k}^{i}=\\delta_{j r} T_{k}^{i r}=T_{k}^{i j}\n$$\n\nand so on. To that end, simply use formula (5.7), with $J=\\left(a_{i}^{i}\\right)$ an orthogonal matrix. Because $J^{T}=J^{-1}$, we have $G=J^{-1} J=I$, or $g_{i j}=\\delta_{i j}$, as desired. Since $G^{-1}=I^{-1}=I$, it is also the case that $g^{i j}=\\delta_{i j}$.", "5.12": "\\section*{GENERALIZED NORM}\n5.12 Show that the length of any contravariant vector $\\left(V^{i}\\right)$ equals the length of its associated covariant vector $\\left(V_{i}\\right)$.\n\nBy definition,\n\n$$\n\\left\\|\\left(V^{i}\\right)\\right\\|=\\sqrt{g_{i j} V^{i} V^{j}} \\quad \\text { and } \\quad\\left\\|\\left(V_{i}\\right)\\right\\|=\\sqrt{g^{i j} V_{i} V_{j}}\n$$\n\nBut, since $V^{i}=g^{i r} V_{r}$ and $g_{i j}=g_{i i}$,\n\n$$\ng_{i j} V^{i} V^{j}=g_{i j}\\left(g^{i r} V_{r}\\right)\\left(g^{j s} V_{s}\\right)=g_{j i} g^{i r} g^{j s} V_{r} V_{s}=\\delta_{j}^{r} g^{j s} V_{r} V_{s}=g^{r s} V_{r} V_{s}\n$$\n\nand the two lengths are equal.", "5.13": "\\section*{GENERALIZED NORM}\n5.13 Assuming a positive definite metric, show that the basic properties of the cartesian inner product $\\mathbf{U} \\cdot \\mathbf{V}$ are shared by the generalized inner product $\\mathbf{U V}$ of contravariant vectors.\n\n(a) $\\mathbf{U V}=\\mathbf{V U}$ (commutative property). Follows from symmetry of $\\left(g_{i j}\\right)$.\n\n(b) $\\mathbf{U}(\\mathbf{V}+\\mathbf{W})=\\mathbf{U V}+\\mathbf{U W}$ (distributive property). Follows from (1.2).\n\n(c) $\\quad(\\lambda \\mathbf{U}) \\mathbf{V}=\\mathbf{U}(\\lambda \\mathbf{V})=\\lambda(\\mathbf{U V})$ (associative property). Follows from $\\lambda U_{i} V^{i}=U_{i}\\left(\\lambda V^{i}\\right)=\\lambda U_{i} V^{i}$.\n\n(d) $\\mathbf{U}^{2} \\geqq 0$ with equality only if $\\mathbf{U}=\\mathbf{0}$ (positive-definiteness). Follows from the assumed positivedefiniteness of $\\left(g_{i j}\\right)$.\n\n(e) ( $\\mathbf{U V})^{2} \\leqq\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right)$ (Cauchy-Schwarz inequality). This may be derived from the other properties, as follows. If $\\mathbf{U}=\\mathbf{0}$, the inequality clearly holds. If $\\mathbf{U} \\neq \\mathbf{0}$, property ( $d$ ) ensures that the quadratic polynomial\n\n$$\nQ(\\lambda) \\equiv(\\lambda \\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2} \\lambda^{2}+2 \\mathbf{U} \\mathbf{V} \\lambda+\\mathbf{V}^{2}\n$$\n\nvanishes for at most one real value of $\\lambda$. Thus, the discriminant of $Q$ cannot be positive:\n\n$$\n(\\mathbf{U V})^{2}-\\left(\\mathbf{U}^{2}\\right)\\left(\\mathbf{V}^{2}\\right) \\leqq 0\n$$\n\nand this is the desired inequality.", "5.14": "\\section*{GENERALIZED NORM}\n5.14 A generalized norm on a vector space is any real-valued functional $\\phi[]$ that satisfies\n\n(i) $\\phi[\\mathbf{V}] \\geqq 0$, with equality only if $\\mathbf{V}=\\mathbf{0}$;\n\n(ii) $\\phi[\\lambda \\mathbf{V}]=|\\lambda| \\phi[\\mathbf{V}]$;\n\n(iii) $\\phi[\\mathbf{U}+\\mathbf{V}] \\leqq \\phi[\\mathbf{U}]+\\phi[\\mathbf{V}]$ (triangle inequality).\n\nVerify these conditions for $\\phi[\\mathbf{V}]=\\|\\mathbf{V}\\|$, the inner-product norm under a positive definite metric.\n\n(i) and (ii) for $\\|\\mathbf{V}\\|$ are evident. As for (iii), the Cauchy-Schwarz inequality gives\n\n$$\n\\begin{aligned}\n\\|\\mathbf{U}+\\mathbf{V}\\|^{2} & =(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U} \\mathbf{V} \\\\\n& \\leqq\\|\\mathbf{U}\\|^{2}+\\|\\mathbf{V}\\|^{2}+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\|=(\\|\\mathbf{U}\\|+\\|\\mathbf{V}\\|)^{2}\n\\end{aligned}\n$$\n\nfrom which (iii) follows at once.", "5.15": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.15 Show that the angle between contravariant vectors is an invariant under a change of coordinate systems.\n\nThe defining expression (5.1) involves only inner products, which are invariants.", "5.16": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.16 In $\\mathbf{R}^{2}$ the family of curves $x^{2}=x^{1}-c$ (parameterized as $x^{1}=t, x^{2}=t-c$ ), has as its system of tangent vectors the vector field $\\mathbf{U}=(1,1)$, constant throughout $\\mathbf{R}^{2}$. If $\\left(x^{i}\\right)$ represent polar coordinates, find the family of orthogonal trajectories, and interpret geometrically.\n\nThe metric is given by\n\n$$\ng=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\nso, by Theorem 5.3, the orthogonality condition becomes\n\n$$\ng_{i j} U^{i} \\frac{d x^{j}}{d u}=(1)(1) \\frac{d x^{1}}{d u}+\\left(x^{1}\\right)^{2}(1) \\frac{d x^{2}}{d u}=0\n$$\n\nor, eliminating the differential $d u$,\n\n$$\nd x^{1}+\\left(x^{1}\\right)^{2} d x^{2}=0\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-073}\n\\end{center}\n\nFig. 5-5\n\nThis is a variables-separable differential equation, whose solution is\n\n$$\nx^{1}=\\frac{1}{x^{2}+d}\n$$\n\nThe given family of curves in the usual polar-coordinate notation is $r=\\theta+c$, which is a family of concentric spirals (solid curves in Fig. 5-5). The orthogonal trajectories,\n\n$$\nr=\\frac{1}{\\theta+d}\n$$\n\nare also spirals, each having an asymptote parallel to the line $\\theta=-d$; these are the dashed curves in Fig. 5-5.\n\nNote: To solve this problem in rectangular coordinates-that is, to find the orthogonal trajectories of the family\n\n$$\n\\frac{y}{x}=\\tan \\left(\\sqrt{x^{2}+y^{2}}-c\\right)\n$$\n\nunder the metric $\\left(g_{i j}\\right)=\\left(\\delta_{i j}\\right)$-would be difficult or impossible. Quite often, the complication of the metric involved in going over to a specialized curvilinear coordinate system is vastly outweighed by the degree to which the problem is simplified.", "5.17": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.17 Find the condition for two curves on a sphere of radius $a$ to be orthogonal, if the curves are represented in spherical coordinates by\n\n$$\n\\mathscr{C}_{1}: \\theta=f(\\varphi) \\quad \\text { and } \\quad \\mathscr{C}_{2}: \\theta=g(\\varphi)\n$$\n\nThe two curves can be parameterized in spherical coordinates $\\left(x^{i}\\right) \\equiv(\\rho, \\varphi, \\theta)$ by\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=t \\\\\n\\theta=f(t)\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\n\\rho=a \\\\\n\\varphi=u \\\\\n\\theta=g(u)\n\\end{array}\\right.\\right.\n$$\n\nAt an intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$ the tangent vectors of $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ are, respectively,\n\n$$\n\\mathbf{U}=\\left(0,1, f^{\\prime}\\left(\\varphi_{0}\\right)\\right) \\quad \\text { and } \\quad \\mathbf{V}=\\left(0,1, g^{\\prime}\\left(\\varphi_{0}\\right)\\right)\n$$\n\nThese are orthogonal if and only if $g_{i j} U^{i} V^{j}=0$, or\n\n$$\n\\begin{aligned}\n0 & =\\left[\\begin{array}{lll}\n0 & 1 & f^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & a^{2} & 0 \\\\\n0 & 0 & \\left(a \\sin \\varphi_{0}\\right)^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\ng^{\\prime}\\left(\\varphi_{0}\\right)\n\\end{array}\\right] \\\\\n& =0+a^{2}+\\left(a \\sin \\varphi_{0}\\right)^{2} f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=\\left(a^{2} \\sin ^{2} \\varphi_{0}\\right)\\left[\\csc ^{2} \\varphi_{0}+f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nHence, the desired criterion is that $f^{\\prime}\\left(\\varphi_{0}\\right) g^{\\prime}\\left(\\varphi_{0}\\right)=-\\csc ^{2} \\varphi_{0}$ at any intersection point $\\left(a, \\varphi_{0}, \\theta_{0}\\right)$.", "5.18": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.18 Show that the contravariant vectors $\\mathbf{U}=\\left(0,1,2 b x^{2}\\right)$ and $\\mathbf{V}=\\left(0,-2 b x^{2},\\left(x^{1}\\right)^{2}\\right)$ are orthogonal under the Euclidean metric for cylindrical coordinates. Interpret geometrically along $x^{1}=a$, $x^{2}=t, x^{3}=b t^{2}$.\n\n$$\n\\begin{aligned}\ng_{i j} U^{i} V^{j} & =\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n0 & 1 & 2 b x^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n0 \\\\\n-2 b x^{2}\\left(x^{1}\\right)^{2} \\\\\n\\left(x^{1}\\right)^{2}\n\\end{array}\\right] \\\\\n& =0-2 b x^{2}\\left(x^{1}\\right)^{2}+2 b x^{2}\\left(x^{1}\\right)^{2}=0\n\\end{aligned}\n$$\n\nThe geometric interpretation is that $x^{1}=a, x^{2}=t, x^{3}=b t^{2}$, for real $t$, represents a sort of variable-pitch helix on the right circular cylinder $r=a$, having tangent field $\\mathbf{U}$. Therefore, any solution of\n\n\n\\begin{equation*}\n\\underbrace{\\frac{d x^{1}}{d u}=V^{1}=0}_{\\text {or } x^{1}=a} \\quad \\frac{d x^{2}}{d u}=V^{2}=-2 b x^{2} \\quad \\frac{d x^{3}}{d u}=V^{3}=a^{2} \\tag{1}\n\\end{equation*}\n\n\nwill represent a curve on that cylinder that is orthogonal to this pseudo-helix. See Problem 5.28.", "5.19": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.19 Show that in any coordinate system $\\left(x^{i}\\right)$ the contravariant vector (recall Problem 3.5) $\\mathbf{V} \\equiv\\left(g^{i \\alpha}\\right)$ is normal to the surface $x^{\\alpha}=$ const. $(\\alpha=1,2, \\ldots, n)$.\n\nBeing \"normal to a surface at the surface point $P$ \" means being orthogonal, at $P$, to the tangent vector of any curve lying in the surface and passing through $P$. Now, for the surface $x^{\\alpha}=$ const., any such tangent vector $\\mathbf{T}$ has as its $\\alpha$ th component\n\n$$\nT^{\\alpha}=\\frac{d x^{\\alpha}}{d t}=0\n$$\n\nWe then have:\n\n$$\n\\mathbf{V T} \\equiv g_{i j} V^{i} T^{j}=g_{i j} g^{i \\alpha} T^{j}=g_{j i} g^{i \\alpha} T^{j}=\\delta_{j}^{\\alpha} T^{j}=T^{\\alpha}=0\n$$\n\nand the proof is complete.", "5.20": "\\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}\n5.20 Show that in any coordinate system $\\left(x^{i}\\right)$, the angle $\\theta$ between the normals to the surfaces $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. is given by\n\n\\begin{equation*}\n\\cos \\theta=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}} \\quad \\text { (no sum) } \\tag{1}\n\\end{equation*}\n\nBy Problem 5.19, $\\mathbf{U}=\\left(g^{i \\alpha}\\right)$ and $\\mathbf{V}=\\left(g^{i \\beta}\\right)$ are the respective normals to $x^{\\alpha}=$ const. and $x^{\\beta}=$ const. Therefore, by the definition (5.11)\n\n$$\n\\cos \\theta=\\frac{U V}{\\|U\\|\\|V\\|}=\\frac{g_{i j} g^{i \\alpha} g^{j \\beta}}{\\sqrt{g_{p q} g^{p \\alpha} g^{q \\alpha}} \\sqrt{g_{r s} g^{r \\beta} g^{s \\beta}}}=\\frac{\\delta_{j}^{\\alpha} g^{j \\beta}}{\\sqrt{\\delta_{q}^{\\alpha} g^{q \\alpha}} \\sqrt{\\delta_{s}^{\\beta} g^{s \\beta}}}=\\frac{g^{\\alpha \\beta}}{\\sqrt{g^{\\alpha \\alpha}} \\sqrt{g^{\\beta \\beta}}}\n$$\n\nIn consequence of (1), orthogonal coordinates are defined as those coordinate systems $\\left(x^{i}\\right)$ relative to which, at all points, $g^{i j}=0 \\quad(i \\neq j)$, or, equivalently, $g_{i j}=0 \\quad(i \\neq j)$. Obviously, orthogonal coordinates need not be rectangular: witness polar, cylindrical, and spherical coordinates.", "6.1": "6.1 Verify that $\\Gamma_{i j k}=\\Gamma_{j i k}$.\n\nBy definition,\n\n$$\n\\Gamma_{i j k}=\\frac{1}{2}\\left(-g_{i j k}+g_{j k i}+g_{k i j}\\right) \\quad \\text { and } \\quad \\Gamma_{j i k}=\\frac{1}{2}\\left(-g_{j i k}+g_{i k j}+g_{k j i}\\right)\n$$\n\nBut $g_{i j k}=g_{j i k}, g_{j k i}=g_{k j i}$, and $g_{k i j}=g_{i k i}$, by symmetry of $g_{i j}$, and the result follows.", "6.2": "6.2 Show that if $\\left(g_{i j}\\right)$ is a diagonal matrix, then for all fixed subscripts $\\alpha$ and $\\beta \\neq \\alpha$ in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\alpha} \\quad($ not summed on $\\alpha$ )\n\n(b) $-\\Gamma_{\\alpha \\alpha \\beta}=\\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\quad$ (not summed on $\\alpha$ )\n\n(c) All remaining Christoffel symbols $\\Gamma_{i j k}$ are zero.\\\\\n(a) By definition, $\\Gamma_{\\alpha \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}+g_{\\alpha \\alpha \\alpha}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\alpha}$.\n\n(b) Since $\\alpha \\neq \\beta$,\n\n$$\n\\begin{aligned}\n& -\\Gamma_{\\alpha \\alpha \\beta}=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}\\right)=-\\frac{1}{2}\\left(-g_{\\alpha \\alpha \\beta}+0+0\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta} \\\\\n& \\Gamma_{\\alpha \\beta \\alpha}=\\Gamma_{\\beta \\alpha \\alpha}=\\frac{1}{2}\\left(-g_{\\alpha \\beta \\alpha}+g_{\\beta \\alpha \\alpha}+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2}\\left(-0+0+g_{\\alpha \\alpha \\beta}\\right)=\\frac{1}{2} g_{\\alpha \\alpha \\beta}\n\\end{aligned}\n$$\n\n(c) Let $i, j, k$ be distinct subscripts. Then $g_{i j}=0$ and $g_{i j k}=0$, implying that $\\Gamma_{i j k}=0$.", "6.3": "6.3 Is it true that if all $\\Gamma_{i j k}$ vanish in some coordinate system, then the metric tensor has constant components in every coordinate system?\n\nBy Lemma 6.2, the conclusion would be valid if the $\\Gamma_{i j k}$ vanished in every coordinate system. But $\\left(\\Gamma_{i j k}\\right)$ is not a tensor, and the conclusion is false. For instance, all $\\bar{\\Gamma}_{i j k}=0$ for the Euclidean metric in rectangular coordinates, but $g_{22}=\\left(x^{1}\\right)^{2}$ in spherical coordinates.", "6.4": "\\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n6.4 If $\\left(g_{i j}\\right)$ is a diagonal matrix, show that for all fixed indices (no summation) in the range $1,2, \\ldots, n$,\n\n(a) $\\Gamma_{\\alpha \\beta}^{\\alpha}=\\Gamma_{\\beta \\alpha}^{\\alpha}=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right)$\n\n(b) $\\Gamma_{\\beta \\beta}^{\\alpha}=-\\frac{1}{2 g_{\\alpha \\alpha}} g_{\\beta \\beta \\alpha} \\quad(\\alpha \\neq \\beta)$\n\n(c) All other $\\Gamma_{j k}^{i}$ vanish.\n\n(a) Both $\\left(g_{i j}\\right)$ and $\\left(g_{i j}\\right)^{-1}=\\left(g^{i j}\\right)$ are diagonal, with nonzero diagonal elements. Thus,\n\n\n\\begin{gather*}\n\\Gamma_{\\alpha \\beta}^{\\alpha}=g^{\\alpha j} \\Gamma_{\\alpha \\beta j}=g^{\\alpha \\alpha} \\Gamma_{\\alpha \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(\\frac{1}{2} \\frac{\\partial g_{\\alpha \\alpha}}{\\partial x^{\\beta}}\\right)=\\frac{\\partial}{\\partial x^{\\beta}}\\left(\\frac{1}{2} \\ln \\left|g_{\\alpha \\alpha}\\right|\\right) \\\\\n\\Gamma_{\\beta \\beta}^{\\alpha}=g^{\\alpha \\alpha} \\Gamma_{\\beta \\beta \\alpha}=\\frac{1}{g_{\\alpha \\alpha}}\\left(-\\frac{1}{2} g_{\\beta \\beta \\alpha}\\right) \\tag{b}\n\\end{gather*}\n\n\n(c) When $i, j, k$ are distinct, $\\Gamma_{j k}^{i}=g^{i r} \\Gamma_{j k r}=g^{i i} \\Gamma_{j k i}=0$ (not summed on $i$ ).", "6.5": "```latex\n\\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n6.5 Calculate the Christoffel symbols of the second kind for the Euclidean metric in spherical coordinates, using Problem 6.4.\n\nWe have $g_{11}=1, g_{22}=\\left(x^{1}\\right)^{2}$, and $g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}$. Noting that $g_{11}$ is a constant and that all $g_{\\alpha \\alpha}$ are independent of $x^{3}$, we obtain the following nonzero symbols from Problem 6.4(a):\n\n$$\n\\begin{aligned}\n& \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(x^{1}\\right)^{2}\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{31}^{3}=\\Gamma_{13}^{3}=\\frac{\\partial}{\\partial x^{1}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\frac{1}{x^{1}} \\\\\n& \\Gamma_{32}^{3}=\\Gamma_{23}^{3}=\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{1}{2} \\ln \\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)\\right)=\\cot x^{2}\n\\end{aligned}\n$$\n\nSimilarly, from Problem $6.4(b)$,\n\n$$\n\\begin{aligned}\n& \\Gamma_{22}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(x^{1}\\right)^{2}=-x^{1} \\\\\n& \\Gamma_{33}^{1}=-\\frac{1}{2(1)} \\frac{\\partial}{\\partial x^{1}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-x^{1} \\sin ^{2} x^{2} \\\\\n& \\Gamma_{33}^{2}=-\\frac{1}{2\\left(x^{1}\\right)^{2}} \\frac{\\partial}{\\partial x^{2}}\\left(\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}\\right)=-\\sin x^{2} \\cos x^{2}\n\\end{aligned}\n$$\n```", "6.6": "\\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}\n\n6.6 Use (6.6) to find the most general 3-dimensional transformation $x^{i}=x^{i}(\\overline{\\mathbf{x}})$ of coordinates such that $\\left(x^{i}\\right)$ is rectangular and $\\left(\\bar{x}^{i}\\right)$ is any other coordinate system for which the Christoffel symbols are\n\n$$\n\\bar{\\Gamma}_{11}^{1}=1 \\quad \\bar{\\Gamma}_{22}^{2}=2 \\quad \\bar{\\Gamma}_{33}^{3}=3 \\quad \\text { all others }=0\n$$\n\nSince $\\Gamma_{s t}^{r}=0$, (6.6) reduces to the system of linear partial differential equations with constant coefficients:\n\n\\begin{equation*}\n\\frac{\\partial^{2} x^{r}}{\\partial \\bar{x}^{i} \\partial \\bar{x}^{j}}=\\bar{\\Gamma}_{i j}^{s} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}} \\tag{1}\n\\end{equation*}\n\nIt is simplest first to solve the intermediate, first-order system\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}^{r}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s}^{r} \\quad\\left(\\bar{u}_{s}^{r}=\\frac{\\partial x^{r}}{\\partial \\bar{x}^{s}}\\right) \\tag{2}\n\\end{equation*}\n\nSince the systems (2) for $r=1,2,3$ are the same, temporarily replace $\\bar{u}_{s}^{r}$ by $\\bar{u}_{s}$, and $x^{r}$ by a single variable $x$; thus,\n\nFor $j=1$, (3) becomes\n\n\\begin{equation*}\n\\frac{\\partial \\bar{u}_{j}}{\\partial \\bar{x}^{i}}=\\bar{\\Gamma}_{i j}^{s} \\bar{u}_{s} \\tag{3}\n\\end{equation*}\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{1}}=\\bar{\\Gamma}_{11}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{11}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{11}^{3} \\bar{u}_{3}=\\bar{u}_{1} \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{2}}=\\bar{\\Gamma}_{21}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{21}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{21}^{3} \\bar{u}_{3}=0 \\\\\n& \\frac{\\partial \\bar{u}_{1}}{\\partial \\bar{x}^{3}}=\\bar{\\Gamma}_{31}^{1} \\bar{u}_{1}+\\bar{\\Gamma}_{31}^{2} \\bar{u}_{2}+\\bar{\\Gamma}_{31}^{3} \\bar{u}_{3}=0\n\\end{aligned}\n$$\n\nHence $\\bar{u}_{1}$ is a function of $\\bar{x}^{1}$ alone, and the first differential equation integrates to give\n\n$$\n\\bar{u}_{1}=b_{1} \\exp \\bar{x}^{1} \\quad\\left(b_{1}=\\text { constant }\\right)\n$$\n\nIn the same way, we find for $j=2$ and $j=3$ :\n\n$$\n\\begin{array}{ll}\n\\bar{u}_{2}=b_{2} \\exp 2 \\bar{x}^{2} & \\left(b_{2}=\\text { constant }\\right) \\\\\n\\bar{u}_{3}=b_{3} \\exp 3 \\bar{x}^{3} & \\left(b_{3}=\\text { constant }\\right)\n\\end{array}\n$$\n\nNow we return to the equations $\\partial x / \\partial \\bar{x}^{i}=\\bar{u}_{i}$ with the solutions just found for the $\\bar{u}_{i}$.\n\n\\begin{equation*}\n\\frac{\\partial x}{\\partial \\bar{x}^{1}}=b_{1} \\exp \\bar{x}^{1} \\quad-\\frac{\\partial x}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\frac{\\partial x}{\\partial \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\tag{4}\n\\end{equation*}\n\nIntegration of the first equation (4) yields\n\n$$\nx=b_{1} \\exp \\bar{x}^{1}+\\varphi\\left(\\bar{x}^{2}, \\bar{x}^{3}\\right)\n$$\n\nand then the second and third equations give:\n\n$$\n\\begin{aligned}\n& \\frac{\\partial \\varphi}{\\partial \\bar{x}^{2}}=b_{2} \\exp 2 \\bar{x}^{2} \\quad \\text { or } \\quad \\varphi=a_{2} \\exp 2 \\bar{x}^{2}+\\psi\\left(\\bar{x}^{3}\\right) \\\\\n& \\frac{d \\psi}{d \\bar{x}^{3}}=b_{3} \\exp 3 \\bar{x}^{3} \\quad \\text { or } \\quad \\psi=a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n\\end{aligned}\n$$\n\nThis means that, with $a_{1}=b_{1}$,\n\n$$\nx=a_{1} \\exp \\bar{x}^{1}+a_{2} \\exp 2 \\bar{x}^{2}+a_{3} \\exp 3 \\bar{x}^{3}+a_{4}\n$$\n\nso that the general solution of (1) is\n\n\\begin{equation*}\nx^{r}=a_{1}^{r} \\exp \\bar{x}^{1}+a_{2}^{r} \\exp 2 \\bar{x}^{2}+a_{3}^{r} \\exp 3 \\bar{x}^{3}+a_{4}^{r} \\tag{5}\n\\end{equation*}\n\nfor $r=1,2,3$.\n\nThe constants $a_{4}^{r}$ in (5) are unimportant; they merely allow any point in $\\mathbf{R}^{3}$ to serve as the origin of the rectangular system $\\left(x^{r}\\right)$. The remaining constants may be chosen at will, subject to a single condition (see Problem 6.27).", "6.7": "\\section*{COVARIANT DIFFERENTIATION}\n6.7 Establish the tensor character of $\\mathbf{T}_{, k}$ (Definition 2), where $\\mathbf{T}$ is a contravariant vector.\n\nBeginning with the transformation law\n\n$$\n\\bar{T}^{i}=T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\n$$\n\ntake the partial derivative with respect to $\\bar{x}^{k}$ and use the chain rule:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial}{\\partial x^{s}}\\left(T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r} \\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{s} \\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nNow use (6.6), with barred and unbarred systems interchanged:\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+T^{r}\\left(\\Gamma_{s r}^{t} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}}-\\bar{\\Gamma}_{u v}^{i} \\frac{\\partial \\bar{x}^{u}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{v}}{\\partial x^{r}}\\right) \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$\n\nSince $\\left(\\partial \\bar{x}^{u} / \\partial x^{s}\\right)\\left(\\partial x^{s} / \\partial \\bar{x}^{k}\\right)=\\delta_{k}^{u}$ and $T^{r}\\left(\\partial \\bar{x}^{v} / \\partial x^{r}\\right)=\\bar{T}^{\\nu}$, this becomes\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}=\\frac{\\partial T^{r}}{\\partial x^{s}} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}+\\Gamma_{s r}^{t} T^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{t}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}-\\bar{\\Gamma}_{k v}^{i} \\bar{T}^{v}\n$$\n\nor (by factoring and using the symmetry of the Christoffel symbols)\n\n$$\n\\frac{\\partial \\bar{T}^{i}}{\\partial \\bar{x}^{k}}+\\bar{\\Gamma}_{t k}^{i} \\bar{T}^{t}=\\left(\\frac{\\partial T^{r}}{\\partial x^{s}}+\\Gamma_{t s}^{r} T^{t}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}} \\quad \\text { or } \\quad \\bar{T}_{, k}^{i}=T_{, s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{k}}\n$$", "6.8": "```latex\n6.8 Show that $\\left(T_{j, k}^{i}\\right)$, as defined by (6.7), is a tensor, using the previously proven facts that $T_{, k}^{i}$ and $T_{i, k}$ are tensorial for all tensors $\\left(T^{i}\\right)$ and $\\left(T_{i}\\right)$.\n\nLet $\\left(V_{i}\\right)$ be any vector and set $U_{j}=T_{j}^{r} V_{r}$. The covariant derivative of the tensor $\\left(U_{j}\\right)$ is the tensor $\\left(U_{j, k}\\right)$, where\n\n$$\nU_{j, k}=\\frac{\\partial U_{j}}{\\partial x^{k}}-\\Gamma_{j k}^{r} U_{r}=\\frac{\\partial}{\\partial x^{k}}\\left(T_{j}^{r} V_{r}\\right)-\\Gamma_{j k}^{r}\\left(T_{r}^{s} V_{s}\\right)=\\frac{\\partial T_{j}^{s}}{\\partial x^{k}} V_{s}+T_{j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{j k}^{r} T_{r}^{s} V_{s}\n$$\n\nBut\n\n$$\nV_{r, k}=\\frac{\\partial V_{r}}{\\partial x^{k}}-\\Gamma_{r k}^{s} V_{s} \\quad \\text { or } \\quad \\frac{\\partial V_{r}}{\\partial x^{k}}=V_{r, k}+\\Gamma_{r k}^{s} V_{s}\n$$\n\nWhen the above expression for $\\partial V_{r} / \\partial x^{k}$ is substituted into the preceding equation and the terms rearranged, the result is:\n\n$$\n\\left(\\frac{\\partial T_{j}^{s}}{\\partial x^{k}}+\\Gamma_{r k}^{s} T_{j}^{r}-\\Gamma_{j k}^{r} T_{r}^{s}\\right) V_{s}=U_{j, k}-T_{j}^{r} V_{r, k}\n$$\n\ni.e.,\n\n$$\nT_{j, k}^{s} V_{s}=\\text { tensor component }\n$$\n\nIt follows at once from the Quotient Theorem (Theorem 4.2) that $\\left(T_{j, k}^{i}\\right)$ is a tensor.\n```", "6.9": "\\section*{COVARIANT DIFFERENTIATION}\n6.9 Extend the notion of covariant differentiation so that it will apply to invariants.\n\nFirst note that the partial derivative of an invariant is a tensor:\n\n$$\n\\frac{\\partial \\bar{E}}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial \\bar{x}^{i}}=\\frac{\\partial E}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nNow, under any reasonable definition, $\\left(E_{, i}\\right)$ must (1) be a tensor; (2) coincide with ( $\\left.\\partial E / \\partial x^{i}\\right)$ in rectangular coordinates. The obvious choice is therefore\n\n$$\n\\left(E_{, i}\\right) \\equiv\\left(\\frac{\\partial E}{\\partial x^{i}}\\right)", "6.10": "```latex\n\\section*{COVARIANT DIFFERENTIATION}\n6.10 Write the formula for the covariant derivative indicated by $T_{k, l}^{i j}$.\n\n$$\nT_{k, l}^{i j}=\\frac{\\partial T_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{r l}^{i} T_{k}^{r j}+\\Gamma_{r l}^{j} T_{k}^{i r}-\\Gamma_{k l}^{r} T_{r}^{i j}\n$$\n```", "6.11": "\\section*{COVARIANT DIFFERENTIATION}\n6.11 Prove that the metric tensor behaves like a constant under covariant differentiation; i.e., $g_{i j, k}=0$ for all $i, j, k$.\n\nBy definition, since $\\left(g_{i j}\\right)$ is covariant of order 2 ,\n\n$$\ng_{i j, k}=\\frac{\\partial g_{i j}}{\\partial x^{k}}-\\Gamma_{i k}^{r} g_{r j}-\\Gamma_{j k}^{r} g_{i r}=g_{i j k}-\\Gamma_{i k j}-\\Gamma_{j k i}=0\n$$\n\nby (6.2). (In a similar manner, it follows that $g^{i j}=0$; see Problem 6.34.)\n\nBecause of the above property of the metric tensor and its inverse, the operation of covariant differentiation commutes with those of raising and lowering indices. For example,\n\n$$\nT_{j, k}^{i}=\\left(g^{i r} T_{r j}\\right)_{, k}=g^{i r} T_{r j, k}\n$$", "6.12": "```latex\n\\section*{ABSOLUTE DIFFERENTIATION}\n6.12 Prove that (6.8) is the result of forming the inner product of the covariant derivative $\\left(T_{, j}^{i}\\right)$ with the tangent vector $\\left(d x^{i} / d t\\right)$ of the curve.\n\n$$\nT_{, j}^{i} \\frac{d x^{j}}{d t}=\\left(\\frac{\\partial T^{i}}{\\partial x^{j}}+\\Gamma_{r j}^{i} T^{r}\\right) \\frac{d x^{j}}{d t}=\\frac{\\partial T^{i}}{\\partial x^{j}} \\frac{d x^{j}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}=\\frac{d T^{i}}{d t}+\\Gamma_{r j}^{i} T^{r} \\frac{d x^{j}}{d t}\n$$\n```", "6.13": "```latex\n\\section*{ABSOLUTE DIFFERENTIATION}\n6.13 A particle is in motion along the circular arc given parametrically in spherical coordinates by $x^{1}=b, x^{2}=\\pi / 4, x^{3}=\\omega t \\quad(t=$ time $)$. Find its acceleration using the formula (6.10) and compare with the result $a=r \\omega^{2}$ from elementary mechanics.\n\nFrom Problem 6.5, we have along the circle\n\n$$\n\\begin{array}{cc}\n\\Gamma_{22}^{1}=-x^{1}=-b & \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}=-b \\sin ^{2} \\frac{\\pi}{4}=-\\frac{b}{2} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2}=-\\sin \\frac{\\pi}{4} \\cos \\frac{\\pi}{4}=-\\frac{1}{2} \\\\\n\\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}=\\frac{1}{b} & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}=\\cot \\frac{\\pi}{4}=1\n\\end{array}\n$$\n\nwith all other symbols vanishing. The components of acceleration are, from (6.9),\n\n$$\n\\begin{aligned}\n& a^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{r s}^{1} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{b}{2}\\right)(\\omega)^{2}=-\\frac{b \\omega^{2}}{2} \\\\\n& a^{2}=\\frac{d^{2} x^{2}}{d t^{2}}+\\Gamma_{r s}^{2} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\left(-\\frac{1}{2}\\right)(\\omega)^{2}=-\\frac{\\omega^{2}}{2} \\\\\n& a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}+\\Gamma_{r s}^{3} \\frac{d x^{r}}{d t} \\frac{d x^{s}}{d t}=0+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0\n\\end{aligned}\n$$\n\nTogether with the metric components along the circle,\n\n$$\ng_{11}=1 \\quad g_{22}=\\left(x^{1}\\right)^{2}=b^{2} \\quad g_{33}=\\left(x^{1}\\right)^{2} \\sin ^{2} x^{2}=\\frac{b^{2}}{2}\n$$\n\nthe acceleration components give, via (6.10),\n\n$$\na=\\sqrt{g_{i j} a^{i} a^{j}}=\\sqrt{(1)\\left(-b \\omega^{2} / 2\\right)^{2}+\\left(b^{2}\\right)\\left(-\\omega^{2} / 2\\right)^{2}+0}=b \\omega^{2} / \\sqrt{2}\n$$\n\nUpon introducing the radius of the circle, using (3.4) with $\\bar{x}^{1}=x=r$ and $x^{3}=0$,\n\nwe obtain $a=r \\omega^{2}$.\n\n$$\nr=b \\sin \\frac{\\pi}{4}=\\frac{b}{\\sqrt{2}}\n$$\n```", "6.14": "```latex\n\\section*{ABSOLUTE DIFFERENTIATION}\n6.14 Verify that $x^{1}=a \\sec x^{2}$ is a geodesic for the Euclidean metric in polar coordinates. [In rectangular coordinates $(x, y)$, the curve is $x=a$, a vertical line.]\n\nFirst choose a parameterization for the curve; say,\n\n\n\\begin{align*}\n& x^{1}=a \\sec t  \\tag{1}\\\\\n& x^{2}=t\n\\end{align*} \\quad(-\\pi / 2<t<\\pi / 2)\n\n\nParameter $t$ is related to the arc-length parameter $s$ via\n\n$$\n\\begin{aligned}\n\\frac{d s}{d t} & =\\sqrt{g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+(a \\sec t)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{a^{2} \\sec ^{2} t \\tan ^{2} t+a^{2} \\sec ^{2} t} \\\\\n& =(a \\sec t) \\sqrt{1+\\tan ^{2} t}=a \\sec ^{2} t\n\\end{aligned}\n$$\n\nor\n\n$$\n\\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a}\n$$\n\nso that for any function $x(t)$\n\n$$\n\\begin{aligned}\n& \\frac{d x}{d s}=\\frac{d x}{d t} \\frac{d t}{d s}=\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t} \\\\\n& \\frac{d^{2} x}{d s^{2}}=\\frac{d}{d t}\\left(\\frac{\\cos ^{2} t}{a} \\frac{d x}{d t}\\right) \\frac{d t}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x}{d t}\n\\end{aligned}\n$$\n\nNow, taking the nonzero Christoffel symbols from Example 6.3, we can rewrite the geodesic equations (6.13) in terms of the independent variable $t$ :\n\n$$\n0=\\frac{d^{2} x^{1}}{d s^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{1}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{1}}{d t}+\\left(-x^{1}\\right) \\frac{\\cos ^{4} t}{a^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{1}}{d t^{2}}-(2 \\tan t) \\frac{d x^{1}}{d t}-x^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\tag{2}\n\\end{equation*}\n\n\nand\n\n$$\n0=\\frac{d^{2} x^{2}}{d s^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d s} \\frac{d x^{2}}{d s}=\\frac{\\cos ^{4} t}{a^{2}} \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{2 \\sin t \\cos ^{3} t}{a^{2}} \\frac{d x^{2}}{d t}+2\\left(\\frac{1}{x^{1}}\\right)\\left(\\frac{\\cos ^{2} t}{a}\\right)^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}\n$$\n\nor\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{2}}{d t^{2}}-(2 \\tan t) \\frac{d x^{2}}{d t}+\\left(\\frac{2}{x^{1}}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=0 \\tag{3}\n\\end{equation*}\n\n\nAll that remains is to verify that the functions (1) satisfy the system (2)-(3). Substituting in (2):\n\n$$\na\\left(\\sec t+2 \\sec t \\tan ^{2} t\\right)-(2 \\tan t)(a \\sec t \\tan t)-(a \\sec t)(1)=0\n$$\n\nSubstituting in (3):\n\n$$\n0-(2 \\tan t)(1)+\\left(\\frac{2}{a \\sec t}\\right)(a \\sec t \\tan t)(1)=0 \\quad \\text { QED }\n```", "6.15": "\\section*{DIFFERENTIATION RULES}\n6.15 Prove the rules for covariant differentiation stated in Section 6.6.\n\n(a) The sum rule obviously holds, as (6.7) is linear in the tensor components.\n\n(b) Let $\\mathbf{T}=\\left(T_{j}^{i}\\right)$ and $\\mathbf{S}=\\left(S_{j}^{i}\\right)$ be two mixed tensors of order 2, with outer product $\\mathbf{U}=\\left(T_{r}^{p} S_{s}^{q}\\right)$. Then,\n\n$$\n\\begin{aligned}\n& T_{r, k}^{p} S_{s}^{q}+T_{r}^{p} S_{s, k}^{q} \\\\\n& \\quad=\\left(\\frac{\\partial T_{r}^{p}}{\\partial x^{k}}+\\Gamma_{t k}^{p} T_{r}^{t}-\\Gamma_{r k}^{t} T_{r}^{p}\\right) S_{s}^{q}+T_{r}^{p}\\left(\\frac{\\partial S_{s}^{q}}{\\partial x^{k}}+\\Gamma_{t i k}^{q} S_{s}^{t}-\\Gamma_{s k}^{t} S_{t}^{q}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\underbrace{\\left.\\frac{\\partial T_{r}^{p}}{\\partial x^{k}} S_{s}^{q}+T_{r}^{p} \\frac{\\partial S_{s}^{q}}{\\partial x^{k}}\\right)}_{\\partial U_{r s}^{p q} / \\partial x^{k}}+\\Gamma_{t k}^{p} U_{r s}^{t q}+\\Gamma_{t k}^{q} U_{r s}^{p t}-\\Gamma_{r k}^{t} U_{t s}^{p q}-\\Gamma_{s k}^{t} U_{r t}^{p q} \\\\\n& \\equiv U_{r s, k}^{p q}\n\\end{aligned}\n$$\n\nand this proof of the outer-product rule extends to arbitrary $\\mathbf{T}$ and $\\mathbf{S}$.\n\n(c) The inner-product rule follows from the outer-product rule and the following useful result: Contraction of indices and covariant differentiation commute. To prove this last, let $\\mathbf{R}=\\left(R_{k}^{i j}\\right)$. Then,\n\n$$\n\\begin{aligned}\nR_{k, l}^{i j} \\delta_{j}^{k} & =\\left(\\frac{\\partial R_{k}^{i j}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{i j}+\\Gamma_{t l}^{j} R_{k}^{i t}-\\Gamma_{k l}^{t} R_{t}^{i j}\\right) \\delta_{j}^{k} \\\\\n& =\\frac{\\partial R_{k}^{i k}}{\\partial x^{l}}+\\Gamma_{t l}^{i} R_{k}^{t k}+0=\\left(R_{k}^{i k}\\right)_{, l} \\quad \\text { QED }\n\\end{aligned}\n$$", "6.16": "```latex\n6.16 Instead of Problem 6.15, why not: \"Each rule is a tensor equation that is valid in rectangular coordinates, where covariant differentiation reduces to partial differentiation. Therefore, each rule holds in every coordinate system.'?\n\nIf the space metric is non-Euclidean, there is no way to transform to a rectangular coordinate system (in which the rules would indeed hold).\n```", "6.17": "```latex\n\\section*{UNIQUENESS OF THE ABSOLUTE DERIVATIVE}\n6.18 Prove Theorem 6.4.\n\nDenote by $\\Delta \\mathbf{T} / \\Delta t$ any tensor that satisfies the hypothesis of the theorem. The tensor equation\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t}=\\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\nis valid in rectangular coordinates $\\left(x^{i}\\right)$, since, in $\\left(x^{i}\\right)$, both sides coincide with $d \\mathbf{T} / d t$. But then (Section 4.3) the equation holds in every coordinate system; i.e.,\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta t}\n```\n", "6.18": "\\section*{UNIQUENESS OF THE ABSOLUTE DERIVATIVE}\n6.18 Prove Theorem 6.4.\n\nDenote by $\\Delta \\mathbf{T} / \\Delta t$ any tensor that satisfies the hypothesis of the theorem. The tensor equation\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t}=\\frac{\\delta \\mathbf{T}}{\\delta t}\n$$\n\nis valid in rectangular coordinates $\\left(x^{i}\\right)$, since, in $\\left(x^{i}\\right)$, both sides coincide with $d \\mathbf{T} / d t$. But then (Section 4.3) the equation holds in every coordinate system; i.e.,\n\n$$\n\\frac{\\Delta \\mathbf{T}}{\\Delta t} \\equiv \\frac{\\delta \\mathbf{T}}{\\delta t}\n$$", "7.1": "\\section*{Solved Problems}\n\\section*{LENGTH IN RIEMANNIAN SPACE}\n7.1 Determine the indicator of the tangent vector $\\mathrm{U}$ to the curve\n\n$$\nx^{1}=t^{3} \\quad x^{2}=t^{2} \\quad x^{3}=t\n$$\n\n$(-\\infty<t<\\infty)$ if the fundamental form is\n\n(a) $\\left(d x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{3}\\right)^{2}-6 d x^{1} d x^{3}+2 x^{1} x^{2} d x^{2} d x^{3}$\n\n(b) $\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+3\\left(d x^{3}\\right)^{2}$\n\n(a) $\\left(3 t^{2}\\right)^{2}+t^{4}(2 t)^{2}+t^{6}(1)^{2}-6\\left(3 t^{2}\\right)(1)+2\\left(t^{3}\\right)\\left(t^{2}\\right)(2 t)(1)=9 t^{6}+9 t^{4}-18 t^{2}=9 t^{2}\\left(t^{2}+2\\right)\\left(t^{2}-1\\right)$\n\nSince $t^{2}+2$ is always positive,\n\n$$\n\\varepsilon(\\mathbf{U})=\\left\\{\\begin{array}{rc}\n+1 & t \\geqq 1 \\\\\n-1 & 0<t<1 \\\\\n+1 & t=0 \\\\\n-1 & -1<t<0 \\\\\n+1 & t \\leqq-1\n\\end{array}\\right.\n$$\n\n(b) $\\varepsilon(\\mathbf{U}) \\equiv+1$, because the form is positive definite.", "7.2": "\\section*{Solved Problems}\n\\subsection*{7.2 Show that the following matrix defines a Riemannian metric on $\\mathbf{R}^{2}$ :}\n\n$$\nG=\\left[\\begin{array}{rr}\nx^{2} & -x^{1} \\\\\n-x^{1} & x^{2}\n\\end{array}\\right] \\quad\\left(x^{1}>0,-x^{1}<x^{2}<x^{1}\\right)\n$$\n\nWe must show that conditions A-D of Section 5.3 are satisfied.\n\nA. Since each $g_{i j}$ is linear in the $x^{i}$, it is differentiable to any order.\n\nB. By observation, the matrix is symmetric.\\\\\nC. $\\left|g_{i j}\\right|=\\left(x^{2}\\right)^{2}-\\left(x^{1}\\right)^{2}<0$ over the given domain.\n\nD. Extend the matrix to a tensor $\\mathbf{g}$ by using the tensor transformation laws to define the $\\bar{g}_{i j}$ in terms of the $g_{i j}$. This will then make the quadratic form $g_{i j} d x^{i} d x^{j}$, hence the distance formula, an invariant.", "7.3": "\\subsection*{7.3 Find the null set of the curve $\\mathscr{C}: x^{2}=\\left(x^{1}\\right)^{2} \\quad\\left(x^{1}>0\\right)$ under the metric of Problem 7.2.}\n\nLet $\\mathscr{C}$ be parameterized by $x^{1}=t, x^{2}=t^{2} \\quad(t>0)$. Then, along $\\mathscr{C}$,\n\n$$\ng_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{cc}\nt^{2} & -t \\\\\n-t & t^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n1 \\\\\n2 t\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n1 & 2 t\n\\end{array}\\right]\\left[\\begin{array}{c}\n-t^{2} \\\\\n-t+2 t^{3}\n\\end{array}\\right]=t^{2}\\left(4 t^{2}-3\\right)\n$$\n\nwhich, for positive $t$, vanishes only at $t=\\sqrt{3} / 2$.", "7.4": "\\subsection*{7.4 REGULAR CURVES: UNIT TANGENT VECTOR}\nLet a regular curve $\\mathscr{C}: x^{i}=x^{i}(s)$ be given in terms of the arc-length parameter; the tangent field is $\\mathbf{T} \\equiv\\left(d x^{i} / d s\\right)$. By definition of arc length,\n\n$$\ns=\\int_{0}^{s}\\|\\mathbf{T}(u)\\| d u\n$$\n\nand differentiation gives $1=\\|\\mathbf{T}(s)\\|$, showing that $\\mathbf{T}$ has unit length at each point of $\\mathscr{C}$.\n\nWhen it is inconvenient or impossible to convert to the arc-length parameter, we can, by (7.3),\\\\\nobtain $\\mathbf{T}$ by normalizing the tangent vector $\\mathbf{U}=\\left(d x^{i} / d t\\right)$ :\n\n\n\\begin{equation*}\n\\mathbf{T}=\\frac{1}{\\|\\mathbf{U}\\|} \\mathbf{U}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U} \\tag{7.4}\n\\end{equation*}\n\n\nIn Problem 7.20 is proved the useful\n\nTheorem 7.1: The absolute derivative $\\delta \\mathbf{T} / \\delta s$ of the unit tangent vector $\\mathbf{T}$ is orthogonal to $\\mathbf{T}$.\n\nEXAMPLE 7.4 The simple cubic $y=x^{3}$ has an inflection point at the origin, or $s=0$ (by arrangement). As shown in Fig. 7-1,\n\n$$\n\\lim _{s \\rightarrow-0} \\mathbf{N}_{0}=(0,-1) \\quad \\lim _{s \\rightarrow+0} \\mathbf{N}_{0}=(0,1)\n$$\n\nTo verify this analytically, make the parameterization $x=t, y=t^{3}$, and calculate $\\mathbf{N}_{0}$ as a function of $t$ $\\left(s^{\\prime}(t)=\\sqrt{1+9 t^{4}}\\right)$.\n\n$$\n\\begin{aligned}\n& \\mathbf{U}=\\left(x^{\\prime}(t), y^{\\prime}(t)\\right)=\\left(1,3 t^{2}\\right) \\\\\n& \\mathbf{T}=\\frac{1}{s^{\\prime}(t)} \\mathbf{U}=\\frac{1}{\\sqrt{1+9 t^{4}}}\\left(1,3 t^{2}\\right) \\\\\n& \\frac{d \\mathbf{T}}{d s}=\\frac{1}{s^{\\prime}(t)} \\frac{d \\mathbf{T}}{d t}=\\frac{6 t}{\\left(1+9 t^{4}\\right)^{2}}\\left(-3 t^{2}, 1\\right) \\\\\n& \\kappa_{0}=\\left\\|\\frac{d \\mathbf{T}}{d s}\\right\\|=\\frac{6|t|}{\\left(1+9 t^{4}\\right)^{3 / 2}}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-096}\n\\end{center}\n\nFig. 7-1\n\n$$\n\\mathbf{N}_{0}=\\frac{1}{\\kappa_{0}} \\frac{d \\mathbf{T}}{d s}=\\frac{t /|t|}{\\sqrt{1+9 t^{4}}}\\left(--3 t^{2}, 1\\right) \\quad(t \\neq 0)\n$$\n\nThe scalar factor $t /|t|$ accounts for the discontinuity in $\\mathbf{N}_{0}$ at $t=0 \\quad(s=0)$.", "7.5": "\\subsection*{7.5 Write $g \\equiv \\operatorname{det} G$ for the determinant of a Riemannian metric. Prove that $|g|$ is a differentiable function of the coordinates.}\n\nApplying the chain rule to $|g|=\\sqrt{g^{2}}$, we have\n\n\\begin{equation*}\n\\frac{\\partial|g|}{\\partial x^{i}}=\\frac{g}{|g|} \\frac{\\partial g}{\\partial x^{i}} \\tag{1}\n\\end{equation*}\n\nSince $\\partial g / \\partial x^{i}$ exists (Property A) and $|g| \\neq 0$ (Property C), the right-hand side of (1) is well-defined.", "7.6": "\\subsection*{7.6 GEODESICS AS SHORTEST ARCS}\nWhen the metric is positive definite, a geodesic may be defined by the zero-curvature conditions (6.13), or, equivalently, by the condition that for any two of its points sufficiently close together, its length between the two points is least among all curves joining those points.\n\nThe minimum-length development employs a variational argument. We need to assume that all curves under consideration are class $C^{2}$ (that is, the parametric functions which represent them have continuous second-order derivatives). Let $x^{i}=x^{i}(t)$ represent a shortest curve (geodesic) passing through $A=\\left(x^{i}(a)\\right)$ and $B=\\left(x^{i}(b)\\right)$, where $b-a$ is as small as necessary. Embed the geodesic in a one-parameter family of $C^{2}$ curves passing through $A$ and $B$ :\n\n$$\nx^{i}=X^{i}(t, u) \\equiv x^{i}(t)+(t-a)(b-t) u \\phi^{i}(t)\n$$\n\nwhere the multipliers $\\phi^{i}(t)$ are arbitrary twice-differentiable functions. The length of a curve in this family is given by\n\n$$\nL(u)=\\int_{a}^{b} \\sqrt{\\varepsilon g_{i j} \\frac{\\partial X^{i}}{\\partial t} \\frac{\\partial X^{j}}{\\partial t}} d t \\equiv \\int_{a}^{b} \\sqrt{w(t, u)} d t\n$$\n\nwith $\\varepsilon=1$ for a positive-definite metric. Since $X^{i}(t, 0)=x^{i}(t) \\quad(i=1,2, \\ldots, n)$, the function $L(u)$ must have a local minimum at $u=0$. Standard calculus techniques yield the following expression of the necessary condition $L^{\\prime}(0)=0$ :\n\n\n\\begin{equation*}\n\\int_{a}^{b}\\left[w^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}-\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right)\\right](t-a)(b-t) \\phi^{k}(t) d t=0 \\tag{7.10}\n\\end{equation*}\n\n\nin which\n\n\n\\begin{equation*}\nw \\equiv w(t, 0)=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t} \\tag{7.11}\n\\end{equation*}\n\n\nSince $(t-a)(b-t)>0$ on $(a, b)$ and $\\phi^{k}(t)$ may be chosen arbitrarily, the bracketed expression in (7.10) must vanish identically over $(a, b)$, for $k=1,2, \\ldots, n$; this leads to (Problem 7.21)\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad(i=1,2, \\ldots, n) \\tag{7.12}\n\\end{equation*}\n\n\nSystem (7.12), with $w$ defined by (7.11), are the differential equations for the geodesics of Riemannian space, in terms of the arbitrary curve parameter $t$. Assuming that these geodesics will be regular curves, we may choose $t=s=\\operatorname{arc}$ length. Then\n\n$$\nw=\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d s}{d s}\\right)^{2}=(1)^{2}=1 \\quad \\text { and } \\quad \\frac{d w}{d s}=0\n$$\n\nso that $(7.12)$ becomes\n\n\n\\begin{equation*}\n\\frac{d^{2} x^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d s} \\frac{d x^{k}}{d s}=0 \\quad(i=1,2, \\ldots, n) \\tag{7.13}\n\\end{equation*}\n\n\nwhich is precisely (6.13).\n\nIt must be emphasized that $L^{\\prime}(0)=0$ is only a necessary condition for minimum length, so that the geodesics are found among the solutions of (7.12) or (7.13).", "7.7": "\\subsection*{7.7 At the point of intersection $(0,0)$, find the angle between the curves}\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=2 t-2 \\\\\nx^{2}=t^{2}-1\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u^{4}-1 \\\\\nx^{2}=25 u^{2}+50 u-75\n\\end{array}\\right.\\right.\n$$\n\nif the Riemannian metric is given by $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}$.\n\nAt $t=1, \\mathbf{T} \\equiv\\left(d x^{i} / d t\\right)=(2,2)$; at $u=1, \\mathbf{U} \\equiv\\left(d x^{i} / d u\\right)=(4,100)$. Hence, using matrices,\n\n$$\n\\begin{aligned}\n& \\mathbf{T U}=\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=-200 \\\\\n& \\|\\mathbf{T}\\|^{2}=\\varepsilon_{1}\\left[\\begin{array}{ll}\n2 & 2\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n2 \\\\\n2\n\\end{array}\\right]=\\left(\\varepsilon_{1}\\right)(-4)=4 \\\\\n& \\|\\mathbf{U}\\|^{2}=\\varepsilon_{2}\\left[\\begin{array}{ll}\n4 & 100\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n4 \\\\\n100\n\\end{array}\\right]=\\left(\\varepsilon_{2}\\right)(-784)=784\n\\end{aligned}\n$$\n\n$$\n\\text { and } \\quad \\cos \\theta=\\frac{-200}{\\sqrt{4} \\sqrt{784}}=-\\frac{25}{7}\n$$\n\nThis is Case 2 of Section 7.2; we have\n\n$$\n\\theta=\\pi+i \\ln \\left(\\frac{25}{7}+\\sqrt{\\left(\\frac{25}{7}\\right)^{2}-1}\\right)=\\pi+i \\ln 7", "7.8": "\\subsection*{7.8 Verify that the vectors of Problem 7.7 do not obey the triangle inequality.}\n\nAs calculated, $\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|=2+28=30$. But\n\n$$\n\\|\\mathbf{T}+\\mathbf{U}\\|^{2}=\\varepsilon_{3}\\left[\\begin{array}{ll}\n6 & 102\n\\end{array}\\right]\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\n6 \\\\\n102\n\\end{array}\\right]=\\varepsilon_{3}(-1188)=1188\n$$\n\nwhence $\\|\\mathbf{T}+\\mathbf{U}\\| \\approx 34.46>\\|\\mathbf{T}\\|+\\|\\mathbf{U}\\|$.", "7.9": "\\section*{Solved Problems}\n\\section*{ARC-LENGTH PARAMETER, UNIT TANGENT VECTOR}\n7.9 Let $\\mathscr{C}: x^{i}=x^{i}(t)$ be any non-null curve. (a) Prove that $\\operatorname{arc}$ length along $\\mathscr{C}$ is defined as a strictly increasing function of $t$. (b) Exhibit the arc-length parameterization of $\\mathscr{C}$.\\\\\n(a)\n\nFor $t_{1}<t_{2}$, the Mean-Value Theorem of calculus gives\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left(t_{2}-t_{1}\\right) s^{\\prime}(\\tau) \\quad\\left(t_{1}<\\tau<t_{2}\\right)\n$$\n\nThe right-hand side is nonnegative, so that $s\\left(t_{1}\\right) \\leqq s\\left(t_{2}\\right)$. But, in view of the identity\n\n$$\ns\\left(t_{2}\\right)-s\\left(t_{1}\\right)=\\left[s\\left(t_{2}\\right)-s\\left(t_{3}\\right)\\right]+\\left[s\\left(t_{3}\\right)-s\\left(t_{1}\\right)\\right]\n$$\n\nwhere $t_{3}$ is any point in $\\left(t_{1}, t_{2}\\right)$, the equality $s\\left(t_{1}\\right)=c=s\\left(t_{2}\\right)$ would imply $s\\left(t_{3}\\right)=c$; i.e., $s(t)$ would be constant on $\\left[t_{1}, t_{2}\\right]$, making $s^{\\prime}(t) \\equiv 0$ on $\\left(t_{1}, t_{2}\\right)$ and thus making $\\mathscr{C}$ a null curve. We conclude that\n\n$$\ns\\left(t_{1}\\right)<s\\left(t_{2}\\right) \\quad \\text { whenever } \\quad t_{1}<t_{2}\n$$\n\n(b) The strictly increasing function $s(t)$ will possess a strictly increasing inverse; denote it as $t=\\theta(s)$. Then $\\mathscr{C}$ admits the parameterization $x^{i}=x^{i}(\\theta(s))$.", "7.10": "\\subsection*{7.10 (a) In rectangular coordinates $\\left(x^{1}, x^{2}\\right)$ but adopting the metric of Problem 7.7, find the null points of the parabola $\\mathscr{C}: x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$. (b) Show that the arc-length parameterization of $\\mathscr{C}$ is differentiable to all orders except at the null points. (c) Find the length of $\\mathscr{C}$.}\n\n\n\\begin{equation*}\n\\varepsilon\\left(\\frac{d s}{d t}\\right)^{2}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}-2 \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=1-4 t \\tag{a}\n\\end{equation*}\n\n\nso there is only one null point, at $t=1 / 4$.\n\n\n\\begin{equation*}\ns=\\int_{0}^{t} \\sqrt{\\varepsilon(1-4 u)} d u \\tag{b}\n\\end{equation*}\n\n\nThus, for $0 \\leqq t \\leqq 1 / 4$,\n\n$$\ns=\\int_{0}^{t} \\sqrt{1-4 u} d u=\\frac{1}{6}\\left[1-(1-4 t)^{3 / 2}\\right]\n$$\n\nand, for $1 / 4 \\leqq t \\leqq 1 / 2$,\n\n$$\ns=\\int_{0}^{1 / 4} \\sqrt{1-4 u} d u+\\int_{1 / 4}^{t} \\sqrt{4 u-1} d u=\\frac{1}{6}\\left[1+(4 t-1)^{3 / 2}\\right]\n$$\n\nInversion of these formulas gives\n\n\\[\nt=\\theta(s)=\\left\\{\\begin{array}{lr}\n\\frac{1}{4}\\left[1-(1-6 s)^{2 / 3}\\right] & 0 \\leqq s \\leqq 1 / 6  \\tag{1}\\\\\n\\frac{1}{4}\\left[1+(6 s-1)^{2 / 3}\\right] & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array}\\right.\n\\]\n\nIt is evident that $\\theta(s)$ is infinitely differentiable except at the null point $s=1 / 6$ (the image of $t=1 / 4)$; the same will be true of the functions $x^{1}=\\theta(s), x^{2}=\\theta^{2}(s)$.\n\n(c) Set $t=1 / 2$ in the applicable expression for $s$ :\n\n$$\ns=\\frac{1}{6}\\left[1+(2-1)^{3 / 2}\\right]=\\frac{1}{3}\n$$", "7.11": "\\subsection*{7.11 Find the arc length of the same curve $\\mathscr{C}$ as in Problem 7.10, but with the normal Euclidean metric, $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}$.}\n\nNow\n\n$$\n\\frac{d s}{d t}=\\sqrt{\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(\\frac{d x^{2}}{d t}\\right)^{2}}=\\sqrt{4 t^{2}+1}\n$$\n\nso that\n\n$$\nL=\\int_{0}^{1 / 2} \\sqrt{4 t^{2}+1} d t=\\left[\\frac{t}{2} \\sqrt{4 t^{2}+1}+\\frac{1}{4} \\ln \\left(2 t+\\sqrt{4 t^{2}+1}\\right)\\right]_{0}^{1 / 2}=\\frac{\\sqrt{2}+\\ln (1+\\sqrt{2})}{4} \\approx 0.574\n$$\n\nas compared to $L \\approx 0.333$ in Problem 7.10 .", "7.12": "\\subsection*{7.12 Using the arc-length parameterization found for the curve $\\mathscr{C}$ in Problem 7.10(b), compute the components $T^{i}(s)$ of the tangent vector and verify that this vector has unit length for all $s \\neq 1 / 6$.}\n\nWe have $\\left(T^{i}\\right)=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$, where $\\theta=\\theta(s)$ is the function defined by (1) in Problem $7.10(b)$. Hence,\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon\\left(\\theta^{\\prime 2}-4 \\theta \\theta^{\\prime 2}\\right)=\\varepsilon(1-4 \\theta) \\theta^{\\prime 2}\n$$\n\nBut, by (1) of Problem 7.10(b),\n\n$$\n1-4 \\theta=\\left\\{\\begin{array}{rrr}\n(1-6 s)^{2 / 3} & 0 \\leqq s \\leqq 1 / 6 \\\\\n-(6 s-1)^{2 / 3} & 1 / 6 \\leqq s \\leqq 1 / 3\n\\end{array} \\quad \\theta^{\\prime}=\\left\\{\\begin{array}{rr}\n(1-6 s)^{-1 / 3} & 0<s<1 / 6 \\\\\n(6 s-1)^{-1 / 3} & 1 / 6<s<1 / 3\n\\end{array}\\right.\\right.\n$$\n\nTherefore, $\\|\\mathbf{T}\\|^{2}=(\\varepsilon)( \\pm 1)=+1$, or $\\|\\mathbf{T}\\| \\equiv 1 \\quad(s \\neq 1 / 6)$.", "7.13": "\\subsection*{7.13 Prove that the Frenet equation (7.9) holds at each point of a regular curve when the metric is positive definite.}\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, we have (from property $\\mathrm{C}$ of $\\mathbf{N}$ ),\n\n\n\\begin{equation*}\n\\mathbf{N}=\\lambda \\frac{\\delta \\mathbf{T}}{\\delta s} \\tag{1}\n\\end{equation*}\n\n\nfrom some real $\\lambda$. Take the inner product with the vector $\\mathbf{N}$ in (1); with $\\varepsilon=\\varepsilon(\\mathbf{N})$,\n\n\n\\begin{equation*}\n\\varepsilon \\mathbf{N}^{2}=\\varepsilon \\lambda \\mathbf{N} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\lambda \\kappa \\quad \\text { or } \\quad 1=\\lambda \\kappa \\tag{2}\n\\end{equation*}\n\n\nThen $\\lambda=1 / \\kappa$, and substitution into (1) yields (7.9).\n\nAt a point where $\\|\\delta \\mathbf{T} / \\delta s\\|=0$, both $\\delta \\mathbf{T} / \\delta s=\\mathbf{0}$ (because the metric is positive definite) and $\\kappa=0$ (by $(7.8)$ ); the Frenet equation then holds trivially.", "7.14": "\\subsection*{7.14 For any regular two-dimensional curve $\\mathscr{C}: x^{i}=x^{i}(s)$, define the contravariant vector}\n\n\n\\begin{equation*}\n\\mathbf{N}=\\left(N^{i}\\right) \\equiv\\left(-T_{2} / \\sqrt{|g|}, T_{1} / \\sqrt{|g|}\\right) \\tag{7.16}\n\\end{equation*}\n\n\nwhere $\\mathbf{T}=\\left(T^{i}\\right)$ is the unit tangent vector along $\\mathscr{C}$ and $g=\\operatorname{det}\\left(g_{i j}\\right)$. Show that $\\mathbf{N}$ is a global unit normal for $\\mathscr{C}$.\n\nWe must show that the three properties of Section 7.5 are possessed by the given vector (except possibly at null points).\n\nA. Since $\\mathscr{C}$ is regular, the $T^{i}$, and with them the $T_{i}=g_{i j} T^{j}$, are in $C^{1}$. The same is true of $|g|$ (Problem 7.5), which function is strictly positive. Therefore, the $N^{i}$ are also in $C^{1}$.\n\nB. By (2.11), $g^{11}=g_{22} / g, g^{12}=g^{21}=-g_{12} / g$, and $g^{22}=g_{11} / g$. Hence,\n\n$$\n\\|\\mathbf{N}\\|^{2}=\\left|g_{11}\\left(T_{2}^{2} /|g|\\right)+2 g_{12}\\left(-T_{1} T_{2} /|g|\\right)+g_{22}\\left(T_{1}^{2} /|g|\\right)\\right|=\\frac{1}{|g|}\\left|g g^{22} T_{2}^{2}+2 g g^{12} T_{1} T_{2}+g g^{11} T_{1}^{2}\\right|=\\frac{|g|}{|g|}\\left|g^{i j} T_{i} T_{j}\\right|=\\left|T^{j} T_{j}\\right|=\\|\\mathbf{T}\\|^{2}\n$$\n\nand so $\\|\\mathbf{N}\\|=\\|\\mathbf{T}\\|=1$.\\\\\nC. $\\mathbf{N}$ is orthogonal to $\\mathbf{T}$ :\n\n$$\nN^{i} T_{i}=-\\frac{T_{2}}{\\sqrt{|g|}} T_{1}+\\frac{T_{1}}{\\sqrt{|g|}} T_{2}=0\n$$\n\nFurthermore, when $\\|\\delta \\mathbf{T} / \\delta s\\| \\neq 0$, then $\\mathbf{N}_{0}$ is defined and is also a vector orthogonal to $\\mathbf{T}$ (by Theorem 7.1). In two dimensions this implies $\\mathbf{N}= \\pm \\mathbf{N}_{0}=\\lambda(\\delta \\mathbf{T} / \\delta s)$.", "7.15": "\\subsection*{7.15 For the curve and metric of Problem 7.10, determine the local normal $\\mathbf{N}_{0}$ and, using Problem 7.14, a global normal $\\mathbf{N}$. Verify that the two stand in the proper relationship.}\n\nWe have $g_{11}=1, g_{12}=g_{21}=-1, g_{22}=0$ (all constants), and $\\mathbf{T}=\\left(\\theta^{\\prime}, 2 \\theta \\theta^{\\prime}\\right)$; therefore, for $s \\neq 1 / 6$,\n\nand\n\n$$\n\\begin{aligned}\n& \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=\\left(\\theta^{\\prime \\prime}, 2 \\theta^{\\prime 2}+2 \\theta \\theta^{\\prime \\prime}\\right) \\\\\n& =\\left\\{\\begin{array}{lr}\n\\left(2(1-6 s)^{-4 / 3},(1-6 s)^{-2 / 3}+(1-6 s)^{-4 / 3}\\right) & 0<s<1 / 6 \\\\\n\\left(-2(6 s-1)^{-4 / 3},(6 s-1)^{-2 / 3}-(6 s-1)^{-4 / 3}\\right) & 1 / 6<s<1 / 3\n\\end{array}\\right. \\\\\n& \\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\sqrt{\\varepsilon g_{i j} \\frac{d T^{i}}{d s} \\frac{d T^{j}}{d s}}=\\left\\{\\begin{array}{l}\n2(1-6 s)^{-1} \\\\\n2(6 s-1)^{-1}\n\\end{array}\\right. \\\\\n& \\text { Thus, } \\quad \\mathbf{N}_{0}=\\frac{\\delta \\mathbf{T}}{\\delta s} /\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{1 / 3}+\\frac{1}{2}(1-6 s)^{-1 / 3}\\right) \\\\\n\\left(-(6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{1 / 3}-\\frac{1}{2}(6 s-1)^{-1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nWith $g=-1$, Problem 7.14 gives $(s \\neq 1 / 6)$ :\n\n$$\n\\begin{aligned}\n& T_{1}=g_{1 j} T^{j}=T^{1}-T^{2}=\\theta^{\\prime}(1-2 \\theta)=\\left\\{\\begin{array}{l}\n\\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3} \\\\\n\\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\n\\end{array}\\right. \\\\\n& T_{2}=g_{2 j} T^{j}=-T^{1}=-\\theta^{\\prime}=\\left\\{\\begin{array}{l}\n-(1-6 s)^{-1 / 3} \\\\\n-(6 s-1)^{-1 / 3}\n\\end{array}\\right. \\\\\n& \\mathbf{N}=\\left(-T_{2}, T_{1}\\right)=\\left\\{\\begin{array}{l}\n\\left((1-6 s)^{-1 / 3}, \\frac{1}{2}(1-6 s)^{-1 / 3}+\\frac{1}{2}(1-6 s)^{1 / 3}\\right) \\\\\n\\left((6 s-1)^{-1 / 3}, \\frac{1}{2}(6 s-1)^{-1 / 3}-\\frac{1}{2}(6 s-1)^{1 / 3}\\right)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\nIt is seen that, as expected, $\\mathbf{N}=+\\mathbf{N}_{0}$ for $s<1 / 6$ and $\\mathbf{N}=-\\mathbf{N}_{0}$ for $s>1 / 6$. neither $\\mathbf{N}_{0}$ nor $\\mathbf{N}$ is defined at the null point $s=1 / 6$. For comparison, recall the situation in Examples 7.4 and 7.5: there the discontinuity in $\\mathbf{N}_{0}$ occurred at a regular point (the cubic has no null points under the Euclidean metric), and $\\mathbf{N}$ (either choice) was defined everywhere.", "7.16": "\\subsection*{7.16 Under the metric of Special Relativity (Example 7.1), a regular curve $\\mathscr{C}$ is given by}\n\n$$\nx^{1}=s^{2} \\quad x^{2}=\\frac{3 s}{5} \\quad x^{3}=\\frac{4 s}{5} \\quad x^{4}=s^{2}\n$$\n\n\\textit{for $0 \\leqq s \\leqq 1$. (a) Verify that $s$ is arc length for $\\mathscr{C}$, and show that the absolute derivative, $\\delta \\mathbf{T} / \\delta s$, of $\\mathbf{T}$ is a null vector at every point of the curve (hence, a local principal normal $\\mathbf{N}_{0}$ is nowhere defined on $\\mathscr{C}$ ). Construct a global principal normal for $\\mathscr{C}$ in such a manner that the corresponding curvature function is nonzero. Is more than one curvature function possible?}\n\n(a) We have $\\left(T^{i}\\right)=(2 s, 3 / 5,4 / 5,2 s)$ and\n\n$$\n\\left|g_{i j} T^{i} T^{j}\\right|=\\left|4 s^{2}+(9 / 25)+(16 / 25)-4 s^{2}\\right|=1\n$$\n\nhence, $s$ is an arc-length parameter. Also, since the $g_{i j}$ are constant, all Christoffel symbols vanish and\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d \\mathbf{T}}{d s}=(2,0,0,2) \\quad\\left\\|\\frac{\\delta T}{\\delta s}\\right\\|=\\sqrt{\\left|2^{2}+0^{2}+0^{2}-2^{2}\\right|}=0\n$$\n\nfor all $s$.\n\n(b) Any differentiable unit vector orthogonal to $\\mathbf{T}$ will do for $\\mathbf{N}$, which then determines the curvature through (7.8). In the orthonormality conditions\n\n$$\n\\begin{aligned}\n& 2 s N^{1}+\\frac{3}{5} N^{2}+\\frac{4}{5} N^{3}-2 s N^{4}=0 \\\\\n& \\left(N^{1}\\right)^{2}+\\left(N^{2}\\right)^{2}+\\left(N^{3}\\right)^{2}-\\left(N^{4}\\right)^{2}= \\pm 1\n\\end{aligned}\n$$\n\nwe may successively set $N^{1}=N^{4}=0, N^{3}=N^{4}=0$, and $N^{2}=N^{4}=0$ to obtain three candidate normals:\n\n$$\n\\begin{gathered}\n\\mathbf{N}_{1}=\\left(0,-\\frac{4}{5}, \\frac{3}{5}, 0\\right) \\quad \\mathbf{N}_{2}=\\frac{1}{\\sqrt{(9 / 25)+4 s^{2}}}\\left(-\\frac{3}{5}, 2 s, 0,0\\right) \\\\\n\\mathbf{N}_{3}=\\frac{1}{\\sqrt{(16 / 25)+4 s^{2}}}\\left(-\\frac{4}{5}, 0,2 s, 0\\right)\n\\end{gathered}\n$$\n\nThe constant $\\mathbf{N}_{1}$ yields $\\kappa_{1} \\equiv 0$; but $\\mathbf{N}_{2}$ and $\\mathbf{N}_{3}$ yield the distinct curvature functions\n\n$$\n\\kappa_{2}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{9} s^{2}}} \\quad \\kappa_{3}=\\frac{-1}{\\sqrt{\\frac{1}{4}+\\frac{25}{16} s^{2}}}\n$$\n\nNote that the Frenet equation is invalid for all these normals.", "7.17": "\\subsection*{7.17 Refer to Problems 7.10 and 7.15. Calculate the curvature functions $\\kappa_{0}$ and $\\kappa$, and discuss the variability of $\\kappa_{0}$ over the parabolic arc $0 \\leqq s \\leqq 1 / 3$.}\n\nOur previous results show that both curvatures are defined everywhere except $s=1 / 6$, with $\\kappa=\\kappa_{0}$ on $0 \\leqq s<1 / 6$ and $\\kappa=-\\kappa_{0}$ on $1 / 6<s \\leqq 1 / 3$ (cf. Problem 7.13). By Problem 7.15,\n\n$$\n\\kappa_{0}=\\left\\|\\frac{\\delta \\mathbf{T}}{\\delta s}\\right\\|=\\frac{2}{|1-6 s|} \\quad(s \\neq 1 / 6)\n$$\n\nwhence\n\n$$\n\\kappa=\\frac{2}{1-6 s} \\quad(s \\neq 1 / 6)\n$$\n\nIt is seen that $\\kappa_{0}$ has the same value, 2, at $s=0$ (the vertex, or point of greatest Euclidean curvature) and the undistinguished point $s=1 / 3$. Moreover, near the ordinary (from the Euclidean viewpoint) point $s=1 / 6$, the absolute curvature becomes arbitrarily large.", "7.18": "\\subsection*{7.18 (a) For any regular two-dimensional curve, derive the formula for absolute curvature}\n\n\n\\begin{equation*}\n\\kappa_{0}=\\sqrt{|g|}\\left|T^{1} \\frac{\\delta T^{2}}{\\delta s}-T^{2} \\frac{\\delta T^{1}}{\\delta s}\\right| \\tag{7.17}\n\\end{equation*}\n\n\n(b) Use (7.17) to check Problem 7.17.\n\n(a) By (7.8) and the remarks made following Example 7.5,\n\n\n\\begin{equation*}\n\\kappa_{0}=|\\kappa|=\\left|N_{j} \\frac{\\delta T^{j}}{\\delta s}\\right|=\\left|N_{1} \\frac{\\delta T^{1}}{\\delta s}+N_{2} \\frac{\\delta T^{2}}{\\delta s}\\right| \\tag{1}\n\\end{equation*}\n\n\nChoosing the global normal $\\left(N^{i}\\right)$ established in Problem 7.14, we have:\n\n$$\n\\begin{aligned}\nN_{1} & =g_{11} N^{1}+g_{12} N^{2}=\\left(g g^{22}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(-g g^{21}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{21} T_{1}+g^{22} T_{2}\\right)=-\\frac{\\gamma}{\\sqrt{|g|}} T^{2} \\\\\nN_{2} & =g_{21} N^{1}+g_{22} N^{2}=\\left(-g g^{12}\\right)\\left(\\frac{-T_{2}}{\\sqrt{|g|}}\\right)+\\left(g g^{11}\\right)\\left(\\frac{T_{1}}{\\sqrt{|g|}}\\right) \\\\\n& =\\frac{\\gamma}{\\sqrt{|g|}}\\left(g^{11} T_{1}+g^{12} T_{2}\\right)=\\frac{\\gamma}{\\sqrt{|g|}} T^{1}\n\\end{aligned}\n$$\n\nSubstitution of these components in (1) yields (7.17).\n\n(b) For the metric of Problem 7.10,\n\n$$\nG=\\left[\\begin{array}{rr}\n1 & -1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$g=\\operatorname{det} G=-1$ and absolute derivatives reduce to ordinary derivatives. Thus we can rewrite (7.17) in terms of the curve parameter $t$, as follows:\n\n$$\n\\kappa_{0}=\\sqrt{|g|} \\frac{\\left(T^{2}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{1}}{T^{2}}\\right)\\right|\n$$\n\nSubstituting\n\n$$\n\\begin{aligned}\n& s^{\\prime}(t)=\\sqrt{\\left|\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right|}=\\sqrt{\\left|1-\\frac{a^{2}}{t^{2}}\\right|}=\\frac{1}{t} \\sqrt{a^{2}-t^{2}}(\\neq 0) \\\\\n& T^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t}=\\frac{t}{\\sqrt{a^{2}-t^{2}}} \\\\\n& T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{a}{\\sqrt{a^{2}-t^{2}}}\n\\end{aligned}\n$$\n\nwe find: $\\kappa_{0}=a t\\left(a^{2}-t^{2}\\right)^{-3 / 2}$.", "7.19": "\\subsection*{7.19 Compute the absolute curvature of the logarithmic curve $\\mathscr{C}: x^{1}=t, x^{2}=a \\ln t$, for $\\frac{1}{2} \\leqq t<a$, if the Riemannian metric is}\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}\n$$\n\nAs the $g_{i j}$ are constants (with $g=-1$ ), we can proceed as in Problem 7.18(b). This time, the most convenient version of $(7.17)$ is\n\n$$\n\\kappa_{0}=\\sqrt{|g|} \\frac{\\left(T^{2}\\right)^{2}}{s^{\\prime}(t)}\\left|\\frac{d}{d t}\\left(\\frac{T^{1}}{T^{2}}\\right)\\right|\n$$\n\nSubstituting\n\n$$\n\\begin{aligned}\n& s^{\\prime}(t)=\\sqrt{\\left|\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right|}=\\sqrt{\\left|1-\\frac{a^{2}}{t^{2}}\\right|}=\\frac{1}{t} \\sqrt{a^{2}-t^{2}}(\\neq 0) \\\\\n& T^{1}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{1}}{d t}=\\frac{t}{\\sqrt{a^{2}-t^{2}}} \\\\\n& T^{2}=\\frac{1}{s^{\\prime}(t)} \\frac{d x^{2}}{d t}=\\frac{a}{\\sqrt{a^{2}-t^{2}}}\n\\end{aligned}\n$$\n\nwe find: $\\kappa_{0}=a t\\left(a^{2}-t^{2}\\right)^{-3 / 2}$.", "7.20": "\\subsection*{7.20 Prove Theorem 7.1.}\nAlong a regular curve we have\n\n$$\n\\|\\mathbf{T}\\|^{2}=\\varepsilon \\mathbf{T} \\mathbf{T}=1 \\quad \\text { or } \\quad \\mathbf{T T}=\\varepsilon\n$$\n\nwhere the indicator $\\varepsilon$ is constant, $|\\varepsilon|=1$, on the curve. By the inner-product rule for absolute differentiation, and the fact that the absolute derivative of an invariant is the ordinary derivative,\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta s} \\mathbf{T}+\\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s} \\equiv 2 \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=\\frac{d}{d s}(\\varepsilon)=0 \\quad \\text { or } \\quad \\mathbf{T} \\frac{\\delta \\mathbf{T}}{\\delta s}=0\n$$", "7.21": "\\subsection*{7.21 Establish (7.12).}\nStart with the conditions\n\n\n\\begin{equation*}\nw^{-1 / 2} \\frac{\\partial g_{i j}}{\\partial x^{k}} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{d}{d t}\\left(2 w^{-1 / 2} g_{i k} \\frac{d x^{i}}{d t}\\right) \\tag{1}\n\\end{equation*}\n\n\nBy use of the product and chain rules, the expression on the right may be written\n\n$$\n-w^{-3 / 2} \\frac{d w}{d t}\\left(g_{i k} \\frac{d x^{i}}{d t}\\right)+2 w^{-1 / 2}\\left(\\frac{\\partial g_{i k}}{\\partial x^{j}} \\frac{d x^{j}}{d t}\\right) \\frac{d x^{i}}{d t}+2 w^{-1 / 2} g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nPut this back in (1), multiply both sides by $w^{1 / 2}$, and go over to the notation $g_{i j k} \\equiv \\partial g_{i j} / \\partial x^{k}$ :\n\n$$\ng_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=-w^{-1} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}+2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}\n$$\n\nwhich rearranges to\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+2 g_{i k j} \\frac{d x^{j}}{d t} \\frac{d x^{i}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nMaking use of the symmetry of $g_{i j}$, the third term on the left may be split into two similar terms, yielding\n\n$$\n2 g_{i k} \\frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{j k i} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}+g_{k i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{w} g_{i k} \\frac{d w}{d t} \\frac{d x^{i}}{d t}\n$$\n\nDivide by 2 , multiply by $g^{p k}$, and sum on $k$ :\n\n$$\n\\delta_{i}^{p} \\frac{d^{2} x^{i}}{d t^{2}}-g^{p k} \\Gamma_{i j k} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\frac{1}{2 w} \\delta_{i}^{p} \\frac{d w}{d t} \\frac{d x^{i}}{d t} \\quad \\text { or } \\quad \\frac{d^{2} x^{p}}{d t^{2}}+\\Gamma_{j k}^{p} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2 w} \\frac{d w}{d t} \\frac{d x^{p}}{d t}\n$$\n\nwhich is $(7.12)$.", "7.22": "\\subsection*{7.22 In a Riemannian 2-space with fundamental form $\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, determine $(a)$ the regular geodesics, $(b)$ the null geodesics.}\n\nHere $g_{11}=1, g_{12}=g_{21}=0, g_{22}=-\\left(x^{2}\\right)^{-2}$; Problem 6.4 gives\n\n$$\n\\Gamma_{22}^{2}=\\frac{d}{d x^{2}}\\left[\\frac{1}{2} \\ln \\left(x^{2}\\right)^{-2}\\right]=-\\frac{1}{x^{2}}\n$$\n\nas the only nonvanishing Christoffel symbol.\n\n(a) The system (7.13) becomes\n\n$$\n\\frac{d^{2} x^{1}}{d s^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d s^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=0\n$$\n\nThe first equation integrates to $x^{1}=a s+x_{0}^{1}$. In the second, let $u \\equiv d x^{2} / d s$ :\n\n$$\nu \\frac{d u}{d x^{2}}-\\frac{1}{x^{2}} u^{2}=0 \\quad \\text { or } \\quad \\frac{d u}{u}=\\frac{d x^{2}}{x^{2}} \\quad \\text { or } \\quad u=c x^{2}\n$$\n\nfrom which\n\n$$\n\\frac{d x^{2}}{d s}=c x^{2} \\quad \\text { or } \\quad \\frac{d x^{2}}{x^{2}}=c d s \\quad \\text { or } \\quad x^{2}=x_{0}^{2} e^{c s}\n$$\n\nAs our notation indicates, an arbitrary point $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ is the origin $(s=0)$ of a family of geodesics that seems to depend on two parameters, $a$ and $c$. However, $s$ must represent arc length, so that\n\n$$\n\\pm 1=\\left(\\frac{d x^{1}}{d s}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d s}\\right)^{2}=a^{2}-c^{2}\n$$\n\nHence, either $a^{2}=c^{2}+1$ (the fundamental form is positive) or $c^{2}=a^{2}+1$ (the fundamental form is negative). Both cases may be accounted for by a single parameter, $\\lambda$, if $s$ is eliminated between the parametric equations for $x^{1}$ and $x^{2}$ :\n\nregular geodesics $\\quad x^{2}=x_{0}^{2} \\exp \\left[\\lambda\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad(|\\lambda| \\neq 1)$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-107}\n\\end{center}\n\nFig. 7-3\n\n(b) System (7.14), in $t$, becomes\n\n$$\n\\begin{gathered}\n\\frac{d^{2} x^{1}}{d t^{2}}=0 \\quad \\frac{d^{2} x^{2}}{d t^{2}}-\\frac{1}{x^{2}}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0 \\\\\n\\left(\\frac{d x^{1}}{d t}\\right)^{2}-\\left(x^{2}\\right)^{-2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}=0\n\\end{gathered}\n$$\n\nIt is clear that the solution may be found by formally replacing $s$ by $t$ in part ( $a$ ) and setting $a^{2}=c^{2}$. Thus, the null geodesics through $\\left(x_{0}^{1}, x_{0}^{2}\\right)$ are given by\n\n$$\n\\text { null geodesics } \\quad x^{2}=x_{0}^{2} \\exp \\left[+\\left(x^{1}-x_{0}^{1}\\right)\\right] \\quad \\text { and } \\quad x^{2}=x_{0}^{2} \\exp \\left[-\\left(x^{1}-x_{0}^{1}\\right)\\right]\n$$\n\nNote that the null geodesics correspond to the exceptional values $\\lambda= \\pm 1$ in part (a). Figure 7-3 is a sketch of the geodesics through the point $(1,-1)$ in cartesian coordinates.", "7.23": "\\subsection*{7.23 Without converting to arc length, verify that in spherical coordinates, under the Euclidean metric}\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}+\\left(x^{1} \\sin x^{2} d x^{3}\\right)^{2}\n$$\n\nany curve of the form $\\mathscr{C}: x^{1}=a \\sec t, x^{2}=t+b, x^{3}=c(a, b, c$ constant) is a geodesic. (It should be apparent that $\\mathscr{C}$ is a straight line.)\n\nThe equations (7.12) must be verified. The Christoffel symbols $\\Gamma_{j k}^{i}$ for spherical coordinates are (Problem 6.5):\n\n$$\ni=1 \\quad \\Gamma_{22}^{1}=-x^{1}, \\quad \\Gamma_{33}^{1}=-x^{1} \\sin ^{2} x^{2}\n$$\n\n$$\n\\begin{array}{lll}\ni=2 & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{1}{x^{1}}, & \\Gamma_{33}^{2}=-\\sin x^{2} \\cos x^{2} \\\\\ni=3 & \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{1}{x^{1}}, & \\Gamma_{23}^{3}=\\Gamma_{32}^{3}=\\cot x^{2}\n\\end{array}\n$$\n\nThe derivatives of the $x^{i}(t)$ are:\n\n$$\n\\begin{gathered}\n\\frac{d x^{1}}{d t}=a \\sec t \\tan t, \\quad \\frac{d^{2} x^{1}}{d t^{2}}=(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right) \\\\\n\\frac{d x^{2}}{d t}=1, \\quad \\frac{d^{2} x^{2}}{d t^{2}}=0 \\quad \\text { and } \\quad \\frac{d x^{3}}{d t}=\\frac{d^{2} x^{3}}{d t^{2}}=0\n\\end{gathered}\n$$\n\nWith $\\varepsilon \\equiv 1,(7.11)$ gives\n\nand\n\n$$\n\\begin{aligned}\n& w=g_{i j} \\frac{d x^{i}}{d t} \\frac{d x^{j}}{d t}=\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\left(x^{1} \\sin x^{2}\\right)^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n&=(a \\sec t \\tan t)^{2}+(a \\sec t)^{2}(1)^{2}+0=a^{2} \\sec ^{4} t \\\\\n& \\frac{1}{2 w} \\frac{d w}{d t}=\\frac{\\left(4 a^{2} \\sec ^{3} t\\right)(\\sec t \\tan t)}{2 a^{2} \\sec ^{4} t}=2 \\tan t\n\\end{aligned}\n$$\n\nFor convenience in the verification of (7.12), let LS denote the left side, and RS the right side, of the equation in question. We obtain:\n\n$$\n\\begin{aligned}\ni=\\mathbf{1} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{1}}{d t^{2}}+\\Gamma_{22}^{1}\\left(\\frac{d x^{2}}{d t}\\right)^{2}+\\Gamma_{33}^{1}\\left(\\frac{d x^{3}}{d t}\\right)^{2} \\\\\n& =(a \\sec t)\\left(\\tan ^{2} t+\\sec ^{2} t\\right)-(a \\sec t)(1)^{2}+0=2 a \\sec t \\tan ^{2} t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{1}}{d t}=(2 \\tan t)(a \\sec t \\tan t)=2 a \\sec t \\tan ^{2} t=\\mathrm{LS} \\\\\ni=\\mathbf{2} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{2}}{d t^{2}}+2 \\Gamma_{12}^{2} \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}+\\Gamma_{33}^{2}\\left(\\frac{d x^{3}}{d t}\\right)^{2}=0+\\frac{2}{a \\sec t}(a \\sec t \\tan t)(1)+0=2 \\tan t \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{2}}{d t}=2 \\tan t=\\mathrm{LS} \\\\\n\\boldsymbol{i}=\\mathbf{3} \\quad \\mathrm{LS} & =\\frac{d^{2} x^{3}}{d t^{2}}+2 \\Gamma_{13}^{3} \\frac{d x^{1}}{d t} \\frac{d x^{3}}{d t}+2 \\Gamma_{23}^{3} \\frac{d x^{2}}{d t} \\frac{d x^{3}}{d t}=0 \\\\\n\\mathrm{RS} & =(2 \\tan t) \\frac{d x^{3}}{d t}=0=\\mathrm{LS}\n\\end{aligned}\n$$", "8.1": "\\subsection*{8.1 Prove (8.1).}\nBy definition of the covariant derivative,\n\n\n\\begin{equation*}\nV_{i, j k}=\\left(V_{i, j}\\right)_{, k}=\\frac{\\partial}{\\partial x^{k}}\\left(V_{i, j}\\right)-\\Gamma_{i k}^{r}\\left(V_{r, j}\\right)-\\Gamma_{j k}^{r}\\left(V_{i, r}\\right) \\tag{1}\n\\end{equation*}\n\n\nSubstitute\n\n$$\nV_{i, j}=\\frac{\\partial V_{i}}{\\partial x^{j}}-\\Gamma_{i j}^{s} V_{s}\n$$\n\nin (1), carry out the differentiation, and remove parentheses:\n\n\n\\begin{equation*}\nV_{i, j k}=\\frac{\\partial^{2} V_{i}}{\\partial x^{k} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}-\\Gamma_{i j}^{s} \\frac{\\partial V_{s}}{\\partial x^{k}}-\\Gamma_{i k}^{r} \\frac{\\partial V_{r}}{\\partial x^{j}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}-\\Gamma_{j k}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{j k}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{2}\n\\end{equation*}\n\n\nInterchanging $j$ and $k$ yields\n\n\n\\begin{equation*}\nV_{i, k j}=\\frac{\\partial^{2} V_{i}}{\\partial x^{j} \\partial x^{k}}-\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i k}^{s} \\frac{\\partial V_{s}}{\\partial x^{j}}-\\Gamma_{i j}^{r} \\frac{\\partial V_{r}}{\\partial x^{k}}+\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s}-\\Gamma_{k j}^{r} \\frac{\\partial V_{i}}{\\partial x^{r}}+\\Gamma_{k j}^{r} \\Gamma_{i r}^{s} V_{s} \\tag{3}\n\\end{equation*}\n\n\nSubtracting (3) from (2), one sees that the first, third, fourth, sixth, and seventh terms on the right of (2) cancel with the first, fourth, third, sixth, and seventh terms on the right of (3), leaving\n\n$$\n\\begin{aligned}\nV_{i, j k}-V_{i, k j} & =-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}} V_{s}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s} V_{s}+\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}} V_{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s} V_{s} \\\\\n& =\\left(\\frac{\\partial \\Gamma_{i k}^{s}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{k}}+\\Gamma_{i k}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\Gamma_{r k}^{s}\\right) V_{s}=R_{i j k}^{s} V_{s}\n\\end{aligned}\n$$", "8.2": "8.2 Show that at any point where the Christoffel symbols vanish,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=0\n$$\n\nIn this case the expression for $R_{j k l}^{i}$ reduces to just $\\partial \\Gamma_{j l}^{i} / \\partial x^{k}-\\partial \\Gamma_{j k}^{i} / \\partial x^{l}$. Therefore,\n\n$$\nR_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=\\frac{\\partial \\Gamma_{j l}^{i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k}^{i}}{\\partial x^{l}}+\\frac{\\partial \\Gamma_{k j}^{i}}{\\partial x^{l}}-\\frac{\\partial \\Gamma_{k l}^{i}}{\\partial x^{j}}+\\frac{\\partial \\Gamma_{l k}^{i}}{\\partial x^{j}}-\\frac{\\partial \\Gamma_{l j}^{i}}{\\partial x^{k}}\n$$\n\nAs all the terms cancel, the desired relationship is proved.", "8.3": "\\subsection*{8.3 Prove that for an arbitrary second-order covariant tensor $\\left(T_{i j}\\right)$}\n\n$$\nT_{i j, k l}-T_{i j, l k}=R_{i k l}^{s} T_{s j}+R_{j k l}^{s} T_{i s}\n$$\n\n(The general formula,\n\n\\begin{equation*}\nT_{i_{1} i_{2} \\ldots i_{p}, k l}-T_{i_{1} i_{2} \\ldots i_{p}, l k}=\\sum_{q=1}^{p} R_{i_{q} k l}^{s} T_{i_{1} \\ldots i_{q-1} s i_{q+1} \\ldots i_{p}} \\tag{8.18}\n\\end{equation*}\n\nwhich is credited to Ricci, is similarly established.)\n\nA direct approach would be quite tedious; instead, first establish that\n\n\\begin{equation*}\nV_{, j k}^{i}-V_{, k j}^{i}=-R_{s j k}^{i} V^{s} \\tag{1}\n\\end{equation*}\n\nfor any contravariant vector $\\left(V^{i}\\right)$ (see Problem 8.16). Now observe that $\\left(V^{q} T_{i q}\\right)$ is a covariant vector, to which (8.1) applies. Thus,\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, k l}-\\left(V^{q} T_{i q}\\right)_{, l k}=R_{i k l}^{s} V^{q} T_{s q} \\tag{2}\n\\end{equation*}\n\nBy the inner-product rule for covariant differentiation,\n\n\\begin{align*}\n& \\left(V^{q} T_{i q}\\right)_{, k}=V_{, k}^{q} T_{i q}+V^{q} T_{i q, k}  \\tag{3}\\\\\n& \\left(V^{q} T_{i q}\\right)_{, k l}=V_{, k l}^{q} T_{i q}+V_{, k}^{q} T_{i q, l}+V_{, l}^{q} T_{i q, k}+V^{q} T_{i q, k l}\n\\end{align*}\n\nInterchange $k$ and $l$ :\n\n\\begin{equation*}\n\\left(V^{q} T_{i q}\\right)_{, l k}=V_{, l k}^{q} T_{i q}+V_{, l}^{q} T_{i q, k}+V_{, k}^{q} T_{i q, l}+V^{q} T_{i q, l k} \\tag{4}\n\\end{equation*}\n\nSubtraction of (4) from (3) will cancel the middle two terms on the right-hand sides, leaving\n\n\\begin{equation*}\nR_{i k l}^{s} V^{q} T_{s q}=\\left(V_{, k l}^{q}-V_{, l k}^{q}\\right) T_{i q}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q} \\tag{5}\n\\end{equation*}\n\nNow use (1) in the right member of (5):\n\n$$\nR_{i k l}^{s} V^{q} T_{s q}=-R_{q k l}^{s} V^{q} T_{i s}+\\left(T_{i q, k l}-T_{i q, l k}\\right) V^{q}\n$$\n\nwhich may be rearranged into\n\n$$\n\\left[\\left(T_{i q, k l}-T_{i q, l k}\\right)-\\left(R_{i k l}^{s} T_{s q}+R_{q k l}^{s} T_{i s}\\right)\\right] V^{q}=0\n$$\n\nBut $\\left(V^{i}\\right)$ is arbitrary, so the bracketed expression must vanish. QED", "8.4": "\\section*{PROPERTIES OF THE RIEMANN TENSOR}\n8.4 Establish (8.4).\n\nBy definition,\n\n$$\n\\begin{aligned}\nR_{i j k l} & =g_{i s} R_{j k l}^{s}=g_{i s} \\frac{\\partial \\Gamma_{j l}^{s}}{\\partial x^{k}}-g_{i s} \\frac{\\partial \\Gamma_{j k}^{s}}{\\partial x^{l}}+g_{i s} \\Gamma_{j l}^{r} \\Gamma_{r k}^{s}-g_{i s} \\Gamma_{j k}^{r} \\Gamma_{r l}^{s} \\\\\n& =\\frac{\\partial\\left(g_{i s} \\Gamma_{j l}^{s}\\right)}{\\partial x^{k}}-\\frac{\\partial g_{i s}}{\\partial x^{k}} \\Gamma_{j l}^{s}-\\frac{\\partial\\left(g_{i s} \\Gamma_{j k}^{s}\\right)}{\\partial x^{l}}+\\frac{\\partial g_{i s}}{\\partial x^{l}} \\Gamma_{j k}^{s}+\\Gamma_{j l}^{r} \\Gamma_{r k i}-\\Gamma_{j k}^{r} \\Gamma_{r l i}\n\\end{aligned}\n$$\n\n$$\n=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{j k}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{r l i}\\right)-\\Gamma_{j l}^{r}\\left(\\frac{\\partial g_{i r}}{\\partial x^{k}}-\\Gamma_{r k i}\\right)\n$$\n\nRecall from (6.2) that for arbitrary index $l$,\n\n$$\n\\frac{\\partial g_{i r}}{\\partial x^{l}}-\\Gamma_{l r i}=\\Gamma_{i l r}\n$$\n\nBy substitution,\n\n$$\nR_{i j k l}=\\frac{\\partial \\Gamma_{j l i}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{j k i}}{\\partial x^{l}}+\\Gamma_{i l r} \\Gamma_{j k}^{r}-\\Gamma_{i k r} \\Gamma_{j l}^{r}\n$$", "8.5": "\\subsection*{8.5 Establish the first skew-symmetry property, $R_{i j k l}=-R_{j i k l}$.}\n\nTo save writing, let\n\n$$\nG_{k l}^{i j} \\equiv \\frac{1}{2}\\left(\\frac{\\partial^{2} g_{i j}}{\\partial x^{k} \\partial x^{l}}+\\frac{\\partial^{2} g_{k l}}{\\partial x^{i} \\partial x^{j}}\\right) \\quad \\text { and } \\quad H_{k l}^{i j} \\equiv \\Gamma_{i j r} \\Gamma_{k l}^{r}\n$$\n\nNote the obvious symmetry properties\n\n$$\nG_{k l}^{i j}=G_{k l}^{j i}=G_{l k}^{i j} \\quad \\text { and } \\quad H_{k l}^{i j}=H_{k l}^{j i}=H_{l k}^{i j}\n$$\n\nAlso, it is clear that $G_{k l}^{i j}=G_{i j}^{k l}$; furthermore,\n\n$$\nH_{k l}^{i j}=\\left(g_{r s} \\Gamma_{i j}^{s}\\right) \\Gamma_{k l}^{r}=\\Gamma_{i j}^{s}\\left(g_{s r} \\Gamma_{k l}^{r}\\right)=\\Gamma_{i j}^{s} \\Gamma_{k l s}=H_{i j}^{k l}\n$$\n\nNow, by (8.5),\n\nand\n\n$$\n\\begin{aligned}\n& R_{i j k l}=G_{j k}^{i l}-G_{j l}^{i k}+H_{j k}^{i l}-H_{j l}^{i k} \\\\\n& R_{j i k l}=G_{i k}^{j l}-G_{i l}^{j k}+H_{i k}^{j l}-H_{i l}^{j k}=G_{j l}^{i k}-G_{j k}^{i l}+H_{j l}^{i k}-H_{j k}^{i l}=-R_{i j k l}\n\\end{aligned}\n$$", "8.6": "```latex\n8.6 List the independent, potentially nonzero components of $R_{i j k l}$ for $n=5$ and verify the formula of Theorem 8.2 in this case.\n\nType A: $R_{1212}, R_{1313}, R_{1414}, R_{1515}$\n\n$R_{2323}, R_{2424}, R_{2525}$\n\n$R_{3434}, R_{3535}$\n\n$R_{4545}$\n\nType B: $R_{1213}, R_{1214}, R_{1215}, R_{1314}, R_{1315}, R_{1415}$\n\n$R_{2123}, R_{2124}, R_{2125}, R_{2324}, R_{2325}, R_{2425}$\n\n$R_{3132}, R_{3134}, R_{3135}, R_{3234}, R_{3235}, R_{3435}$\n\n$R_{4142}, R_{4143}, R_{4145}, R_{4243}, R_{4245}, R_{4345}$\n\n$R_{5152}, R_{5153}, R_{5154}, R_{5253}, R_{5254}, R_{5354}$\n\nType C: $R_{1234}, R_{1235}, R_{1245}, R_{1345}, R_{2345}$\n\n$R_{1324}, R_{1325}, R_{1425}, R_{1435}, R_{2435}$\n\nThere are 10 components of types $\\mathrm{A}$ and $\\mathrm{C}$ each, and 30 of type B; or 50 altogether. From the formula,\n\n$$\n\\frac{n^{2}\\left(n^{2}-1\\right)}{12}=\\frac{5^{2}\\left(5^{2}-1\\right)}{12}=\\frac{(25)(24)}{12}=50\n$$\n```", "8.7": "\\section*{RIEMANNIAN CURVATURE}\n8.7 Prove (8.11).\n\nBy Corollary 8.3 and the corresponding result for the $G_{i j k l}$,\n\n$$\n\\mathrm{K}=\\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}}=\\frac{R_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}{G_{1212}\\left[\\left(U^{1}\\right)^{2}\\left(V^{2}\\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\\left(U^{2}\\right)^{2}\\left(V^{1}\\right)^{2}\\right]}=\\frac{R_{1212}}{G_{1212}}=\\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}}\n$$", "8.8": "\\subsection*{8.8 Calculate $\\mathrm{K}$ for the Riemannian metric $\\varepsilon d s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}$, using the result of Problem 8.7. 6.4 ,}\n\nWe have only to calculate $R_{1212}=g_{11} R_{212}^{1}$. The nonvanishing Christoffel symbols are, by Problem\n\n$$\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}}\n$$\n\nConsequently,\n\nand\n\n$$\n\\begin{gathered}\nR_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n=\\frac{1}{\\left(x^{1}\\right)^{2}}-\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right)\\left(-\\frac{1}{x^{1}}\\right)=\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\n\\mathrm{~K}=\\frac{g_{11} R_{212}^{1}}{g_{11} g_{22}}=\\frac{R_{212}^{1}}{g_{22}}=\\frac{\\left(x^{1}\\right)^{-2}}{-\\left(x^{1}\\right)^{-2}}=-1 .\n\\end{gathered}\n$$", "8.9": "\\section*{RIEMANNIAN CURVATURE}\n8.9 Derive the form (8.12) of the curvature equation.\n\nWe need only establish the summations over the type-C terms in the numerator; the rest of the work was done in Example 8.4.\n\nFirst of all, let us verify that all $R_{i j k l}$ with $i j k l$ a permutation of $a b c d$, where $a<b<c<d$ are distinct integers, are generated by the skew and block symmetries of the three components $R_{a b c d}, R_{a c b d}$, and $R_{a d b c}$. Examination of Table 8-1, which uses an obvious notation for the symmetry operators, shows that all $4 !=24$ permutations are accounted for. Consequently, the type-C part of the numerator of (8.12) is [cf. the equation preceding $(8.10)$ ]\n\n\n\\begin{equation*}\n2 \\sum R_{a b c d} W_{a b c d}+2 \\sum R_{a c b d} W_{a c b d}+2 \\sum R_{a d b c} W_{a d b c} \\tag{1}\n\\end{equation*}\n\n\nTable 8-1\n\n\\begin{center}\n\\begin{tabular}{|c|l|l|l|}\n\\hline\n\\multirow{2}{*}{}\\begin{tabular}{c}\nSymmetry \\\\\nOperator \\\\\n\\end{tabular} & \\multicolumn{3}{|c|}{Subscript Chain} \\\\\n\\cline { 2 - 4 }\n & $\\boldsymbol{a b c d}$ & $\\boldsymbol{a} \\boldsymbol{b} \\boldsymbol{d}$ & $\\boldsymbol{a d b} \\boldsymbol{c}$ \\\\\n\\hline\n$\\mathrm{I}$ & $a b c d$ & $a c b d$ & $a d b c$ \\\\\n$\\mathrm{~S}_{1}$ & $b a c d$ & $c a b d$ & $d a b c$ \\\\\n$\\mathrm{~S}_{2}$ & $a b d c$ & $a c d b$ & $a d c b$ \\\\\n$\\mathrm{~S}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{2} \\mathrm{~S}_{1}$ & $b a d c$ & $c a d b$ & $d a c b$ \\\\\n\\hline\n$\\mathrm{B}$ & $c d a b$ & $b \\bar{d} a c$ & $b \\bar{b} a \\bar{d}$ \\\\\n$\\mathrm{BS}_{1}=\\mathrm{S}_{2} \\mathrm{~B}$ & $c d b a$ & $b d c a$ & $b c d a$ \\\\\n$\\mathrm{BS}_{2}=\\mathrm{S}_{1} \\mathrm{~B}$ & $d c a b$ & $d b a c$ & $c b a d$ \\\\\n$\\mathrm{BS}_{1} \\mathrm{~S}_{2}=\\mathrm{S}_{1} \\mathrm{~S}_{2} \\mathrm{~B}$ & $d c b a$ & $d b c a$ & $c b d a$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nThe first summation is over all $a<b<c<d$ that yield a nonzero $R_{a b c d}$ in the basic set; similarly for the second summation. The third summation does not involve the basic set, but the symmetries of $R_{i j k l}$ (shared by $W_{i j k l}$ ) allow its absorption in the first two summations. Thus, by Bianchi's identity,\n\n\n\\begin{equation*}\n2 R_{a d b c} W_{a d b c}=2\\left(-R_{a b c d}-R_{a c d b}\\right) W_{a d b c}=-2 R_{a b c d} W_{a d b c}-2 R_{a c b d} W_{a d c b} \\tag{2}\n\\end{equation*}\n\n\nand substitution of (2) in (1) produces the expression given in (8.12).", "8.10": "\\section*{RIEMANNIAN CURVATURE}\n8.10 Prove (8.13).\n\nWe have\n\n$$\n\\begin{aligned}\nW_{i j k l}(\\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V}) & =\\left|\\begin{array}{cc}\n\\lambda U^{i}+\\nu V^{i} & \\lambda U^{j}+\\nu V^{j} \\\\\n\\mu U^{i}+\\omega V^{i} & \\mu U^{j}+\\omega V^{j}\n\\end{array}\\right|\\left|\\begin{array}{cc}\n\\lambda U^{k}+\\nu V^{k} & \\lambda U^{l}+\\nu V^{l} \\\\\n\\mu U^{k}+\\omega V^{k} & \\mu U^{l}+\\omega V^{l}\n\\end{array}\\right| \\\\\n& =\\left|\\begin{array}{cc}\n\\lambda & \\nu \\\\\n\\mu & \\omega\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{i} & U^{j} \\\\\nV^{i} & V^{j}\n\\end{array}\\right|\\left|\\begin{array}{ll}\nU^{k} & U^{l} \\\\\nV^{k} & V^{l}\n\\end{array}\\right|=(\\lambda \\omega-\\nu \\mu)^{2} W_{i j k l}(\\mathbf{U}, \\mathbf{V})\n\\end{aligned}\n$$\n\nso that the quantity $(\\lambda \\omega-\\nu \\mu)^{2}$ factors out of all terms in (8.12) for $\\mathrm{K}(\\mathbf{x} ; \\lambda \\mathbf{U}+\\nu \\mathbf{V}, \\mu \\mathbf{U}+\\omega \\mathbf{V})$, leaving $\\mathrm{K}(\\mathbf{x} ; \\mathbf{U}, \\mathbf{V})$.", "8.11": "\\subsection*{8.11 Find the isotropic points in the Riemannian space $\\mathbf{R}^{3}$ with metric}\n\n$$\ng_{11}=1 \\quad g_{22}=g_{33}=\\left(x^{1}\\right)^{2}+1 \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nand calculate the curvature $\\mathrm{K}$ at those points.\n\nFollow Example 8.4. By Problem 6.4, the nonzero Christoffel symbols are\n\n$$\n\\Gamma_{22}^{1}=-x^{1} \\quad \\Gamma_{33}^{1}=-x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\n$$\n\nThen:\n\nwhich give\n\n$$\n\\begin{aligned}\n& R_{212}^{1}=\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{313}^{1}=\\frac{\\partial \\Gamma_{33}^{1}}{\\partial x^{1}}+\\Gamma_{33}^{1} \\Gamma_{11}^{1}-\\Gamma_{31}^{3} \\Gamma_{33}^{1}=-1-\\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}\\left(-x^{1}\\right)=-\\frac{1}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{323}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}=-x^{1} \\cdot \\frac{x^{1}}{\\left(x^{1}\\right)^{2}+1}=-\\frac{\\left(x^{1}\\right)^{2}}{\\left(x^{1}\\right)^{2}+1} \\\\\n& R_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{aligned}\n$$\n\n(A) $\\quad R_{1212}=g_{11} R_{212}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{1313}=g_{11} R_{313}^{1}=-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1}, R_{2323}=g_{22} R_{323}^{2}=-\\left(x^{1}\\right)^{2}$\n\nThe corresponding terms for the denominator of $(8.10)$ are\n\n\n\\begin{align*}\nG_{1212}=g_{11} g_{22} & =\\left(x^{1}\\right)^{2}+1, \\quad G_{1313}=g_{11} g_{33}=\\left(x^{1}\\right)^{2}+1, \\quad G_{2323}=g_{22} g_{33}=\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2}  \\tag{A}\\\\\n\\mathrm{~K} & =\\frac{-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1212}-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-1} W_{1313}-\\left(x^{1}\\right)^{2} W_{2323}}{\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1212}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right]^{2} W_{2323}} \\\\\n& =-\\left[\\left(x^{1}\\right)^{2}+1\\right]^{-2} \\frac{W_{1212}+W_{1313}+\\left(x^{1}\\right)^{2}\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}{W_{1212}+W_{1313}+\\left[\\left(x^{1}\\right)^{2}+1\\right] W_{2323}}\n\\end{align*}\n\n\nIf $\\mathrm{K}$ is to be independent of the $W_{i j k l}$ (which vary with ine direction of the 2 -flat), then $\\left(x^{1}\\right)^{2}=1$, or $x^{1}= \\pm 1$. Therefore, the isotropic points compose two surfaces, on which the curvature has the value $\\mathrm{K}=-[1+1]^{-2} \\cdot 1=-1 / 4$.", "8.12": "\\section*{RIEMANNIAN CURVATURE}\n8.12 Show that every point of $\\mathbf{R}^{3}$ is isotropic for the metric\n\n$$\nd s^{2}=\\left(x^{1}\\right)^{-2}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{2}\\right)^{2}+\\left(x^{1}\\right)^{-2}\\left(d x^{3}\\right)^{2}\n$$\n\nProblem 6.4 gives as the nonvanishing Christoffel symbols:\n\n$$\n\\begin{gathered}\n\\Gamma_{11}^{1}=-\\frac{1}{x^{1}} \\quad \\Gamma_{22}^{1}=\\frac{1}{x^{1}} \\quad \\Gamma_{33}^{1}=\\frac{1}{x^{1}} \\\\\n\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=-\\frac{1}{x^{1}} \\quad \\Gamma_{13}^{3}=\\Gamma_{31}^{3}=-\\frac{1}{x^{1}}\n\\end{gathered}\n$$\n\nAs in earlier problems, we proceed to calculate a basic set of $R_{i j k l}$, via $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (no sum).\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{r} \\Gamma_{r 1}^{1}-\\Gamma_{21}^{r} \\Gamma_{r 2}^{1}=-\\frac{1}{\\left(x^{1}\\right)^{2}}-0+\\Gamma_{22}^{1} \\Gamma_{11}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& =-\\frac{1}{\\left(x^{1}\\right)^{2}}+\\frac{1}{x^{1}}\\left(-\\frac{1}{x^{1}}\\right)-\\left(-\\frac{1}{x^{1}}\\right) \\frac{1}{x^{1}}=-\\frac{1}{\\left(x^{1}\\right)^{2}}\n\\end{aligned}\n$$\n\nSimilarly, $R_{313}^{1}=-1 /\\left(x^{1}\\right)^{2}$. For the remainder, the partial-derivative terms all drop out, yielding\n\n$$\n\\begin{gathered}\nR_{323}^{2}=\\Gamma_{33}^{r} \\Gamma_{r 2}^{2}-\\Gamma_{32}^{r} \\Gamma_{r 3}^{2}=\\Gamma_{33}^{1} \\Gamma_{12}^{2}-0=-\\frac{1}{\\left(x^{1}\\right)^{2}} \\\\\nR_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0\n\\end{gathered}\n$$\n\nOur basic set is thus\n\n(A) $R_{1212}=R_{1313}=R_{2323}=-1 /\\left(x^{1}\\right)^{4}$\n\nand, by Example 8.3,\n\n(A) $G_{1212}=G_{1313}=G_{2323}=1 /\\left(x^{1}\\right)^{4}$\n\nis a basic set of $G_{i j k l}$. Formula (8.10) or (8.12) now gives\n\n$$\n\\mathrm{K}=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}=\\frac{\\left[-\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}{\\left[\\left(x^{1}\\right)^{-4}\\right]\\left(W_{1212}+W_{1313}+W_{2323}\\right)}=-1\n$$\n\nIt is seen that this Riemannian space is more than just isotropic; it is a space of constant curvature.", "8.13": "\\section*{THE RICCI TENSOR}\n8.13 For the metric of Example 8.4, calculate (a) $R_{i j},(b) R_{j}^{i}$, (c) $R$.\n\n(a) From $R_{i j}=R_{i j k}^{k}=R_{i j 1}^{1}+R_{i j 2}^{2}+R_{i j 3}^{3}$ and the fact that $g_{i j}=0$ for $i \\neq j$, it follows that\n\n\n\\begin{equation*}\nR_{i j}=g^{11} R_{1 i j 1}+g^{22} R_{2 i j 2}+g^{33} R_{3 i j 3} \\tag{1}\n\\end{equation*}\n\n\nwhere $g^{11}=1, g^{22}=1 / 2 x^{1}, g^{33}=1 / 2 x^{2}$. Now, a basic set of the $R_{i j k l}$ was computed as\n\n$$\nR_{1221}\\left(=-R_{1212}\\right)=-\\frac{1}{2 x^{1}} \\quad R_{2332}\\left(=-R_{2323}\\right)=-\\frac{1}{2 x^{2}} \\quad R_{3123}\\left(=-R_{3132}\\right)=-\\frac{1}{2 x^{1}}\n$$\n\nand the only other nonzero components of the form $R_{\\text {aija }}$ generated by these are\n\n$$\nR_{2112}=-\\frac{1}{2 x^{1}} \\quad R_{3223}=-\\frac{1}{2 x^{2}} \\quad R_{3213}=-\\frac{1}{2 x^{1}}\n$$\n\nHence, the nonzero $R_{i j}$ may be read off from (1) as\n\n\n\\begin{gather*}\nR_{11}=g^{22} R_{2112}=-\\frac{1}{4\\left(x^{1}\\right)^{2}} \\\\\nR_{22}=g^{11} R_{1221}+g^{33} R_{3223}=-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}} \\\\\nR_{33}=g^{22} R_{2332}=-\\frac{1}{4 x^{1} x^{2}} \\\\\nR_{12}=g^{33} R_{3123}=-\\frac{1}{4 x^{1} x^{2}}=g^{33} R_{3213}=R_{21} \\\\\nR_{j}^{i}=g^{i k} R_{k j}=g^{i i} R_{i j} \\quad \\text { (no summation on } i \\text { ) }  \\tag{b}\\\\\n=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33} \\\\\n=(1)\\left[-\\frac{1}{4\\left(x^{1}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{1}}\\right)\\left[-\\frac{1}{2 x^{1}}-\\frac{1}{4\\left(x^{2}\\right)^{2}}\\right]+\\left(\\frac{1}{2 x^{2}}\\right)\\left(-\\frac{1}{4 x^{1} x^{2}}\\right)=-\\frac{x^{1}+2\\left(x^{2}\\right)^{2}}{\\left(2 x^{1} x^{2}\\right)^{2}}\n\\end{gather*}\n\n\n$$\nR=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33}\n$$", "8.14": "\\section*{THE RICCI TENSOR}\n8.14 Derive (8.16) from (8.14).\n\nFormula (8.14) involves two summations of the form $\\Gamma_{i s}^{s}$. By (6.4) and (6.1b),\n\n$$\n\\begin{aligned}\n\\Gamma_{i s}^{s} & =g^{s r} \\Gamma_{i s r}=\\frac{1}{2} g^{s r}\\left(-g_{i s r}+g_{s r i}+g_{r i s}\\right)=-\\frac{1}{2} g^{s r} g_{s i r}+\\frac{1}{2} g^{s r} g_{s r i}+\\frac{1}{2} g^{r s} g_{s i r} \\\\\n& =\\frac{1}{2} g^{s r} g_{r s i} \\equiv \\frac{1}{2} g^{s r} \\frac{\\partial g_{r s}}{\\partial x^{i}}=\\frac{\\partial}{\\partial x^{i}}(\\ln \\sqrt{|g|})\n\\end{aligned}\n$$\n\nwhere Lemma 8.5 was used in the last step. Now substitute in (8.14):\n\n$$\n\\begin{aligned}\nR_{i j} & =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}-\\Gamma_{i j}^{r} \\frac{\\partial(\\ln \\sqrt{|g|})}{\\partial x^{r}} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\left(\\frac{1}{\\sqrt{|g|}} \\sqrt{|g|} \\frac{\\partial \\Gamma_{i j}^{s}}{\\partial x^{s}}+\\frac{1}{\\sqrt{|g|}} \\frac{\\partial(\\sqrt{|g|})}{\\partial x^{s}} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s} \\\\\n& =\\frac{\\partial^{2}(\\ln \\sqrt{|g|})}{\\partial x^{i} \\partial x^{j}}-\\frac{1}{\\sqrt{|g|}} \\frac{\\partial}{\\partial x^{s}}\\left(\\sqrt{|g|} \\Gamma_{i j}^{s}\\right)+\\Gamma_{i s}^{r} \\Gamma_{r j}^{s}\n\\end{aligned}\n$$", "9.1": "\\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}\n9.1 Test the compatibility conditions (Theorem 9.2) for the system\n\n\n\\begin{equation*}\n\\frac{\\partial u_{0}}{\\partial x^{1}}=\\frac{u_{0}}{x^{1}} \\quad \\frac{\\partial u_{0}}{\\partial x^{2}}=2 x^{2} u_{0} \\tag{1}\n\\end{equation*}\n\n\nIf it is compatible, solve the system.\n\nIn the notation of Theorem 9.2, there is only the condition corresponding to $\\lambda=0, j=1, k=2$ to be satisfied.\n\n$$\n\\begin{gathered}\n\\frac{\\partial F_{01}}{\\partial u_{0}} F_{02}+\\frac{\\partial F_{01}}{\\partial x^{2}} \\stackrel{?}{=} \\frac{\\partial F_{02}}{\\partial u_{0}} F_{01}+\\frac{\\partial F_{02}}{\\partial x^{1}} \\\\\n\\frac{\\partial}{\\partial u_{0}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\cdot 2 x^{2} u_{0}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{0}}{x^{1}}\\right) \\stackrel{?}{=} \\frac{\\partial}{\\partial u_{0}}\\left(2 x^{2} u_{0}\\right) \\cdot \\frac{u_{0}}{x^{1}}+\\frac{\\partial}{\\partial x^{1}}\\left(2 x^{2} u_{0}\\right) \\\\\n\\frac{2 x^{2} u_{0}}{x^{1}}=\\frac{2 x^{2} u_{0}}{x^{1}}\n\\end{gathered}\n$$\n\nTherefore, the system is compatible. The first equation (1) integrates to $u_{0}=x^{1} \\phi\\left(x^{2}\\right)$; the second equation then gives\n\n$$\nx^{1} \\phi^{\\prime}=2 x^{2} x^{1} \\phi \\quad \\text { whence } \\quad \\phi=c \\exp \\left(x^{2}\\right)^{2}\n$$\n\nHence the solution of (1) is $u_{0}=c x^{1} \\exp \\left(x^{2}\\right)^{2}$.", "9.2": "\\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}\n9.2 Show that $\\mathbf{R}^{3}$ under the metric $d s^{2}=\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{1}\\right)^{2}+\\left[\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right]\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}$ is Euclidean.\n\nThis metric has $g_{33}=$ const., and $g_{11}$ and $g_{22}$ independent of $x^{3}$. Problem 6.4 then shows that $\\Gamma_{j k}^{i}=0$ whenever $i, j$, or $k$ equals 3 ; consequently, of the six independent components of the Riemann tensor, only $R_{1212}$ is possibly nonzero. But (from Problem 6.4), with $z \\equiv\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}$,\n\n$$\n\\begin{array}{lll}\n\\Gamma_{11}^{1}=\\frac{x^{1}}{z} & \\Gamma_{12}^{1}=\\Gamma_{21}^{1}=\\frac{x^{2}}{z} & \\Gamma_{22}^{1}=-\\frac{x^{1}}{z} \\\\\n\\Gamma_{11}^{2}=-\\frac{x^{2}}{z} & \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{x^{1}}{z} & \\Gamma_{22}^{2}=\\frac{x^{2}}{z}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\nR_{212}^{1} & =\\frac{\\partial \\Gamma_{22}^{1}}{\\partial x^{1}}-\\frac{\\partial \\Gamma_{21}^{1}}{\\partial x^{2}}+\\Gamma_{22}^{1} \\Gamma_{11}^{1}+\\Gamma_{22}^{2} \\Gamma_{21}^{1}-\\Gamma_{21}^{1} \\Gamma_{12}^{1}-\\Gamma_{21}^{2} \\Gamma_{22}^{1} \\\\\n& \\left.=\\frac{-z+x^{1}\\left(2 x^{1}\\right)}{z^{2}}-\\frac{z-x^{2}\\left(2 x^{2}\\right)}{z^{2}}+\\left(-\\frac{x^{1}}{z}\\right)\\left(\\frac{x^{1}}{z}\\right)+\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right)-\\frac{x^{2}}{z}\\left(\\frac{x^{2}}{z}\\right) \\right\\rvert\\,-\\frac{x^{1}}{z}\\left(-\\frac{x^{1}}{z}\\right)=0\n\\end{aligned}\n$$\n\nConsequently, $R_{1212}=0=\\mathrm{K}$. As the metric is clearly positive definite, Theorem 9.1 implies that the space is Euclidean.", "9.3": "\\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}\n9.3 For the Euclidean space of Problem 9.2, exhibit a transformation from the given coordinate system $\\left(x^{i}\\right)$ to a rectangular system $\\left(\\bar{x}^{i}\\right)$.\n\nUsing the Christoffel symbols as calculated in Problem 9.2, we obtain from (9.3) the following system for the $u_{i}$ :\n\n\\[\n\\begin{array}{lll}\n\\frac{\\partial u_{1}}{\\partial x^{1}}=\\frac{x^{1} u_{1}-x^{2} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{2}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{1}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{2}}{\\partial x^{1}}=\\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{2}}=\\frac{-x^{1} u_{1}+x^{2} u_{2}}{z} & \\frac{\\partial u_{2}}{\\partial x^{3}}=0 \\\\\n\\frac{\\partial u_{3}}{\\partial x^{1}}=0 & \\frac{\\partial u_{3}}{\\partial x^{2}}=0 & \\frac{\\partial u_{3}}{\\partial x^{3}}=0 \\tag{3}\n\\end{array}\n\\]\n\nThus $u_{1}$ and $u_{2}$ are functions of $x^{1}, x^{2}$ alone, and $u_{3}=$ const. Since the $g_{i j}$ are all polynomials of degree 2 in $x^{1}, x^{2}$, use the method of undetermined coefficients, assuming polynomial forms\n\n$$\nu_{i}=a_{i}\\left(x^{1}\\right)^{2}+b_{i} x^{1} x^{2}+c_{i}\\left(x^{2}\\right)^{2}+d_{i} x^{1}+e_{i} x^{2}+f_{i} \\quad(i=1,2)\n$$\n\nThe (compatibility) relation $\\partial u_{1} / \\partial x^{2}=\\partial u_{2} / \\partial x^{1}$ implied by the second equation (1) and the first equation (2) requires\n\n$$\nb_{1}=2 a_{2} \\quad 2 c_{1}=b_{2} \\quad e_{1}=d_{2}\n$$\n\nSimilarly, $\\partial u_{1} / \\partial x^{1}=-\\partial u_{2} / \\partial x^{2}$ implies\n\n$$\n2 a_{1}=-b_{2} \\quad b_{1}=-2 c_{2} \\quad d_{1}=-e_{2}\n$$\n\nUsing the first equation (1), or $z\\left(\\partial u_{1} / \\partial x^{1}\\right)=x^{1} u_{1}-x^{2} u_{2}$, we get:\n\n$$\na_{1}=0 \\quad a_{2}=0 \\quad c_{1}=b_{2} \\quad b_{1}=-c_{2} \\quad d_{1}=-e_{2} \\quad f_{1}=0=-f_{2}\n$$\n\nIt follows that $b_{1}=b_{2}=c_{1}=c_{2}=0$, and therefore (renotating $d_{1}$ and $e_{1}$ )\n\n$$\nu_{1}=a x^{1}+b x^{2} \\quad u_{2}=b x^{1}-a x^{2} \\quad u_{3}=c\n$$\n\n[Note: This solution of (1)-(2)-(3) may be obtained by the method of characteristics, without any prior assumptions.]\n\nThe first equations (9.3),\n\n$$\n\\frac{\\partial w}{\\partial x^{1}}=a x^{1}+b x^{2} \\quad \\frac{\\partial w}{\\partial x^{2}}=b x^{1}-a x^{2} \\quad \\frac{\\partial w}{\\partial x^{3}}=c\n$$\n\nmay now be integrated to give\n\n$$\nw=\\frac{a}{2}\\left(x^{1}\\right)^{2}+b x^{1} x^{2}-\\frac{a}{2}\\left(x^{2}\\right)^{2}+c x^{3}+d\n$$\n\nor, replacing $\\bar{x}^{k}$ and corresponding superscripts, and with $d=0$,\n\n$$\n\\bar{x}^{k}=\\frac{a^{k}}{2}\\left(x^{1}\\right)^{2}+b^{k} x^{1} x^{2}-\\frac{a^{k}}{2}\\left(x^{2}\\right)^{2}+c^{k}\\left(x^{2}\\right)^{2}\n$$\n\nIt is clear that we may take $c^{1}=c^{2}=0=a^{3}=b^{3}$ and $c^{3}=1$ :\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=\\frac{1}{2} a^{1}\\left(x^{1}\\right)^{2}+b^{1} x^{1} x^{2}-\\frac{1}{2} a^{1}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{2}=\\frac{1}{2} a^{2}\\left(x^{1}\\right)^{2}+b^{2} x^{1} x^{2}-\\frac{1}{2} a^{2}\\left(x^{2}\\right)^{2} \\\\\n& \\bar{x}^{3}=x^{3}\n\\end{aligned}\n$$\n\nThe Jacobian matrix is\n\n$$\nJ=\\left[\\begin{array}{ccc}\na^{1} x^{1}+b^{1} x^{2} & b^{1} x^{1}-a^{1} x^{2} & 0 \\\\\na^{2} x^{1}+b^{2} x^{2} & b^{2} x^{1}-a^{2} x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nSince $J^{T} J=G$, we must have\n\n$$\n\\left(a^{1}\\right)^{2}+\\left(a^{2}\\right)^{2}=1 \\quad a^{1} b^{1}+a^{2} b^{2}=0 \\quad\\left(b^{1}\\right)^{2}+\\left(b^{2}\\right)^{2}=1\n$$\n\nso take $a^{1}=0, a^{2}=1, b^{2}=0, b^{1}=1$. The transformation is, finally,\n\n$$\n\\bar{x}^{1}=x^{1} x^{2} \\quad \\bar{x}^{2}=\\frac{1}{2}\\left[\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\right] \\quad \\bar{x}^{3}=x^{3}\n$$", "9.4": "\\section*{FLAT RIEMANNIAN SPACES}\n9.4 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n\nSince the metric is not positive definite, it cannot be Euclidean. To determine flatness, it suffices to examine $R_{1212}=g_{11} R_{212}^{1}$. But Problem 6.4 shows that $R_{212}^{1}=0$; hence the space is flat.", "9.5": "\\section*{FLAT RIEMANNIAN SPACES}\n9.5 Show that if the metric tensor is constant, the space is flat and the coordinate transformation $\\bar{x}=A x$, where $A$ is a rank- $n$ matrix of eigenvectors of $G=\\left(g_{i j}\\right)$, diagonalizes the metric (i.e., $\\bar{g}_{i j}=0$ if $i \\neq j$ ).\n\nSince all partial derivatives of $g_{i j}$ are zero, all Christoffel symbols will vanish and all $R_{i j k l}=0$, making $\\mathrm{K}=0$. Thus, by Theorem 9.3, the space is flat. By Chapters 2 and 3, if $\\bar{x}=A x$, then $J=A$ and\n\n$$\nG=J^{T} \\bar{G} J=A^{T} \\bar{G} A\n$$\n\nHowever, since $G$ is real and symmetric, its eigenvectors form an orthogonal matrix which we now choose as $A$, with\n\n$$\nA G A^{-1}=A G A^{T}=D \\quad \\text { (diagonal matrix of eigenvalues of } G \\text { ) }\n$$\n\nHence, $\\bar{G}=A G A^{T}=D \\quad$ QED.", "9.6": "\\section*{FLAT RIEMANNIAN SPACES}\n9.6 Find the signature of the flat metric\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}-2\\left(d x^{3}\\right)^{2}+2\\left(d x^{4}\\right)^{2}-4 d x^{2} d x^{3}-4 d x^{2} d x^{4}-10 d x^{3} d x^{4}\n$$\n\nis\n\nIn view of Problem 9.5, it suffices to find the eigenvalues $\\lambda$ of $G=\\left(g_{i j}\\right)$. The characteristic equation\n\n$$\n\\begin{aligned}\n|G-\\lambda I| & =\\left|\\begin{array}{cccc}\n4-\\lambda & 0 & 0 & 0 \\\\\n0 & 5-\\lambda & -2 & -2 \\\\\n0 & -2 & -2-\\lambda & -5 \\\\\n0 & -2 & -5 & 2-\\lambda\n\\end{array}\\right| \\\\\n& =(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & -2 & -2 \\\\\n-2 & -2-\\lambda & -5 \\\\\n-2 & -5 & 2-\\lambda\n\\end{array}\\right|=-(4-\\lambda)\\left|\\begin{array}{ccc}\n5-\\lambda & 2 & 0 \\\\\n-2 & 2+\\lambda & -3+\\lambda \\\\\n-2 & 5 & 7-\\lambda\n\\end{array}\\right| \\\\\n& =-(4-\\lambda)(5-\\lambda)\\left(37-\\lambda^{2}\\right)=0\n\\end{aligned}\n$$\n\nfrom which the eigenvalues are $\\lambda=+4,+5,+\\sqrt{37},-\\sqrt{37}$. This means that there is a transformation which changes the metric into the form\n\n$$\n\\varepsilon d s^{2}=4\\left(d x^{1}\\right)^{2}+5\\left(d x^{2}\\right)^{2}+\\sqrt{37}\\left(d x^{3}\\right)^{2}-\\sqrt{37}\\left(d x^{4}\\right)^{2}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left(d \\bar{x}^{2}\\right)^{2}+\\left(d \\bar{x}^{3}\\right)^{2}-\\left(d \\bar{x}^{4}\\right)^{2}\n$$\n\nwith the obvious change of coordinates. Hence, the signature is $(+++-)$, or some permutation thereof (Theorem 9.5).", "9.7": "```latex\n\\section*{FLAT RIEMANNIAN SPACES}\n9.7 Show that the conditions $R_{i j k l}=0$ are sufficient for the compatibility of (9.3).\n\nIn the notation of Theorem 9.2, (9.3) takes the form (with $m=n$ )\n\n$$\n\\begin{array}{ll}\n\\boldsymbol{\\lambda}=\\mathbf{0} & \\frac{\\partial u_{0}}{\\partial x^{j}}=F_{0 j} \\equiv u_{j} \\\\\n\\lambda>0 & \\frac{\\partial u_{\\lambda}}{\\partial x^{j}}=F_{\\lambda j} \\equiv u_{r} \\Gamma_{\\lambda j}^{r}(\\mathbf{x})\n\\end{array}\n$$\n\nThe corresponding compatibility conditions are\n\n$$\n\\boldsymbol{\\lambda}=\\mathbf{0} \\quad \\delta_{j}^{\\nu} u_{r} \\Gamma_{\\nu k}^{r}=\\delta_{k}^{\\nu} u_{r} \\Gamma_{\\nu j}^{r}\n$$\n\nor $u_{r} \\Gamma_{j k}^{r}=u_{r} \\Gamma_{k j}^{r}$, which holds trivially, and\n\n$$\n\\boldsymbol{\\lambda}>\\mathbf{0} \\quad \\delta_{r}^{\\nu} \\Gamma_{\\lambda j}^{r} u_{s} \\Gamma_{\\nu k}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}=\\delta_{r}^{\\nu} \\Gamma_{\\lambda k}^{r} u_{s} \\Gamma_{\\nu j}^{s}+u_{r} \\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}\n$$\n\nwhich rearranges to\n\n$$\n(\\underbrace{\\frac{\\partial \\Gamma_{\\lambda j}^{r}}{\\partial x^{k}}-\\frac{\\partial \\Gamma_{\\lambda k}^{r}}{\\partial x^{j}}+\\Gamma_{\\lambda j}^{s} \\Gamma_{s k}^{r}-\\Gamma_{\\lambda k}^{s} \\Gamma_{s j}^{r}}_{R_{\\lambda k j}^{r}}) u_{r}=0\n$$\n\nThus, $R_{r \\lambda k j}=0$ forces $R_{\\lambda k j}^{r}=0$ and compatibility.\n```", "9.8": "\\subsection*{9.8 Prove Lemma 9.7.}\nAs $\\left(R_{i j k l}\\right)$ and $\\left(G_{i j k l}\\right)$ are tensors [see Example 8.3] and $\\mathrm{K}$ is an invariant,\n\n$$\n\\left(T_{i j k l}\\right) \\equiv\\left(R_{i j k l}-\\mathrm{K} G_{i j k l}\\right)\n$$\n\nis a tensor of the same type and order. It must be proved that all $T_{i j k l}=0$ at an isotropic point $P$. Since $\\mathrm{K}$ is independent of direction at $P$, so are the $T_{i j k l}$; and (8.7) gives\n\n\n\\begin{equation*}\nT_{i j k l} U^{i} V^{j} U^{k} V^{l}=0 \\quad\\left(T_{i j k l}=T_{i j k l}(P)\\right) \\tag{1}\n\\end{equation*}\n\n\nIf we define the second-order tensor $\\left(S_{i k}\\right) \\equiv\\left(T_{i j k l} V^{j} V^{l}\\right)$, we find that $S_{i k}=S_{k i}$, and by (1), $S_{i k} U^{i} U^{k}=0$ at $P$ for any $\\left(U^{i}\\right)$. It follows that all $S_{i k}=0$ at $P$. Now set $V^{i}=\\delta_{a}^{i}$. Then, at $P$,\n\n$$\n0=S_{i k}=T_{i j k l} \\delta_{a}^{j} \\delta_{a}^{l}=T_{i a k a}\n$$\n\nfor arbitrary (fixed) index $a$. Next set $V^{i}=\\delta_{a}^{i}+\\delta_{b}^{i}$ for arbitrary fixed indices $a$ and $b$ :\n\n$$\n0=T_{i j k l} V^{j} V^{l}=T_{i j k l}\\left(\\delta_{a}^{j}+\\delta_{b}^{j}\\right)\\left(\\delta_{a}^{l}+\\delta_{b}^{l}\\right)=T_{i a k a}+T_{i a k b}+T_{i b k a}+T_{i b k b}\n$$\n\nor $T_{i a k b}+T_{i b k a}=0$. Therefore, since $T_{i j k l}$ obeys the same symmetry laws as $R_{i j k l}$ and $G_{i j k l}$,\n\n\n\\begin{align*}\nT_{i j k l}-T_{i l j k} & =0  \\tag{2}\\\\\nT_{i j k l}+T_{i k l j}+T_{i l j k} & =0 \\tag{3}\n\\end{align*}\n\n\nAdding (2) and (3),\n\n\n\\begin{equation*}\n2 T_{i j k l}+T_{i k l j}=0 \\tag{4}\n\\end{equation*}\n\n\nBut, from (2), $T_{i k l j}=T_{i j k l}$, so that (4) implies $T_{i j k l}=0$, as desired.", "9.9": "\\subsection*{9.9 Prove Theorems 9.1 and 9.3.}\nWe already know that if the space is either Euclidean or flat, $K \\equiv 0$. Suppose, conversely, that $K \\equiv 0$; then every point is isotropic, and Lemma 9.7 implies that all $R_{i j k l}$ vanish. It then follows from Problem 9.7 that there exists a coordinate system $\\left(\\bar{x}^{i}\\right)$ for which $\\bar{\\Gamma}_{j k}^{i}=0$ or $\\bar{g}_{i j}=$ const. By Problem 9.5 , there exists another coordinate system, $\\left(y^{i}\\right)$, in which the metric takes the form (for real constants $a_{i}$ )\n\n$$\n\\varepsilon d s^{2}=\\varepsilon_{1} a_{1}^{2}\\left(d y^{1}\\right)^{2}+\\varepsilon_{2} a_{2}^{2}\\left(d y^{2}\\right)^{3}+\\cdots+\\varepsilon_{n} a_{n}^{2}\\left(d y^{n}\\right)^{2}\n$$\n\nThe transformation $\\bar{y}^{1}=a_{1} y^{1}, \\bar{y}^{2}=a_{2} y^{2}, \\ldots, \\bar{y}^{n}=a_{n} y^{n}$ now reduces the metric to\n\n\n\\begin{equation*}\n\\varepsilon d s^{2}=\\varepsilon_{1}\\left(d \\bar{y}^{1}\\right)^{2}+\\varepsilon_{2}\\left(d \\bar{y}^{2}\\right)^{2}+\\cdots+\\varepsilon_{n}\\left(d \\bar{y}^{n}\\right)^{2} \\tag{1}\n\\end{equation*}\n\n\nand the space is flat. This proves Theorem 9.3. If the given metric is positive definite, then in $(1), \\varepsilon_{i}=1$ for each $i$. In this case the metric is Euclidean, proving Theorem 9.1.", "9.10": "\\subsection*{9.10 Prove Theorem 9.6.}\nIf $\\left(y^{i}\\right)$ are normal coordinates, then the geodesic through $O$ and any point $P$ in some neighborhood $\\mathcal{N}$ of $O$ has the parametric form\n\n$$\ny^{i}=s p^{i} \\quad\\left(p^{i}=\\text { const. }\\right)\n$$\n\nThis geodesic thus obeys the differential equations\n\n$$\n\\frac{d y^{i}}{d s}=p^{i} \\quad \\text { and } \\quad \\frac{d^{2} y^{i}}{d s^{2}}=0\n$$\n\nBut it must also satisfy (9.5), $\\delta \\mathbf{T} / \\delta s=0$, in the coordinates $\\left(y^{i}\\right)$ :\n\n$$\n\\frac{d^{2} y^{i}}{d s^{2}}+\\Gamma_{j k}^{i} \\frac{d y^{j}}{d s} \\frac{d y^{k}}{d s}=0\n$$\n\nThus, by substit\u0131tion, $\\Gamma_{j k}^{i} p^{j} p^{k}=0$ for all directions $\\left(p^{i}\\right)$ at $O$. But $\\Gamma_{j k}^{i}$ is symmetric for each $i$; hence, $\\Gamma_{j k}^{i}=0$ at $O$ for all $i, j, k$. Also, $\\Gamma_{i j k}=g_{k r} \\Gamma_{i j}^{r}=0$; hence, $\\partial g_{i j} / \\partial y^{k}=0$ at $O$, by (6.2). Finally, since $g^{i j} g_{j r}=\\delta_{r}^{i}$, the product rule for differentiation yields $\\partial g^{i j} / \\partial y^{k}=0$ at $O$.", "9.11": "\\subsection*{9.11 Prove that at the origin of a Riemannian coordinate system $\\left(y^{i}\\right)$,}\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}} \\quad(\\text { all } j \\text { and } k ; \\text { summed on } i)\n$$\n\nSince $\\Gamma_{j k}^{i}$ and $\\partial g^{i j} / \\partial y^{k}$ all vanish at the origin $O$ of the Riemannian coordinate system,\n\n\\begin{equation*}\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{\\partial}{\\partial y^{k}}\\left(g^{i r} \\Gamma_{j i r}\\right)=g^{i r} \\frac{\\partial}{\\partial y^{k}}\\left[\\frac{1}{2}\\left(-g_{j i r}+g_{i r j}+g_{r j i}\\right)\\right]=\\frac{1}{2} g^{i r}\\left(-g_{j i r k}+g_{i r j k}+g_{r j i k}\\right) \\tag{1}\n\\end{equation*}\n\nat $O$, with $g_{i j k l} \\equiv \\partial^{2} g_{i j} / \\partial y^{k} \\partial y^{l}$. But, since $g^{i r}=g^{r i}$,\n\n$$\ng^{i r} g_{j i r k}=g^{r i} g_{j i r k}=g^{i r} g_{j r i k}=g^{i r} g_{r j i k}\n$$\n\nand (1) becomes\n\n$$\n\\frac{\\partial \\Gamma_{j i}^{i}}{\\partial y^{k}}=\\frac{1}{2} g^{i r} g_{i r j k}=\\frac{1}{2} g^{i r} g_{i r k j}=\\frac{\\partial \\Gamma_{k i}^{i}}{\\partial y^{j}}", "9.12": "\\subsection*{9.12 Prove the identity $R_{i j k l, u}+R_{i l j k, u}=R_{i k u l, j}+R_{i k j u, l}$.}\n\nCovariant differentiation of Bianchi's first identity, (8.6), gives $R_{i j k l, u}+R_{i k l j, u}+R_{i l j k, u}=0$. Then the second identity, $(9.8)$, yields\n\n$$\nR_{i j k l, u}+R_{i l j k, u}=-R_{i k l j, u}=R_{i k j u, l}+R_{i k u l, j}\n$$", "9.13": "```latex\n\\section*{SCHUR'S THEOREM}\n9.14 Prove Schur's theorem (Theorem 9.8).\n\nBy Lemma 9.7, $R_{i j k l}=G_{i j k l} \\mathrm{~K}$ throughout $\\mathcal{N}$. Take the covariant derivative of both sides with respect to $x^{u}$, then permute indices ( $G_{i j k l, u}=0$ because $g_{i j, u}=0$ in general):\n\n$$\nR_{i j k l, u}=G_{i j k l} \\mathrm{~K}_{, u} \\quad R_{i j l u, k}=G_{i j l u} \\mathrm{~K}_{, k} \\quad R_{i j u k, l}=G_{i j u k} \\mathrm{~K}_{, l}\n$$\n\nAdd the three equations and apply $(9.8)$ :\n\n\n\\begin{equation*}\nG_{i j k l} \\mathrm{~K}_{, u}+G_{i j l u} \\mathrm{~K}_{, k}+G_{i j u k} \\mathrm{~K}_{, l}=0 \\tag{1}\n\\end{equation*}\n\n\nMultiply both sides of (1) by $g^{i k} g^{j l}$ and sum. Since\n\n$$\n\\begin{aligned}\n& g^{i k} g^{j l} G_{i j k l}=g^{i k} g^{j l}\\left(g_{i k} g_{j l}-g_{i l} g_{j k}\\right)=\\delta_{k}^{k} \\delta_{l}^{l}-\\delta_{l}^{k} \\delta_{k}^{l}=n^{2}-n \\\\\n& g^{i k} g^{j l} G_{i j l u}=g^{i k} g^{j l}\\left(g_{i l} g_{j u}-g_{i u} g_{j l}\\right)=\\delta_{l}^{k} \\delta_{u}^{l}-\\delta_{u}^{k} \\delta_{l}^{l}=\\delta_{u}^{k}-n \\delta_{u}^{k} \\\\\n& g^{i k} g^{j l} G_{i j u k}=g^{i k} g^{j l}\\left(g_{i u} g_{j k}-g_{i k} g_{j u}\\right)=\\delta_{u}^{k} \\delta_{k}^{l}-\\delta_{k}^{k} \\delta_{u}^{l}=\\delta_{u}^{l}-n \\delta_{u}^{l}\n\\end{aligned}\n$$\n\nthat summation yields the relation\n\n$$\n\\begin{aligned}\n0 & =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+\\left(\\delta_{u}^{k}-n \\delta_{u}^{k}\\right) \\mathrm{K}_{, k}+\\left(\\delta_{u}^{l}-n \\delta_{u}^{l}\\right) \\mathrm{K}_{, t} \\\\\n& =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}=(n-2)(n-1) \\mathrm{K}_{, u}\n\\end{aligned}\n$$\n\nFor $n \\geqq 3, \\mathrm{~K}_{, u}=\\partial \\mathrm{K} / \\partial x^{u}=0$. Since $u$ was arbitrary, $\\mathrm{K}$ must be constant over $\\mathcal{N}$. QED\n```", "9.14": "\\section*{SCHUR'S THEOREM}\n9.14 Prove Schur's theorem (Theorem 9.8).\n\nBy Lemma 9.7, $R_{i j k l}=G_{i j k l} \\mathrm{~K}$ throughout $\\mathcal{N}$. Take the covariant derivative of both sides with respect to $x^{u}$, then permute indices ( $G_{i j k l, u}=0$ because $g_{i j, u}=0$ in general):\n\n$$\nR_{i j k l, u}=G_{i j k l} \\mathrm{~K}_{, u} \\quad R_{i j l u, k}=G_{i j l u} \\mathrm{~K}_{, k} \\quad R_{i j u k, l}=G_{i j u k} \\mathrm{~K}_{, l}\n$$\n\nAdd the three equations and apply $(9.8)$ :\n\n\n\\begin{equation*}\nG_{i j k l} \\mathrm{~K}_{, u}+G_{i j l u} \\mathrm{~K}_{, k}+G_{i j u k} \\mathrm{~K}_{, l}=0 \\tag{1}\n\\end{equation*}\n\n\nMultiply both sides of (1) by $g^{i k} g^{j l}$ and sum. Since\n\n$$\n\\begin{aligned}\n& g^{i k} g^{j l} G_{i j k l}=g^{i k} g^{j l}\\left(g_{i k} g_{j l}-g_{i l} g_{j k}\\right)=\\delta_{k}^{k} \\delta_{l}^{l}=n^{2}-n \\\\\n& g^{i k} g^{j l} G_{i j l u}=g^{i k} g^{j l}\\left(g_{i l} g_{j u}-g_{i u} g_{j l}\\right)=\\delta_{l}^{k} \\delta_{u}^{l}-\\delta_{u}^{k} \\delta_{l}^{l}=\\delta_{u}^{k}-n \\delta_{u}^{k} \\\\\n& g^{i k} g^{j l} G_{i j u k}=g^{i k} g^{j l}\\left(g_{i u} g_{j k}-g_{i k} g_{j u}\\right)=\\delta_{u}^{k} \\delta_{k}^{l}-\\delta_{k}^{k} \\delta_{u}^{l}=\\delta_{u}^{l}-n \\delta_{u}^{l}\n\\end{aligned}\n$$\n\nthat summation yields the relation\n\n$$\n\\begin{aligned}\n0 & =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+\\left(\\delta_{u}^{k}-n \\delta_{u}^{k}\\right) \\mathrm{K}_{, k}+\\left(\\delta_{u}^{l}-n \\delta_{u}^{l}\\right) \\mathrm{K}_{, t} \\\\\n& =\\left(n^{2}-n\\right) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}+(1-n) \\mathrm{K}_{, u}=(n-2)(n-1) \\mathrm{K}_{, u}\n\\end{aligned}\n$$\n\nFor $n \\geqq 3, \\mathrm{~K}_{, u}=\\partial \\mathrm{K} / \\partial x^{u}=0$. Since $u$ was arbitrary, $\\mathrm{K}$ must be constant over $\\mathcal{N}$. QED", "9.15": "\\subsection*{9.15 Prove Theorem 9.9.}\nWe must prove that $G_{i, r}^{r}=0$. Multiply both sides of $(9.8)$ by $g^{i l} g^{j k}$ and sum:\n\n$$\n\\begin{aligned}\n0 & =g^{i l} g^{j k} R_{i j k l, u}-g^{i l} g^{j k} R_{i j u l, k}-g^{i l} g^{j k} R_{j i u k, l} \\\\\n& =g^{j k} R_{j k l, u}^{l}-g^{j k} R_{j u l, k}^{l}-g^{i l} R_{i u k, l}^{k}=g^{j k} R_{j k, u}-g^{j k} R_{j u, k}-g^{i l} R_{i u, l} \\\\\n& =R_{k, u}^{k}-R_{u, k}^{k}-R_{u, l}^{l}=2\\left(\\frac{1}{2} R_{, u}-R_{u, k}^{k}\\right)\n\\end{aligned}\n$$\n\nor, changing $u$ to $i$ and $k$ to $r, \\frac{1}{2} \\delta_{i}^{r} R_{, r}-R_{i, r}^{r}=0$. But, by Problem 6.32, $\\delta_{i, j}^{r}=0$ for all $i, j, r$; hence,\n\n$$\n\\left(R_{i}^{r}-\\frac{1}{2} \\delta_{i}^{r} R\\right)_{, r}=0 \\quad \\text { or } \\quad G_{i, r}^{r}=0\n$$", "9.16": "```latex\n\\section*{THE EINSTEIN TENSOR}\n\\subsection*{9.15 Prove Theorem 9.9.}\nWe must prove that $G_{i, r}^{r}=0$. Multiply both sides of $(9.8)$ by $g^{i l} g^{j k}$ and sum:\n\n$$\n\\begin{aligned}\n0 & =g^{i l} g^{j k} R_{i j k l, u}-g^{i l} g^{j k} R_{i j u l, k}-g^{i l} g^{j k} R_{j i u k, l} \\\\\n& =g^{j k} R_{j k l, u}^{l}-g^{j k} R_{j u l, k}^{l}-g^{i l} R_{i u k, l}^{k}=g^{j k} R_{j k, u}-g^{j k} R_{j u, k}-g^{i l} R_{i u, l} \\\\\n& =R_{k, u}^{k}-R_{u, k}^{k}-R_{u, l}^{l}=2\\left(\\frac{1}{2} R_{, u}-R_{u, k}^{k}\\right)\n\\end{aligned}\n$$\n\nor, changing $u$ to $i$ and $k$ to $r, \\frac{1}{2} \\delta_{i}^{r} R_{, r}-R_{i, r}^{r}=0$. But, by Problem 6.32, $\\delta_{i, j}^{r}=0$ for all $i, j, r$; hence,\n\n$$\n\\left(R_{i}^{r}-\\frac{1}{2} \\delta_{i}^{r} R\\right)_{, r}=0 \\quad \\text { or } \\quad G_{i, r}^{r}=0\n$$\n\n9.16 Show that $G_{i j}$, the associated Einstein tensor obtained by lowering the index $i$ in $G_{j}^{i}$, is symmetric.\n\nBy definition,\n\n$$\nG_{i j}=g_{i k} G_{j}^{k}=g_{i k}\\left(R_{j}^{k}-\\frac{1}{2} \\delta_{j}^{k} R\\right)=R_{i j}-\\frac{1}{2} g_{i j} R\n$$\n\nwhich is obviously symmetric (by symmetry of the Ricci tensor).\n```", "10.1": "\\section*{CURVE THEORY; THE MOVING FRAME}\n10.1 The curve\n\n$$\n\\mathscr{C}:\\left\\{\\begin{array} { l } \n{ x = t } \\\\\n{ y = t ^ { 4 } } \\\\\n{ z = 0 }\n\\end{array} \\quad ( t < 0 ) \\quad \\left\\{\\begin{array}{l}\nx=t \\\\\ny=0 \\\\\nz=t^{4}\n\\end{array} \\quad(t \\geqq 0)\\right.\\right.\n$$\n\nlies partly in the $x y$-plane and partly in the $x z$-plane (Fig. 10-10). Show that it is regular of class $C^{3}$, but that it possesses no principal normal vector.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-148}\n\\end{center}\n\nFig. 10-10\n\nThe component functions for $\\mathbf{r}(t)$ are\n\n$$\nx(t)=t \\quad y(t)=\\left\\{\\begin{array}{ll}\nt^{4} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad z(t)= \\begin{cases}0 & t<0 \\\\\nt^{4} & t \\geqq 0\\end{cases}\\right.\n$$\n\nWhen $t<0, \\dot{y}(t)=4 t^{3}$. As $t \\rightarrow 0$,\n\n$$\n\\lim _{t \\rightarrow-0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow-0} \\frac{t^{4}}{t}=0 \\quad \\lim _{t \\rightarrow+0} \\frac{y(t)-y(0)}{t-0}=\\lim _{t \\rightarrow+0} \\frac{0}{t}=0\n$$\n\nhence, $y(t)$ is differentiable at $t=0$. Clearly, $\\dot{y}(t)=0$ for $t>0$. A similar analysis applies to $z(t)$. Hence:\n\n$$\n\\dot{y}(t)=\\left\\{\\begin{array}{ll}\n4 t^{3} & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dot{z}(t)= \\begin{cases}0 & t<0 \\\\\n4 t^{3} & t \\geqq 0\\end{cases}\\right.\n$$\n\nwhich are continuous functions. Continuing the analysis up to the third derivatives:\n\n$$\n\\dddot{y}(t)=\\left\\{\\begin{array}{ll}\n24 t & t<0 \\\\\n0 & t \\geqq 0\n\\end{array} \\quad \\dddot{z}(t)= \\begin{cases}0 & t<0 \\\\\n24 t & t \\geqq 0\\end{cases}\\right.\n$$\n\nHence, $x(t)$ being differentiable to all orders, $\\mathbf{r}(t)$ is of class $C^{3}$. Furthermore, because $\\dot{x}(t) \\equiv 1, \\dot{\\mathbf{r}}(t) \\neq 0$ for all $t$ and $\\mathscr{C}$ is regular. However, the principal normal, which exists for the separate parts of $\\mathscr{C}$ (lying in the $x y$-plane for $t<0$ and in the $x z$-plane for $t>0$ ), cannot possibly be continuous at $t=0$, let alone differentiable. Hence, $\\mathscr{C}$ does not possess a principal normal.", "10.2": "10.2 (a) Describe the curve $\\mathbf{r}=\\left(\\cos t, \\sin t, \\tan ^{-1} t\\right)$, where $0 \\leqq t$ and where the principal value of the arctangent is understood. (b) Find the arc length between the points $\\mathbf{r}(0)$ and $\\mathbf{r}(1)$.\\\\\n(a) This is a form of the circular helix, except that the pitch decreases with increasing $t$. The curve lies on the right circular cylinder $x^{2}+y^{2}=1$; beginning at $(1,0,0)$, it winds around the cylinder and approaches the circle $x^{2}+y^{2}=1, z=\\pi / 2$ asymptotically as $t \\rightarrow \\infty$.\n\n(b)\n\n$$\n\\dot{\\mathbf{r}}=\\left(-\\sin t, \\cos t, \\frac{1}{t^{2}+1}\\right) \\quad \\text { or } \\quad \\frac{d s}{d t}=\\sqrt{\\sin ^{2} t+\\cos ^{2} t+\\frac{1}{\\left(t^{2}+1\\right)^{2}}}\n$$\n\nA numerical method of integration is required. Using Simpson's rule on a programmable calculator, one obtains\n\n$$\nL=\\int_{0}^{1} \\frac{\\sqrt{t^{4}+2 t^{2}+2}}{t^{2}+1} d t \\approx 1.27797806\n$$", "10.3": "\\subsection*{10.3 Find the moving frame for the curve}\n\n$$\n\\mathscr{C}: \\mathbf{r}=\\left(\\frac{3-3 t^{3}}{5}, \\frac{4+4 t^{3}}{5}, 3 t\\right) \\quad(t \\text { real })\n$$\n\nShow that the binormal vector $\\mathbf{B}$ is constant, so that the curve is actually planar.\n\nMaking the calculations required in (10.5):\n\nand\n\n$$\n\\dot{\\mathbf{r}}=\\left(\\frac{-9 t^{2}}{5}, \\frac{12 t^{2}}{5}, 3\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{\\frac{81}{25} t^{4}+\\frac{144}{25} t^{4}+9}=3 \\sqrt{t^{4}+1}\n$$\n\n$$\n\\mathbf{T}=\\frac{\\left(-9 t^{2} / 5,12 t^{2} / 5,3\\right)}{3 \\sqrt{t^{4}+1}}=\\frac{\\left(-3 t^{2}, 4 t^{2}, 5\\right)}{5 \\sqrt{t^{4}+1}}\n$$\n\n$$\n\\begin{aligned}\n& \\ddot{\\mathbf{r}}=\\left(\\frac{-18 t}{5}, \\frac{24 t}{5}, 0\\right) \\\\\n& \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{9}{5} t^{2} & \\frac{12}{5} t^{2} & 3 \\\\\n-\\frac{18 t}{5} & \\frac{24 t}{5} & 0\n\\end{array}\\right|=\\left(-\\frac{72 t}{5},-\\frac{54 t}{5}, 0\\right)=\\frac{-18 t}{5}(4,3,0) \\\\\n& \\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|=\\frac{18|t|}{5} \\sqrt{4^{2}+3^{2}+0^{2}}=18|t| \\\\\n& (\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}=\\left(9 t^{4}+9\\right)\\left(-\\frac{18}{5} t, \\frac{24}{5} t, 0\\right)=\\left(-\\frac{162}{5} t^{5}-\\frac{162}{5} t, \\frac{216}{5} t^{5}+\\frac{216}{5} t, 0\\right) \\\\\n& (\\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(\\frac{9 \\cdot 18}{25} t^{3}+\\frac{24 \\cdot 12}{25} t^{3}+0\\right)\\left(-\\frac{9}{5} t^{2}, \\frac{12}{5} t^{2}, 3\\right)=\\left(-\\frac{162}{5} t^{5}, \\frac{216}{5} t^{5}, 54 t^{3}\\right) \\\\\n& (\\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}=\\left(-\\frac{162}{5} t, \\frac{216}{5} t,-54 t^{3}\\right)=18 t\\left(-\\frac{9}{5}, \\frac{12}{5},-3 t^{2}\\right) \\\\\n& \\quad \\mathbf{N}=\\varepsilon \\frac{-18 t\\left(9 / 5,-12 / 5,3 t^{2}\\right)}{\\left(3 \\sqrt{t^{4}+1}\\right)(18|t|)}=-\\frac{\\varepsilon t}{|t|} \\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}}\n\\end{aligned}\n$$\n\nand\n\nNow choose $\\varepsilon=+1$ when $t<0$ and -1 otherwise, making\n\n$$\n\\mathbf{N}=\\frac{\\left(3,-4,5 t^{2}\\right)}{5 \\sqrt{t^{4}+1}} \\quad \\mathbf{B}=\\varepsilon \\frac{(-18 t / 5)(4,3,0)}{18|t|}=\\frac{-\\varepsilon t}{|t|}\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)=\\left(\\frac{4}{5}, \\frac{3}{5}, 0\\right)\n$$", "10.4": "10.4 Establish the general formulas (10.5) for the moving frame of the curve $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}(t)$, with an arbitrary parameter $t$.\n\nBy definition,\n\n$$\n\\frac{d s}{d t}=\\|\\dot{\\mathbf{r}}\\| \\equiv(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{1 / 2} \\quad \\text { or } \\quad \\frac{d t}{d s}=\\|\\dot{\\mathbf{r}}\\|^{-1}\n$$\n\nand we have at once for the unit tangent vector\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\dot{\\mathbf{r}} \\frac{d t}{d s}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\nTo obtain a principal normal, first calculate\n\n$$\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\| \\equiv \\frac{d}{d t}(\\dot{\\mathbf{r}})^{1 / 2}=\\frac{1}{2}(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})^{-1 / 2}(\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}}+\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}})=\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}\n$$\n\n(Note the general formula $d\\|\\mathbf{u}\\| / d t=\\mathbf{u \\dot { u }} /\\|\\mathbf{u}\\|$.) Hence,\n\nand\n\n$$\n\\begin{gathered}\n\\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=-\\|\\dot{\\mathbf{r}}\\|^{-2} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|=-\\frac{\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\dot{\\mathbf{T}}=\\ddot{\\mathbf{r}} \\frac{d t}{d s}+\\dot{\\mathbf{r}} \\frac{d}{d t}\\|\\dot{\\mathbf{r}}\\|^{-1}=\\frac{\\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|}-\\frac{(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{3}} \\\\\n\\mathbf{T}^{\\prime}=\\dot{\\mathbf{T}} \\frac{d t}{d s}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{4}}=-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\n\\end{gathered}\n$$\n\nwhere the last step used the vector identify $\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}$. It follows that $\\mathbf{N}$ can be constructed by normalizing the vector\n\n$$\n\\mathbf{N}^{*} \\equiv-\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\n$$\n\nSince $\\dot{\\mathbf{r}}$ and $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$ are orthogonal, $\\left\\|\\mathbf{N}^{*}\\right\\|=\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|$ and so, provided $\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{N}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nFinally, for the binormal vector, with $\\mathbf{v} \\equiv \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\neq \\mathbf{0}$,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\frac{\\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|} \\times \\varepsilon \\frac{\\mathbf{v} \\times \\dot{\\mathbf{r}}}{\\|\\mathbf{v}\\|\\|\\dot{\\mathbf{r}}\\|}=\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\mathbf{v}-(\\dot{\\mathbf{r}} \\mathbf{v}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2} \\mathbf{v}-(0) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\mathbf{v}\\|}=\\varepsilon \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$", "10.5": "\\section*{CURVATURE AND TORSION}\n10.5 Find the curvature and the torsion of the circular helix\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c=\\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nwhere $s$ is arc length.\n\nBy differentiation with respect to arc length,\n\n$$\n\\mathbf{T}=\\mathbf{r}^{\\prime}=\\left(-\\frac{a}{c} \\sin \\frac{s}{c}, \\frac{a}{c} \\cos \\frac{s}{c}, \\frac{b}{c}\\right) \\quad \\mathbf{T}^{\\prime}=\\left(-\\frac{a}{c^{2}} \\cos \\frac{s}{c},-\\frac{a}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nNormalizing $\\mathbf{T}^{\\prime}$, choose\n\n$$\n\\mathbf{N}=\\left(-\\cos \\frac{s}{c},-\\sin \\frac{s}{c}, 0\\right)\n$$\n\nand, correspondingly,\n\n$$\n\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n-\\frac{a}{c} \\sin \\frac{s}{c} & \\frac{a}{c} \\cos \\frac{s}{c} & \\frac{b}{c} \\\\\n-\\cos \\frac{s}{c} & -\\sin \\frac{s}{c} & 0\n\\end{array}\\right|=\\left(\\frac{b}{c} \\sin \\frac{s}{c},-\\frac{b}{c} \\cos \\frac{s}{c}, \\frac{a}{c}\\right)\n$$\n\n$$\n\\mathbf{B}^{\\prime}=\\left(\\frac{b}{c^{2}} \\cos \\frac{s}{c}, \\frac{b}{c^{2}} \\sin \\frac{s}{c}, 0\\right)\n$$\n\nThen, by (10.6),\n\n$$\n\\kappa=\\frac{a}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{a}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{a}{c^{2}} \\quad \\tau=\\frac{b}{c^{2}} \\cos ^{2} \\frac{s}{c}+\\frac{b}{c^{2}} \\sin ^{2} \\frac{s}{c}+0^{2}=\\frac{b}{c^{2}}\n$$\n\n[If we introduce the \"time\" parameter $t=c s$, we then have:\n\n$$\n\\frac{d z}{d t}=\\frac{b}{c^{2}}=\\tau\n$$\n\ni.e. the rate at which the helix rises out of the $x y$-plane (its osculating plane at $t=0$ ) is given by its (constant) torsion.]", "10.6": "\\subsection*{10.6 Find the curvature and torsion of the curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(t$ real $)$}.\n\nUse the formulas $(10.8)$ :\n\n$$\n\\begin{gathered}\n\\dot{\\mathbf{r}}=\\left(2 t+\\sqrt{2}, 2 t-\\sqrt{2}, 2 t^{2}\\right) \\quad\\|\\dot{\\mathbf{r}}\\|=\\sqrt{(2 t+\\sqrt{2})^{2}+(2 t-\\sqrt{2})^{2}+4 t^{4}}=2\\left(t^{2}+1\\right) \\\\\n\\ddot{\\mathbf{r}}=(2,2,4 t) \\quad \\ddot{\\mathbf{r}}=(0,0,4) \\\\\n\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n2 t+\\sqrt{2} & 2 t-\\sqrt{2} & 2 t^{2} \\\\\n2 & 2 & 4 t\n\\end{array}\\right|=4\\left(t^{2}-t \\sqrt{2},-\\left(t^{2}+t \\sqrt{2}\\right), \\sqrt{2}\\right) \\\\\n(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})^{2}=16\\left[\\left(t^{2}-t \\sqrt{2}\\right)^{2}+\\left(t^{2}+t \\sqrt{2}\\right)^{2}+2\\right]=32\\left(t^{2}+1\\right)^{2} \\\\\n\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]=\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=(0,0,4) \\cdot 4\\left(t^{2}-t \\sqrt{2},-t^{2}-t \\sqrt{2}, \\sqrt{2}\\right)=16 \\sqrt{2}\n\\end{gathered}\n$$\n\nHence\n\n$$\n\\kappa=\\frac{\\varepsilon \\sqrt{32\\left(t^{2}+1\\right)^{2}}}{8\\left(t^{2}+1\\right)^{3}}=\\frac{\\varepsilon}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}} \\quad \\tau=\\frac{16 \\sqrt{2}}{32\\left(t^{2}+1\\right)^{2}}=\\frac{1}{\\sqrt{2}\\left(t^{2}+1\\right)^{2}}\n$$", "10.7": "\\subsection*{10.7 Prove (10.8).}\nUsing the results of Problem 10.4, we have\n\n$$\n\\begin{aligned}\n\\kappa=\\mathbf{N T}^{\\prime} & =\\left(\\varepsilon \\frac{(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) \\times \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\\right) \\cdot\\left(-\\frac{\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|^{4}}\\right)=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})\\|^{2}}{\\|\\dot{\\mathbf{r}}\\|^{5}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\\\\n& =\\varepsilon \\frac{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2} \\sin ^{2}(\\pi / 2)}{\\|\\dot{\\mathbf{r}}\\|^{5} \\mid \\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}} \\|}=\\varepsilon \\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}{\\|\\dot{\\mathbf{r}}\\|^{3}}\n\\end{aligned}\n$$\n\nThe torsion requires the computation of $\\mathbf{B}^{\\prime}$. By (10.5),\n\n$$\n\\varepsilon \\mathbf{B}=\\frac{\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|} \\equiv \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n$$\n\nwhence\n\n$$\n\\varepsilon_{\\varepsilon} \\dot{\\mathbf{B}}=\\frac{d}{d t}\\left(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\right)=\\frac{1}{\\|\\mathbf{v}\\|} \\dot{\\mathbf{v}}+\\frac{d}{d t}\\left(\\frac{1}{\\|\\mathbf{v}\\|}\\right) \\mathbf{v}=\\frac{\\dot{\\mathbf{v}}}{\\|\\mathbf{v}\\|}-\\frac{(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}=\\frac{\\|\\mathbf{v}\\|^{2} \\dot{\\mathbf{v}}-(\\mathbf{v} \\dot{\\mathbf{v}}) \\mathbf{v}}{\\|\\mathbf{v}\\|^{3}}\n$$\n\nBut $\\dot{\\mathbf{r}}=d(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}) / d t=(\\ddot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})+(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})=\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}$; hence,\n\n$$\n\\varepsilon \\mathbf{B}^{\\prime}=\\frac{\\varepsilon \\dot{\\mathbf{B}}}{\\|\\dot{\\mathbf{r}}\\|}=\\frac{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})-[(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})](\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{3}}\n$$\n\nDot this with\n\n$$\n\\varepsilon \\mathbf{N}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}}) \\ddot{\\mathbf{r}}-(\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}) \\dot{\\mathbf{r}}}{\\|\\dot{\\mathbf{r}}\\|\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|}\n$$\n\nfrom (10.5), and use $\\mathbf{u} \\cdot(\\mathbf{u} \\times \\mathbf{w})=0$ :\n\n$$\n\\begin{gathered}\n\\mathbf{N B}^{\\prime}=\\frac{(\\dot{\\mathbf{r}} \\dot{\\mathbf{r}})\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}[\\ddot{\\mathbf{r}} \\cdot(\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}})]-0-0+0}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{4}}=\\frac{\\|\\dot{\\mathbf{r}}\\|^{2}(-\\operatorname{det}[\\ddot{\\mathbf{r}} \\dot{\\mathbf{r}} \\ddot{\\mathbf{r}}])}{\\|\\dot{\\mathbf{r}}\\|^{2}\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}} \\\\\n\\tau=\\frac{\\operatorname{det}[\\dot{\\mathbf{r}} \\ddot{\\mathbf{r}} \\ddot{\\mathbf{r}}]}{\\|\\dot{\\mathbf{r}} \\times \\ddot{\\mathbf{r}}\\|^{2}}\n\\end{gathered}\n$$\n\nor", "10.8": "\\section*{CURVATURE OF SURFACES}\n10.8 Prove $(a) \\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B},(b) \\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}$.\n\n(a) Since $\\mathbf{N N}=1, \\mathbf{N}^{\\prime}$ is orthogonal to $\\mathbf{N}$, which puts it in the plane of $\\mathbf{T}$ and $\\mathbf{B}$. Therefore, for certain real $\\lambda$ and $\\mu$,\n\n\n\\begin{equation*}\n\\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}+\\mu \\mathbf{B} \\tag{1}\n\\end{equation*}\n\n\nDot both sides by $\\mathbf{T}$, then by $\\mathbf{B}$, and use $\\mathbf{T N}=0, \\kappa=\\mathbf{N T}^{\\prime}$, and $\\tau=-\\mathbf{N B}^{\\prime}$ :\n\n$$\n\\begin{aligned}\n& \\mathbf{T} \\mathbf{N}^{\\prime}=\\lambda \\mathbf{T}^{2}+\\mu \\mathbf{T B}=\\lambda \\quad \\text { or } \\quad \\lambda=-\\mathbf{T}^{\\prime} \\mathbf{N}=-\\kappa \\\\\n& \\mathbf{B N}^{\\prime}=\\tau=\\lambda \\mathbf{B T}+\\mu \\mathbf{B}^{2}=\\mu\n\\end{aligned}\n$$\n\nSubstitution for $\\lambda$ and $\\mu$ in (1) then yields the desired results.\n\n(b) From $\\mathbf{B}=\\mathbf{T} \\times \\mathbf{N}$ and part (a),\n\n$$\n\\begin{aligned}\n\\mathbf{B}^{\\prime} & =\\mathbf{T}^{\\prime} \\times \\mathbf{N}+\\mathbf{T} \\times \\mathbf{N}^{\\prime}=(\\kappa \\mathbf{N}) \\times \\mathbf{N}+\\mathbf{T} \\times(-\\boldsymbol{\\kappa} \\mathbf{T}+\\tau \\mathbf{B}) \\\\\n& =0+0+\\tau(\\mathbf{T} \\times \\mathbf{B})=\\tau(-\\mathbf{N})=-\\tau \\mathbf{N}\n\\end{aligned}\n$$", "10.9": "\\subsection*{10.9 Prove that if a curve has $\\kappa^{\\prime}=0$ at some point, then $\\mathbf{N}^{\\prime \\prime}$ is orthogonal to $\\mathbf{T}$ at that point.}\n\nFrom $\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B}$, it follows that $\\mathbf{N}^{\\prime \\prime}=-\\kappa^{\\prime} \\mathbf{T}-\\kappa \\mathbf{T}^{\\prime}+\\tau^{\\prime} \\mathbf{B}+\\tau \\mathbf{B}^{\\prime}$. But $\\kappa^{\\prime}=0$; and from the Serret-Frenet formulas for $\\mathbf{T}^{\\prime}$ and $\\mathbf{B}^{\\prime}$ we obtain\n\n$$\n\\mathbf{N}^{\\prime \\prime}=-\\kappa(\\kappa \\mathbf{N})+\\tau^{\\prime} \\mathbf{B}+\\tau(-\\tau \\mathbf{N})=\\left(-\\kappa^{2}-\\tau^{2}\\right) \\mathbf{N}+\\tau^{\\prime} \\mathbf{B}\n$$\n\nAs $\\mathbf{N}^{\\prime \\prime}$ is in the plane of $\\mathbf{N}$ and $\\mathbf{B}$, it is orthogonal to $\\mathbf{T}$.", "10.10": "\\section*{SURFACES IN EUCLIDEAN SPACE}\n10.10 Show that a surface of revolution is regular and exhibit the unit surface normal.\n\nThe Gaussian form of a surface of revolution about the $z$-axis (Fig. 10-11) is\n\n$$\n\\mathbf{r}=\\left(f\\left(x^{1}\\right) \\cos x^{2}, f\\left(x^{1}\\right) \\sin x^{2}, g\\left(x^{1}\\right)\\right) \\quad\\left(f\\left(x^{1}\\right)>0\\right)\n$$\n\nso\n\n$$\n\\mathbf{r}_{1}=\\left(f^{\\prime}\\left(x^{1}\\right) \\cos x^{2}, f^{\\prime}\\left(x^{1}\\right) \\sin x^{2}, g^{\\prime}\\left(x^{1}\\right)\\right) \\quad \\mathbf{r}_{2}=\\left(-f\\left(x^{1}\\right) \\sin x^{2}, f\\left(x^{1}\\right) \\cos x^{2}, 0\\right)\n$$\n\nand $\\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left(-f g^{\\prime} \\cos x^{2},-f g^{\\prime} \\sin x^{2}, f f^{\\prime}\\left(\\cos ^{2} x^{2}+\\sin ^{2} x^{2}\\right)\\right)$, with norm\n\n$$\nE=\\sqrt{f^{2} g^{\\prime 2} \\cos ^{2} x^{2}+f^{2} g^{\\prime 2} \\sin ^{2} x^{2}+f^{2} f^{\\prime 2}}=f \\sqrt{f^{\\prime 2}+g^{\\prime 2}}\n$$\n\nNow $f=f\\left(x^{1}\\right) \\neq 0$; further, the generating curve is regular, which means that, with $t=x^{1}$, the tangent vector of that curve,\n\n$$\n\\left(\\frac{d x}{d t}, 0, \\frac{d z}{d t}\\right)=\\left(f^{\\prime}, 0, g^{\\prime}\\right)\n$$\n\nis non-null and $f^{\\prime 2}+g^{\\prime 2} \\neq 0$. Therefore, $E \\neq 0$ and the surface is regular.\n\nThe unit surface normal is\n\n$$\n\\mathbf{n}=\\frac{1}{E}\\left(\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right)=\\left(-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\cos x^{2},-\\frac{g^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}} \\sin x^{2}, \\frac{f^{\\prime}}{\\sqrt{f^{\\prime 2}+g^{\\prime 2}}}\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153(1)}\n\\end{center}\n\nFig. 10-11", "10.11": "\\subsection*{10.11 Identify the $x^{1}$ - and $x^{2}$-curves for the right helicoid (Example 10.4) and describe the behavior of the unit surface normal along an $x^{1}$-curve.}\n\nThe $x^{1}$-curves $\\left(x^{2}=\\right.$ const.) are given by\n\n$$\n\\mathbf{r}=\\left(0,0, a x^{2}\\right)+x^{1}\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad\\left(x^{1} \\geqq 0\\right)\n$$\n\nthus, they are rays parallel to the $x y$-plane. The $x^{2}$-curves $\\left(x^{1}=\\right.$ const.) are given by\n\n$$\n\\sqrt{x^{2}+y^{2}}=x^{1} \\quad z=a x^{2}\n$$\n\ni.e., circular helices of radii $x^{1}$.\n\nWe have:\n\n$$\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, 0\\right) \\quad \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, a\\right)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153}\n\\end{center}\n\nFig. 10-12\n\n$$\n\\begin{aligned}\n& \\mathbf{r}_{1} \\times \\mathbf{r}_{2}=\\left|\\begin{array}{ccc}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\cos x^{2} & \\sin x^{2} & 0 \\\\\n-x^{1} \\sin x^{2} & x^{1} \\cos x^{2} & a\n\\end{array}\\right|=\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n& \\mathbf{n}=\\frac{\\mathbf{r}_{1} \\times \\mathbf{r}_{2}}{\\left\\|\\mathbf{r}_{1} \\times \\mathbf{r}_{2}\\right\\|}=\\left(\\frac{a \\sin x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{-a \\cos x^{2}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}, \\frac{x^{1}}{\\sqrt{a^{2}+\\left(x^{1}\\right)^{2}}}\\right) \\\\\n&=(\\cos \\omega) \\mathbf{u}+(\\sin \\omega) \\mathbf{v}\n\\end{aligned}\n$$\n\nand\n\nwhere $\\omega \\equiv \\tan ^{-1}\\left(x^{1} / a\\right), \\mathbf{u} \\equiv\\left(\\sin x^{2},-\\cos x^{2}, 0\\right), \\mathbf{v} \\equiv(0,0,1)$. On an $\\mathbf{x}^{1}$-ray, $\\mathbf{u}$ and $\\mathbf{v}$ are fixed unit vectors, while $\\omega$ increases from 0 to a $\\pi / 2$ as $x^{1}$ increases from 0 to $\\infty$. Thus, $\\mathbf{n}$ traces out a quarter-circle as the ray is described (see Fig. 10-12).", "10.12": "I'm sorry, but there doesn't seem to be a Solved Problem 10.12 in the text you've provided.", "10.13": "I'm sorry, but there doesn't seem to be a Solved Problem 10.13 in the provided text.", "10.14": "\\subsection*{10.14 Let $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$ be two curves on the right circular cone $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, 2 x^{1}\\right)$ whose pre-images in the parameter plane are}\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=3-t \\\\\nx^{2}=t / 2\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=\\sigma>0 \\\\\nx^{2}=\\sigma^{2}\n\\end{array}\\right.\\right.\n$$\n\n\\textit{At the point of intersection, find the angle between $\\mathscr{C}_{1}$ and $\\mathscr{C}_{2}$, and show that orthogonality in the $x^{1} x^{2}$-plane does not carry over to the cone.}\n\nThe intersection point $P^{\\prime}$ of the two pre-image curves is determined by the simultaneous equations\n\n$$\n3-t=\\sigma \\quad \\text { and } \\quad \\frac{t}{2}=\\sigma^{2}\n$$\n\nwhich give $t=2, \\tau=1$, and $P^{\\prime}=(1,1)$. Thus, the two tangent vectors at $P^{\\prime}$ are:\n\n$$\n\\left(u^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d t}\\right)\\right|_{t=2}=\\left(-1, \\frac{1}{2}\\right) \\quad\\left(v^{i}\\right)=\\left.\\left(\\frac{d x^{i}}{d \\sigma}\\right)\\right|_{\\sigma=1}=(1,2)\n$$\n\nConsidered in the Euclidean sense, $\\left(u^{i}\\right)$ and $\\left(v^{i}\\right)$ are orthogonal.\n\nTo express the angle between tangents at the image of $P^{\\prime}$, we adopt the metric (2) of Problem 10.12 (with $a=2$ ) and apply (10.18) for $x^{1}=1, x^{2}=1$ :\n\n$$\n\\cos \\theta=\\frac{\\left(1+2^{2}\\right)(-1)(1)+(1)^{2}\\left(\\frac{1}{2}\\right)(2)}{D}=\\frac{-4}{D} \\neq 0\n$$\n\nTherefore, the curves are not orthogonal at the image of $P^{\\prime}$.", "10.15": "\\subsection*{10.15 Prove Theorem 10.3 and verify Corollary 10.4 geometrically for the right helicoid (Example 10.4) and for any surface of revolution (Problem 10.12).}\n\nThe proof consists merely in taking $\\left(u^{i}\\right)=(1,0)$ and $\\left(v^{i}\\right)=(0,1)$ in (10.18). (Compare Problem 5.31.)\n\nAs is clear from Problem 10.11, the right helicoid is a ruled surface, generated by a half-line (an $x^{1}$-curve), pivoted on the $z$-axis, that rotates parallel to the $x y$-plane while the pivot point travels up the $z$-axis. A given point $P$ of the generator thus describes a helical $x^{2}$-curve (Fig. 10-8), which is necessarily everywhere orthogonal to the generator (i.e., to the $x^{1}$-curves). As for surfaces of revolution, it is clear that the parameter lines that match the revolved planar curve ( $x^{1}$-curves, or meridians) and the circles traced by individual points of the planar curve ( $x^{2}$-curves, or parallels of latitude) are mutually orthogonal. By previous computations, $g_{12}=0$ for both the helicoid and for the general surface of revolution.", "10.16": "\\subsection*{10.16 Show that under a change of coordinates $x^{1}=x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}=x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$ in the plane, the surface metric $\\left(g_{i j}\\right)$ transforms as a second-order covariant tensor.}\n\nWe have by substitution $\\mathbf{r}\\left(x^{1}, x^{2}\\right)=\\mathbf{r}\\left(x^{1}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right), x^{2}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)\\right) \\equiv \\overline{\\mathbf{r}}\\left(\\bar{x}^{1}, \\bar{x}^{2}\\right)$, the latter being the \"new\" parameterization for $\\mathscr{S}$. To compute the metric under this parameterization, write (by the chain rule for partial derivatives and the bilinearity of the inner product)\n\n$$\n\\begin{aligned}\n\\bar{g}_{i j} & =\\overline{\\mathbf{r}}_{i} \\overline{\\mathbf{r}}_{j} \\equiv \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{i}} \\frac{\\partial \\overline{\\mathbf{r}}}{\\partial \\bar{x}^{j}}=\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{p}} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\frac{\\partial \\mathbf{r}}{\\partial x^{q}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right)=\\left(\\mathbf{r}_{p} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}}\\right) \\cdot\\left(\\mathbf{r}_{q} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\\right) \\\\\n& =\\mathbf{r}_{p} \\mathbf{r}_{q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}} \\equiv g_{p q} \\frac{\\partial x^{p}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{q}}{\\partial \\bar{x}^{j}}\n\\end{aligned}\n$$\n\nwhich is the correct formula for tensor character.", "10.17": "\\section*{GEODESICS}\n10.17 (a) Find the Christoffel symbols of the second kind for the sphere of radius $a$. (b) Verify that the great circles passing through the north and south poles (i.e., the $x^{1}$-curves) are geodesics.\\\\\n(a) The FFF for the sphere of radius $a$ may be calculated from Problem 10.12:\n\n$$\ng_{11}=a^{2} \\quad g_{12}=0=g_{21} \\quad g_{22}=a^{2} \\sin ^{2} x^{1}\n$$\n\nThe formulas from Problem 6.4 can be used, since $\\left(g_{i j}\\right)$ is diagonal; the nonzero Christoffel symbols are found to be:\n\n$$\n\\Gamma_{22}^{1}=-\\sin x^{1} \\cos x^{1} \\quad \\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\cot x^{1}\n$$\n\n(b) We want to show that the family of curves $x^{1}=t, x^{2}=d=$ const. are integral curves of the differential system (7.11)-(7.12), which may be conveniently written as\n\n$$\n\\frac{d^{2} x^{i}}{d t^{2}}+\\Gamma_{j k}^{i} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}=\\frac{1}{2} \\frac{d x^{i}}{d t}\\left[\\frac{d}{d t} \\ln \\left(g_{j k} \\frac{d x^{j}}{d t} \\frac{d x^{k}}{d t}\\right)\\right]\n$$\n\nor, for the given metric,\n\n$$\n\\begin{array}{ll}\ni=1 & \\frac{d^{2} x^{1}}{d t^{2}}-\\left(\\sin x^{1} \\cos x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}=\\frac{1}{2} \\frac{d x^{1}}{d t}\\left[\\frac{d}{d t} \\ln \\left(a^{2}\\left(\\frac{d x^{1}}{d t}\\right)^{2}+\\left(a^{2} \\sin ^{2} x^{1}\\right)\\left(\\frac{d x^{2}}{d t}\\right)^{2}\\right)\\right] \\\\\ni=2 & \\frac{d^{2} x^{2}}{d t^{2}}+\\left(2 \\cot x^{1}\\right) \\frac{d x^{1}}{d t} \\frac{d x^{2}}{d t}=\\frac{1}{2} \\frac{d x^{2}}{d t}\\left[\\frac{d}{d t} \\ln \\mathrm{I}\\right.\n\\end{array}\n$$\n\nSince $d x^{1} / d t=1$ and $d x^{2} / d t=0$, both equations reduce to $0=0$, and the verification is complete.", "10.18": "\\section*{CURVATURE OF SURFACES}\n10.18 Prove Theorem 10.6: A curve on a regular surface is a geodesic if and only if, by proper choice of the principal normal, $\\mathbf{N}=\\mathbf{n}$.\n\nLet any curve on the surface be given by $\\mathscr{C}: \\mathbf{r}=\\mathbf{r}\\left(x^{1}(s), x^{2}(s)\\right)$, where $s=\\operatorname{arc}$ length, Then,\n\n$$\n\\mathbf{T}=\\mathbf{r}_{i} \\frac{d x^{i}}{d s}\n$$\n\nand the first formula (10.9) gives\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s}\\left(\\frac{\\partial \\mathbf{r}_{i}}{\\partial x^{j}} \\frac{d x^{j}}{d s}\\right) \\equiv \\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\tag{1}\n\\end{equation*}\n\n\nDot both sides of (1) by the vector $\\mathbf{r}_{k}$ and use the resuit of Problem 10.48:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\mathbf{r}_{i} \\mathbf{r}_{k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\mathbf{r}_{i j} \\mathbf{r}_{k} \\equiv \\frac{d^{2} x^{i}}{d s^{2}} g_{i k}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\Gamma_{i j k} \\tag{2}\n\\end{equation*}\n\n\nMultiply both sides of (2) by $g^{k l}$ and sum on $k$ :\n\n\n\\begin{equation*}\ng^{k l} \\kappa \\mathbf{r}_{k} \\mathbf{N}=\\frac{d^{2} x^{i}}{d s^{2}} \\delta_{i}^{l}+\\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} g^{k l} \\Gamma_{i j k}=\\frac{d^{2} x^{l}}{d s^{2}}+\\Gamma_{i j}^{l} \\frac{d x^{i}}{d s} \\frac{d x^{j}}{d s} \\tag{3}\n\\end{equation*}\n\n\nNow if $\\mathscr{C}$ is a geodesic, the right side of (3) vanishes, and this implies that $\\kappa \\mathbf{r}_{k} \\mathbf{N}=0$ for $k=1,2$. If $\\kappa \\neq 0$, then $\\mathbf{r}_{1} \\mathbf{N}=0=\\mathbf{r}_{2} \\mathbf{N}$; so that $\\mathbf{N}$ is orthogonal to both $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ (thus to the tangent plane). Therefore, but for orientation, $\\mathbf{N}=\\mathbf{n}$. If $\\kappa=0$ at some point $P$ and there is a sequence of points along the curve approaching $P$ for which $\\kappa \\neq 0$, then, by continuity, $\\mathbf{N}=\\mathbf{n}$. Otherwise, $\\kappa=0$ on an interval and the curve is a straight line on that interval, in which case its principal normal $\\mathbf{N}$ can be chosen to agree with $\\mathbf{n}$. Conversely, if the curve has the property that $\\mathbf{N}=\\mathbf{n}$ at all points, then $\\mathbf{r}_{k} \\mathbf{N}=\\mathbf{r}_{k} \\mathbf{n}=0$ and the left side of (3) vanishes, showing that the pre-image of $\\mathscr{C}$ satisfies the differential equations for a geodesic.", "10.19": "I'm sorry, but there doesn't seem to be a Solved Problem 10.19 in the provided text.", "10.20": "10.20 Demonstrate that the intrinsic curvature $\\tilde{\\kappa}$ of a curve on a surface can be different from its curvature $\\kappa$ as a curve in $\\mathbf{E}^{3}$.\n\nOne example is a circle on a sphere of radius $a$. If the circle also has radius $a$, it is a great circle and, hence, a geodesic with zero intrinsic curvature. But its curvature as a (planar) curve in $\\mathbf{E}^{3}$ is $1 / a$. Another example is the circular helix: its curvature is nonzero as a curve in $\\mathbf{E}^{3}$, but as a geodesic on a circular cylinder its intrinsic curvature is zero.", "10.21": "\\section*{SECOND FUNDAMENTAL FORM}\n10.21 (a) Find the SFF for the right circular cone of Problem 10.12. (b) At the point $P(1,0, a)$, calculate the curvature of the normal section having the direction $\\mathbf{j}$ at $P$ (see Fig. 10-16).\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157(1)}\n\\end{center}\n\nFig. 10-16\\\\\n(a) From $\\mathbf{r}=\\left(x^{1} \\cos x^{2}, x^{1} \\sin x^{2}, a x^{1}\\right) \\quad\\left(x^{1}>0\\right)$, we obtain:\n\n$$\n\\begin{array}{ccc}\n\\mathbf{r}_{1}=\\left(\\cos x^{2}, \\sin x^{2}, a\\right) & \\mathbf{r}_{2}=\\left(-x^{1} \\sin x^{2}, x^{1} \\cos x^{2}, 0\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) & \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) & \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{array}\n$$\n\nand by Problem 10.10, $\\mathbf{n}=\\left(a^{2}+1\\right)^{-1 / 2}\\left(-a \\cos x^{2},-a \\sin x^{2}, 1\\right)$. The coefficients in II are thus $f_{12} \\equiv \\mathbf{n r}_{11}=0, f_{12}=f_{21} \\equiv \\mathbf{n} \\mathbf{r}_{12}=0, f_{22} \\equiv \\mathbf{n r}_{22}=\\left(a^{2}+1\\right)^{-1 / 2} a x^{1}$.\n\n(b) The direction $\\mathbf{j}$ at $P$ corresponds to the direction $\\left(u^{1}, u^{2}\\right)$ at $P^{\\prime}=(1,0)$ in the parameter plane, where\n\n$$\n\\begin{aligned}\n\\mathbf{j} & =u^{1} \\mathbf{r}_{1}(P)+u^{2} \\mathbf{r}_{2}(P) \\\\\n(0,1,0) & =u^{1}(1,0, a)+u^{2}(0,1,0) \\\\\n(0,1,0) & =\\left(u^{1}, u^{2}, a u^{1}\\right)\n\\end{aligned}\n$$\n\nThus, $u^{1}=0$ and $u^{2}=1$. Appropriating I from Problem 10.12, we have\n\n$$\n\\kappa_{g f}=\\frac{\\mathrm{II}\\left(u^{1}, u^{2}\\right)}{\\mathrm{I}\\left(u^{1}, u^{2}\\right)}=\\frac{f_{22}(P)\\left(u^{2}\\right)^{2}}{\\left(a^{2}+1\\right)\\left(u^{1}\\right)^{2}+(1)^{2}\\left(u^{2}\\right)^{2}}=f_{22}(P)=\\frac{a}{\\sqrt{a^{2}+1}}\n$$", "10.22": "\\subsection*{10.22 Develop geometrically a notion of \"parallel transport\" of a vector along a curve $\\mathscr{C}$ on a regular surface $\\mathscr{S}$.}\n\nImagine $\\mathscr{C}$, and with it $\\mathscr{S}$, as being rolled without slipping onto a fixed plane $\\mathscr{F}$, in such fashion that the point of contact is aways on $\\mathscr{C}$ and the tangent plane to $\\mathscr{S}$ at the point of contacts always coincides with $\\mathscr{F}$. This maps $\\mathscr{C}$ to a (planar) curve $\\mathscr{C}^{*}$ in $\\mathscr{F}$ that has the same arc-length parameter and the same tangent vector. Then, any vector in $\\mathscr{F}$ that is attached to the point of contact and that remains parallel to itself (in the ordinary Euclidean sense) as the contact point describes $\\mathscr{C}^{*}$ may-under the inverse mapping $\\mathscr{C}^{*} \\rightarrow \\mathscr{C}$-be considered as undergoing parallel transport along $\\mathscr{C}$. In general, parallel transport of a given vector around a closed curve on a surface does not reproduce the initial vector.", "10.23": "\\subsection*{10.23 Prove (10.23).}\n\nStart with the formula for the unit tangent vector of any curve $\\mathscr{C}$ on $\\mathscr{S}$ :\n\n$$\n\\mathbf{T}=\\frac{u^{i} \\mathbf{r}_{i}}{\\sqrt{g_{k l} u^{k} u^{i}}} \\quad\\left(u^{i} \\equiv \\frac{d x^{i}}{d t}\\right)\n$$\n\nThen\n\n\n\\begin{equation*}\n\\dot{\\mathbf{T}}=\\frac{d}{d t}\\left(\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}}\\right) \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\dot{\\mathbf{r}}_{i}=Q^{i} \\mathbf{r}_{i}+\\frac{u^{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i j} u^{j} \\tag{1}\n\\end{equation*}\n\n\nwhere $Q^{i}$ is an abbreviation for the scalar coefficient of $\\mathbf{r}_{i}$. Now the Frenet formula gives $\\kappa \\mathbf{N}=\\mathbf{T}^{\\prime}=$ $\\dot{\\mathbf{T}} / \\sqrt{g_{k l} u^{k} u^{\\prime}}$; together with (1), this yields:\n\n\n\\begin{equation*}\n\\kappa \\mathbf{N}=\\frac{Q_{i}}{\\sqrt{g_{k l} u^{k} u^{l}}} \\mathbf{r}_{i}+\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\tag{2}\n\\end{equation*}\n\n\nDot both sides of (2) by $\\mathbf{n}$ (the surface normal) and use the fact that $\\mathbf{r}_{i} \\mathbf{n}=0$ for each $i$ :\n\n\n\\begin{equation*}\n\\kappa \\mathbf{n} \\mathbf{N}=\\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \\mathbf{r}_{i j} \\mathbf{n} \\equiv \\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} f_{i j} \\tag{3}\n\\end{equation*}\n\n\nIf $\\mathscr{C}$ is a normal section $\\mathscr{C}_{\\mathscr{F}}$ at $P$, and $\\kappa, \\mathbf{N}$, and the right side of (3) are all evaluated at $P$, then $\\kappa=\\kappa_{\\mathscr{F}}$, $\\mathbf{n N}=\\mathbf{n}^{2}=1$, and (3) becomes the desired expression\n\n$$\n\\kappa_{\\mathscr{F}}=\\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}\n$$", "10.24": "\\section*{CURVATURE OF SURFACES}\n10.24 Show that the maximum and minimum values of the function\n\n$$\nF(\\mathbf{u})=\\frac{a_{i j} u^{i} u^{j}}{b_{k l} u^{k} u^{l}}=\\frac{\\mathbf{u}^{T} A \\mathbf{u}}{\\mathbf{u}^{T} B \\mathbf{u}}\n$$\n\nwhere $A=\\left[a_{i j}\\right]_{22}, B=\\left[b_{i j}\\right]_{22}, \\mathbf{u}=\\left(u^{1}, u^{2}\\right) \\neq(0,0)$, with $B$ positive definite, are the two roots of the quadratic equation in $\\lambda$\n\n\\[\n\\operatorname{det}(A-\\lambda B) \\equiv\\left|\\begin{array}{ll}\na_{11}-\\lambda b_{11} & a_{12}-\\lambda b_{12}  \\tag{1}\\\\\na_{21}-\\lambda b_{21} & a_{22}-\\lambda b_{22}\n\\end{array}\\right|=0\n\\]\n\n(hence, eigenvalues of $B^{-1} A$ ), and that the extreme values of $F$ occur for vectors $\\mathbf{u}$ satisfying $(A-x B) \\mathbf{u}=\\mathbf{0}$, where $x$ takes on the two eigenvalues of $B^{-1} A$ (hence, eigenvectors of $B^{-1} A$ ).\n\nWe may assume without loss of generality that $A$ and $B$ are symmetric. Let $\\mathscr{G}$ be any simple closed curve in the $u^{1} u^{2}$-plane having the origin in its interior. The Weierstrass theorem guarantees that $F(\\mathbf{u})$ asumes a largest value on $\\mathscr{G}$; say, $F(\\mathbf{w})=M$. Because $F$ is constant on rays emanating from the origin $(F(\\lambda \\mathbf{u})=F(\\mathbf{u})$ for any $\\lambda \\neq 0)$, the absolute maximum on $\\mathscr{G}$ is both an absolute and a relative maximum in the $u^{1} u^{2}$-plane; hence, the gradient of $F$ must vanish at $\\mathbf{w}$. We have:\n\n$$\n\\begin{gathered}\n\\frac{\\partial F(\\mathbf{u})}{\\partial u^{P}}=\\frac{\\left(b_{k l} u^{k} u^{l}\\right)\\left(2 a_{p j} u^{j}\\right)-\\left(a_{i j} u^{i} u^{j}\\right)\\left(2 b_{p l} u^{l}\\right)}{\\left(b_{k l} u^{k} u^{l}\\right)^{2}}=\\frac{2}{b_{k l} u^{k} u^{l}}\\left[a_{p j} u^{j}-F(\\mathbf{u})\\left(b_{p l} u^{l}\\right)\\right] \\\\\n\\nabla F(\\mathbf{u})=\\frac{2}{\\mathbf{u}^{T} B \\mathbf{u}}[A \\mathbf{u}-F(\\mathbf{u}) B \\mathbf{u}]\n\\end{gathered}\n$$\n\nor\n\nTherefore, $A \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0}$, which shows (i) that $M$ is an eigenvalue of $B^{-1} A$ and thus is a root of the characteristic equation (1); (ii) that $\\mathbf{w}$ is an eigenvector belonging to $M$.\n\nA like consideration of the minimum value, $m$, of $F$ on $\\mathscr{G}$ leads to the other eigenvalue and associated eigenvector.", "10.25": "\\section*{CURVATURE OF SURFACES}\n10.25 Prove that the extreme normal curvatures $\\kappa_{1}, \\kappa_{2}$ are the two roots of the quadratic equation $(10.25)$.\n\nIn Problem 10.24, take $a_{i j}=f_{i j}$ and $b_{k l}=g_{k l}$; expand (1) to obtain (10.25).", "10.26": "\\subsection*{10.26 Prove that the two normal section curves through $P$ on $\\mathscr{S}$ giving rise to $\\max \\kappa_{\\mathscr{F}}=\\kappa_{1}$ and $\\min \\kappa_{\\mathscr{F}}=\\kappa_{2}$ are orthogonal when $\\kappa_{1} \\neq \\kappa_{2}$ (that is, when $P$ is not an umbilical point on $\\mathscr{S}$ ).}\n\nLet us prove the general result, in the notation of Problem 10.24. We have:\n\n$$\nA \\mathbf{w}-M B \\mathbf{w}=\\mathbf{0} \\quad A \\mathbf{v}-m B \\mathbf{v}=\\mathbf{0}\n$$\n\nWith the inner product of column vectors defined as $\\mathbf{p} \\cdot \\mathbf{q} \\equiv \\mathbf{p}^{T} B \\mathbf{q}$, multiply the first equation by $\\mathbf{v}^{T}$ and the second by $\\mathbf{w}^{T}$, and subtract:\n\n$$\n(m-M) \\mathbf{v} \\cdot \\mathbf{w}=0\n$$\n\nHence, if $m \\neq M, \\mathbf{v}$ and $\\mathbf{w}$ are orthogonal.", "10.27": "\\subsection*{10.27 Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for the right helicoid. Show that as $x^{1} \\rightarrow \\infty, \\mathrm{K}$ tends to zero (the surface becomes \"flatter\" as the distance from its axis increases without bound).}\n\nFrom Problem 10.11 and Example 10.4,\n\n$$\n\\begin{gathered}\n\\mathbf{n}=\\frac{1}{\\sqrt{\\left(x^{1}\\right)^{2}+a^{2}}}\\left(a \\sin x^{2},-a \\cos x^{2}, x^{1}\\right) \\\\\n\\mathbf{r}_{11}=(0,0,0) \\quad \\mathbf{r}_{12}=\\mathbf{r}_{21}=\\left(-\\sin x^{2}, \\cos x^{2}, 0\\right) \\quad \\mathbf{r}_{22}=\\left(-x^{1} \\cos x^{2},-x^{1} \\sin x^{2}, 0\\right)\n\\end{gathered}\n$$\n\nso that\n\nand\n\n$$\n\\begin{gathered}\nf_{11}=\\mathbf{n r}_{11}=0 \\quad f_{12}=f_{21}=-a / \\sqrt{\\left(x^{1}\\right)^{2}+a^{2}} \\quad f_{22}=0 \\\\\n\\mathrm{~K}=\\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0-a^{2} /\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]}=-\\frac{a^{2}}{\\left[\\left(x^{1}\\right)^{2}+a^{2}\\right]^{2}} \\rightarrow 0 \\quad \\text { as } x^{1} \\rightarrow \\infty \\\\\n\\mathrm{H}=\\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}}=\\frac{0+0-2(0)}{g}=0\n\\end{gathered}\n$$", "10.28": "\\subsection*{10.28 Complete the proof of $(10.27 a)$.}\n\nThe relations $\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}$ and $\\mathbf{n}_{j} \\mathbf{r}_{i}=-f_{i j}$ imply\n\n$$\n-f_{i j}=u_{j}^{k} g_{k i}\n$$\n\nMultiply both sides by $g^{i s}$ and sum over $i$, obtaining $u_{j}^{s}=-g^{l s} f_{l j}$. Hence,\n\n$$\n\\mathbf{n}_{i}=u_{i}^{k} \\mathbf{r}_{k}=-g^{l k} f_{l i} \\mathbf{r}_{k}=-g^{l k} f_{i l} \\mathbf{r}_{k}\n$$", "10.29": "\\subsection*{10.29 Prove that if a curve has $\\kappa^{\\prime}=0$ at some point, then $\\mathbf{N}^{\\prime \\prime}$ is orthogonal to $\\mathbf{T}$ at that point.}\n\nFrom $\\mathbf{N}^{\\prime}=-\\kappa \\mathbf{T}+\\tau \\mathbf{B}$, it follows that $\\mathbf{N}^{\\prime \\prime}=-\\kappa^{\\prime} \\mathbf{T}-\\kappa \\mathbf{T}^{\\prime}+\\tau^{\\prime} \\mathbf{B}+\\tau \\mathbf{B}^{\\prime}$. But $\\kappa^{\\prime}=0$; and from the Serret-Frenet formulas for $\\mathbf{T}^{\\prime}$ and $\\mathbf{B}^{\\prime}$ we obtain\n\n$$\n\\mathbf{N}^{\\prime \\prime}=-\\kappa(\\kappa \\mathbf{N})+\\tau^{\\prime} \\mathbf{B}+\\tau(-\\tau \\mathbf{N})=\\left(-\\kappa^{2}-\\tau^{2}\\right) \\mathbf{N}+\\tau^{\\prime} \\mathbf{B}\n$$\n\nAs $\\mathbf{N}^{\\prime \\prime}$ is in the plane of $\\mathbf{N}$ and $\\mathbf{B}$, it is orthogonal to $\\mathbf{T}$.", "11.1": "\\section*{VELOCITY AND ACCELERATION}\n11.1 Find the velocity and acceleration vectors and the scalars $v$ and $a$ for a particle whose equation of motion (along a twisted cubic) is $\\mathbf{x}=\\left(t, t^{2}, t^{3}\\right) \\quad(-1 \\leqq t \\leqq 1)$. Determine the extreme values of $v$ and $a$, and where they are assumed.\n\n$$\n\\begin{array}{lll}\n\\mathbf{v}=\\left(1,2 t, 3 t^{2}\\right) & \\text { and } & v=\\sqrt{1+4 t^{2}+9 t^{4}} \\\\\n\\mathbf{a}=(0,2,6 t) & \\text { and } & a=\\sqrt{4+36 t^{2}}\n\\end{array}\n$$\n\nHence $v$ and $a$ have maxima at $t= \\pm 1$, where $v=\\sqrt{14}$ and $a=\\sqrt{40}$. They have minima at $t=0$, where $v=1$ and $a=2$.", "11.2": "\\section*{PARTICLE DYNAMICS}\n11.2 A particle travels at constant speed $v$ on a curve with positive curvature. Show that its acceleration is greatest where the curvature is greatest.\n\nBy (11.8) with $\\dot{v}=0, a=\\kappa v^{2}$ or $a / \\kappa=$ const.", "11.3": "\\section*{PARTICLE DYNAMICS}\n11.3 Compute the contravariant acceleration components in a coordinate system $\\left(x^{i}\\right)$ connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ by $\\bar{x}^{1}=\\left(x^{1}\\right)^{2}, \\bar{x}^{2}=x^{2}, \\bar{x}^{3}=x^{3}$.\n\nUse (5.7):\n\n$$\nG=J^{T} J=\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n2 x^{1} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n4\\left(x^{1}\\right)^{2} & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$\n\nHence, the Christoffel symbols are given by\n\n$$\n\\Gamma_{11}^{1}=\\frac{\\partial}{\\partial x^{1}}\\left[\\frac{1}{2} \\ln 4\\left(x^{1}\\right)^{2}\\right]=\\frac{1}{x^{1}} \\quad \\text { (all others zero) }\n$$\n\nand (11.9) gives\n\n$$\na^{1}=\\frac{d^{2} x^{1}}{d t^{2}}+\\frac{1}{x^{1}}\\left(\\frac{d x^{1}}{d t}\\right)^{2} \\quad a^{2}=\\frac{d^{2} x^{2}}{d t^{2}} \\quad a^{3}=\\frac{d^{2} x^{3}}{d t^{2}}\n$$", "11.4": "\\section*{NEWTON'S SECOND LAW}\n11.4 Show that Newton's second law is consistent with Newton's first law: A particle that is not acted upon by an outside force is at rest or is in motion along a straight line at constant velocity. Assume a rectangular coordinate system.\n\n$\\mathbf{F}=\\mathbf{0}$ implies $d \\mathbf{v} / d t=\\mathbf{0}$, or $\\mathbf{v}=\\mathbf{d}$ (constant). Then,\n\n$$\n\\frac{d \\mathbf{x}}{d t}=\\mathbf{d} \\quad \\text { or } \\quad \\mathbf{x}=t \\mathbf{d}+\\mathbf{x}_{0}\n$$\n\nwhich is the parametric equation for a point (if $\\mathbf{d}=\\mathbf{0}$ ) or for a straight line (if $\\mathbf{d} \\neq \\mathbf{0}$ ), along which $v=\\|\\mathbf{d}\\|=$ const.", "11.5": "11.5 Prove the equivalence of (11.13) and (11.12b).\n\nFor simplicity, take $m=1$ in (11.13). By the chain rule and the symmetry of $\\left(g_{i j}\\right)$,\n\n$$\n\\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial v^{i}}\\right)-\\frac{\\partial T}{\\partial x^{i}}=\\frac{d}{d t}\\left(g_{i r} v^{r}\\right)-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{\\partial T}{\\partial g_{r s}} \\frac{\\partial g_{r s}}{\\partial x^{i}}+\\frac{d g_{i r}}{d t} v^{r}\n$$\n\n$$\n\\begin{aligned}\n& =g_{i r} \\frac{d v^{r}}{d t}-g_{r s i}\\left(\\frac{1}{2} v^{r} v^{s}\\right)+\\frac{\\partial g_{i r}}{\\partial x^{s}} \\frac{d x^{s}}{d t} v^{r}=g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+g_{i r s} v^{s} v^{r} \\\\\n& =g_{i r} \\frac{d v^{r}}{d t}-\\frac{1}{2} g_{r s i} v^{r} v^{s}+\\frac{1}{2} g_{s i r} v^{s} v^{r}+\\frac{1}{2} g_{i r s} v^{s} v^{r}=g_{i r} \\frac{d v^{r}}{d t}+\\Gamma_{r s i} v^{r} v^{s}\n\\end{aligned}\n$$\n\nThe final expression is exactly the right-hand side of $(11.12 b)$ (for $m=1$ ).", "11.6": "11.6 Solve (1) of Example 11.3 when the force field is of the form\n\n$$\ng(u, \\theta)=A u+h(\\theta)\n$$\n\nwhere $A$ is a constant and $h(\\theta)$ is periodic of period $2 \\pi$.\n\nWith primes denoting $\\theta$-derivatives, we must solve\n\n$$\nu^{\\prime \\prime}+u=A u+h(\\theta) \\quad \\text { or } \\quad u^{\\prime \\prime}+(1-A) u=h(\\theta)\n$$\n\nThe general solution to the homogeneous equation is\n\n$$\nu= \\begin{cases}P \\cos (\\sqrt{1-A} \\theta+\\alpha) & A<1 \\\\ \\alpha \\theta+\\beta & A=1 \\\\ Q \\exp (\\sqrt{A-1} \\theta)+R \\exp (-\\sqrt{A-1} \\theta) & A>1\\end{cases}\n$$\n\nA particular solution of the nonhomogeneous equation may be obtained in the form $u=u_{H} w$, where $u_{H}$ is any particular solution of the homogeneous equation. In fact, substitution in the differential equation yields\n\n$$\n2 u_{H}^{\\prime} w^{\\prime}+u_{H} w^{\\prime \\prime}=h \\quad \\text { or } \\quad\\left(u_{H}^{2} w^{\\prime}\\right)^{\\prime}=u_{H} h\n$$\n\nand this last equation can be solved by two quadratures:\n\n$$\nw^{\\prime}(\\theta)=\\frac{1}{u_{H}^{2}(\\theta)} \\int_{0}^{\\theta} u_{H}(\\phi) h(\\phi) d \\phi \\quad \\text { and } \\quad w(\\theta)=\\int_{0}^{\\theta} \\frac{d \\psi}{u_{H}^{2}(\\psi)} \\int_{0}^{\\psi} u_{H}(\\phi) h(\\phi) d \\phi\n$$\n\nThe integrals are easily evaluated when $h(\\phi)$ is represented as a Fourier series.", "11.7": "11.7 If $h(\\theta)=0$ in Problem 11.6, identify the orbits corresponding to (a) $A=0,(b) A=1,(c)$ $A=5 / 4$.\n\n(a) The curve $1 / r=P \\cos (\\theta+\\alpha)$, or $r \\cos (\\theta+\\alpha)=1 / P$, is a straight line (Fig. 11-1).\n\n(b) The curve $1 / r=\\alpha \\theta+\\beta$ is a hyperbolic spiral that degenerates into a circle for $\\alpha=0$.\n\n(c) The curve $1 / r=Q e^{\\theta / 2}+R e^{-\\theta / 2}$ is a complex spiral which, in the case $Q=0, R=1$, reduces to the simple logarithmic spiral $r=e^{\\theta / 2}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-169}\n\\end{center}\n\nFig. 11-1", "11.8": "\\section*{DIFFERENTIAL OPERATORS}\n11.8 Calculate the Laplacian for spherical coordinates by the tensor formula. (The calculation is very tedious by other methods.)\n\nWe have\n\n$$\nG=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{2}\n\\end{array}\\right] \\quad G^{-1}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & \\left(x^{1}\\right)^{-2} & 0 \\\\\n0 & 0 & \\left(x^{1} \\sin x^{2}\\right)^{-2}\n\\end{array}\\right]\n$$\n\nand $g=\\left(x^{1}\\right)^{4} \\sin ^{2} x^{2}$, so that in (11.15),\n\nTherefore,\n\n$$\n\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right)\\left(g^{i 1} \\frac{\\partial f}{\\partial x^{1}}+g^{i 2} \\frac{\\partial f}{\\partial x^{2}}+g^{i 3} \\frac{\\partial f}{\\partial x^{3}}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\sqrt{g} g^{1 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}} \\\\\n& \\sqrt{g} g^{2 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1}\\right)^{2}} \\frac{\\partial f}{\\partial x^{2}}=\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}} \\\\\n& \\sqrt{g} g^{3 j} \\frac{\\partial f}{\\partial x^{j}}=\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{1}{\\left(x^{1} \\sin x^{2}\\right)^{2}} \\frac{\\partial f}{\\partial x^{3}}=\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\n\\end{aligned}\n$$\n\nand so\n\n$$\n\\begin{aligned}\n& \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right]=\\frac{\\partial}{\\partial x^{1}} {\\left[\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}\\right]+\\frac{\\partial}{\\partial x^{2}}\\left[\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}\\right]+\\frac{\\partial}{\\partial x^{3}}\\left[\\left(\\csc x^{2}\\right) \\frac{\\partial f}{\\partial x^{3}}\\right] } \\\\\n&=2 x^{1}\\left(\\sin x^{2}\\right) \\frac{\\partial f}{\\partial x^{1}}+\\left(x^{1}\\right)^{2}\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{1}\\right)^{2}}+\\left(\\cos x^{2}\\right) \\frac{\\partial f}{\\partial x^{2}}+\\left(\\sin x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{2}\\right)^{2}} \\\\\n&+\\left(\\csc x^{2}\\right) \\frac{\\partial^{2} f}{\\left(\\partial x^{3}\\right)^{2}}\n\\end{aligned}\n$$\n\nIn writing the final steps we convert to $\\rho=x^{1}, \\varphi=x^{2}$, and $\\theta=x^{3}$ :\n\n$$\n\\begin{aligned}\n\\nabla^{2} f & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left[\\sqrt{g} g^{i j} \\frac{\\partial f}{\\partial x^{j}}\\right] \\\\\n& =\\frac{1}{\\rho^{2} \\sin \\varphi}\\left[(2 \\rho \\sin \\varphi) \\frac{\\partial f}{\\partial \\rho}+\\left(\\rho^{2} \\sin \\varphi\\right) \\frac{\\partial^{2} f}{\\partial \\rho^{2}}+(\\cos \\varphi) \\frac{\\partial f}{\\partial \\varphi}+(\\sin \\varphi) \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+(\\csc \\varphi) \\frac{\\partial^{2} f}{\\partial \\theta^{2}}\\right] \\\\\n& =\\frac{\\partial^{2} f}{\\partial \\rho^{2}}+\\frac{1}{\\rho^{2}} \\frac{\\partial^{2} f}{\\partial \\varphi^{2}}+\\frac{1}{\\rho^{2} \\sin ^{2} \\varphi} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{2}{\\rho} \\frac{\\partial f}{\\partial \\rho}+\\frac{\\cot \\varphi}{\\rho^{2}} \\frac{\\partial f}{\\partial \\varphi}\n\\end{aligned}\n$$", "11.9": "\\section*{DIFFERENTIAL OPERATORS}\n11.9 Calculate the divergence in spherical coordinates $(\\rho, \\varphi, \\theta)$ of $(a)$ a contravariant vector, $\\mathbf{u}=\\left(u^{i}\\right) ;(b)$ a vector specified by its physical components, $\\mathbf{u}=u_{(1)} \\mathbf{e}_{1}+u_{(2)} \\mathbf{e}_{2}+u_{(3)} \\mathbf{e}_{3}$.\n\n(a) We plug into the formula (11.14):\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}\\left(\\sqrt{g} u^{i}\\right)=\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{i} \\frac{1}{\\sqrt{g}} \\frac{\\partial}{\\partial x^{i}}(\\sqrt{g}) \\\\\n& =\\frac{\\partial u^{i}}{\\partial x^{i}}+u^{1}\\left(\\frac{2}{x^{1}}\\right)+u^{2}\\left(\\frac{\\cos x^{2}}{\\sin x^{2}}\\right)+u^{3}(0)\n\\end{aligned}\n$$\n\nThus\n\n$$\n\\operatorname{div} \\mathbf{u}=\\frac{\\partial u^{1}}{\\partial \\rho}+\\frac{\\partial u^{2}}{\\partial \\varphi}+\\frac{\\partial u^{3}}{\\partial \\theta}+\\frac{2}{\\rho} u^{1}+(\\cot \\varphi) u^{2}\n$$\n\n(b) By Example 11.2, we apply (11.4) to the contravariant vector having components\n\n$$\nu^{1}=\\frac{u_{(1)}}{1} \\quad u^{2}=\\frac{u_{(2)}}{x^{1}} \\quad u^{3}=\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\n$$\n\nHence, from (a),\n\n$$\n\\begin{aligned}\n\\operatorname{div} \\mathbf{u} & =\\frac{\\partial}{\\partial x^{1}} u_{(1)}+\\frac{\\partial}{\\partial x^{2}}\\left(\\frac{u_{(2)}}{x^{1}}\\right)+\\frac{\\partial}{\\partial x^{3}}\\left(\\frac{u_{(3)}}{x^{1} \\sin x^{2}}\\right)+u_{(1)}\\left(\\frac{2}{x^{1}}\\right)+\\frac{u_{(2)}}{x^{1}}\\left(\\cot x^{2}\\right) \\\\\n& =\\frac{\\partial u_{(\\rho)}}{\\partial \\rho}+\\frac{1}{\\rho} \\frac{\\partial u_{(\\varphi)}}{\\partial \\varphi}+\\frac{1}{\\rho \\sin \\varphi} \\frac{\\partial u_{(\\theta)}}{\\partial \\theta}+\\frac{2}{\\rho} u_{(\\rho)}+\\frac{\\cot \\varphi}{\\rho} u_{(\\varphi)}\n\\end{aligned}\n$$\n\nIt is in this last form that \"the divergence in spherical coordinates\" is generally encountered in reference books.", "11.10": "\\begin{equation*}\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{u})=\\nabla(\\boldsymbol{\\nabla} \\cdot \\mathbf{u})-\\nabla^{2} \\mathbf{u} \\tag{1}\n\\end{equation*}\n\n\n(\"curl curl equals grad div minus del-square\").\n\nBoth sides of (1) are (cartesian) vectors; we shall show that they are componentwise equal.\n\nBy (11.7), the $i$ th component of curl $\\mathbf{u}$ is $e_{i j k}\\left(\\partial u^{k} / \\partial x^{j}\\right)$. Therefore, the $i$ th component of curl (curl $\\mathbf{u}$ ) is [use (3.23)]:\n\n$$\n\\begin{aligned}\ne_{i r s} \\frac{\\partial}{\\partial x^{r}}\\left(e_{s j k} \\frac{\\partial u^{k}}{\\partial x^{i}}\\right) & =e_{i r s} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=e_{s i r} e_{s j k} \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}} \\\\\n& =\\left(\\delta_{i j} \\delta_{r k}-\\delta_{i k} \\delta_{r j}\\right) \\frac{\\partial^{2} u^{k}}{\\partial x^{r} \\partial x^{j}}=\\frac{\\partial^{2} u^{r}}{\\partial x^{r} \\partial x^{i}}-\\frac{\\partial^{2} u^{i}}{\\partial x^{r} \\partial x^{r}} \\\\\n& \\equiv \\frac{\\partial}{\\partial x^{i}}(\\operatorname{div} \\mathbf{u})-\\nabla^{2} u^{i}\n\\end{aligned}\n$$\n\nThe first term on the right is recognized as the $i$ th component of grad (div $\\mathbf{u}$ ), and the second term is (by definition) the $i$ th component of the Laplacian of the vector $\\mathbf{u}$. QED", "11.11": "```\n11.11 Prove that the array represented in rectangular coordinates $\\left(x^{i}\\right)$ by\n\n$$\n\\operatorname{curl} \\mathbf{u}=\\left(e_{i j k} \\frac{\\partial u^{k}}{\\partial x^{j}}\\right)\n$$\n\nis a direct cartesian tensor.\n\nIt suffices to show that $\\left(e_{i j k}\\right)$ is a direct cartesian tensor, since $\\left(\\partial u^{i} / \\partial x^{j}\\right)$ is known to be a (direct) cartesian tensor. Therefore, given the orthogonal transformation $\\bar{x}^{i}=a_{j}^{i} x^{j}$, with $\\left|a_{j}^{i}\\right|=+1$, define the $3^{3}=27$ quantities\n\n$$\n\\tau_{i j k} \\equiv e_{r s t} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}} \\frac{\\partial x^{t}}{\\partial \\bar{x}^{k}}=e_{r s t} a_{r}^{i} a_{s}^{j} a_{t}^{k}\n$$\n\nWe observe that:\n\n(i) $\\tau_{i j k}=0$ when two subscripts have the same value; e.g.,\n\n\n\\begin{align*}\n\\tau_{i 22}=e_{r s t} a_{r}^{i} a_{s}^{2} a_{t}^{2} & =-e_{r t s} a_{r}^{i} a_{s}^{2} a_{t}^{2}=-e_{r s t} a_{r}^{i} a_{t}^{2} a_{s}^{2}=-\\tau_{i 22} \\\\\n\\tau_{123} & =e_{r s t} a_{r}^{1} a_{s}^{2} a_{t}^{3}=\\left|a_{j}^{i}\\right|=+1 \\tag{ii}\n\\end{align*}\n\n\n(iii) $\\tau_{i j k}$ changes sign when any two subscripts are interchanged; e.g.\n\n$$\n\\tau_{k j i}=e_{r s t} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-e_{t s r} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-\\tau_{i j k}\n$$\n\nBut these three properties identify $\\tau_{i j k}$ with $\\bar{e}_{i j k}$, and the proof is complete.\n```", "11.12": "\\section*{DIFFERENTIAL OPERATORS}\n11.12 Show that in a vacuum with zero charge $(\\rho=0)$, the electric field $\\mathbf{E}$ satisfies the vector wave equation\n\n$$\n\\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}=c^{2} \\nabla^{2} \\mathbf{E}\n$$\n\nFrom Maxwell's equations (11.19), along with the identity (11.23),\n\n$$\n\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\frac{1}{c} \\frac{\\partial}{\\partial t}(\\boldsymbol{\\nabla} \\times \\mathbf{H})=-\\frac{1}{c}\\left(\\frac{1}{c} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\\right)=-\\frac{1}{c^{2}} \\frac{\\partial^{2} \\mathbf{E}}{\\partial t^{2}}\n$$\n\nBut $\\boldsymbol{\\nabla} \\cdot \\mathbf{E}=0$ and Problem 11.10 imply $\\boldsymbol{\\nabla} \\times(\\boldsymbol{\\nabla} \\times \\mathbf{E})=-\\nabla^{2} \\mathbf{E}$, and the wave equation follows.", "12.1": "\\subsection*{12.1 Calculate $\\varepsilon$ and $\\Delta s$ for the event pairs: (a) $E_{1}(5,1,-2,0)$ and $E_{2}(0,3,1,-3),(b)$ $E_{1}(5,1,3,3)$ and $E_{2}(2,-1,1,1),(c) E_{1}(7,2,4,4)$ and $E_{2}(4,1,2,6),(d) E_{1} \\equiv$ flash of light in Chicago at 7 p.m. and $E_{2} \\equiv$ flash of light in St. Louis ( 400 miles away) at 7.00000061 p.m. (e) Determine the interval type in each case.}\n\n(a) $\\varepsilon(\\Delta s)^{2}=5^{2}-(-2)^{2}-(-3)^{2}-3^{2}=25-4-9-9=3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=1$.\n\n(b) $\\varepsilon(\\Delta s)^{2}=9-4-4-4=-3$, or $\\Delta s=\\sqrt{3}$ and $\\varepsilon=-1$.\n\n(c) $\\varepsilon(\\Delta s)^{2}=9-1-4-4=0$, or $\\Delta s=0$ and $\\varepsilon=1$.\n\n(d) With $c=186300 \\mathrm{mi} / \\mathrm{sec}, \\varepsilon(\\Delta s)^{2}=(0.002196 c)^{2}-(400)^{2} \\approx 7375 \\mathrm{mi}^{2}$, or $\\Delta s \\approx 85.8 \\mathrm{mi}$ and $\\varepsilon=1$.\n\n(e) Timelike, spacelike, lightlike, and timelike, respectively.", "12.2": "\\subsection*{12.2 Show that (a) simultaneous events have a spacelike interval; (b) copositional events have a timelike interval; $(c)$ the interval between two light flashes is lightlike if they are simultaneous to an observer who is present at the site of one of the flashes.}\n\n\\begin{gather*}\n\\varepsilon(\\Delta s)^{2}=0^{2}-\\left(\\Delta x^{1}\\right)^{2}-\\left(\\Delta x^{2}\\right)^{2}-\\left(\\Delta x^{3}\\right)^{2}<0  \\tag{a}\\\\\n\\varepsilon(\\Delta s)^{2}=\\left(\\Delta x^{0}\\right)^{2}-0>0 \\tag{b}\n\\end{gather*}\n\n$$\n\\varepsilon(\\Delta s)-(\\Delta x)-0<0\n$$\n\n(c) Let the observer measure the proximate flash as $E_{1}(0,0,0,0)$. The distant flash $E_{2}\\left(c \\Delta t, \\Delta x^{1}, \\Delta x^{2}, \\Delta x^{3}\\right)$ will be registered simultaneously, at $x^{0}=0$, if\n\n$$\n\\Delta t=-\\frac{\\sqrt{\\left(\\Delta x^{1}\\right)^{2}+\\left(\\Delta x^{2}\\right)^{2}+\\left(\\Delta x^{3}\\right)^{2}}}{c}\n$$\n\nBut then $\\varepsilon(\\Delta s)^{2}=0$ and the interval is lightlike. (Note that the (negative) time coordinate of $E_{2}$ is calculated, not measured.)", "12.3": "\\section*{THE LORENTZ GROUP}\n12.3 Prove the following lemma involving the metric of SR, $g_{i j}$, as given by (12.6).\n\nLemma 12.4: If $C=\\left(c_{i j}\\right)$ is a symmetric $4 \\times 4$ matrix such that $c_{i j} x^{i} x^{j}=0$ for all $\\left(x^{i}\\right)$ such that $g_{i j} x^{i} x^{j}=0$, there exists a fixed real number $\\lambda$ for which $c_{i j}=\\lambda g_{i j}(C=\\lambda G)$.\n\nObserve that the vector $(1, \\pm 1,0,0)$ satisfies $g_{i j} x^{i} x^{j}=0$. Hence, substituting these components into the equation $c_{i j} x^{i} x^{j}=0$ yields\n\n$$\nc_{00} \\pm c_{01} \\pm c_{10}+c_{11}=0 \\quad \\text { or } \\quad c_{00}+c_{11}=0=c_{01}=c_{10}\n$$\n\n(by symmetry of $C$ ). Similarly, using the vectors $(1,0, \\pm 1,0)$ and $(1,0,0, \\pm 1)$, we get\n\n$$\nc_{00}=-c_{11}=-c_{22}=-c_{33}=\\lambda \\quad c_{i j}=0 \\quad(i=0 \\text { or } j=0)\n$$\n\nFinally, employing the vectors $(\\sqrt{2}, 1,1,0),(\\sqrt{2}, 1,0,1)$, and $(\\sqrt{2}, 0,1,1)$, we obtain $c_{12}=c_{13}=c_{23}=$ 0 .", "12.4": "\\subsection*{12.4 Establish the transformation (12.7) between inertial frames under the postulates for SR.}\n\nFrom (12.6) and (12.5),\n\n$$\ng_{i j} x^{i} x^{j}=0=g_{i j} \\bar{x}^{i} \\bar{x}^{j}=g_{i j}\\left(a_{r}^{i} x^{r}\\right)\\left(a_{s}^{j} x^{s}\\right)=g_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}\n$$\n\nthat is,\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}=0 \\quad \\text { whenever } \\quad g_{i j} x^{i} x^{j}=0 \\tag{1}\n\\end{equation*}\n\nNow apply Lemma 12.4 to (1), with $g_{r s} a_{i}^{r} a_{j}^{s}=c_{i j}$, where $C=\\left(c_{i j}\\right)=A^{T} G A$ is symmetric. We obtain\n\n\\begin{equation*}\ng_{r s} a_{i}^{r} a_{j}^{s}=\\lambda g_{i j} \\quad \\text { or } \\quad A^{T} G A=\\lambda G \\tag{2}\n\\end{equation*}\n\nIt remains to show that $\\lambda=1$. Since $G^{2}=I$, multiplication of (2) by the matrix $\\lambda^{-1} G$ gives $\\left(G\\left(\\lambda^{-1} A^{T}\\right) G\\right) A=I$, which shows that the inverse of $A$ is\n\n\\[\nB=\\frac{1}{\\lambda} G A^{T} G=\\left[\\begin{array}{rrrr}\na_{0}^{0} / \\lambda & -a_{0}^{1} / \\lambda & -a_{0}^{2} / \\lambda & -a_{0}^{3} / \\lambda  \\tag{3}\\\\\n-a_{1}^{0} / \\lambda & a_{1}^{1} / \\lambda & a_{1}^{2} / \\lambda & a_{1}^{3} / \\lambda \\\\\n-a_{2}^{0} / \\lambda & a_{2}^{1} / \\lambda & a_{2}^{2} / \\lambda & a_{2}^{3} / \\lambda \\\\\n-a_{3}^{0} / \\lambda & a_{3}^{1} / \\lambda & a_{3}^{2} / \\lambda & a_{3}^{3} / \\lambda\n\\end{array}\\right] \\equiv\\left[b_{j}^{i}\\right]_{44}\n\\]\n\nIn particular, $b_{0}^{0}=a_{0}^{0} / \\lambda$. Now since observers $O$ and $\\bar{O}$ are receding from each other at constant velocity $v$ and are using identical measuring devices, it is clear that each views the other in the same way. It follows that $a_{0}^{0}=b_{0}^{0}$ and $\\lambda=a_{0}^{0} / b_{0}^{0}=1$ (see Problem 12.5).", "12.5": "\\subsection*{12.5 With reference to Problem 12.4, give a \"thought-experiment\" which leads to the conclusion that $a_{0}^{0}=b_{0}^{0}$.}\n\nConsider the motion of $O$ in $\\bar{O}$ 's frame: Transform the point $(c t, 0,0,0)$ under $\\mathscr{T}$ to get\n\n$$\n\\bar{x}^{0}=c \\bar{t}=a_{0}^{0} c t \\quad \\text { or } \\quad \\bar{t}=a_{0}^{0} t\n$$\n\nThus, 1 second on $O$ 's clock is $a_{0}^{0}$ seconds on $\\bar{O}$ 's; reciprocally, 1 second on $\\bar{O}$ 's clock is $b_{0}^{0}$ seconds on $O$ 's. Thus, $a_{0}^{0}=b_{0}^{0}$.", "12.6": "\\subsection*{12.6 Prove Theorem 12.1 from (12.7). [Note that Problem 12.4 did not make use of Theorem 12.1, so the proof will be logically correct.]}\n\nBy (12.7), $\\left(g_{i j}\\right)$ is a covariant tensor under Lorentz transformations, so that $g_{i j} \\Delta x^{i} \\Delta x^{j}$ is an invariant (under Lorentz transformations).", "12.7": "\\subsection*{12.7 Verify that the following matrix is Lorentz:}\n\n$$\n\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\frac{\\sqrt{6}}{2} & \\frac{1}{2} & \\frac{1}{2} \\\\\n1 & \\frac{\\sqrt{6}}{2} & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n0 & 0 & -\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n\\end{array}\\right]\n$$\n\nWe verify directly the conditions $(12.7 c)$ :\n\n$$\n\\begin{array}{cc}\n(\\sqrt{3})^{2}-1^{2}-1^{2}-0^{2}=3-2=1 & (\\sqrt{2})^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-\\left(\\frac{\\sqrt{6}}{2}\\right)^{2}-0^{2}=2-\\frac{3}{2}-\\frac{3}{2}=-1 \\\\\n0^{2}-\\left(\\frac{1}{2}\\right)^{2}-\\left(-\\frac{1}{2}\\right)^{2}-\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)^{2}=-1 & (\\sqrt{3})(\\sqrt{2})-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-(1)\\left(\\frac{\\sqrt{6}}{2}\\right)-0=0 \\\\\n(\\sqrt{3})(0)-(1)\\left(\\frac{1}{2}\\right)-(1)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{\\sqrt{2}}{2}\\right)=0 & (\\sqrt{2})(0)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(\\frac{1}{2}\\right)-\\left(\\frac{\\sqrt{6}}{2}\\right)\\left(-\\frac{1}{2}\\right)-(0)\\left( \\pm \\frac{1}{2}\\right)=0 \\\\\n0-\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(-\\frac{1}{2}\\right)+\\left(\\frac{\\sqrt{2}}{2}\\right)\\left(\\frac{\\sqrt{2}}{2}\\right)=0-\\frac{1}{4}-\\frac{1}{4}+\\frac{1}{2}=0\n\\end{array}\n$$", "12.8": "\\subsection*{12.8 Show that a matrix $A$ which preserves $\\mathbf{x}^{T} G \\mathbf{x}=0$ necessarily preserves $\\mathbf{x}^{T} G \\mathbf{x}=q$.}\n\nThis is really Problem 12.6 in another guise. By Problem 12.4, $A$ must satisfy $A^{T} G A=G$. But then\n\n$$\n(A \\mathbf{x})^{T} G(A \\mathbf{x})=\\mathbf{x}^{T}\\left(A^{T} G A\\right) \\mathbf{x}=\\mathbf{x}^{T} G \\mathbf{x}=q\n$$", "12.9": "\\subsection*{12.9 (a) Exhibit the inverse, $B$, of a given Lorentz matrix, $A$. (b) If we define a matrix $A$ to be pseudo-orthogonal when there exists a matrix $J$ whose square is the identity and $A^{T} J A=J$, show that all Lorentz matrices are pseudo-orthogonal.}\n\n(a) Set $\\lambda=1$ in (3) of Problem 12.4.\n\n(b) If $A$ is a Lorentz matrix, then $G$ clearly fills the role of $J$ in the definition of pseudo-orthogonal matrix.", "12.10": "\\subsection*{12.10 Prove that the Lorentz matrices compose a group under matrix multiplication.}\n\nWe are required to show that (a) the product of two Lorentz matrices is Lorentz, $(b)$ the inverse of a Lorentz matrix is Lorentz.\n\n(a)\n\n$$\n(P Q)^{T} G(P Q)=Q^{T}\\left(P^{T} G P\\right) Q=Q^{T} G Q=G\n$$\n\n(b) Using Problem 12.4 with $\\lambda=1, B=A^{-1}=G A^{T} G$, and\n\n$$\nB^{T} G B=\\left(G A^{T} G\\right)^{T} G B=G A G^{2} B=G A B=G\n$$", "12.11": "\\section*{SIMPLE LORENTZ MATRICES}\n12.11 Derive the simple form (12.9) of the transformation equations for SR by considering how observers $O$ and $\\bar{O}$ will view events occurring on a circular cylinder about their common $x$-axis.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-186}\n\\end{center}\n\nFig. 12-4\n\nAt any time $t$, let $E_{1}$ and $E_{2}$ be two events taking place at the points of space $(q, 1,0)$ and $(q, 0,1)$, respectively, which lie on a unit cylinder about $O$ 's $x$-axis (Fig. 12-4). Thus, with $p=c t$, we have space-time coordinates $E_{1}(p, q, 1,0)$ and $E_{2}(p, q, 0,1)$. Since the axes of $O$ are not turning with respect to $\\bar{O}$ 's, these two events will be viewed by observer $\\bar{O}$ as $E_{1}(\\bar{p}, \\bar{q}, 1,0)$ and $E_{2}\\left(\\bar{p}^{*}, \\bar{q}^{*}, 0,1\\right)$, respectively. The transformation equations (12.5) give:\n\n$$\n\\text { (I) }\\left\\{\\begin{array} { l } \n{ \\overline { p } = a _ { 0 } ^ { 0 } p + a _ { 1 } ^ { 0 } q + a _ { 2 } ^ { 0 } } \\\\\n{ \\overline { q } = a _ { 0 } ^ { 1 } p + a _ { 1 } ^ { 1 } q + a _ { 2 } ^ { 1 } } \\\\\n{ 1 = a _ { 0 } ^ { 2 } p + a _ { 1 } ^ { 2 } q + a _ { 2 } ^ { 2 } } \\\\\n{ 0 = a _ { 0 } ^ { 3 } p + a _ { 1 } ^ { 3 } q + a _ { 2 } ^ { 3 } }\n\\end{array} \\quad \\text { (II) } \\left\\{\\begin{array}{l}\n\\bar{p}^{*}=a_{0}^{0} p+a_{1}^{0} q+a_{3}^{0} \\\\\n\\bar{q}^{*}=a_{0}^{1} p+a_{1}^{1} q+a_{3}^{1} \\\\\n0=a_{0}^{2} p+a_{1}^{2} q+a_{3}^{2} \\\\\n1=a_{0}^{3} p+a_{2}^{3} q+a_{3}^{3}\n\\end{array}\\right.\\right.\n$$\n\nObserving just the last equation of (I) and the third equation of (II), we may, since $p$ and $q$ are arbitrary, take $p=q=0$, then $p=1, q=0$, and $p=0, q=1$. It follows that all six of the coefficients vanish: $a_{0}^{2}=a_{1}^{2}=a_{3}^{2}=a_{0}^{3}=a_{1}^{3}=a_{2}^{3}=0$. Using the third equation of (I) and the last equation of (II), we find that $a_{2}^{2}=a_{3}^{3}=1$. It follows that the last two equations of $\\mathscr{T}$ reduce to $\\bar{x}^{2}=x^{2}$ and $\\bar{x}^{3}=x^{3}$. Now to concentrate on the first two: If $p=q=0$, then event $E_{1}$ is $(0,0,1,0)$-occurring when $t=\\bar{t}=0$ at $x^{1}=0$, the instant when $\\bar{x}^{1}=0$. That is, $\\bar{p}=\\bar{q}=0$, with the result $a_{2}^{0}=a_{2}^{1}=0$. Similarly, using $E_{2}$, $p=q=0$ implies $\\bar{p}^{*}=\\bar{q}^{*}=0$ and $a_{3}^{0}=a_{3}^{1}=0$.", "12.12": "\\subsection*{12.12 Consider event $E_{1}$, a lightning flash at the point $(v, 0,0)$ at time $t=1 \\mathrm{~s}$ in $O$ 's frame, and event $E_{2}$, a lightning flash at $(-v, 0,0)$ at time $\\bar{t}=1 \\mathrm{~s}$ in $\\bar{O}$ 's frame. By determining the corresponding events in the opposing frames of reference, deduce (12.11).}\n\nSince at $t=1$ observer $\\bar{O}$ has reached the point $(v, 0,0)$, the lightning strikes $\\bar{O}$ 's origin at time $\\bar{t}$. Hence, $E_{1}$ has coordinates $(c, v, 0,0)$ in $O$ and $(c \\bar{t}, 0,0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ we obtain\n\n$$\nc \\bar{t}=a c+b v \\quad 0=d c+e v\n$$\n\nThe second equation gives $d=-\\beta e$.\n\nSince $O$ has progressed backwards to the point $(-v, 0,0)$ in $\\bar{O}$ at the time $\\bar{t}=1$ at which $E_{2}$ occurs, this event has coordinates $(c t, 0,0,0)$ in $O$ and $(c,-v, 0,0)$ in $\\bar{O}$. Substituting these into $\\mathscr{T}$ yields\n\n$$\nc=a c t+b(0) \\quad-v=d c t+e(0)\n$$\n\nwhich upon division give $d=-\\beta a$. Hence, $a=e$.", "12.13": "\\subsection*{12.13 Show that a $4 \\times 4$ matrix is both Lorentz and orthogonal if and only if it has the form}\n\n\\[\nR=\\left[\\begin{array}{rrrr} \n\\pm 1 & 0 & 0 & 0  \\tag{1}\\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\n\\]\n\nwhere the $3 \\times 3$ matrix $\\left[\\begin{array}{lll}\\mathbf{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal.\n\nA Lorentz matrix $A=\\left(a_{j}^{i}\\right)$ is also orthogonal if and only if its inverse $B$, as obtained in Problem 12.4 (with $\\lambda=1$ ), is equal to $A^{T}$ and is itself orthogonal. This observation immediately yields the form (1).", "12.14": "\\subsection*{12.14 Prove Theorem 12.2.}\nSince $\\|\\mathbf{r}\\|^{2}=b^{-2}\\left[\\left(a_{1}^{0}\\right)^{2}+\\left(a_{2}^{0}\\right)^{2}+\\left(a_{3}^{0}\\right)^{2}\\right]=b^{-2}\\left[\\left(a_{0}^{0}\\right)^{2}-1\\right]=1$ (using Problem 12.36), the matrix $\\left[\\begin{array}{lll}\\mathrm{r} & \\mathbf{s} & \\mathbf{t}\\end{array}\\right]$ is orthogonal and $R_{2}^{T}$ has the form of the matrix in Problem 12.13, making it Lorentz and orthogonal. It follows that $R_{2}$ is orthogonal (and Lorentz), with $R_{2}^{-1}=R_{2}^{T}$; hence, $L=R_{1} L^{*} R_{2}$.\n\nNow, as the product of Lorentz matrices, $R_{1}$ is Lorentz; to show it is orthogonal, consider $L R_{2}^{T}\\left(L^{*}\\right)^{-1}$, which may be written as\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{llll}\na_{0} & b_{0} & c_{0} & d_{0} \\\\\na_{1} & b_{1} & c_{1} & d_{1} \\\\\na_{2} & b_{2} & c_{2} & d_{2} \\\\\na_{3} & b_{3} & c_{3} & d_{3}\n\\end{array}\\right]\\left[\\begin{array}{llll}\n1 & 0 & 0 & 0 \\\\\n0 & r_{1} & s_{1} & t_{1} \\\\\n0 & r_{2} & s_{2} & t_{2} \\\\\n0 & r_{3} & s_{3} & t_{3}\n\\end{array}\\right]\\left[\\begin{array}{rrrr}\na & -b & 0 & 0 \\\\\n-b & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]}\n\\end{aligned}\n$$\n\n[The omitted rows have the form $\\left(a_{i}, b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}, b_{i} s_{1}+c_{i} s_{2}+d_{i} s_{3}, b_{i} t_{1}+c_{i} t_{2}+d_{i} t_{3}\\right.$ ), with $i=1,2,3$.] We first concentrate on proving that the top row and first column of this product are $( \\pm 1,0,0,0)$. The 00 -element of the product is\n\n$$\na_{0} a+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right)(-b)=\\varepsilon a_{0}^{2}+\\frac{\\varepsilon}{b}\\left(b_{0}^{2}+c_{0}^{2}+d_{0}^{2}\\right)(-b)=\\varepsilon\\left(a_{0}^{2}-b_{0}^{2}-c_{0}^{2}-d_{0}^{2}\\right)=\\varepsilon\n$$\n\nagain using the fact that the transpose of a Lorentz matrix is Lorentz. The next element in the top row of the product is\n\n$$\n-a_{0} b+\\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\\right) a=-a_{0} b+\\frac{b}{\\varepsilon}\\left(r^{2}\\right) \\varepsilon a_{0}=-a_{0} b+b a_{0}=0\n$$\n\nFor the third and fourth elements,\n\n$$\nb_{0} s_{1}+c_{0} s_{2}+d_{0} s_{3}=\\frac{b}{\\varepsilon} \\mathbf{r s}=0 \\quad \\text { and } \\quad b_{0} t_{1}+c_{0} t_{2}+d_{0} t_{3}=\\frac{b}{\\varepsilon} \\mathbf{r t}=0\n$$\n\nNow for the first column of the product; its elements, beginning with the second, are (for $i=1,2,3$ )\n\n$$\na_{i} a+\\left(b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}\\right)(-b)=\\varepsilon a_{i} a_{0}-\\varepsilon\\left(b_{i} b_{0}+c_{i} c_{0}+d_{i} d_{0}\\right)=0\n$$\n\nHence, the product matrix becomes\n\n$$\nR_{1}=\\left[\\begin{array}{llll}\n\\varepsilon & 0 & 0 & 0 \\\\\n0 & & & \\\\\n0 & & R & \\\\\n0 & & &\n\\end{array}\\right]\n$$\n\nand the $3 \\times 3$ matrix $R$ must be orthogonal, since $R_{1}$ is Lorentz.", "12.15": "\\subsection*{12.15 Apply Theorem 12.2 to the Lorentz matrix of Problem 12.7, and demonstrate the physical significance of this matrix by computing the velocity $v$ between the two observers involved.}\n\nWe proceed to calculate $a, b$, and the vectors $\\mathbf{r}, \\mathbf{s}$, and $\\mathbf{t}$ :\n\n$$\na=\\sqrt{3} \\quad \\varepsilon=1 \\quad b=-\\sqrt{3-1}=-\\sqrt{2} \\quad \\mathbf{r}=-\\frac{1}{\\sqrt{2}}(\\sqrt{2}, 0,0)=(-1,0,0)\n$$\n\nHence, we may take $\\mathbf{s}=(0,1,0)$ and $\\mathbf{t}=(0,0,1)$, yielding\n\n$$\nR_{2}=\\left[\\begin{array}{rrrr}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nand\n\n$$\n\\begin{aligned}\nR_{1} & =\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n1 & \\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & \\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cccc}\n\\sqrt{3} & -\\sqrt{2} & 0 & 0 \\\\\n1 & -\\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\\\\n1 & -\\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sqrt{3} & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & \\sqrt{3} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & -\\sqrt{2} / 2 & 1 / 2 & 1 / 2 \\\\\n0 & -\\sqrt{2} / 2 & -1 / 2 & -1 / 2 \\\\\n0 & 0 & -\\sqrt{2} / 2 & \\sqrt{2} / 2\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nBy Corollary 12.3,\n\n$$\nv=c \\sqrt{1-(\\sqrt{3})^{-2}}=\\sqrt{\\frac{2}{3}} c\n$$", "12.16": "\\section*{LENGTH CONTRACTION, TIME DILATION}\n12.16 A pole-vaulter runs at the rate $(\\sqrt{3} / 2) c$ (in $\\mathrm{m} / \\mathrm{s}$ ) and carries a pole that is $20 \\mathrm{~m}$ long in his reference frame [the rest length of the pole is $20 \\mathrm{~m}$ ]. He approaches a barn that is open at both ends and is $10 \\mathrm{~m}$ long, as measured by a ground observer. To the ground observer, will the pole fit inside the barn? What is the pole-vaulter's conclusion?\n\nTo the ground observer, the pole undergoes length contraction with the factor $\\sqrt{1-\\beta^{2}}$, where $\\beta=\\sqrt{3} / 2$. Hence, the length of the pole in the frame of the ground observer is\n\n$$\n20 \\sqrt{1-(\\sqrt{3} / 2)^{2}}=10 \\mathrm{~m}\n$$\n\nand so, for her, the pole exactly fits inside the barn (instantaneously). To the runner, however, the barn is $10(1 / 2)=5 \\mathrm{~m}$ long, so that the $20-\\mathrm{m}$ pole does not fit.\n\nThis example shows that order relations are not preserved under the Lorentz transformation.", "12.17": "\\subsection*{12.17 (the Twin Paradox) One of a pair of twins embarks on a journey into outer space, taking one year (earth time) to accelerate to $(3 / 4) c$, then spends the next 20 years cruising to reach a galaxy 15 light-years away. An additional year is spend in decelerating in order to explore one of its solar systems. After one year of exploration $(\\beta=0)$, the twin returns to earth by the same schedule-one year of acceleration, 20 years of cruising, and one year of deceleration. Estimate the difference in the ages of the twins after the journey has ended.}\n\nIn order to apply SR, replace the four periods of acceleration or deceleration by four periods of uniform motion at speed (3/8)c (the time-average speed under constant acceleration). These account for 4 years by the earth clock; but to the space twin, who measures proper (shortest) time intervals, the time lapse is $(\\beta=3 / 8)$\n\n$$\n4 \\sqrt{1-(3 / 8)^{2}} \\approx 3.71 \\text { years }\n$$\n\nSimilarly, the 40 earth-years of cruising at $\\beta=3 / 4$ corresponds to a proper-time interval of\n\n$$\n40 \\sqrt{1-(3 / 4)^{2}} \\approx 26.46 \\text { years }\n$$\n\nThus, the space twin has aged $3.71+26.46+1 \\approx 31$ years while the earth twin has aged $4+40+1=$ 45 years.\n\nThe space twin returns biologically younger by some 14 years. While the accelerations and decelerations between the two twins were reciprocal, the forces in the situation acted on the space twin alone.", "12.18": "\\subsection*{12.18 Prove the basic integrity of (12.16) by solving algebraically for $v_{2}$ as a function of $v_{1}$ and $v_{3}$ to verify that $v_{2}$ follows the correct format for composition of velocities.}\n\nSolving,\n\n$$\nv_{2}=\\frac{-v_{1}+v_{3}}{1-v_{1} v_{3} / c^{2}}\n$$\n\nwhich is precisely (12.16) under the substitution $\\left(v_{1}, v_{2}, v_{3}\\right) \\rightarrow\\left(-v_{1}, v_{3}, v_{2}\\right)$.", "12.19": "\\subsection*{12.19 A light source at $O$ sends a spherical wavefront (Fig. 12-5(a)) advancing in all directions at velocity $c$; it reaches the ends of a diameter $A B$ centered at $O$ simultaneously, as determined by $O$. But as far as $\\bar{O}$ is concerned, the spherical wave, centered at $\\bar{O}$, moves with him (invariance of the light cone) and therefore reaches point $B$ before it reaches point $A$. Calculate the time difference on $\\bar{O}$ 's clock for these two events (light reaching $B$ and light reaching $A$ ) if $\\beta=1 / 2$ and if $A B=6 \\mathrm{~m}$.}\n\nSince $A B=6 \\mathrm{~m}$ and $O$ is the midpoint of segment $A B, O$ assigns spatial coordinates $B(3,0,0)$ and $A(-3,0,0)$ to the endpoints. It takes $3 / c$ seconds for light to reach $A$ and $B$, so $O$ calculates the time coordinate as $x^{0}=c(3 / c)=3 \\mathrm{~m}$. The space-time coordinates of the two events are thus\n\n$$\nE_{B}(3,3,0,0) \\quad \\text { and } \\quad E_{A}(3,-3,0,0)\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189(1)}\n\\end{center}\n\n(b)\n\nFig. 12-5\n\nSubstitute these values and $\\beta=1 / 2$ into the first equation (12.13) to obtain $\\bar{t}_{B}=\\sqrt{3} / c, \\bar{t}_{A}=3 \\sqrt{3} / c$. Hence, $\\Delta \\bar{t}=2 \\sqrt{3} / c$ (in s), while $\\Delta t=0$.\n\nIt is seen that simultaneity is not an invariant of Lorentz transformations.", "12.20": "\\subsection*{12.20 Derive the composition of velocities formula, (12.16).}\n\nAccording to Section 12.4, we must have $v_{i}=-b_{i} c / a_{i}$ for $i=1,2,3$. Composing the simple Lorentz transformations, we have\n\n$$\n\\left[\\begin{array}{cccc}\na_{1} & b_{1} & 0 & 0 \\\\\nb_{1} & a_{1} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{cccc}\na_{2} & b_{2} & 0 & 0 \\\\\nb_{2} & a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\na_{1} a_{2}+b_{1} b_{2} & a_{1} b_{2}+a_{2} b_{1} & 0 & 0 \\\\\na_{2} b_{1}+a_{1} b_{2} & b_{1} b_{2}+a_{1} a_{2} & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$\n\nwhence $a_{3}=a_{1} a_{2}+b_{1} b_{2}, b_{3}=a_{1} b_{2}+a_{2} b_{1}$, and\n\n$$\nv_{3}=-\\frac{\\left(a_{1} b_{2}+a_{2} b_{1}\\right) c}{a_{1} a_{2}+b_{1} b_{2}}=\\frac{\\gamma\\left(-v+v_{x}\\right)}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}}\n$$\n\nThus, observer $O$ calculates the net momentum vector of the system as follows, using the rest mass $m$ of $B_{1}$ for $m_{1}$ and the \"perceived mass\" $\\hat{m}$ of $B_{2}$ for $m_{2}$ :\n\n$$\n\\text { before impact } \\quad \\begin{aligned}\nm_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m_{1}(\\mathbf{0})+m_{2}(v \\mathbf{i})=\\hat{m} v \\mathbf{i} \\\\\n\\text { after impact } \\quad m_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m(\\varepsilon \\mathbf{i}+\\delta \\mathbf{j})+\\hat{m}\\left[\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\\right] \\\\\n& =\\left(m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(m \\delta-\\hat{m} \\frac{\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n\\end{aligned}\n$$\n\nSince $O$ is using the universal laws of physics as they apply to his frame (Postulate 1 of SR), the two momentum vectors above must be the same. Hence,\n\n$$\n\\hat{m} v=m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}} \\quad \\text { and } \\quad 0=m-\\hat{m} \\frac{\\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\n$$\n\n(after division by $\\delta$ ). Now take the limit as $\\varepsilon \\rightarrow 0$ :\n\n$$\n\\hat{m} v=\\hat{m} v \\quad \\text { and } \\quad 0=m-\\hat{m} \\sqrt{1-\\beta^{2}}\n$$\n\nThe right-hand equation is the connection between $m$ and $\\hat{m}$.", "12.21": "\\subsection*{12.21 A physicist wants to compose two equal velocities $v=v_{1}=v_{2}$ to produce a resultant velocity that is $90 \\%$ of the velocity of light. What velocity must he use?}\n\nFrom (12.16),\n\n$$\n0.90 c=\\frac{2 v}{1+v^{2} / c^{2}} \\quad \\text { or } \\quad 0.90=\\frac{2 \\beta}{1+\\beta^{2}}\n$$\n\nSolving the quadratic, $\\beta \\approx 0.627$ (as compared to the Newtonian value 0.45 ).", "12.22": "\\subsection*{12.22 Establish (12.19) and (12.20), the Lorentz transformations of velocity and acceleration, that define how $\\bar{O}$ tracks the motion of a particle in $O$ 's frame.}\n\nTo simplify notation, let $\\gamma \\equiv\\left(1-\\beta^{2}\\right)^{-1 / 2}$. Then $\\mathscr{T}$ is\n\n$$\nc \\bar{t}=\\gamma(c t-\\beta x) \\quad \\bar{x}=\\gamma(-\\beta c t+x) \\quad \\bar{y}=y \\quad \\bar{z}=z\n$$\n\nDifferentiate the first equation with respect to $\\bar{t}$ and use the chain rule:\n\n$$\nc=\\gamma\\left(c-\\beta v_{x}\\right) \\frac{d t}{d \\bar{t}} \\quad \\text { or } \\quad \\frac{d t}{d \\bar{t}}=\\frac{1}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}\n$$\n\nNow differentiate the last three equations:\n\n$$\n\\begin{aligned}\n& \\bar{v}_{x}=\\gamma\\left(-\\beta c+v_{x}\\right) \\frac{d t}{d \\bar{t}}=\\frac{\\gamma\\left(-v+v_{x}\\right)}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{y}=v_{y} \\frac{d t}{d \\bar{t}}=\\frac{v_{y}}{\\gamma\\left(1-v v_{x} / c^{2}\\right)}=\\frac{v_{y} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}} \\\\\n& \\bar{v}_{z}=v_{z} \\frac{d t}{d \\bar{t}}=\\frac{v_{z} \\sqrt{1-\\beta^{2}}}{1-v_{x} v / c^{2}}\n\\end{aligned}\n$$\n\nBy differentiation of the velocity components just found,\n\n$$\n\\begin{aligned}\n\\bar{a}_{x} & =\\frac{d \\bar{v}_{x}}{d t} \\frac{d t}{d \\bar{t}}=\\frac{\\left(a_{x}-0\\right)\\left(1-v_{x} v / c^{2}\\right)-\\left(v_{x}-v\\right)\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1}{\\gamma\\left(1-v_{x} v / c^{2}\\right)} \\\\\n& =\\frac{a_{x}-a_{x} v_{x} v / c^{2}+v_{x} a_{x} v / c^{2}-a_{x} v^{2} / c^{2}}{\\gamma\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}\n\\end{aligned}\n$$\n\n$$\n\\bar{a}_{y}=\\frac{a_{y}\\left(1-v_{x} v / c^{2}\\right)-v_{y}\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1-\\beta^{2}}{1-v v_{x} / c^{2}}=\\frac{a_{y}+\\left(a_{x} v_{y}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)}{\\left(1-v_{x} / c^{2}\\right)^{3}}\\left(1-\\beta^{2}\\right)\n$$\n\nThe formula for $\\bar{a}_{z}$ is derived as that for $\\bar{a}_{y}$, with $z$ replacing $y$ throughout.", "12.23": "\\subsection*{12.23 Show that if the curve of motion in $O$ 's frame is the path of $\\bar{O}$ itself, the clock in $\\bar{O}$ 's frame (the clock moving with the particle) measures proper time.}\n\nBy (3) of Problem 12.4,\n\n$$\n\\begin{aligned}\nx^{0} & =a_{0}^{0} \\bar{x}^{0}-a_{0}^{1} \\bar{x}^{1}-a_{0}^{2} \\bar{x}^{2}-a_{0}^{3} \\bar{x}^{3} \\\\\nx^{i} & =-a_{i}^{0} \\bar{x}^{0}+a_{i}^{1} \\bar{x}^{1}+a_{i}^{2} \\bar{x}^{2}+a_{i}^{3} \\bar{x}^{3} \\quad(i=1,2,3)\n\\end{aligned}\n$$\n\nNow the motion of $\\bar{O}$ relative to itself is obviously $\\bar{x}^{1}=\\bar{x}^{2}=\\bar{x}^{3}=0$. Hence,\n\n$$\nx^{0}=a_{0}^{0} c u \\quad x^{1}=-a_{1}^{0} c u \\quad x^{2}=-a_{2}^{0} c u \\quad x^{3}=-a_{3}^{0} c u\n$$\n\ngive the trajectory of $\\bar{O}$ in $O$ 's frame, with parameter $u=\\bar{t}$. Therefore, the tangent field to the trajectory is\n\n$$\n\\left(\\frac{d x^{i}}{d u}\\right)=\\left(a_{0}^{0} c,-a_{1}^{0} c,-a_{2}^{0} c,-a_{3}^{0} c\\right)\n$$\n\nso that the proper time parameter for this curve is defined as\n\n$$\n\\tau=\\frac{1}{c} \\int_{0}^{\\bar{t}} \\sqrt{\\left|\\left(a_{0}^{0} c\\right)^{2}-\\left(a_{1}^{0} c\\right)^{2}-\\left(a_{2}^{0} c\\right)^{2}-\\left(a_{3}^{0} c\\right)^{2}\\right|} d u=\\sqrt{\\left|\\left(a_{0}^{0}\\right)^{2}-\\left(a_{1}^{0}\\right)^{2}-\\left(a_{2}^{0}\\right)^{2}-\\left(a_{3}^{0}\\right)^{2}\\right|} \\int_{0}^{\\bar{t}} d u\n$$\n\nBecause the inverse transformation is Lorentz, the factor in front of the integral sign equals 1 , and so $\\tau=\\bar{t}$.", "12.24": "\\subsection*{12.24 Derive the identities (12.25).}\n\nBy (12.21),\n\n$$\n\\begin{aligned}\nu_{i} u^{i} & =g_{i j} u^{i} u^{j} \\equiv\\left(u^{0}\\right)^{2}-\\left(u^{1}\\right)^{2}-\\left(u^{2}\\right)^{2}-\\left(u^{3}\\right)^{2} \\\\\n& =\\left[\\left(v_{t}\\right)^{2}-\\left(v_{x}\\right)^{2}-\\left(v_{y}\\right)^{2}-\\left(v_{z}\\right)^{2}\\right]\\left(\\frac{d t}{d \\tau}\\right)^{2}=\\left[c^{2}-\\hat{v}^{2}\\right] \\frac{1}{1-\\hat{v}^{2} / c^{2}}=c^{2}\n\\end{aligned}\n$$\n\nand from this,\n\n$$\n0=\\frac{d}{d \\tau}\\left(c^{2}\\right)=\\frac{d}{d \\tau}\\left(u_{i} u^{i}\\right)=2 u_{i} b^{i}\n$$\n\nBy differentiation of the velocity components just found,\n\n$$\n\\begin{aligned}\n\\bar{a}_{x} & =\\frac{d \\bar{v}_{x}}{d t} \\frac{d t}{d \\bar{t}}=\\frac{\\left(a_{x}-0\\right)\\left(1-v_{x} v / c^{2}\\right)-\\left(v_{x}-v\\right)\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1}{\\gamma\\left(1-v_{x} v / c^{2}\\right)} \\\\\n& =\\frac{a_{x}-a_{x} v_{x} v / c^{2}+v_{x} a_{x} v / c^{2}-a_{x} v^{2} / c^{2}}{\\gamma\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-\\beta^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}\n\\end{aligned}\n$$\n\n$$\n\\bar{a}_{y}=\\frac{a_{y}\\left(1-v_{x} v / c^{2}\\right)-v_{y}\\left(0-a_{x} v / c^{2}\\right)}{\\left(1-v_{x} v / c^{2}\\right)^{2}} \\frac{1-\\beta^{2}}{1-v v_{x} / c^{2}}=\\frac{a_{y}+\\left(a_{x} v_{y}-v_{x} a_{y}\\right)\\left(v / c^{2}\\right)}{\\left(1-v_{x} / c^{2}\\right)^{3}}\\left(1-\\beta^{2}\\right)\n$$\n\nThe formula for $\\bar{a}_{z}$ is derived as that for $\\bar{a}_{y}$, with $z$ replacing $y$ throughout.", "12.25": "\\subsection*{12.25 Establish the formulas in (12.26).}\n\n$$\n\\begin{aligned}\nu^{i} & =\\frac{d x^{i}}{d \\tau}=\\frac{d x^{i}}{d t} \\frac{d t}{d \\tau}=\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}} \\\\\nb^{i} & =\\frac{d u^{i}}{d \\tau}=\\left[\\frac{d}{d t}\\left(\\frac{v_{i}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\\right] \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{1 / 2}-v_{i}(1 / 2)\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}\\left(-2 a_{x} v_{x}-2 a_{y} v_{y}-2 a_{z} v_{z}\\right) / c^{2}}{1-\\hat{v}^{2} / c^{2}} \\frac{d t}{d \\tau} \\\\\n& =\\frac{a_{i}\\left(1-\\hat{v}^{2} / c^{2}\\right)+v_{i}\\left(a_{x} v_{x}+a_{y} v_{y}+a_{z} v_{z}\\right) / c^{2}}{\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}=\\frac{a_{i}}{1-\\hat{v}^{2} / c^{2}}+\\frac{(\\mathbf{v a}) v_{i}}{c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{2}}\n\\end{aligned}\n$$", "12.26": "\\subsection*{12.26 Derive the equation for uniformly accelerated motion along the $x$-axis of an inertial frame:}\n\n$$\nx^{2}-c^{2} t^{2}=\\frac{c^{4}}{\\alpha^{2}}\n$$\n\nLet $\\bar{O}$ be an instantaneous rest frame at some point $t_{1}$, and let $O$ be a given (stationary) frame in which the motion curve is traced. Since the motion is along the $x$-axis of $O$,\n\n$$\nv_{y}=v_{z}=a_{y}=a_{z}=0 \\quad \\text { and } \\quad \\bar{v}_{y}=\\bar{v}_{z}=\\bar{a}_{y}=\\bar{a}_{z}=0\n$$\n\nwhereby $\\bar{a}_{x}=\\alpha=$ const. (assuming $\\bar{a}_{x}>0$ ). At $t=t_{1}, v=v_{x}$ (the constant velocity of $\\bar{O}$ is by definition equal to the instantaneous velocity of the particle); thus, from (12.20),\n\n\n\\begin{equation*}\n\\alpha=\\frac{a_{x}\\left(1-v^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x} v / c^{2}\\right)^{3}}=\\frac{a_{x}\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3}}=\\frac{a_{x}}{\\left(1-v_{x}^{2} / c^{2}\\right)^{3 / 2}} \\tag{1}\n\\end{equation*}\n\n\nSince $t_{1}$ is arbitrary, (1) must hold for all $t$. Writing $\\dot{x}, \\ddot{x}$ for the derivatives of $x(t)$, we have from (1):\n\n\n\\begin{equation*}\nc^{3} \\ddot{x}=\\alpha\\left(c^{2}-\\dot{x}^{2}\\right)^{3 / 2} \\tag{2}\n\\end{equation*}\n\n\nMake the substitution $y=\\dot{x}$ and (2) becomes\n\n\n\\begin{equation*}\nc^{3} \\frac{d y}{d t}=\\alpha\\left(c^{2}-y^{2}\\right)^{3 / 2} \\quad \\text { or } \\quad \\int \\frac{c^{3} d y}{\\left(c^{2}-y^{2}\\right)^{3 / 2}}=\\int \\alpha d t \\tag{3}\n\\end{equation*}\n\n\nStandard techniques of integration yield the first integral\n\n\n\\begin{equation*}\n\\frac{c y}{\\sqrt{c^{2}-y^{2}}}=\\alpha t \\tag{4}\n\\end{equation*}\n\n\n(where we have taken the initial velocity to be zero). Solving (4) for $y$ (assumed positive for positive $t$ ) and then integrating the equation $\\dot{x}=y(t)$, we obtain\n\n$$\nx=c \\sqrt{c^{2}+\\alpha^{2} t^{2}} / \\alpha \\quad \\text { or } \\quad x^{2}-c^{2} t^{2}=c^{4} / \\alpha^{2}\n$$\n\n(where we also take the initial position as zero). This is the desired equation, which represents a hyperbola in the $x t$ plane. By contrast, the Newtonian equation is the parabola $x=\\frac{1}{2} \\alpha t^{2}$.", "12.27": "\\subsection*{12.27 Show that the observed mass of a particle with rest mass $m$, moving at velocity $v$, is $\\hat{m}=m\\left(1-v^{2} / c^{2}\\right)^{-1 / 2}$, by considering the following experiment. Let each observer $O$ and $\\bar{O}$ carry a ball with rest mass $m$ near his origin and so situated as to collide obliquely at $t=\\bar{t}=0$ (when their origins coincide). See Fig. 12-6. Suppose this collision imparts reciprocal velocities of $\\varepsilon$ in the positive $x$-direction and negative $\\bar{x}$-direction. Calculate the momentum of the system before and after collision (which is preserved), and what each observer sees based on the equations of SR; then take the limit as $\\varepsilon \\rightarrow 0$.}\n\nThe velocity vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ of balls $B_{1}$ and $B_{2}$ before collision are, as seen by $O,(0,0,0)=\\mathbf{0}$ and $(v, 0,0)=v$ i. Observer $\\bar{O}$ calculates these vectors as $\\overline{\\mathbf{v}}_{1}=(-v, 0,0)$ and $\\overline{\\mathbf{v}}_{2}=(0,0,0)$ (either by\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192}\n\\end{center}\n\n(a) Before impact\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192(1)}\n\\end{center}\n\n(b) After impact\n\nFig. 12-6\\\\\nreciprocity or by use of (12.19). After collision, observer $O$ calculates the velocity of $B_{1}$ as $\\mathbf{v}_{1}=$ $(\\varepsilon, \\delta, 0)=\\varepsilon \\mathbf{i}+\\delta \\mathbf{j}$, assuming $B_{1}$ has the proper alignment with $B_{2}$. Reciprocally, observer $\\bar{O}$ calculates the velocity of $B_{2}$ as $\\overline{\\mathbf{v}}_{2}=(-\\varepsilon,-\\delta, 0)$. To find $\\mathbf{v}_{2}$, use the inverse of (12.19), with $\\bar{v}_{x}=-\\varepsilon$ and $\\bar{v}_{y}=-\\delta$ :\n\n$$\n\\mathbf{v}_{2}=v_{x} \\mathbf{i}+v_{y} \\mathbf{j}=\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n$$\n\nThus, observer $O$ calculates the net momentum vector of the system as follows, using the rest mass $m$ of $B_{1}$ for $m_{1}$ and the \"perceived mass\" $\\hat{m}$ of $B_{2}$ for $m_{2}$ :\n\n$$\n\\text { before impact } \\quad \\begin{aligned}\nm_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m_{1}(\\mathbf{0})+m_{2}(v \\mathbf{i})=\\hat{m} v \\mathbf{i} \\\\\n\\text { after impact } \\quad m_{1} \\mathbf{v}_{1}+m_{2} \\mathbf{v}_{2} & =m(\\varepsilon \\mathbf{i}+\\delta \\mathbf{j})+\\hat{m}\\left[\\left(\\frac{-\\varepsilon+v}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(\\frac{-\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\\right] \\\\\n& =\\left(m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{i}+\\left(m \\delta-\\hat{m} \\frac{\\delta \\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\\right) \\mathbf{j}\n\\end{aligned}\n$$\n\nSince $O$ is using the universal laws of physics as they apply to his frame (Postulate 1 of SR), the two momentum vectors above must be the same. Hence,\n\n$$\n\\hat{m} v=m \\varepsilon+\\hat{m} \\frac{v-\\varepsilon}{1-\\varepsilon v / c^{2}} \\quad \\text { and } \\quad 0=m-\\hat{m} \\frac{\\sqrt{1-\\beta^{2}}}{1-\\varepsilon v / c^{2}}\n$$\n\n(after division by $\\delta$ ). Now take the limit as $\\varepsilon \\rightarrow 0$ :\n\n$$\n\\hat{m} v=\\hat{m} v \\quad \\text { and } \\quad 0=m-\\hat{m} \\sqrt{1-\\beta^{2}}\n$$\n\nThe right-hand equation is the connection between $m$ and $\\hat{m}$.", "12.28": "\\subsection*{12.28 Show that the Minkowski force is a 4-vector.}\n\nWe must show that $\\bar{K}^{i}=a_{j}^{i} K^{j}$, if $\\bar{x}^{i}=a_{j}^{i} x^{j}$, where $\\left(a_{j}^{i}\\right)$ is any Lorentz matrix. Since $\\tau$ is invariant and $a_{i}^{i}=$ const. we may differentiate the coordinate transformation with respect to $\\tau$ across the equal sign:\n\n$$\n\\frac{d}{d \\tau}\\left(\\bar{x}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} x^{j}\\right) \\quad \\text { or } \\quad \\bar{u}^{i}=a_{j}^{i} u^{j}\n$$\n\n(proving that $\\left(u^{i}\\right)$ is a 4-vector). Multiply both sides by $m$ and differentiate again, using the fact that the rest mass of a particle is invariant:\n\n$$\n\\bar{K}^{i}=\\frac{d}{d \\tau}\\left(\\bar{m} \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(m \\bar{u}^{i}\\right)=\\frac{d}{d \\tau}\\left(a_{j}^{i} m u^{j}\\right)=a_{j}^{i} \\frac{d}{d \\tau}\\left(m u^{j}\\right)=a_{j}^{i} K^{j}\n$$", "12.29": "\\subsection*{12.29 Establish (12.32).}\nDefinition (12.30), $K^{i}=d\\left(m u^{i}\\right) / d \\tau=m b^{i}$, along with the second identity (12.25), gives at once $u_{i} K^{i}=0$.\n\nFrom $u_{i} K^{i}=g_{i j} u^{i} K^{j}=u^{0} K^{0}-u^{q} K^{q}=0$ and the first formula (12.26), we have\n\n$$\n\\frac{v_{0} K^{0}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}-\\frac{v_{q} K^{q}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}=0 \\quad \\text { or } \\quad c K^{0}=\\mathbf{v K}\n$$\n\nBy (12.31) and $c K^{0}=\\mathbf{v K}$,\n\n$$\n\\frac{1}{c} \\mathbf{v F}=\\frac{1}{c} \\mathbf{v K} \\sqrt{1-\\hat{v}^{2} / c^{2}}=K^{0} \\sqrt{1-\\hat{v}^{2} / c^{2}}=F_{0}\n$$\n\nUsing the first definition (12.29),\n\n$$\n\\mathbf{v F}=c F_{0}=\\frac{d}{d t}\\left(\\frac{m c^{2}}{\\sqrt{1-\\hat{v}^{2} / c^{2}}}\\right)\n$$", "12.30": "\\subsection*{12.30 Show that as $\\hat{v} \\rightarrow 0, \\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+O\\left(\\hat{v}^{4} / c^{2}\\right)$. Interpret this result.}\n\nThe expression for relativistic energy, $\\hat{E}=m c^{2}\\left(1-\\hat{v}^{2} / c^{2}\\right)^{-1 / 2}$ may be expanded by the binomial theorem:\n\n$$\n(1+x)^{\\alpha}=1+\\alpha x+\\frac{\\alpha(\\alpha-1)}{2 !} x^{2}+\\cdots \\quad(-1<x<1)\n$$\n\nThe result is\n\n$$\n\\hat{E}=m c^{2}+\\frac{1}{2} m \\hat{v}^{2}+\\frac{3 m \\hat{v}^{4}}{8 c^{2}}+\\cdots\n$$\n\nThus, at low speeds, the total energy of particle is very nearly the sum of its rest energy (which includes all sorts of potential energy) and its classical kinetic energy.", "12.31": "\\subsection*{12.31 Prove that $\\bar{\\square} \\bar{f}=\\square f$.}\n\nAs the $g_{i j}$ are constants, $\\square f \\equiv g^{i j} f_{, i j}=$ invariant.", "12.32": "\\subsection*{12.32 Prove that if $\\left(F^{i j}\\right)$ is any matrix of functions of the 3-vectors $\\mathbf{U}$ and $\\mathbf{V}$ such that $\\partial F^{i j} / \\partial x^{j}=$ $0 \\quad(i=0,1,2,3)$ for all inertial frames and $F^{i j}(\\mathbf{0}, \\mathbf{0})=0$ for all $i, j$, where $\\mathbf{0}=(0,0,0)$, then $\\left(F^{i j}\\right)$ is a second-order contravariant tensor under Lorentz transformations.\n\nLet $\\left(u_{i}\\right)$ be any constant, covariant vector under Lorentz transformations [hence, $\\left(\\bar{u}_{i}\\right)=\\left(b_{i}^{k} u_{k}\\right)$ is also constant]. Define\n\n$$\nS^{i} \\equiv u_{k} F^{k i} \\quad \\bar{S}^{i} \\equiv \\bar{u}_{k} \\bar{F}^{k i}\n$$\n\nBy the given conditions $\\partial \\bar{F}^{i j} / \\partial \\bar{x}^{j}=0$,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=\\bar{u}_{k} \\frac{\\partial \\bar{F}^{k i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial S^{i}}{\\partial x^{i}}\n$$\n\nSuppose that at some point $\\left(x_{0}^{i}\\right), \\bar{S}^{i}=h^{i}\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$; then,\n\n$$\n\\frac{\\partial \\bar{S}^{i}}{\\partial \\bar{x}^{i}}=0=\\frac{\\partial h^{i}}{\\partial S^{j}} \\frac{\\partial S^{j}}{\\partial x^{k}} \\frac{\\partial x^{k}}{\\partial \\bar{x}^{i}} \\quad \\text { or } \\quad\\left(b_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}\\right) \\frac{\\partial S^{j}}{\\partial x^{k}}=0\n$$\n\nfor an arbitrary matrix $\\left(\\partial S^{j} / \\partial x^{k}\\right)$ having $\\partial S^{i} / \\partial x^{i}=0$. By a well-known lemma (Problem 12.57), there exists a real number $\\lambda=\\lambda\\left(S^{0}, S^{1}, S^{2}, S^{3}\\right)$ such that\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda \\delta_{j}^{k} \\tag{1}\n\\end{equation*}\n\n\nNow differentiate both sides of (1) with respect to $S^{l}$ :\n\n\n\\begin{equation*}\nb_{i}^{k} \\frac{\\partial^{2} h^{i}}{\\partial S^{j} \\partial S^{l}}=\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k} \\tag{2}\n\\end{equation*}\n\n\nwhich is symmetrical in $j$ and $l$; therefore,\n\n\n\\begin{equation*}\n\\frac{\\partial \\lambda}{\\partial S^{l}} \\delta_{j}^{k}=\\frac{\\partial \\lambda}{\\partial S^{j}} \\delta_{l}^{k} \\tag{3}\n\\end{equation*}\n\n\nfor all $j, k, l$. Let $k=l \\neq j$ in (3):\n\n$$\n\\frac{\\partial \\lambda}{\\partial S^{k}} \\cdot 0=\\frac{\\partial \\lambda}{\\partial S^{j}} \\cdot 1 \\quad \\text { or } \\quad \\frac{\\partial \\lambda}{\\partial S^{j}}=0\n$$\n\nHence $\\lambda$ is constant with respect to the $S^{i}$ and (1) inverts to give\n\n\n\\begin{equation*}\n\\frac{\\partial h^{i}}{\\partial S^{j}}=\\lambda a_{j}^{i} \\tag{4}\n\\end{equation*}\n\n\nIntegrating (4),\n\n\n\\begin{equation*}\nh^{i} \\equiv \\bar{S}^{i}=\\lambda a_{j}^{i} S^{j}+T^{i} \\tag{5}\n\\end{equation*}\n\n\nFor the special assignment $\\mathbf{U}=\\mathbf{V}=\\mathbf{0}$, we have (since $\\overline{\\mathbf{0}}=\\mathbf{0}$ ):\n\n\n\\begin{align*}\n& S^{i}=\\left(u_{1}\\right)(0)+\\left(u_{2}\\right)(0)+\\left(u_{3}\\right)(0)+\\left(u_{4}\\right)(0)=0  \\tag{6}\\\\\n& \\bar{S}^{i}=\\left(\\bar{u}_{1}\\right)(0)+\\left(\\bar{u}_{2}\\right)(0)+\\left(\\bar{u}_{3}\\right)(0)+\\left(\\bar{u}_{4}\\right)(0)=0\n\\end{align*}\n\n\nTogether, (5) and (6) imply $T^{i}=0 \\quad(i=0,1,2,3)$; consequently,\n\n\n\\begin{equation*}\n\\bar{S}^{i}=\\lambda a_{j}^{i} S^{j} \\tag{7}\n\\end{equation*}\n\n\nSimilarly, there exists a real number $\\mu$ such that\n\n\n\\begin{equation*}\nS^{i}=\\mu b_{j}^{i} \\bar{S}^{j} \\tag{8}\n\\end{equation*}\n\n\nIt follows that $\\bar{S}^{i}=\\lambda a_{i}^{i} \\mu b_{k}^{j} \\bar{S}^{k}=\\lambda \\mu \\bar{S}^{i}$, or $\\lambda \\mu=1$. But we can exploit the reciprocal relationship between observers $O$ and $\\bar{O}$, as in Problem 12.4; to show that $\\lambda=\\mu$. Therefore, $\\lambda=\\mu=1$ and (7) or (8) becomes the transformation law of a (contravariant) 4-vector. Finally, we conclude from the Quotient Theorem that if $F^{k i} u_{k} \\equiv S^{i}$ is a tensor for an arbitrary covariant vector $\\left(u_{i}\\right),\\left(F^{i j}\\right)$ is a second-order contravariant tensor.", "12.33": "I'm sorry, but there doesn't seem to be a Solved Problem 12.33 in the text provided.", "13.1": "13.1 (a) Show that the set of polynomials\n\n$$\np_{1}(t)=1+t \\quad p_{2}(t)=t+t^{2} \\quad p_{3}(t)=t^{2}+t^{3} \\quad p_{4}(t)=t^{3}-1\n$$\n\nis a basis for the vector space $\\mathbf{P}^{3}$ (polynomials of degree $\\leqq 3$ ). (b) Find the components of the polynomial $p(t)=t^{3}$ relative to this basis.\n\n(a) Since the dimension of $\\mathbf{P}^{3}$ is 4 and there are 4 vectors, it suffices to show they are linearly independent. Suppose that, for all $t$,\n\nor\n\n$$\n\\begin{gathered}\n\\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=0 \\\\\n\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}\\right) t^{3}=0\n\\end{gathered}\n$$\n\nSince this is an identity, we must have\n\n$$\n0=\\lambda^{1}-\\lambda^{4}=\\lambda^{1}+\\lambda^{2}=\\lambda^{2}+\\lambda^{3}=\\lambda^{3}+\\lambda^{4}\n$$\n\nThus $\\lambda^{1}=\\lambda^{4}, \\lambda^{1}=-\\lambda^{2}=\\lambda^{3}$; so the last equation gives $\\lambda^{1}+\\lambda^{1}=0$ or $\\lambda^{1}=0$, and all $\\lambda^{i}$ vanish, thus proving linear independence.\n\n(b) To find the linear combination yielding $p(t)=t^{3}$, write\n\n$$\n\\begin{aligned}\n& \\qquad \\lambda^{1}(1+t)+\\lambda^{2}\\left(t+t^{2}\\right)+\\lambda^{3}\\left(t^{2}+t^{3}\\right)+\\lambda^{4}\\left(t^{3}-1\\right)=t^{3} \\\\\n& \\text { i.e. } \\quad\\left(\\lambda^{1}-\\lambda^{4}\\right) \\cdot 1+\\left(\\lambda^{1}+\\lambda^{2}\\right) t+\\left(\\lambda^{2}+\\lambda^{3}\\right) t^{2}+\\left(\\lambda^{3}+\\lambda^{4}-1\\right) t^{3}=0 \\\\\n& \\text { or } \\quad \\lambda^{1}=\\lambda^{4} \\quad \\lambda^{1}=-\\lambda^{2}=\\lambda^{3} \\quad \\lambda^{1}+\\lambda^{1}-1=0\n\\end{aligned}\n$$\n\nHence, $\\lambda^{1}=-\\lambda^{2}=\\lambda^{3}=\\lambda^{4}=1 / 2$.", "13.2": "13.2 (a) Model the 4-group by manipulating an ordinary $8 \\frac{1}{2}$ by 11 sheet of paper, in the following way: Let $s$ be the operation of turning the sheet over sideways (as in a book) and setting it on its original location; $u$, the operation of turning the sheet upside down (end-for-end); $b$, both operations ( $s$ followed by $u$, resulting in a $180^{\\circ}$ rotation of the page, face up); and $e$, doing nothing (identity). Interpret the group operation (multiplication) as one operation followed by another (thus, for example, by the above definitions, $b=s u$, reading from left to right). $(b)$ Show that the 4-group cannot be isomorphic to the cyclic group on four elements, $\\mathbf{C}^{4}$.\\\\\n(a) This is one of those problems in mathematics that is best handled without formulas or equations. By simple observation, the operation $u s$ also results in a $180^{\\circ}$ rotation; hence, $b=s u=u s$. It is also clear that if we apply $s$ twice, or $u$ twice, the sheet is left in its original state; $s^{2}=u^{2}=e$. Next, observe that the associative law is valid, so long as we keep the order of the operations intact. It follows that\n\n$$\nb^{2}=(s u)(u s)=s(u)^{2} s=s^{2}=e\n$$\n\nWhen we multiply all the group elements by $b$, we obtain:\n\n$$\nb e=b \\quad b b=e \\quad b u=(s u) u=s u^{2}=s \\quad b s=(s u) s=(u s) s=u\n$$\n\nHence, the multiplication table for this group may be displayed and may be seen to coincide with that for the 4-group:\n\n\\begin{center}\n\\begin{tabular}{c|cccc}\n$\\cdot$ & $e$ & $s$ & $u$ & $b$ \\\\\n\\hline\n$e$ & $e$ & $s$ & $u$ & $b$ \\\\\n$s$ & $s$ & $e$ & $b$ & $u$ \\\\\n$u$ & $u$ & $b$ & $e$ & $s$ \\\\\n$b$ & $b$ & $u$ & $s$ & $e$ \\\\\n\\end{tabular}\n\\end{center}\n\n(b) For the cyclic group, $\\left\\{e, z, z^{2}, z^{3}\\right\\}$, with $z^{4}=e$ for some $z$, we could not have $z^{2}=e$, which is the characteristic property of all elements of the 4-group.", "13.3": "13.3 The simple Lorentz group can be studied by compressing the $4 \\times 4$ matrices down to $2 \\times 2$ matrices:\n\n$$\n\\left[\\begin{array}{llll}\na & b & 0 & 0 \\\\\nb & a & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad\\left(a^{2}-b^{2}=1\\right)\n$$\n\nShow explicitly that all real $2 \\times 2$ matrices of the above form constitute an abelian group (the group $\\mathbf{L}(2)$ ) under matrix multiplication, and that $\\mathbf{L}(2)$ is a subgroup of the following two larger groups:\n\n$$\n\\begin{aligned}\n& \\mathbf{G L}(2, \\mathbf{R}) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d \\neq b c \\\\\n& \\mathbf{S U}(2) \\text { : matrices of the form }\\left[\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right], \\quad a d-b c=1\n\\end{aligned}\n$$\n\nSince for a matrix in $\\mathbf{L}(2)$,\n\n$$\na d-b c=a^{2}-b^{2}=1\n$$\n\nall such matrices belong to $\\mathbf{S U}(2)$, which, in turn, is a $\\operatorname{subgroup}$ of $\\mathbf{G L}(2, \\mathbf{R})$. Now verify the group properties:\n\n(1) $u v$ belongs to the group for all $u, v$.\n\nIf\n\n$$\n\\begin{gathered}\nA=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right] \\\\\nA B=\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]\\left[\\begin{array}{ll}\nc & d \\\\\nd & c\n\\end{array}\\right]=\\left[\\begin{array}{ll}\na c+b d & a d+b c \\\\\nb c+a d & b d+a c\n\\end{array}\\right] \\equiv\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right] \\\\\nx^{2}-y^{2}=\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)=(1)(1)=1\n\\end{gathered}\n$$\n\n(2) $(u v) w=u(v w)$. Yes: matrix multiplication is associative.\n\n(3) For some $e$ and all $u$, $e u=u e=u$. Yes: the identity matrix has $1^{2}-0^{2}=1$, so is a member of $\\mathbf{L}(2)$.\n\n(4) Given $u, u^{-1} u=u u^{-1}=e$ for some $u^{-1}$.\n\n$$\n\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]^{-1}=\\frac{1}{a^{2}-b^{2}}\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]=\\left[\\begin{array}{rr}\na & -b \\\\\n-b & a\n\\end{array}\\right]\n$$\n\nwhich is in $\\mathbf{L}(2)$.\\\\\n$u v=v u \\quad$ (abelian group).\n\n\\[\nB A=\\left[\\begin{array}{ll}\nc & d  \\tag{5}\\\\\nd & c\n\\end{array}\\right]\\left[\\begin{array}{ll}\na & b \\\\\nb & a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nc a+d b & c b+d a \\\\\nd a+c b & d b+c a\n\\end{array}\\right]=\\left[\\begin{array}{ll}\nx & y \\\\\ny & x\n\\end{array}\\right]=A B\n\\]", "13.4": "\\section*{VECTOR SPACE CONCEPTS}\n13.4 (a) Show that the space $\\mathbf{P}$ of all real-valued polynomials in a real variable $x$ is infinitedimensional. (b) Conclude that $C^{k}(\\mathbf{R})$ is infinite-dimensional.\n\n(a) Suppose that $\\mathbf{P}$ had the finite basis $\\left\\{p_{1}, p_{2}, \\ldots, p_{n}\\right\\}$. Then, for any real polynomial $p(x)$, there exist constants $a_{1}, \\ldots, a_{n}$ such that\n\n\\begin{equation*}\na_{1} p_{1}(x)+a_{2}(x)+\\cdots+a_{n} p_{n}(x)=p(x) \\tag{1}\n\\end{equation*}\n\nWrite (1) for the $n+1$ values $x_{1}<x_{2}<\\cdots<x_{n+1}$ as a matrix equation:\n\n\\[\na_{1}\\left[\\begin{array}{c}\np_{1}\\left(x_{1}\\right)  \\tag{2}\\\\\np_{1}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{1}\\left(x_{n+1}\\right)\n\\end{array}\\right]+a_{2}\\left[\\begin{array}{c}\np_{2}\\left(x_{1}\\right) \\\\\np_{2}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{2}\\left(x_{n+1}\\right)\n\\end{array}\\right]+\\cdots+a_{n}\\left[\\begin{array}{c}\np_{n}\\left(x_{1}\\right) \\\\\np_{n}\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np_{n}\\left(x_{n+1}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{c}\np\\left(x_{1}\\right) \\\\\np\\left(x_{2}\\right) \\\\\n\\cdots \\\\\np\\left(x_{n+1}\\right)\n\\end{array}\\right]\n\\]\n\nThe column vectors on the left are elements of $\\mathbf{R}^{n+1}$, and as there are $n$ of them, they do not span $\\mathbf{R}^{n+1}$ (see Problem 13.5). To finish the proof, we have only to choose a vector on the right of (2) that is not in the span of those on the left-say, $\\left(z_{1}, z_{2}, \\ldots, z_{n+1}\\right)$-and then to exhibit a polynomial $p$ that takes on those values at $x_{1}, x_{2}, \\ldots, x_{n+1}$. The polynomial provided by Lagrange's interpolation formula does the job.\n\n(b) For any $k$, the vector space $C^{k}(\\mathbf{R})$ contains the infinite-dimensional subspace $\\mathbf{P}$; thus, it too is infinite-dimensional.", "13.5": "\\section*{VECTOR SPACE CONCEPTS}\n13.5 The set $\\mathbf{S}$ of all linear combinations of a fixed set of $n$ vectors, $\\left\\{\\mathbf{b}_{1}, \\mathbf{b}_{2}, \\ldots, \\mathbf{b}_{n}\\right\\}$, is called the span of the given vectors; it is obviously a vector space. Prove that this space has dimension $m \\leqq n$, with equality if and only if the given vectors are linearly independent.\n\nFirst we show that any $n+1$ vectors in $\\mathbf{S}$ are linearly dependent. Suppose, on the contrary, that $\\left\\{\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n+1}\\right\\}$ are linearly independent. Then, because the sequence of vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nis necessarily dependent, the well-known exchange lemma tells us that a sequence\n\n$$\n\\begin{array}{lllllll}\n\\mathbf{u}_{1} & \\mathbf{b}_{1} & \\ldots & \\mathbf{b}_{j-1} & \\mathbf{b}_{j+1} & \\ldots & \\mathbf{b}_{n}\n\\end{array}\n$$\n\nalso spans $\\mathbf{S}$. Repeating the argument $n-1$ times, we arrive at the result that the vectors\n\n$$\n\\begin{array}{lllll}\n\\mathbf{u}_{n} & \\mathbf{u}_{n-1} & \\ldots & \\mathbf{u}_{2} & \\mathbf{u}_{1}\n\\end{array}\n$$\n\nspan $\\mathbf{S}$, making $\\mathbf{u}_{n+1}$ dependent on them-a contradiction.\n\nIf, therefore, $\\left\\{\\mathbf{b}_{i}\\right\\}$ is linearly independent, it constitutes a basis for $\\mathbf{S}$, and $m=n$. On the other hand, if only $m<n$ of the $\\mathbf{b}_{i}$ are linearly independent, the above argument shows that any basis consists of exactly $m$ vectors.", "13.6": "\\subsection*{13.6 Prove Theorem 13.1.}\nIt is almost trivial that any two vector spaces of dimension $n$ are isomorphic [if $\\left\\{\\mathbf{b}_{i}^{(1)}\\right\\}$ and $\\left\\{\\mathbf{b}_{i}^{(2)}\\right\\}$ are bases, set up the correspondence $\\left.v^{i} \\mathbf{b}_{i}^{(1)} \\leftrightarrow v^{i} \\mathbf{b}_{i}^{(2)}\\right]$. Thus it is necessary to prove only that $\\mathbf{V}^{*}$ is $n$-dimensional if $\\mathbf{V}$ is; in other words, to prove that the set of vectors $\\left\\{\\boldsymbol{\\beta}^{i}\\right\\}$ defined by (13.4) (i) is linearly independent and (ii) has $\\mathbf{V}^{*}$ as its span. (Problem 13.5 will then immediately yield Theorem 13.1.)\n\nProof of (i): By (13.5), for $j=1,2, \\ldots, n$,\n\n$$\n\\lambda_{i} \\boldsymbol{\\beta}^{i}(\\mathbf{v})=0 \\quad \\rightarrow \\quad \\lambda_{i} \\boldsymbol{\\beta}^{i}\\left(\\mathbf{b}_{j}\\right)=0 \\rightarrow \\lambda_{i} \\delta_{j}^{i}=0 \\quad \\rightarrow \\quad \\lambda_{j}=0\n$$\n\nProof of (ii): If $\\boldsymbol{\\beta}(\\mathbf{v})$ is an arbitrary element of $\\mathbf{V}^{*}$, then, by $(13.4 b)$,\n\n$$\n\\boldsymbol{\\beta}(\\mathbf{v})=\\boldsymbol{\\beta}\\left(v^{i} \\mathbf{b}_{i}\\right)=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) v^{i}=\\boldsymbol{\\beta}\\left(\\mathbf{b}_{i}\\right) \\boldsymbol{\\beta}^{i}(\\mathbf{v})\n$$\n\nthat is, $\\boldsymbol{\\beta}$ is a linear combination of the $\\boldsymbol{\\beta}^{i}$.", "13.7": "13.7 Prove the inverse relation between the matrices $A$ and $\\bar{A}$ of (13.6).\n\nBy definition $\\overline{\\mathbf{b}}_{i}=A_{i}^{j} \\mathbf{b}_{j}$ and $\\overline{\\boldsymbol{\\beta}}^{j}=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}$, so look at (13.5): $\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\delta_{i}^{j}$. By the algebra of mappings and the fact that each $\\overline{\\boldsymbol{\\beta}}^{j}$ and $\\boldsymbol{\\beta}^{k}$ is linear, we have\n\n$$\n\\begin{aligned}\n\\delta_{i}^{j}=\\overline{\\boldsymbol{\\beta}}^{j}\\left(\\overline{\\mathbf{b}}_{i}\\right) & =\\left(\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\right)\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(\\overline{\\mathbf{b}}_{i}\\right)=\\bar{A}_{k}^{j} \\boldsymbol{\\beta}^{k}\\left(A_{i}^{r} \\mathbf{b}_{r}\\right) \\\\\n& =\\bar{A}_{k}^{j}{ }^{j} A_{i}^{r} \\boldsymbol{\\beta}^{k}\\left(\\mathbf{b}_{r}\\right)=\\bar{A}_{k}^{j} A_{i}^{r} \\delta_{r}^{k}=\\bar{A}_{k}^{j} A_{i}^{k}\n\\end{aligned}\n$$\n\nthat is, $\\bar{A} A=I$.", "13.8": "\\section*{TENSORS ON VECTOR SPACES}\n13.8 Which of the following represent linear mappings of $\\left(\\mathbf{R}^{3}\\right)^{*}$ (taking the one-forms on $\\mathbf{R}^{3}$ into the reals), and so constitute (contravariant) tensors of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$ ?\\\\\n(a) $T\\left(a_{1} d x^{1}+a_{2} d x^{2}+a_{3} d x^{3}\\right)=a_{1} a_{2} a_{3}$\\\\\n(b) $T\\left(a_{i} d x^{i}\\right)=a_{1}-a_{3}$\\\\\n(c) $T\\left(a_{i} d x^{i}\\right)=1$\\\\\n(d) $T\\left(a_{i} d x^{i}\\right)=0$\n\n(b) and (d)-the only linear mappings.", "13.9": "\\section*{TENSORS ON VECTOR SPACES}\n13.9 Associated with a particular basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ of a vector space of dimension $n$, we are given some set of numbers $\\left\\{C_{k}^{i j} ; \\quad i, j, k=1, \\ldots, n\\right\\}$. Then we define another set of numbers (and assume a similar definition for all changes of bases), $\\left\\{\\bar{C}_{k}^{i j} ; i, j, k=1, \\ldots, n\\right\\}$, such that\n\n$$\n\\bar{C}_{k}^{i j}=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s}\n$$\n\nand call these numbers the components of the \"tensor\" $C$ on the new basis $\\left\\{\\overline{\\mathbf{b}}_{i}\\right\\}$. Show that this \"tensor\" is indeed a tensor per Definition 2.\n\nWe have only to define the functional\n\n$$\nT\\left(\\boldsymbol{\\omega}_{1}, \\boldsymbol{\\omega}_{2} ; \\mathbf{v}\\right)=T\\left(a_{i} \\boldsymbol{\\beta}^{i}, b_{j} \\boldsymbol{\\beta}^{j} ; v^{k} \\mathbf{b}_{k}\\right)=a_{i} b_{j} C_{k}^{i j} v^{k}\n$$\n\nwhich, by inspection, is a type $\\left(\\begin{array}{l}2 \\\\ 1\\end{array}\\right)$ tensor. We have:\n\n$$\n\\begin{aligned}\n& T_{k}^{i j}=T\\left(\\boldsymbol{\\beta}^{i}, \\boldsymbol{\\beta}^{j} ; \\mathbf{b}_{k}\\right)=T\\left(\\delta_{r}^{i} \\boldsymbol{\\beta}^{r}, \\delta_{s}^{j} \\boldsymbol{\\beta}^{s} ; \\delta_{k}^{t} \\mathbf{b}_{t}\\right)=\\delta_{r}^{i} \\delta_{s}^{j} C_{t}^{r s} \\delta_{k}^{t}=C_{k}^{i j} \\\\\n& \\bar{T}_{k}^{i j}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\boldsymbol{\\beta}}^{j} ; \\overline{\\mathbf{b}}_{k}\\right)=T\\left(\\bar{A}_{r}^{i} \\boldsymbol{\\beta}^{r}, \\bar{A}_{s}^{j} \\overline{\\boldsymbol{\\beta}}^{s} ; A_{k}^{t} \\mathbf{b}_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} T\\left(\\beta^{r}, \\beta^{s} ; b_{t}\\right)=\\bar{A}_{r}^{i} \\bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s} \\equiv \\bar{C}_{k}^{i j}\n\\end{aligned}\n$$\n\nwhich show that $T$ and $C$ coincide in all coordinate systems.", "13.10": "\\section*{TENSORS ON VECTOR SPACES}\n13.10 In terms of the components $g_{i j}$ of a metric tensor $G(\\mathbf{u}, \\mathbf{v})$, show that:\n\n(a) $G$ is symmetric if and only if $g_{i j}=g_{j i}$ for all $i, j$.\n\n(b) $G$ is nonsingular if and only if $\\left|g_{i j}\\right| \\neq 0$.\n\n(c) $G$ is positive definite if, for all vectors $\\left(u^{i}\\right) \\neq \\mathbf{0}, g_{i j} u^{i} u^{j} \\neq 0$ and $g_{11}>0$.\n\nBy Section 13.5, $g_{i j}=G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)$ where $\\left\\{\\mathbf{b}_{i}\\right\\}$ is some basis for $\\mathbf{V}$. Then, if $\\mathbf{u}=u^{i} \\mathbf{b}_{i}$ and $\\mathbf{v}=v^{i} \\mathbf{b}_{i}$ are any two vectors in $\\mathbf{V}$,\n\n$$\nG(\\mathbf{u}, \\mathbf{v})=u^{i} v^{j} G\\left(\\mathbf{b}_{i}, \\mathbf{b}_{j}\\right)=g_{i j} u^{i} v^{j}\n$$\n\n(a) $G(\\mathbf{u}, \\mathbf{v})=G(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$, if and only if\n\n$$\ng_{i j} u^{i} v^{j}=g_{i j} v^{i} u^{j}=g_{j i} u^{i} v^{j} \\quad \\text { or } \\quad\\left(g_{i j}-g_{j i}\\right) u^{i} v^{j}=0\n$$\n\nfor all real $u^{i}, v^{j}$, which is true if and only if $g_{i j}=g_{j i}$.\n\n(b) In matrix form, the nonsingularity criterion reads:\n\n$$\n\\left[u^{T} G v=0, \\text { for all } u\\right] \\rightarrow v=0\n$$\n\nBut $u^{T} G v$ vanishes for all $u$ if and only if $G v$ is the zero vector. Hence the criterion takes the form\n\n$$\nG v=0 \\rightarrow v=0\n$$\n\nwhich defines $G$ as a nonsingular matrix (a matrix with nonvanishing determinant).\n\n(c) For each fixed $\\mathbf{u}$ and a scalar parameter $\\lambda$, we have\n\n$$\ng_{i j}\\left(u^{i}+\\lambda \\mathbf{b}_{1}^{i}\\right)\\left(u^{j}+\\lambda \\mathbf{b}_{1}^{j}\\right)=g_{i j}\\left(u^{i}+\\lambda \\delta_{1}^{i}\\right)\\left(u^{j}+\\lambda \\delta_{1}^{j}\\right)=G(\\mathbf{u}, \\mathbf{u})+b \\lambda+g_{11} \\lambda^{2} \\equiv P(\\lambda)\n$$\n\nwhere $b \\equiv\\left(g_{1 j}+g_{j 1}\\right) u^{j}$. If $\\mathbf{u}$ is not in the span of $\\mathbf{b}_{1}$, the quadratic form is, by hypothesis, nonzero. Hence, the discriminant of $P(\\lambda)$ is negative:\n\n$$\nb^{2}-4 g_{11} G(\\mathbf{u}, \\mathbf{u})<0 \\quad \\text { or } \\quad G(\\mathbf{u}, \\mathbf{u})>\\frac{b^{2}}{4 g_{11}} \\geqq 0\n$$\n\nIt only remains to note that if $\\mathbf{u}=\\kappa \\mathbf{b}_{1} \\quad(\\kappa \\neq 0)$, then $G(\\mathbf{u}, \\mathbf{u})=\\kappa^{2} g_{11}$, which is again positive.", "13.11": "\\section*{TENSORS ON VECTOR SPACES}\n13.11 Show that positive-definiteness of a type- $\\left(\\begin{array}{l}0 \\\\ 2\\end{array}\\right)$ tensor $G$ implies its nonsingularity.\n\nIf $G(\\mathbf{u}, \\mathbf{v})=0$ for all $\\mathbf{u}$ and some $\\mathbf{v}$, then $G(\\mathbf{v}, \\mathbf{v})=0$; and so, by positive-definiteness, $\\mathbf{v}=0$.", "13.12": "\\section*{TENSORS ON VECTOR SPACES}\n13.12 A covariant tensor $A(\\mathbf{u}, \\mathbf{v})$ is antisymmetric if and only if $A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}, \\mathbf{v}$. Show that a criterion for antisymmetry is:\n\n$$\nA(\\mathbf{u}, \\mathbf{u})=0 \\quad(\\text { all } \\mathbf{u})\n$$\n\nBy bilinearity,\n\n$$\nA(\\mathbf{u}+\\mathbf{v}, \\mathbf{u}+\\mathbf{v})=A(\\mathbf{u}, \\mathbf{u})+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+A(\\mathbf{v}, \\mathbf{v})\n$$\n\nThus, if $A(\\mathbf{u}, \\mathbf{u})=0$ for all $\\mathbf{u}$,\n\n$$\n0=0+A(\\mathbf{u}, \\mathbf{v})+A(\\mathbf{v}, \\mathbf{u})+0 \\quad \\text { or } \\quad A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})\n$$\n\nConversely, suppose $A(\\mathbf{u}, \\mathbf{v})=-A(\\mathbf{v}, \\mathbf{u})$, for all $\\mathbf{u}$ and $\\mathbf{v}$. Then, with $\\mathbf{u}=\\mathbf{v}$, we have $A(\\mathbf{u}, \\mathbf{u})=-A(\\mathbf{u}, \\mathbf{u})$, or $A(\\mathbf{u}, \\mathbf{u})=0$.", "13.13": "\\section*{MANIFOLDS}\n13.13 (a) Show that the 1 -sphere $\\mathbf{S}^{1}$ (a circle in $\\mathbf{R}^{2}$ ) can be made into a $C^{\\infty} 1$-manifold by constructing an atlas with two charts. (b) Show that a one-chart atlas does not exist [thus, a circle is not homeomorphic to a line or interval].\n\n(a) The standard parameterization of the circle,\n\n$$\n\\varphi^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos \\theta \\\\\ny^{2}=a \\sin \\theta\n\\end{array} \\quad(0 \\leq \\theta<2 \\pi)\\right.\n$$\n\nis insufficient, since the inverse map $\\varphi$ is discontinuous at point $p$ (Fig. 13-6). But if we define\n\n$$\n\\varphi_{p}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(-\\pi<x^{1}<\\pi\\right) \\quad \\varphi_{q}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=a \\cos x^{1} \\\\\ny^{2}=a \\sin x^{1}\n\\end{array} \\quad\\left(0<x^{1}<2 \\pi\\right)\\right.\\right.\n$$\n\nthen $\\left(\\mathbf{S}^{1}-q, \\varphi_{p}\\right)$ and $\\left(\\mathbf{S}^{1}-p, \\varphi_{q}\\right)$ will constitute an atlas. Since there are no 'singular' points involved, it is clear that $\\varphi_{p}, \\varphi_{q}$ and their inverses are $C^{\\infty}$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-214}\n\\end{center}\n\nFig. 13-6\n\n(b) Suppose (U, $\\phi$ ) covered $\\mathbf{S}^{1}\\left(\\mathbf{U}=\\mathbf{S}^{1}\\right)$ and $\\phi$ mapped $\\mathbf{S}^{1}$ to the real line $\\left(x^{1}\\right)$, with both $\\phi$ and $\\boldsymbol{\\phi}^{-1}$ continuous. It is not too difficult to see that $\\phi$ maps the circle to a closed interval I: for continuous maps take bounded, closed sets to bounded, closed sets, and connected sets to connected sets; and the only bounded, closed, connected subsets of the real line are closed finite intervals. For any point $P$ on the circle let $P^{\\prime}$ be its diametrically opposite point. The map $g(t) \\equiv \\phi\\left[\\left(\\phi^{-1}(t)\\right)^{\\prime}\\right]$ takes a real number $t$ in $\\mathbf{I}$, maps it to a unique point $P$ on $\\mathbf{S}^{1}$, goes to the (unique) diametrically opposite point $P^{\\prime}$, and returns to a unique real number $t^{\\prime}$ in $\\mathbf{I}$; it is thus a continuous map from $\\mathbf{I}$ to $\\mathbf{I}$. As such, it must (by a familiar theorem of analysis) have a fixed point:\n\n$$\ng\\left(t_{0}\\right)=t_{0} \\text { for some } t_{0} \\text { in } \\mathbf{I}\n$$\n\nBut this means that $\\phi$ sends some pair of diametrically opposite points on $\\mathbf{S}^{1}$ to the same real number, denying one-oneness of $\\phi$.", "13.14": "\\section*{MANIFOLDS}\n13.14 A manifold in $\\mathbf{R}^{4}$ is defined by the charts $\\left(k=\\ldots,-2,-1,0,1,2, \\ldots ; x^{1}>0\\right)$\n\n$$\n\\mathbf{r}_{(k)}:\\left\\{\\begin{array}{l}\ny^{1}=x^{1} \\cos x^{2} \\cos x^{3} \\\\\ny^{2}=x^{1} \\cos x^{2} \\sin x^{3} \\\\\ny^{3}=x^{1} \\sin x^{2} \\\\\ny^{4}=a\\left(x^{2}+x^{3}\\right)\n\\end{array} \\quad\\left((k-1) \\frac{\\pi}{2}<x^{3}<(k+1) \\frac{\\pi}{2}\\right)\\right.\n$$\n\n(a) Show that on each coordinate patch, the mapping $\\mathbf{r}_{(k)}$ is one-to-one; hence, $\\varphi_{(k)}=$ $\\mathbf{r}_{(k)}^{-1}: U_{(k)} \\rightarrow \\mathbf{R}^{3}$ exists. (b) Show that both $\\varphi_{(k)}$ and $\\varphi_{(k)}^{-1}$ are continuous. (c) Show that the manifold is generated by a line in $\\mathbf{R}^{4}$ moving along an axis orthogonal to it, with the axis, in turn, orthogonal to the hyperplane $y^{4}=0$ (use vector geometry in $\\mathbf{R}^{4}$ ). Verify that the parameter $x^{1}$ measures the distance from a given point on the manifold to the axis. (d) Show that the parametric section $x^{3}=0$ is a right helicoid (Example 10.4), lying in the hyperplane $y^{2}=0$ ( $\\mathbf{R}^{3}$ coordinatized by $\\left.y^{1}, y^{3}, y^{4}\\right)$.\n\n(a) Assume that $\\mathbf{r}_{(k)}\\left(x^{i}\\right)=\\mathbf{r}_{(k)}\\left(u^{i}\\right)$; we want to show that $\\left(x^{1}, x^{2}, x^{3}\\right)=\\left(u^{1}, u^{2}, u^{3}\\right)$. Now,\n\n$$\n\\left.\\begin{array}{c}\nx^{1} \\cos x^{2} \\cos x^{3}=u^{1} \\cos u^{2} \\cos u^{2} \\\\\nx^{1} \\cos x^{2} \\sin x^{3}=u^{1} \\cos u^{2} \\sin u^{3}\n\\end{array}\\right\\} \\rightarrow \\tan x^{3}=\\tan u^{3}\n$$\n\nBut, for $\\mathrm{U}_{(k)}$, the argument of the tangent function is restricted to a range of $\\pi$ units; so $x^{3}=u^{3}$. It follows that\n\n$$\na\\left(x^{2}+x^{3}\\right)=a\\left(u^{2}+u^{3}\\right) \\rightarrow x^{2}=u^{2}\n$$\n\nFinally, from $x^{1} \\sin x^{2}=u^{1} \\sin u^{2}$, we obtain $x^{1}=u^{1}$.\\\\\n(b) From the form of $\\varphi_{(k)}^{-1} \\equiv \\mathbf{r}_{(k)}$, this function is $C^{\\infty}$. To solve for $\\left(x^{i}\\right)$ in terms of $\\left(y^{i}\\right)$ (to find $\\left.\\varphi_{(k)}\\right)$, write\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(x^{1}\\right)^{2}\\left(\\cos ^{2} x^{2}\\right)\\left(\\cos ^{2} x^{3}+\\sin ^{2} x^{3}\\right)+\\left(x^{1}\\right)^{2}\\left(\\sin ^{2} x^{2}\\right)=\\left(x^{1}\\right)^{2}\n$$\n\nor $x^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\quad\\left(\\right.$ since $\\left.x^{1}>0\\right)$. Then\n\n$$\n\\sin x^{2}=\\frac{y^{3}}{x^{1}}=\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\n$$\n\nor, for a suitable branch of the function $\\sin ^{-1}$,\n\n$$\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n$$\n\nIt is seen that\n\n$$\n\\varphi_{(k)}:\\left\\{\\begin{array}{l}\nx^{1}=\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}} \\\\\nx^{2}=\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right) \\\\\nx^{3}=\\frac{y^{4}}{a}-\\sin ^{-1}\\left(\\frac{y^{3}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}}\\right)\n\\end{array}\\right.\n$$\n\nis continuous (in fact, $C^{\\infty}$ ).\n\n(c) The axis orthogonal to $y^{4}=0$ is the vector $\\mathbf{e}_{4}$ in $\\mathbf{R}^{4}$. At the point $y^{4}=a\\left(x^{2}+x^{3}\\right)=$ const. on the manifold, we have (with $x^{2}, x^{3}$ constants and $x^{1}=t$ )\n\n$$\ny^{1}=t \\cos x^{2} \\cos x^{3} \\quad y^{2}=t \\cos x^{2} \\sin x^{3} \\quad y^{3}=t \\sin x^{2} \\quad y^{4}=\\text { const } .\n$$\n\n-a straight line with direction vector orthogonal to $\\mathbf{e}_{4}$. A previous calculation gives the distance from $\\left(y^{1}, y^{2}, y^{3}, y^{4}\\right)$ on $\\mathbf{M}$ to $\\left(0,0,0, a\\left(x^{2}+x^{3}\\right)\\right)$ as\n\n$$\n\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}}=x^{1}\n$$\n\n(d) Set $x^{3}=0$ and the map reduces to\n\n$$\ny^{1}=x^{1} \\cos x^{2} \\quad y^{2}=0 \\quad y^{3}=x^{1} \\sin x^{2} \\quad y^{4}=a x^{2}\n$$", "13.15": "\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.15 Derive the charts of Example 13.11(a), using stereographic projection (Fig. 13-7).\n\nAs $P$ is a \"convex\" combination of $Q$ and $\\mathrm{N}$,\n\n\\begin{equation*}\n\\left(y^{1}, y^{2}, y^{3}\\right)=\\lambda\\left(x^{1}, x^{2}, 0\\right)+(1-\\lambda)(0,0, a) \\tag{1}\n\\end{equation*}\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-215}\n\\end{center}\n\nFig. 13-7\n\nTo determine $\\lambda(\\lambda>0)$, write\n\n$$\na^{2}=\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=\\left(\\lambda x^{1}\\right)^{2}+\\left(\\lambda x^{2}\\right)^{2}+[(1-\\lambda) a]^{2}\n$$\n\nand solve, obtaining\n\n\\begin{equation*}\n\\lambda=\\frac{2 a^{2}}{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}+a^{2}} \\tag{2}\n\\end{equation*}\n\n[Note that $\\lambda$ is less than or greater than 1 according as $P$ lies in the northern or southern hemisphere; $\\lambda \\neq 0$, so this patch omits the north pole.] Together, (1) and (2) yield the chart $\\varepsilon=+1$; the chart $\\varepsilon=-1$ is obtained by changing $a$ to $-a$ in the above (stereographic projection from the south pole).", "13.16": "\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.16 The hyperboloid of one sheet $4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}-\\left(y^{3}\\right)^{2}=16$ is a $C^{\\infty} 2$-manifold $\\mathbf{M}$, by the coordinatization $(k=1,2)$\n\n$$\n\\varphi_{(k)}^{-1}:\\left\\{\\begin{array}{l}\ny^{1}=2 \\cos x^{1} \\cosh x^{2} \\\\\ny^{2}=2 \\sin x^{1} \\cosh x^{2} \\\\\ny^{3}=4 \\sinh x^{2}\n\\end{array} \\quad\\left((k-2) \\pi<x^{1}<k \\pi\\right)\\right.\n$$\n\nwith $\\mathbf{U}_{(1)}=\\mathbf{U}_{p}$ and $p=(2,0,0), \\mathbf{U}_{(2)}=\\mathbf{U}_{q}$ and $q=(-2,0,0)$. Represent the vector field on $\\mathbf{M}$ given by\n\n$$\n\\left(V^{i}\\right)=\\left(4 \\sinh x^{2}, 4 \\cosh x^{2}\\right)\n$$\n\nin terms of $(a)$ a vector basis for the tangent space $T_{p}(\\mathbf{M})$, and $(b)$ extrinsically. (c) Describe this field geometrically.\n\n(a) By the usual tools of surface theory (Section 10.5):\n\n$$\n\\begin{aligned}\n& \\mathbf{r}=\\left(2 \\cos x^{1} \\cosh x^{2}, 2 \\sin x^{1} \\cosh x^{2}, 4 \\sinh x^{2}\\right) \\\\\n& E_{1}=\\mathbf{r}_{1}=\\left(-2 \\sin x^{1} \\cosh x^{2}, 2 \\cos x^{1} \\cosh x^{2}, 0\\right) \\\\\n& E_{2}=\\mathbf{r}_{2}=\\left(2 \\cos x^{1} \\sinh x^{2}, 2 \\sin x^{1} \\sinh x^{2}, 4 \\cosh x^{2}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 0\\right) \\\\\n& \\quad+\\left(8 \\cos x^{1} \\sinh x^{2} \\cosh x^{2}, 8 \\sin x^{1} \\sinh x^{2} \\cosh x^{2}, 16 \\cosh ^{2} x^{2}\\right) \\\\\n& \\quad=\\left(4\\left(\\cos x^{1}-\\sin x^{1}\\right) \\sinh 2 x^{2}, 4\\left(\\cos x^{1}+\\sin x^{1}\\right) \\sinh 2 x^{2}, 16 \\cosh ^{2} x^{2}\\right)\n\\end{aligned}\n$$\n\n(b) From the equations for $y^{1}, y^{2}, y^{3}$ we may calculate:\n\n$$\n\\begin{array}{ll}\n\\cosh x^{2}=\\frac{1}{2} \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}} & \\sinh x^{2}=\\frac{1}{4} y^{3} \\\\\n\\cos x^{1}=\\frac{y^{1}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}} & \\sin x^{1}=\\frac{y^{2}}{\\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}\n\\end{array}\n$$\n\nso that\n\n$$\n\\begin{aligned}\n& E_{1}=\\left(-y^{2}, y^{1}, 0\\right) \\\\\n& E_{2}=\\left(\\frac{y^{1} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, \\frac{y^{2} y^{3}}{2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& \\left(V^{i}\\right)=\\left(y^{3}, 2 \\sqrt{\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}}\\right) \\\\\n& V=V^{i} E_{i}=\\left(-y^{2} y^{3}, y^{1} y^{3}, 0\\right)+\\left(y^{1} y^{3}, y^{2} y^{3}, 4\\left(y^{1}\\right)^{2}+4\\left(y^{2}\\right)^{2}\\right) \\\\\n& \\quad=\\left(y^{3}\\left(y^{1}-y^{2}\\right), y^{3}\\left(y^{1}+y^{2}\\right),\\left(y^{3}\\right)^{2}+16\\right)\n\\end{aligned}\n$$\n\n(using the equation of the hyperboloid). Hence, in terms of the coordinates $\\left(y^{i}\\right)$,\n\n$$\nV=\\sigma=y^{3}\\left(y^{1}-y^{2}\\right) d y^{1}+y^{3}\\left(y^{1}+y^{2}\\right) d y^{2}+\\left[\\left(y^{3}\\right)^{2}+16\\right] d y^{3}\n$$\n\n(c) See Fig. 13-8 and note that the first component is zero in the plane $y^{1}=y^{2}$. Hence, along the curve of intersection, the field is always parallel to the $y^{2} y^{3}$-plane. Similarly, along $y^{1}=-y^{2}$, the field is parallel to the $y^{1} y^{3}$-plane. On the circle $y^{3}=0$ the field is $(0,0,16)$, or vertical. Since the third component is $\\geqq 16$, there is always a vertical component.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217}\n\\end{center}\n\nFig. 13-8", "13.17": "\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.17 Show that the restrictions of (a) $\\sigma_{1}=y^{1} d y^{2}-y^{2} d y^{1}$ and (b) $\\sigma_{2}=\\left(y^{2}-y^{3}\\right) d y^{1}-\\left(y^{1}+\\right.$ $\\left.y^{3}\\right) d y^{2}+\\left(y^{1}+y^{2}\\right) d y^{3}$ to the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}$ are vector fields. (See Fig. 13-9 for a graph of selected values of $\\sigma_{1}$.) By the well-known \"Hairy-Ball Theorem\" (every head of hair has a cowlick), every continuous vector field on $\\mathbf{S}^{2}$ (and also on $\\mathbf{S}^{n}$, for all even integers $n$ ) is zero at some point on the sphere. In fact, the field must vanish at some point of an arbitrarily selected, open hemisphere. (c) Find the zero points explicitly.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217(1)}\n\\end{center}\n\nFig. 13-9\\\\\n(a) The normal vector to $\\mathbf{S}^{2}$ is $\\omega=2 y^{1} d y^{1}+2 y^{2} d y^{2}+2 y^{3} d y^{3}$ and\n\n\n\\begin{gather*}\n\\sigma_{1} \\cdot \\frac{1}{2} \\omega=\\left(-y^{2}\\right)\\left(y^{1}\\right)+\\left(y^{1}\\right)\\left(y^{2}\\right)+(0)\\left(y^{3}\\right)=0 \\\\\n\\sigma_{2} \\cdot \\frac{1}{2} \\omega=\\left(y^{2}-y^{3}\\right)\\left(y^{1}\\right)-\\left(y^{1}+y^{3}\\right)\\left(y^{2}\\right)+\\left(y^{1}+y^{2}\\right)\\left(y^{3}\\right)  \\tag{b}\\\\\n=y^{1} y^{2}-y^{1} y^{3}-y^{1} y^{2}-y^{2} y^{3}+y^{1} y^{3}+y^{2} y^{3}=0\n\\end{gather*}\n\n\n(c) If $\\sigma_{1}=0,-y^{2}=y^{1}=0$ and $0^{2}=0^{2}+\\left(y^{3}\\right)^{2}=a^{2}$, or $y^{3}= \\pm a$. Thus, the zero points are $(0,0, \\pm a)$. For $\\sigma_{2}=0$,\n\n$$\ny^{2}-y^{3}=y^{1}+y^{3}=y^{1}+y^{2}=0 \\quad \\rightarrow \\quad y^{2}=y^{3}=-y^{1}\n$$\n\nand $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=a^{2}=3\\left(y^{1}\\right)^{2}$, or $y^{1}= \\pm a / \\sqrt{3}$. Hence, the zero points are $\\pm(a / \\sqrt{3}$, $-a / \\sqrt{3},-a / \\sqrt{3})$.", "13.18": "\\section*{VECTOR FIELDS ON MANIFOLDS}\n13.18 Consider a manifold whose coordinatization is not easily determined ( $\\mathbf{S O}(n)$, of Example 13.2(e), is such a manifold in $\\mathbf{R}^{n^{2}}$ ) for which, therefore, base vectors $\\mathbf{r}_{i}=E_{i}$ for $T_{p}(\\mathbf{M})$ are unavailable. Develop a reasonable definition of $T_{p}(\\mathbf{M})$ in this situation, which possesses the salient properties of a \"tangent space\" at point $p$.\n\nTo get an idea of what may be desirable, examine the case when the vectors $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{n}$ are available. Each tangent vector has the form $V=V^{i} \\mathbf{r}_{i}$, and when $V$ is the tangent vector of a curve $\\mathscr{C}$ on M-the image of $\\mathscr{C}^{\\prime}: x^{i}=x^{i}(t)$ in the coordinate space $\\mathbf{R}^{n}-$ then\n\n$$\nV=\\frac{d x^{i}}{d t} \\mathbf{r}_{i} \\quad \\text { or } \\quad V^{i}=\\frac{d x^{i}}{d t}\n$$\n\nThus $\\left(V^{i}\\right)$ is a direction vector. Recall that\n\n$$\n\\begin{gathered}\n\\mathbf{r}=\\mathbf{r}\\left(x^{1}, \\ldots, x^{n}\\right) \\equiv \\mathbf{r}\\left(y^{1}\\left(x^{1}, \\ldots, x^{n}\\right), \\dot{y}^{2}\\left(x^{1}, \\ldots, x^{n}\\right), \\ldots, y^{m}\\left(x^{1}, \\ldots, x^{n}\\right)\\right) \\\\\n\\mathbf{r}_{i}=\\left(\\frac{\\partial y^{1}}{\\partial x^{i}}, \\frac{\\partial y^{2}}{\\partial x^{i}}, \\ldots, \\frac{\\partial y^{m}}{\\partial x^{i}}\\right)\n\\end{gathered}\n$$\n\nwhence\n\nThus when we write $V^{i} \\mathbf{r}_{i}$ we are actually indicating the $m$ directional derivatives\n\n$$\nV^{i} \\frac{\\partial y^{j}}{\\partial x^{i}} \\equiv \\nabla y^{j} \\cdot V \\quad(1 \\leqq j \\leqq m)\n$$\n\nwith each $y^{j}\\left(x^{1}, x^{2}, \\ldots, x^{n}\\right)$ a $C^{\\infty}$ real-valued function (defined on $\\mathbf{M}$ if we identify the points of $\\mathbf{M}$ with their coordinates $\\left(x^{i}\\right)$ in $\\mathbf{R}^{n}$ ). It is customary to let the directional derivative of a function $f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}$ in the direction $V$ be denoted\n\n$$\nV(f) \\equiv \\nabla f \\cdot V\n$$\n\nThus, each vector $V$ maps a differentiable, real-valued function $f$ to its directional derivative in the direction $V$. The properties of this mapping are immediate: If $f$ and $g$ denote any two differentiable functions from $\\mathbf{M}$ to $\\mathbf{R}$, with $f g$ denoting the ordinary product of two functions, and if $a$ and $b$ are two scalar constants, then\n\n$$\n\\begin{aligned}\n\\text { linearity } & V(a f+b g)=a V(f)+b V(g) \\\\\n\\text { Leibniz' rule } & V(f g)=V(f) g+f V(g)\n\\end{aligned}\n$$\n\nWith this in mind, and armed with the knowledge that the directional derivatives of all functions on $\\mathbf{M}$ would be enough information to construct the basis $\\left\\{\\mathbf{r}_{i}\\right\\}$ when $\\mathbf{r}$ is known, we frame\n\nDefinition 9: By $\\mathbf{C}^{\\infty}(p)$ will be understood the real-valued $C^{\\infty}$ functions on $\\mathbf{U}_{p}$, such that any two functions that agree on some neighborhood of $p$ are identified.\n\nDefinition 10: The tangent space $T_{p}(\\mathbf{M})$ at $p$ is the set of all mappings $V_{p}: \\mathbf{C}^{\\infty}(p) \\rightarrow \\mathbf{R}$ that satisfy for all $a, b$ in $\\mathbf{R}$ and $f, g$ in $\\mathbf{C}^{\\infty}(p)$ the two conditions\\\\\n(i) $V_{p}(a f+b g)=a V_{p}(f)+b V_{p}(g)$\n\n(ii) $V_{p}(f g)=V_{p}(f) g+f V_{p}(g)$\n\nwith the vector-space operations in $T_{p}(\\mathbf{M})$ defined by\n\n$$\n\\begin{aligned}\n\\left(U_{p}+V_{p}\\right)(f) & \\equiv U_{p}(f)+V_{p}(f) \\\\\n\\left(a V_{p}\\right)(f) & \\equiv a V_{p}(f)\n\\end{aligned}\n$$\n\nAny $V_{p}$ in $T_{p}(\\mathbf{M})$ will be called a tangent vector to $\\mathbf{M}$ at $p$. This definition has the advantage not only of dispensing with coordinates, but of enabling one to extend naturally a mapping $F: \\mathbf{M} \\rightarrow \\mathbf{N}$ (from one manifold to another) to a mapping $F_{*}: T_{p}(\\mathbf{M}) \\rightarrow T_{p^{\\prime}}(\\mathbf{N})$ at each point $p$ in $\\mathbf{M}$, where $p^{\\prime}=F(p). Such an extension cannot be accomplished using the more elementary definition.\n\nRemark 2: The vectors of $T_{p}(\\mathbf{M})$ as originally defined, if regarded as mappings on $\\mathbf{C}^{\\infty}(p)$, are members of the abstract $T_{p}(\\mathbf{M})$ (Definition 10). In more advanced treatments it is shown that the reverse is true and that $\\operatorname{dim} T_{p}(\\mathbf{M})=\\operatorname{dim} \\mathbf{M}=n$. Hence, the two approaches to tangent spaces are equivalent.", "13.19": "\\section*{TENSOR FIELDS}\n13.19 Show that tensor fields always have the property of being bilinear with respect to scalar functions (as well as to scalar constants), unlike differential operators.\n\nWe must show that for any scalar function $f$ on $\\mathbf{M}$ and any tensor $T$ of type $\\left(\\begin{array}{c}0 \\\\ r\\end{array}\\right)$,\n\n$$\nT\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right)=f T\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n$$\n\nThis is true, since it is true at each point $p$ of $\\mathbf{M}$ :\n\n$$\n\\begin{aligned}\nT_{p}\\left(V_{1}, \\ldots, f V_{i}, \\ldots, V_{r}\\right) & \\equiv T\\left(V_{1 p}, \\ldots, f(p) V_{i p}, \\ldots, V_{r p}\\right)=f(p) T\\left(V_{1 p}, \\ldots, V_{i p}, \\ldots, V_{r p}\\right) \\\\\n& =f T_{p}\\left(V_{1}, \\ldots, V_{i}, \\ldots, V_{r}\\right)\n\\end{aligned}\n$$", "13.20": "\\section*{TENSOR FIELDS}\n13.20 Show how to interpret the tangent vector to a curve on a surface $S$ as a (contravariant) tensor of type $\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)$.\n\nLet $\\mathbf{c}=\\mathbf{c}(t)$ be a given curve on $\\mathbf{M}=\\mathbf{S}$, with\n\n$$\n\\mathbf{c}_{*}(t)=\\frac{d \\mathbf{c}}{d t}=\\frac{\\partial \\mathbf{y}}{\\partial x^{i}} \\frac{d x^{i}}{d t}\n$$\n\nDefine for any one-form $\\omega=a_{i} d z^{i}$ the linear mapping from $T^{*}(\\mathbf{M})$ to $\\mathbf{R}$ :\n\n$$\nT(\\omega)=a_{i} \\frac{d x^{i}}{d t} \\equiv \\omega\\left(\\frac{d \\mathbf{x}}{d t}\\right)\n$$\n\nUnder the standard basis $\\left\\{d z^{1}, d z^{2}, \\ldots, d z^{n}\\right\\}$ of $T_{p}^{*}(\\mathbf{M})$, and with $\\omega=d z^{i} \\equiv \\delta_{j}^{i} d z^{i}$,\n\n$$\nT^{i}=T\\left(d z^{i}\\right)=\\delta_{j}^{i} \\frac{d x^{j}}{d t}=\\frac{d x^{i}}{d t}\n$$\n\n(We saw earlier that the $d x^{i} / d t$ were contravariant components.)", "13.21": "\\section*{TENSOR FIELDS}\n13.21 Show how to interpret the gradient of a function as a tensor of type $\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$.\n\nLet $f$ have gradient $\\nabla f \\equiv\\left(\\partial f / \\partial x^{i}\\right)$. Define the linear mapping\n\n$$\nT(V)=V^{i} \\frac{\\partial f}{\\partial x^{i}} \\quad\\left(\\frac{\\partial f}{\\partial x^{i}} \\text { fixed }\\right)\n$$\n\nUse the basis $\\left\\{E_{1}, E_{2}, \\ldots, E_{n}\\right\\}$ for $T_{p}(\\mathbf{M})$; then with $V=E_{i} \\equiv \\delta_{i}^{j} E_{j}$,\n\n$$\nT_{i}=T\\left(E_{i}\\right)=\\delta_{i}^{j} \\frac{\\partial f}{\\partial x^{j}}=\\frac{\\partial f}{\\partial x^{i}}\n$$"}, {"1.15": {"question": "```latex\n1.15 Write out the expression $a_{i} b_{i}(n=6)$ in full.\n```", "answer": "$1.15 a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}+a_{4} b_{4}+a_{5} b_{5}+a_{6} b_{6}$"}, "1.16": {"question": "```latex\n1.16 Write out the expression $R_{j k i}^{i} \\quad(n=4)$ in full. Which are free and which are dummy indices? How many summations are there?\n```", "answer": "1.16 $R_{j k 1}^{1}+R_{j k 2}^{2}+R_{j k 3}^{3}+R_{j k 4}^{4}$. The index $i$ is a dummy index, while $j$ and $k$ are free indices; there are 16 summations."}, "1.17": {"question": "```latex\n1.17 Evaluate $\\delta_{j}^{i} x_{i}$ ( $n$ arbitrary).\n```", "answer": "$1.17 x_{j}$"}, "1.18": {"question": "```latex\n1.18 For $n$ arbitrary, evaluate (a) $\\delta_{i i}$, (b) $\\delta_{i j} \\delta_{i j}$, (c) $\\delta_{i j} \\delta_{k}^{j} c_{i k}$.\n```", "answer": "1.18 (a) $n$; (b) $\\delta_{i j} \\delta_{i j}=\\delta_{i i}=n ;(c) \\delta_{i j} c_{i j}=c_{i i}=c_{11}+c_{22}+c_{33}+\\cdots+c_{n n}$"}, "1.19": {"question": "```latex\n1.19 Use the summation convention to indicate $a_{13} b_{13}+a_{23} b_{23}+a_{33} b_{33}$, and state the value of $n$.\n```", "answer": "$1.19 a_{i 3} b_{i 3} \\quad(n=3)$"}, "1.20": {"question": "```latex\n1.20 Use the summation convention to indicate\n\n$$\na_{11}\\left(x_{1}\\right)^{2}+a_{12} x_{1} x_{2}+a_{13} x_{1} x_{3}+a_{21} x_{2} x_{1}+a_{22}\\left(x_{2}\\right)^{2}+a_{23} x_{2} x_{3}+a_{31} x_{3} x_{1}+a_{32} x_{3} x_{2}+a_{33}\\left(x_{3}\\right)^{2}\n$$\n```", "answer": "$1.20 \\quad a_{i j} x_{i} x_{j} \\quad(n=3)$"}, "1.21": {"question": "```latex\n1.21 Use the summation convention and free subscripts to indicate the following linear system, stating the value of $n$ :\n\n$$\n\\begin{aligned}\n& y_{1}=c_{11} x_{1}+c_{12} x_{2} \\\\\n& y_{2}=c_{21} x_{1}+c_{22} x_{2}\n\\end{aligned}\n$$\n```", "answer": "$1.21 y_{i}=c_{i j} x_{j} \\quad(n=2)$"}, "1.22": {"question": "```latex\n1.22 Find the following partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}\\right) \\quad(k=1,2,3)\n$$\n```", "answer": "$1.22 a_{1 k} \\quad(k=2,3)$"}, "1.23": {"question": "```latex\n1.23 Use the Kronecker delta to calculate the partial derivative if the $a_{i j}$ are constants:\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)\n$$\n```", "answer": "$1.23 \\frac{\\partial}{\\partial x_{k}}\\left(a_{i j} x_{j}\\right)=a_{i j} \\frac{\\partial}{\\partial x_{k}}\\left(x_{j}\\right)=a_{i j} \\delta_{j k}=a_{i k}$"}, "1.24": {"question": "```latex\n1.24 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{k}}\\left[a_{i j} x_{i}\\left(x_{j}\\right)^{2}\\right]\n$$\n\nwhere the $a_{i j}$ are constants such that $a_{i j}=a_{j i}$.\n```", "answer": "$1.24 a_{i k}\\left[\\left(x_{i}\\right)^{2}+2 x_{i} x_{k}\\right]$ [not summed on $k$ ]"}, "1.25": {"question": "```latex\n1.25 Calculate\n\n$$\n\\frac{\\partial}{\\partial x_{l}}\\left(a_{i j k} x_{i} x_{j} x_{k}\\right)\n$$\n\nwhere the $a_{i j k}$ are constants.\n```", "answer": "$1.25\\left(a_{l i j}+a_{i l j}+a_{i j l}\\right) x_{i} x_{j}$"}, "1.26": {"question": "```latex\n1.26 Solve Problem 1.11 without the symmetry condition on $a_{i j}$.\n```", "answer": "$1.26 a_{k l}+a_{l k}$"}, "1.27": {"question": "```latex\n1.27 Evaluate: (a) $b_{j}^{i} y_{i}$ if $y_{i}=T_{i}^{j j}$, (b) $a_{i j} y_{j}$ if $y_{i}=b_{i j} x_{j}$, (c) $a_{i j k} y_{i} y_{j} y_{k}$ if $y_{i}=b_{i j} x_{j}$.\n```", "answer": "1.27 (a) $b_{j}^{i} T_{i}^{r r}$; (b) $a_{i j} b_{j r} x_{r}$; (c) $a_{i j k} b_{i r} b_{j s} b_{k t} x_{r} x_{s} x_{t}$"}, "1.28": {"question": "```latex\n1.28 If $\\varepsilon_{i}=1$ for all $i$, prove that\n\n(a) $\\left(a_{1}+a_{2}+\\cdots+a_{n}\\right)^{2} \\equiv \\varepsilon_{i} \\varepsilon_{j} a_{i} a_{j}$\n\n(b) $a_{i}\\left(1+x_{i}\\right) \\equiv a_{i} \\varepsilon_{i}+a_{i} x_{i}$\n\n(c) $a_{i j}\\left(x_{i}+x_{j}\\right) \\equiv 2 a_{i j} \\varepsilon_{i} x_{j}$ if $a_{i j}=a_{j i}$\n```", "answer": "1.28 (c) $a_{i j}\\left(x_{i}+x_{j}\\right)=a_{i j}\\left(\\varepsilon_{j} x_{i}+\\varepsilon_{i} x_{j}\\right)=a_{i j} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=a_{j i} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=2 a_{i j} \\varepsilon_{i} x_{j}$"}, "2.24": {"question": "2.24 Display the matrices $(a)\\left[u^{i j}\\right]_{35},(b)\\left[u^{j i}\\right]_{35},(c)\\left[u^{i j}\\right]_{53},(d)\\left[\\delta_{j}^{i}\\right]_{36}$.", "answer": "not contained in text"}, "2.25": {"question": "2.25 Carry out the following matrix multiplications:\n\n$$\n\\text { (a) }\\left[\\begin{array}{rrr}\n3 & -1 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 2 & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n2\n\\end{array}\\right] \\quad \\text { (b) }\\left[\\begin{array}{rr}\n3 & -1 \\\\\n2 & 0\n\\end{array}\\right]\\left[\\begin{array}{rrr}\n1 & 1 & -1 \\\\\n2 & 1 & 1\n\\end{array}\\right]\n$$", "answer": "2.25\n\n(a) $\\left[\\begin{array}{l}5 \\\\ 0 \\\\ 5\\end{array}\\right]$\n\n(b) $\\left[\\begin{array}{lll}1 & 2 & -4 \\\\ 2 & 2 & -2\\end{array}\\right]$"}, "2.26": {"question": "2.26 Prove by the product rule and by use of the summation convention the associative law for matrices:\n\n$$\n(A B) C=A(B C)\n$$\n\nwhere $A \\equiv\\left(a_{i j}\\right), B \\equiv\\left(b_{i j}\\right)$, and $C \\equiv\\left(c_{i j}\\right)$ are arbitrary matrices, but compatible for multiplication.", "answer": "not contained in text"}, "2.27": {"question": "2.27 Prove: (a) if $A$ and $B$ are symmetric matrices and if $A B=B A=C$, then $C$ is symmetric; (b) if $A$ and $B$ are skew-symmetric and if $A B=-B A=C$, then $C$ is skew-symmetric.", "answer": "1.27 (a) $b_{j}^{i} T_{i}^{r r}$; (b) $a_{i j} b_{j r} x_{r}$; (c) $a_{i j k} b_{i r} b_{j s} b_{k t} x_{r} x_{s} x_{t}$"}, "2.28": {"question": "2.28 Prove that the product of two orthogonal matrices is orthogonal.", "answer": "1.28 (c) $a_{i j}\\left(x_{i}+x_{j}\\right)=a_{i j}\\left(\\varepsilon_{j} x_{i}+\\varepsilon_{i} x_{j}\\right)=a_{i j} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=a_{j i} \\varepsilon_{j} x_{i}+a_{i j} \\varepsilon_{i} x_{j}=2 a_{i j} \\varepsilon_{i} x_{j}$"}, "2.29": {"question": "2.29 Evaluate the determinants\n\n$$\n\\text { (a) }\\left|\\begin{array}{rr}\n3 & -2 \\\\\n1 & 5\n\\end{array}\\right| \\quad \\text { (b) }\\left|\\begin{array}{rrr}\n2 & 1 & -1 \\\\\n3 & 0 & 1 \\\\\n1 & -1 & 2\n\\end{array}\\right| \\quad \\text { (c) }\\left|\\begin{array}{rrrrr}\n-1 & 1 & -1 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 1 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n-1 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 0 & 0 & 0\n\\end{array}\\right|\n$$", "answer": "2.29 (a) $17 ;(b) 0 ;(c)-1$"}, "2.30": {"question": "2.30 In the Laplace expansion of the fourth-order determinant $\\left|a_{i j}\\right|$, the six-term summation $e_{2 i j k} a_{12} a_{2 i} a_{3 j} a_{4 k}$ appears. (a) Write out this sum explicitly, then (b) represent it as a third-order determinant.", "answer": "2.30 (a) $-a_{12} a_{21} a_{33} a_{44}+a_{12} a_{21} a_{34} a_{43}+a_{12} a_{23} a_{31} a_{44}-a_{12} a_{23} a_{34} a_{41}-a_{12} a_{24} a_{31} a_{43}+a_{12} a_{24} a_{33} a_{41}$\n\n(b) $-a_{12}\\left|\\begin{array}{lll}a_{21} & a_{23} & a_{24} \\\\ a_{31} & a_{33} & a_{34} \\\\ a_{41} & a_{43} & a_{44}\\end{array}\\right| \\equiv a_{12} A_{12}$"}, "2.31": {"question": "2.31 Prove that if a matrix has two rows the same, its determinant is zero. (Hint: First show that interchanging any two subscripts reverses the sign of the permutation symbol.)", "answer": "not contained in text"}, "2.32": {"question": "2.32 Calculate the inverse of\n\n$$\n\\text { (a) }\\left[\\begin{array}{ll}\n3 & 1 \\\\\n5 & 2\n\\end{array}\\right] \\quad(b)\\left[\\begin{array}{rrr}\n0 & 1 & 2 \\\\\n1 & -1 & 0 \\\\\n2 & 1 & -1\n\\end{array}\\right]\n$$", "answer": "2.32 (a) $\\left[\\begin{array}{rr}2 & -1 \\\\ -5 & 3\\end{array}\\right] \\quad$ (b) $\\frac{1}{7}\\left[\\begin{array}{rrr}1 & 3 & 2 \\\\ 1 & -4 & 2 \\\\ 3 & 2 & -1\\end{array}\\right]$"}, "2.33": {"question": "2.33 (a) Verify the following formulas for the permutation symbols $e_{i j}$ and $e_{i j k}$ (for distinct values of the indices only):\n\n$$\ne_{i j}=\\frac{j-i}{|j-i|} \\quad e_{i j k}=\\frac{(j-i)(k-i)(k-j)}{|j-i||k-i||k-j|}\n$$\n\n(b) Prove the general formula:\n\n$$\ne_{i_{1} i_{2} \\cdots i_{n}}=\\frac{\\left(i_{2}-i_{1}\\right)\\left(i_{3}-i_{1}\\right) \\cdots\\left(i_{n}-i_{1}\\right)\\left(i_{3}-i_{2}\\right) \\cdots\\left(i_{n}-i_{2}\\right) \\cdots\\left(i_{n}-i_{n-1}\\right)}{\\left|i_{2}-i_{1}\\right|\\left|i_{3}-i_{1}\\right| \\cdots\\left|i_{n}-i_{1}\\right|\\left|i_{3}-i_{2}\\right| \\cdots\\left|i_{n}-i_{2}\\right| \\cdots\\left|i_{n}-i_{n-1}\\right|} \\equiv \\prod_{p>q} \\frac{i_{p}-i_{q}}{\\left|i_{p}-i_{q}\\right|}\n$$", "answer": "2.33 One need verify only that (i) interchanging a pair of consecutive indices changes the sign of a single factor in the product; (ii)\n\n$$\n\\prod_{p>q} \\frac{p-q}{|p-q|}=\\prod 1=1\n$$"}, "2.34": {"question": "2.34 Calculate the angle between the $\\mathbf{R}^{6}$-vectors $\\mathbf{x}=(3,-1,0,1,2,-3)$ and $\\mathbf{y}=(-2,1,0,1,0,0)$.", "answer": "not contained in text"}, "2.35": {"question": "2.35 Find two linearly independent vectors in $\\mathbf{R}^{3}$ which are orthogonal to the vector $(3,-2,1)$.", "answer": "2.35 One pair are $(2,3,0)$ and $(-3,-2,5)$."}, "2.36": {"question": "2.36 Solve for $x$ and $y$ by use of matrices:\n\n$$\n\\begin{aligned}\n& 3 x-4 y=-23 \\\\\n& 5 x+3 y=10\n\\end{aligned}\n$$", "answer": "$2.36\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]=\\left[\\begin{array}{r}-1 \\\\ 5\\end{array}\\right]$"}, "2.37": {"question": "2.37 Write out the quadratic form in $\\mathbf{R}^{3}$ represented by $Q=\\mathbf{x}^{T} A \\mathbf{x}$, where\n\n$$\nA=\\left[\\begin{array}{rrr}\n1 & 4 & 3 \\\\\n4 & 2 & 0 \\\\\n3 & 0 & -1\n\\end{array}\\right]\n$$", "answer": "$2.37 \\quad Q=x_{1}^{2}+2 x_{2}^{2}-x_{3}^{2}+8 x_{1} x_{2}+6 x_{1} x_{3}$"}, "2.38": {"question": "2.38 Represent with a symmetric matrix $A$ the quadratic form in $\\mathbf{R}^{4}$\n\n$$\nQ=-3 x_{1}^{2}-x_{2}^{2}+x_{3}^{2}-x_{1} x_{2}-x_{1} x_{3}+6 x_{1} x_{4}\n$$", "answer": "$2.38 \\quad A=\\left[\\begin{array}{rrrr}-3 & -\\frac{1}{2} & -\\frac{1}{2} & 3 \\\\ -\\frac{1}{2} & -1 & 0 & 0 \\\\ -\\frac{1}{2} & 0 & 1 & 0 \\\\ 3 & 0 & 0 & 0\\end{array}\\right]$"}, "2.39": {"question": "2.39 Given the hyperplane $c_{r} x_{r}=1$, how do the coefficients $c_{i}$ transform under a change of coordinates $\\bar{x}_{i}=a_{i j} x_{j}$ ?", "answer": "$2.39 \\quad \\bar{c}_{i}=c_{r} b_{r i}$, where $\\left(b_{i j}\\right)=\\left(a_{i j}\\right)^{-1}$."}, "2.40": {"question": "2.40 Calculate the $g_{i j}$ for the distance formula (2.14) in a barred coordinate system defined by $\\overline{\\mathbf{x}}=A \\mathbf{x}$, with\n\n$$\nA=\\left[\\begin{array}{rr}\n1 & -2 \\\\\n2 & 3\n\\end{array}\\right]\n$$", "answer": "$2.40 \\quad g_{11}=13 / 49, g_{12}=g_{21}=4 / 49, g_{22}=5 / 49$"}, "2.41": {"question": "2.41 Test the distance formula of Problem 2.40 on the pair of points whose unbarred coordinates are $(2,-1)$ and $(2,-4)$.", "answer": "$2.41 \\quad d(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}})=3=d(\\mathbf{x}, \\mathbf{y})$"}, "2.42": {"question": "2.42 (a) Show that for independent functions $\\bar{x}_{i}=\\bar{x}_{i}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$,\n\n\\begin{equation*}\n\\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=\\delta_{j}^{i} \\tag{1}\n\\end{equation*}\n\n(b) Take the partial derivative with respect to $x_{k}$ of (1) to establish the formula\n\n\\begin{equation*}\n\\frac{\\partial^{2} \\bar{x}_{i}}{\\partial x_{k} \\partial x_{r}} \\frac{\\partial x_{r}}{\\partial \\bar{x}_{j}}=-\\frac{\\partial^{2} x_{r}}{\\partial \\bar{x}_{s} \\partial \\bar{x}_{j}} \\frac{\\partial \\bar{x}_{i}}{\\partial x_{r}} \\frac{\\partial \\bar{x}_{s}}{\\partial x_{k}} \\tag{2}\n\\end{equation*}", "answer": "not contained in text"}, "3.23": {"question": "Supplementary Problems\n\n3.23 Suppose that the following transformation connects the $\\left(x^{i}\\right)$ and $\\left(\\bar{x}^{i}\\right)$ coordinate systems:\n\n$$\n\\mathscr{T}:\\left\\{\\begin{array}{l}\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\\\\n\\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n\\end{array}\\right.\n$$\n\n(a) Calculate the Jacobian matrix $J$ and the Jacobian $\\mathscr{J}$. Show that $\\mathscr{J} \\neq 0$ over all of $\\mathbf{R}^{2}$. (b) Give equations for $\\mathscr{T}^{-1}$. (c) Calculate the Jacobian matrix $\\bar{J}$ of $\\mathscr{T}^{-1}$ and compare with $J^{-1}$.", "answer": "3.23 (a) $\\mathscr{I}=-2 \\exp \\left(2 x^{1}\\right)<0$\n\n(b) $\\mathscr{T}^{-1}:\\left\\{\\begin{array}{l}x^{1}=\\frac{1}{2} \\ln \\left(\\bar{x}^{-1} \\bar{x}^{2}\\right) \\\\ x^{2}=\\frac{1}{2} \\ln \\left(\\bar{x}^{1} / \\bar{x}^{2}\\right)\\end{array} \\quad\\left(\\bar{x}^{1}, \\bar{x}^{2}>0\\right)\\right.$\n\n(c) $\\bar{J}=\\left[\\begin{array}{rr}1 / 2 \\bar{x}^{1} & 1 / 2 \\bar{x}^{2} \\\\ 1 / 2 \\bar{x}^{1} & -1 / 2 \\bar{x}^{2}\\end{array}\\right]=\\left[\\begin{array}{rr}\\exp \\left(x^{1}+x^{2}\\right) & \\exp \\left(x^{1}+x^{2}\\right) \\\\ \\exp \\left(x^{1}-x^{2}\\right) & -\\exp \\left(x^{1}-x^{2}\\right)\\end{array}\\right]^{-1}$"}, "3.24": {"question": "Supplementary Problems\n\n3.24 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector, and if the components $S_{i j} \\equiv T_{i} T_{j}+T_{j} T_{i}$ are defined in each coordinate system, then $\\left(S_{i j}\\right)$ is a symmetric covariant tensor. (Compare Problem 3.12.)", "answer": "not contained in text"}, "3.25": {"question": "Supplementary Problems\n\n3.25 Prove that if $\\left(T_{i}\\right)$ defines a covariant vector and, in each coordinate system, we define\n\n$$\n\\frac{\\partial T_{i}}{\\partial x^{j}}-\\frac{\\partial T_{j}}{\\partial x^{i}}=T_{i j}\n$$\n\nthen $\\left(T_{i j}\\right)$ is a skew-symmetric covariant tensor of the second order. [Hint: Model the proof on Problem 3.3.]", "answer": "not contained in text"}, "3.26": {"question": "Supplementary Problems\n\n3.26 Convert the partial differential equation\n\n$$\ny \\frac{\\partial f}{\\partial x}=x \\frac{\\partial f}{\\partial y}\n$$\n\nto polar form (making use of the fact that $\\nabla f$ is a covariant vector), and solve for $f(x, y)$.", "answer": "3.26 $\\frac{\\partial \\bar{f}}{\\partial \\theta}=0$, so that $f(x, y)=\\bar{f}(r)=\\bar{f}\\left(\\sqrt{x^{2}+y^{2}}\\right)=g\\left(x^{2}+y^{2}\\right)$."}, "3.27": {"question": "Supplementary Problems\n\n3.27 Show that the quadratic form $Q=g_{i j} x^{i} x^{j}$ is an affine invariant provided $\\left(g_{i j}\\right)$ is a covariant affine tensor. [Converse of Problem 3.21(a).]", "answer": "not contained in text"}, "3.28": {"question": "Supplementary Problems\n\n3.28 Prove that the partial derivatives of a contravariant vector $\\left(T^{i}\\right)$ define a mixed affine tensor of order two. [Hint: Compare Problem 2.23.]", "answer": "not contained in text"}, "3.29": {"question": "Supplementary Problems\n\n3.29 Prove that the Kronecker delta $\\left(\\delta_{j}^{i}\\right)$, uniformly defined in all coordinate systems, is a mixed tensor of order two.", "answer": "3.29 $\\delta_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=\\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{j}}=\\delta^{i}{ }_{j}=\\bar{\\delta}^{i}{ }_{j}$"}, "3.30": {"question": "Supplementary Problems\n\n3.30 Show that the permutation symbol $\\left(e_{i j}\\right)$ of order two, uniformly defined in all coordinate systems, is not-Problem 3.20 notwithstanding-covariant under arbitrary coordinate changes. [Hint: Use $x^{1}=$ $\\bar{x}^{1} \\bar{x}^{2}, x^{2}=\\bar{x}^{2}$, at the point $\\left.\\left(\\bar{x}^{i}\\right)=(1,2).\\right]$", "answer": "3.30 The inverse Jacobian matrix at $(1,2)$ is\n\n$$\n\\bar{J}=\\left[\\begin{array}{cc}\n\\bar{x}^{2} & \\bar{x}^{1} \\\\\n0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nBy Problem 3.14(a), covariance of the matrix\n\nwould imply the matrix equation\n\n$$\nE \\equiv\\left[e_{i j}\\right]_{22}=\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\n$$\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n2 & 0 \\\\\n1 & 1\n\\end{array}\\right]\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]\\left[\\begin{array}{ll}\n2 & 1 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\nor\n\nwhich is patently false.\n\n$$\n\\left[\\begin{array}{rr}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right]=\\left[\\begin{array}{rr}\n0 & 2 \\\\\n-2 & 0\n\\end{array}\\right]\n$$"}, "3.31": {"question": "Supplementary Problems\n\n3.31 By use of (3.23), establish the familiar identity for the vector product of three vectors,\n\n$$\n\\mathbf{u} \\times(\\mathbf{v} \\times \\mathbf{w})=(\\mathbf{u w}) \\mathbf{v}-(\\mathbf{u v}) \\mathbf{w}\n$$\n\nor, in coordinate form,\n\n$$\ne_{i j k} u_{j}\\left(e_{k r s} v_{r} w_{s}\\right)=\\left(u_{j} w_{j}\\right) v_{i}-\\left(u_{j} v_{j}\\right) w_{i}\n$$", "answer": "not contained in text"}, "3.32": {"question": "Supplementary Problems\n\n3.32 (a) Show that if $\\left(T_{j}^{i}\\right)$ is a mixed tensor, then $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ is not generally a tensor. (b) Show that a mixed tensor of order two, symmetric in a given coordinate system, will transform as a symmetric tensor if the Jacobian matrix is orthogonal.", "answer": "3.32 (a) $\\left(T_{j}^{i}+T_{i}^{j}\\right)$ represents a tensor if and only if\n\n$$\n\\left(T_{s}^{r}+T_{r}^{s}\\right) \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}=T_{s}^{r} \\frac{\\partial \\bar{x}^{i}}{\\partial x^{r}} \\frac{\\partial x^{s}}{\\partial \\bar{x}^{j}}+T_{r}^{s} \\frac{\\partial \\bar{x}^{j}}{\\partial x^{s}} \\frac{\\partial x^{r}}{\\partial \\bar{x}^{i}}\n$$\n\nwhich requires that $J T \\bar{J}=\\bar{J}^{T} T J^{T}$. This last relation, in turn, generally requires that $\\bar{J}=J^{T}$; i.e., $J$ must be an orthogonal matrix.\n\n(b) $\\bar{T}=J T \\bar{J}$, so that $\\bar{T}^{T}=\\bar{T}$ if $\\bar{J}=J^{T}$."}, "3.33": {"question": "Supplementary Problems\n\n3.33 Prove: (a) If $\\left(T_{j}^{i}\\right)$ is a mixed tensor of order two, $T_{i}^{i}$ is an invariant; $(b)$ if $\\left(S_{j k}^{i}\\right)$ and $\\left(T^{i}\\right)$ are tensors of the type and order indicated, $S_{j r}^{r} T^{j}$ is an invariant.", "answer": "not contained in text"}, "3.34": {"question": "Supplementary Problems\n\n3.34 If $\\mathbf{T} \\equiv\\left(T_{m l}^{i j k}\\right)$ is a tensor, contravariant of order 3 and covariant of order 2 , show that $\\mathbf{S} \\equiv\\left(T_{k j}^{i j k}\\right)$ is a contravariant vector.", "answer": "not contained in text"}, "3.35": {"question": "Supplementary Problems\n\n3.35 Show that the derivative, $d \\mathbf{T} / d t$, of the tangent vector $\\mathbf{T} \\equiv\\left(T^{i}\\right)=\\left(d x^{i} / d t\\right)$ to a curve $x^{i}=x^{i}(t)$ is a contravariant affine tensor. Is it a cartesian tensor?", "answer": "3.35 As $\\mathbf{T}$ is a tensor (Example 3.4), it is an affine tensor: $\\bar{T}^{i}=a_{r}^{i} T^{r}$. Thus,\n\n$$\n\\frac{d \\bar{T}^{i}}{d t}=a_{r}^{i} \\frac{d T^{r}}{d t}\n$$\n\nshowing $d \\mathbf{T} / d t$ also to be an affine tensor. Any affine tensor is a fortiori a cartesian tensor."}, "3.36": {"question": "Supplementary Problems\n\n3.36 (a) Use the theory of tensors to prove that the scalar product $\\mathbf{u v} \\equiv u_{i} v_{i}$ of two vectors $\\mathbf{u}=\\left(u_{i}\\right)$ and $\\mathbf{v}=\\left(v_{i}\\right)$ is a cartesian invariant. (b) Is uv an affine invariant?", "answer": "not contained in text"}, "4.17": {"question": "4.17 If $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$ are contravariant vectors, verify that $\\left(2 U^{i}+3 V^{i}\\right)$ is also a contravariant vector.", "answer": "not contained in text"}, "4.18": {"question": "4.18 Verify that the outer product of a contravariant vector and a covariant vector is a mixed tensor of order two.", "answer": "not contained in text"}, "4.19": {"question": "4.19 How many potentially different mixed tensors of order two can be defined by taking the outer product of $\\mathbf{S}=\\left(S_{k}^{i j}\\right)$ and $\\mathbf{T}=\\left(T_{j k}^{i}\\right)$, then contracting twice?", "answer": "4.19 Write $[\\mathbf{S T}]=\\left(U_{l m n}^{i j k}\\right)$. There are $\\left(\\begin{array}{c}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations for the contraction indices $u$ and $v$ among the contravariant indices, and, for each of these, $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ways of choosing locations among the covariant indices. A given quartet of locations can be filled in 2 inequivalent ways. Thus, the desired number is\n\n$$\n\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot\\left(\\begin{array}{l}\n3 \\\\\n2\n\\end{array}\\right) \\cdot 2=18\n$$"}, "4.20": {"question": "4.20 Show that if $T_{k l}^{i j}$ are tensor components, $T_{i j}^{i j}$ is an invariant.", "answer": "not contained in text"}, "4.21": {"question": "4.21 Prove that if $T_{j k l}^{i} U^{j} \\equiv S_{k l}^{i}$ are components of a tensor for any contravariant vector $\\left(U^{j}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Apply the Quotient Theorem to $\\left(M_{k l j}^{i}\\right) \\equiv\\left(T_{j k l}^{i}\\right)$. More generally, the Quotient Theorem is valid for all choices of the inner product.]", "answer": "not contained in text"}, "4.22": {"question": "4.22 Prove that if $T_{j k l}^{i} S^{k l} \\equiv U_{j}^{i}$ are tensor components for arbitrary contravariant tensors $\\left(S^{k l}\\right)$, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the indicated type. [Hint: Follow Problem 4.9.]", "answer": "not contained in text"}, "4.23": {"question": "4.23 Prove that if $T_{j k l}^{i} U^{k} U^{l} \\equiv V_{j}^{i}$ are components of a tensor for an arbitrary contravariant vector $\\left(U^{i}\\right)$, and if $\\left(T_{j k l}^{i}\\right)$ is symmetric in the last two lower indices in all coordinate systems, then $\\left(T_{j k l}^{i}\\right)$ is a tensor of the type indicated.", "answer": "4.23 First, use the device of Problem 4.11 to establish that $T_{j k l}^{i} U^{k} V^{l}$ are tensor components for all $\\left(U^{i}\\right)$ and $\\left(V^{i}\\right)$; then apply the Quotient Theorem twice."}, "4.24": {"question": "4.24 Show that Theorem 4.3 and Corollary 4.4 are equivalent.", "answer": "not contained in text"}, "4.25": {"question": "4.25 Prove the assertion of Example 4.7.", "answer": "not contained in text"}, "4.26": {"question": "4.26 Prove that if an invariant $E$ can be expressed as the inner product of vectors $\\left(U_{i}\\right)$ and $\\left(V^{i}\\right)$ in one coordinate system, then $E$ has that representation in any coordinate system.", "answer": "not contained in text"}, "5.21": {"question": "Supplementary Problems\n\n5.21 Using the Euclidean metric for polar coordinates, compute the length of arc for the curve\n\n$$\n\\mathscr{C}: x^{1}=2 a \\cos t, \\quad x^{2}=t \\quad(0 \\leqq t \\leqq \\pi / 2)\n$$\n\nand interpret geometrically.", "answer": "5.21 $L=a \\pi$; semicircle of radius $a$."}, "5.22": {"question": "5.22 Is the form $Q\\left(u^{1}, u^{2}, u^{3}\\right) \\equiv 8\\left(u^{1}\\right)^{2}+\\left(u^{2}\\right)^{2}-6 u^{1} u^{3}+\\left(u^{3}\\right)^{2}$ positive definite?", "answer": "5.22 No: $Q(1,0,3)=-1$."}, "5.23": {"question": "5.23 Using the metric\n\n$$\nG=\\left[\\begin{array}{rrc}\n12 & 4 & 0 \\\\\n4 & 1 & 1 \\\\\n0 & 1 & \\left(x^{1}\\right)^{2}\n\\end{array}\\right]\n$$\n\ncalculate the length of the curve given by $x^{1}=3-t, x^{2}=6 t+3, x^{3}=\\ln t$, where $1 \\leqq t \\leqq e$.", "answer": "5.23 $L=2+e$"}, "5.24": {"question": "5.24 A draftsman calculated several distances between points on his drawing using a set of vertical lines and his $\\mathrm{T}$-square. He obtained the distance from $(1,2)$ to $(4,6)$ the usual way:\n\n$$\n\\sqrt{(4-1)^{2}+(6-2)^{2}}=5\n$$\n\nThen he noticed his T-square was out several degrees, throwing off all measurements. An accurate reading showed his T-square measured $95.8^{\\circ}$. Find, to three decimal places, the error committed in his calculations for the answer 5 obtained above. [Hint: Use Problem 5.9 in the special case $x_{1}^{3}=x_{2}^{3}=0$, with $\\alpha=95.8^{\\circ}$.]", "answer": "5.24 The true distance formula, $\\overline{P_{1} P_{2}}=\\sqrt{\\left(x_{1}^{1}-x_{2}^{1}\\right)^{2}+\\left(x_{1}^{2}-x_{2}^{2}\\right)^{2}-0.2021125\\left(x_{1}^{1}-x_{2}^{1}\\right)\\left(x_{1}^{2}-x_{2}^{2}\\right)}$, yields 4.751. for an error of +0.249 ."}, "5.25": {"question": "5.25 In curvilinear coordinates $\\left(x^{i}\\right)$, show that the contravariant vectors\n\n$$\n\\mathbf{U}=\\left(-x^{1} / x^{2}, 1,0\\right) \\quad \\mathbf{V}=\\left(1 / x^{2}, 0,0\\right)\n$$\n\nare an orthonormal pair, if $\\left(x^{i}\\right)$ is related to rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=x^{2} \\quad \\bar{x}^{2}=x^{3} \\quad \\bar{x}^{3}=x^{1} x^{2}\n$$\n\nwhere $x^{2} \\neq 0$.", "answer": "$$\nG=\\left[\\begin{array}{ccc}\n\\left(x^{2}\\right)^{2} & x^{1} x^{2} & 0 \\\\\nx^{1} x^{2} & 1+\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n$$"}, "5.26": {"question": "5.26 Express in $\\left(x^{i}\\right)$ the covariant vectors associated with $\\mathbf{U}$ and $\\mathbf{V}$ of Problem 5.25.", "answer": "$5.26\\left(U_{i}\\right)=(0,1,0),\\left(V_{i}\\right)=\\left(x^{2}, x^{1}, 0\\right)$"}, "5.27": {"question": "5.27 Even though $\\left(g_{i j}\\right)$ may define a non-Euclidean metric, prove that the norm (5.10) still obeys the following \"Euclidean\" laws: $(a)$ the law of cosines, $(b)$ the Pythagorean theorem.", "answer": "5.27 (a) $\\|\\mathbf{U}+\\mathbf{V}\\|^{2}=(\\mathbf{U}+\\mathbf{V})^{2}=\\mathbf{U}^{2}+\\mathbf{V}^{2}+2 \\mathbf{U V}=\\|\\mathbf{U}\\|^{2}+\\left\\|\\mathbf{V}^{2}\\right\\|+2\\|\\mathbf{U}\\|\\|\\mathbf{V}\\| \\cos \\theta$\n\n(b) Take $\\theta=\\pi / 2$ in (a)."}, "5.28": {"question": "5.28 (a) Solve system (1) of Problem 5.18. (b) Does the solution found in (a) include all curves orthogonal to the given pseudo-helix? Explain.", "answer": "5.28 (a) $x^{2}=C \\exp \\left(-2 b x^{3} / a^{2}\\right)$ (a one-parameter family of spirals on the cylinder $x^{1}=a$ )\\\\\n(b) No: the curves of (a) have tangent field $\\mathbf{V}$ all along their length; but, for orthogonality, it is necessary only that the tangent at intersections with the pseudo-helix be V. For example, the curve $x^{2}=x^{3}$ on $x^{1}=a$ is also orthogonal to the pseudo-helix at the point $x^{2}=-a^{2} / 2 b, x^{3}=a^{4} / 4 b$."}, "5.29": {"question": "5.29 Find the family of orthogonal trajectories in polar coordinates for the family of spirals $x^{1}=c x^{2} \\quad(c=$ const.). [Hint: Parameterize the family as $x^{1}=c e^{t}, x^{2}=e^{t}$.]", "answer": "$5.29 \\quad x^{1}=d \\exp \\left(-\\left(x^{2}\\right)^{2} / 2\\right) \\quad(d=$ const. $)$"}, "5.30": {"question": "5.30 Find the condition for two curves, $z=f(\\theta)$ and $z=g(\\theta)$, on a right circular cylinder of radius $a$ to be orthogonal.", "answer": "5.30 $f^{\\prime}\\left(\\theta_{0}\\right) g^{\\prime}\\left(\\theta_{0}\\right)=-a^{2}$ at intersection points."}, "5.31": {"question": "5.31 Let $\\left(x^{i}\\right)$ be any coordinate system and $\\left(g_{i j}\\right)$ any positive definite metric tensor realized in that system. Define the coordinate axes as the curves $\\mathscr{C}_{\\alpha}: x^{i}=t \\delta_{\\alpha}^{i} \\quad(\\alpha=1,2, \\ldots n)$. Show that the angle $\\phi$ between the coordinate axes $\\mathscr{C}_{\\alpha}$ and $\\mathscr{C}_{\\beta}$ satisfies the relation\n\n$$\n\\cos \\phi=\\frac{g_{\\alpha \\beta}}{\\sqrt{g_{\\alpha \\alpha}} \\sqrt{g_{\\beta \\beta}}} \\quad \\text { (no sum) }\n$$\n\nand is thus distinct, in general, from the angle $\\theta$ of Problem 5.20.", "answer": "5.32 (a) $g^{i \\alpha}=\\lambda(\\alpha) \\delta_{\\alpha}^{i}$, which is tantamount to $g^{i j}=g_{i j}=0$ for $i \\neq j$."}, "5.32": {"question": "5.32 Refer to Problems 5.20 and 5.31. (a) What property must the metric tensor $\\left(g_{i j}\\right)$ possess in $\\left(x^{i}\\right)$ for the coordinate axis $\\mathscr{C}_{\\alpha}$ to be normal to the surface $x^{\\alpha}=$ const. (in which case $\\theta=\\phi$ )? $(b)$ Show that the property of $(a)$ is equivalent to the mutual orthogonality of the coordinate axes.", "answer": "5.32 (a) $g^{i \\alpha}=\\lambda(\\alpha) \\delta_{\\alpha}^{i}$, which is tantamount to $g^{i j}=g_{i j}=0$ for $i \\neq j$."}, "5.33": {"question": "5.33 Under the metric\n\n$$\nG=\\left[\\begin{array}{cc}\n1 & \\cos 2 x^{2} \\\\\n\\cos 2 x^{2} & 1\n\\end{array}\\right] \\quad\\left(2 x^{2} / \\pi \\text { nonintegral }\\right)\n$$\n\ncompute the norm of the vector $\\mathbf{V}=\\left(d x^{i} / d t\\right)$ evaluated along the curve $x^{1}=-\\sin 2 t, x^{2}=t$, and use it to find the arc length between $t=0$ and $t=\\pi / 2$.", "answer": "$5.33\\|\\mathbf{V}\\|=1, L=\\pi / 2$"}, "5.34": {"question": "5.34 Under the Euclidean metric for spherical coordinates, (5.4), determine a particular family of curves that intersect\n\n$$\nx^{1}=a \\quad x^{2}=b t \\quad x^{3}=t\n$$\n\northogonally. (Cf. Problem 5.28.)", "answer": "5.34 $x^{1}=a, x^{3}=b \\cot x^{2}+c(c=$ const. $)$"}, "6.19": {"question": "6.19 Find the general solution of the linear system\n\n$$\n\\frac{\\partial^{2} \\bar{x}^{i}}{\\partial x^{j} \\partial x^{k}}=a_{j k}^{i}=\\text { const. }\n$$\n\nwith $a_{j k}^{i}$ symmetric in the two lower subscripts. [Hint: Set $y_{k}^{i}=\\partial \\bar{x}^{i} / \\partial x^{k}-a_{r k}^{i} x^{r}$.]", "answer": "6.19 $\\bar{x}^{i}=\\frac{1}{2} a_{r s}^{i} x^{r} x^{s}+b_{r}^{i} x^{r}+c^{i} \\quad$ (the $b_{j}^{i}$ and $c^{i}$ constants)"}, "6.20": {"question": "6.20 A two-dimensional coordinate system $\\left(x^{i}\\right)$ is connected to a rectangular coordinate system $\\left(\\bar{x}^{i}\\right)$ through\n\n$$\n\\bar{x}^{1}=2\\left(x^{1}\\right)^{2}+x^{2} \\quad \\bar{x}^{2}=-x^{1}+3 x^{2}\n$$\n\n(a) Exhibit the metric tensor in $\\left(x^{i}\\right)$.\n\n(b) Calculate the Christoffel symbols of the first kind for $\\left(x^{i}\\right)$ directly from the definition (6.1).", "answer": "6.20(a)\n\n$$\nG=\\left[\\begin{array}{cc}\n16\\left(x^{1}\\right)^{2}+1 & 4 x^{1}-3 \\\\\n4 x^{1}-3 & 10\n\\end{array}\\right]\n$$\n\n(b) $\\Gamma_{111}=16 x^{1}, \\Gamma_{112}=4$, all others 0"}, "6.21": {"question": "6.21 (a) Derive the formula\n\n$$\n\\Gamma_{i j k}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{j}} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{k}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is rectangular and $\\left(x^{i}\\right)$ is any other coordinate system. [Hint: Interchange barred and unbarred coordinate systems in (6.3)]. (b) Derive the analogous formula\n\n$$\n\\Gamma_{j k}^{i}=\\frac{\\partial^{2} \\bar{x}^{r}}{\\partial x^{i} \\partial x^{k}} \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}\n$$\n\nwhen $\\left(\\bar{x}^{i}\\right)$ is such that all $\\bar{g}_{i j}$ arc constant. [Hint: Interchange barred and unbarred coordinate systems in $(6.5)$.]", "answer": "not contained in text"}, "6.22": {"question": "6.22 Let the coordinate system $\\left(x^{i}\\right)$ be connected to a system of rectangular coordinates $\\left(\\bar{x}^{i}\\right)$ via\n\n$$\n\\bar{x}^{1}=\\exp \\left(x^{1}+x^{2}\\right) \\quad \\bar{x}^{2}=\\exp \\left(x^{1}-x^{2}\\right)\n$$\n\nUse Problem 6.21(b) to compute the nonzero Christoffel symbols of the second kind for $\\left(x^{i}\\right)$.", "answer": "6.22 The values, in $\\left(x^{i}\\right)$, of the $\\partial x^{i} / \\partial \\bar{x}^{j}$ are easiest found by inverting $J \\equiv\\left(\\partial \\bar{x}^{i} / \\partial x^{j}\\right)$. Final results are:\n\n$$\n\\Gamma_{11}^{1}=\\Gamma_{22}^{1}-\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=1\n$$"}, "6.23": {"question": "6.23 If\n\n$$\n\\begin{aligned}\n& \\bar{x}^{1}=-\\exp d_{1} x^{1}+\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{2}=2 \\exp d_{1} x^{1}-\\exp d_{2} x^{2}+\\exp d_{3} x^{3} \\\\\n& \\bar{x}^{3}=\\exp d_{1} x^{1}-2 \\exp d_{2} x^{2}+3 \\exp d_{3} x^{3}\n\\end{aligned}\n$$\n\nand if all $\\bar{\\Gamma}_{j k}^{i}=0$, find the $\\Gamma_{j k}^{i}$.", "answer": "6.23 From Problem 6.21(b), $\\Gamma_{j k}^{i}=0$ for $j \\neq k$; while, for $j=k=\\alpha$ (no summation on $\\alpha$ ),\n\n$$\n\\Gamma_{\\alpha \\alpha}^{i}=\\frac{\\partial}{\\partial x^{\\alpha}}\\left(\\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=\\left(d_{\\alpha} \\frac{\\partial \\bar{x}^{r}}{\\partial x^{\\alpha}}\\right) \\frac{\\partial x^{i}}{\\partial \\bar{x}^{r}}=d_{\\alpha} \\delta_{\\alpha}^{i}\n$$"}, "6.24": {"question": "6.24 Derive (6.6) by solving (6.5) for the second derivative and then changing indices.", "answer": "not contained in text"}, "6.25": {"question": "6.25 Prove that all $\\Gamma_{j k}^{i}$ vanish only if all $g_{i j}$ are constant.", "answer": "not contained in text"}, "6.26": {"question": "6.26 Calculate the nonzero Christoffel symbols of both kinds for the Euclidean metric in cylindrical coordinates, (5.3).", "answer": "6.26-\\Gamma_{221}=\\Gamma_{212}=\\Gamma_{122}=x^{1} ; \\Gamma_{21}^{2}=\\Gamma_{12}^{2}=1 / x^{1}, \\Gamma_{22}^{1}=-x^{1}"}, "6.27": {"question": "6.27 Express the condition that the transformation (5) of Problem 6.6 be bijective (Section 2.6).", "answer": "6.27\n\n$$\n\\overline{\\mathscr{J}}=\\left|\\begin{array}{lll}\na_{1}^{1} \\exp \\bar{x}^{1} & 2 a_{2}^{1} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{1} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{2} \\exp \\bar{x}^{-1} & 2 a_{2}^{2} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{2} \\exp 3 \\bar{x}^{3} \\\\\na_{1}^{3} \\exp \\bar{x}^{1} & 2 a_{2}^{3} \\exp 2 \\bar{x}^{2} & 3 a_{3}^{3} \\exp 3 \\bar{x}^{3}\n\\end{array}\\right|=\\left[6 \\exp \\left(\\bar{x}^{1}+2 \\bar{x}^{2}+3 \\bar{x}^{3}\\right)\\right] \\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0\n$$\n\nHence the condition is $\\operatorname{det}\\left(a_{j}^{i}\\right) \\neq 0$."}, "6.28": {"question": "6.28 Show that if $\\Gamma_{i j k}$ are constant, then $g_{i j}$ are linear in the variables $\\left(x^{i}\\right)$; but that this is not necessarily true if $\\Gamma_{j k}^{i}$ are constant. (For a counterexample, use the metric $g_{11}=\\exp 2 x^{1}, g_{12}=g_{21}=0, g_{22}=1$.)", "answer": "not contained in text"}, "6.29": {"question": "6.29 What is the most general two-dimensional transformation $\\bar{x}^{i}=\\bar{x}^{i}(\\mathbf{x})$ of coordinates such that $\\left(\\bar{x}^{i}\\right)$ are rectangular and the Christoffel symbols $\\Gamma_{j k}^{i}$ in $\\left(x^{i}\\right)$ are those for the metric of polar coordinates (Example 6.3)?", "answer": "6.29 $\\bar{x}^{i}=A^{i} x^{1} \\sin x^{2}+B^{i} x^{1} \\cos x^{2}+C^{i} \\quad(i=1,2)$, with\n\n$$\nx^{1}\\left|\\begin{array}{ll}\nA^{1} & B^{1} \\\\\nA^{2} & B^{2}\n\\end{array}\\right| \\neq 0\n$$\n\nfor a bijection."}, "6.30": {"question": "6.30 Is the covariant derivative of a tensor with constant components equal to zero as in ordinary differentiation? Explain your answer.", "answer": "No, because of the presence of Christoffel symbols in (6.7)."}, "6.31": {"question": "6.31 If $T_{j r s}^{i}$ are tensor components, write out the components of the covariant derivative, $T_{j r s, k}^{i}$.", "answer": "6.31 $T_{j r s, k}^{i}=\\frac{\\partial T_{j r s}^{i}}{\\partial x^{k}}+\\Gamma_{u k}^{i} T_{j r s}^{u}-\\Gamma_{j k}^{u} T_{u r s}^{i}-\\Gamma_{r k}^{u} T_{j u s}^{i}-\\Gamma_{s k}^{u} T_{j r u}^{i}$"}, "6.32": {"question": "6.32 Show that $\\delta_{i, k}^{i}=0$ for all $i, j, k$.", "answer": "not contained in text"}, "6.33": {"question": "6.33 For any tensor $\\mathbf{T}$, verify that $(\\mathbf{g} * \\mathbf{T})_{, k}=\\mathbf{g} * \\mathbf{T}{ }_{, k}$, where $*$ denotes either an outer or inner product.", "answer": "not contained in text"}, "6.34": {"question": "6.34 Use Problem 6.32 and $g_{j r} g^{r i}=\\delta_{j}^{i}$ to show that the covariant derivative of $\\mathbf{g}^{-1}$ is zero.", "answer": "not contained in text"}, "6.35": {"question": "6.35 Use the recursive method of Problem 6.8 to verify that $\\left(T_{i j, k}\\right)$ is a tensor.", "answer": "not contained in text"}, "6.36": {"question": "6.36 Using tensor methods in polar coordinates, find the curvature of the circle\n\n$$\nx^{1}=b \\quad x^{2}=t\n$$", "answer": "6.36 $\\kappa=1 / b$"}, "6.37": {"question": "6.37 If the metric for $\\left(x^{i}\\right)$ is\n\n$$\nG=\\left[\\begin{array}{cc}\n\\left(x^{1}\\right)^{2} & 0 \\\\\n0 & 1\n\\end{array}\\right]\n$$\n\n(a) write the differential equations of the geodesics in terms of the dependent variables $u=\\left(x^{1}\\right)^{2}$ and $v=x^{2} ;(b)$ integrate these equations and eliminate the arc-length parameter from the solution.", "answer": "6.37 (a) $\\frac{d^{2} u}{d s^{2}}=\\frac{d^{2} v}{d s^{2}}=0$\n\n(b) $x^{2}=p\\left(x^{1}\\right)^{2}+q$ (a two-parameter family of \"parabolas\")"}, "6.38": {"question": "6.38 Find the geodesics on the surface of a sphere of radius $a$ by $(a)$ writing the geodesic equations for the spherical coordinates $x^{2}$ and $x^{3}$ (the $x^{1}$-equation is trivial for $x^{1}=a=$ const. and may be ignored); $(b)$ exhibiting a particular solution of these two equations; and (c) generalizing on (b). Use Problem 6.5 for the Christoffel symbols.", "answer": "6.38(a)\n\n$$\n\\begin{aligned}\n& \\frac{d^{2} x^{2}}{d s^{2}}-\\left(\\sin x^{2} \\cos x^{2}\\right)\\left(\\frac{d x^{3}}{d s}\\right)^{2}=0 \\\\\n& \\frac{d^{2} x^{3}}{d s^{2}}+\\left(2 \\cot x^{2}\\right) \\frac{d x^{2}}{d s} \\frac{d x^{3}}{d s}=0\n\\end{aligned}\n$$\n\n(b) $x^{2}=\\frac{1}{a} s \\quad x^{3}=0$\n\n(c) The solution (b) represents an arc of a particular great circle $\\left(x^{2}+z^{2}=a^{2}\\right.$, in the usual cartesian coordinates) on the sphere. By symmetry of the sphere, all great-circular arcs, and only these, will be geodesics."}, "7.24": {"question": "```latex\n7.24 Determine the fundamental indicator $\\varepsilon(\\mathbf{U})$ if $\\left(U^{i}\\right)=(2 t,-2 t, 1)$ at the point $\\left(x^{i}\\right)=\\left(t^{2},-t^{2}, t\\right)$. The Riemannian metric is given by\n\n$$\n\\left(g_{i j}\\right)=\\left[\\begin{array}{ccc}\n2 x^{1} & x^{3} & 0 \\\\\nx^{3} & 2 x^{2} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right] \\quad\\left(4 x^{1} x^{2} \\neq\\left(x^{3}\\right)^{2}\\right)\n$$\n```", "answer": "$7.24 \\varepsilon= \\begin{cases}+1 & 0<|t| \\leqq 1 / 2 \\\\ -1 & |t|>1 / 2\\end{cases}$"}, "7.25": {"question": "```latex\n7.25 Find the null points of the curve $\\mathscr{C}: x^{1}=t, x^{2}=t^{4} \\quad$ ( $t$ real), if the metric is\n\n$$\n\\varepsilon d s^{2}=8\\left(x^{1} d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n```", "answer": "$7.25 t=0,1$"}, "7.26": {"question": "```latex\n7.26 Find the arc length of the curve in Problem 7.25 if $0 \\leqq t \\leqq 2$.\n```", "answer": "$7.26 L=8 \\sqrt{2} / 3$"}, "7.27": {"question": "```latex\n7.27 Find the null points of the curve $\\mathscr{C}: x^{1}=t^{3}+1, x^{2}=t^{2}, x^{3}=t$, if the metric is\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(x^{3} d x^{3}\\right)^{2}\n$$\n```", "answer": "$7.27 t=\\sqrt{5} / 3\\left[t=0\\right.$, which makes $\\gamma \\equiv\\left|g_{i j}\\right|=0$, is disallowed $]$"}, "7.28": {"question": "```latex\n7.28 Find the arc length of the curve in Problem 7.27 if $\\frac{1}{2} \\leqq t \\leqq 1$.\n```", "answer": "$7.28 \\quad L=(64+11 \\sqrt{11}) / 216 \\approx 0.465$"}, "7.29": {"question": "```latex\n7.29 Find the angle between the curves\n\n$$\n\\mathscr{C}_{1}:\\left\\{\\begin{array}{l}\nx^{1}=5 t \\\\\nx^{2}=2 \\\\\nx^{3}=3 t\n\\end{array} \\quad \\mathscr{C}_{2}:\\left\\{\\begin{array}{l}\nx^{1}=u \\\\\nx^{2}=2 \\\\\nx^{3}=3 u^{2} / 25\n\\end{array}\\right.\\right.\n$$\n\nat each of the points of intersection, if the fundamental form is $\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2}-\\left(d x^{3}\\right)^{2}$.\n```", "answer": "$7.29 \\quad \\theta=i \\ln 2$ at $(0,2,0) ; \\theta=\\cos ^{-1}(7 / 4 \\sqrt{11})$ at $(5,2,3)$"}, "7.30": {"question": "```latex\n7.30 If $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2},(a)$ find the length $L$ of the curve $\\mathscr{C}: x^{1}=12 t^{2}, x^{2}=8 t^{3}$, for $0 \\leqq t \\leqq 2$. (b) Find an arc-length parameterization, $x^{i}=x^{i}(s)$, for $\\mathscr{C}$, with $s=0$ corresponding to $t=1$. (c) Show that the $x^{i}(s)$ are differentiable to all orders except at points of nullity.\n```", "answer": "7.30 (a) $L=8(1+3 \\sqrt{3}) \\approx 49.57$\n\n(b) $x^{1}=3\\left(\\sigma s^{2 / 3}+4\\right), x^{2}=\\left(\\sigma s^{2 / 3}+4\\right)^{3 / 2}$, where\n\n$$\n\\sigma=\\left\\{\\begin{array}{lr}\n+1 & -8 \\leqq s>0 \\\\\n-1 & 0 \\leqq s \\leqq 24 \\sqrt{3}\n\\end{array}\\right.\n$$\n\n(c) The null points are $t=0(s=-8)$ and $t=1(s=0)$."}, "7.31": {"question": "```latex\n7.31 Find the arc length of the curve of Problem.7.30, but with the Euclidean metric.\n```", "answer": "$7.31 L=8(5 \\sqrt{5}-1) \\approx 81.44$"}, "7.32": {"question": "```latex\n7.32 Compute $\\mathbf{T}=\\left(d x^{i} / d s\\right)$ from the arc-length parameterization found in Problem 7.30 and verify that $\\mathbf{T}$ has unit length at all points except $s=0$.\n```", "answer": "$7.32 For $s \\neq 0, \\mathbf{T}=\\left(2|s|^{-1 / 3}, \\sqrt{\\sigma+4 s^{-2 / 3}}\\right)$ and $\\|\\mathbf{T}\\|^{2}=|-\\sigma|=+1$"}, "7.33": {"question": "```latex\n7.33 Calculate the components $N^{i}$ of the unit principal normal of the curve of Problem 7.30, using (7.16) (Problem 7.14).\n```", "answer": "$7.33 \\quad N^{1}=T^{2}, N^{2}=T^{1}$"}, "7.34": {"question": "```latex\n7.34 Calculate both the curvature $\\kappa$ and the absolute curvature $\\kappa_{0}$ for the curve of Problem 7.30. Discuss the numerical behavior of $\\kappa_{0}$ along the curve.\n```", "answer": "For $s \\neq-8,0$,\n\n7.36\n\n$$\n\\begin{gathered}\n\\kappa=\\frac{-2}{3 s \\sqrt{\\sigma s^{2 / 3}+4}} \\quad \\kappa_{0}=|\\kappa| \\\\\n\\kappa_{0}=|\\kappa|=\\frac{2}{3|s|\\left(s^{2 / 3}-4\\right)^{1 / 2}} \\quad(s \\neq 8)\n\\end{gathered}\n$$\n\nAt the null point $(0,0)$, both Euclidean and Riemannian absolute curvatures become infinite; but at the null point $(12,8)$, only the Riemannian curvature becomes infinite."}, "7.35": {"question": "```latex\n7.35 Use the formula of Problem 7.18(b) to confirm the value of $\\kappa_{0}$ found in Problem 7.34.\n```", "answer": "Not contained in text."}, "7.36": {"question": "```latex\n7.36 Compute $\\kappa_{0}$ under the Euclidean metric for the curve of Problem 7.30; compare with the result obtained in Problem 7.34. For convenience, let $t=0$ correspond to $s=8$.\n```", "answer": "$$\n\\begin{gathered}\n\\kappa=\\frac{-2}{3 s \\sqrt{\\sigma s^{2 / 3}+4}} \\quad \\kappa_{0}=|\\kappa| \\\\\n\\kappa_{0}=|\\kappa|=\\frac{2}{3|s|\\left(s^{2 / 3}-4\\right)^{1 / 2}} \\quad(s \\neq 8)\n\\end{gathered}\n$$\n\nAt the null point $(0,0)$, both Euclidean and Riemannian absolute curvatures become infinite; but at the null point $(12,8)$, only the Riemannian curvature becomes infinite."}, "7.37": {"question": "```latex\n7.37 Without calculating an arc-length parameter, find the vectors $\\mathbf{T}$ and $\\mathbf{N}$, and the curvature $\\kappa$, for the \"parabola\" $x^{1}=t, x^{2}=t^{2} \\quad\\left(0 \\leqq t \\leqq \\frac{1}{2}\\right)$ under the Riemannian metric\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 d x^{1} d x^{2}\n$$\n```", "answer": "$7.37 \\mathbf{T}=|1-4 t|^{-1 / 2}(1,2 t), \\mathbf{N}=|1-4 t|^{-1 / 2}(1,1-2 t), \\kappa=2|1-4 t|^{-3 / 2}$"}, "7.38": {"question": "```latex\n7.38 Show that the first-quadrant portion $\\left(x^{i}>0\\right)$ of the hypocycloid $\\mathscr{H}$ of four cusps\n\n$$\n\\left(x^{1}\\right)^{2 / 3}+\\left(x^{2}\\right)^{2 / 3}=a^{2 / 3} \\quad(a>0)\n$$\n\nmay be parameterized as $x^{1}=a \\cos ^{3} t, x^{2}=a \\sin ^{3} t$, with $0 \\leqq t \\leqq \\pi / 2$. Find the arc length under the two metrics\n\n$$\n\\text { (a) } \\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(d x^{2}\\right)^{2} \\quad \\text { (b) } \\quad d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(d x^{2}\\right)^{2}\n$$\n\n(c) Without computing an arc-length parameter, find $\\mathbf{T}$ and $\\kappa_{0}$ for $\\mathscr{H}$ under both metrics.\n```", "answer": "$7.38 \\quad$ (a) $L=a$. (b) $L=3 a / 2$. (c) Riemannian: $\\mathbf{T}=|\\cos 2 t|^{-1 / 2}(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a)(\\csc 2 t)|\\cos 2 t|^{-3 / 2}$; Euclidean: $\\mathbf{T}=(-\\cos t, \\sin t), \\kappa_{0}=(2 / 3 a) \\csc 2 t$"}, "7.39": {"question": "```latex\n7.39 (a) Determine the Christoffel symbols of the second kind for the Riemannian metric $\\varepsilon d s^{2}=x^{1}\\left(d x^{1}\\right)^{2}+$ $x^{2}\\left(d x^{2}\\right)^{2}$. (b) Without converting to an arc-length parameter, verify that all curves $x^{1}=t^{2}, x^{2}=$ $\\left(a t^{3}+b\\right)^{2 / 3}$, where $a$ and $b$ are arbitrary constants, are geodesics.\n```", "answer": "7.39 (a) $\\Gamma_{11}^{1}=1 / 2 x^{1}, \\Gamma_{22}^{2}=1 / 2 x^{2}$, others zero"}, "8.15": {"question": "```latex\n8.15 The absolute partial derivatives of a tensor $\\mathbf{T}=\\left(T_{j}^{i} \\ldots\\right)$ defined on a 2-manifold $\\mathscr{M}: x^{i}=x^{i}(u, v)$ are defined as\n\n$$\n\\frac{\\delta \\mathbf{T}}{\\delta u} \\equiv\\left(T_{j}^{i} \\ldots, k, \\frac{\\partial x^{k}}{\\partial u}\\right) \\quad \\text { and } \\quad \\frac{\\delta \\mathbf{T}}{\\delta v} \\equiv\\left(T_{j}^{i} \\ldots, \\frac{\\partial x^{k}}{\\partial v}\\right)^{\\dot{*}}\n$$\n\nSince $\\left(\\partial x^{i} / \\partial u\\right)$ and $\\left(\\partial x^{i} / \\partial v\\right)$ are vectors, the inner products produce a pair of tensors of the same type and order as $\\mathbf{T}$; thus the operation of absolute partial differentiation may be repeated indefinitely. Prove that if $\\left(V^{i}\\right)$ is any contravariant vector defined on $\\mathcal{M}$,\n\n$$\n\\frac{\\delta}{\\delta u}\\left(\\frac{\\delta V^{i}}{\\delta v}\\right)-\\frac{\\delta}{\\delta v}\\left(\\frac{\\delta V^{i}}{\\delta u}\\right)=R_{s k l}^{i} V^{s} \\frac{\\partial x^{k}}{\\partial u} \\frac{\\partial x^{l}}{\\partial v}\n$$\n\n[Hint: Expand the left side and use Problem 8.16.]\n```", "answer": "not contained in text"}, "8.16": {"question": "```latex\n8.16 Prove that for any vector $\\left(V^{i}\\right), V_{, k l}^{i}-V_{, l k}^{i}=-R_{s k l}^{i} V^{s}$.\n```", "answer": "8.16 By Problem 6.34 and $(8: 1)$,\n\n$$\n\\begin{aligned}\nV_{, k l}^{i}-V_{, l k}^{i} & =g^{i r}\\left(V_{r, k l}-V_{r, l k}\\right)=g^{i r} R_{r k l}^{s} V_{s} \\\\\n& =g^{i r}\\left(g_{s t} R_{r k l}^{s}\\right) V^{t}=\\left(g^{i r} R_{t r k l}\\right) V^{t}=-R_{t k l}^{i} V^{t}\n\\end{aligned}\n$$"}, "8.17": {"question": "```latex\n8.17 For an arbitrary second-order contravariant tensor $\\left(T^{i j}\\right)$, show that\n\n$$\nT_{, k l}^{i j}-T_{, l k}^{i j}=-R_{s k l}^{i} T^{s j}-R_{s k l}^{j} T^{i s}\n$$\n\n[Hint: Lower superscripts and use Problem 8.3.]\n```", "answer": "not contained in text"}, "8.18": {"question": "```latex\n8.18 For an arbitrary mixed tensor $\\left(T_{j}^{i}\\right)$ show that\n\n$$\nT_{j, k l}^{i}-T_{j, l k}^{i}=-R_{s k l}^{i} T_{j}^{s}+R_{j k l}^{s} T_{s}^{i}\n$$\n```", "answer": "not contained in text"}, "8.19": {"question": "```latex\n8.19 Verify the symmetry properties (8.6) for the $G_{i j k l}[\\operatorname{see}(8.7)]$ and for the $W_{i j k l}[\\operatorname{see}(8.9)]$.\n```", "answer": "not contained in text"}, "8.20": {"question": "```latex\n8.20 Derive (8.5) from (8.4). [Hint: It is helpful to adopt the notation $g_{i j k l}$ for $\\partial^{2} g_{i j} / \\partial x^{k} \\partial x^{l}$.]\n```", "answer": "not contained in text"}, "8.21": {"question": "```latex\n8.21 List the independent (nonzero) components of $R_{i j k i}$ when $n=4$ and verify Theorem 8.2 for this case.\n```", "answer": "not contained in text"}, "8.22": {"question": "```latex\n8.22 Calculate the Riemannian curvature K for the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-2 x^{1}\\left(d x^{2}\\right)^{2}$.\n```", "answer": "8.22 $\\mathrm{K}=1 / 4\\left(x^{1}\\right)^{2}$"}, "8.23": {"question": "```latex\n8.23 Confirm that $\\mathrm{K}=0$ for the Euclidean metric of polar coordinates,\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1} d x^{2}\\right)^{2}\n$$\n\n$(a)$ by a calculation; $(b)$ by noting that $\\mathrm{K}$ is an invariant.\n```", "answer": "not contained in text"}, "8.24": {"question": "```latex\n8.24 Rework Example 8.4 for the pairs $(a) \\mathbf{U}_{(1)}=(1,0,1), \\mathbf{V}_{(1)}=(1,1,1)$ and $(b) \\mathbf{U}_{(2)}=(0,1,0), \\mathbf{V}_{(2)}=$ $(2,1,2)$. (c) Explain why the answers should be the same for $(a)$ and $(b)$.\n```", "answer": "8.24 (a) and (b) $\\mathrm{K}=\\frac{x^{1}+x^{2}}{4\\left(x^{1}\\right)^{2} x^{2}\\left(1+2 x^{2}\\right)}$\n\n(c) $\\mathbf{U}_{(2)}=-\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}, \\mathbf{V}_{(2)}=\\mathbf{U}_{(1)}+\\mathbf{V}_{(1)}$"}, "8.25": {"question": "```latex\n8.25 Let the surface of the 3 -sphere of radius $a$ be metrized by setting $x^{1}=a$ in spherical coordinates and then allowing $x^{1}, x^{2}$ to replace $x^{2}, x^{3}$, respectively:\n\n$$\nd s^{2}=a^{2}\\left(d x^{1}\\right)^{2}+\\left(a \\sin x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}\n$$\n\nDetermine $\\mathrm{K}$ for this non-Euclidean $\\mathbf{R}^{2}$.\n```", "answer": "8.25 $\\mathrm{K}=1 / a^{2}$"}, "8.26": {"question": "```latex\n8.26 If the metric for Riemannian $\\mathbf{R}^{3}$ is given by\n\n$$\ng_{11}=f\\left(x^{2}\\right) \\quad g_{22}=g\\left(x^{2}\\right) \\quad g_{33}=h\\left(x^{2}\\right)\n$$\n\nand $g_{i j}=0$ for $i \\neq j$, write explicit formulas for $(a) \\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right),(b) R$.\n```", "answer": "8.26 Basic sets of nonvanishing terms are:\n\n$$\n\\text { (A) } R_{1212}=-\\frac{1}{4}\\left(2 f^{\\prime \\prime}-\\frac{f^{\\prime 2}}{f}-\\frac{f^{\\prime} g^{\\prime}}{g}\\right), R_{1313}=-\\frac{1}{4} \\frac{f^{\\prime} h^{\\prime}}{g}, R_{2323}=-\\frac{1}{4}\\left(2 h^{\\prime \\prime}-\\frac{h^{\\prime 2}}{h}-\\frac{h^{\\prime} g^{\\prime}}{g}\\right)\n$$\n\nand\n\n(A) $G_{1212}=f g, G_{1313}=f h, G_{2323}=g h$\n\nso that\n\n(a) $\\mathrm{K}\\left(x^{2} ; \\mathbf{U}, \\mathbf{V}\\right)=\\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{f g W_{1212}+f h W_{1313}+g h W_{2323}}$\n\n(b) $\\quad R=-\\frac{2}{f g h}\\left(h R_{1212}+g R_{1313}+f R_{2323}\\right)$"}, "8.27": {"question": "```latex\n8.27 Specialize the results of Problem 8.26 to the case $f\\left(x^{2}\\right) \\equiv g\\left(x^{2}\\right) \\equiv h\\left(x^{2}\\right)$.\n```", "answer": "not contained in text"}, "8.28": {"question": "```latex\n8.28 Find the isotropic points for the Riemannian metric\n\n$$\nd s^{2}=\\left(\\ln x^{2}\\right)\\left(d x^{1}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{2}\\right)^{2}+\\left(\\ln x^{2}\\right)\\left(d x^{3}\\right)^{2} \\quad\\left(x^{2}>1\\right)\n$$\n\nand find the curvature $\\mathrm{K}$ at those points. [Hint: Use Problem 8.27.]\n```", "answer": "8.28 Isotropic points compose the surface $x^{2}=e^{-3 / 2}$, over which $\\mathrm{K}=2 e^{3 / 27}$."}, "8.29": {"question": "```latex\n8.29 Show that $\\mathbf{R}^{3}$ under the metric\n\n$$\ng_{11}=e^{x^{2}} \\quad g_{22}=1 \\quad g_{33}=e^{x^{2}} \\quad g_{i j}=0 \\quad(i \\neq j)\n$$\n\nhas constant Riemannian curvature with all points isotropic, and find that curvature.\n```", "answer": "8.29 $\\mathrm{K}=-1 / 4$"}, "8.30": {"question": "```latex\n8.30 Show that in a Riemannian 2-space [for which (8.11) holds]: (a) $R_{i j}=-g_{i j} \\mathrm{~K},(b) R_{j}^{i}=-\\delta_{j}^{i} \\mathrm{~K}$, and (c) $R=-2 \\mathrm{~K}$.\n```", "answer": "not contained in text"}, "8.31": {"question": "```latex\n8.31 Calculate the Ricci tensor $R_{i j}$ for Problem 8.13 using (8.16), and compare your answers with those obtained earlier.\n```", "answer": "not contained in text"}, "8.32": {"question": "```latex\n8.32 Use Problem 8.30 to calculate the Ricci tensors of both kinds and the curvature invariant for the spherical metric of Problem 8.25.\n```", "answer": "8.32 $R_{11}=-1, R_{12}=R_{21}=0, R_{22}=\\sin ^{2} x^{1} ; R_{1}^{1}=-1 / a^{2}=R_{2}^{2}, R_{2}^{1}=0=R_{1}^{2} ; R=-2 / a^{2}$"}, "8.33": {"question": "```latex\n8.33 Calculate the Ricci tensors of both kinds and the curvature invariant for the (hyperbolic) metric of Problem 8.12. [Hint: Problem 8.27 can be used to good advantage here.]\n```", "answer": "8.33 $\\quad R_{11}=R_{22}=R_{33}=2 /\\left(x^{1}\\right)^{2}$, others $0 ; R_{1}^{1}=R_{2}^{2}=R_{3}^{3}=2$, others $0 ; R=6$"}, "8.34": {"question": "```latex\n8.34 Prove that for any tensor $\\left(T^{i j}\\right)$, symmetric or not, $T_{, i j}^{i j}=T_{, j i}^{i j}$. [Hint: Use Problem 8.17 and the symmetry of the Ricci tensor.]\n```", "answer": "8.34 $g_{i j}=\\left(x^{1}\\right)^{4} \\delta_{i j}$ has $R=0, \\mathrm{~K} \\neq 0$ (use Problem 8.27)."}, "8.35": {"question": "```latex\n8.35 Is identical vanishing equivalent for the Riemannian curvature and the Ricci curvature invariant? Can you find an example where one is zero everywhere but not the other?\n```", "answer": "not contained in text"}, "8.36": {"question": "```latex\n8.36 Is constancy in space equivalent for the two curvatures $\\mathrm{K}$ and $R$ ?\n```", "answer": "8.36 No implication either way."}, "9.17": {"question": "```latex\n9.17 Solve, if compatible, the 'system $\\partial u_{\\lambda} / \\partial x^{j}=F_{\\lambda j}$, with\n\n$$\n\\begin{aligned}\n& F_{01}=x^{2} / 2 u_{0} \\quad F_{02}=x^{1} / 2 u_{0} \\\\\n& F_{01}=u_{0} x^{1} \\quad F_{02}=u_{1} x^{2} \\quad F_{11}=u_{0} x^{1} \\quad F_{12}=u_{1} x^{2}\n\\end{aligned}\n$$\n```", "answer": "9.17 (a) $u_{0}= \\pm \\sqrt{x^{1} x^{2}+a} \\quad(a=$ const. $) ;(b)$ incompatible"}, "9.18": {"question": "```latex\n9.18 Verify that $d s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2}$ represents the Euclidean metric (in polar coordinates).\n```", "answer": "not contained in text"}, "9.19": {"question": "```latex\n9.19 Consider the metric $\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1} d x^{2}\\right)^{2}-\\left(x^{1} d x^{3}\\right)^{2}$. Show that $R_{1212}=2$ and that, therefore, the space is not flat.\n```", "answer": "not contained in text"}, "9.20": {"question": "```latex\n9.20 Determine whether the following metric is flat and/or Euclidean:\n\n$$\n\\varepsilon d s^{2}=\\left(d x^{1}\\right)^{2}-\\left(x^{1}\\right)^{2}\\left(d x^{2}\\right)^{2} \\quad(n=2)\n$$\n```", "answer": "9.20 flat, non-Euclidean"}, "9.21": {"question": "```latex\n9.21 Determine whether the following metric is flat and/or Euclidean:\n\n$$\nd s^{2}=\\left(d x^{1}\\right)^{2}+\\left(x^{3}\\right)^{2}\\left(d x^{2}\\right)^{2}+\\left(d x^{3}\\right)^{2}\n$$\n```", "answer": "9.21 Euclidean"}, "9.22": {"question": "```latex\n9.22 Find the signature of the metric for $\\mathbf{R}^{3}$ given by\n\n$$\n\\varepsilon d s^{2}=2\\left(d x^{1}\\right)^{2}+2\\left(d x^{2}\\right)^{2}+5\\left(d x^{3}\\right)^{2}-8 d x^{1} d x^{2}-4 d x^{1} d x^{3}-d x^{2} d x^{3}\n$$\n```", "answer": "9.22 (++-)"}, "9.23": {"question": "```latex\n9.23 Prove that $R_{i j k}^{i}=0$. [Hint: Use the first of (8.6).]\n```", "answer": "not contained in text"}, "9.24": {"question": "```latex\n9.24 Use Problem 9.11 to obtain a simplified proof for Problem 8.34.\n```", "answer": "not contained in text"}, "9.25": {"question": "```latex\n9.25 Show that the Einstein invariant, $G \\equiv G_{i}^{i}$, vanishes if the space is flat. [Hint: Use Corollary 9.4.]\n```", "answer": "not contained in text"}, "9.26": {"question": "```latex\n9.26 In the general theory of relativity one encounters the Schwarzschild metric,\n\n$$\n\\varepsilon d s^{2}=e^{\\varphi}\\left(d x^{1}\\right)^{2}+\\left(x^{1}\\right)^{2}\\left[\\left(d x^{2}\\right)^{2}+\\left(\\sin ^{2} x^{2}\\right)\\left(d x^{3}\\right)^{2}\\right]-e^{\\psi}\\left(d x^{4}\\right)^{2}\n$$\n\nwhere both $\\varphi$ and $\\psi$ are functions of $x^{1}$ and $x^{4}$ only. Calculate the nonzero components of the Einstein tensor.\n```", "answer": "9.26 With the notation $f_{i} \\equiv \\partial f / \\partial x^{i}$, for any function $f$ :\n\n$$\n\\begin{aligned}\n& G_{1}^{1}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[-\\frac{\\psi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\\\\n& G_{2}^{2}=e^{-\\varphi}\\left(-\\frac{\\psi_{11}}{2}-\\frac{\\psi_{1}^{2}}{4}+\\frac{\\varphi_{1} \\psi_{1}}{4}+\\frac{\\varphi_{1}}{2 x^{1}}-\\frac{\\psi_{i}}{2 x^{1}}\\right)+e^{-\\psi}\\left(\\frac{\\varphi_{44}}{2}+\\frac{\\varphi_{4}^{2}}{4}-\\frac{\\varphi_{4} \\psi_{4}}{4}\\right)=G_{3}^{3} \\\\\n& G_{4}^{4}=\\frac{1}{\\left(x^{1}\\right)^{2}}+e^{-\\varphi}\\left[\\frac{\\varphi_{1}}{x^{1}}-\\frac{1}{\\left(x^{1}\\right)^{2}}\\right] \\quad G_{4}^{1}=-\\varphi_{4} e^{-\\varphi / x^{1}} \\quad G_{1}^{4}=\\varphi_{4} e^{-\\psi / x^{1}}\n\\end{aligned}\n$$"}, "10.30": {"question": "```latex\n10.30 (a) Describe geometrically the curve whose parametric vector equation is\n\n$$\n\\mathbf{r}=\\left(\\cos t, \\sin t,(1-t)^{-1}\\right) \\quad(0 \\leqq t<1)\n$$\n\nWhat happens as $t \\rightarrow 1$ ? (b) Use a programmable calculator and Simpson's rule to find the arc length for $0 \\leqq t \\leqq 1 / 2$ accurate to 6 places.\n```", "answer": "10.30 (a) The curve lies on a right circular cylinder of unit radius, beginning at the point $(1,0,1)$ and rising in helix fashion, approaching $\\infty$ asymptotic to the vertical line $x=\\cos 1, y=\\sin 1$, as $t \\rightarrow 1$.\n\n\n\\begin{equation*}\nL=\\int_{0}^{1 / 2} \\frac{\\sqrt{(1-t)^{4}+1}}{(1-t)^{2}} d t \\approx 1.13209039 \\tag{b}\n\\end{equation*}"}, "10.31": {"question": "```latex\n10.31 Find the exact length of the space curve $\\mathbf{r}=\\left(t^{2}+t \\sqrt{2}, t^{2}-t \\sqrt{2}, 2 t^{3} / 3\\right) \\quad(-1 \\leqq t \\leqq 1)$.\n```", "answer": "not contained in text"}, "10.32": {"question": "```latex\n10.32 (a) Using the arc-length parameterization of the right circular helix,\n\n$$\n\\mathbf{r}=\\left(a \\cos \\frac{s}{c}, a \\sin \\frac{s}{c}, \\frac{b s}{c}\\right) \\quad\\left(c \\equiv \\sqrt{a^{2}+b^{2}}\\right)\n$$\n\nfind the coordinate equations of the tangent line to the helix at any point $P \\equiv \\mathbf{r}(s)$. (b) Show that the tangent line intersects the $x y$-plane at a point $Q \\equiv \\mathbf{r}^{*}(s)$ such that $P Q=s$. (c) By thinking of a string wound along the helix, interpret the result of $(b)$.\n```", "answer": "10.32 (a) $\\mathbf{T}=(-(a / c) \\sin (s / c),(a / c) \\cos (s / c), b / c)$. Hence the tangent line, $\\mathbf{r}(t) \\equiv \\mathbf{r}+t \\mathbf{T}$, has the coordinate equations\n\n$$\nx=a \\cos \\frac{s}{c}-\\frac{a t}{c} \\sin \\frac{s}{c} \\quad y=a \\sin \\frac{s}{c}+\\frac{a t}{c} \\cos \\frac{s}{c} \\quad z=\\frac{b s}{c}+\\frac{b t}{c}\n$$\n\n(b) $Q$ corresponds to $t=-s$ and $P Q=\\|-s \\mathbf{T}\\|=s$.\n\n(c) The interpretation is that $Q$ can be thought of as the free end of the taut string as it is unwound from the helix. [The locus of $Q, \\mathbf{r}^{*}=\\mathbf{r}(s)-s \\mathbf{r}^{\\prime}(s)$, is called an involute of the helix.]"}, "10.33": {"question": "```latex\n10.33 Show that for the curve $y=x^{5}$ in the $x y$-plane, parameterized as $\\mathbf{r}=\\left(t, t^{5}, 0\\right)$, the vector $\\mathbf{T}^{\\prime} /\\left\\|\\mathbf{T}^{\\prime}\\right\\|$ has an essential point of discontinuity at $t=0$.\n```", "answer": "10.33\n\n$$\n\\begin{gathered}\n\\frac{\\mathbf{T}^{\\prime}}{\\left\\|\\mathbf{T}^{\\prime}\\right\\|}=\\frac{t /|t|}{\\left(1+25 t^{8}\\right)^{1 / 2}}\\left(-5 t^{4}, 1,0\\right) \\\\\n\\kappa=\\frac{20 t^{3} \\sqrt{2}}{\\left(1+50 t^{8}\\right)^{3 / 2}} \\quad \\tau=0\n\\end{gathered}\n$$"}, "10.34": {"question": "```latex\n10.34 Find the curvature and the torsion of the curve $\\mathbf{r}=\\left(t, t^{5}+a, t^{5}-a\\right)$.\n```", "answer": "not contained in text"}, "10.35": {"question": "```latex\n10.35 Prove that a curve is planar if and only if its torsion vanishes.\n```", "answer": "10.35 Let the curve $\\mathbf{r}=\\mathbf{r}(s)$ lie in the plane $\\mathbf{b r}=$ const., where $\\mathbf{b}=$ const. and $\\|\\mathbf{b}\\|=1$. Differentiate twice with respect to $s: \\mathbf{b T}=0$ and $\\mathbf{b T}^{\\prime}=0$; hence, $\\mathbf{b}(\\kappa \\mathbf{N})=0$ or $\\mathbf{b N}=0$. It follows that $\\mathbf{b}=\\mathbf{B}$, the binormal vector, so that $\\mathbf{B}^{\\prime}=\\mathbf{0}$ and $\\tau=-\\mathbf{B}^{\\prime} \\mathbf{N}=0$. Conversely, if $\\tau=0$ for a curve $\\mathbf{r}=\\mathbf{r}(s)$, then $\\mathbf{B}^{\\prime}=-\\tau \\mathbf{N}=\\mathbf{0}$ and $\\mathbf{B}$ is a constant unit vector. Define the function $Q(s) \\equiv \\mathbf{B} \\cdot(\\mathbf{r}(s)-\\mathbf{r}(0))$; we have\n\n$$\nQ^{\\prime}=\\mathbf{B r}^{\\prime}=\\mathbf{B} \\mathbf{T}=0\n$$\n\nwhence $Q=$ const. $=Q(0)=0$. Therefore, the curve lies in the plane\n\n$$\n\\mathbf{B r}=\\mathbf{B r}(0)=\\text { const. }\n$$"}, "10.36": {"question": "```latex\n10.36 Prove that a planar curve with constant nonzero curvature $\\kappa$ is a circle. [Hint: $\\mathbf{T}=(\\cos \\theta, \\sin \\theta, 0)$ and $\\mathbf{N}=(-\\sin \\theta, \\cos \\theta, 0)$ imply $\\kappa=\\theta^{\\prime}$ or $\\theta=\\kappa s+a$; show that the radius is $\\left.1 / \\kappa.\\right]$\n```", "answer": "not contained in text"}, "10.37": {"question": "```latex\n10.37 Verify the Serret-Frenet formulas for the circular helix.\n```", "answer": "not contained in text"}, "10.38": {"question": "```latex\n10.38 Show that if included in the range of the map $\\mathbf{r}\\left(x^{1}, x^{2}\\right)$, the vertex of the right circular cone is a singular point.\n```", "answer": "not contained in text"}, "10.39": {"question": "```latex\n10.39 Calculate the unit normal for the catenoid as parameterized in Fig. 10-14 and show that the surface is regular.\n```", "answer": "not contained in text"}, "10.40": {"question": "```latex\n10.40 Find the length of the curve on the right helicoid (Example 10.4) given by $x^{1}=t^{2}, x^{2}=\\ln t$, with $1 \\leqq t \\leqq 2$, in the special case when the parameter $a=1$.\n```", "answer": "10.40\n\n$$\nL=\\int_{1}^{2} \\frac{\\sqrt{5 t^{4}+1}}{t} d t=\\frac{1}{2}\\left[9-\\sqrt{6}+\\ln \\frac{2}{5}(\\sqrt{6}+1)\\right]\n$$"}, "10.41": {"question": "```latex\n10.41 Find the two possible directions for a curve $\\mathscr{C}$ ' in the parameter plane whose image on the paraboloid of Fig. 10-17 meets the circle $x^{2}+y^{2}=4, z=4$ at $P(0,2,4)$ at an angle of $\\pi / 3$.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-161}\n\\end{center}\n\nFig. 10-17\n```", "answer": "10.41 $\\left(v^{1}, v^{2}\\right)=(\\sqrt{12 / 29}, \\sqrt{17 / 29})$ or $(-\\sqrt{12 / 29}, \\sqrt{17 / 29})$"}, "10.42": {"question": "```latex\n10.42 Use Theorem 10.6 to show that an elliptical helix is not in general a geodesic on an elliptical cylinder.\n```", "answer": "10.42 $v=\\sqrt{\\frac{2}{3}} c$"}, "10.43": {"question": "```latex\n10.43 Calculate the Christoffel symbols of the second kind for the right helicoid (Example 10.4). Show that circular helices on the surface are geodesics.\n```", "answer": "10.43 $\\Gamma_{12}^{2}=\\Gamma_{21}^{2}=\\frac{2 x^{1}}{\\left(x^{1}\\right)^{2}+a^{2}} ;$ all others zero"}, "10.44": {"question": "```latex\n10.44 Exhibit the SFF for the general surface of revolution (Problem 10.10).\n```", "answer": "not contained in text"}, "10.45": {"question": "```latex\n10.45 Establish the formulas below for any surface of revolution, with $G \\equiv g^{\\prime} / f^{\\prime}$ (see Problems 10.12 and $10.44)$ :\n\n$$\n\\mathrm{K}=\\frac{G G^{\\prime}}{f f^{\\prime}\\left(1+G^{2}\\right)^{2}} \\quad \\text { and } \\quad \\mathrm{H}=\\frac{f G^{\\prime}+g^{\\prime}\\left(1+G^{2}\\right)}{f\\left|f^{\\prime}\\right|\\left(1+G^{2}\\right)^{3 / 2}}\n$$\n\nUse these formulas to verify that a sphere of radius $a$ has Gaussian curvature $1 / a^{2}$ and mean curvature $-2 / a$.\n```", "answer": "10.45 $v=(4 / 5) c$"}, "10.46": {"question": "```latex\n10.46 (a) Calculate $\\mathrm{K}$ and $\\mathrm{H}$ for two different parameterizations of the paraboloid $z=a\\left(x^{2}+y^{2}\\right)$ : (i) as the surface of revolution for which $f=x^{1}, g=a\\left(x^{1}\\right)^{2}$; (ii) as the surface $\\mathbf{r}=\\left(\\bar{x}^{1}, \\bar{x}^{2}, a\\left(\\bar{x}^{1}\\right)^{2}+a\\left(\\bar{x}^{2}\\right)^{2}\\right) .(b)$ Interpret the results of $(a)$.\n```", "answer": "10.46(a)\\\\\n(i) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{2}}$\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(x^{1}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(x^{1}\\right)^{2}\\right]^{3 / 2}}$\\\\\n(ii) $\\mathrm{K}=\\frac{4 a^{2}}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{2}}$,\\\\\n$\\mathrm{H}=\\frac{4 a\\left[1+2 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+2 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]}{\\left[1+4 a^{2}\\left(\\bar{x}^{1}\\right)^{2}+4 a^{2}\\left(\\bar{x}^{2}\\right)^{2}\\right]^{3 / 2}}$\n\n(b) Consistent with the invariance of $\\mathrm{K}$ and $\\mathrm{H}$, the change of parameters $\\bar{x}^{1}=x^{1} \\cos x^{2}, \\bar{x}^{2}=$ $x^{1} \\sin x^{2}-$ i.e., the transformation from polar to rectangular coordinates in the parameter planetakes the forms (i) into the forms (ii)."}, "10.47": {"question": "```latex\n10.47 Infer from Problem 10.45 that $\\mathrm{H} \\equiv 0$ for any catenoid. [A surface with $\\mathrm{H}=0$ at all points is called a minimal surface. Among minimal surfaces are those that solve \"soap-bubble\" problems, which require a minimum in surface area.]\n```", "answer": "10.47 Approximately $25 \\%$ slow."}, "10.48": {"question": "```latex\n10.48 Prove that $\\Gamma_{i j k}=\\mathbf{r}_{i j} \\mathbf{r}_{k}$. Hint: $\\left.\\quad\\left(\\mathbf{r}_{i} \\mathbf{r}_{j}\\right)_{k}=\\mathbf{r}_{i k} \\mathbf{r}_{j}+\\mathbf{r}_{i} \\mathbf{r}_{j k}.\\right]$\n```", "answer": "10.48 About 45 years old."}, "10.49": {"question": "```latex\n10.49 Surfaces for which the Gaussian curvature is a negative constant are very rare. One such surface can be constructed as follows. (a) A tractrix is the involute of a catenary (see Problem 10.32(c)). Write the vector equation for the involute of the catenary $\\mathbf{r}=\\left(a \\cosh x^{1}, 0, a x^{1}\\right)$ (see Fig. 10-14). (b) Using Problem 10.45, show that $\\mathrm{K}=-1 / a^{2}$ for the tractroid generated by revolving the tractrix of $(a)$ about the $z$-axis.\n```", "answer": "10.49 $\\approx 17000 \\mathrm{mi} / \\mathrm{sec}$"}, "10.50": {"question": "```latex\n10.50 Prove that the catenoid,\n\n$$\nI=\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{1}\\right)^{2}+\\left(a^{2} \\cosh ^{2} x^{1}\\right)\\left(d x^{2}\\right)^{2}\n$$\n\nand the right helicoid,\n\n$$\n\\mathrm{I}=\\left(d \\bar{x}^{1}\\right)^{2}+\\left[\\left(\\bar{x}^{1}\\right)^{2}+a^{2}\\right]\\left(d \\bar{x}^{2}\\right)^{2}\n$$\n\nare locally isometric.\n```", "answer": "10.50 For constants $\\hat{F}$ and $\\hat{\\bar{a}} \\equiv \\hat{F} / m$, and with $\\mathbf{F}=(\\hat{F}, 0,0)$ and $\\mathbf{v}=\\left(v_{x}, 0,0\\right),(12.29)$ becomes identical with (1) of Problem 12.26."}, "11.13": {"question": "Supplementary Problems\n\n11.13 Show that if $v$ is constant, a particle describes equal lengths of arc in equal periods of time.", "answer": "not contained in text"}, "11.14": {"question": "Supplementary Problems\n\n11.14 (a) Show that a particle whose path is given by $\\mathbf{x}=(\\cos t, \\sin t, \\cot t)$, for $\\pi / 4 \\leqq t<\\pi / 2$, has velocity decreasing to $\\sqrt{2}$ as $t \\rightarrow \\pi / 2$. (b) What is the behavior of the acceleration as $t \\rightarrow \\pi / 2$ ? (c) Find the extreme values of $v$ and $a$ for this particle.", "answer": "not contained in text"}, "11.15": {"question": "Supplementary Problems\n\n11.15 For what kind of motion, if any, is $a=d v / d t$ ?", "answer": "11.15 Rectilinear motion [use (11.8) to show that $\\kappa$ must vanish]."}, "11.16": {"question": "Supplementary Problems\n\n11.16 Develop a formula for \u00e0 for a particle that has constant speed $v$.", "answer": "11.16 From (11.7) and (10.9), $\\dot{\\mathbf{a}}=-\\kappa^{2} v^{3} \\mathbf{T}+\\dot{\\kappa} v^{2} \\mathbf{N}+\\kappa \\tau v^{3} \\mathbf{B}$."}, "11.17": {"question": "Supplementary Problems\n\n11.17 Calculate the acceleration components (contravariant) in spherical coordinates $(\\rho, \\varphi, \\theta)$.", "answer": "11.17 $\\nabla^{2} f=\\frac{\\partial^{2} f}{\\partial r^{2}}+\\frac{1}{r^{2}} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{\\partial^{2} f}{\\partial z^{2}}+\\frac{1}{r} \\frac{\\partial f}{\\partial r}$"}, "11.18": {"question": "Supplementary Problems\n\n11.18 Prove that motion under a central force is planar.", "answer": "11.18 Let the center of force be the origin of rectangular coordinates for $\\mathbf{E}^{3}$, with the particle's path given by $\\mathbf{r}=\\mathbf{r}(t)$. By Newton's second law, $f \\mathbf{r}=m \\ddot{\\mathbf{r}}$, so that\n\n$$\n\\frac{d}{d t}(\\mathbf{r} \\times \\dot{\\mathbf{r}})=\\mathbf{r} \\times \\ddot{\\mathbf{r}}=\\mathbf{r} \\times\\left(\\frac{f}{m} \\mathbf{r}\\right)=\\mathbf{0}\n$$\n\nand $\\mathbf{r} \\times \\dot{\\mathbf{r}}=\\mathbf{p}=$ const. It follows that $\\mathbf{p} \\cdot \\mathbf{r}=0$."}, "11.19": {"question": "Supplementary Problems\n\n11.19 Calculate the Laplacian for cylindrical coordinates $(r, \\theta, z)$.", "answer": "11.19 $\\nabla^{2} f=\\frac{\\partial^{2} f}{\\partial r^{2}}+\\frac{1}{r^{2}} \\frac{\\partial^{2} f}{\\partial \\theta^{2}}+\\frac{\\partial^{2} f}{\\partial z^{2}}+\\frac{1}{r} \\frac{\\partial f}{\\partial r}$"}, "11.20": {"question": "Supplementary Problems\n\n11.20 Show that $\\nabla^{2} f=g^{i j} f_{, i j}$. [Hint: Write (11.15) at the origin of Riemannian coordinates.]", "answer": "not contained in text"}, "11.21": {"question": "Supplementary Problems\n\n11.21 Prove (11.22) and (11.23).", "answer": "not contained in text"}, "11.22": {"question": "Supplementary Problems\n\n11.22 Prove that $\\operatorname{curl}(\\operatorname{grad} f)=\\mathbf{0}$ for any $C^{2}$ scalar field $f$.", "answer": "not contained in text"}, "11.23": {"question": "Supplementary Problems\n\n11.23 Show that in a charge-free vacuum, $\\mathbf{H}$ also satisfies the vector wave equation.", "answer": "not contained in text"}, "11.24": {"question": "Supplementary Problems\n\n11.24 Show that, relative to an orthogonal curvilinear coordinate system $\\left(x^{1}, x^{2}, x^{3}\\right)$, an arbitrary contravariant vector $\\mathbf{v}=\\left(v^{i}\\right)$ has the representation\n\n$$\n\\mathbf{v}=v_{(1)} \\mathbf{e}_{1}+v_{(2)} \\mathbf{e}_{2}+v_{(3)} \\mathbf{e}_{3}\n$$\n\nwhere $v_{(\\alpha)}$ is the physical component and $\\mathbf{e}_{\\alpha}$ is the unit normal to the surface $x^{\\alpha}=$ const. [Hint: Use Problems 5.19 and 5.20].", "answer": "not contained in text"}, "12.34": {"question": "12.34 Suppose two events consist of light signals, and an observer sends one of the signals himself. Classify the space-time interval between the events if the observer sees the distant light signal $(a)$ before he sends his own signal, (b) after he sends his own signal, (c) at the same time he sends his own signal.", "answer": "12.34 (a) timelike; (b) spacelike; (c) lightlike"}, "12.35": {"question": "12.35 Assuming that any velocity less than $c$ is attainable, suppose that a concert in Los Angeles begins at 8:0508 p.m. and one in New York City, 3000 miles away (consider this the accurate distance), begins at 8:0506 p.m. could a person physically attend both events (opening measures only)? Is the pair of events timelike or spacelike?", "answer": "12.35 Yes: travel at $4167 \\mathrm{mi} / \\mathrm{sec} \\ll c$. Timelike interval."}, "12.36": {"question": "12.36 Show that the transpose of a Lorentz matrix is Lorentz.", "answer": "12.36 Premultiply $A^{T} G A=G$ by $A G$, and postmultiply by $A^{-1} G$."}, "12.37": {"question": "12.37 Verify the expressions (12.12).", "answer": "not contained in text"}, "12.38": {"question": "12.38 An event occurs at $\\bar{O}$ 's origin at some time $\\bar{t}$. (a) How does $O$ view this event? (b) What is the significance of $a_{0}^{0}>0$ ?", "answer": "12.38 (a) $t=a_{0}^{0} \\bar{t}, x^{1}=-a_{0}^{1} c \\bar{t}, x^{2}=-a_{0}^{2} c \\bar{c}, x^{3}=-a_{0}^{3} c \\bar{c}$. (b) $a_{0}^{0}>0$ if $t$ and $\\bar{t}$ have the same sign; that is, if the\\\\\nclocks of the two observers are both turning clockwise or both counterclockwise.\n\n$$\n\\bar{x}^{0}=\\frac{5}{3} x^{0}-\\frac{4}{3} x^{1} \\quad \\bar{x}^{1}=-\\frac{4}{3} x^{0}+\\frac{5}{3} x^{1} \\quad \\bar{x}^{2}=x^{2} \\quad \\bar{x}^{3}=x^{3}\n$$"}, "12.39": {"question": "12.39 Write out the simple Lorentz transformation connecting inertial frames $O$ and $\\bar{O}$ that move apart at $80 \\%$ of the velocity of light.", "answer": "not contained in text"}, "12.40": {"question": "12.40 (a) Confirm that a photon (a particle with the velocity of light in some inertial frame) will be viewed as having the velocity of light in all other inertial frames. (b) What must be the rest mass of such a particle?", "answer": "12.40\n\n(b) zero"}, "12.41": {"question": "12.41 Show that the following matrix is Lorentz, and use Theorem 12.2 to find the matrices $L^{*}, R_{1}$, and $R_{2}$, and the velocity $v$ between the two observers.\n\n$$\nL=\\left[\\begin{array}{crcr}\n5 / 4 & 1 / 2 & 1 / 4 & -1 / 2 \\\\\n-3 / 4 & -5 / 6 & -5 / 12 & 5 / 6 \\\\\n0 & 2 / 3 & 2 / 15 & 11 / 15 \\\\\n0 & -1 / 3 & 14 / 15 & 2 / 15\n\\end{array}\\right]\n$$", "answer": "12.41\n\n$$\n\\begin{aligned}\nL^{*} & =\\left[\\begin{array}{cccc}\n5 / 4 & -3 / 4 & 0 & 0 \\\\\n-3 / 4 & 5 / 4 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\\\\nv & =(3 / 5) c\n\\end{aligned}\n$$"}, "12.42": {"question": "12.42 Verify that the following matrix is Lorentz and calculate the velocity between the two observers without finding the simple Lorentz matrix $L^{*}$.\n\n$$\nL=\\left[\\begin{array}{cccc}\n3 / \\sqrt{3} & 1 / \\sqrt{3} & 2 / \\sqrt{3} & -1 / \\sqrt{3} \\\\\n1 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & -1 \\\\\n0 & 1 / \\sqrt{3} & -1 / \\sqrt{3} & -1 / \\sqrt{3}\n\\end{array}\\right]\n$$", "answer": "12.42 $v=\\sqrt{\\frac{2}{3}} c$"}, "12.43": {"question": "12.43 Show that by definition the proper-time parameter $\\tau$ is an invariant with respect to all Lorentz transformations", "answer": "not contained in text"}, "12.44": {"question": "12.44 Verify the formula for the composition of velocities by (i) multiplying the two simple Lorentz matrices below; (ii) calculating from (12.15) the velocities belonging to the two matrices and to their product; (iii) showing that the three velocities obey (12.16).\n\n$$\nL_{1}=\\left[\\begin{array}{cccc}\n13 / 12 & 5 / 12 & 0 & 0 \\\\\n5 / 12 & 13 / 12 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right] \\quad L_{2}=\\left[\\begin{array}{cccc}\n17 / 8 & -15 / 8 & 0 & 0 \\\\\n-15 / 8 & 17 / 8 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n$$", "answer": "not contained in text"}, "12.45": {"question": "12.45 An electron gun shoots particles in opposite directions at one-half the velocity of light. At what relative velocity are the particles receding from each other?", "answer": "12.45 $v=(4 / 5) c$"}, "12.46": {"question": "12.46 Show that the composition of two velocities less than $c$ is also less than $c$.", "answer": "not contained in text"}, "12.47": {"question": "12.47 How slow would your watch run relative to a stationary clock if you were moving at $2 / 3$ the velocity of light?", "answer": "12.47 Approximately $25 \\%$ slow."}, "12.48": {"question": "12.48 At the age of 20, an astronaut left her twin brother on earth to go exploring in outer space. The first two years the spaceship gradually accelerated to a cruising speed 95 percent of the velocity of light. Traveling at that speed for 25 years, it reached a distant galaxy (23.75 light years away) and then decelerated for two years. Two years were spent exploring the galaxy before the journey back home, which followed the schedule of the trip outward. How old is the astronaut when she rejoins her 80-year-old brother? (Use an average rate for clock-retardation during the 8 years in acceleration/deceleration.)", "answer": "12.48 About 45 years old."}, "12.49": {"question": "12.49 How fast would a pole-vaulter have to run for his 20-foot pole to fit (instantaneously) inside a barn, in the judgment of a ground observer for whom the barn is 19 feet 11 inches long?", "answer": "12.49 $\\approx 17000 \\mathrm{mi} / \\mathrm{sec}$"}, "12.50": {"question": "12.50 An alternate definition of uniformly accelerated motion is motion under a constant Lorentz force. Verify that the two definitions are equivalent for one-dimensional motion.", "answer": "12.50 For constants $\\hat{F}$ and $\\hat{\\bar{a}} \\equiv \\hat{F} / m$, and with $\\mathbf{F}=(\\hat{F}, 0,0)$ and $\\mathbf{v}=\\left(v_{x}, 0,0\\right),(12.29)$ becomes identical with (1) of Problem 12.26."}, "12.51": {"question": "12.51 Show that $g^{r s} a_{r}^{i} a_{s}^{j}=g^{i j}$.", "answer": "not contained in text"}, "12.52": {"question": "12.52 Prove that the array (12.46) is a 4-vector.", "answer": "12.52 Since $\\partial \\bar{s}^{i} / \\partial \\bar{x}^{i}=0=\\partial s^{i} / \\partial x^{i}$ (the equation of continuity), $\\left(s^{i}\\right)$ may be identified with the vector $\\left(S^{i}\\right)$ of Problem 12.32."}, "12.53": {"question": "12.53 Show that the matrices $\\tilde{\\mathscr{F}}$ and $\\mathscr{F}$ of (12.44) are connected via $\\tilde{F}^{i j}=\\frac{1}{2} e_{i j k l} g_{k r} g_{l s} F^{r s}$. [Hint: First evaluate the matrix product $G F G \\equiv P$.]", "answer": "not contained in text"}, "12.54": {"question": "12.54 Define Faraday's two-form by\n\n$$\n\\Phi \\equiv G \\tilde{\\mathscr{F}} G=\\left[\\begin{array}{cccc}\n0 & -E_{1} & -E_{2} & -E_{3} \\\\\nE_{1} & 0 & H_{3} & -H_{2} \\\\\nE_{2} & -H_{3} & 0 & H_{1} \\\\\nE_{3} & H_{2} & -H_{1} & 0\n\\end{array}\\right]=\\left[\\Phi_{i j}\\right]_{44}\n$$\n\nor, inversely, $\\tilde{\\mathscr{F}}=G \\Phi G$. Show that $(a) \\mathscr{F}$ is related to $\\Phi$ through $F^{i j}=-\\frac{1}{2} e_{i j k l} \\Phi_{k l} ;(b)$ Maxwell's equations can be written in terms of the single matrix $\\Phi$ as\n\n$$\n\\frac{\\partial \\Phi_{i j}}{\\partial x^{k}}+\\frac{\\partial \\Phi_{k i}}{\\partial x^{j}}+\\frac{\\partial \\Phi_{j k}}{\\partial x^{i}}=0 \\quad g_{i k} g_{i l} \\frac{\\partial \\Phi_{k l}}{\\partial x^{j}}=s^{i}\n$$", "answer": "12.54 (a) By analogy with the evaluation of $\\frac{1}{2} e_{i j k l} P_{k l}$ in Problem 12.53,\n\n$$\n\\left[\\frac{1}{2} e_{i j k l}\\left(-\\Phi_{k l}\\right)\\right]_{44}=\\left[\\begin{array}{cccc}\n0 & -\\Phi_{23} & \\Phi_{13} & -\\Phi_{12} \\\\\n* & 0 & -\\Phi_{03} & \\Phi_{02} \\\\\n* & * & 0 & -\\Phi_{01} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n0 & -H_{1} & -H_{2} & -H_{3} \\\\\n* & 0 & E_{3} & -E_{2} \\\\\n* & * & 0 & E_{1} \\\\\n* & * & * & 0\n\\end{array}\\right]=\\left[F^{i j}\\right]_{44}\n$$\n\n(b) Let $(a b c d)$ denote a permutation of (0123). Then $\\Phi_{a b}=-e_{a b c d} F^{c d} \\quad$ (no summation) and\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\Phi_{a b}}{\\partial x^{c}}+\\frac{\\partial \\Phi_{c a}}{\\partial x^{b}}+\\frac{\\partial \\Phi_{b c}}{\\partial x^{a}} & =-e_{a b c d} \\frac{\\partial F^{c d}}{\\partial x^{c}}-e_{c a b d} \\frac{\\partial F^{b d}}{\\partial x^{b}}-e_{b c a d} \\frac{\\partial F^{a d}}{\\partial x^{a}} \\\\\n& =-e_{a b c d}\\left(\\frac{\\partial F^{c d}}{\\partial x^{c}}+\\frac{\\partial F^{b d}}{\\partial x^{b}}+\\frac{\\partial F^{a d}}{\\partial x^{d}}\\right)= \\pm \\frac{\\partial F^{j d}}{\\partial x^{j}}=0\n\\end{aligned}\n$$\n\nThe second set of equations is derivable directly from $(12.45 b)$, the definition of $\\Phi$, and the fact that the $g_{i j}$ are constants."}, "12.55": {"question": "12.55 The energy flux in an electromagnetic field is specified by the Poynting vector, $\\mathbf{p}=\\mathbf{E} \\times \\mathbf{H}$. By direct matrix multiplication or otherwise, derive the formula\n\n$$\n\\frac{1}{2}(\\tilde{\\mathscr{F}} \\mathscr{F}-\\tilde{F} \\tilde{\\mathscr{F}})=\\left[\\begin{array}{cccc}\n0 & 0 & 0 & 0 \\\\\n* & 0 & p_{3} & -p_{2} \\\\\n* & * & 0 & p_{1} \\\\\n* & * & * & 0\n\\end{array}\\right] \\text { (antisymmetric matrix) }\n$$", "answer": "not contained in text"}, "12.56": {"question": "12.56 Verify that for simple Lorentz matrices $A$ (hence, no rotation of axes allowed): (a) $\\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V})=$ $G \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) G ;(b) \\overline{\\mathscr{F}}(\\overline{\\mathbf{V}}, \\overline{\\mathbf{U}})=B^{T} \\mathscr{F}(\\mathbf{V}, \\mathbf{U}) B$, where $B=A^{-1} ;$ and $(c) \\tilde{\\mathscr{F}}(\\overline{\\mathbf{U}}, \\overline{\\mathbf{V}})=A \\tilde{\\mathscr{F}}(\\mathbf{U}, \\mathbf{V}) A^{T}$ (thereby proving that $\\left(F^{i j}\\right)$ is a contravariant tensor under simple Lorentz transformations).", "answer": "not contained in text"}, "12.57": {"question": "12.57 Prove that if $A \\equiv\\left[A_{i j}\\right]_{n n}$ satisfies $A_{i j} B_{i j}=0$ for every $B \\equiv\\left[B_{i j}\\right]_{n n}$ that has zero trace $\\left(B_{i i}=0\\right)$, then $A=\\lambda I$, for some real $\\lambda$. [Hint: First take $B$ as having all elements zero, except for one off-diagonal element. Then choose $B_{\\alpha \\alpha}=-B_{\\beta \\beta}=1 \\quad(\\alpha \\neq \\beta ;$ no summation $)$, with all other $B_{i j}$ zero.]", "answer": "not contained in text"}, "13.22": {"question": "```latex\n13.22 The set of all $2 \\times 2$ matrices of the form\n\n$$\n\\left[\\begin{array}{rr} \n\\pm 1 & 0 \\\\\n0 & \\pm 1\n\\end{array}\\right]\n$$\n\nwhere all possible combinations of signs are taken, forms a four-element subset of $\\mathbf{G L}(2, \\mathbf{R})$. Is it a subgroup?\n```", "answer": "13.22 Yes; it is isomorphic to the 4-group."}, "13.23": {"question": "```latex\n13.23 Prove that $\\mathrm{SU}(n)$, the set of all $n \\times n$ matrices over the complex numbers having determinant +1 , is a subgroup of $\\mathbf{G L}(n, \\mathbf{C})$. [Hint: $\\operatorname{det} A B=(\\operatorname{det} A)(\\operatorname{det} B)$ holds for complex matrices.]\n```", "answer": "not contained in text"}, "13.24": {"question": "```latex\n13.24 Show that the operator $L(f)=\\int_{0}^{1} f(x) d x$ is a linear functional over the set of continuous, real-valued functions on $[0,1]$.\n```", "answer": "not contained in text"}, "13.25": {"question": "```latex\n13.25 In terms of the standard basis $\\left\\{d x^{i}\\right\\}$ of $\\left(\\mathbf{R}^{3}\\right)^{*}$, a new basis is defined by\n\n$$\n\\boldsymbol{\\beta}^{1}=d x^{1}-2 d x^{3} \\quad \\boldsymbol{\\beta}^{2}=2 d x^{1}+d x^{2} \\quad \\boldsymbol{\\beta}^{3}=d x^{1}+d x^{3}\n$$\n\nFind the corresponding dual basis $\\left\\{\\mathbf{b}_{i}\\right\\}$ for $\\mathbf{R}^{3}$ in terms of $\\left(\\mathbf{e}_{i}\\right)$, using (13.6). Check your answer by making several calculations of the form $\\omega(v)=\\bar{\\omega}(\\bar{v})$ (a change of basis does not affect the value a linear functional assigns to a vector).\n```", "answer": "not contained in text"}, "13.26": {"question": "```latex\n13.26 Consider a tensor $T(\\boldsymbol{\\omega} ; \\mathbf{v})$ over a vector space of dimension $n$ and its dual, with components $T_{j}^{i}$. (a) Show that the trace $\\tau(T) \\equiv T_{i}^{i}$ is invariant under changes of bases. (b) Find $\\tau(T)$ for the tensor defined by $T(\\boldsymbol{\\omega} ; \\mathbf{v})=\\boldsymbol{\\omega}(\\mathbf{v})$.\n```", "answer": "13.26 (a) By (13.6),\n\n$$\n\\bar{T}_{i}^{i}=T\\left(\\overline{\\boldsymbol{\\beta}}^{i}, \\overline{\\mathbf{b}}_{i}\\right)=T\\left(\\bar{A}_{j}^{i} \\boldsymbol{\\beta}^{j}, A_{i}^{k} \\mathbf{b}_{k}\\right)=\\bar{A}_{j}^{i} A_{i}^{k} T\\left(\\boldsymbol{\\beta}^{j}, \\mathbf{b}_{k}\\right)=\\delta_{j}^{k} T_{k}^{j}=T_{j}^{j}\n$$\n\n(b) $\\tau(T)=n$"}, "13.27": {"question": "```latex\n13.27 Show that every metric tensor $G$ induces a one-to-one mapping (which is an isomorphism, since it is linear) $\\hat{G}: \\mathbf{V} \\rightarrow \\mathbf{V}^{*}$ from a vector space to its dual, under the definition: For each fixed $\\mathbf{u}$ in $\\mathbf{V}$, let $\\hat{G}(\\mathbf{u})$ be the linear functional $\\hat{G}(\\mathbf{u})(\\mathbf{v})=G(\\mathbf{u}, \\mathbf{v})$, for all $\\mathbf{v}$ in $\\mathbf{V}$. This proves for vector spaces of arbitrary dimension:\n\nTheorem 13.3: If $\\mathbf{V}$ possesses a metric tensor, then $\\mathbf{V}$ is isomorphic to its dual $\\mathbf{V}^{*}$.\n```", "answer": "13.27 Suppose that $\\hat{G}\\left(\\mathbf{u}_{1}\\right)=\\hat{G}\\left(\\mathbf{u}_{2}\\right)$. Then $G\\left(\\mathbf{u}_{1}, \\mathbf{v}\\right)=G\\left(\\mathbf{u}_{2}, \\mathbf{v}\\right)$, or by symmetry, $G\\left(\\mathbf{v}, \\mathbf{u}_{1}\\right)=G\\left(\\mathbf{v}, \\mathbf{u}_{2}\\right)$ for all $\\mathbf{v}$ By nonsingularity, $\\mathbf{u}_{1}=\\mathbf{u}_{2}$."}, "13.28": {"question": "```latex\n13.28 Find a convenient atlas showing that the set in $\\mathbf{R}^{4}$ given by the equation\n\n$$\n\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}-\\left(y^{4}\\right)^{2}=a^{2}\n$$\n\ncan be made into a $C^{\\infty} 3$-manifold. [Hint: Use radicals, as in Example 13.11(b); here, 6 charts will suffice.]\n```", "answer": "13.28\n\n$$\n\\begin{aligned}\n& \\varphi_{p}^{-1}: \\begin{cases}y^{1}=\\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{2}=x^{1} & \\\\\ny^{3}=x^{2} & \\mathbf{U}_{p}: y^{1}>0 \\\\\ny^{4}=x^{3} & p=(a, 0,0,0)\\end{cases}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231}\n\\end{center}\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(2)}\n\\end{center}\n\n$$\n\\begin{aligned}\n& \\varphi_{ \\pm r}^{-1}:\\left\\{\\begin{array}{l|l}\ny^{1}=x^{1} & \\mathbf{U}_{r}: y^{3}>0 \\\\\ny^{2}=x^{2} & \\begin{array}{c}\n\\mathbf{U}_{-r}: y^{3}<0 \\\\\nr=(0,0, a, 0)\n\\end{array} \\\\\ny^{3}= \\pm \\sqrt{a^{2}-\\left(x^{1}\\right)^{2}-\\left(x^{2}\\right)^{2}+\\left(x^{3}\\right)^{2}} \\\\\ny^{4}=x^{3}\n\\end{array}\\right.\n\\end{aligned}\n$$"}, "13.29": {"question": "```latex\n13.29 Show that the restriction of $\\sigma=y^{1} d y^{2}-y^{2} d y^{1}+y^{3} d y^{4}-y^{4} d y^{3}$ on $\\mathbf{R}^{4}$ to the sphere $\\mathbf{S}^{3}$ is a nonzero vector field on $\\mathbf{S}^{3}$.\n```", "answer": "not contained in text"}, "13.30": {"question": "```latex\n13.30 Extend Problem 13.29 to the sphere $\\mathbf{S}^{2 k-1}(k \\geqq 2)$.\n```", "answer": "13.30\n\n$$\n\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}+y^{6} d y^{5}-y^{5} d y^{6}+\\cdots+y^{2 k} d y^{2 k-1}-y^{2 k-1} d y^{2 k}\n$$"}, "13.31": {"question": "```latex\n13.31 Show that if there are only two points, $p_{1}$ and $p_{2}$, on $\\mathbf{S}^{2}$ where a vector field is zero, those points must be antipodal (endpoints of a diameter).\n```", "answer": "13.31 If $p_{1}$ and $p_{2}$ are not antipodal, there exists a closed hemisphere containing neither one, on which the given (continuous) vector field is nonzero-an impossibility by Problem 13.17(b)."}, "13.32": {"question": "```latex\n13.32 Show, by geometric reasoning, that there exists a continuous, nonzero vector field on the torus.\n```", "answer": "13.32 As shown in Fig. 13-10, let a unit tangent vector be constructed to a generating circle; as the circle is revolved to generate the torus, the tangent vector is obviously propagated continuously to all points of the torus.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(1)}\n\\end{center}\n\nFig. 13-10"}, "13.33": {"question": "```latex\n13.33 Show that the restrictions of the following one-forms to $\\mathbf{S}^{4},\\left(y^{1}\\right)^{2}+\\cdots+\\left(y^{5}\\right)^{2}=1$, are vector fields on $\\mathbf{S}^{4}$, and find the points where they are zero:\n\n(a) $\\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}$\n\n(b) $\\sigma=\\left(y^{2}-y^{3}-y^{4}\\right) d y^{1}+\\left(y^{3}-y^{1}\\right) d y^{2}+\\left(y^{1}-y^{2}+y^{5}\\right) d y^{3}+y^{1} d y^{4}-y^{3} d y^{5}$\n```", "answer": "13.33 Zero points are: $(a)(0,0,0,0, \\pm a) ;(b) \\pm(0, a / \\sqrt{3}, 0, a / \\sqrt{3}, a / \\sqrt{3})$."}, "13.35": {"question": "```latex\n13.35 Without resorting to coordinate patches, express extrinsically the collection of tangent spaces $T(\\mathbf{M})$, if $\\mathbf{M}$ is the hyperboloid of one sheet $\\left(y^{1}\\right)^{2}-4\\left(y^{2}\\right)^{2}+4\\left(y^{3}\\right)^{2}=4$.\n```", "answer": "13.35 With $\\omega \\equiv 2\\left(y^{1} d y^{1}-4 y^{2} d y^{2}+4 y^{3} d y^{3}\\right), \\sigma=f d y^{1}+g d y^{2}+h d y^{3}$ must be orthogonal to $\\omega$, for $C^{\\infty}$ functions $f, g, h$. Hence,\n\n$$\ny^{1} f-4 y^{2} g+4 y^{3} h=0\n$$\n\nReplace $f, g$ by $4 y^{3} F, y^{3} G$, and solve for $h$. Similarly, replace $g$ by $y^{1} G$ and $h$ by $y_{1} H$ and solve for $f$; etc. All possible tangent vectors are given by one of three distinct types $\\left(F, G, H\\right.$ denote arbitrary $C^{\\infty}$ functions of $y^{1}, y^{2}, y^{3}$ ):\n\n(1) $\\sigma=4 y^{3} F d y^{1}+y^{3} G d y^{2}+\\left(y^{2} G-y^{1} F\\right) d y^{3}$\n\n(2) $\\sigma=\\left(4 y^{2} G-4 y^{3} H\\right) d y^{1}+y^{1} G d y^{2}+y^{1} H d y^{3}$\n\n(3) $\\sigma=4 y^{2} F d y^{1}+\\left(y^{1} F+y^{3} H\\right) d y^{2}+y^{2} H d y^{3}$"}, "13.36": {"question": "```latex\n13.36 For the manifold $\\mathbf{M}$ of Problem 13.35, consider the coordinate patch\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=\\sqrt{1-\\left(x^{1} / 2\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n\nvalid for $y^{3}>0$. Find an expression for an arbitrary vector in $T_{p}(\\mathbf{M})$.\n```", "answer": "13.36\n\n$$\nU=U_{1}^{i} \\mathbf{r}_{i} \\equiv\\left(2 U^{1} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}}, 2 U^{2} \\sqrt{4-\\left(x^{1}\\right)^{2}+4\\left(x^{2}\\right)^{2}},-x^{1} U^{1}+4 x^{2} U^{2}\\right)\n$$\n\nfor any two $C^{\\infty}$ functions $U^{1}, U^{2}$ on $\\left(x^{1}, x^{2}\\right)$."}, "13.37": {"question": "```latex\n13.37 One way to show that two surfaces meet at right angles is to show that along the curve of intersection the normal vector to one lies in the tangent space of the other. Illustrate this idea for the sphere $\\left(y^{1}\\right)^{2}+\\left(y^{2}\\right)^{2}+\\left(y^{3}\\right)^{2}=16$ and the cone $\\left(y^{3}\\right)^{2}=9\\left(y^{1}\\right)^{2}+9\\left(y^{2}\\right)^{2}$, the latter coordinatized by\n\n$$\ny^{1}=x^{1} \\quad y^{2}=x^{2} \\quad y^{3}=3 \\sqrt{\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}}\n$$\n```", "answer": "13.37 The normal vector to the sphere is represented by $\\sigma=y^{1} d y^{1}+y^{2} d y^{2}+y^{3} d y^{3}$; the tangent space of the cone is given by\n\n$$\n\\left(U_{1}, U_{2},\\left(3 x^{1} U^{1}+3 x^{2} U^{2}\\right)\\left(\\left(x^{1}\\right)^{2}+\\left(x^{2}\\right)^{2}\\right)^{-1 / 2}\\right)\n$$\n\nSet $U^{1}=y^{1}$ and $U^{2}=y^{2}$."}}]