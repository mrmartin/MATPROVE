\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{mathrsfs}
\usepackage{multirow}

\DeclareUnicodeCharacter{0131}{$\imath$}

\begin{document}
\section*{Chapter 1}
\section*{The Einstein Summation Convention}
\subsection*{1.1 INTRODUCTION}
A study of tensor calculus requires a certain amount of background material that may seem unimportant in itself, but without which one could not proceed very far. Included in that prerequisite material is the topic of the present chapter, the summation convention. As the reader proceeds to later chapters he or she will see that it is this convention which makes the results of tensor analysis surveyable.

\subsection*{1.2 REPEATED INDICES IN SUMS}
A certain notation introduced by Einstein in his development of the Theory of Relativity streamlines many common algebraic expressions. Instead of using the traditional sigma for sums, the strategy is to allow the repeated subscript to become itself the designation for the summation. Thus,

$$
a_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}+\cdots+a_{n} x_{n} \equiv \sum_{i=1}^{n} a_{i} x_{i}
$$

becomes just $a_{i} x_{i}$, where $1 \leqq i \leqq n$ is adopted as the universal range for summation.

EXAMPLE 1.1 The expression $a_{i j} x_{k}$ does not indicate summation, but both $a_{i i} x_{k}$ and $a_{i j} x_{j}$ do so over the respective ranges $1 \leqq i \leqq n$ and $1 \leqq j \leqq n$. If $n=4$, then

$$
\begin{aligned}
& a_{\mathrm{ii}} x_{k} \equiv a_{11} x_{k}+a_{22} x_{k}+a_{33} x_{k}+a_{44} x_{k} \\
& a_{i j} x_{j} \equiv a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}+a_{i 4} x_{4}
\end{aligned}
$$

\section*{Free and Dummy Indices}
In Example 1.1, the expression $a_{i j} x_{j}$ involves two sorts of indices. The index of summation, $j$, which ranges over the integers $1,2,3, \ldots, n$, cannot be preempted. But at the same time, it is clear that the use of the particular character $j$ is inessential; e.g., the expressions $a_{i r} x_{r}$ and $a_{i v} x_{v}$ represent exactly the same sum as $a_{i j} x_{j}$ does. For this reason, $j$ is called a dummy index. The index $i$, which may take on any particular value $1,2,3, \ldots, n$ independently, is called a free index. Note that, although we call the index $i$ "free" in the expression $a_{i j} x_{j}$, that "freedom" is limited in the sense that generally, unless $i=k$,

$$
a_{i \mathrm{j}} x_{\mathrm{j}} \neq a_{k \mathrm{j}} x_{\mathrm{j}}
$$

EXAMPLE 1.2 If $n=3$, write down explicitly the equations represented by the expression $y_{i}=a_{i r} x_{r}$.

Holding $i$ fixed and summing over $r=1,2,3$ yields

$$
y_{i}=a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}
$$

Next, setting the free index $i=1,2,3$ leads to three separate equations:

$$
\begin{aligned}
& y_{1}=a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3} \\
& y_{2}=a_{21} x_{1}+a_{22} x_{2}+a_{23} x_{3} \\
& y_{3}=a_{31} x_{1}+a_{32} x_{2}+a_{33} x_{3}
\end{aligned}
$$

\section*{Einstein Summation Convention}
Any expression involving a twice-repeated index (occurring twice as a subscript, twice as a superscript, or once as a subscript and once as a superscript) shall automatically stand for its sum\\
over the values $1,2,3, \ldots, n$ of the repeated index. Unless explicitly stated otherwise, the single exception to this rule is the character $n$, which represents the range of all summations.

Remark 1: Any free index in an expression shall have the same range as summation indices, unless stated otherwise.

Remark 2: No index may occur more than twice in any given expression.

EXAMPLE 1.3 (a) According to Remark 2, an expression like $a_{i i} x_{i}$ is without meaning. (b) The meaningless expression $a_{i}^{i} x_{i} x_{i}$ might be presumed to represent $a_{j}^{i}\left(x_{i}\right)^{2}$, which is meaningful. (c) An expression of the form $a_{i}\left(x_{i}+y_{i}\right)$ is considered well-defined, for it is obtained by composition of the meaningful expressions $a_{i} z_{i}$ and $x_{i}+y_{i}=z_{i}$. In other words, the index $i$ is regarded as occuring once in the term $\left(x_{i}+y_{i}\right)$.

\subsection*{1.3 DOUBLE SUMS}
An expression can involve more than one summation index. For example, $a_{i j} x_{i} y_{j}$ indicates a summation taking place on both $i$ and $j$ simultaneously. If an expression has two summation (dummy) indices, there will be a total of $n^{2}$ terms in the sum; if there are three indices, there will be $n^{3}$ terms; and so on. The expansion of $a_{i j} x_{i} y_{j}$ can be arrived at logically by first summing over $i$, then over $j$ :

$$
\begin{array}{cc}
a_{i j} x_{i} y_{j}=a_{1 j} x_{1} y_{j}+a_{2 j} x_{2} y_{j}+a_{3 j} x_{3} y_{j}+\cdots+a_{n j} x_{n} y_{j} & \text { [summed over } i] \\
=\left(a_{11} x_{1} y_{1}+a_{12} x_{1} y_{2}+\cdots+a_{1 n} x_{1} y_{n}\right) & \text { [summed over } j] \\
+\left(a_{21} x_{2} y_{1}+a_{22} x_{2} y_{2}+\cdots+a_{2 n} x_{2} y_{n}\right) & \\
+\left(a_{31} x_{3} y_{1}+a_{32} x_{3} y_{2}+\cdots+a_{3 n} x_{3} y_{n}\right) & \\
\left.\cdots \cdots \cdots \cdots \cdots \cdots \cdots+a_{n n} x_{n} y_{n}\right) &
\end{array}
$$

The result is the same if one sums over $j$ first, and then over $i$.

EXAMPLE 1.4 If $n=2$, the expression $y_{i}=c_{i}^{r} a_{r s} x_{s}$ stands for the two equations:

$$
\begin{aligned}
& y_{1}=c_{1}^{1} a_{11} x_{1}+c_{1}^{2} a_{21} x_{1}+c_{1}^{1} a_{12} x_{2}+c_{1}^{2} a_{22} x_{2} \\
& y_{2}=c_{2}^{1} a_{11} x_{1}+c_{2}^{2} a_{21} x_{1}+c_{2}^{1} a_{12} x_{2}+c_{2}^{2} a_{22} x_{2}
\end{aligned}
$$

\subsection*{1.4 SUBSTITUTIONS}
Suppose it is required to substitute $y_{i}=a_{i j} x_{j}$ in the equation $Q=b_{i j} y_{i} x_{j}$. Disregard of Remark 2 above would lead to an absurd expression like $Q=b_{i j} a_{i j} x_{j} x_{j}$. The correct procedure is first to identify any dummy indices in the expression to be substituted that coincide with indices occurring in the main expression. Changing these dummy indices to characters not found in the main expression, one may then carry out the substitution in the usual fashion.

STEP $1 Q=b_{i j} y_{i} x_{j}, \quad y_{i}=a_{i j} x_{j} \quad$ [dummy index $j$ is duplicated]

STEP $2 y_{i}=a_{i r} x_{r} \quad$ [change dummy index from $j$ to $r$ ]

STEP $3 Q=b_{i j}\left(a_{i r} x_{r}\right) x_{j}=a_{i r} b_{i j} x_{r} x_{j} \quad$ [substitute and rearrange]

EXAMPLE 1.5 If $y_{i}=a_{i j} x_{j}$, express the quadratic form $Q=g_{i j} y_{i} y_{j}$ in terms of the $x$-variables.

First write: $y_{i}=a_{i r} x_{r}, y_{j}=a_{j s} x_{s}$. Then, by substitution,

$$
Q=g_{i j}\left(a_{i r} x_{r}\right)\left(a_{j s} x_{s}\right)=g_{i j} a_{i r} a_{j s} x_{r} x_{s}
$$

or $Q=h_{r s} x_{r} x_{s}$, where $h_{r s} \equiv g_{i j} a_{i r} a_{j s}$.

\subsection*{1.5 KRONECKER DELTA AND ALGEBRAIC MANIPULATIONS}
A much used symbol in tensor calculus has the effect of annihilating the "off-diagonal" terms in a double summation.

\section*{Kronecker Delta}
\[
\delta_{i j} \equiv \delta_{j}^{i} \equiv \delta^{i j} \equiv \begin{cases}1 & i=j  \tag{1.1}\\ 0 & i \neq j\end{cases}
\]

Clearly, $\delta_{i j}=\delta_{j i}$ for all $i, j$.

EXAMPLE 1.6 If $n=3$,

$$
\begin{aligned}
\delta_{i j} x_{i} x_{j} & =1 x_{1} x_{1}+0 x_{1} x_{2}+0 x_{1} x_{3}+0 x_{2} x_{1}+1 x_{2} x_{2}+0 x_{2} x_{3}+0 x_{3} x_{1}+0 x_{3} x_{2}+1 x_{3} x_{3} \\
& =\left(x_{1}\right)^{2}+\left(x_{2}\right)^{2}+\left(x_{3}\right)^{2}=x_{i} x_{i}
\end{aligned}
$$

In general, $\delta_{i j} x_{i} x_{j}=x_{i} x_{i}$ and $\delta_{j}^{r} a_{i r} x_{i}=a_{i j} x_{i}$.

EXAMPLE 1.7 Suppose that $T^{i}=g_{r}^{i} a_{r s} y_{s}$ and $y_{i}=b_{i r} x_{r}$. If further, $a_{i r} b_{r j}=\delta_{i j}$, find $T^{i}$ in terms of the $x_{r}$.

First write $y_{s}=b_{s t} x_{t}$. Then, by substitution,

$$
T^{i}=g_{r}^{i} a_{r s} b_{s t} x_{t}=g_{r}^{i} \delta_{r t} x_{t}=g_{r}^{i} x_{r}
$$

\section*{Algebra and the Summation Convention}
Certain routine manipulations in tensor calculus can be easily justified by properties of ordinary sums. However, some care is warranted. For instance, the identity (1.2) below not only involves the distributive law for real numbers, $a(x+y) \equiv a x+a y$, but also requires a rearrangement of terms utilizing the associative and commutative laws. At least a mental verification of such operations must be made, if false results are to be avoided.

EXAMPLE 1.8 The following nonidentities should be carefully noted:

$$
\begin{gathered}
a_{i j}\left(x_{i}+y_{j}\right) \not \equiv a_{i j} x_{i}+a_{i j} y_{j} \\
a_{i j} x_{i} y_{j} \neq a_{i j} y_{i} x_{j} \\
\left(a_{i j}+a_{j i}\right) x_{i} y_{j} \neq 12 a_{i j} x_{i} y_{j}
\end{gathered}
$$

Listed below are several valid identities; they, and others like them, will be used repeatedly from now on.


\begin{gather*}
a_{i j}\left(x_{j}+y_{j}\right) \equiv a_{i j} x_{j}+a_{i j} y_{j}  \tag{1:2}\\
a_{i j} x_{i} y_{j} \equiv a_{i j} y_{j} x_{i}  \tag{1.3}\\
a_{i j} x_{i} x_{j} \equiv a_{j i} x_{i} x_{j}  \tag{1.4}\\
\left(a_{i j}+a_{j i}\right) x_{i} x_{j} \equiv 2 a_{i j} x_{i} x_{j}  \tag{1.5}\\
\left(a_{i j}-a_{j i}\right) x_{i} x_{j} \equiv 0 \tag{1.6}
\end{gather*}


\section*{Solved Problems}
\section*{REPEATED INDICES}
1.1 Use the summation convention to write the following, and assign the value of $n$ in each case:

(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{41} b_{14}$

(b) $a_{11} b_{11}+a_{12} b_{12}+a_{13} b_{13}+a_{14} b_{14}+a_{15} b_{15}+a_{16} b_{16}$

(c) $c_{11}^{i}+c_{22}^{i}+c_{33}^{i}+c_{44}^{i}+c_{55}^{i}+c_{66}^{i}+c_{77}^{i}+c_{88}^{i} \quad(1 \leqq i \leqq 8)$

(a) $a_{i 1} b_{1 i} \quad(n=4) ;(b) a_{1 i} b_{1 i} \quad(n=6) ;(c) c_{j j}^{i} \quad(n=8)$.

1.2 Use the summation convention to write each of the following systems, state which indices are free and which are dummy indices, and fix the value of $n$ :

$$
\begin{array}{lc}
c_{11} x_{1}+c_{12} x_{2}+c_{13} x_{3}=2 & \text { (b) } a_{j}^{1} x_{1}+a_{j}^{2} x_{2}+a_{j}^{3} x_{3}+a_{j}^{4} x_{4}=b_{j} \\
c_{21} x_{1}+c_{22} x_{2}+c_{23} x_{3}=-3 & (j=1,2) \\
c_{31} x_{1}+c_{32} x_{2}+c_{33} x_{3}=5 &
\end{array}
$$

(a) Set $d_{1}=2, d_{2}=-3$, and $d_{3}=5$. Then one can write the system as $c_{i j} x_{j}=d_{i} \quad(n=3)$. The free index is $i$ and the dummy index is $j$.

(b) Here, the range of the free index does not match that of the dummy index $(n=4)$, and this fact must be indicated:

$$
a_{j}^{i} x_{i}=b_{j} \quad(j=1,2)
$$

The free index is $j$ and the dummy index is $i$.

1.3 Write out explicitly the summations

$$
c_{i}\left(x_{i}+y_{i}\right) \quad c_{j} x_{j}+c_{k} y_{k}
$$

where $n=4$ for both, and compare the results.

$$
\begin{aligned}
c_{i}\left(x_{i}+y_{i}\right) & =c_{1}\left(x_{1}+y_{1}\right)+c_{2}\left(x_{2}+y_{2}\right)+c_{3}\left(x_{3}+y_{3}\right)+c_{4}\left(x_{4}+y_{4}\right) \\
& =c_{1} x_{1}+c_{1} y_{1}+c_{2} x_{2}+c_{2} y_{2}+c_{3} x_{3}+c_{3} y_{3}+c_{4} x_{4}+c_{4} y_{4} \\
c_{j} x_{j}+c_{k} y_{k} & =c_{1} x_{1}+c_{2} x_{2}+c_{3} x_{3}+c_{4} x_{4}+c_{1} y_{1}+c_{2} y_{2}+c_{3} y_{3}+c_{4} y_{4}
\end{aligned}
$$

The two summations are identical except for the order in which the terms occur, constituting a special case of (1.2).

\section*{DOUBLE SUMS}
1.4 If $n=3$, expand $Q=a^{i j} x_{i} x_{j}$.

$$
\begin{aligned}
Q & =a^{1 j} x_{1} x_{j}+a^{2 j} x_{2} x_{j}+a^{3 j} x_{3} x_{j} \\
& =a^{11} x_{1} x_{1}+a^{12} x_{1} x_{2}+a^{13} x_{1} x_{3}+a^{21} x_{2} x_{1}+a^{22} x_{2} x_{2}+a^{23} x_{2} x_{3}+a^{31} x_{3} x_{1}+a^{32} x_{3} x_{2}+a^{33} x_{3} x_{3}
\end{aligned}
$$

1.5 Use the summation convention to write the following, and state the value of $n$ necessary in each case:

(a) $a_{11} b_{11}+a_{21} b_{12}+a_{31} b_{13}+a_{12} b_{21}+a_{22} b_{22}+a_{32} b_{23}+a_{13} b_{31}+a_{23} b_{32}+a_{33} b_{33}$

?

(b) " $g_{11}^{1}+g_{12}^{1}+\dot{g}_{21}^{1}+g_{22}^{1}+g_{11}^{2}+g_{12}^{2}+g_{21}^{2}+g_{22}^{2}$\\
(a) $a_{i 1} b_{1 i}+a_{i 2} b_{2 i}+a_{i 3} b_{3 i} \equiv a_{i j} b_{j i} \quad(n=3)$.

(b) Set $c_{i}=1$ for each $i \quad(n=2)$. Then the expression may be written

$$
\begin{aligned}
g_{11}^{i} c_{i}+g_{12}^{i} c_{i}+g_{21}^{i} c_{i}+g_{22}^{i} c_{i} & =\left(g_{11}^{i}+g_{12}^{i}+g_{21}^{i}+g_{22}^{i}\right) c_{i} \\
& =\left(g_{j k}^{i} c_{j} c_{k}\right) c_{i}=g_{j k}^{i} c_{i} c_{j} c_{k}
\end{aligned}
$$

1.6 If $n=2$, write out explicitly the triple summation $c_{r s t} x^{r} y^{s} z^{t}$.

Any expansion technique that yields all $2^{3}=8$ terms will do. In this case we shall interpret the triplet rst as a three-digit integer, and list the terms in increasing order of that integer:

$$
\begin{aligned}
& c_{r s t} x^{r} y^{s} z^{t}=c_{111} x^{1} y^{1} z^{1}+c_{112} x^{1} y^{1} z^{2}+c_{121} x^{1} y^{2} z^{1}+c_{122} x^{1} y^{2} z^{2} \\
& +c_{211} x^{2} y^{1} z^{1}+c_{212} x^{2} y^{1} z^{2}+c_{221} x^{2} y^{2} z^{1}+c_{222} x^{2} y^{2} z^{2}
\end{aligned}
$$

1.7 Show that $a_{i j} x_{i} x_{j}=0$ if $a_{i j} \equiv i-j$.

Because, for all $i$ and $j, a_{i j}=-a_{j i}$ and $x_{i} x_{j}=x_{i} x_{i}$, the "off-diagonal" terms $a_{i j} x_{i} x_{j} \quad(i<j$; no sum) and $a_{j i} x_{j} x_{i} \quad\left(j>i\right.$; no sum) cancel in pairs, while the "diagonal" terms $a_{i i}\left(x_{i}\right)^{2}$ are zero to begin with. Thus the sum is zero.

The result also follows at once from (1.5).

1.8 If the $a_{i j}$ are constants, calculate the partial derivative

$$
\frac{\partial}{\partial x_{k}}\left(a_{i j} x_{i} x_{j}\right)
$$

Reverting to $\Sigma$-notation, we have:

$$
\begin{aligned}
\sum_{i, j} a_{i j} x_{i} x_{j} & =\sum_{\substack{i \neq k \\
j \neq k}} a_{i j} x_{i} x_{j}+\sum_{\substack{i=k \\
j \neq k}} a_{i j} x_{i} x_{j}+\sum_{\substack{i \neq k \\
j=k}} a_{i j} x_{i} x_{j}+\sum_{\substack{i=k \\
j=k}} a_{i j} x_{i} x_{j} \\
& =C+\left(\sum_{j \neq k} a_{k j} x_{j}\right) x_{k}+\left(\sum_{i \neq k} a_{i k} x_{i}\right) x_{k}+a_{k k}\left(x_{k}\right)^{2}
\end{aligned}
$$

where $C$ is independent of $x_{k}$. Differentiating with respect to $x_{k}$,

$$
\begin{aligned}
\frac{\partial}{\partial x_{k}}\left(\sum_{i, j} a_{i j} x_{i} x_{j}\right) & =0+\sum_{j \neq k} a_{k j} x_{j}+\sum_{i \neq k} a_{i k} x_{i}+2 a_{k k} x_{k} \\
& =\sum_{j} a_{k j} x_{j}+\sum_{i} a_{i k} x_{i}
\end{aligned}
$$

or, going back to the Einstein summation convention,

$$
\frac{\partial}{\partial x_{k}}\left(a_{i j} x_{i} x_{j}\right)=a_{k i} x_{i}+a_{i k} x_{i}=\left(a_{i k}+a_{k i}\right) x_{i}
$$

\section*{SUBSTITUTIONS, KRONECKER DELTA}
1.9 Express $b^{i j} y_{i} y_{j}$ in terms of $x$-variables, if $y_{i}=c_{i j} x_{j}$ and $b^{i j} c_{i k}=\delta_{k}^{j}$.

$$
b^{i j} y_{i} y_{j}=b^{i j}\left(c_{i r} x_{r}\right)\left(c_{j s} x_{s}\right)=\left(b^{i j} c_{i r}\right) x_{r} c_{j s} x_{s}=\delta^{i}{ }_{r} x_{r} c_{j s} x_{s}=x_{j} c_{j s} x_{s}=c_{i j} x_{i} x_{j}
$$

1.10 Rework Problem 1.8 by use of the product rule for differentiation and the fact that

$$
\frac{\partial x_{p}}{\partial x_{q}}=\delta_{p q}
$$

$$
\begin{aligned}
\frac{\partial}{\partial x_{k}}\left(a_{i j} x_{i} x_{j}\right) & =a_{i j} \frac{\partial}{\partial x_{k}}\left(x_{i} x_{j}\right)=a_{i j}\left(x_{j} \frac{\partial x_{i}}{\partial x_{k}}+x_{i} \frac{\partial x_{j}}{\partial x_{k}}\right) \\
& =a_{i j}\left(x_{j} \delta_{i k}+x_{i} \delta_{j k}\right)=a_{k j} x_{j}+a_{i k} x_{i} \\
& =\left(a_{i k}+a_{k i}\right) x_{i}
\end{aligned}
$$

1.11 If $a_{i j}=a_{j i}$ are constants, calculate

$$
\frac{\partial^{2}}{\partial x_{k} \partial x_{l}}\left(a_{i j} x_{i} x_{j}\right)
$$

Using Problem 1.8, we have

$$
\begin{aligned}
\frac{\partial^{2}}{\partial x_{k} \partial x_{l}}\left(a_{i j} x_{i} x_{j}\right) & =\frac{\partial}{\partial x_{k}}\left[\frac{\partial}{\partial x_{l}}\left(a_{i j} x_{i} x_{j}\right)\right]=\frac{\partial}{\partial x_{k}}\left[\left(a_{l j}+a_{j l}\right) x_{j}\right] \\
& =\frac{\partial}{\partial x_{k}}\left(2 a_{i l} x_{i}\right)=2 a_{i l} \frac{\partial}{\partial x_{k}}\left(x_{i}\right)=2 a_{i l} \delta_{k i}=2 a_{k l}
\end{aligned}
$$

1.12 Consider a system of linear equations of the form $y^{i}=a^{i j} x_{j}$ and suppose that $\left(b_{i j}\right)$ is a matrix of numbers such that for all $i$ and $j, b_{i r} a^{r j}=\delta_{i}^{j}$ [that is, the matrix $\left(b_{i j}\right)$ is the inverse of the matrix $\left(a^{i j}\right)$ ]. Solve the system for $x_{i}$ in terms of the $y^{j}$.

Multiply both sides of the $i$ th equation by $b_{k i}$ and sum over $i$ :

$$
b_{k i} y^{i}=b_{k i} a^{i j} x_{j}=\delta_{k}^{j} x_{j}=x_{k}
$$

or $x_{i}=b_{i j} y^{j}$.

1.13 Show that, generally, $a_{i j k}\left(x_{i}+y_{j}\right) z_{k} \neq a_{i j k} x_{i} z_{k}+a_{i j k} y_{j} z_{k}$.

Simply observe that on the left side there are no free indices, but on the right, $j$ is free for the first term and $i$ is free for the second.

1.14 Show that $c_{i j}\left(x_{i}+y_{i}\right) z_{j} \equiv c_{i j} x_{i} z_{j}+c_{i j} y_{i} z_{j}$.

Let us prove (1.2); the desired identity will then follow upon setting $a_{i j} \equiv c_{j i}$.

$$
\begin{aligned}
a_{i j} x_{j}+a_{i j} y_{j} & \equiv \sum_{j} a_{i j} x_{j}+\sum_{j} a_{i j} y_{j}=\sum_{j}\left(a_{i j} x_{j}+a_{i j} y_{j}\right) \\
& =\sum_{j} a_{i j}\left(x_{j}+y_{j}\right) \equiv a_{i j}\left(x_{j}+y_{j}\right)
\end{aligned}
$$

\section*{Supplementary Problems}
1.15 Write out the expression $a_{i} b_{i}(n=6)$ in full.

1.16 Write out the expression $R_{j k i}^{i} \quad(n=4)$ in full. Which are free and which are dummy indices? How many summations are there?

1.17 Evaluate $\delta_{j}^{i} x_{i}$ ( $n$ arbitrary).

1.18 For $n$ arbitrary, evaluate (a) $\delta_{i i}$, (b) $\delta_{i j} \delta_{i j}$, (c) $\delta_{i j} \delta_{k}^{j} c_{i k}$.

1.19 Use the summation convention to indicate $a_{13} b_{13}+a_{23} b_{23}+a_{33} b_{33}$, and state the value of $n$.

1.20 Use the summation convention to indicate

$$
a_{11}\left(x_{1}\right)^{2}+a_{12} x_{1} x_{2}+a_{13} x_{1} x_{3}+a_{21} x_{2} x_{1}+a_{22}\left(x_{2}\right)^{2}+a_{23} x_{2} x_{3}+a_{31} x_{3} x_{1}+a_{32} x_{3} x_{2}+a_{33}\left(x_{3}\right)^{2}
$$

1.21 Use the summation convention and free subscripts to indicate the following linear system, stating the value of $n$ :

$$
\begin{aligned}
& y_{1}=c_{11} x_{1}+c_{12} x_{2} \\
& y_{2}=c_{21} x_{1}+c_{22} x_{2}
\end{aligned}
$$

1.22 Find the following partial derivative if the $a_{i j}$ are constants:

$$
\frac{\partial}{\partial x_{k}}\left(a_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3}\right) \quad(k=1,2,3)
$$

1.23 Use the Kronecker delta to calculate the partial derivative if the $a_{i j}$ are constants:

$$
\frac{\partial}{\partial x_{k}}\left(a_{i j} x_{j}\right)
$$

1.24 Calculate

$$
\frac{\partial}{\partial x_{k}}\left[a_{i j} x_{i}\left(x_{j}\right)^{2}\right]
$$

where the $a_{i j}$ are constants such that $a_{i j}=a_{j i}$.

1.25 Calculate

$$
\frac{\partial}{\partial x_{l}}\left(a_{i j k} x_{i} x_{j} x_{k}\right)
$$

where the $a_{i j k}$ are constants.

1.26 Solve Problem 1.11 without the symmetry condition on $a_{i j}$.

1.27 Evaluate: (a) $b_{j}^{i} y_{i}$ if $y_{i}=T_{i}^{j j}$, (b) $a_{i j} y_{j}$ if $y_{i}=b_{i j} x_{j}$, (c) $a_{i j k} y_{i} y_{j} y_{k}$ if $y_{i}=b_{i j} x_{j}$.

1.28 If $\varepsilon_{i}=1$ for all $i$, prove that

(a) $\left(a_{1}+a_{2}+\cdots+a_{n}\right)^{2} \equiv \varepsilon_{i} \varepsilon_{j} a_{i} a_{j}$

(b) $a_{i}\left(1+x_{i}\right) \equiv a_{i} \varepsilon_{i}+a_{i} x_{i}$

(c) $a_{i j}\left(x_{i}+x_{j}\right) \equiv 2 a_{i j} \varepsilon_{i} x_{j}$ if $a_{i j}=a_{j i}$

\section*{Chapter 2}
\section*{Basic Linear Algebra for Tensors}
\subsection*{2.1 INTRODUCTION}
Familiarity with the topics in this chapter will result in a much greater appreciation for the geometric aspects of tensor calculus. The main purpose is to reformulate the expressions of linear algebra and matrix theory using the summation convention.

\subsection*{2.2 TENSOR NOTATION FOR MATRICES, VECTORS, AND DETERMINANTS}
In the ordinary matrix notation $\left(a_{i j}\right)$, the first subscript, $i$, tells what row the number $a_{i j}$ lies in, and the second, $j$, designates the column. A fuller notation is $\left[a_{i j}\right]_{m n}$, which exhibits the number of rows, $m$, and the number of columns, $n$. This notation may be extended as follows.

\section*{Upper-Index Matrix Notation}
\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-017}
\end{center}

Note that, for mixed indices (one upper, one lower), it is the upper index that designates the row, and the lower index, the column. In the case of pure superscripts, the scheme is identical to the familiar one for subscripts.

\section*{EXAMPLE 2.1}
$$
\begin{array}{r}
{\left[c_{j}^{i}\right]_{23} \equiv\left[\begin{array}{lll}
c_{1}^{1} & c_{2}^{1} & c_{3}^{1} \\
c_{1}^{2} & c_{2}^{2} & c_{3}^{2}
\end{array}\right] .\left[d_{i}^{j}\right]_{23} \equiv\left[\begin{array}{lll}
d_{1}^{1} & d_{2}^{1} & d_{3}^{1} \\
d_{1}^{2} & d_{2}^{2} & d_{3}^{2}
\end{array}\right] \equiv\left[d_{j}^{i}\right]_{23}} \\
{\left[x_{s}^{r}\right]_{14} \equiv\left[\begin{array}{llll}
x_{1}^{1} & x_{2}^{1} & x_{3}^{1} & x_{4}^{1}
\end{array}\right] \quad\left[y^{p q}\right]_{42} \equiv\left[\begin{array}{ll}
y^{11} & y^{12} \\
y^{21} & y^{22} \\
y^{31} & y^{32} \\
y^{41} & y^{42}
\end{array}\right]}
\end{array}
$$

\section*{Vectors}
A real $n$-dimensional vector is any column matrix $\mathbf{v}=\left[x_{i j}\right]_{n 1}$ with real components $x_{i} \equiv x_{i 1}$; one usually writes simply $\mathbf{v}=\left(x_{i}\right)$. The collection of all real $n$-dimensional vectors is the $n$-dimensional real vector space denoted $\mathbf{R}^{n}$.

Vector sums are determined by coordinatewise addition, as are matrix sums: if $A \equiv\left[a_{i j}\right]_{m n}$ and $B \equiv\left[b_{i j}\right]_{m n}$, then

$$
A+B \equiv\left[a_{i j}+b_{i j}\right]_{m n}
$$

Scalar multiplication of a vector or matrix is defined by

$$
\lambda\left[a_{i j}\right]_{m n} \equiv\left[\lambda a_{i j}\right]_{m n}
$$

\section*{Basic Formulas}
The essential formulas involving matrices, vectors, and determinants are now given in terms of the summation convention.

Matrix multiplication. If $A \equiv\left[a_{i j}\right]_{m n}$ and $B \equiv\left[b_{i j}\right]_{n k}$, then


\begin{equation*}
A B=\left[a_{i r} b_{r j}\right]_{m k} \tag{2.1a}
\end{equation*}


Analogously, for mixed or upper indices,


\begin{equation*}
A B \equiv\left[a_{j}^{i}\right]_{m n}\left[b_{j}^{i}\right]_{n k}=\left[a_{r}^{i} b_{j}^{r}\right]_{m k} \quad A B \equiv\left[a^{i j}\right]_{m n}\left[b^{i j}\right]_{n k}=\left[a^{i r} b^{r j}\right]_{m k} \tag{2.1b}
\end{equation*}


wherein $i$ and $j$ are not summed on.

Identity matrix. In terms of the Kronecker deltas, the identity matrix of order $n$ is

$$
I=\left[\delta_{i j}\right]_{n n} \equiv\left[\delta_{j}^{i}\right]_{n n} \equiv\left[\delta^{i j}\right]_{n n}
$$

which has the property $I A=A I=A$ for any square matrix $A$ of order $n$.

Inverse of a square matrix. A square matrix $A \equiv\left[a_{i j}\right]_{n n}$ is invertible if there exists a (unique) matrix $B \equiv\left[b_{i j}\right]_{n n}$, called the inverse of $A$, such that $A B=B A=I$. In terms of components, the criterion reads:


\begin{equation*}
a_{i r} b_{r j}=b_{i r} a_{r j}=\delta_{i j} \tag{2.2a}
\end{equation*}


or, for mixed or upper indices,


\begin{equation*}
a_{r}^{i} b_{j}^{r}=b_{r}^{i} a_{j}^{r}=\delta_{j}^{i} \quad a^{i r} b^{r j}=b^{i r} a^{r j}=\delta^{i j} \tag{2.2b}
\end{equation*}


Transpose of a matrix. Transposition of an arbitrary matrix is defined by $A^{T} \equiv\left[a_{i j}\right]_{m n}^{T}=\left[a_{i j}^{\prime}\right]_{n m}$, where $a_{i j}^{\prime}=a_{j i}$ for all $i$, $j$. If $A^{T}=A$ (that is, $a_{i j}=a_{j i}$ for all $i, j$ ), then $A$ is called symmetric; if $A^{T}=-A$ (that is, $a_{i j}=-a_{j i}$ for all $i, j$ ), then $A$ is called antisymmetric or skew-symmetric.

Orthogonal matrix. A matrix $A$ is orthogonal if $A^{T}=A^{-1}$ (or if $A^{T} A=A A^{T}=I$ ).

Permutation symbol. The symbol $e_{i j k \ldots w}$ (with $n$ subscripts) has the value zero if any pair of subscripts are identical, and equals $(-1)^{p}$ otherwise, where $p$ is the number of subscript transpositions (interchanges of consecutive subscripts) required to bring $(i j k \ldots w)$ to the natural order $(123 \ldots n)$.

Determinant of a square matrix. If $A \equiv\left[a_{i j}\right]_{n n}$ is any square matrix, define the scalar


\begin{equation*}
\operatorname{det} A \equiv e_{i_{1} i_{2} i_{3} \cdots i_{n}} a_{1 i_{1}} a_{2 i_{2}} a_{3 i_{3}} \cdots a_{n i_{n}} \tag{2.3}
\end{equation*}


Other notations are $|A|,\left|a_{i j}\right|$, and $\operatorname{det}\left(a_{i j}\right)$. The chief properties of determinants are


\begin{equation*}
|A B|=|A||B| \quad\left|A^{T}\right|=|A| \tag{2.4}
\end{equation*}


Laplace expansion of a determinant. For each $i$ and $j$, let $M_{i j}$ be the determinant of the square matrix of order $n-1$ obtained from $A \equiv\left[a_{i j}\right]_{n n}$ by deleting the $i$ th row and $j$ th column; $M_{i j}$ is called the minor of $a_{i j}$ in $|A|$. Define the cofactor of $a_{i j}$ to be the scalar


\begin{equation*}
A_{i j}=(-1)^{k} M_{i j} \quad \text { where } \quad k=i+j \tag{2.5}
\end{equation*}


Then the Laplace expansions of $|A|$ are given by

$$
\begin{aligned}
& |A|=a_{1 j} A_{1 j}=a_{2 j} A_{2 j}=\cdots=a_{n j} A_{n j} \quad \text { [row expansions] } \\
& |A|=a_{i 1} A_{i 1}=a_{i 2} A_{i 2}=\cdots=a_{i n} A_{\text {in }} \quad \text { [column expansions] }
\end{aligned}
$$

Scalar product of vectors. If $\mathbf{u}=\left(x_{i}\right)$ and $\mathbf{v}=\left(y_{i}\right)$, then


\begin{equation*}
\mathbf{u v} \equiv \mathbf{u} \cdot \mathbf{v} \equiv \mathbf{u}^{T} \mathbf{v}=x_{i} y_{i} \tag{2.7}
\end{equation*}


If $\mathbf{u}=\mathbf{v}$, the notation $\mathbf{u u} \equiv \mathbf{u}^{2} \equiv \mathbf{v}^{2}$ will often be used. Vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal if $\mathbf{u v}=0$.

Norm (length) of a vector. If $\mathbf{u}=\left(x_{i}\right)$, then


\begin{equation*}
\|\mathbf{u}\| \equiv \sqrt{\mathbf{u}^{2}}=\sqrt{x_{i} x_{i}} \tag{2.8}
\end{equation*}


Angle between two vectors. The angle $\theta$ between two nonzero vectors, $\mathbf{u}=\left(x_{i}\right)$ and $\mathbf{v}=\left(y_{i}\right)$, is defined by


\begin{equation*}
\cos \theta \equiv \frac{\mathbf{u v}}{\|\mathbf{u}\|\|\mathbf{v}\|}=\frac{x_{i} y_{i}}{\sqrt{x_{j} x_{j}} \sqrt{y_{k} y_{k}}} \quad(0 \leqq \theta \leqq \pi) \tag{2.9}
\end{equation*}


It follows that $\theta=\pi / 2$ if $\mathbf{u}$ and $\mathbf{v}$ are nonzero orthogonal vectors.

Vector product in $\mathbf{R}^{3}$. If $\mathbf{u}=\left(x_{i}\right)$ and $\mathbf{v}=\left(y_{i}\right)$, and if the standard basis vectors are designated

$$
\mathbf{i} \equiv\left(\delta_{i 1}\right) \quad \mathbf{j} \equiv\left(\delta_{i 2}\right) \quad \mathbf{k} \equiv\left(\delta_{i 3}\right)
$$

then

\[
\mathbf{u} \times \mathbf{v} \equiv\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k}  \tag{2.10a}\\
x_{1} & x_{2} & x_{3} \\
y_{1} & y_{2} & y_{3}
\end{array}\right|=\left|\begin{array}{ll}
x_{2} & x_{3} \\
y_{2} & y_{3}
\end{array}\right| \mathbf{i}-\left|\begin{array}{ll}
x_{1} & x_{3} \\
y_{1} & y_{3}
\end{array}\right| \mathbf{j}+\left|\begin{array}{ll}
x_{1} & x_{2} \\
y_{1} & y_{2}
\end{array}\right| \mathbf{k}
\]

Expressing the second-order determinants by means of $(2.3)$, one can rewrite $(2.10 a)$ in terms of components only:


\begin{equation*}
\mathbf{u} \times \mathbf{v}=\left(e_{i j k} x_{j} y_{k}\right) \tag{2.10b}
\end{equation*}


\subsection*{2.3 INVERTING A MATRIX}
There are a number of algorithms for computing the inverse of $A \equiv\left[a_{i j}\right]_{n n}$, where $|A| \neq 0$ (a necessary and sufficient condition for $A$ to be invertible). When $n$ is large, the method of elementary row operations is efficient. For small $n$, it is practical to apply the explicit formula


\begin{equation*}
A^{-1}=\frac{1}{|A|}\left[A_{i j}\right]_{n n}^{T} \tag{2.11a}
\end{equation*}


Thus, for $n=2$,

\[
\left[\begin{array}{ll}
a_{11} & a_{12}  \tag{2.11b}\\
a_{21} & a_{22}
\end{array}\right]^{-1}=\frac{1}{|A|}\left[\begin{array}{rr}
a_{22} & -a_{12} \\
-a_{21} & a_{11}
\end{array}\right]
\]

in which $|A|=a_{11} a_{22}-a_{12} a_{21}$; and, for $n=3$,

in which

\[
\left[\begin{array}{lll}
a_{11} & a_{12} & a_{13}  \tag{2.11c}\\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right]^{-1}=\frac{1}{|A|}\left[\begin{array}{lll}
A_{11} & A_{21} & A_{31} \\
A_{12} & A_{22} & A_{32} \\
A_{13} & A_{23} & A_{33}
\end{array}\right]
\]

$$
A_{11}=a_{22} a_{33}^{\dot{b}}-a_{23} a_{32} \quad A_{21}=-\left(a_{12} a_{33}-a_{13} a_{32}\right) \quad \ldots
$$

\subsection*{2.4 MATRIX EXPRESSIONS FOR LINEAR SYSTEMS AND QUADRATIC FORMS}
Because of the product rule for matrices and the rule for matrix equality, one can write a system of equations such as

$$
\begin{array}{r}
3 x-4 y=2 \\
-5 x+8 y=7
\end{array}
$$

in the matrix form

$$
\left[\begin{array}{rr}
3 & -4 \\
-5 & 8
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{l}
2 \\
7
\end{array}\right]
$$

In general, any $m \times n$ system of equations


\begin{equation*}
a_{i j} x_{j}=b_{i} \quad(1 \leqq i \leqq m) \tag{2.12a}
\end{equation*}


can be written in the matrix form


\begin{equation*}
A \mathbf{x}=\mathbf{b} \tag{2.12b}
\end{equation*}


where $A \equiv\left[a_{i j}\right]_{m n}, \mathbf{x} \equiv\left(x_{i}\right)$, and $\mathbf{b} \equiv\left(b_{i}\right)$. One advantage in doing this is that, if $m=n$ and $A$ is invertible, the solution of the system can proceed entirely by matrices: $\mathbf{x}=A^{-1} \mathbf{b}$.

Another useful fact for work with tensors is that a quadratic form $Q$ (a homogeneous second-degree polynomial) in the $n$ variables $x_{1}, x_{2}, \ldots, x_{n}$ also has a strictly matrix representation:


\begin{equation*}
Q=a_{i j} x_{i} x_{j}=\mathbf{x}^{T} A \mathbf{x} \tag{2.13}
\end{equation*}


where the row matrix $\mathbf{x}^{T}$ is the transpose of the column matrix $\mathbf{x}=\left(x_{i}\right)$ and where $A \equiv\left[a_{i j}\right]_{n n}$.

\section*{EXAMPLE 2.2}
$$
\left[\begin{array}{lll}
x_{1} & x_{2} & x_{3}
\end{array}\right]\left[\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{lll}
x_{1} & x_{2} & x_{3}
\end{array}\right]\left[\begin{array}{l}
a_{1 j} x_{j} \\
a_{2 j} x_{j} \\
a_{3 j} x_{j}
\end{array}\right]=\left[x_{i}\left(a_{i j} x_{j}\right)\right]=a_{i j} x_{i} x_{j}
$$

The matrix $A$ that produces a given quadratic form is not unique. In fact, the matrix $B=\frac{1}{2}\left(A+A^{T}\right)$ may always be substituted for $A$ in (2.13); i.e., the matrix of a quadratic form may always be assumed symmetric.

EXAMPLE 2.3 Write the quadratic equation

$$
3 x^{2}+y^{2}-2 z^{2}-5 x y-6 y z=10
$$

using a symmetric matrix.

The quadratic form (2.13) is given in terms of the nonsymmetric matrix

$$
A=\left[\begin{array}{rrr}
3 & -5 & 0 \\
0 & 1 & -6 \\
0 & 0 & -2
\end{array}\right]
$$

The symmetric equivalent is obtained by replacing each off-diagonal element by one-half the sum of that element and its mirror image in the main diagonal. Hence, the desired representation is

$$
\left[\begin{array}{lll}
x & y & z
\end{array}\right]\left[\begin{array}{ccc}
3 & -5 / 2 & 0 \\
-5 / 2 & 1 & -3 \\
0 & -3 & -2
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
z
\end{array}\right]=10
$$

\subsection*{2.5 LINEAR TRANSFORMATIONS}
Of utmost importance for the study of tensor calculus is a basic knowledge of transformation theory and changes in coordinate systems. A set of linear equations like


\begin{align*}
& \bar{x}=5 x-2 y \\
& \bar{y}=3 x+2 y \tag{I}
\end{align*}


defines a linear transformation (or linear mapping) from each point $(x, y)$ to its corresponding image $(\bar{x}, \bar{y})$. In matrix form, a, linear transformation may be written $\overline{\mathbf{x}}=A \mathbf{x}$; if, as in $(I)$, the mapping is one-one, then $|A| \neq 0$.

There is always an alias-alibi aspect of such transformations: When $(\bar{x}, \bar{y})$ is regarded as defining new coordinates (a new name) for $(x, y)$, one is dealing with the alias aspect; when $(\bar{x}, \bar{y})$ is regarded as a new position (place) for $(x, y)$, the alibi aspect emerges. In tensor calculus, one is generally more interested in the alias aspect: the two coordinate systems related by $\overline{\mathbf{x}}=A \mathbf{x}$ are referred to as the unbarred and the barred systems.

EXAMPLE 2.4 In order to find the image of the point $(0,-1)$ under $(I)$, merely set $x=0$ and $y=-1$; the result is

$$
\bar{x}=5(0)-2(-1)=2 \quad \bar{y}=3(0)+2(-1)=-2
$$

Hence, $\overline{(0,-1)}=(2,-2)$. Similarly, we find that $\overline{(2,1)}=(8,8)$.

If we regard $(\bar{x}, \bar{y})$ merely as a different coordinate system, we would say that two fixed points, $P$ and $Q$, have the respective coordinates $(0,-1)$ and $(2,1)$ in the unbarred system, and $(2,-2)$ and $(8,8)$ in the barred system.

\section*{Distance in a Barred Coordinate System}
What is the expression for the (invariant) distance between two points in terms of differing aliases? Let $\overline{\mathbf{x}}=A \mathbf{x}(|A| \neq 0)$ define an invertible linear transformation between unbarred and barred coordinates. It is shown in Problem 2.20 that the desired distance formula is


\begin{equation*}
d(\overline{\mathbf{x}}, \overline{\mathbf{y}})=\sqrt{(\overline{\mathbf{x}}-\overline{\mathbf{y}})^{T} G(\overline{\mathbf{x}}-\overline{\mathbf{y}})}=\sqrt{g_{i j} \Delta \bar{x}_{i} \Delta \bar{x}_{j}} \tag{2.14}
\end{equation*}


where $\left[g_{i j}\right]_{n n} \equiv G=\left(A A^{T}\right)^{-1}$ and $\overline{\mathbf{x}}-\overline{\mathbf{y}}=\left(\Delta \bar{x}_{i}\right)$. If $A$ is orthogonal (a rotation of the axes), then $g_{i j}=\delta_{i j}$, and (2.14) reduces to the ordinary form

$$
d(\overline{\mathbf{x}}, \overline{\mathbf{y}})=\|\overline{\mathbf{x}}-\overline{\mathbf{y}}\|=\sqrt{\Delta \bar{x}_{i} \Delta \bar{x}_{i}}
$$

[cf. (2.8)].

EXAMPLE 2.5 Calculate the distance between points $P$ and $Q$ of Example 2.4 in terms of their barred coordinates. Verify that the same distance is found in the unbarred coordinate system.

First calculate the matrix $G=\left(A A^{T}\right)^{-1}=\left(A^{-1}\right)^{T} A^{-1}$ (see Problem 2.13):

and $\quad G=\frac{1}{16}\left[\begin{array}{rr}2 & 2 \\ -3 & 5\end{array}\right]^{T} \cdot \frac{1}{16}\left[\begin{array}{rr}2 & 2 \\ -3 & 5\end{array}\right]=\frac{1}{256}\left[\begin{array}{rr}2 & -3 \\ 2 & 5\end{array}\right]\left[\begin{array}{rr}2 & 2 \\ -3 & 5\end{array}\right]=\frac{1}{256}\left[\begin{array}{rr}13 & -11 \\ -11 & 29\end{array}\right]$

Hence $g_{11}=13 / 256, g_{12}=g_{21}=-11 / 256$, and $g_{22}=29 / 256$. Now, with $\overline{\mathbf{x}}-\overline{\mathbf{y}}=\left[\begin{array}{ll}2-8 & -2-8\end{array}\right]^{T}=\left[\begin{array}{ll}-6 & -10\end{array}\right]^{T}$, (2.14) gives:

$$
\begin{aligned}
d^{2} & =g_{i j} \Delta \bar{x}_{i} \Delta \bar{x}_{j} \\
& =\frac{13}{256}(-6)^{2}+2 \cdot \frac{-11}{256}(-6)(-10)+\frac{29}{256}(-10)^{2} \\
& =\frac{13(36)-22(60)+29(100)}{256}=8
\end{aligned}
$$

In the unbarred system, the distance between $P(0,-1)$ and $Q(2,1)$ is given, in agreement, by the Pythagorean theorem:

$$
d^{2}=(0-2)^{2}+(-1-1)^{2}=8
$$

\subsection*{2.6 GENERAL COORDINATE TRANSFORMATIONS}
A general mapping or transformation $T$ of $\mathbf{R}^{n}$ may be indicated in functional (vector) or in component form:

$$
\overline{\mathbf{x}}=T(\mathbf{x}) \quad \text { or } \quad \bar{x}_{i}=T_{i}\left(x_{1}, x_{2}, \ldots, x_{n}\right)
$$

In the alibi description, any point $\mathbf{x}$ in the domain of $T$ (possibly the whole of $\mathbf{R}^{n}$ ) has as its image the point $T(\mathbf{x})$ in the range of $T$. Considered as a coordinate transformation (the alias description), $T$ sets up, for each point $P$ in its domain, a correspondence between $\left(x_{i}\right)$ and $\left(\bar{x}_{i}\right)$, the coordinates of $P$ in two different systems. As explained below, $T$ may be interpreted as a coordinate transformation only if a certain condition is fulfilled.

\section*{Bijections, Curvilinear Coordinates}
A map $T$ is called a bijection or a one-one mapping if it maps each pair of distinct points $\mathbf{x} \neq \mathbf{y}$ in its domain into distinct points $T(\mathbf{x}) \neq T(\mathbf{y})$ in its range. Whenever $T$ is bijective, we call the image $\overline{\mathbf{x}}=T(\mathbf{x})$ a set of admissible coordinates for $\mathbf{x}$, and the aggregate of all such coordinates (alibi: the range of $T$ ), a coordinate system.

Certain coordinate systems are named after the characteristics of the mapping $T$. For example, if $T$ is linear, the $\left(\bar{x}_{i}\right)$-system is called affine; and if $T$ is a rigid motion, $\left(\bar{x}_{i}\right)$ is called rectangular or cartesian. [It is presumed in making this statement that the original coordinate system $\left(x_{i}\right)$ is the familiar cartesian coordinate system of analytic geometry, or its natural extension to vectors in $\mathbf{R}^{n}$.] Nonaffine coordinate systems are generally called curvilinear coordinates; these include polar coordinates in two dimensions, and cylindrical and spherical coordinates in three dimensions.

\subsection*{2.7 THE CHAIN RULE FOR PARTIAL DERIVATIVES}
In working with curvilinear coordinates, one needs the Jacobian matrix (Chapter 3) and, therefore, the chain rule of multivariate calculus. The summation convention makes possible a compact statement of this rule: If $w=f\left(x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right)$ and $x_{i}=x_{i}\left(u_{1}, u_{2}, \ldots, u_{m}\right) \quad(i=$ $1,2, \ldots, n)$, where all functions involved have continuous partial derivatives, then


\begin{equation*}
\frac{\partial w}{\partial u_{j}}=\frac{\partial f}{\partial x_{i}} \frac{\partial x_{i}}{\partial u_{j}} \quad(1 \leqq j \leqq m) \tag{2.15}
\end{equation*}


\section*{Solved Problems}
\section*{TENSOR NOTATION}
2.1 Display explicitly the matrices $(a)\left[b_{i}^{j}\right]_{42},(b)\left[b_{j}^{i}\right]_{24},(c)\left[\delta^{i j}\right]_{33}$.

$$
\begin{gathered}
\text { (a) }\left[b_{i}^{j}\right]_{42}=\left[\begin{array}{ll}
b_{1}^{1} & b_{2}^{1} \\
b_{1}^{2} & b_{2}^{2} \\
b_{1}^{3} & b_{2}^{3} \\
b_{1}^{4} & b_{2}^{4}
\end{array}\right] \quad \text { (b) }\left[b_{j}^{i}\right]_{24}=\left[\begin{array}{llll}
b_{1}^{1} & b_{2}^{1} & b_{3}^{1} & b_{4}^{1} \\
b_{1}^{2} & b_{2}^{2} & b_{3}^{2} & b_{4}^{2}
\end{array}\right] \\
\text { (c) }\left[\delta^{i j}\right]_{33}=\left[\begin{array}{lll}
\delta^{11} & \delta^{12} & \delta^{13} \\
\delta^{21} & \delta^{22} & \delta^{23} \\
\delta^{31} & \delta^{32} & \delta^{33}
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
\end{gathered}
$$

From (a) and (b) it is evident that merely interchanging the indices $i$ and $j$ in a matrix $A \equiv\left[a_{i j}\right]_{m n}$ does not necessarily yield the transpose, $A^{T}$.

\subsection*{2.2 Given}
$$
A=\left[\begin{array}{ccc}
a & -a & -a \\
2 b & b & -b \\
4 c & 2 c & -2 c
\end{array}\right] \quad B=\left[\begin{array}{rrr}
2 & 4 & -6 \\
-1 & -2 & 3 \\
3 & 6 & -9
\end{array}\right]
$$

verify that $A B \neq B A$.

$$
A B=\left[\begin{array}{ccc}
2 a+a-3 a & 4 a+2 a-6 a & -6 a-3 a+9 a \\
4 b-b-3 b & 8 b-2 b-6 b & -12 b+3 b+9 b \\
8 c-2 c-6 c & 16 c-4 c-12 c & -24 c+6 c+18 c
\end{array}\right]=\left[\begin{array}{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right] \equiv \mathrm{O}
$$

but $\quad B A=\left[\begin{array}{ccc}2 a+8 b-24 c & -2 a+4 b-12 c & -2 a-4 b+12 c \\ -a-4 b+12 c & a-2 b+6 c & a+2 b-6 c \\ 3 a+12 b-36 c & -3 a+6 b-18 c & -3 a-6 b+18 c\end{array}\right] \neq \mathrm{O}$

Thus, the commutative law $(A B=B A)$ fails for matrices. Further, $A B=\mathrm{O}$ does not imply that $A=\mathrm{O}$ or $B=$ O.

2.3 Prove by use of tensor notation and the product rule for matrices that $(A B)^{T}=B^{T} A^{T}$, for any two conformable matrices $A$ and $B$.

Let $A \equiv\left[a_{i j}\right]_{m n}, B \equiv\left[b_{i j}\right]_{n k}, A B \equiv\left[c_{i j}\right]_{m k}$, and, for all $i$ and $j$,

$$
a_{i j}^{\prime}=a_{j i} \quad b_{i j}^{\prime}=b_{j i} \quad c_{i j}^{\prime}=c_{j i}
$$

Hence, $A^{T}=\left[a_{i j}^{\prime}\right]_{n m}, B^{T}=\left[b_{i j}^{\prime}\right]_{k n}$, and $(A B)^{T}=\left[c_{i j}^{\prime}\right]_{k m}$. We must show that $B^{T} A^{T}=\left[c_{i j}^{\prime}\right]_{k m}$. By definition of matrix product, $B^{T} A^{T}=\left[b_{i r}^{\prime}{ }^{\prime} a_{r j}^{\prime}\right]_{k m}$, and since

$$
b_{i r}^{\prime} a_{r j}^{\prime}=b_{r i} a_{j r}=a_{j r} b_{r i}=c_{j i}=c_{i j}^{\prime}
$$

the desired result follows.

2.4 Show that any matrix of the form $A=B^{T} B$ is symmetric.

By Problem 2.3 and the involutory nature of the transpose operation,

$$
A^{T}=\left(B^{T} B\right)^{T}=B^{T}\left(B^{T}\right)^{T}=B^{T} B=A
$$

2.5 From the definition, (2.3), of a determinant of order 3, derive the Laplace expansion by cofactors of the first row.

In the case $n=3,(2.3)$ becomes

$$
\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right| \equiv\left|a_{i j}\right|=e_{i j k} a_{1 i} a_{2 j} a_{3 k}
$$

Since $e_{i j k}=0$ if any two subscripts coincide, we write only terms for which (ijk) is a permutation of (123):

$$
\begin{aligned}
\left|a_{i j}\right| & =e_{123} a_{11} a_{22} a_{33}+e_{132} a_{11} a_{23} a_{32}+e_{213} a_{12} a_{21} a_{33} \\
& \quad+e_{231} a_{12} a_{23} a_{31}+e_{312} a_{13} a_{21} a_{32}+e_{321} a_{13} a_{22} a_{31} \\
& =a_{11} a_{22} a_{33}-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31} \\
& =a_{11}\left(a_{22} a_{33}-a_{23} a_{32}\right)-a_{12}\left(a_{21} a_{33}-a_{23} a_{31}\right)+a_{13}\left(a_{21} a_{32}-a_{22} a_{31}\right)
\end{aligned}
$$

But, for $n=2,(2.3)$ gives

$$
\left|\begin{array}{ll}
a_{22} & a_{23} \\
a_{32} & a_{33}
\end{array}\right| \equiv+A_{11}=e_{12} a_{22} a_{33}+e_{2}: a_{23} a_{32}=a_{22} a_{33}-a_{23} a_{32}
$$

and the analogous expansions of $-A_{12}$ and $+A_{13}$. Hence,

$$
\left|a_{i j}\right|=a_{11} A_{11}+a_{12} A_{12}+a_{13} A_{13}=a_{1 j} A_{1 j}
$$

as in $(2.6)$.

\subsection*{2.6 Evaluate:}
$$
\begin{aligned}
& \text { (a) }\left|\begin{array}{cc}
b & -2 a \\
-2 c & b
\end{array}\right| \quad \text { (b) }\left|\begin{array}{rrr}
5 & -2 & 15 \\
-10 & 0 & 10 \\
15 & 0 & 30
\end{array}\right| \\
& \left|\begin{array}{cc}
b \cdot & -2 a \\
-2 c & b
\end{array}\right|=b \cdot b-(-2 a)(-2 c)=b^{2}-4 a c
\end{aligned}
$$

(b) Because of the zeros in the second column, it is simplest to expand by that column:

$$
\begin{aligned}
\left|\begin{array}{rrr}
5 & -2 & 15 \\
-10 & 0 & 10 \\
15 & 0 & 30
\end{array}\right| & =-(-2)\left|\begin{array}{rr}
-10 & 10 \\
15 & 30
\end{array}\right|+0\left|\begin{array}{rr}
5 & 15 \\
15 & 30
\end{array}\right|-0\left|\begin{array}{rr}
5 & 15 \\
-10 & 10
\end{array}\right| \\
& =2\left|\begin{array}{rr}
-10 & 10 \\
15 & 30
\end{array}\right|=2(10)(15)\left|\begin{array}{rr}
-1 & 1 \\
1 & 2
\end{array}\right|=300(-2-1)=-900
\end{aligned}
$$

2.7 Calculate the angle between the following two vectors in $\mathbf{R}^{5}$ :

$$
\mathbf{x}=(1,0,-2,-1,0) \quad \text { and } \quad \mathbf{y}=(0,0,2,2,0)
$$

We have:

$$
\begin{aligned}
& \mathbf{x y}=(1)(0)+(0)(0)+(-2)(2)+(-1)(2)+(0)(0)=-6 \\
& \mathbf{x}^{2}=1^{2}+0^{2}+(-2)^{2}+(-1)^{2}+0^{2}=6 \\
& \mathbf{y}^{2}=0^{2}+0^{2}+2^{2}+2^{2}+0^{2}=8
\end{aligned}
$$

and (2.9) gives

$$
\cos \theta=\frac{-6}{\sqrt{6} \cdot \sqrt{8}}=-\frac{\sqrt{3}}{2} \quad \text { or } \quad \theta=\frac{5 \pi}{6}
$$

2.8 Find three linearly independent vectors in $\mathbf{R}^{4}$ which are orthogonal to the vector $(3,4,1,-2)$.

It is useful to choose vectors having as many zero components as possible. The components $(0,1,0,2)$ clearly work, and $(1,0,-3,0)$ also. Finally, $(0,0,2,1)$ is orthogonal to the given vector, and seems not to be dependent on the first two chosen. To check independence, suppose scalars $x, y$, and $z$ exist such that

$$
x\left[\begin{array}{l}
0 \\
1 \\
0 \\
2
\end{array}\right]+y\left[\begin{array}{r}
1 \\
0 \\
-3 \\
0
\end{array}\right]+z\left[\begin{array}{l}
0 \\
0 \\
2 \\
1
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0 \\
0
\end{array}\right] \quad \text { or } \quad \begin{aligned}
& x(0)+y(1)+z(0)=0 \\
& x(1)+y(0)+z(0)=0 \\
& x(0)+y(-3)+z(2)=0 \\
& x(2)+y(0)+z(1)=0
\end{aligned}
$$

This system has the sole solution $x=y=z=0$, and the vectors are independent.

2.9 Prove that the vector product in $R^{3}$ is anticommutative: $\mathbf{x} \times \mathbf{y}=-\mathbf{y} \times \mathbf{x}$.

By $(2.10 b)$,

$$
\mathbf{x} \times \mathbf{y}=\left(e_{i j k} x_{j} y_{k}\right) \quad \text { and } \quad \mathbf{y} \times \mathbf{x}=\left(e_{i j k} y_{j} x_{k}\right)
$$

But $e_{i k j}=-e_{i j k}$, so that

$$
e_{i j k} y_{j} x_{k}=e_{i k j} y_{k} x_{j}=-e_{i j k} y_{k} x_{j}=-e_{i j k} x_{j} y_{k}
$$

\section*{INVERTING A MATRIX}
2.10 Establish the generalized Laplace expansion theorem: $a_{r j} A_{s j}=|A| \delta_{r s}$.

Consider the matrix

$$
A^{*}=\left[\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
\ldots & \ldots & \ldots & \ldots \\
a_{r 1} & a_{r 2} & \ldots & a_{r n} \\
\ldots & \cdots & \ldots & \cdots \\
a_{r 1} & a_{r 2} & \ldots & a_{r n} \\
\ldots & \ldots & \ldots & \ldots \\
a_{n 1} & a_{n 2} & \ldots & a_{n n}
\end{array}\right] \text { row } \boldsymbol{r}
$$

which is obtained from matrix $A$ by replacing its $s$ th row by its $r$ th row $(r \neq s)$. By (2.6), applied to row $r$ of $A^{*}$,

$$
\operatorname{det} A^{*}=a_{r j} A_{r j}^{*} \quad(\text { not summed on } r)
$$

Now, because rows $r$ and $s$ are identical, we have for all $j$,

$$
A_{r j}^{*}=(-1)^{p} A_{s j}^{*}=(-1)^{p} A_{s j} \quad(p \equiv r-s)
$$

Therefore, $\operatorname{det} A^{*}=(-1)^{p} a_{r j} A_{s j}$. But it is easy to see (Problem 2.31) that, with two rows the same, $\operatorname{det} A^{*}=0$. We have thus proved that

$$
a_{r j} A_{s j}=0 \quad(r \neq s)
$$

and this, together with (2.6) for the case $r=s$, yields the theorem.

2.11 Given a matrix $A \equiv\left[a_{i j}\right]_{n n}$, with $|A| \neq 0$, use Problem 2.10 to show that

$$
A B=I \quad \text { where } \quad B=\frac{1}{|A|}\left[A_{i j}\right]_{n n}^{T}
$$

Since the $(i, j)$-element of $B$ is $A_{j i} /|A|$,

$$
A B=\left[a_{i k}\left(A_{j k} /|A|\right)\right]_{n n}=\frac{1}{|A|}\left[|A| \delta_{i j}\right]_{n n}=\frac{|A|}{|A|}\left[\delta_{i j}\right]_{n n}=I
$$

[It follows from basic facts of linear algebra that also $B A=I$; therefore, $A^{-1}=B$, which establishes (2.11a).]

2.12 Invert the matrix

$$
A=\left[\begin{array}{rrr}
-2 & 0 & 1 \\
3 & 1 & 0 \\
2 & -2 & 3
\end{array}\right]
$$

row:

Use (2.11c). To evaluate $|A|$, add twice the third column to the first and then expand by the first

$$
|A|=\left|\begin{array}{rrr}
0 & 0 & 1 \\
3 & 1 & 0 \\
8 & -2 & 3
\end{array}\right|=1 \cdot\left|\begin{array}{rr}
3 & 1 \\
8 & -2
\end{array}\right|=-6-8=-14
$$

Then, computing cofactors as we go,

$$
A^{-1}=\frac{1}{-14}\left[\begin{array}{rrr}
3 & -2 & -1 \\
-9 & -8 & 3 \\
-8 & -4 & -2
\end{array}\right]=\left[\begin{array}{rrr}
-3 / 14 & 1 / 7 & 1 / 14 \\
9 / 14 & 4 / 7 & -3 / 14 \\
4 / 7 & 2 / 7 & 1 / 7
\end{array}\right]
$$

2.13 Let $A$ and $B$ be invertible matrices of the same order. Prove that $(a)\left(A^{T}\right)^{-1}=\left(A^{-1}\right)^{T}$ (i.e., the operations of transposition and inversion commute); (b) $(A B)^{-1}=B^{-1} A^{-1}$.

(a) Transpose the equations $A A^{-1}=A^{-1} A=I$, recalling Problem 2.3, to obtain

$$
\left(A^{-1}\right)^{T} A^{T}=A^{T}\left(A^{-1}\right)^{T}=I^{T}=I
$$

which show that $A^{T}$ is invertible, with inverse $\left(A^{T}\right)^{-1}=\left(A^{-1}\right)^{T}$.

(b) By the associative law for matrix multiplication,

$$
(A B)\left(B^{-1} A^{-1}\right)=A\left(B B^{-1}\right) A^{-1}=A I A^{-1}=A A^{-1}=I
$$

and, similarly,

$$
\left(B^{-1} A^{-1}\right)(A B)=I
$$

Hence, $(A B)^{-1}=B^{-1} A^{-1}$.

\section*{LINEAR SYSTEMS; QUADRATIC FORMS}
2.14 Write the following system of equations in matrix form, then solve by using the inverse matrix:

$$
\begin{aligned}
3 x-4 y & =-18 \\
-5 x+8 y & =34
\end{aligned}
$$

The matrix form of the system is

\[
\left[\begin{array}{rr}
3 & -4  \tag{1}\\
-5 & 8
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{r}
-18 \\
34
\end{array}\right]
\]

The inverse of the $2 \times 2$ coefficient matrix is:

$$
\left[\begin{array}{rr}
3 & -4 \\
-5 & 8
\end{array}\right]^{-1}=\frac{1}{24-(+20)}\left[\begin{array}{ll}
8 & 4 \\
5 & 3
\end{array}\right]=\left[\begin{array}{cc}
2 & 1 \\
5 / 4 & 3 / 4
\end{array}\right]
$$

Premultiplying (1) by this matrix gives

$$
I\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{cc}
2 & 1 \\
5 / 4 & 3 / 4
\end{array}\right]\left[\begin{array}{r}
-18 \\
34
\end{array}\right]=\left[\begin{array}{r}
-2 \\
3
\end{array}\right]
$$

or $x=-2, y=3$.

2.15 If $\left[b^{i j}\right]=\left[a_{i j}\right]^{-1}$, solve the $n \times n$ system


\begin{equation*}
y_{i}=a_{i j} x_{j} \tag{1}
\end{equation*}


for the $x_{j}$ in terms of the $y_{i}$.

Multiply both sides of (1) by $b^{k i}$ and sum on $i$ :

$$
b^{k i} y_{i}=b^{k i} a_{i j} x_{j}=\delta_{j}^{k} x_{j}=x_{k}
$$

Therefore, $x_{j}=b^{j i} y_{i}$.

2.16 Write the quadratic form in $\mathbf{R}^{4}$

$$
Q=7 x_{1}^{2}-4 x_{1} x_{3}+3 x_{1} x_{4}-x_{2}^{2}+10 x_{2} x_{4}+x_{3}^{2}-6 x_{3} x_{4}+3 x_{4}^{2}
$$

in the matrix form $\mathbf{x}^{T} A \mathbf{x}$ with $A$ symmetric.

$$
Q=\left[\begin{array}{llll}
x_{1} & x_{2} & x_{3} & x_{4}
\end{array}\right]\left[\begin{array}{rrrr}
7 & 0 & -4 & 3 \\
0 & -1 & 0 & 10 \\
0 & 0 & 1 & -6 \\
0 & 0 & 0 & 3
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{array}\right]=\left[\begin{array}{llll}
x_{1} & x_{2} & x_{3} & x_{4}
\end{array}\right]\left[\begin{array}{rrrr}
7 & 0 & -2 & 3 / 2 \\
0 & -1 & 0 & 5 \\
-2 & 0 & 1 & -3 \\
3 / 2 & 5 & -3 & 3
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{array}\right]
$$

\section*{LINEAR TRANSFORMATIONS}
2.17 Show that under a change of coordinates $\bar{x}_{i}=a_{i j} x_{j}$, the quadric hypersurface $c_{i j} x_{i} x_{j}=1$ transforms to $\bar{c}_{i j} \bar{x}_{i} \bar{x}_{j}=1$, where

$$
\bar{c}_{i j}=c_{r s} b_{r i} b_{s j} \quad \text { with } \quad\left(b_{i j}\right)=\left(a_{i j}\right)^{-1}
$$

This will be worked using matrices, from which the component form can be easily deduced. The hypersurface has the equation $\mathbf{x}^{T} C \mathbf{x}=1$ in unbarred coordinates, and $\overline{\mathbf{x}}=A \mathbf{x}$ defines a barred coordinate system. Substituting $\mathbf{x}=B \overline{\mathbf{x}} \quad\left(B=A^{-1}\right)$ into the equation of the quadric, we have

$$
(B \overline{\mathbf{x}})^{T} C(B \overline{\mathbf{x}})=1 \quad \text { or } \quad \overline{\mathbf{x}}^{T} B^{T} C B \overline{\mathbf{x}}=1
$$

Thus, in the barred coordinate system, the equation of the quadric is $\overline{\mathbf{x}}^{T} \bar{C} \overline{\mathbf{x}}=1$, where $\bar{C}=B^{T} C B$.

\section*{DISTANCE IN A BARRED COORDINATE SYSTEM}
2.18 Calculate the coefficients $g_{i j}$ in the distance formula (2.14) for the barred coordinate system in $\mathbf{R}^{2}$ defined by $\bar{x}_{i}=a_{i j} x_{j}$, where $a_{11}=a_{22}=1, a_{12}=0$, and $a_{21}=2$.

We have merely to calculate $G=\left(A A^{T}\right)^{-1}$, where $A=\left(a_{i j}\right)$ :

$$
A A^{T}=\left[\begin{array}{ll}
1 & 0 \\
2 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 2 \\
0 & 1
\end{array}\right]=\left[\begin{array}{cc}
1 \cdot 1+0 & 1 \cdot 2+0 \\
2 \cdot 1+0 & 2 \cdot 2+1 \cdot 1
\end{array}\right]=\left[\begin{array}{ll}
1 & 2 \\
2 & 5
\end{array}\right]
$$

By $(2.11 b)$,

$$
\left(A A^{T}\right)^{-1}=\left[\begin{array}{ll}
1 & 2 \\
2 & 5
\end{array}\right]^{-1}=\frac{1}{5-4}\left[\begin{array}{rr}
5 & -2 \\
-2 & 1
\end{array}\right]=\left[\begin{array}{rr}
5 & -2 \\
-2 & 1
\end{array}\right]
$$

Thus, $g_{11}=5, g_{12}=g_{21}=-2, g_{22}=1$.

2.19 Test the distance formula obtained in Problem 2.18 by finding the distance between the aliases of $\left(x_{i}\right)=(1,-3)$ and $\left(y_{i}\right)=(0,-2)$, which points are a distance $\sqrt{2}$ apart.

The coordinates for the given points in the barred system are found to be

$$
\overline{\mathbf{x}}=\left[\begin{array}{ll}
1 & 0 \\
2 & 1
\end{array}\right]\left[\begin{array}{r}
1 \\
-3
\end{array}\right]=\left[\begin{array}{r}
1 \\
-1
\end{array}\right] \quad \overline{\mathbf{y}}=\left[\begin{array}{ll}
1 & 0 \\
2 & 1
\end{array}\right]\left[\begin{array}{r}
0 \\
-2
\end{array}\right]=\left[\begin{array}{r}
0 \\
-2
\end{array}\right]
$$

or $\left(\bar{x}_{i}\right)=(1,-1)$ and $\left(\bar{y}_{i}\right)=(0,-2)$. Using the $g_{i j}$ calculated in Problem 2.18,

$$
d(\overline{\mathbf{x}}, \overline{\mathbf{y}})=\sqrt{5(1-0)^{2}-2 \cdot 2(1-0)(-1+2)+1(-1+2)^{2}}=\sqrt{2}
$$

2.20 Prove formula (2.14).

In unbarred coordinates, the distance formula has the matrix form

$$
d(\mathbf{x}, \mathbf{y})=\|\mathbf{x}-\mathbf{y}\|=\sqrt{(\mathbf{x}-\mathbf{y})^{T}(\mathbf{x}-\mathbf{y})}
$$

Now, $\overline{\mathbf{x}}=A \mathbf{x}$ or $\mathbf{x}=B \overline{\mathbf{x}}$, where $B=A^{-1}$; so we have by substitution,

$$
\begin{aligned}
d(\mathbf{x}, \mathbf{y}) & =\sqrt{(B \overline{\mathbf{x}}-B \overline{\mathbf{y}})^{T}(B \overline{\mathbf{x}}-B \overline{\mathbf{y}})}=\sqrt{(B(\overline{\mathbf{x}}-\overline{\mathbf{y}}))^{T} B(\overline{\mathbf{x}}-\overline{\mathbf{y}})} \\
& =\sqrt{(\overline{\mathbf{x}}-\overline{\mathbf{y}})^{T} B^{T} B(\overline{\mathbf{x}}-\overline{\mathbf{y}})}=\sqrt{(\overline{\mathbf{x}}-\overline{\mathbf{y}})^{T} G(\overline{\mathbf{x}}-\overline{\mathbf{y}})} \\
& =d(\overline{\mathbf{x}}, \overline{\mathbf{y}})
\end{aligned}
$$

where $G \equiv B^{T} B=\left(A^{-1}\right)^{T} A^{-1}=\left(A^{T}\right)^{-1} A^{-1}=\left(A A^{T}\right)^{-1}$, the last two equalities following from Problem 2.13 .

\section*{RECTANGULAR COORDINATES}
2.21 Suppose that $\left(x^{i}\right)=(x, y, z)$ and $\left(\bar{x}^{i}\right)=(\bar{x}, \bar{y}, \bar{z})$ (the use of superscripts here anticipates future notation) denote two rectangular coordinate systems at $O$ and that the direction angles of the $\bar{x}^{i}$-axis relative to the $x$-, $y$-, and $z$-axes are $\left(\alpha_{i}, \beta_{i}, \gamma_{i}\right), i=1,2,3$. Show that the correspondence between the coordinate systems is given by $\overline{\mathbf{x}}=A \mathbf{x}$, where $\mathbf{x}=(x, y, z)$, $\overline{\mathbf{x}}=(\bar{x}, \bar{y}, \bar{z})$, and where the matrix

$$
A=\left[\begin{array}{lll}
\cos \alpha_{1} & \cos \beta_{1} & \cos \gamma_{1} \\
\cos \alpha_{2} & \cos \beta_{2} & \cos \gamma_{2} \\
\cos \alpha_{3} & \cos \beta_{3} & \cos \gamma_{3}
\end{array}\right]
$$

is orthogonal.

Let the unit vectors along the $\bar{x}$-, $\bar{y}$, and $\bar{z}$-axes be $\overline{\mathbf{i}}=\overrightarrow{O P}, \overline{\mathbf{j}}=\overrightarrow{O Q}$, and $\overline{\mathbf{k}}=\overrightarrow{O R}$, respectively (see Fig. 2-1). If $\overline{\mathbf{x}}$ is the position vector of any point $W(x, y, z)$, then

$$
\overline{\mathbf{x}}=\bar{x} \overline{\mathbf{i}}+\bar{y} \overline{\mathbf{j}}+\bar{z} \overline{\mathbf{k}}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-028}
\end{center}

Fig. 2-1

We know that the $(x, y, z)$-coordinates of $P$ are $\left(\cos \alpha_{1}, \cos \beta_{1}, \cos \gamma_{1}\right)$. Similar statements hold for the coordinates of $Q$ and $R$, respectively. Hence:

$$
\begin{aligned}
\overline{\mathbf{i}} & =\left(\cos \alpha_{1}\right) \mathbf{i}+\left(\cos \beta_{1}\right) \mathbf{j}+\left(\cos \gamma_{1}\right) \mathbf{k} \\
\overline{\mathbf{j}} & =\left(\cos \alpha_{2}\right) \mathbf{i}+\left(\cos \beta_{2}\right) \mathbf{j}+\left(\cos \gamma_{2}\right) \mathbf{k} \\
\overline{\mathbf{k}} & =\left(\cos \alpha_{3}\right) \mathbf{i}+\left(\cos \beta_{3}\right) \mathbf{j}+\left(\cos \gamma_{3}\right) \mathbf{k}
\end{aligned}
$$

Substituting these into the expression for $\overline{\mathbf{x}}$ and collecting coefficients of $\mathbf{i}, \mathbf{j}$, and $\mathbf{k}$ :

$$
\begin{aligned}
\overline{\mathbf{x}}=\left(\bar{x} \cos \alpha_{1}\right. & \left.+\bar{y} \cos \alpha_{2}+\bar{z} \cos \alpha_{3}\right) \mathbf{i} \\
& +\left(\bar{x} \cos \beta_{1}+\bar{y} \cos \beta_{2}+\bar{z} \cos \beta_{3}\right) \mathbf{j} \\
& +\left(\bar{x} \cos \gamma_{1}+\bar{y} \cos \gamma_{2}+\bar{z} \cos \gamma_{3}\right) \mathbf{k}
\end{aligned}
$$

Hence, the $x$-coordinate of $W$ is the coefficient of $\mathbf{i}$, or

Similarly,

$$
x=\bar{x} \cos \alpha_{1}+\bar{y} \cos \alpha_{2}+\bar{z} \cos \alpha_{3}
$$

$$
\begin{aligned}
& y=\bar{x} \cos \beta_{1}+\bar{y} \cos \beta_{2}+\bar{z} \cos \beta_{3} \\
& z=\bar{x} \cos \gamma_{1}+\bar{y} \cos \gamma_{2}+\bar{z} \cos \gamma_{3}
\end{aligned}
$$

In terms of the matrix $A$ defined above, we can write these three equations in the matrix form


\begin{equation*}
\mathbf{x}=A^{T} \overline{\mathbf{x}} \tag{1}
\end{equation*}


Now, the $(i, j)$-element of the matrix $A A^{T}$ is

$$
\cos \alpha_{i} \cos \alpha_{j}+\cos \beta_{i} \cos \beta_{j}+\cos \gamma_{i} \cos \gamma_{j}
$$

for $i, j=1,2,3$. Note that the diagonal elements,

$$
\left(\cos \alpha_{i}\right)^{2}+\left(\cos \beta_{i}\right)^{2}+\left(\cos \gamma_{i}\right)^{2} \quad(i=1,2,3)
$$

are the three quantities $\overrightarrow{O P} \cdot \overrightarrow{O P}, \overrightarrow{O Q} \cdot \overrightarrow{O Q}, \overrightarrow{O R} \cdot \overrightarrow{O R}$; i.e., they are unity. If $i \neq j$, then the corresponding element of $A A^{T}$ is either $\overrightarrow{O P} \cdot \overrightarrow{O Q}, \overrightarrow{O P} \cdot \overrightarrow{O R}$, or $\overrightarrow{O Q} \cdot \overrightarrow{O R}$, and is therefore zero (since these vectors are mutually orthogonal). Hence, $A A^{T}=I$ (and also $A^{T} A=I$ ), and, from (1),

$$
A \mathbf{x}=A A^{T} \overrightarrow{\mathbf{x}}=\overline{\mathbf{x}}
$$

\section*{CURVILINEAR COORDINATES}
2.22 A curvilinear coordinate system $(\bar{x}, \bar{y})$ is defined in terms of rectangular coordinates $(x, y)$ by


\begin{align*}
& \bar{x}=x^{2}-x y  \tag{1}\\
& \bar{y}=x y
\end{align*}


Show that in the barred coordinate system the equation of the line $y=x-1$ is $\bar{y}=\bar{x}^{2}-\bar{x}$. [In the alibi interpretation, (1) deforms the straight line into a parabola.]

It helps initially to parameterize the equation of the line as $x=t, y=t-1$. Substitution of $x=t$, $y=t-1$ in the change-of-coordinates formula gives the parametric equations of the line in the barred coordinate system:


\begin{align*}
& \bar{x}=t^{2}-t(t-1)=t \\
& \bar{y}=t(t-1)=t^{2}-t \tag{2}
\end{align*}


Now $t$ may be eliminated from (2) to give $\bar{y}=\bar{x}^{2}-\bar{x}$.

\section*{CHAIN RULE}
2.23 Suppose that under a change of coordinates, $\bar{x}_{i}=\bar{x}_{i}\left(x_{1}, x_{2}, \ldots, x_{n}\right)(1 \leqq i \leqq n)$, the realvalued vector functions $\left(\bar{T}_{i}\right)$ and $\left(T_{i}\right)$ are related by the formula


\begin{equation*}
\bar{T}_{i}=T_{r} \frac{\partial x_{r}}{\partial \bar{x}_{i}} \tag{1}
\end{equation*}


Find the transformation rule for the partial derivatives of $\left(T_{i}\right)$-that is, express the $\partial \bar{T}_{i} / \partial \bar{x}_{j}$ in terms of the $\partial T_{r} / \partial x_{s}$-given that all second-order partial derivatives are zero.

Begin by taking the partial derivative with respect to $\bar{x}_{j}$ of both sides of (1), using the product rule:

$$
\frac{\partial \bar{T}_{i}}{\partial \bar{x}_{j}}=\frac{\partial}{\partial \bar{x}_{j}}\left\{T_{r} \frac{\partial x_{r}}{\partial \bar{x}_{i}}\right\}=\frac{\partial T_{r}}{\partial \bar{x}_{j}} \frac{\partial x_{r}}{\partial \bar{x}_{i}}+T_{r} \frac{\partial}{\partial \bar{x}_{j}}\left\{\frac{\partial x_{r}}{\partial \bar{x}_{i}}\right\}
$$

By assumption, the second term on the right is zero; and, by the chain rule,

$$
\frac{\partial T_{r}}{\partial \bar{x}_{j}}=\frac{\partial T_{r}}{\partial x_{s}} \frac{\partial x_{s}}{\partial \bar{x}_{j}}
$$

Consequently, the desired transformation rule is

$$
\frac{\partial \bar{T}_{i}}{\partial \bar{x}_{j}}=\frac{\partial T_{r}}{\partial x_{s}} \frac{\partial x_{s}}{\partial \bar{x}_{j}} \frac{\partial x_{r}}{\partial \bar{x}_{i}}
$$

\section*{Supplementary Problems}
2.24 Display the matrices $(a)\left[u^{i j}\right]_{35},(b)\left[u^{j i}\right]_{35},(c)\left[u^{i j}\right]_{53},(d)\left[\delta_{j}^{i}\right]_{36}$.

2.25 Carry out the following matrix multiplications:

$$
\text { (a) }\left[\begin{array}{rrr}
3 & -1 & 2 \\
0 & 1 & -1 \\
1 & 2 & 0
\end{array}\right]\left[\begin{array}{l}
1 \\
2 \\
2
\end{array}\right] \quad \text { (b) }\left[\begin{array}{rr}
3 & -1 \\
2 & 0
\end{array}\right]\left[\begin{array}{rrr}
1 & 1 & -1 \\
2 & 1 & 1
\end{array}\right]
$$

2.26 Prove by the product rule and by use of the summation convention the associative law for matrices:

$$
(A B) C=A(B C)
$$

where $A \equiv\left(a_{i j}\right), B \equiv\left(b_{i j}\right)$, and $C \equiv\left(c_{i j}\right)$ are arbitrary matrices, but compatible for multiplication.

2.27 Prove: (a) if $A$ and $B$ are symmetric matrices and if $A B=B A=C$, then $C$ is symmetric; (b) if $A$ and $B$ are skew-symmetric and if $A B=-B A=C$, then $C$ is skew-symmetric.

2.28 Prove that the product of two orthogonal matrices is orthogonal.

2.29 Evaluate the determinants

$$
\text { (a) }\left|\begin{array}{rr}
3 & -2 \\
1 & 5
\end{array}\right| \quad \text { (b) }\left|\begin{array}{rrr}
2 & 1 & -1 \\
3 & 0 & 1 \\
1 & -1 & 2
\end{array}\right| \quad \text { (c) }\left|\begin{array}{rrrrr}
-1 & 1 & -1 & 1 & 0 \\
1 & 0 & 1 & 1 & 1 \\
0 & 1 & 0 & 0 & 0 \\
-1 & 1 & 0 & 1 & 1 \\
1 & 1 & 0 & 0 & 0
\end{array}\right|
$$

2.30 In the Laplace expansion of the fourth-order determinant $\left|a_{i j}\right|$, the six-term summation $e_{2 i j k} a_{12} a_{2 i} a_{3 j} a_{4 k}$ appears. (a) Write out this sum explicitly, then (b) represent it as a third-order determinant.

2.31 Prove that if a matrix has two rows the same, its determinant is zero. (Hint: First show that interchanging any two subscripts reverses the sign of the permutation symbol.)

2.32 Calculate the inverse of

$$
\text { (a) }\left[\begin{array}{ll}
3 & 1 \\
5 & 2
\end{array}\right] \quad(b)\left[\begin{array}{rrr}
0 & 1 & 2 \\
1 & -1 & 0 \\
2 & 1 & -1
\end{array}\right]
$$

2.33 (a) Verify the following formulas for the permutation symbols $e_{i j}$ and $e_{i j k}$ (for distinct values of the indices only):

$$
e_{i j}=\frac{j-i}{|j-i|} \quad e_{i j k}=\frac{(j-i)(k-i)(k-j)}{|j-i||k-i||k-j|}
$$

(b) Prove the general formula:

$$
e_{i_{1} i_{2} \cdots i_{n}}=\frac{\left(i_{2}-i_{1}\right)\left(i_{3}-i_{1}\right) \cdots\left(i_{n}-i_{1}\right)\left(i_{3}-i_{2}\right) \cdots\left(i_{n}-i_{2}\right) \cdots\left(i_{n}-i_{n-1}\right)}{\left|i_{2}-i_{1}\right|\left|i_{3}-i_{1}\right| \cdots\left|i_{n}-i_{1}\right|\left|i_{3}-i_{2}\right| \cdots\left|i_{n}-i_{2}\right| \cdots\left|i_{n}-i_{n-1}\right|} \equiv \prod_{p>q} \frac{i_{p}-i_{q}}{\left|i_{p}-i_{q}\right|}
$$

2.34 Calculate the angle between the $\mathbf{R}^{6}$-vectors $\mathbf{x}=(3,-1,0,1,2,-3)$ and $\mathbf{y}=(-2,1,0,1,0,0)$.

2.35 Find two linearly independent vectors in $\mathbf{R}^{3}$ which are orthogonal to the vector $(3,-2,1)$.

2.36 Solve for $x$ and $y$ by use of matrices:

$$
\begin{aligned}
& 3 x-4 y=-23 \\
& 5 x+3 y=10
\end{aligned}
$$

2.37 Write out the quadratic form in $\mathbf{R}^{3}$ represented by $Q=\mathbf{x}^{T} A \mathbf{x}$, where

$$
A=\left[\begin{array}{rrr}
1 & 4 & 3 \\
4 & 2 & 0 \\
3 & 0 & -1
\end{array}\right]
$$

2.38 Represent with a symmetric matrix $A$ the quadratic form in $\mathbf{R}^{4}$

$$
Q=-3 x_{1}^{2}-x_{2}^{2}+x_{3}^{2}-x_{1} x_{2}-x_{1} x_{3}+6 x_{1} x_{4}
$$

2.39 Given the hyperplane $c_{r} x_{r}=1$, how do the coefficients $c_{i}$ transform under a change of coordinates $\bar{x}_{i}=a_{i j} x_{j}$ ?

2.40 Calculate the $g_{i j}$ for the distance formula (2.14) in a barred coordinate system defined by $\overline{\mathbf{x}}=A \mathbf{x}$, with

$$
A=\left[\begin{array}{rr}
1 & -2 \\
2 & 3
\end{array}\right]
$$

2.41 Test the distance formula of Problem 2.40 on the pair of points whose unbarred coordinates are $(2,-1)$ and $(2,-4)$.

2.42 (a) Show that for independent functions $\bar{x}_{i}=\bar{x}_{i}\left(x_{1}, x_{2}, \ldots, x_{n}\right)$,


\begin{equation*}
\frac{\partial \bar{x}_{i}}{\partial x_{r}} \frac{\partial x_{r}}{\partial \bar{x}_{j}}=\delta_{j}^{i} \tag{1}
\end{equation*}


(b) Take the partial derivative with respect to $x_{k}$ of (1) to establish the formula


\begin{equation*}
\frac{\partial^{2} \bar{x}_{i}}{\partial x_{k} \partial x_{r}} \frac{\partial x_{r}}{\partial \bar{x}_{j}}=-\frac{\partial^{2} x_{r}}{\partial \bar{x}_{s} \partial \bar{x}_{j}} \frac{\partial \bar{x}_{i}}{\partial x_{r}} \frac{\partial \bar{x}_{s}}{\partial x_{k}} \tag{2}
\end{equation*}


\section*{Chapter 3}
\section*{General Tensors}
\subsection*{3.1 COORDINATE TRANSFORMATIONS}
At this point the notation for coordinates will be changed to that usual in tensor calculus.

\section*{Superscripts for Vector Components}
The coordinates of a point (vector) in $\mathbf{R}^{n}$ will henceforth be denoted as $\left(x^{1}, x^{2}, x^{3}, \ldots, x^{n}\right)$. Thus, the familiar subscripts are now replaced by superscripts, and the upper position is no longer reserved for exponents alone. It will be clear by context whether a character represents a vector component or the power of a scalar.

EXAMPLE 3.1 If a power of some vector component is to be indicated, obviously parentheses are necessary; thus, $\left(x^{3}\right)^{2}$ and $\left(x^{n-1}\right)^{5}$ represent, respectively, the square of the third component, and the $(n-1)$ st component raised to the fifth power, of the vector $\mathbf{x}$. If $u$ is introduced as a real number, then $u^{2}$ and $u^{3}$ are powers of $u$ and not vector components. If $(c)^{k}$ appears without explanation, the parentheses indicate the use of the superscript $k$ as an exponent and not as the index of a vector component.

\section*{Rectangular Coordinates}
Coordinates in $\mathbf{R}^{n}$ are called rectangular (also rectangular cartesian or cartesian) if they are patterned after the usual orthogonal coordinate systems of two- and three-dimensional analytic geometry. A general definition that is workable in this setting is, in effect, an assertion of the converse of the Pythagorean theorem.

Definition 1: A coordinate system $\left(x^{i}\right)$ is rectangular if the distance between two arbitrary points $P\left(x^{1}, x^{2}, \ldots, x^{n}\right)$ and $Q\left(y^{1}, y^{2}, \ldots, y^{n}\right)$ is given by

$$
P Q=\sqrt{\left(x^{1}-y^{1}\right)^{2}+\left(x^{2}-y^{2}\right)^{2}+\cdots+\left(x^{n}-y^{n}\right)^{2}} \equiv \sqrt{\delta_{i j} \Delta x^{i} \Delta x^{j}}
$$

where $\Delta x^{i} \equiv x^{i}-y^{i}$.

Under orthogonal coordinate changes, which are isometric, the above formula for distance is invariant (cf. Section 2.5). Hence, all coordinate systems $\left(\bar{x}^{i}\right)$ defined by $\bar{x}^{i}=a_{r}^{i} x^{r}$, where $\left(a_{j}^{i}\right)$ is such that $a_{i}^{r} a_{j}^{r}=\delta_{i j}$, are rectangular. It can be shown that these are the only rectangular coordinate systems whose origin coincides with that of the $\left(x^{i}\right)$-system.

\section*{Curvilinear Coordinates}
Suppose that in some region of $\mathbf{R}^{n}$ two coordinate systems are defined, and that these two systems are connected by equations of the form


\begin{equation*}
\mathscr{T}: \bar{x}^{i}=\bar{x}^{i}\left(x^{1}, x^{2}, \ldots, x^{n}\right) \quad(1 \leqq i \leqq n) \tag{3.1}
\end{equation*}


where, for each $i$, the function, or scalar field, $x^{i}\left(x^{1}, x^{2}, \ldots, x^{n}\right)$ maps the given region in $\mathbf{R}^{n}$ to the reals and has continuous second-partial derivatives at every point in the region (is class $C^{2}$ ). The transformation $\mathscr{T}$, if bijective, is called a coordinate transformation, as in Section 2.6. If $\left(x^{i}\right)$ are ordinary rectangular coordinates, the $\left(\bar{x}^{i}\right)$ are called curvilinear coordinates unless $\mathscr{T}$ is linear, in which case $\left(\bar{x}^{i}\right)$ are called affine coordinates.

For convenience, the three most common curvilinear-coordinate systems are presented below. In each case, a "reverse" notation is employed: the two- or three-dimensional curvilinear system $\left(x^{i}\right)$ is defined by the mapping $\mathscr{T}$ that takes it into a rectangular system $\left(\bar{x}^{i}\right)$ of the same dimension.

Polar coordinates (Fig. 3-1). Let $\left(\bar{x}^{1}, \bar{x}^{2}\right)=(x, y)$ and $\left(x^{1}, x^{2}\right)=(r, \theta)$, under the restriction $r>0$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033}
\end{center}

Fig. 3-1

Then,

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{1} \cos x^{2}  \tag{3.2}\\
\bar{x}^{2}=x^{1} \sin x^{2}
\end{array} \quad \mathscr{T}^{-1}:\left\{\begin{array}{l}
x^{1}=\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}} \\
x^{2}=\tan ^{-1}\left(\bar{x}^{2} / \bar{x}^{-1}\right)
\end{array}\right.\right.
\]

(The inverse given here is, in the equation for $x^{2}$, valid only in the first and fourth quadrants of the $\bar{x}_{1} \bar{x}_{2}$-plane; other solutions must be used over the other two quadrants. Likewise for the $\theta$-coordinate in the cylindrical and spherical systems.)

Cylindrical coordinates (Fig. 3-2). If $\left(\bar{x}^{1}, \bar{x}^{2}, \bar{x}^{3}\right)=(x, y, z)$ and $\left(x^{1}, x^{2}, x^{3}\right)=(r, \theta, z)$, where $r>0$,

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{1} \cos x^{2}  \tag{3.3}\\
\bar{x}^{2}=x^{1} \sin x^{2} \\
\bar{x}^{3}=x^{3}
\end{array} \quad \mathscr{T}^{-1}:\left\{\begin{array}{l}
x^{1}=\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}} \\
x^{2}=\tan ^{-1}\left(\bar{x}^{2} / \bar{x}^{-1}\right) \\
x^{3}=\bar{x}^{3}
\end{array}\right.\right.
\]

Spherical coordinates (Fig. 3-3). If $\left(\bar{x}_{1}, \bar{x}_{2}, \bar{x}_{3}\right)=(x, y, z)$ and $\left(x^{1}, x^{2}, x^{3}\right)=(\rho, \varphi, \theta)$, where $\rho>0$ and $0 \leqq \varphi \leqq \pi$,

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{1} \sin x^{2} \cos x^{3}  \tag{3.4}\\
\bar{x}^{2}=x^{1} \sin x^{2} \sin x^{3} \\
\bar{x}^{3}=x^{1} \cos x^{2}
\end{array} \quad \mathscr{T}^{-1}:\left\{\begin{array}{l}
x^{1}=\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}+\left(\bar{x}^{3}\right)^{2}} \\
x^{2}=\cos ^{-1}\left(\bar{x}^{3} / \sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}+\left(\bar{x}^{3}\right)^{2}}\right) \\
x^{3}=\tan ^{-1}\left(\bar{x}^{2} / \bar{x}^{1}\right)
\end{array}\right.\right.
\]

(Caution: In an older but still common notation for spherical coordinates, $\theta$ denotes the polar angle and $\varphi$ the equatorial angle.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-033(1)}
\end{center}

Fig. 3-2

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-034}
\end{center}

Fig. 3-3

\section*{The Jacobian}
 matrix,The $n^{2}$ first-order partial derivatives $\partial \bar{x}^{i} / \partial x^{j}$ arising from (3.1) are normally arranged in an $n \times n$

\[
J=\left[\begin{array}{cccc}
\frac{\partial \bar{x}^{1}}{\partial x^{1}} & \frac{\partial \bar{x}^{1}}{\partial x^{2}} & \cdots & \frac{\partial \bar{x}^{1}}{\partial x^{n}}  \tag{3.5}\\
\frac{\partial \bar{x}^{2}}{\partial x^{1}} & \frac{\partial \bar{x}^{2}}{\partial x^{2}} & \cdots & \frac{\partial \bar{x}^{2}}{\partial x^{n}} \\
\cdots \cdots & \cdots & \cdots \\
\frac{\partial \bar{x}^{n}}{\partial x^{1}} & \frac{\partial \bar{x}^{n}}{\partial x^{2}} & \cdots & \frac{\partial \bar{x}^{n}}{\partial x^{n}}
\end{array}\right]
\]

Matrix $J$ is the Jacobian matrix, and its determinant $\mathscr{F} \equiv \operatorname{det} J$ is the Jacobian, of the transformation $\mathscr{T}$.

EXAMPLE 3.2 In $\mathbf{R}^{2}$ let a curvilinear coordinate system $\left(\bar{x}^{i}\right)$ be defined from rectangular coordinates $\left(x^{i}\right)$ by the equations

$$
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{1} x^{2} \\
\bar{x}^{2}=\left(x^{2}\right)^{2}
\end{array}\right.
$$

Since $\partial \bar{x}^{1} / \partial x^{1}=x^{2}, \partial \bar{x}^{1} / \partial x^{2}=x^{1}, \partial \bar{x}^{2} / \partial x^{1}=0$, and $\partial \bar{x}^{2} / \partial x^{2}=2 x^{2}$, the Jacobian of $\mathscr{T}$ is

$$
\mathscr{J}=\left|\begin{array}{cc}
x^{2} & x^{1} \\
0 & 2 x^{2}
\end{array}\right|=2\left(x^{2}\right)^{2}
$$

A well-known theorem from analysis states that $\mathscr{T}$ is locally bijective on an open set $\mathscr{U}$ in $\mathbf{R}^{n}$ if and only if $\mathscr{F} \neq 0$ at each point of $U$. When $\mathscr{F} \neq 0$ in $U$ and $\mathscr{T}$ is class $C^{2}$ in $\mathscr{U}$, then (3.1) is termed an admissible change of coordinates for $\mathcal{U}$.

EXAMPLE 3.3 The curvilinear coordinates of Example 3.2 are admissible for the regions $x^{2}>0$ and $x^{2}<0$ (both open sets in the plane). See Problem 3.1.

In an admissible change of coordinates, the inverse transformation $\mathscr{T}^{-1}$ (the local existence of which is guaranteed by the theorem mentioned above) is also class $C^{2}$, on $\bar{U}$, the image of $\mathscr{U}$ under $\mathscr{T}$. Moreover, if $\mathscr{T}^{-1}$ has the form


\begin{equation*}
\mathscr{T}^{-1}: x^{i}=x^{i}\left(\bar{x}^{-1}, \bar{x}^{2}, \ldots, \bar{x}^{n}\right) \quad(1 \leqq i \leqq n) \tag{3.6}
\end{equation*}


on $\bar{U}$, the Jacobian matrix $\bar{J}$ of $\mathscr{T}^{-1}$ is the inverse of $J$. Thus, $J \bar{J}=\bar{J} J=I$, or


\begin{equation*}
\frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{r}}{\partial \bar{x}^{j}}=\frac{\partial x^{i}}{\partial \bar{x}^{r}} \frac{\partial \bar{x}^{r}}{\partial x^{j}}=\delta_{j}^{i} \tag{3.7}
\end{equation*}


[cf. Problem 2.42(a)]. It also follows that $\overline{\mathscr{F}}=1 / \overline{\mathscr{g}}$.

\section*{General Coordinate Systems}
In later developments it will be necessary to adopt coordinate systems that are not tied to rectangular coordinates in any way [via (3:1)] and to define distance in terms of an arc-length formula for arbitrary curves, with points represented abstractly by $n$-tuples $\left(x^{1}, x^{2}, \ldots, x^{n}\right)$. Each such distance functional or metric will be invariant under admissible changes of coordinates, and admissible coordinate systems will exist for each separate metric. Under such metrics, $\mathbf{R}^{n}$ will generally become non-Euclidean; e.g., the angle sum of a triangle will not invariably equal $\pi$.

Although the curvilinear coordinate systems presented above are explicitly associated with the Euclidean metric (since they are connected via (3.1) with rectangular coordinates and Euclidean space), those same systems could be formally adopted in a non-Euclidean space if some purpose were served by doing so. The point to be made is that the space metric and the coordinate system used to describe that metric are completely independent of each other, except in the single instance of rectangular coordinates, whose very definition (see Definition 1) involves the Euclidean metric.

\section*{Usefulness of Coordinate Changes}
A primary concern in studying tensor analysis is the manner in which a change of coordinates affects the way geometrical objects or physical laws are described. For example, in rectangular coordinates the equation of a circle of radius $a$ centered at the origin is quadratic,

$$
\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}=a^{2}
$$

but in polar coordinates, (3.2), that same circle has the simple linear equation $x^{1}=a$. The reader is no doubt familiar with the sometimes dramatic change that takes place in a differential equation under a change of variables, which is nothing but a change of coordinates. This idea of changing the description of phenomena by changing coordinate systems lies at the heart of not only what a tensor means, but how it is used in practice.

\subsection*{3.2 FIRST-ORDER TENSORS}
Consider a vector field $\mathbf{V}=\left(V^{i}\right)$ defined on some subset $\mathscr{S}$ of $\mathbf{R}^{n}$ [that is, for each $i$, the component $V^{i}=V^{i}(\mathbf{x})$ is a scalar field (real-valued function) as $\mathbf{x}$ varies over $\mathscr{Y}$ ]. In each admissible coordinate system of a region $U$ containing $\mathscr{S}$, let the $n$ components $V^{1}, V^{2}, \ldots, V^{n}$ of $\mathbf{V}$ be expressible as $n$ real-valued functions; say, as

$$
T^{1}, \quad T^{2}, \ldots, \quad T^{n} \quad \text { in the }\left(x^{i}\right) \text {-system }
$$

and

$$
\bar{T}^{1}, \quad \bar{T}^{2}, \ldots, \quad \bar{T}^{n} \quad \text { in the }\left(\bar{x}^{i}\right) \text {-system }
$$

where $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$ are related by (3.1) and (3.6).

Definition 2: The vector field $\mathbf{V}$ is a contravariant tensor of order one (or contravariant vector) provided its components $\left(T^{i}\right)$ and $\left(\bar{T}^{i}\right)$ relative to the respective coordinate systems $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$ obey the law of transformation


\begin{equation*}
\text { contravariant vector } \quad \bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \quad(1 \leqq i \leqq n) \tag{3.8}
\end{equation*}


EXAMPLE 3.4 Let $\mathscr{C}$ be a curve given parametrically in the $\left(x^{i}\right)$-system by

$$
x^{i}=x^{i}(t) \quad(a \leqq t \leqq b)
$$

The tangent vector field $\mathbf{T}=\left(T^{i}\right)$ is defined by the usual differentiation formula

$$
T^{i}=\frac{d x^{i}}{d t}
$$

Under a change of coordinates (3.1), the same curve is given in the $\left(\bar{x}^{i}\right)$-system by

$$
\bar{x}^{i}=\bar{x}^{i}(t) \equiv \bar{x}^{i}\left(x^{1}(t), x^{2}(t), \ldots, x^{n}(t)\right) \quad(a \leqq t \leqq b)
$$

and the tangent vector for $\mathscr{C}$ in the $\left(\bar{x}^{i}\right)$-system has components

$$
\bar{T}^{i}=\frac{d \bar{x}^{i}}{d t}
$$

But, by the chain rule,

$$
\frac{d \bar{x}^{i}}{d t}=\frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{d x^{r}}{d t} \quad \text { or } \quad \bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}
$$

proving that $\mathbf{T}$ is a contravariant vector. (Note that because $\mathbf{T}$ is defined only on the curve $\mathscr{C}$, we have $\mathscr{S}=\mathscr{C}$ for this particular vector field.) We conclude in general that under a change of coordinates, the tangent vector of $a$ smooth curve transforms as a contravariant tensor of order one.

Remark 1: In some treatments of the subject, tensors are defined to possess certain weights, with (3.8) replaced by


\begin{equation*}
\text { weighted contravariant vector } \quad \bar{T}^{i}=w T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \quad(1 \leqq i \leqq n) \tag{3.9}
\end{equation*}


for some real-valued function $w$ (the "weight of $\mathbf{T}$ ").

In framing the next definition we (arbitrarily) shift to a subscript notation for the components of the vector field.

Definition 3: The vector field $\mathbf{V}$ is a covariant tensor of order one (or covariant vector) provided its components $\left(T_{i}\right)$ and $\left(\bar{T}_{i}\right)$ relative to an arbitrary pair of coordinate systems $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$, respectively, obey the law of transformation


\begin{equation*}
\text { covariant vector } \quad \bar{T}_{i}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \quad(1 \leqq i \leqq n) \tag{3.10}
\end{equation*}


EXAMPLE 3.5 Let $F(\mathbf{x})$ denote a differentiable scalar field defined in a coordinate system $\left(x^{i}\right)$ of $\mathbf{R}^{n}$. The gradient of $F$ is defined as the vector field

$$
\nabla F \equiv\left(\frac{\partial F}{\partial x^{1}}, \frac{\partial F}{\partial x^{2}}, \ldots, \frac{\partial F}{\partial x^{n}}\right)
$$

In a barred coordinate system, the gradient is given by $\overline{\nabla F}=\left(\partial \bar{F} / \partial \bar{x}^{i}\right)$, where $\bar{F}(\overline{\mathbf{x}}) \equiv F \circ \mathbf{x}(\overline{\mathbf{x}})$. The chain rule for partial derivatives, together with the functional relations (3.6), gives

$$
\frac{\partial \bar{F}}{\partial \bar{x}^{i}}=\frac{\partial F}{\partial x^{r}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}
$$

which is just (3.10) for $T_{i}=\partial F / \partial x^{i}, \bar{T}_{i}=\partial \bar{F} / \partial \bar{x}^{i}$. Thus, the gradient of an arbitrary differentiable function is $a$ covariant vector.

Remark 2: Tangent vectors and gradient vectors are really two different kinds of vectors. Tensor calculus is vitally concerned with the distinction between contravariance and covariance, and consistently employs upper indices to indicate the one and lower indices to indicate the other.

Remark 3: From this point on, we shall frequently refer to first-order tensors, contravariant or covariant as the case may be, simply as "vectors"; they are, of course, actually vector fields, defined on $\mathbf{R}^{n}$. This usage will coexist with our earlier employment of "vectors" to denote real $n$-tuples; i.e., elements of $\mathbf{R}^{n}$. There is no conflict here insofar as the $n$-tuples make up the vector field corresponding to the identity mapping $V^{i}(\mathbf{x})=$ $x^{i} \quad(i=1,2, \ldots, n)$. But the vector $\left(x^{i}\right)$ does not enjoy the transformation property of a tensor; so, to emphasize that fact, we shall sometimes refer to it as a position vector.

\subsection*{3.3 INVARIANTS}
Objects, functions, equations, or formulas that are independent of the coordinate system used to express them have intrinsic value and are of fundamental significance; they are called invariants. Roughly speaking, the product of a contravariant vector and a covariant vector always is an invariant. The following is a more precise statement of this fact.

Theorem 3.1: Let $S^{i}$ and $T_{i}$ be the components of a contravariant and covariant vector, respectively. If the inner product $E \equiv S^{r} T_{r}$ is defined in each coordinate system, then $E$ is an invariant.

EXAMPLE 3.6 In Examples 3.4 and 3.5 it was established that the tangent vector, $\left(S^{i}\right)=\left(d x^{i} / d t\right)$, to a curve $\mathscr{C}$ and the gradient of a function, $\left(T_{i}\right)=\left(\partial F / \partial x^{i}\right)$, are contravariant and covariant vectors, respectively. Let us verify Theorem 3.1 for these two vectors. Define

$$
E=S^{r} T_{r} \equiv \frac{\partial F}{\partial x^{r}} \frac{d x^{r}}{d t}
$$

Now, by the chain rule,

$$
E=\frac{d F}{d t}
$$

so the assertion of Theorem 3.1 is that the value of

$$
\frac{d}{d t}\left[F \circ\left(x^{i}(t)\right)\right] \equiv \frac{d}{d t}[\hat{F}(t)]
$$

is independent of the particular coordinate system $\left(x^{i}\right)$ used to specify the curve. To visualize this, the reader should study Fig. 3-4, which shows how the composition $\hat{F}=F^{\circ}\left(x^{i}(t)\right)$ works out in $\mathbf{R}^{3}$. It is apparent here that the map $\hat{F}$ entirely bypasses the coordinate system $\left(x^{1}, x^{2}, x^{3}\right)$. Thus, $\hat{F}$ - and with it, $d \hat{F} / d t-$ is an invariant with respect to coordinate changes.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-037}
\end{center}

Fig. 3-4

\subsection*{3.4 HIGHER-ORDER TENSORS}
Tensors of arbitrary order may be defined. Although most work does not involve tensors of order greater than 4 , the general definition will be included here for completeness. We begin with the three types of second-order tensors.

\section*{Second-Order Tensors}
Let $\mathbf{V}=\left(V^{i j}\right)$ denote a matrix field; that is, $\left(V^{i j}\right)$ is an $n \times n$ matrix of scalar fields $V^{i j}(\mathbf{x})$, all defined over the same region $\mathscr{U}=\{\mathbf{x}\}$ in $\mathbf{R}^{n}$. As before, it will be assumed that $\mathbf{V}$ has a representation $\left(T^{i j}\right)$ in $\left(x^{i}\right)$ and $\left(\bar{T}^{i j}\right)$ in $\left(\bar{x}^{i}\right)$, where $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$ are admissible coordinates related by (3.1) and (3.6).

Definition 4: The matrix field $\mathbf{V}$ is a contravariant tensor of order two if its components $\left(T^{i j}\right)$ in $\left(x^{i}\right)$ and $\left(\bar{T}^{i j}\right)$ in $\left(\bar{x}^{i}\right)$ obey the law of transformation


\begin{equation*}
\text { contravariant tensor } \quad \bar{T}^{i j}=T^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \quad(1 \leqq i, j \leqq n) \tag{3.11}
\end{equation*}


Again going over to subscript notation for the components of the matrix field, we state

Definition 5: The matrix field $\mathbf{V}$ is a covariant tensor of order two if its components $\left(T_{i j}\right)$ in $\left(x^{i}\right)$ and $\left(\bar{T}_{i j}\right)$ in $\left(\bar{x}^{i}\right)$ obey the law of transformation


\begin{equation*}
\text { covariant tensor } \quad \bar{T}_{i j}=T_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad(1 \leqq i, j \leqq n) \tag{3.12}
\end{equation*}


Theorem 3.2: Suppose that $\left(T_{i j}\right)$ is a covariant tensor of order two. If the matrix $\left[T_{i j}\right]_{n n}$ is invertible on $\mathscr{U}$, with inverse matrix $\left[T^{i j}\right]_{n n}$, then $\left(T^{i j}\right)$ is a contravariant tensor of order two.

Definition 6: The matrix field $\mathbf{V}$ is a mixed tensor of order two, contravariant of order one and covariant of order one, if its components $\left(T_{j}^{i}\right)$ in $\left(x^{i}\right)$ and $\left(\bar{T}_{j}^{i}\right)$ in $\left(\bar{x}^{i}\right)$ obey the law of transformation


\begin{equation*}
\text { mixed tensor } \quad \bar{T}_{j}^{i}=T_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad(1 \leqq i, j \leqq n) \tag{3.13}
\end{equation*}


\section*{Tensors of Arbitrary Order}
Vector and matrix fields are inadequate for higher-order tensors. It is necessary to introduce a generalized vector field $\mathbf{V}$, which is an ordered array of $n^{m}(m=p+q)$ scalar fields, $\left(V_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right)$, defined over a region $\mathcal{U}$ in $\mathbf{R}^{n}$; let $\left(T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right.$ ) denote the set of component-functions in various coordinate systems which are defined on $U$.

Definition 7: The generalized vector field $\mathbf{V}$ is a tensor of order $m=p+q$, contravariant of order $p$ and covariant of order $q$, if its components $\left(T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right)$ in $\left(x^{i}\right)$ and $\left(\bar{T}_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right)$ in $\left(\bar{x}^{i}\right)$ obey the law of transformation

general tensor $\bar{T}_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}=T_{s_{1} s_{2} \ldots s_{q}}^{r_{1} r_{2} \ldots r_{p}} \frac{\partial \bar{x}^{i_{1}}}{\partial x^{r_{1}}} \frac{\partial \bar{x}^{i_{2}}}{\partial x^{r_{2}}} \cdots \frac{\partial \bar{x}^{i_{p}}}{\partial \bar{x}_{p}^{r_{p}}} \frac{\partial x^{s_{1}}}{\partial \bar{x}^{j_{1}}} \frac{\partial x^{s_{2}}}{\partial \bar{x}^{j_{2}}} \cdots \frac{\partial x^{s_{q}}}{\partial \bar{x}^{j_{q}}}$

with the obvious range for free indices.

\subsection*{3.5 THE STRESS TENSOR}
It was the concept of stress in mechanics that originally led to the invention of tensors (tenseur, that which exerts tension, stress). Suppose that the unit cube is in equilibrium under forces applied to three of its faces [Fig. 3-5(a)]. Since each face has unit area, each force vector represents the force

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-039(1)}
\end{center}

(b)

Fig. 3-5

per unit area, or stress. Those forces are represented in the component form in Fig. 3-5(b). Using the standard basis $\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{2}$, we have

\[
\begin{array}{ll}
\mathbf{v}_{1}=\sigma^{1 s} \mathbf{e}_{s} & (\text { stress on face } 1) \\
\mathbf{v}_{2}=\sigma^{2 s} \mathbf{e}_{s} & (\text { stress on face } 2)  \tag{3.15}\\
\mathbf{v}_{3}=\sigma^{3 s} \mathbf{e}_{s} & (\text { stress on face } 3)
\end{array}
\]

\section*{Stress on a Cube Section}
The question arises: What stress $\mathbf{F}$ is transmitted to a planar cross section of the cube that has unit normal $\mathbf{n}$ ? To answer this, refer to Fig. 3-6, which shows the tetrahedron formed by the cross section and the coordinate planes. Let $A$ be the cross-sectional area. By the assumed equilibrium of the cube, the stresses on the $x^{1} x^{2}-, x^{1} x^{3}$, and $x^{2} x^{3}$-bases of the tetrahedron are $-\mathbf{v}_{3},-\mathbf{v}_{2}$, and $-\mathbf{v}_{1}$, respectively, as shown componentwise in Fig. 3-6. Hence, the forces on these same bases are $B_{1}\left(-\mathbf{v}_{3}\right), B_{2}\left(-\mathbf{v}_{2}\right)$, and $B_{3}\left(-\mathbf{v}_{1}\right)$, respectively. For the tetrahedron itself to be in equilibrium, the resultant force on it must vanish:

$$
A \mathbf{F}+B_{1}\left(-\mathbf{v}_{3}\right)+B_{2}\left(-\mathbf{v}_{2}\right)+B_{3}\left(-\mathbf{v}_{1}\right)=0
$$

or, solving for $\mathbf{F}$,


\begin{equation*}
\mathbf{F}=\frac{B_{3}}{A} \mathbf{v}_{1}+\frac{B_{2}}{A} \mathbf{v}_{2}+\frac{B_{1}}{A} \mathbf{v}_{3} \tag{3.16}
\end{equation*}


But $B_{3}$ is the projection of $A$ in the $x^{2} x^{3}$-plane: $B_{3}=A \mathbf{n e}_{1}$ or $B_{3} / A=\mathbf{n e}_{1}$. Similarly, $B_{2} / A=\mathbf{n e}_{2}$ and $B_{1} / A=\mathbf{n e}_{3}$. Substituting these expressions and the expressions (3.15) into (3.16), we find that


\begin{equation*}
\mathbf{F}=\sigma^{r s}\left(\mathbf{n} \mathbf{e}_{r}\right) \mathbf{e}_{s} \tag{3.17}
\end{equation*}


\section*{Contravariance of Stress}
\section*{Under Change of Coordinates}
An interesting formula results from (3.17) when we change the basis of $\mathbf{R}^{3}$ by a transformation of the form $\mathbf{e}_{i}=a_{i}^{j} \mathbf{f}_{j}$ (with $\left|a_{i}^{j}\right| \neq 0$ ). In terms of coordinates,

$$
x^{i} \mathbf{e}_{i}=x^{i}\left(a_{i}^{j} \mathbf{f}_{j}\right)=\left(a_{i}^{j} x^{i}\right) \mathbf{f}_{j} \equiv \bar{x}^{j} \mathbf{f}_{j}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-040}
\end{center}

Fig. 3-6

That is, we have a new coordinate system $\left(\vec{x}^{i}\right)$ that is related to $\left(x^{i}\right)$ via


\begin{equation*}
\bar{x}^{j}=a_{i}^{j} x^{i} \tag{3.18}
\end{equation*}


Note that here we have

$$
\frac{\partial \bar{x}^{j}}{\partial x^{i}}=a_{i}^{j}
$$

Substituting $\mathbf{e}_{r}=a_{r}^{i} \mathbf{f}_{i}$ into (3.17) yields the stress components $\left(\bar{\sigma}^{i j}\right)$ in the new coordinate system, as follows:

with


\begin{gather*}
\mathbf{F}=\sigma^{r s}\left[\mathbf{n}\left(a_{r}^{i} \mathbf{f}_{i}\right)\right]\left(a_{s}^{j} \mathbf{f}_{j}\right)=\sigma^{r s} a_{r}^{i} a_{s}^{j}\left(\mathbf{n f}_{i}\right) \mathbf{f}_{j} \equiv \bar{\sigma}^{i j}\left(\mathbf{n f}_{i}\right) \mathbf{f}_{j} \\
\bar{\sigma}^{i j}=\sigma^{r s} a_{r}^{i} a_{s}^{j}=\sigma^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \tag{3.19}
\end{gather*}


A comparison of (3.19) with the transformation law (3.11) leads to the conclusion that the stress components $\sigma^{i j}$ define a second-order contravariant tensor, at least for linear coordinate changes.

\subsection*{3.6 CARTESIAN TENSORS}
Tensors corresponding to admissible linear coordinate changes, $\mathscr{T}: \bar{x}^{i}=a_{j}^{i} x^{j} \quad\left(\left|a_{j}^{i}\right| \neq 0\right)$, are called affine tensors. If ( $a_{j}^{i}$ ) is orthogonal (and $\mathscr{T}$ is distance-preserving), the corresponding tensors are cartesian tensors. Now, an object that is a tensor with respect to all one-one linear transformations is necessarily a tensor with respect to all orthogonal linear transformations, but the converse is not true. Hence, affine tensors are special cartesian tensors. Likewise, affine invariants are particular cartesian invariants.

\section*{Affine Tensors}
A transformation of the form $\mathscr{T}: \bar{x}^{i}=a_{j}^{i} x^{j} \quad\left(\left|a_{j}^{i}\right| \neq 0\right)$ takes a rectangular coordinate system $\left(x^{i}\right)$ into a system $\left(\bar{x}^{i}\right)$ having oblique axes; thus affine tensors are defined on the class of all such\\
oblique coordinate systems. Since the Jacobian matrices of $\mathscr{T}$ and $\mathscr{T}^{-1}$ are


\begin{equation*}
J=\left[\frac{\partial \bar{x}^{i}}{\partial x^{j}}\right]_{n n}=\left[a_{j}^{i}\right]_{n n} \quad \text { and } \quad J^{-1}=\left[\frac{\partial x^{i}}{\partial \bar{x}^{j}}\right]_{n n} \equiv\left[b_{j}^{i}\right]_{n n} \tag{3.20}
\end{equation*}


the transformation laws for affine tensors are:

\[
\begin{array}{rll}
\text { contravariant } & \bar{T}^{i}=a_{r}^{i} T^{r}, \quad \bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, \quad \bar{T}^{i j k}=a_{r}^{i} a_{s}^{j} a_{t}^{k} T^{r s t}, \ldots \\
\text { covariant } & \bar{T}_{i}=b_{i}^{r} T_{r}, \quad \bar{T}_{i j}=b_{i}^{r} b_{j}^{s} T_{r s}, \quad \bar{T}_{i j k}=b_{i}^{r} b_{j}^{s} b_{k}^{t} T_{r s t}, \quad \ldots  \tag{3.21}\\
\text { mixed } & \bar{T}_{j}^{i}=a_{r}^{i} b_{j}^{s} T_{s}^{r}, \quad \bar{T}_{j k}^{i}=a_{r}^{i} b_{j}^{s} b_{k}^{t} T_{s t}^{r}, \ldots
\end{array}
\]

Under the less stringent conditions (3.21), more objects can qualify as tensors than before; for instance, an ordinary position vector $\mathbf{x}=\left(x^{i}\right)$ becomes an (affine) tensor (see Problem 3.9), and the partial derivatives of a tensor define an (affine) tensor (as implied by Problem 2.23).

\section*{Cartesian Tensors}
When the above linear transformation $\mathscr{T}$ is restricted to be orthogonal, then $J^{-1}=J^{T}$, or

$$
b_{j}^{i}=a_{i}^{j} \quad(1 \leqq i, j \leqq n)
$$

so that the transformation laws for cartesian tensors are, from (3.21),

$$
\begin{array}{rlll}
\text { contravariant } & \bar{T}^{i}=a_{r}^{i} T^{r}, & \bar{T}^{i j}=a_{r}^{i} a_{s}^{j} T^{r s}, & \ldots \\
\text { covariant } & \bar{T}_{i}=a_{r}^{i} T_{r}, & \bar{T}_{i j}=a_{r}^{i} a_{s}^{j} T_{r s}, & \ldots \\
\text { mixed } & & \bar{T}_{j}^{i}=a_{r}^{i} a_{s}^{j} T_{s}^{r}, & \ldots
\end{array}
$$

A striking feature of these forms is that contravariant and covariant behaviors do not distinguish themselves. Consequently, all cartesian tensors are notated the same way-with subscripts:


\begin{align*}
\begin{array}{r}
\text { allowable } \\
\text { coordinate changes }
\end{array} & \bar{x}_{i}=a_{i j} x_{j} \quad \text { or } \quad x_{i}=a_{j i} \bar{x}_{j} \\
\begin{array}{r}
\text { cartesian } \\
\text { tensor laws }
\end{array} & \bar{T}_{i}=a_{i r} T_{r}, \quad \bar{T}_{i j}=a_{i r} a_{j s} T_{r s}, \ldots \tag{3.22}
\end{align*}


Because an orthogonal transformation takes one rectangular coordinate system into another (having the same origin), cartesian tensors appertain to the rectangular (cartesian) coordinate systems. There are, of course, even more cartesian tensors than affine tensors.

Note that $J J^{T}=I$ implies $\mathscr{J}^{2}=1$, or $\mathscr{J}= \pm 1$. Objects that obey the tensor laws (3.22) when the allowable coordinate changes are such that

$$
\mathscr{J}=\left|a_{i j}\right|=+1
$$

are called direct cartesian tensors.

\section*{Solved Problems}
\section*{CHANGE OF COORDINATES}
3.1 For the transformation of Example 3.2, (a) obtain the equations for $\mathscr{T}^{-1} ;(b)$ compute $\bar{J}$ from (a), and compare with $J^{-1}$.

(a) Solving $\bar{x}^{1}=x^{1} x^{2}, \bar{x}^{2}=\left(x^{2}\right)^{2}$ for $x^{1}$ and $x^{2}$, we find that

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{1} x^{2}  \tag{1}\\
\bar{x}^{2}=\left(x^{2}\right)^{2}
\end{array} \quad \mathscr{T}^{-1}:\left\{\begin{array}{l}
x^{1}=\bar{x}^{1} / \sqrt{\bar{x}^{2}} \\
x^{2}=\sqrt{\bar{x}^{2}}
\end{array}\right.\right.
\]

is a one-one mapping between the regions $x^{2}>0$ and $\bar{x}^{2}>0$, and that

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=x^{\prime} x^{2}  \tag{2}\\
\bar{x}^{2}=\left(x^{2}\right)^{2}
\end{array} \quad \mathscr{T}^{-1}:\left\{\begin{array}{l}
x^{1}=-\bar{x}^{1} / \sqrt{\bar{x}^{2}} \\
x^{2}=-\sqrt{\bar{x}^{2}}
\end{array}\right.\right.
\]

is one-one between $x^{2}<0$ and $\bar{x}^{2}>0$. Note that the two regions of the $x^{1} x^{2}$-plane are separated by the line on which the Jacobian of $\mathscr{T}$ vanishes.

(b) From Example 3.2,

$$
J=\left[\begin{array}{cc}
x^{2} & x^{1} \\
0 & 2 x^{2}
\end{array}\right] \quad \text { and so } \quad J^{-1}=\frac{1}{2\left(x^{2}\right)^{2}}\left[\begin{array}{cc}
2 x^{2} & -x^{1} \\
0 & x^{2}
\end{array}\right]
$$

valid in both regions $x^{2}>0$ and $x^{2}<0$. Now, on $\bar{x}^{2}>0$, differentiation of the inverse transformation (1), followed by a change back to unbarred coordinates, yields

$$
\bar{J}=\left[\begin{array}{ll}
\frac{\partial x^{1}}{\partial \bar{x}^{1}} & \frac{\partial x^{1}}{\partial \bar{x}^{2}} \\
\frac{\partial x^{2}}{\partial \bar{x}^{1}} & \frac{\partial x^{2}}{\partial \bar{x}^{2}}
\end{array}\right]=\left[\begin{array}{cc}
\left(\bar{x}^{2}\right)^{-1 / 2} & -\frac{1}{2} \bar{x}^{1}\left(\bar{x}^{2}\right)^{-3 / 2} \\
0 & \frac{1}{2}\left(\bar{x}^{2}\right)^{-1 / 2}
\end{array}\right]=\left[\begin{array}{cc}
\left(x^{2}\right)^{-1} & -\frac{1}{2} x^{1}\left(x^{2}\right)^{-2} \\
0 & \frac{1}{2}\left(x^{2}\right)^{-1}
\end{array}\right]
$$

It is seen that on $x^{2}>0, \bar{J}=J^{-1}$.

Similarly, from (2), with $x^{2}<0$,

$$
\bar{J}=\left[\begin{array}{cc}
-\left(\bar{x}^{2}\right)^{-1 / 2} & \frac{1}{2} \bar{x}^{1}\left(\bar{x}^{2}\right)^{-3 / 2} \\
0 & -\frac{1}{2}\left(\bar{x}^{-2}\right)^{-1 / 2}
\end{array}\right]=\left[\begin{array}{cc}
+\left(x^{2}\right)^{-1} & -\frac{1}{2} x^{1}\left(x^{2}\right)^{-2} \\
0 & +\frac{1}{2}\left(x^{2}\right)^{-1}
\end{array}\right]=J^{-1}
$$

3.2 For polar coordinates as defined by (3.2), (a) calculate the Jacobian matrix of $\mathscr{T}$ and infer the region over which $\mathscr{T}$ is bijective; $(b)$ calculate the Jacobian matrix of $\mathscr{T}^{-1}$ for the region

$$
\{(r, \theta) \mid r>0,-\pi / 2<\theta<\pi / 2\}
$$

i.e., the right half-plane, and verify that it is the inverse of the matrix of $(a)$.

(a)

$$
J=\left[\begin{array}{cc}
\frac{\partial}{\partial x^{1}}\left(x^{1} \cos x^{2}\right) & \frac{\partial}{\partial x^{2}}\left(x^{1} \cos x^{2}\right) \\
\frac{\partial}{\partial x^{1}}\left(x^{1} \sin x^{2}\right) & \frac{\partial}{\partial x^{2}}\left(x^{1} \sin x^{2}\right)
\end{array}\right]=\left[\begin{array}{ll}
\cos x^{2} & -x^{1} \sin x^{2} \\
\sin x^{2} & x^{1} \cos x^{2}
\end{array}\right]
$$

whence $\mathscr{J}=x^{1} \equiv r$. Therefore, $\mathscr{T}$ is bijective on the open set $r>0$, which is the entire plane punctured at the origin.

(b) For $\mathscr{T}^{-1}$ we have, over the right half-plane,

$$
\begin{gathered}
\frac{\partial x^{1}}{\partial \bar{x}^{1}}=\frac{\bar{x}^{1}}{\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}} \quad \frac{\partial x^{1}}{\partial \bar{x}^{2}}=\frac{\bar{x}^{2}}{\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}} \\
\frac{\partial x^{2}}{\partial \bar{x}^{1}}=\frac{1}{1+\left(\bar{x}^{2} / \bar{x}^{1}\right)^{2}}\left[-\frac{\bar{x}^{2}}{\left(\bar{x}^{1}\right)^{2}}\right]=\frac{-\bar{x}^{2}}{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}} \quad \frac{\partial x^{2}}{\partial \bar{x}^{2}}=\frac{\bar{x}^{1}}{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}
\end{gathered}
$$

and so

$$
\bar{J}=\left[\begin{array}{cc}
\frac{\bar{x}^{1}}{\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}} & \frac{\bar{x}^{2}}{\sqrt{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}} \\
\frac{-\bar{x}^{2}}{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}} & \frac{\bar{x}^{1}}{\left(\bar{x}^{1}\right)^{2}+\left(\bar{x}^{2}\right)^{2}}
\end{array}\right]=\left[\begin{array}{cc}
\cos x^{2} & \sin x^{2} \\
-\frac{\sin x^{2}}{x^{1}} & \frac{\cos x^{2}}{x^{1}}
\end{array}\right]
$$

Now compute $J^{-1}$ :

$$
J^{-1}=\frac{1}{x^{1}}\left[\begin{array}{cc}
x^{1} \cos x^{2} & x^{1} \sin x^{2} \\
-\sin x^{2} & \cos x^{2}
\end{array}\right]=\left[\begin{array}{cc}
\cos x^{2} & \sin x^{2} \\
-\frac{\sin x^{2}}{x^{1}} & \frac{\cos x^{2}}{x^{1}}
\end{array}\right]=\bar{J}
$$

\section*{CONTRAVARIANT VECTORS}
3.3 If $\mathbf{V}=\left(T^{i}\right)$ is a contravariant vector, show that the partial derivatives $T_{j}^{i} \equiv \partial T^{i} / \partial x^{j}$, defined in each coordinate system, transform according to the rule

$$
\bar{T}_{j}^{i}=T_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}+T^{r} \frac{\partial^{2} \bar{x}^{i}}{\partial x^{r} \partial x^{s}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

Differentiate both sides of

$$
\bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}
$$

with respect to $\bar{x}^{j}$, using the product rule:


\begin{equation*}
\bar{T}_{j}^{i} \equiv \frac{\partial \bar{T}^{i}}{\partial \bar{x}^{j}}=\frac{\partial}{\partial \bar{x}^{j}}\left(T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)=\frac{\partial T^{r}}{\partial \bar{x}^{j}} \frac{\partial \bar{x}^{i}}{\partial x^{r}}+T^{r} \frac{\partial}{\partial \bar{x}^{j}}\left(\frac{\partial \bar{x}^{i}}{\partial x^{r}}\right) \tag{1}
\end{equation*}


By the chain rule for partial derivatives, $(2.15)$,

$$
\frac{\partial T^{r}}{\partial \bar{x}^{j}}=\frac{\partial T^{r}}{\partial x^{s}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \equiv T_{s}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad \text { and } \quad \frac{\partial}{\partial \bar{x}^{j}}\left(\frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)=\left[\frac{\partial}{\partial x^{s}}\left(\frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)\right] \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

Substituting these expressions into (1) yields the desired formula.

3.4 Suppose that $\left(T^{i}\right)$ is a contravariant vector on $\mathbf{R}^{2}$ and that $\left(T^{i}\right)=\left(x^{2}, x^{1}\right)$ in the $\left(x^{i}\right)$-system.

Calculate $\left(\bar{T}^{i}\right)$ in the $\left(\bar{x}^{i}\right)$-system, under the change of coordinates

$$
\begin{aligned}
& \bar{x}^{1}=\left(x^{2}\right)^{2} \neq 0 \\
& \bar{x}^{2}=x^{1} x^{2}
\end{aligned}
$$

By definition of contravariance,

$$
\bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}=T^{1} \frac{\partial \bar{x}^{i}}{\partial x^{1}}+T^{2} \frac{\partial \bar{x}^{i}}{\partial x^{2}}
$$

Note that the top row of the Jacobian matrix $J$ is needed for the case $i=1$, and the bottom row is needed for $i=2$.

$$
J=\left[\begin{array}{ll}
\frac{\partial \bar{x}^{1}}{\partial x^{1}} & \frac{\partial \bar{x}^{1}}{\partial x^{2}} \\
\frac{\partial \bar{x}^{2}}{\partial x^{1}} & \frac{\partial \bar{x}^{2}}{\partial x^{2}}
\end{array}\right]=\left[\begin{array}{cc}
0 & 2 x^{2} \\
x^{2} & x^{1}
\end{array}\right]
$$

Thus,

$$
\bar{T}^{1}=T^{1}(0)+T^{2}\left(2 x^{2}\right)=2 x^{1} x^{2} \quad \bar{T}^{2}=T^{1}\left(x^{2}\right)+T^{2}\left(x^{1}\right)=\left(x^{2}\right)^{2}+\left(x^{1}\right)^{2}
$$

which, in terms of barred coordinates, are

$$
\bar{T}^{1}=2 \bar{x}^{2} \quad \bar{T}^{2}=\bar{x}^{1}+\frac{\left(\bar{x}^{2}\right)^{2}}{\bar{x}^{1}}
$$

3.5 Show that a contravariant vector can be constructed the components of which take on a given set of values $(a, b, c, \ldots)$ in some particular coordinate system. (The prescribed values may be point functions.)

Let $(a, b, c, \ldots) \equiv\left(a^{i}\right)$ be the given values to be assigned in the coordinate system $\left(x^{i}\right)$. Set $V^{i}=a^{i}$ for the values in $\left(x^{i}\right)$, and for any other admissible coordinate system $\left(\bar{x}^{i}\right)$, set $\bar{V}^{i}=a^{r}\left(\partial \bar{x}^{i} / \partial x^{r}\right)$. To show that $\left(V^{i}\right)$ is a contravariant tensor, let $\left(y^{i}\right)$ and $\left(\bar{y}^{i}\right)$ be any two admissible coordinate systems. Then, $y^{i}=f^{i}\left(x^{1}, x^{2}, \ldots, x^{n}\right)$ and $\bar{y}^{i}=g^{i}\left(x^{1}, x^{2}, \ldots, x^{n}\right)$, and, by definition, the values of $\left(V^{i}\right)$ in $\left(y^{i}\right)$ and $\left(\bar{y}^{i}\right)$ are, respectively, $T^{i}=a^{r}\left(\partial y^{i} / \partial x^{r}\right)$ and $\bar{T}^{i}=a^{r}\left(\partial \bar{y}^{i} / \partial x^{r}\right)$. But, by the chain rule,

$$
\bar{T}^{i}=a^{r} \frac{\partial \bar{y}^{i}}{\partial x^{r}}=a^{r} \frac{\partial \bar{y}^{i}}{\partial y^{s}} \frac{\partial y^{s}}{\partial x^{r}}=T^{s} \frac{\partial \bar{y}^{i}}{\partial y^{s}} \quad \text { QED }
$$

\section*{COVARIANT VECTORS}
3.6 Calculate $\left(\bar{T}_{i}\right)$ in the $\left(\bar{x}^{i}\right)$-system if $\mathbf{V}=\left(T_{i}\right) \equiv\left(x^{2}, x^{1}+2 x^{2}\right)$ is a covariant vector under the coordinate transformation of Problem 3.4.

To avoid radicals, compute $J^{-1}$ in terms of $\left(x^{i}\right)$ :

$$
J^{-1}=\left[\begin{array}{cc}
\frac{-x^{1}}{2\left(x^{2}\right)^{2}} & \frac{1}{x^{2}} \\
\frac{1}{2 x^{2}} & 0
\end{array}\right]
$$

By covariance,

$$
\bar{T}_{i}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}}=T_{1} \frac{\partial x^{1}}{\partial \bar{x}^{i}}+T_{2} \frac{\partial x^{2}}{\partial \bar{x}^{i}} \quad(i=1,2)
$$

For $i=1$, read off the partials from the first column of $J^{-1}$ :

$$
\bar{T}_{1}=T_{1}\left(-x^{1} / 2\left(x^{2}\right)^{2}\right)+T_{2}\left(1 / 2 x^{2}\right)=-x^{1} / 2 x^{2}+x^{1} / 2 x^{2}+1=1
$$

Similarly, for $i=2$, use the second column of $J^{-1}$ :

$$
\bar{T}_{2}=T_{1}\left(1 / x^{2}\right)+T_{2}(0)=x^{2}\left(1 / x^{2}\right)=1
$$

Hence, $\left(\bar{T}_{i}\right)=(1,1)$ at all points in the $\left(\bar{x}^{i}\right)$-system $\left(\bar{x}^{1}=0\right.$ excluded $)$.

3.7 Use the fact that $\nabla f$ is a covariant vector (Example 3.5) to bring the partial differential equation


\begin{equation*}
x \frac{\partial f}{\partial x}=y \frac{\partial f}{\partial y} \tag{1}
\end{equation*}


into simpler form by the change of variables $\bar{x}=x y, \bar{y}=(y)^{2}$; then solve.

Write $\nabla f=(\partial f / \partial x, \partial f / \partial y) \equiv\left(T_{i}\right),\left(x^{1}, x^{2}\right)=(x, y),\left(\bar{x}^{1}, \bar{x}^{2}\right)=(\bar{x}, \bar{y})$, and

$$
\bar{T}_{i} \equiv \frac{\partial \bar{f}}{\partial \bar{x}^{i}}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}}
$$

Again calculating $J$ first, then its inverse, we have

so that

$$
\left(\frac{\partial x^{i}}{\partial \bar{x}^{j}}\right) \equiv J^{-1}=\left[\begin{array}{cc}
y & x \\
0 & 2 y
\end{array}\right]^{-1}=\left[\begin{array}{cc}
\frac{1}{y} & \frac{-x}{2(y)^{2}} \\
0 & \frac{1}{2 y}
\end{array}\right]
$$

$$
\begin{aligned}
& \frac{\partial \bar{f}}{\partial \bar{x}} \equiv \bar{T}_{1}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{1}}=T_{1} \cdot \frac{1}{y}+T_{2} \cdot 0=\frac{1}{y} \frac{\partial f}{\partial x} \\
& \frac{\partial \bar{f}}{\partial \bar{y}} \equiv \bar{T}_{2}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{2}}=T_{1} \cdot \frac{-x}{2(y)^{2}}+T_{2} \cdot \frac{1}{2 y}=-\frac{x}{2(y)^{2}} \frac{\partial f}{\partial x}+\frac{1}{2 y} \frac{\partial f}{\partial y}
\end{aligned}
$$

But, by (1),

$$
\frac{\partial \bar{f}}{\partial \bar{y}}=\frac{1}{2(y)^{2}}\left(-x \frac{\partial f}{\partial x}+y \frac{\partial f}{\partial y}\right)=0
$$

which implies that $\bar{f}=F(\bar{x})$, a function of $\bar{x}$ alone; therefore, $f=F(x y)$ is the general solution to (1).

\section*{INVARIANTS}
\subsection*{3.8 Prove Theorem 3.1.}
We must show that if $\left(S^{i}\right)$ and $\left(T_{i}\right)$ are tensors of the indicated types and order, then the quantity $E \equiv S^{i} T_{i}$ is invariant with respect to coordinate changes; that is, $\bar{E}=E$, where $\bar{E}=\bar{S}^{i} \bar{T}_{i}$. But observe that

$$
\bar{S}^{i}=S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \quad \text { and } \quad \bar{T}_{i}=T_{s} \frac{\partial x^{s}}{\partial \bar{x}^{i}}
$$

so that, in view of $(3.7)$,

$$
\bar{E}=\bar{S}^{i} \bar{T}_{i}=S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \cdot T_{s} \frac{\partial x^{s}}{\partial \bar{x}^{i}}=S^{r} T_{s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{i}}=S^{r} T_{s} \delta_{r}^{s}=S^{r} T_{r}=E
$$

3.9 Show that under linear coordinate changes of $\mathbf{R}^{n}, \bar{x}^{i}=a_{j}^{i} x^{j} \quad\left(\left|a_{j}^{i}\right| \neq 0\right)$, the equation of a hyperplane $A_{i} x^{i}=1$ is invariant provided the normal vector $\left(A_{i}\right)$ is covariant.

In view of Theorem 3.1, it suffices to show that $\left(T^{i}\right)=\left(x^{i}\right)$ is a contravariant affine tensor. But this is immediate:

$$
\bar{T}^{i} \equiv \bar{x}^{i}=a_{j}^{i} x^{j} \equiv a_{j}^{i} T^{j}
$$

which is the transformation law (3.21).

\section*{SECOND-ORDER CONTRAVARIANT TENSORS}
3.10 Suppose that the components of a contravariant tensor $T$ of order 2 in a coordinate system $\left(x^{i}\right)$ of $\mathbf{R}^{2}$ are $T^{11}=1, T^{12}=1, T^{21}=-1$, and $T^{22}=2$. (a) Find the components $\bar{T}^{j j}$ of $\mathbf{T}$ in the $\left(\bar{x}^{i}\right)$-system, connected to the $\left(x^{i}\right)$-system via

$$
\begin{aligned}
& \bar{x}^{1}=\left(x^{1}\right)^{2} \neq 0 \\
& \bar{x}^{2}=x^{1} x^{2}
\end{aligned}
$$

(b) Compute the values of the $\bar{T}^{i j}$ at the point which corresponds to $x^{1}=1, x^{2}=-2$.

For economy of effort, the problem will be worked using matrices.

(a) Writing

$$
J_{j}^{i} \equiv J_{i}^{\prime j} \equiv \frac{\partial \bar{x}^{i}}{\partial x^{j}}
$$

we have from $(2.1 b)$,

$$
\bar{T}^{i j}=T^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{i}}{\partial x^{s}}=J_{r}^{i} T^{r s} J_{j}^{s}
$$

That is,

$$
\begin{aligned}
\bar{T} & =J T J^{T} \\
& =\left[\begin{array}{cc}
2 x^{1} & 0 \\
x^{2} & x^{1}
\end{array}\right]\left[\begin{array}{rr}
1 & 1 \\
-1 & 2
\end{array}\right]\left[\begin{array}{cc}
2 x^{1} & x^{2} \\
0 & x^{1}
\end{array}\right]=\left[\begin{array}{cc}
4\left(x^{1}\right)^{2} & 2 x^{1} x^{2}+2\left(x^{1}\right)^{2} \\
2 x^{1} x^{2}-2\left(x^{1}\right)^{2} & 2\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}
\end{array}\right]
\end{aligned}
$$

(b) At the point $(1,-2)$,

$$
\begin{array}{ll}
\bar{T}^{11}=4(1)^{2}=4 & \bar{T}^{12}=2(1)(-2)+2(1)^{2}=-2 \\
\bar{T}^{21}=2(1)(-2)-2(1)^{2}=-6 & \bar{T}^{22}=2(1)^{2}+(-2)^{2}=6
\end{array}
$$

3.11 Show that if $\left(S^{i}\right)$ and $\left(T^{i}\right)$ are contravariant vectors on $\mathbf{R}^{n}$, the matrix $\left[U^{i j}\right] \equiv\left[S^{i} T^{j}\right]_{n n}$, defined in this manner for all coordinate systems, represents a contravariant tensor of order 2.

Multiply

to obtain

$$
\bar{S}^{i}=S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \quad \text { and } \quad \bar{T}^{j}=T^{s} \frac{\partial \bar{x}^{j}}{\partial x^{s}}
$$

$$
\bar{U}^{i j}=\bar{S}^{i} \bar{T}^{j}=S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \cdot T^{s} \frac{\partial \bar{x}^{j}}{\partial x^{s}}=U^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}}
$$

which is the tensor law. (The notion of the "outer product" of two tensors will be further developed in Chapter 4.)

\section*{SECOND-ORDER COVARIANT TENSORS}
3.12 Show that if $T_{i}$ are the components of covariant vector $\mathbf{T}$, then $S_{i j} \equiv T_{i} T_{j}-T_{j} T_{i}$ are the components of a skew-symmetric covariant tensor $\mathbf{S}$.

The skew-symmetry is obvious. From the transformation law for $\mathbf{T}$,

or

$$
\begin{gathered}
\bar{T}_{i} \bar{T}_{j}-\bar{T}_{j} \bar{T}_{i}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \cdot T_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}}-T_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \cdot T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \\
=T_{r} T_{s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}-T_{s} T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=\left(T_{r} T_{s}-T_{s} T_{r}\right) \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \\
\bar{S}_{i j}=S_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
\end{gathered}
$$

which establishes the covariant tensor character of $\mathbf{S}$.

3.13 If a symmetric array $\left(T_{i j}\right)$ transforms according to

$$
\bar{T}_{i j}=T_{r t} \frac{\partial x^{k}}{\partial \bar{x}^{s}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}} \frac{\partial \bar{x}^{r}}{\partial x^{k}}
$$

show that it defines a second-order covariant tensor.

$$
\begin{aligned}
\bar{T}_{i j} & =T_{r t}\left(\frac{\partial \bar{x}^{r}}{\partial x^{k}} \frac{\partial x^{k}}{\partial \bar{x}^{s}}\right) \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}=T_{r t} \delta_{s}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}} \\
& =T_{s t} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}=T_{t s} \frac{\partial x^{t}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
\end{aligned}
$$

3.14 Let $\mathbf{U}=\left(U_{i j}\right)$ be a covariant tensor of order 2 . Under the same coordinate change as in Problem 3.10, (a) calculate the components $\bar{U}_{i j}$, if $U_{11}=x^{2}, U_{12}=U_{21}=0, U_{22}=x^{1} ;(b)$ verify that the quantity $T^{i j} U_{i j}=E$ is an invariant, where the $T^{i j}$ and $\bar{T}^{i j}$ are obtained from Problem 3.10.

(a) In terms of the inverse Jacobian matrix, the covariant transformation law is

$$
\bar{U}_{i j}=\frac{\partial x^{r}}{\partial \bar{x}^{i}} U_{r s} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=\bar{J}_{i}^{r} U_{r s} \bar{J}_{j}^{s}=\bar{J}_{r}^{\prime i} U_{r s} \bar{J}_{j}^{s} \quad \text { or } \quad \bar{U}=\bar{J}^{T} U \bar{J}
$$

Substituting

$$
\bar{J}=\left[\begin{array}{cc}
2 x^{1} & 0 \\
x^{2} & x^{1}
\end{array}\right]^{-1}=\left[\begin{array}{rr}
\frac{1}{2 x^{1}} & 0 \\
-\frac{x^{2}}{2\left(x^{1}\right)^{2}} & \frac{1}{x^{1}}
\end{array}\right] \quad U=\left[\begin{array}{cc}
x^{2} & 0 \\
0 & x^{1}
\end{array}\right]
$$

we find

$$
\bar{U}=\left[\begin{array}{cc}
\frac{1}{2 x^{1}} & -\frac{x^{2}}{2\left(x^{1}\right)^{2}} \\
0 & \frac{1}{x^{1}}
\end{array}\right]\left[\begin{array}{cc}
x^{2} & 0 \\
0 & x^{1}
\end{array}\right]\left[\begin{array}{rr}
\frac{1}{2 x^{1}} & 0 \\
-\frac{x^{2}}{2\left(x^{1}\right)^{2}} & \frac{1}{x^{1}}
\end{array}\right]=\left[\begin{array}{cc}
\frac{x^{1} x^{2}+\left(x^{2}\right)^{2}}{4\left(x^{1}\right)^{3}} & -\frac{x^{2}}{2\left(x^{1}\right)^{2}} \\
-\frac{x^{2}}{2\left(x^{1}\right)^{2}} & \frac{1}{x^{1}}
\end{array}\right]
$$

from which the $\bar{U}_{i j}$ may be read off.

(b) Continuing in the matrix approach, we note that $E$ is the trace (sum of diagonal elements) of the matrix $T U^{T}$.

$$
\begin{aligned}
& T U^{T}=\left[\begin{array}{rr}
1 & 1 \\
-1 & 2
\end{array}\right]\left[\begin{array}{cc}
x^{2} & 0 \\
0 & x^{1}
\end{array}\right]=\left[\begin{array}{rr}
x^{2} & x^{1} \\
-x^{2} & 2 x^{1}
\end{array}\right] \\
& E=x^{2}+2 x^{1}
\end{aligned}
$$

and

$$
\begin{aligned}
& \bar{T} \bar{U}^{T}=\left[\begin{array}{cc}
4\left(x^{1}\right)^{2} & 2 x^{1} x^{2}+2\left(x^{1}\right)^{2} \\
2 x^{1} x^{2}-2\left(x^{1}\right)^{2} & 2\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}
\end{array}\right]\left[\begin{array}{cc}
\frac{x^{1} x^{2}+\left(x^{2}\right)^{2}}{4\left(x^{1}\right)^{3}} & -\frac{x^{2}}{2\left(x^{1}\right)^{2}} \\
-\frac{x^{2}}{2\left(x^{1}\right)^{2}} & \frac{1}{x^{1}}
\end{array}\right] \\
&=\left[\begin{array}{cc}
0 & 2 x^{1} \\
-\frac{3 x^{2}}{2} & x^{2}+2 x^{1}
\end{array}\right] \\
& \bar{E}=x^{2}+2 x^{1}=E
\end{aligned}
$$

3.15 Prove Theorem 3.2.

Observe first of all that if a covariant matrix (second-order tensor) $U$ has inverse $V$ in unbarred coordinates, then $\bar{U}$ has inverse $\bar{V}$ in barred coordinates; i.e., $(\bar{U})^{-1}=\overline{U^{-1}}$. Now, by Problem 3.14(a),

$$
\bar{U}=\bar{J}^{T} U \bar{J}
$$

Inverting both sides of this matrix equation, applying Problem 2.13 , and recalling that $J \bar{J}=I$, we obtain

$$
\overline{U^{-1}}=\bar{J}^{-1} U^{-1}\left(\bar{J}^{T}\right)^{-1}=J U^{-1} J^{T}
$$

which is the contravariant law for $U^{-1}$ [see Problem 3.10(a)].

\section*{MIXED TENSORS}
3.16 Compute the formulas for the tensor components $\left(\bar{T}_{j}^{i}\right)$ in polar coordinates in terms of $\left(T_{j}^{i}\right)$ in rectangular coordinates, if the tensor is symmetric in rectangular coordinates. (In contrast to Section 3.1, it is now the curvilinear coordinates that are barred.)

The general formula calls for the calculations

$$
\bar{T}_{j}^{i}=T_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=\frac{\partial \bar{x}^{i}}{\partial x^{r}} T_{s}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad\left(T_{j}^{i}=T_{i}^{j}\right)
$$

Using $(2.1 b)$, this may be written in matrix form as


\begin{equation*}
\bar{T}=J T J^{-1}=\bar{J}^{-1} T \bar{J} \tag{1}
\end{equation*}


where $T=\left[T_{j}^{i}\right]_{22}$ and where

$$
\bar{J}=\left[\begin{array}{cc}
\cos \theta & -r \sin \theta \\
\sin \theta & r \cos \theta
\end{array}\right]
$$

is the Jacobian matrix of the transformation from $(r, \theta)$ to $(x, y)$. Thus,

$$
\begin{aligned}
& \bar{T}=\left[\begin{array}{cc}
\cos \theta & \sin \theta \\
-\frac{\sin \theta}{r} & \frac{\cos \theta}{r}
\end{array}\right]\left[\begin{array}{cc}
T_{1}^{1} & T_{2}^{1} \\
T_{2}^{1} & T_{2}^{2}
\end{array}\right]\left[\begin{array}{cc}
\cos \theta & -r \sin \theta \\
\sin \theta & r \cos \theta
\end{array}\right] \\
& =\left[\begin{array}{cc}
\cos \theta & \sin \theta \\
-\frac{\sin \theta}{r} & \frac{\cos \theta}{r}
\end{array}\right]\left[\begin{array}{cc}
T_{1}^{1} \cos \theta+T_{2}^{1} \sin \theta & -r T_{1}^{1} \sin \theta+r T_{2}^{1} \cos \theta \\
T_{2}^{1} \cos \theta+T_{2}^{2} \sin \theta & -r T_{2}^{1} \sin \theta+r T_{2}^{2} \cos \theta
\end{array}\right]
\end{aligned}
$$

The final matrix multiplication can be carried out routinely, simplifying by means of trigonometric identities:

$$
\bar{T}=\left[\begin{array}{cc}
T_{1}^{1} \cos ^{2} \theta+T_{2}^{1} \sin 2 \theta+T_{2}^{2} \sin ^{2} \theta & -\frac{r}{2} T_{1}^{1} \sin 2 \theta+r T_{2}^{1} \cos 2 \theta+\frac{r}{2} T_{2}^{2} \sin 2 \theta \\
-T_{1}^{1} \frac{\sin 2 \theta}{2 r}+T_{2}^{1} \frac{\cos 2 \theta}{r}+T_{2}^{2} \frac{\sin 2 \theta}{2 r} & T_{1}^{1} \sin ^{2} \theta-T_{2}^{1} \sin 2 \theta+T_{2}^{2} \cos ^{2} \theta
\end{array}\right]
$$

Observe that $\bar{T}$ does not share the symmetry of $T: \quad \bar{T}_{1}^{2}=r^{-2} \bar{T}_{2}^{1}$.

3.17 Prove that the determinant of a mixed tensor of order two is invariant.

By (1) of Problem 3.16, we have-whether or not $T$ is symmetric-

$$
|\bar{T}|=\left|J T J^{-1}\right|=|J||T|\left|J^{-1}\right|=\mathscr{J}|T| \mathscr{J}^{-1}=|T|
$$

\section*{GENERAL TENSORS}
3.18 Display the transformation law for a third-order tensor that is contravariant of order two and covariant of order one.

Take $p=2$ and $q=1$ in Definition 7 and, to avoid unnecessary subscripts, write $i, j, k, r, s, t$ in place of $i_{1}, i_{2}, j_{1}, r_{1}, r_{2}, s_{1}$. Then (3.14) gives

$$
\bar{T}_{k}^{i j}=T_{t}^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}
$$

3.19 Let $\mathbf{T}=\left(T_{k l m}^{i j}\right)$ denote a tensor of the order and type indicated by the indices. Prove that $\mathbf{S}=\left(T_{k}\right) \equiv\left(T_{k i j}^{i j}\right)$ is a covariant vector.

The transformation law (3.14) for $\mathbf{T}$ is

$$
\bar{T}_{k l m}^{i j}=T_{t u v}^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \frac{\partial x^{u}}{\partial \bar{x}^{l}} \frac{\partial x^{v}}{\partial \bar{x}^{m}}
$$

Set $l=i, m=j$ and sum:

$$
\begin{aligned}
\bar{T}_{k} \equiv \bar{T}_{k i j}^{i j} & =T_{t u v}^{r s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \frac{\partial x^{u}}{\partial \bar{x}^{i}} \frac{\partial x^{v}}{\partial \bar{x}^{j}}=T_{t u v}^{r s}\left(\frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{u}}{\partial \bar{x}^{i}}\right)\left(\frac{\partial \bar{x}^{j}}{\partial x^{s}} \frac{\partial x^{v}}{\partial \bar{x}^{j}}\right) \frac{\partial x^{t}}{\partial \bar{x}^{k}} \\
& =T_{t u v}^{r s} \delta_{r}^{u} \delta_{s}^{v} \frac{\partial x^{t}}{\partial \bar{x}^{k}}=T_{t r s}^{r s} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \equiv T_{t} \frac{\partial x^{t}}{\partial \bar{x}^{k}}
\end{aligned}
$$

\section*{CARTESIAN TENSORS}
3.20 Show that the permutation symbol $\left(e_{i j}\right)$ defines a direct cartesian tensor over $\mathbf{R}^{2}$. Assume that $e_{i j}$ is defined the same way for all rectangular coordinate systems.

If the coordinate change is $\bar{x}_{i}=a_{i j} x_{j}$, where $\left(a_{i j}\right)^{T}\left(a_{k l}\right)=\left(\delta_{p q}\right)$ and

$$
\left|a_{i j}\right|=a_{11} a_{22}-a_{12} a_{21}=1
$$

we must establish the cartesian tensor law (3.22):

$$
\bar{e}_{i j}=e_{r s} a_{i r} a_{j s} \quad(n=2)
$$

We examine separately the four possible cases:

$$
\begin{aligned}
\boldsymbol{i}=\boldsymbol{j}=\mathbf{1} & e_{r s} a_{1 r} a_{1 s}=a_{11} a_{12}-a_{12} a_{11}=0=\bar{e}_{11} \\
\boldsymbol{i}=\mathbf{1}, \boldsymbol{j}=\mathbf{2} & e_{r s} a_{1 r} a_{2 s}=a_{11} a_{22}-a_{12} a_{21}=1=\bar{e}_{12} \\
\boldsymbol{i}=\mathbf{2}, \boldsymbol{j}=\mathbf{1} & e_{r s} a_{2 r} a_{1 s}=a_{21} a_{12}-a_{22} a_{11}=-1=\bar{e}_{21} \\
\boldsymbol{i}=\boldsymbol{j}=\mathbf{2} & e_{r s} a_{2 r} a_{2 s}=a_{21} a_{22}-a_{22} a_{21}=0=\bar{e}_{22}
\end{aligned}
$$

3.21 Prove that (a) the coefficients $c_{i j}$ of the quadratic form $c_{i j} x^{i} x^{j}=1$ transform as an affine tensor and $(b)$ the trace $c_{i i}$ of $\left(c_{i j}\right)$ is a cartesian invariant.

(a) If $\bar{x}^{i}=a_{j}^{i} x^{j}$ and $x^{i}=b_{j}^{i} \bar{x}^{j}$, where $\left(b_{j}^{i}\right)=\left(a_{j}^{i}\right)^{-1}$, the quadratic form goes over into

$$
1=c_{i j}\left(b_{r}^{i} \bar{x}^{r}\right)\left(b_{s}^{j} \bar{x}^{s}\right) \equiv \bar{c}_{r s} \bar{x}^{r} \bar{x}^{s}
$$

with $\bar{c}_{r s}=b_{r}^{i} b_{s}^{j} c_{i j}$. But this formula is just (3.21) for a covariant affine tensor of order two.

(b) Assuming an orthogonal transformation, $\left(b_{j}^{i}\right)=\left(a_{j}^{i}\right)^{T}$, we have

$$
\bar{c}_{r s}=b_{r}^{i} a_{j}^{s} c_{i j}
$$

Hence, $\bar{c}_{r r}=\left(b_{r}^{i} a_{j}^{r}\right) c_{i j}=\delta_{j}^{i} c_{i j}=c_{i i}$.

3.22 Establish the identity between the permutation symbol and the Kronecker delta:


\begin{equation*}
e_{r i j} e_{r k l} \equiv \delta_{i k} \delta_{j l}-\delta_{i l} \delta_{j k} \tag{3.23}
\end{equation*}


The identity implies $n=3$, so that there are potentially $3^{4}=81$ separate cases to consider. However, this number can be quickly reduced to only 4 cases by the following reasoning: If either $i=j$ or $k=l$, then both sides vanish. For example, if $i=j$, then on the left $e_{r i j}=0$, and on the right,

$$
\delta_{i k} \delta_{j t}-\delta_{j l} \delta_{i k}=0
$$

Hence, we need only consider the cases in which both $i \neq j$ and $k \neq l$. Upon writing out the sum on the left, two of the terms drop out, since $i \neq j$ :

$$
e_{1 i j} e_{1 k l}+e_{2 i j} e_{2 k l}+e_{3 i j} e_{3 k l}=e_{1^{\prime} 2^{\prime} 3^{\prime}} e_{1^{\prime} k l} \quad\left(i=2^{\prime}, j=3^{\prime}\right)
$$

where $\left(1^{\prime} 2^{\prime} 3^{\prime}\right)$ denotes some permutation of (123). Thus, there are left only two cases, each with two subcases.

Case 1: $\quad e_{1^{\prime} 2^{\prime} 3^{\prime}}, e_{1^{\prime} k l} \neq 0$ (with $i=2^{\prime}, j=3^{\prime}$ ). Here, either $k=2^{\prime}$ and $l=3^{\prime}$ or $k=3^{\prime}$ and $l=2^{\prime}$. If the former, then the left member of (3.23) is +1 , while the right member equals

$$
\delta_{2^{\prime} 2^{\prime}} \delta_{3^{\prime} 3^{\prime}}-\delta_{2^{\prime} 3^{\prime}} \delta_{3^{\prime} 2^{\prime}}=1-0=1
$$

If the latter, then both members equal -1 , as can be easily verified.

Case 2: $\quad e_{1^{\prime} 2^{\prime} 3^{\prime}}, e_{1^{\prime} k l}=0$ (with $i=2^{\prime}, j=3^{\prime}$ ). Since $k \neq l$, either $k=1^{\prime}$ or $l=1^{\prime}$. If $k=1^{\prime}$, then the right member of (3.23) equals

$$
\delta_{2^{\prime} 1^{\prime}} \delta_{3^{\prime} l}-\delta_{2^{\prime} l^{\prime}} \delta_{3^{\prime} 1^{\prime}}=0-0=0
$$

If $l=1^{\prime}$, we have $\delta_{2^{\prime} k} \delta_{3^{\prime} 1^{\prime}}-\delta_{2^{\prime} 1^{\prime}} \delta_{3^{\prime} k}=0-0=0$.

This completes the examination of all cases, and the identity is established.

\section*{Supplementary Problems}
3.23 Suppose that the following transformation connects the $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$ coordinate systems:

$$
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{1}=\exp \left(x^{1}+x^{2}\right) \\
\bar{x}^{2}=\exp \left(x^{1}-x^{2}\right)
\end{array}\right.
$$

(a) Calculate the Jacobian matrix $J$ and the Jacobian $\mathscr{J}$. Show that $\mathscr{J} \neq 0$ over all of $\mathbf{R}^{2}$. (b) Give equations for $\mathscr{T}^{-1}$. (c) Calculate the Jacobian matrix $\bar{J}$ of $\mathscr{T}^{-1}$ and compare with $J^{-1}$.

3.24 Prove that if $\left(T_{i}\right)$ defines a covariant vector, and if the components $S_{i j} \equiv T_{i} T_{j}+T_{j} T_{i}$ are defined in each coordinate system, then $\left(S_{i j}\right)$ is a symmetric covariant tensor. (Compare Problem 3.12.)

3.25 Prove that if $\left(T_{i}\right)$ defines a covariant vector and, in each coordinate system, we define

$$
\frac{\partial T_{i}}{\partial x^{j}}-\frac{\partial T_{j}}{\partial x^{i}}=T_{i j}
$$

then $\left(T_{i j}\right)$ is a skew-symmetric covariant tensor of the second order. [Hint: Model the proof on Problem 3.3.]

3.26 Convert the partial differential equation

$$
y \frac{\partial f}{\partial x}=x \frac{\partial f}{\partial y}
$$

to polar form (making use of the fact that $\nabla f$ is a covariant vector), and solve for $f(x, y)$.

3.27 Show that the quadratic form $Q=g_{i j} x^{i} x^{j}$ is an affine invariant provided $\left(g_{i j}\right)$ is a covariant affine tensor. [Converse of Problem 3.21(a).]

3.28 Prove that the partial derivatives of a contravariant vector $\left(T^{i}\right)$ define a mixed affine tensor of order two. [Hint: Compare Problem 2.23.]

3.29 Prove that the Kronecker delta $\left(\delta_{j}^{i}\right)$, uniformly defined in all coordinate systems, is a mixed tensor of order two.

3.30 Show that the permutation symbol $\left(e_{i j}\right)$ of order two, uniformly defined in all coordinate systems, is not-Problem 3.20 notwithstanding-covariant under arbitrary coordinate changes. [Hint: Use $x^{1}=$ $\bar{x}^{1} \bar{x}^{2}, x^{2}=\bar{x}^{2}$, at the point $\left.\left(\bar{x}^{i}\right)=(1,2).\right]$

3.31 By use of (3.23), establish the familiar identity for the vector product of three vectors,

$$
\mathbf{u} \times(\mathbf{v} \times \mathbf{w})=(\mathbf{u w}) \mathbf{v}-(\mathbf{u v}) \mathbf{w}
$$

or, in coordinate form,

$$
e_{i j k} u_{j}\left(e_{k r s} v_{r} w_{s}\right)=\left(u_{j} w_{j}\right) v_{i}-\left(u_{j} v_{j}\right) w_{i}
$$

3.32 (a) Show that if $\left(T_{j}^{i}\right)$ is a mixed tensor, then $\left(T_{j}^{i}+T_{i}^{j}\right)$ is not generally a tensor. (b) Show that a mixed tensor of order two, symmetric in a given coordinate system, will transform as a symmetric tensor if the Jacobian matrix is orthogonal.

3.33 Prove: (a) If $\left(T_{j}^{i}\right)$ is a mixed tensor of order two, $T_{i}^{i}$ is an invariant; $(b)$ if $\left(S_{j k}^{i}\right)$ and $\left(T^{i}\right)$ are tensors of the type and order indicated, $S_{j r}^{r} T^{j}$ is an invariant.

3.34 If $\mathbf{T} \equiv\left(T_{m l}^{i j k}\right)$ is a tensor, contravariant of order 3 and covariant of order 2 , show that $\mathbf{S} \equiv\left(T_{k j}^{i j k}\right)$ is a contravariant vector.

3.35 Show that the derivative, $d \mathbf{T} / d t$, of the tangent vector $\mathbf{T} \equiv\left(T^{i}\right)=\left(d x^{i} / d t\right)$ to a curve $x^{i}=x^{i}(t)$ is a contravariant affine tensor. Is it a cartesian tensor?

3.36 (a) Use the theory of tensors to prove that the scalar product $\mathbf{u v} \equiv u_{i} v_{i}$ of two vectors $\mathbf{u}=\left(u_{i}\right)$ and $\mathbf{v}=\left(v_{i}\right)$ is a cartesian invariant. (b) Is uv an affine invariant?

\section*{Chapter 4}
\section*{Tensor Operations; Tests for Tensor Character}
\subsection*{4.1 FUNDAMENTAL OPERATIONS}
From two given tensors,


\begin{equation*}
\mathbf{S}=\left(S_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right) \quad \mathbf{T}=\left(T_{l_{1} l_{2} \ldots l_{s}}^{k_{1} k_{2} \ldots k_{r}}\right) \tag{4.1}
\end{equation*}


certain operations, to be described, will produce a third tensor.

\section*{Sums, Linear Combinations}
Let $p=r$ and $q=s$ in (4.1). Since the transformation law (3.14) is linear in the tensor components, it is clear that


\begin{equation*}
\mathbf{S}+\mathbf{T} \equiv\left(S_{j_{1} j_{2} \ldots j_{s}}^{i_{1} i_{2} \ldots i_{r}}+T_{j_{1} j_{2} \ldots j_{s}}^{i_{1} i_{2} \ldots i_{r}}\right) \tag{4.2a}
\end{equation*}


is a tensor of the same type and order as the original two tensors. More generally, if $\mathbf{T}_{1}, \mathbf{T}_{2}, \ldots, \mathbf{T}_{\mu}$ are tensors of the same type and order and if $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{\mu}$ are invariant scalars, then


\begin{equation*}
\lambda_{1} \mathbf{T}_{1}+\lambda_{2} \mathbf{T}_{2}+\cdots+\lambda_{\mu} \mathbf{T}_{\mu} \tag{4.2b}
\end{equation*}


is a tensor of that same type and order.

\section*{Outer Product}
The outer product of the tensors $\mathbf{S}$ and $\mathbf{T}$ of (4.1) is the tensor


\begin{equation*}
[\mathbf{S T}] \equiv\left(S_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}} \cdot T_{l_{1} l_{2} \ldots i_{s}}^{k_{1} k_{2} \ldots k_{r}}\right) \tag{4.3}
\end{equation*}


which is of order $m=p+q+r+s$ (the sum of the orders of $\mathbf{S}$ and $\mathbf{T}$ ), contravariant of order $p+r$ and covariant of order $q+s$. Note that $[\mathbf{S T}]=[\mathbf{T S}]$.

EXAMPLE 4.1 Given two tensors, $\mathbf{S}=\left(S_{j}^{i}\right)$ and $\mathbf{T}=\left(T_{k}\right)$, the outer product $[\mathbf{S T}]=\left(S_{j}^{i} T_{k}\right) \equiv\left(P_{j k}^{i}\right)$ is a tensor because

$$
\bar{P}_{j k}^{i} \equiv \bar{S}_{j}^{i} \bar{T}_{k}=\left(S_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}\right)\left(T_{u} \frac{\partial x^{u}}{\partial \bar{x}^{k}}\right)=P_{s u}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{u}}{\partial \bar{x}^{k}}
$$

\section*{Inner Product}
To take the inner product of two tensors, one equates an upper (contravariant) index of one tensor to a lower (covariant) index of the other, and sums products of components over the repeated index. In effect, the contravariant and covariant behaviors cancel out, which lowers the total order of the two tensors.

To state this more formally, set $i_{\alpha}=u=l_{\beta}$ in (4.1). Then the inner product corresponding to this pair of indices is


\begin{equation*}
\mathbf{S T} \equiv\left(S_{j_{1} j_{2} \ldots j_{q}}^{i_{1} \ldots i_{p}} T_{l_{1} \ldots u \ldots l_{s}}^{k_{1} k_{2} \ldots k_{r}}\right) \tag{4.4}
\end{equation*}


It is seen that there will exist $p s+r q$ inner products ST and TS; in general, all of these will be distinct. Each will be a tensor of order

$$
m=p+q+r+s-2
$$

EXAMPLE 4.2 From the tensors $\mathbf{S}=\left(S^{i j}\right)$ and $\mathbf{T}=\left(T_{k l m}\right)$, form the inner product $\mathbf{U}=\left(U_{k m}^{j}\right) \equiv\left(S^{u j} T_{k u m}\right)$. We have:

$$
\begin{aligned}
\bar{U}_{k m}^{j} & =\left(S^{p r} \frac{\partial \bar{x}^{u}}{\partial x^{p}} \frac{\partial \bar{x}^{j}}{\partial x^{r}}\right)\left(T_{s q t} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{q}}{\partial \bar{x}^{u}} \frac{\partial x^{t}}{\partial \bar{x}^{m}}\right) \\
& =S^{r} T_{s q t}\left(\frac{\partial \bar{x}^{u}}{\partial x^{p}} \frac{\partial x^{q}}{\partial \bar{x}^{u}}\right) \frac{\partial \bar{x}^{j}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{m}}=S^{p r} T_{s q t} \delta_{p}^{q} \frac{\partial \bar{x}^{j}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{m}} \\
& =S^{p r} T_{s p t} \frac{\partial \bar{x}^{j}}{\partial \bar{x}^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{m}} \equiv U_{s t}^{r} \frac{\partial \bar{x}^{j}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{m}}
\end{aligned}
$$

which verifies that $\mathbf{U}$ is a tensor of order 3 , contravariant of order 1 and covariant of order 2 .

EXAMPLE 4.3 With $\left(T_{i j}\right)$ and $\left(T^{i j}\right)$ as in Theorem 3.2,

$$
T^{i u} T_{u j}=\delta_{j}^{i}
$$

As an inner product, the left side defines a second-order tensor that is contravariant of order one and covariant of order one. This constitutes a new proof (cf. Problem 3.29) of the tensor nature of the Kronecker delta.

In the special case when $\mathbf{S}$ is a contravariant vector and $\mathbf{T}$ is a covariant vector, the inner product ST is of the form $S^{i} T_{i}$, which is an invariant (Theorem 3.1). Because the tensor ST is of order

$$
m=p+q+r+s-2=1+0+0+1-2=0
$$

an invariant is regarded as a tensor of order zero.

\section*{Contraction}
Another order-reducing operation, like the inner product but applying to single tensors, is that of contracting a tensor on a pair of indices. In tensor $\mathbf{S}$ of (4.1) set $i_{\alpha}=u=j_{\beta}$ and sum on $u$; the resulting tensor (Problem 4.7),


\begin{equation*}
\mathbf{S}^{\prime}=\left(S_{j_{1} \ldots u \ldots i_{p}}^{i_{1} \ldots u \ldots i_{p}}\right) \tag{4.5}
\end{equation*}


is called a contraction of $\mathbf{S}$, with contraction indices $i_{\alpha}$ and $j_{\beta}$. $\mathbf{S}^{\prime}$ is contravariant of order $p-1$ and covariant of order $q-1$.

\section*{Combined Operations}
It is clear that one may form new tensors from old in a variety of ways by performing a sequence of the tensor operations discussed above. For example, one might form the outer product of two tensors, then take an inner product of this with a third tensor; or contract on one or more pairs of indices, either before or after taking a product. It is noteworthy that an inner product of two tensors may be characterized as a contraction of their outer product: $\mathbf{S T}=[\mathbf{S T}]^{\prime}$. See Fig. 4-1.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-053}
\end{center}

Fig. 4-1

\subsection*{4.2 TESTS FOR TENSOR CHARACTER}
It is useful to have an alternative method for verifying tensor character that does not directly appeal to the tensor transformation laws. Roughly stated, the principle is this: If it can be shown that the inner product $\mathbf{T V}$ is a tensor for all vectors $\mathbf{V}$, then $\mathbf{T}$ is a tensor. This idea is often referred to as the Quotient Rule for tensors; the official Quotient Theorem is our Theorem 4.2 below.

The following statements are useful criteria or "tests" for tensor character; they may all be derived as special cases of the Quotient Theorem.

(1) If $T_{i} V^{i} \equiv E$ is invariant for all contravariant vectors $\left(V^{i}\right)$, then $\left(T_{i}\right)$ is a covariant vector (tensor of order 1).

(2) If $T_{i j} V^{i} \equiv U_{j}$ are components of a covariant vector for all contravariant vectors $\left(V^{i}\right)$, then $\left(T_{i j}\right)$ is a covariant tensor of order 2.

(3) If $T_{i j} U^{i} V^{j} \equiv E$ is invariant for all contravariant vectors $\left(U^{i}\right)$ and $\left(V^{i}\right)$, then $\left(T_{i j}\right)$ is a covariant tensor of order 2.

(4) If $\left(T_{i j}\right)$ is symmetric and $T_{i j} V^{i} V^{j} \equiv E$ is invariant for all contravariant vectors $\left(V^{i}\right)$, then $\left(T_{i j}\right)$ is a covariant tensor of order 2.

EXAMPLE 4.4 Establish criterion (1).

Since $E$ is invariant, $\bar{E}=E$, or $\bar{T}_{i} \bar{V}^{i}=T_{i} V^{i}$. Substitute in this equation the transformation law for $\left(V^{i}\right)$ and change the dummy index on the right:

$$
\bar{T}_{i}\left(V^{j} \frac{\partial \bar{x}^{i}}{\partial x^{j}}\right)=T_{j} V^{j} \quad \text { or } \quad\left(T_{j}-\bar{T}_{i} \frac{\partial \bar{x}^{i}}{\partial x^{j}}\right) V^{j}=0
$$

The latter equation must hold when $\left(V^{i}\right)$ is any of the contravariant vectors represented in $\left(x^{i}\right)$ by $\left(\delta_{1}^{i}\right),\left(\delta_{2}^{i}\right), \ldots,\left(\delta_{n}^{i}\right)$; their existence is guaranteed by Problem 3.5. Thus, for the $k$ th of these vectors $(1 \leqq k \leqq n)$,

$$
\left(T_{k}-\bar{T}_{i} \frac{\partial \bar{x}^{i}}{\partial x^{k}}\right) \cdot 1=0 \quad \text { or } \quad T_{k}=\bar{T}_{i} \frac{\partial \bar{x}^{i}}{\partial x^{k}}
$$

which is the law of transformation-from $\left(\bar{x}^{i}\right)$ to $\left(x^{i}\right)$-of a covariant vector.

The method of Example 4.4 may be easily extended to establish the following result, which in turn implies the Quotient Theorem.

Lemma 4.1: If $T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \cdots V_{(q)}^{j_{q}} \equiv E$ is an invariant for arbitrary covariant vectors $\left(U_{i_{\alpha}}^{(\alpha)}\right) \equiv \mathbf{U}^{(\alpha)} \quad(\alpha=1,2, \ldots, p)$ and arbitrary contravariant vectors $\left(V_{(\beta)}^{j_{\beta}}\right) \equiv \mathbf{V}_{(\beta)} \quad(\beta=1,2, \ldots, q)$, then $\left(T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right)$ is a tensor of the type indicated by its indices.

Theorem 4.2 (Quotient Theorem): If $T_{j_{1} j_{2} \ldots j_{q} k}^{i_{1} i_{2} \ldots i_{p}} V^{k} \equiv S_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}$ are components of a tensor for an arbitrary contravariant vector $\left(V^{k}\right)$, then $\left(T_{j_{1} j_{2} \ldots j_{q} j_{q+1}}^{i_{1} i_{2} \ldots i_{p}}\right)$ is a tensor of the type and order indicated.

\subsection*{4.3 TENSOR EQUATIONS}
Much of the importance of tensors in mathematical physics and engineering resides in the fact that if a tensor equation or identity is true in one coordinate system, then it is true in all coordinate systems.

EXAMPLE 4.5 Suppose that in some special coordinate system, $\left(x^{i}\right)$, the covariant tensor $\mathbf{T}=\left(T_{i j}\right)$ vanishes. The components of $\mathbf{T}$ in any other coordinate system, $\left(\bar{x}^{i}\right)$, are given by

$$
\bar{T}_{i j}=T_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=0+0+\cdots+0=0
$$

Therefore, $\mathbf{T}=\mathbf{0}$ in every coordinate system.

EXAMPLE 4.6 Consider a putative equation


\begin{equation*}
R_{i j k} U^{k}=A W_{i}^{k l} M_{j k} I_{l} \tag{1}
\end{equation*}


connecting six entities that may or may not be tensors. If it can be shown that (i) $\mathbf{T}=\left(T_{i j}\right) \equiv\left(R_{i j k} U^{k}-\right.$ $A W_{i}^{k l} M_{i k} U_{l}$ ) is a tensor, and (ii) a special coordinate system exists in which all $T_{i j}$ are zero, then (1) is valid in every coordinate system.

EXAMPLE 4.7 A second-order covariant tensor, or a second-order contravariant tensor, that is known to be symmetric in one coordinate system must be symmetric in every coordinate system. (This statement does not extend to a second-order mixed tensor; see Problem 3.16.)

Another application of the principle yields (Problem 4.15) a useful fact in tensor analysis, often taken for granted:

Theorem 4.3: If $\left(T_{i j}\right)$ is a covariant tensor of order two whose determinant vanishes in one particular coordinate system, then its determinant vanishes in all coordinate systems.

Corollary 4.4: A covariant tensor of order two that is invertible in one coordinate system is invertible in all coordinate systems.

\section*{Solved Problems}
\section*{TENSOR SUMS}
4.1 Show that if $\lambda$ and $\mu$ are invariants and $S^{i}$ and $T^{i}$ are components of contravariant vectors, the vector defined in all coordinate systems by $\left(\lambda S^{i}+\mu T^{i}\right)$ is a contravariant vector.

Since $\bar{\lambda}=\lambda$ and $\bar{\mu}=\mu$,

as desired.

$$
\bar{\lambda} \bar{S}^{i}+\bar{\mu} \bar{T}^{i}=\lambda\left(S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)+\mu\left(T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)=\left(\lambda S^{r}+\mu T^{r}\right) \frac{\partial \bar{x}^{i}}{\partial x^{r}}
$$

4.2 Prove that (a) the array defined in each coordinate system by $\left(T_{i j}-T_{j i}\right)$, where $\left(T_{i j}\right)$ is a given covariant tensor, is a covariant tensor; $(b)$ the array defined in each coordinate system by $\left(T_{j}^{i}-T_{i}^{j}\right)$, where $\left(T_{j}^{i}\right)$ is a given mixed tensor, is not generally a tensor, but is a cartesian tensor.

(a) By (4.2b), the array is a tensor if and only if $\left(T_{i j}^{*}\right) \equiv\left(T_{j i}\right)$ is a covariant tensor. But the transformation law for $\left(T_{i j}\right)$ gives

$$
\bar{T}_{j i}=T_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \quad \text { or } \quad \bar{T}_{i j}^{*}=T_{s r}^{*} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \frac{\partial x^{r}}{\partial \bar{x}^{j}}
$$

which shows that $\left(T_{i j}^{*}\right)$ is indeed a covariant tensor.

(b) We give a second proof [recall Problem 3.32(a)], based on (4.2b). The question is whether $\left(U_{j}^{i}\right) \equiv\left(T_{i}^{j}\right)$ is a tensor. From the transformation law for $\left(T_{j}^{i}\right)$,

$$
\bar{T}_{i}^{j}=T_{s}^{r} \frac{\partial \bar{x}^{j}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \quad \text { or } \quad \bar{U}_{j}^{i}=U_{r}^{s} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \frac{\partial \bar{x}^{j}}{\partial x^{r}}
$$

Thus, $\left(U_{i}^{i}\right)$ does not obey a tensor law, unless, for all $p, q$,

$$
\frac{\partial \bar{x}^{p}}{\partial x^{q}}=\frac{\partial x^{q}}{\partial \bar{x}^{p}} \quad \text { or } \quad J=\left(J^{-1}\right)^{T}
$$

i.e., unless the Jacobian matrix is orthogonal-as it is for orthogonal linear transformations (cartesian tensors).

\section*{OUTER PRODUCT}
4.3 Show that the outer product of two contravariant vectors is a contravariant tensor of order two.

With $\left(S^{i}\right)$ and $\left(T^{i}\right)$ as the given vectors,

$$
\bar{S}^{i} \bar{T}^{j}=\left(S^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)\left(T^{s} \frac{\partial \bar{x}^{j}}{\partial x^{s}}\right)=S^{r} T^{s} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial \bar{x}^{j}}{\partial x^{s}}
$$

which is the correct transformation law for the outer product to be a contravariant tensor of order two.

\section*{INNER PRODUCT}
4.4 Prove that the inner product $\left(T^{r} U_{i r}\right)$ is a tensor if $\left(T^{i}\right)$ and $\left(U_{i j}\right)$ are tensors of the types indicated.

With $V_{j} \equiv T^{i} U_{j i}$,

$$
\bar{V}_{j}=\left(T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)\left(U_{s t} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}\right)=\left(T^{r} U_{s t} \delta_{r}^{t}\right) \frac{\partial x^{s}}{\partial \bar{x}^{j}}=V_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

which is the desired transformation law.

4.5 Prove that if $\mathbf{g}=\left(g_{i j}\right)$ is a covariant tensor of order two, and $\mathbf{U}=\left(U^{i}\right)$ and $\mathbf{V}=\left(V^{i}\right)$ are contravariant vectors, then the double inner product $\mathbf{g U V}=g_{i j} U^{i} V^{j}$ is an invariant.

The transformation laws are

$$
\bar{g}_{i j}=g_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad \bar{U}^{i}=U^{t} \frac{\partial \bar{x}^{i}}{\partial x^{i}} \quad \bar{V}^{j}=V^{u} \frac{\partial \bar{x}^{j}}{\partial x^{u}}
$$

Multiply, and sum over $i$ and $j$ :

$$
\overline{\mathbf{g}} \overline{\mathbf{U}} \overline{\mathbf{V}}=\bar{g}_{i j} \bar{U}^{i} \bar{V}^{j}=g_{r s} U^{t} V^{u} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial \bar{x}^{i}}{\partial x^{t}} \frac{\partial \bar{x}^{j}}{\partial x^{u}}=g_{r s} U^{t} V^{u} \delta_{t}^{r} \delta_{u}^{s}=g_{r s} U^{r} V^{s}=\mathbf{g} \mathbf{U} \mathbf{V}
$$

\section*{CONTRACTION}
4.6 Assuming that contraction of a tensor yields a tensor, how many tensors may be created by repeated contraction of the tensor $\mathbf{T}=\left(T_{k l}^{i j}\right)$ ?

Single contraction produces the four mixed tensors

$$
\left(T_{u l}^{u j}\right) \quad\left(T_{k u}^{u j}\right) \quad\left(T_{u l}^{i u}\right) \quad\left(T_{k u}^{i u}\right)
$$

and double contraction produces the two zero-order tensors (invariants) $T_{u v}^{u v}$ and $T_{v u}^{u v}$. Thus there are six tensors, in general all distinct.

4.7 Show that any contraction of the tensor $\mathbf{T}=\left(T_{j k}^{i}\right)$ results in a covariant vector.

We may contract on either $i=j$ or $i=k$. For $\left(S_{k}\right) \equiv\left(T_{i k}^{i}\right)$, we have the transformation law

$$
\bar{S}_{k} \equiv \bar{T}_{i k}^{i}=T_{s t}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}=T_{s t}^{r} \delta_{r}^{s} \frac{\partial x^{t}}{\partial \bar{x}^{k}}=T_{r t}^{r} \frac{\partial x^{t}}{\partial \bar{x}^{k}}=S_{t} \frac{\partial x^{t}}{\partial \bar{x}^{k}}
$$

and, for $\left(U_{j}\right) \equiv\left(T_{j i}^{i}\right)$,

$$
\bar{U}_{j} \equiv \bar{T}_{j i}^{i}=T_{s t}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}=T_{s t}^{r} \delta_{r}^{t} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=T_{s r}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=U_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

In either case, the transformation law is that of a covariant vector.

\section*{COMBINED OPERATIONS}
4.8 Suppose that $\mathbf{S}=\left(S_{k}^{i j}\right)$ and $\mathbf{T}=\left(T_{j}^{i}\right)$ are tensors from which a contravariant vector $\mathbf{V}=\left(V^{i}\right)$ is to be constructed using a combination of outer/inner products and contractions. (a) Show that there are six possibilities for $\mathbf{V}$, which can all be distinct. (b) Verify that each possible $\mathbf{V}$ is obtainable as a contraction of an inner product ST.

(a) Writing $[\mathbf{S T}] \equiv \mathbf{U}=\left(U_{l m}^{i j k}\right)$, we obtain the contravariant vectors as the double contractions of $\mathbf{U}$ :

$$
\begin{array}{llllll}
\left(U_{u v}^{u v k}\right) & \left(U_{v u}^{u v k}\right) & \left(U_{u v}^{u j v}\right) & \left(U_{v u}^{u j v}\right) & \left(U_{u v}^{i u v}\right) & \left(U_{v u}^{i u v}\right)
\end{array}
$$

(b) The vector $\left(U_{u v}^{u v k}\right) \equiv\left(S_{u}^{u v} T_{v}^{k}\right)$ may be obtained by first taking the inner product $\left(S_{l}^{i v} T_{v}^{k}\right)$ and then contracting on $i=u=l$. Likewise for the other five vectors of $(a)$.

\section*{TESTS FOR TENSOR CHARACTER}
4.9 Prove criterion (2) of Section 4.2 without invoking the Quotient Theorem.

We are to verify that $\left(T_{i j}\right)$ is a covariant tensor of order two if it is given that for every contravariant vector $\left(V^{i}\right), T_{i j} V^{i} \equiv U_{j}$ are components of a covariant vector. Start out with the transformation law for $\left(U_{j}\right)$ [from $\left(x^{i}\right)$ to $\left.\left(\bar{x}^{i}\right)\right]$ :

$$
\bar{U}_{j}=U_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad \text { or } \quad \bar{T}_{i j} \bar{V}^{i}=T_{i s} V^{i} \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

Now substitute the transformation law for $\bar{V}^{i}$ [from $\left(\bar{x}^{i}\right)$ to $\left.\left(x^{i}\right)\right]$ :

$$
\bar{T}_{i j} \bar{V}^{i}=T_{i s}\left(\overline{\bar{V}}^{p} \frac{\partial x^{i}}{\partial \bar{x}^{p}}\right) \frac{\partial x^{s}}{\partial \bar{x}^{j}}
$$

Replace the dummy index $i$ by $p$ on the left and by $r$ on the right:

$$
\bar{T}_{p j} \bar{V}^{p}=T_{r s} \bar{V}^{p} \frac{\partial x^{r}}{\partial \bar{x}^{p}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \quad \text { or } \quad\left(\bar{T}_{p j}-T_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{p}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}\right) \bar{V}^{p}=0
$$

The proof is concluded as in Example 4.4.

4.10 Prove criterion (3) of Section 4.2.

Here we must show that $\left(T_{i j}\right)$ is a covariant tensor, assuming that $T_{i j} U^{i} V^{j}$ is invariant. Using criterion (1), we conclude that $\left(T_{i j} U^{i}\right)$ is a covariant vector. Using criterion (2), it follows that since $\left(U^{i}\right)$ is arbitrary, $\left(T_{i j}\right)$ is a covariant tensor of order two, the desired conclusion.

4.11 Prove criterion (4) of Section 4.2.

We wish to show that if $\left(T_{i j}\right)$ is a symmetric array such that $T_{i j} V^{i} V^{j}$ is an invariant for every contravariant vector $\left(V^{i}\right)$, then $\left(T_{i j}\right)$ is a (symmetric) covariant tensor of order two.

Let $\left(U^{i}\right)$ and $\left(V^{i}\right)$ denote arbitrary contravariant vectors and let $\left(W^{i}\right) \equiv\left(U^{i}+V^{i}\right)$, a contravariant vector by $(4.2 a)$. Then,

$$
\begin{aligned}
T_{i j} W^{i} W^{j} & \equiv T_{i j}\left(U^{i}+V^{i}\right)\left(U^{j}+V^{j}\right) \\
& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} U^{j}+T_{i j} U^{i} V^{j}+T_{i j} V^{i} V^{j} \\
& =T_{i j} U^{i} U^{j}+T_{i j} V^{i} V^{j}+2 T_{i j} U^{i} V^{j}
\end{aligned}
$$

where the symmetry of $\left(T_{i j}\right)$ has been used in the last step. Now, by hypothesis, the left-hand side and the first two terms of the right-hand side of the above identity are invariants. Therefore, $T_{i j} U^{i} V^{j}$ must be an invariant, and the desired conclusion follows from criterion (3).

4.12 Use Lemma 4.1 to write a proof of the Quotient Theorem, Theorem 4.2.

In the notation of the theorem and lemma, $S_{j_{1} j_{2} \cdots j_{q}}^{i_{1} i_{2} \ldots i_{p}} \cdot U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \cdots V_{(q)}^{j_{q}}$ is a tensor of order zero, or an invariant, for arbitrary $\mathbf{U}^{(\alpha)}$ and $\mathbf{V}_{(\beta)}$; that is,

$$
T_{j_{1} j_{2} \cdots j_{q} k}^{i_{1} i_{2} \cdots i_{p}} U_{i_{1}}^{(1)} U_{i_{2}}^{(2)} \cdots U_{i_{p}}^{(p)} V_{(1)}^{j_{1}} V_{(2)}^{j_{2}} \cdots V_{(q)}^{j_{q}} V^{k}
$$

is an invariant, with $\left(V^{k}\right)$ also arbitrary. It then follows from Lemma 4.1 (with $q$ replaced by $q+1$ ) that $\left(T_{j_{1} j_{2} \ldots j_{q^{k}}}^{i_{1} i_{2} \ldots i_{p}}\right)$ is a tensor, contravariant of order $p$ and covariant of order $q+1$.

From the above method of proof, it is clear that the Quotient Theorem is equally valid when the "divisor" is an arbitrary covariant vector. This form of the theorem will be used in Problem 4.13.

4.13 Use the Quotient Theorem to prove Theorem 3.2.

If $\mathbf{U}=\left(U^{i}\right)$ is a contravariant vector, the inner product

$$
\mathbf{V}=\mathbf{T} \mathbf{U} \equiv\left(T_{i j} U^{j}\right)
$$

is a covariant vector. Moreover, because $\left[T_{i j}\right]_{n n}$ has an inverse, it follows that as $\mathbf{U}$ runs through all contravariant vectors, $\mathbf{V}$ runs through all covariant vectors. Thus,

$$
\mathbf{U}=\mathbf{T}^{-1} \mathbf{V} \equiv\left(T^{i l} V_{j}\right)
$$

is a tensor for an arbitrary $\left(V_{i}\right)$, making $\left(T^{i j}\right)$ a contravariant tensor of order two.

\section*{TENSOR EQUATIONS}
4.14 Prove that if $\left(T_{j k l}^{i}\right)$ is a tensor such that, in the $\left(x^{i}\right)$-system, $T_{j k l}^{i}=3 T_{l j k}^{i}$, then $T_{j k l}^{i}=3 T_{l j k}^{i}$ in all coordinate systems.

We must prove that $\bar{T}_{j k l}^{i}=3 \bar{T}_{l j k}^{i}$ in $\left(\bar{x}^{i}\right)$. But

$$
\begin{aligned}
\bar{T}_{j k l}^{i}-3 \bar{T}_{l j k}^{i} & =T_{r s t}^{p} \frac{\partial \bar{x}^{i}}{\partial x^{p}} \frac{\partial x^{r}}{\partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}-3 T_{r s t}^{p} \frac{\partial \bar{x}^{i}}{\partial x^{p}} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \\
& =T_{r s t}^{p} \frac{\partial \bar{x}^{i}}{\partial x^{p}} \frac{\partial x^{r}}{\partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{i}}-3 T_{t r s}^{p} \frac{\partial \bar{x}^{i}}{\partial x^{p}} \frac{\partial x^{t}}{\partial \bar{x}^{i}} \frac{\partial x^{r}}{\partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \\
& =\left(T_{r s t}^{p}-3 T_{t r s}^{p}\right) \frac{\partial \bar{x}^{i}}{\partial x^{p}} \frac{\partial x^{r}}{\partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \frac{\partial x^{t}}{\partial \bar{x}^{l}}=0
\end{aligned}
$$

as desired.

\subsection*{4.15 Prove Theorem 4.3.}
By Problem 3.14(a), the covariant transformation law has the matrix expression

$$
\bar{T}=\bar{J}^{T} T \bar{J} \quad \text { whence } \quad|\bar{T}|=\overline{\mathscr{J}}^{2}|T|
$$

Thus, $|T|=0$ implies $|\bar{T}|=0$.

4.16 Prove that if a mixed tensor $\left(T_{j}^{i}\right)$ can be expressed as the outer product of contravariant and covariant vectors $\left(U^{i}\right)$ and $\left(V_{j}\right)$ in one coordinate system, then $\left(T_{j}^{i}\right)$ is the outer product of those vectors in general.

We must prove that $\bar{T}_{j}^{i}=\bar{U}^{i} \bar{V}_{j}$ for any admissible coordinate system $\left(\bar{x}^{i}\right)$. But, by hypothesis,

$$
\bar{T}_{j}^{i}-\bar{U}^{i} \bar{V}_{j}=T_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}-\left(U^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right)\left(V_{s} \frac{\partial x^{s}}{\partial \bar{x}^{j}}\right)=\left(T_{s}^{r}-U^{r} V_{s}\right) \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=0
$$

\section*{Supplementary Problems}
4.17 If $\left(U^{i}\right)$ and $\left(V^{i}\right)$ are contravariant vectors, verify that $\left(2 U^{i}+3 V^{i}\right)$ is also a contravariant vector.

4.18 Verify that the outer product of a contravariant vector and a covariant vector is a mixed tensor of order two.

4.19 How many potentially different mixed tensors of order two can be defined by taking the outer product of $\mathbf{S}=\left(S_{k}^{i j}\right)$ and $\mathbf{T}=\left(T_{j k}^{i}\right)$, then contracting twice?

4.20 Show that if $T_{k l}^{i j}$ are tensor components, $T_{i j}^{i j}$ is an invariant.

4.21 Prove that if $T_{j k l}^{i} U^{j} \equiv S_{k l}^{i}$ are components of a tensor for any contravariant vector $\left(U^{j}\right)$, then $\left(T_{j k l}^{i}\right)$ is a tensor of the indicated type. [Hint: Apply the Quotient Theorem to $\left(M_{k l j}^{i}\right) \equiv\left(T_{j k l}^{i}\right)$. More generally, the Quotient Theorem is valid for all choices of the inner product.]

4.22 Prove that if $T_{j k l}^{i} S^{k l} \equiv U_{j}^{i}$ are tensor components for arbitrary contravariant tensors $\left(S^{k l}\right)$, then $\left(T_{j k l}^{i}\right)$ is a tensor of the indicated type. [Hint: Follow Problem 4.9.]

4.23 Prove that if $T_{j k l}^{i} U^{k} U^{l} \equiv V_{j}^{i}$ are components of a tensor for an arbitrary contravariant vector $\left(U^{i}\right)$, and if $\left(T_{j k l}^{i}\right)$ is symmetric in the last two lower indices in all coordinate systems, then $\left(T_{j k l}^{i}\right)$ is a tensor of the type indicated.

4.24 Show that Theorem 4.3 and Corollary 4.4 are equivalent.

4.25 Prove the assertion of Example 4.7.

4.26 Prove that if an invariant $E$ can be expressed as the inner product of vectors $\left(U_{i}\right)$ and $\left(V^{i}\right)$ in one coordinate system, then $E$ has that representation in any coordinate system.

\section*{Chapter 5}
\section*{The Metric Tensor}
\subsection*{5.1 INTRODUCTION}
The notion of distance (or metric) is fundamental in applied mathematics. Frequently, the distance concept most useful in a particular application is non-Euclidean (under which the Pythagorean relation for geodesic right triangles is not valid). Tensor calculus provides a natural tool for the investigation of general formulations of distance; it studies not only non-Euclidean metrics but also the forms assumed by the Euclidean metric in particular coordinate systems.

Calculus texts often contain derivations of arc-length formulas for polar coordinates that apparently apply only to that one coordinate system; here we develop a concise method for obtaining the arc-length formula for any admissible coordinate system. The theory culminates in later chapters with a method for distinguishing between a metric that is genuinely non-Euclidean and one that is Euclidean but disguised by the peculiarities of a particular system of coordinates.

\subsection*{5.2 ARC LENGTH IN EUCLIDEAN SPACE}
The classical expressions from calculus for arc length in various coordinate systems lead to a general formula of the type


\begin{equation*}
L=\int_{a}^{b} \sqrt{\left|g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}\right|} d t \tag{5.1a}
\end{equation*}


where $g_{i j}=g_{i j}\left(x^{1}, x^{2}, \ldots, x^{n}\right)=g_{j i}$ are functions of the coordinates and $L$ gives the length of the arc $a \leqq t \leqq b$ of the curve $x^{i}=x^{i}(t) \quad(1 \leqq i \leqq n)$.

EXAMPLE 5.1 The arc-length formula for Euclidean three-space in a rectangular coordinate system $\left(x^{1}, x^{2}, x^{3}\right)$ may be recalled:

$$
L=\int_{a}^{b} \sqrt{\left(\frac{d x^{1}}{d t}\right)^{2}+\left(\frac{d x^{2}}{d t}\right)^{2}+\left(\frac{d x^{3}}{d t}\right)^{2}} d t=\int_{a}^{b} \sqrt{\delta_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}} d t
$$

This is $(5.1 a)$, with $g_{i j}=\delta_{i j}$.

The formula in Example 5.1 has the equally informative differential form

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2}=\delta_{i j} d x^{i} d x^{j}
$$

More generally, (5.1a) is equivalent to


\begin{equation*}
\pm d s^{2}=g_{i j} d x^{i} d x^{j} \tag{5.1b}
\end{equation*}


EXAMPLE 5.2 For convenient reference, formulas for the Euclidean metric in the nonrectangular coordinate systems heretofore considered are collected below.

Polar coordinates: $\left(x^{1}, x^{2}\right)=(r, \theta)$; Fig. 3-1.


\begin{equation*}
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2} \tag{5.2}
\end{equation*}


Cylindrical coordinates: $\left(x^{1}, x^{2}, x^{3}\right)=(r, \theta, z)$; Fig. 3-2.


\begin{equation*}
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2} \tag{5.3}
\end{equation*}


Spherical coordinates: $\left(x^{1}, x^{2}, x^{3}\right)=(\rho, \varphi, \theta)$; Fig. 3-3.


\begin{equation*}
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2}+\left(x^{1} \sin x^{2}\right)^{2}\left(d x^{3}\right)^{2} \tag{5.4}
\end{equation*}


Affine coordinates: (see Fig. 5-1).


\begin{align*}
d s^{2}=\left(d x^{1}\right)^{2} & +\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2} \\
& +2 \cos \alpha d x^{1} d x^{2}+2 \cos \beta d x^{1} d x^{3}+2 \cos \gamma d x^{2} d x^{3} \tag{5.5}
\end{align*}


Formula (5.5) is derived in Problem 5.9. Note that the matrix $\left(g_{i j}\right)$ defining the Euclidean metric is nondiagonal in affine coordinates.

Although formulated for Euclidean space, (5.1) is extended, in the section that follows, to provide the distance concept for non-Euclidean spaces as well.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-061}
\end{center}

Fig. 5-1

\subsection*{5.3 GENERALIZED METRICS; THE METRIC TENSOR}
Assume that a matrix field $\mathbf{g}=\left(g_{i j}\right)$ exists satisfying in all (admissible) coordinate systems $\left(x^{i}\right)$ and in some (open) region of space:\\
A. $\mathbf{g}$ is of differentiability class $C^{2}$ (i.e., all second-order partial derivatives of the $g_{i j}$ exist and are continuous).\\
B. $\mathbf{g}$ is symmetric (i.e., $g_{i j}=g_{j i}$ ).\\
C. $\mathbf{g}$ is nonsingular (i.e., $\left|g_{i j}\right| \neq 0$ ).

D. The differential form $(5.1 \mathrm{~b})$, and hence the distance concept generated by $\mathbf{g}$, is invariant with respect to a change of coordinates.

Sometimes, particularly in geometric applications of tensors, a property str ger than $\mathrm{C}$ above is assumed:

$\mathrm{C}^{\prime} . \quad \mathbf{g}$ is positive definite [i.e., $g_{i j} v^{i} v^{j}>0$ for all nonzero vectors $\mathbf{v}=\left(v^{1}, v^{2}, \ldots, v^{n}\right)$ ].

Under property $\mathrm{C}^{\prime},\left|g_{i j}\right|$ and $g_{11}, g_{22}, \ldots, g_{n n}$ are all positive. Furthermore, the inverse matrix field $\mathbf{g}^{-1}$ is also positive definite.

For later use we define the arc-length parameter for a curve $\mathscr{C}: x^{i}=x^{i}(t) \quad(a \leqq t \leqq b)$ :


\begin{equation*}
s(t)=\int_{a}^{t} \sqrt{\varepsilon g_{i j} \frac{d x^{i}}{d u} \frac{d x^{j}}{d u}} d u \tag{5.6a}
\end{equation*}


where $\varepsilon=+1$ or -1 according as

$$
g_{i j} \frac{d x^{i}}{d u} \frac{d x^{j}}{d u} \geqq 0 \quad \text { or } \quad g_{i j} \frac{d x^{i}}{d u} \frac{d x^{j}}{d u}<0
$$

The functional $\varepsilon$ is called the indicator of the vector $\left(d x^{i} / d u\right)$ relative to the metric $\left(g_{i j}\right)$. One can, of course, use absolute value signs instead of the indicator, but the latter notation works better in algebraic manipulations. In terms of the arc-length parameter, the length of $\mathscr{C}$ is $L=s(b)$.

Differentiating (5.6a) and squaring yields the equivalent formula


\begin{equation*}
\left(\frac{d s}{d t}\right)^{2}=\varepsilon g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t} \tag{5.6b}
\end{equation*}


Finally, introducing the differentials

$$
d x^{i} \equiv \frac{d x^{i}(t)}{d t} d t
$$

the values of which are independent of the choice of curve parameter, we retrieve $(5.1 b)$ as


\begin{equation*}
\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \tag{5.6c}
\end{equation*}


EXAMPLE 5.3 Suppose that on $\mathbf{R}^{3}$ a matrix field is given in $\left(x^{i}\right)$ by

$$
\left(g_{i j}\right)=\left[\begin{array}{ccc}
\left(x^{1}\right)^{2}-1 & 1 & 0 \\
1 & \left(x^{2}\right)^{2} & 0 \\
0 & 0 & \frac{64}{9}
\end{array}\right] \quad \text { where } \quad\left[\left(x^{1}\right)^{2}-1\right]\left(x^{2}\right)^{2} \neq 1
$$

(a) Show that, if extended to all admissible coordinate systems according to the transformation law for covariant tensors, this matrix field is a metric; i.e., it satisfies properties A-D above. (b) For this metric, compute the arc-length parameter and the length of the curve

$$
\mathscr{C}:\left\{\begin{array}{l}
x^{1}=2 t-1 \\
x^{2}=2 t^{2} \\
x^{3}=t^{3}
\end{array} \quad(0 \leqq t \leqq 1)\right.
$$

(a) Property A obtains since $g_{i j}$ is a polynomial in $x^{1}$ and $x^{2}$ for each $i$, $j$. Since the matrix $\left(g_{i j}\right)$ is symmetric, property B holds. Since

$$
\left|g_{i j}\right|=\frac{64}{9}\left|\begin{array}{cc}
\left(x^{1}\right)^{2}-1 & 1 \\
1 & \left(x^{2}\right)^{2}
\end{array}\right|=\frac{64}{9}\left\{\left(x^{2}\right)^{2}\left[\left(x^{1}\right)^{2}-1\right]-1\right\} \neq 0
$$

property C obtains. Property D follows from Problem 4.5.

(b) It is convenient, here and later, to rewrite $(5.6 b)$ as the matrix product


\begin{equation*}
\varepsilon\left(\frac{d s}{d t}\right)^{2}=\left(\frac{d x^{i}}{d t}\right)^{T}\left(g_{i j}\right)\left(\frac{d x^{i}}{d t}\right) \tag{5.6d}
\end{equation*}


Along the given curve, this becomes

$$
\begin{aligned}
\varepsilon\left(\frac{d s}{d t}\right)^{2} & =\left[\begin{array}{lll}
2 & 4 t & 3 t^{2}
\end{array}\right]\left[\begin{array}{ccc}
(2 t-1)^{2}-1 & 1 & 0 \\
1 & \left(2 t^{2}\right)^{2} & 0 \\
0 & 0 & \frac{64}{9}
\end{array}\right]\left[\begin{array}{c}
2 \\
4 t \\
3 t^{2}
\end{array}\right] \\
& =64 t^{6}+64 t^{4}+16 t^{2}=\left(8 t^{3}+4 t\right)^{2}
\end{aligned}
$$

Hence, $\varepsilon=1$ and

$$
s(t)=\int_{0}^{t}\left(8 u^{3}+4 u\right) d u=\left[2 u^{4}+2 u^{2}\right]_{0}^{t}=2 t^{4}+2 t^{2}
$$

from which $L=2(1)^{4}+2(1)^{2}=4$.

The properties postulated of $\mathbf{g}$ make it a tensor, the so-called fundamental or metric tensor. In fact, property $D$ ensures that

$$
g_{i j} V^{i} V^{j} \equiv E
$$

is an invariant for every contravariant vector $\left(V^{i}\right)=\left(d x^{i} / d t\right)$. (By solving an ordinary differential equation, one can exhibit the curve that possesses a given tangent vector.) Then, in view of property B, criterion (4) of Section 4.2 implies

Theorem 5.1: The metric $\mathbf{g}=\left(g_{i j}\right)$ is a covariant tensor of the second order.

In Problem 3.14(a), the matrix equation $U=J^{T} \bar{U} J$ was found for the transformation of a second-order covariant tensor $\mathbf{U}$. If $\left(\bar{x}^{i}\right)$ is a rectangular system and $\mathbf{U}=\mathbf{g}$ is the Euclidean metric tensor, then in $\left(x^{i}\right), U=G$, and in $\left(\bar{x}^{i}\right), \bar{U}=\bar{G}=I$; thus we have proved

Theorem 5.2: If the Jacobian matrix of the transformation from a given coordinate system $\left(x^{i}\right)$ to a rectangular system $\left(\bar{x}^{i}\right)$ is $J=\left(\partial \bar{x}^{i} / \partial x^{j}\right)$, then the matrix $G \equiv\left(g_{i j}\right)$ of the Euclidean metric tensor in the $\left(x^{i}\right)$-system is given by


\begin{equation*}
G=J^{T} J \tag{5.7}
\end{equation*}


Remark 1: Equation (5.7) illustrates the following well-known result of matrix theory: Any symmetric, positive definite matrix $A$ has a nonsingular "square root" $C$ such that $A=C^{T} C$.

It should be emphasized that only the Euclidean metric admits of a representation of the form (5.7). For, by very definition, if $\mathbf{g}$ is non-Euclidean, there exists no coordinate system $\left(\bar{x}^{i}\right)$ in which $\bar{G}=I$.

EXAMPLE 5.4 Cylindrical coordinates $\left(x^{i}\right)$ and rectangular coordinates $\left(\bar{x}^{i}\right)$ are connected through

$$
\bar{x}^{1}=x^{1} \cos x^{2} \quad \bar{x}^{2}=x^{1} \sin x^{2} \quad \bar{x}^{3}=x^{3}
$$

Thus

$$
J=\left[\begin{array}{ccc}
\cos x^{2} & -x^{1} \sin x^{2} & 0 \\
\sin x^{2} & x^{1} \cos x^{2} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

and the (Euclidean) metric for cylindrical coordinates is given by

$$
\begin{aligned}
G=J^{T} J & =\left[\begin{array}{ccc}
\cos x^{2} & \sin x^{2} & 0 \\
-x^{1} \sin x^{2} & x^{1} \cos x^{2} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
\cos x^{2} & -x^{1} \sin x^{2} & 0 \\
\sin x^{2} & x^{1} \cos x^{2} & 0 \\
0 & 0 & 1
\end{array}\right] \\
& =\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & 1
\end{array}\right]
\end{aligned}
$$

or $g_{11}=g_{33}=1, g_{22}=\left(x^{1}\right)^{2}$, and $g_{i j}=0$ for $i \neq j$. These results verify (5.3).

In spite of the apparent restriction to the Euclidean distance concept, in connection with such results as Theorem 5.2, the reader should keep in mind that one is free to choose as the metric tensor for $\mathbf{R}^{n}$ any $\mathbf{g}$ that obeys properties A-D above. For instance, it can be shown by methods to be developed later that the metric chosen in Example 5.3 is non-Euclidean.

\subsection*{5.4 CONJUGATE METRIC TENSOR; RAISING AND LOWERING INDICES}
One of the fundamental concepts of tensor calculus resides in the "raising" or "lowering" of indices in tensors. If we are given a contravariant vector $\left(T^{i}\right)$ and if, for the moment, $\left(g_{i j}\right)$ represents any covariant tensor of the second order, then we know (Problem 4.4) that the inner product $\left(S_{i}\right)=\left(g_{i j} T^{j}\right)$ is a covariant vector. Now, if $\left(g_{i j}\right)$ is in fact the metric tensor whereby distance in $\mathbf{R}^{n}$ is defined, it will prove useful in many contexts to consider $\left(S_{i}\right)$ and $\left(T^{i}\right)$ as covariant and contravariant aspects of a single notion. Thus, we write $T_{i}$ instead of $S_{i}$ :

$$
T_{i}=g_{i j} T^{j}
$$

and say that taking the inner product with the metric tensor has lowered a contravariant index to a covariant index. to

Because the matrix $\left(g_{i j}\right)$ is invertible (property $\mathrm{C}$ of Section 5.3), the above relation is equivalent

$$
T^{i}=g^{i j} T_{j}
$$

where $\left(g^{i j}\right)=\left(g_{i j}\right)^{-1}$; now we say that a covariant index has been raised to a contravariant index.

Definition 1: The inverse of the fundamental matrix field (metric tensor),

$$
\left[g^{i j}\right]_{n n}=\left[g_{i j}\right]_{n n}^{-1}
$$

is called the conjugate metric tensor.

Both metric tensors are freely applied to create new, more covariant $(\mathbf{g})$ or more contravariant $\left(\mathbf{g}^{-1}\right)$ counterparts to given tensors. Thus, starting with the mixed tensor $\left(T_{k}^{i j}\right)$,

$$
\begin{aligned}
T^{i j k} & \equiv g^{i r} T_{r}^{j k} \\
T_{i k}^{j} & \equiv g_{i r} T_{k}^{j r}
\end{aligned}
$$

and

$$
T_{i j k} \equiv g_{i s} T_{j k}^{s} \equiv g_{i s} g_{j r} T_{k}^{s r} .
$$

\subsection*{5.5 GENERALIZED INNER-PRODUCT SPACES}
Suppose that a metric $\mathbf{g}$ has been imposed on $\mathbf{R}^{n}$ and that $\mathbf{U}$ and $\mathbf{V}$ are two vectors on the metric space. It is essential to the definition of a geometrically significant inner product UV that its value depend only on the vectors $\mathbf{U}$ and $\mathbf{V}$, and not on the particular coordinate system used to specify these vectors. (There are other requirements on an inner product, but they are secondary.) This fact motivates

Definition 2: To each pair of contravariant vectors $\mathbf{U}=\left(U^{i}\right)$ and $\mathbf{V}=\left(V^{i}\right)$ is associated the real number


\begin{equation*}
\mathbf{U V} \equiv g_{i j} U^{i} V^{j} \equiv U^{i} V_{i} \equiv U_{i} V^{i} \tag{5.8}
\end{equation*}


called the (generalized) inner product of $\mathbf{U}$ and $\mathbf{V}$.

In similar fashion, the inner product of two covariant vectors is defined as


\begin{equation*}
\mathbf{U} \mathbf{V} \equiv g^{i j} U_{i} V_{j} \equiv U^{i} V_{i} \equiv U_{i} V^{i} \tag{5.9}
\end{equation*}


consistent with (5.8). We therefore have the rule: To obtain the inner product of two vectors of the same type, convert one vector to the opposite type and then take the tensor inner product.

Remark 2: It follows from Problem 4.5-or, more fundamentally, from property D of $\mathbf{g}$ - that the inner product (5.8) or (5.9) is an invariant, as required.

According to (4.2), the set of all contravariant vectors on $\mathbf{R}^{n}$ is a vector space, as is the set of all covariant vectors on $\mathbf{R}^{n}$. With an inner product as defined above, these vector spaces become (generalized) inner-product spaces.

\subsection*{5.6 CONCEPTS OF LENGTH AND ANGLE}
Expressions (2.7) and (2.8) readily extend to a generalized inner-product space, provided the metric is positive definite. The norm (or length) of an arbitrary vector $\mathbf{V}=\left(V^{i}\right)$ or $\mathbf{V}=\left(V_{i}\right)$ is the nonnegative real number


\begin{equation*}
\|\mathbf{V}\| \equiv \sqrt{\mathbf{V}^{2}}=\sqrt{V_{i} V^{i}} \tag{5.10}
\end{equation*}


Remark 3: The norm of a vector-and thus the notion of a normed linear space-can be defined abstractly (see Problem 5.14), without reference to an inner product.

EXAMPLE 5.5 Show that under the Euclidean metric (5.2) for polar coordinates, the vectors

$$
\left(U^{i}\right)=\left(3 / 5,4 / 5 x^{1}\right) \quad \text { and } \quad\left(V^{i}\right)=\left(-4 / 5,3 / 5 x^{1}\right)
$$

are orthonormal.

Using matrices, we have:

$$
\begin{aligned}
\|\mathbf{U}\|^{2}=U^{i} U_{i}=g_{i j} U^{i} U^{j} & =\left[\begin{array}{ll}
\frac{3}{5} & \frac{4}{5 x^{1}}
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & \left(x^{1}\right)^{2}
\end{array}\right]\left[\begin{array}{c}
\frac{3}{5} \\
\frac{4}{5 x^{1}}
\end{array}\right] \\
& =\left[\begin{array}{ll}
\frac{3}{5} & \frac{4}{5 x^{1}}
\end{array}\right]\left[\begin{array}{c}
\frac{3}{5} \\
\frac{4 x^{1}}{5}
\end{array}\right] \\
& =\frac{9}{25}+\frac{16 x^{1}}{25 x^{1}}=1
\end{aligned}
$$

or $\|\mathbf{U}\|=1$; likewise, $\|\mathbf{V}\|=1$. Now we verify that the vectors are orthogonal:

$$
\begin{aligned}
\mathbf{U V}=g_{i j} U^{i} V^{j} & =\left[\begin{array}{ll}
\frac{3}{5} & \frac{4}{5 x^{1}}
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & \left(x^{1}\right)^{2}
\end{array}\right]\left[\begin{array}{c}
-\frac{4}{5} \\
\frac{3}{5 x^{1}}
\end{array}\right] \\
& =\left[\begin{array}{ll}
\frac{3}{5} & \frac{4}{5 x^{1}}
\end{array}\right]\left[\begin{array}{r}
-\frac{4}{5} \\
\frac{3 x^{1}}{5}
\end{array}\right] \\
& =-\frac{12}{25}+\frac{12 x^{1}}{25 x^{1}}=0
\end{aligned}
$$

Both normality and orthogonality depend, of course, on the metric alone, and not on the (polar) coordinate system.

The angle $\theta$ between two non-null contravariant vectors $\mathbf{U}$ and $\mathbf{V}$ is defined by


\begin{equation*}
\cos \theta \equiv \frac{\mathbf{U V}}{\|\mathbf{U}\|\|\mathbf{V}\|}=\frac{g_{i j} U^{i} V^{j}}{\sqrt{g_{p q} U^{p} U^{q}} \sqrt{g_{r s} V^{r} V^{s}}} \quad(0 \leqq \theta \leqq \pi) \tag{5.11}
\end{equation*}


That $\theta$ is well-defined follows from the Cauchy-Schwarz inequality, which may be written in the form

$$
-1 \leqq \frac{\mathbf{U V}}{\|\mathbf{U}\|\|\mathbf{V}\|} \leqq 1
$$

\section*{(see Problem 5.13).}
The tangent field to a family of smooth curves is a contravariant vector (Example 3.4), so that (5.11) yields the geometrical

Theorem 5.3: In a general coordinate system, if $\left(U^{i}\right)$ and $\left(V^{i}\right)$ are the tangent vectors to two families of curves, then the families are mutually orthogonal if and only if $g_{i j} U^{i} V^{j}=0$.

EXAMPLE 5.6 Show that each member of the family of curves given in polar coordinates by


\begin{equation*}
e^{1 / r}=a(\sec \theta+\tan \theta) \quad(a \geqq 0) \tag{1}
\end{equation*}


is orthogonal to each of the curves (limaons of Pascal)


\begin{equation*}
r=\sin \theta+c \quad(c \geqq 0) \tag{2}
\end{equation*}


(Figure 5-2 indicates the orthogonality of the curve $a=1$ to the family of limaons.)

In polar coordinates $x^{1}=r, x^{2}=\theta$, and with curve parameter $t,(1)$ becomes-after taking logarithms-

$$
\frac{1}{x^{1}}=\ln a+\ln |\sec t+\tan t| \quad x^{2}=t
$$

With curve parameter $u$, (2) becomes

$$
x^{1}=\sin u+c \quad x^{2}=u
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-066}
\end{center}

Fig. 5-2

Differentiation of $\left(1^{\prime}\right)$ with respect to $t$ yields

$$
-\frac{1}{\left(x^{1}\right)^{2}} \frac{d x^{1}}{d t}=\sec t=\sec x^{2} \quad \frac{d x^{2}}{d t}=1
$$

so that the tangent vector to family $\left(1^{\prime}\right)$ is

$$
\left(U^{1}, U^{2}\right)=\left(-\left(x^{1}\right)^{2} \sec x^{2}, 1\right)
$$

Similarly, the tangent vector to family $\left(2^{\prime}\right)$ is

$$
\left(V^{1}, V^{2}\right)=(\cos u, 1)=\left(\cos x^{2}, 1\right)
$$

Applying Theorem 5.3, with the Euclidean metric tensor in polar coordinates,

$$
\begin{aligned}
g_{i j} U^{i} V^{j} & =g_{11} U^{1} V^{1}+g_{22} U^{2} V^{2}+0 \\
& =(1)\left[-\left(x^{1}\right)^{2} \sec x^{2}\right]\left(\cos x^{2}\right)+\left(x^{1}\right)^{2}(1)(1) \\
& \ddots-\left(x^{1}\right)^{2}+\left(x^{1}\right)^{2}=0
\end{aligned}
$$

Observe that nonparametric forms of the tangent vectors are used in the orthogonality condition. This is necessary because the metric tensor at the intersection point $\left(x^{1}, x^{2}\right)$ of a curve (1) and a curve (2) depends on neither the parameter $t$ along (1) nor the parameter $u$ along (2).

\section*{Solved Problems}
\section*{ARC LENGTH}
5.1 A curve is given in spherical coordinates $\left(x^{i}\right)$ by

$$
x^{1}=t \quad x^{2}=\arcsin \frac{1}{t} \quad x^{3}=\sqrt{t^{2}-1}
$$

Find the length of the arc $1 \leqq t \leqq 2$.

By $(5.4)$,

$$
\left(\frac{d s}{d t}\right)^{2}=\left(\frac{d x^{1}}{d t}\right)^{2}+\left(x^{1}\right)^{2}\left(\frac{d x^{2}}{d t}\right)^{2}+\left(x^{1} \sin x^{2}\right)^{2}\left(\frac{d x^{3}}{d t}\right)^{2}
$$

so we first calculate the $\left(d x^{i} / d t\right)^{2}$ :

$$
\left(\frac{d x^{1}}{d t}\right)^{2}=1 \quad\left(\frac{d x^{2}}{d t}\right)^{2}=\left(\frac{-1 / t^{2}}{\sqrt{1-(1 / t)^{2}}}\right)^{2}=\frac{1}{t^{2}\left(t^{2}-1\right)} \quad\left(\frac{d x^{3}}{d t}\right)^{2}=\left(\frac{1}{2} \frac{2 t}{\sqrt{t^{2}-1}}\right)^{2}=\frac{t^{2}}{t^{2}-1}
$$

Then

$$
\left(\frac{d s}{d t}\right)^{2}=1+t^{2} \cdot \frac{1}{t^{2}\left(t^{2}-1\right)}+\left(t \cdot \frac{1}{t}\right)^{2} \cdot \frac{t^{2}}{t^{2}-1}=\frac{2 t^{2}}{t^{2}-1}
$$

and $(5.1 a)$ gives

$$
\left.L=\int_{1}^{2} \frac{\sqrt{2} t}{\sqrt{t^{2}-1}} d t=\sqrt{2\left(t^{2}-1\right)}\right]_{1}^{2}=\sqrt{6}
$$

5.2 Find the length of the curve

$$
\mathscr{C}:\left\{\begin{array}{l}
x^{1}=1 \\
x^{2}=t
\end{array} \quad(1 \leqq t \leqq 2)\right.
$$

if the metric is that of the hyperbolic plane $\left(x^{2}>0\right)$ :

$$
g_{11}=g_{22}=\frac{1}{\left(x^{2}\right)^{2}} \quad g_{12}=g_{21}=0
$$

Since $\left(d x^{i} / d t\right)=(0,1),(5.6 d)$ yields $(\varepsilon=1)$

$$
\begin{gathered}
\left(\frac{d s}{d t}\right)^{2}=\left[\begin{array}{ll}
0 & 1
\end{array}\right]\left[\begin{array}{cc}
\frac{1}{t^{2}} & 0 \\
0 & \frac{1}{t^{2}}
\end{array}\right]\left[\begin{array}{c}
0 \\
1
\end{array}\right]=\frac{1}{t^{2}} \\
L=\int_{1}^{2} \frac{1}{t} d t=\ln 2
\end{gathered}
$$

and

\section*{GENERALIZED METRICS}
5.3 Is the form $d x^{2}+3 d x d y+4 d y^{2}+d z^{2}$ positive definite?

It must be determined whether the polynomial $Q \equiv\left(u^{1}\right)^{2}+3 u^{1} u^{2}+4\left(u^{2}\right)^{2}+\left(u^{3}\right)^{2}$ is positive unless $u^{1}=u^{2}=u^{3}=0$. By completing the square,

$$
Q=\left(u^{1}\right)^{2}+3 u^{1} u^{2}+\frac{9}{4}\left(u^{2}\right)^{2}+\frac{7}{4}\left(u^{2}\right)^{2}+\left(u^{3}\right)^{2}=\left(u^{1}+\frac{3}{2} u^{2}\right)^{2}+\frac{7}{4}\left(u^{2}\right)^{2}+\left(u^{3}\right)^{2}
$$

All terms are perfect squares with positive coefficients; hence the form is indeed positive definite.

5.4 Show that the formula (5.1a) for arc length does not depend on the particular parameterization of the curve.

Given a curve $\mathscr{C}: x^{i}=x^{i}(t) \quad(a \leqq t \leqq b)$, suppose that $x^{i}=x^{i}(\bar{t}) \quad(\bar{a} \leqq \bar{t} \leqq \bar{b})$ is a different parameterization, where $\bar{t}=\phi(t)$, with $\phi^{\prime}(t)>0$ and $\bar{a}=\phi(a), \bar{b}=\phi(b)$. Then, by the chain rule and substitution rule for integrals,

$$
\begin{aligned}
L & =\int_{a}^{b} \sqrt{\left|g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}\right|} d t=\int_{a}^{b} \sqrt{\left|g_{i j} \frac{d x^{i}}{d \bar{t}} \frac{d x^{j}}{d \bar{t}}\left(\phi^{\prime}(t)\right)^{2}\right|} d t \\
& =\int_{a}^{b} \sqrt{\left|g_{i j} \frac{d x^{i}}{d \bar{t}} \frac{d x^{j}}{d \bar{t}}\right|} \phi^{\prime}(t) d t=\int_{\bar{a}}^{\bar{b}} \sqrt{\left|g_{i j} \frac{d x^{i}}{d \bar{t}} \frac{d x^{j}}{d \bar{t}}\right|} d \bar{t}=\bar{L}
\end{aligned}
$$

\section*{TENSOR PROPERTY OF THE METRIC}
5.5 Find the Euclidean metric tensor (in matrix form) for spherical coordinates, using Theorem 5.2 .

Since spherical coordinates $\left(x^{i}\right)$ are connected to rectangular coordinates $\left(\bar{x}^{i}\right)$ via

$$
\bar{x}^{-1}=x^{1} \sin x^{2} \cos x^{3} \quad \bar{x}^{2}=\bar{x}^{-1} \sin x^{2} \sin x^{3} \quad \bar{x}^{3}=x^{1} \cos x^{2}
$$

we have

$$
J^{T} J=\left[\begin{array}{ccc}
\sin x^{2} \cos x^{3} & \sin x^{2} \sin x^{3} & \cos x^{2} \\
x^{1} \cos x^{2} \cos x^{3} & x^{1} \cos x^{2} \sin x^{3} & -x^{1} \sin x^{2} \\
-x^{1} \sin x^{2} \sin x^{3} & x^{1} \sin x^{2} \cos x^{3} & 0
\end{array}\right]\left[\begin{array}{ccc}
\sin x^{2} \cos x^{3} & x^{1} \cos x^{2} \cos x^{3} & -x^{1} \sin x^{2} \sin x^{3} \\
\sin x^{2} \sin x^{3} & x^{1} \cos x^{2} \sin x^{3} & x^{1} \sin x^{2} \cos x^{3} \\
\cos x^{2} & -x^{1} \sin x^{2} & 0
\end{array}\right]
$$

Since $G=J^{T} J$ is known to be symmetric (see Problem 2.4), we need only compute the elements on or above the main diagonal:

$$
G=\left[\begin{array}{ccc}
\left(\sin ^{2} x^{2}\right)(1)+\cos ^{2} x^{2} & \left(x^{1} \sin x^{2} \cos x^{2}\right)(1)-x^{1} \sin x^{2} \cos x^{2} & g_{13} \\
g_{21} & \left(\left(x^{1}\right)^{2} \cos ^{2} x^{2}\right)(1)+\left(x^{1}\right)^{2} \sin ^{2} x^{2} & g_{23} \\
g_{31} & g_{32} & \left(\left(x^{1}\right)^{2} \sin ^{2} x^{2}\right)(1)
\end{array}\right]
$$

where

$$
\begin{aligned}
& g_{13}=\left(x^{1} \sin ^{2} x^{2}\right)\left(-\sin x^{3} \cos x^{3}+\cos x^{3} \sin x^{3}\right)=0 \\
& g_{23}=\left(\left(x^{1}\right)^{2} \sin x^{2} \cos x^{2}\right)\left(-\cos x^{3} \sin x^{3}+\sin x^{3} \cos x^{3}\right)=0
\end{aligned}
$$

Hence

$$
G=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & \left(x^{1} \sin x^{2}\right)^{2}
\end{array}\right]
$$

5.6 Find the components $g_{i j}$ of the Euclidean metric tensor in the special coordinate system $\left(x^{i}\right)$ defined from rectangular coordinates $\left(\bar{x}^{i}\right)$ by $x^{1}=\bar{x}^{1}, x^{2}=\exp \left(\bar{x}^{2}-\bar{x}^{1}\right)$.

We must compute $J^{T} J$, where $J$ is the Jacobian matrix of the transformation $\bar{x}^{i}=\bar{x}^{i}\left(x^{1}, x^{2}\right)$. Thus, we solve the above equations for the $\bar{x}^{i}$ :

Hence

$$
\bar{x}^{1}=x^{1} \quad \bar{x}^{2}=x^{1}+\ln x^{2}
$$

$$
\begin{aligned}
& \text { Hence } \quad J=\left[\begin{array}{ll}
1 & 0 \\
1 & \left(x^{2}\right)^{-1}
\end{array}\right] \\
& \text { and } G=\left[\begin{array}{cc}
1 & 1 \\
0 & \left(x^{2}\right)^{-1}
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
1 & \left(x^{2}\right)^{-1}
\end{array}\right]=\left[\begin{array}{cc}
2 & \left(x^{2}\right)^{-1} \\
\left(x^{2}\right)^{-1} & \left(x^{2}\right)^{-2}
\end{array}\right] \\
& \text { or } g_{11}=2, g_{10}=g_{01}=\left(x^{2}\right)^{-1}, g_{23}=\left(x^{2}\right)^{-2} \text {. }
\end{aligned}
$$

or $g_{11}=2, g_{12}=g_{21}=\left(x^{2}\right)^{-1}, g_{22}=\left(x^{2}\right)^{-2}$.

5.7 (a) Using the metric of Problem 5.6, calculate the length of the curve

$$
\mathscr{C}: x^{1}=3 t, \quad x^{2}=e^{t} \quad(0 \leqq t \leqq 2)
$$

(b) Interpret geometrically.

(a) First calculate the $d x^{i} / d t$ :

Then

$$
\frac{d x^{1}}{d t}=3 \quad \frac{d x^{2}}{d t}=e^{t}
$$

Then

$$
\begin{gathered}
\left(\frac{d s}{d t}\right)^{2}=2\left(\frac{d x^{1}}{d t}\right)^{2}+2\left(x^{2}\right)^{-1}\left(\frac{d x^{1}}{d t}\right)\left(\frac{d x^{2}}{d t}\right)+\left(x^{2}\right)^{-2}\left(\frac{d x^{2}}{d t}\right)^{2} \\
=2(9)+2 e^{-t}(3)\left(e^{t}\right)+e^{-2 t}\left(e^{2 t}\right)=25 \\
L=\int_{0}^{2} 5 d t=10
\end{gathered}
$$

and

(b) From the transformation equations of Problem 5.6, the curve is described in rectangular coordinates by $\bar{x}^{2}=\frac{4}{3} \bar{x}^{1}$; it is therefore a straight line joining the points which correspond to $t=0$ and $t=2$, or $(0,0)$ and $(6,8)$. The distance from $(0,0)$ to $(6,8)$ is

$$
\sqrt{6^{2}+8^{2}}=10
$$

as found in $(a)$.

5.8 Making use of the Euclidean metric for cylindrical coordinates, (5.3), calculate the length of arc along the circular helix

$$
\bar{x}^{1}=a \cos t \quad \bar{x}^{2}=a \sin t \quad \bar{x}^{3}=b t
$$

with $a$ and $b$ positive constants, from $t=0$ to $t=c>0$. See Fig. 5-3.

In cylindrical coordinates $\left(x^{i}\right)$, where

$$
\bar{x}^{1}=x^{1} \cos x^{2} \quad \bar{x}^{2}=x^{1} \sin x^{2} \quad \bar{x}^{3}=x^{3}
$$

the helical arc is represented by the linear equations

$$
\begin{gathered}
x^{1}=a \quad x^{2}=t \quad x^{3}=b t \quad(0 \leqq t \leqq c) \\
\left(\frac{d s}{d t}\right)^{2}=\left[\begin{array}{lll}
0 & 1 & b
\end{array}\right]\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & a^{2} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
0 \\
1 \\
b
\end{array}\right]=\left[\begin{array}{lll}
0 & 1 & b
\end{array}\right]\left[\begin{array}{c}
0 \\
a^{2} \\
b
\end{array}\right]=a^{2}+b^{2}
\end{gathered}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070(1)}
\end{center}

Fig. 5-3

whence

$$
L=\int_{0}^{c} \sqrt{a^{2}+b^{2}} d t=c \sqrt{a^{2}+b^{2}}
$$

5.9 (Affine Coordinates in $\mathbf{R}^{3}$ ) Carpenters taking measurements in a room notice that at the corner they had used as reference point the angles were not true. If the actual measures of the angles are as given in Fig. 5-4, what correction in the usual metric formula,

$$
\overline{P_{1} P_{2}}=\sqrt{\sum_{i=1}^{3}\left(x_{1}^{i}-x_{2}^{i}\right)^{2}}
$$

should be made to compensate for the errors?

We are asked, in effect, to display $\mathbf{g}=\left(g_{i j}\right)$ for three-dimensional affine coordinates $\left(x^{i}\right)$. Instead of applying Theorem 5.2, it is much simpler to recall from Problem 3.9 that position vectors are contravariant affine vectors-in particular, the unit vectors

$$
\mathbf{u}=\left(\delta_{1}^{i}\right) \quad \mathbf{v}=\left(\delta_{2}^{i}\right) \quad \mathbf{w}=\left(\delta_{3}^{i}\right)
$$

along the oblique axes (Fig. 5-4). We can now use (5.11) in inverse fashion, to obtain:

$$
\cos \alpha=\frac{g_{i j} \delta_{1}^{i} \delta_{2}^{j}}{\sqrt{g_{p q} \delta_{1}^{p} \delta_{1}^{q}} \sqrt{g_{r s} \delta_{2}^{r} \delta_{2}^{s}}}=\frac{g_{12}}{\sqrt{g_{11}} \sqrt{g_{22}}}=g_{12}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-070}
\end{center}

Fig. 5-4\\
since, obviously, $g_{11}=g_{22}=g_{33}=1 \quad\left( \pm d s=d x^{1}\right.$ for motion parallel to $\mathbf{u}$; etc. $)$. Likewise,

$$
\cos \beta=g_{13} \quad \cos \gamma=g_{23}
$$

and the complete symmetric matrix is

$$
G=\left[\begin{array}{ccc}
1 & \cos \alpha & \cos \beta \\
\cos \alpha & 1 & \cos \gamma \\
\cos \beta & \cos \gamma & 1
\end{array}\right]=\left[\begin{array}{ccc}
1 & -0.01745 & -0.00873 \\
-0.01745 & 1 & 0.01745 \\
-0.00873 & 0.01745 & 1
\end{array}\right]
$$

It follows that the carpenters must use as the corrected distance formula

$$
\overline{P_{1} P_{2}}=\sqrt{g_{i j}\left(x_{1}^{i}-x_{2}^{i}\right)\left(x_{1}^{j}-x_{2}^{j}\right)}
$$

where the $g_{i j}$ have the numerical values given above.

\section*{RAISING AND LOWERING INDICES}
5.10 Given that $\left(V^{i}\right)$ is a contravariant vector on $\mathbf{R}^{3}$, find its associated covariant vector $\left(V_{i}\right)$ in cylindrical coordinates $\left(x^{i}\right)$ under the Euclidean metric.

Since

$$
\left[g_{i j}\right]_{33}=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

and $V_{i}=g_{i r} V^{r}$, we have in matrix form,

$$
\left[\begin{array}{l}
V_{1} \\
V_{2} \\
V_{3}
\end{array}\right]=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
V^{1} \\
V^{2} \\
V^{3}
\end{array}\right]=\left[\begin{array}{c}
V^{1} \\
\left(x^{1}\right)^{2} V^{2} \\
V^{3}
\end{array}\right]
$$

5.11 Show that under orthogonal coordinate changes, starting with any particular system of rectangular coordinates, the raising and lowering of indices has no effect on tensors, consistent with the fact (Section 3.6) that there is no distinction between contravariant and covariant cartesian tensors.

It suffices to show merely that $g_{i j}=\delta_{i j}=g^{i j}$ for any admissible coordinate system $\left(x^{i}\right)$, for then it will follow that

$$
T^{i}=\delta^{i j} T_{j}=T_{i} \quad T_{i j k}=\delta_{i r} T_{j k}^{r}=T_{j k}^{i} \quad T_{j k}^{i}=\delta_{j r} T_{k}^{i r}=T_{k}^{i j}
$$

and so on. To that end, simply use formula (5.7), with $J=\left(a_{i}^{i}\right)$ an orthogonal matrix. Because $J^{T}=J^{-1}$, we have $G=J^{-1} J=I$, or $g_{i j}=\delta_{i j}$, as desired. Since $G^{-1}=I^{-1}=I$, it is also the case that $g^{i j}=\delta_{i j}$.

\section*{GENERALIZED NORM}
5.12 Show that the length of any contravariant vector $\left(V^{i}\right)$ equals the length of its associated covariant vector $\left(V_{i}\right)$.

By definition,

$$
\left\|\left(V^{i}\right)\right\|=\sqrt{g_{i j} V^{i} V^{j}} \quad \text { and } \quad\left\|\left(V_{i}\right)\right\|=\sqrt{g^{i j} V_{i} V_{j}}
$$

But, since $V^{i}=g^{i r} V_{r}$ and $g_{i j}=g_{i i}$,

$$
g_{i j} V^{i} V^{j}=g_{i j}\left(g^{i r} V_{r}\right)\left(g^{j s} V_{s}\right)=g_{j i} g^{i r} g^{j s} V_{r} V_{s}=\delta_{j}^{r} g^{j s} V_{r} V_{s}=g^{r s} V_{r} V_{s}
$$

and the two lengths are equal.

5.13 Assuming a positive definite metric, show that the basic properties of the cartesian inner product $\mathbf{U} \cdot \mathbf{V}$ are shared by the generalized inner product $\mathbf{U V}$ of contravariant vectors.

(a) $\mathbf{U V}=\mathbf{V U}$ (commutative property). Follows from symmetry of $\left(g_{i j}\right)$.

(b) $\mathbf{U}(\mathbf{V}+\mathbf{W})=\mathbf{U V}+\mathbf{U W}$ (distributive property). Follows from (1.2).

(c) $\quad(\lambda \mathbf{U}) \mathbf{V}=\mathbf{U}(\lambda \mathbf{V})=\lambda(\mathbf{U V})$ (associative property). Follows from $\lambda U_{i} V^{i}=U_{i}\left(\lambda V^{i}\right)=\lambda U_{i} V^{i}$.

(d) $\mathbf{U}^{2} \geqq 0$ with equality only if $\mathbf{U}=\mathbf{0}$ (positive-definiteness). Follows from the assumed positivedefiniteness of $\left(g_{i j}\right)$.

(e) ( $\mathbf{U V})^{2} \leqq\left(\mathbf{U}^{2}\right)\left(\mathbf{V}^{2}\right)$ (Cauchy-Schwarz inequality). This may be derived from the other properties, as follows. If $\mathbf{U}=\mathbf{0}$, the inequality clearly holds. If $\mathbf{U} \neq \mathbf{0}$, property ( $d$ ) ensures that the quadratic polynomial

$$
Q(\lambda) \equiv(\lambda \mathbf{U}+\mathbf{V})^{2}=\mathbf{U}^{2} \lambda^{2}+2 \mathbf{U} \mathbf{V} \lambda+\mathbf{V}^{2}
$$

vanishes for at most one real value of $\lambda$. Thus, the discriminant of $Q$ cannot be positive:

$$
(\mathbf{U V})^{2}-\left(\mathbf{U}^{2}\right)\left(\mathbf{V}^{2}\right) \leqq 0
$$

and this is the desired inequality.

5.14 A generalized norm on a vector space is any real-valued functional $\phi[]$ that satisfies

(i) $\phi[\mathbf{V}] \geqq 0$, with equality only if $\mathbf{V}=\mathbf{0}$;

(ii) $\phi[\lambda \mathbf{V}]=|\lambda| \phi[\mathbf{V}]$;

(iii) $\phi[\mathbf{U}+\mathbf{V}] \leqq \phi[\mathbf{U}]+\phi[\mathbf{V}]$ (triangle inequality).

Verify these conditions for $\phi[\mathbf{V}]=\|\mathbf{V}\|$, the inner-product norm under a positive definite metric.

(i) and (ii) for $\|\mathbf{V}\|$ are evident. As for (iii), the Cauchy-Schwarz inequality gives

$$
\begin{aligned}
\|\mathbf{U}+\mathbf{V}\|^{2} & =(\mathbf{U}+\mathbf{V})^{2}=\mathbf{U}^{2}+\mathbf{V}^{2}+2 \mathbf{U} \mathbf{V} \\
& \leqq\|\mathbf{U}\|^{2}+\|\mathbf{V}\|^{2}+2\|\mathbf{U}\|\|\mathbf{V}\|=(\|\mathbf{U}\|+\|\mathbf{V}\|)^{2}
\end{aligned}
$$

from which (iii) follows at once.

\section*{ANGLE BETWEEN CONTRAVARIANT VECTORS}
5.15 Show that the angle between contravariant vectors is an invariant under a change of coordinate systems.

The defining expression (5.1) involves only inner products, which are invariants.

5.16 In $\mathbf{R}^{2}$ the family of curves $x^{2}=x^{1}-c$ (parameterized as $x^{1}=t, x^{2}=t-c$ ), has as its system of tangent vectors the vector field $\mathbf{U}=(1,1)$, constant throughout $\mathbf{R}^{2}$. If $\left(x^{i}\right)$ represent polar coordinates, find the family of orthogonal trajectories, and interpret geometrically.

The metric is given by

$$
g=\left[\begin{array}{cc}
1 & 0 \\
0 & \left(x^{1}\right)^{2}
\end{array}\right]
$$

so, by Theorem 5.3, the orthogonality condition becomes

$$
g_{i j} U^{i} \frac{d x^{j}}{d u}=(1)(1) \frac{d x^{1}}{d u}+\left(x^{1}\right)^{2}(1) \frac{d x^{2}}{d u}=0
$$

or, eliminating the differential $d u$,

$$
d x^{1}+\left(x^{1}\right)^{2} d x^{2}=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-073}
\end{center}

Fig. 5-5

This is a variables-separable differential equation, whose solution is

$$
x^{1}=\frac{1}{x^{2}+d}
$$

The given family of curves in the usual polar-coordinate notation is $r=\theta+c$, which is a family of concentric spirals (solid curves in Fig. 5-5). The orthogonal trajectories,

$$
r=\frac{1}{\theta+d}
$$

are also spirals, each having an asymptote parallel to the line $\theta=-d$; these are the dashed curves in Fig. 5-5.

Note: To solve this problem in rectangular coordinates-that is, to find the orthogonal trajectories of the family

$$
\frac{y}{x}=\tan \left(\sqrt{x^{2}+y^{2}}-c\right)
$$

under the metric $\left(g_{i j}\right)=\left(\delta_{i j}\right)$-would be difficult or impossible. Quite often, the complication of the metric involved in going over to a specialized curvilinear coordinate system is vastly outweighed by the degree to which the problem is simplified.

5.17 Find the condition for two curves on a sphere of radius $a$ to be orthogonal, if the curves are represented in spherical coordinates by

$$
\mathscr{C}_{1}: \theta=f(\varphi) \quad \text { and } \quad \mathscr{C}_{2}: \theta=g(\varphi)
$$

The two curves can be parameterized in spherical coordinates $\left(x^{i}\right) \equiv(\rho, \varphi, \theta)$ by

$$
\mathscr{C}_{1}:\left\{\begin{array}{l}
\rho=a \\
\varphi=t \\
\theta=f(t)
\end{array} \quad \mathscr{C}_{2}:\left\{\begin{array}{l}
\rho=a \\
\varphi=u \\
\theta=g(u)
\end{array}\right.\right.
$$

At an intersection point $\left(a, \varphi_{0}, \theta_{0}\right)$ the tangent vectors of $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$ are, respectively,

$$
\mathbf{U}=\left(0,1, f^{\prime}\left(\varphi_{0}\right)\right) \quad \text { and } \quad \mathbf{V}=\left(0,1, g^{\prime}\left(\varphi_{0}\right)\right)
$$

These are orthogonal if and only if $g_{i j} U^{i} V^{j}=0$, or

$$
\begin{aligned}
0 & =\left[\begin{array}{lll}
0 & 1 & f^{\prime}\left(\varphi_{0}\right)
\end{array}\right]\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & a^{2} & 0 \\
0 & 0 & \left(a \sin \varphi_{0}\right)^{2}
\end{array}\right]\left[\begin{array}{c}
0 \\
1 \\
g^{\prime}\left(\varphi_{0}\right)
\end{array}\right] \\
& =0+a^{2}+\left(a \sin \varphi_{0}\right)^{2} f^{\prime}\left(\varphi_{0}\right) g^{\prime}\left(\varphi_{0}\right)=\left(a^{2} \sin ^{2} \varphi_{0}\right)\left[\csc ^{2} \varphi_{0}+f^{\prime}\left(\varphi_{0}\right) g^{\prime}\left(\varphi_{0}\right)\right]
\end{aligned}
$$

Hence, the desired criterion is that $f^{\prime}\left(\varphi_{0}\right) g^{\prime}\left(\varphi_{0}\right)=-\csc ^{2} \varphi_{0}$ at any intersection point $\left(a, \varphi_{0}, \theta_{0}\right)$.

5.18 Show that the contravariant vectors $\mathbf{U}=\left(0,1,2 b x^{2}\right)$ and $\mathbf{V}=\left(0,-2 b x^{2},\left(x^{1}\right)^{2}\right)$ are orthogonal under the Euclidean metric for cylindrical coordinates. Interpret geometrically along $x^{1}=a$, $x^{2}=t, x^{3}=b t^{2}$.

$$
\begin{aligned}
g_{i j} U^{i} V^{j} & =\left[\begin{array}{lll}
0 & 1 & 2 b x^{2}
\end{array}\right]\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{c}
0 \\
-2 b x^{2} \\
\left(x^{1}\right)^{2}
\end{array}\right]=\left[\begin{array}{lll}
0 & 1 & 2 b x^{2}
\end{array}\right]\left[\begin{array}{c}
0 \\
-2 b x^{2}\left(x^{1}\right)^{2} \\
\left(x^{1}\right)^{2}
\end{array}\right] \\
& =0-2 b x^{2}\left(x^{1}\right)^{2}+2 b x^{2}\left(x^{1}\right)^{2}=0
\end{aligned}
$$

The geometric interpretation is that $x^{1}=a, x^{2}=t, x^{3}=b t^{2}$, for real $t$, represents a sort of variable-pitch helix on the right circular cylinder $r=a$, having tangent field $\mathbf{U}$. Therefore, any solution of


\begin{equation*}
\underbrace{\frac{d x^{1}}{d u}=V^{1}=0}_{\text {or } x^{1}=a} \quad \frac{d x^{2}}{d u}=V^{2}=-2 b x^{2} \quad \frac{d x^{3}}{d u}=V^{3}=a^{2} \tag{1}
\end{equation*}


will represent a curve on that cylinder that is orthogonal to this pseudo-helix. See Problem 5.28.

5.19 Show that in any coordinate system $\left(x^{i}\right)$ the contravariant vector (recall Problem 3.5) $\mathbf{V} \equiv\left(g^{i \alpha}\right)$ is normal to the surface $x^{\alpha}=$ const. $(\alpha=1,2, \ldots, n)$.

Being "normal to a surface at the surface point $P$ " means being orthogonal, at $P$, to the tangent vector of any curve lying in the surface and passing through $P$. Now, for the surface $x^{\alpha}=$ const., any such tangent vector $\mathbf{T}$ has as its $\alpha$ th component

$$
T^{\alpha}=\frac{d x^{\alpha}}{d t}=0
$$

We then have:

$$
\mathbf{V T} \equiv g_{i j} V^{i} T^{j}=g_{i j} g^{i \alpha} T^{j}=g_{j i} g^{i \alpha} T^{j}=\delta_{j}^{\alpha} T^{j}=T^{\alpha}=0
$$

and the proof is complete.

5.20 Show that in any coordinate system $\left(x^{i}\right)$, the angle $\theta$ between the normals to the surfaces $x^{\alpha}=$ const. and $x^{\beta}=$ const. is given by


\begin{equation*}
\cos \theta=\frac{g^{\alpha \beta}}{\sqrt{g^{\alpha \alpha}} \sqrt{g^{\beta \beta}}} \quad \text { (no sum) } \tag{1}
\end{equation*}


By Problem 5.19, $\mathbf{U}=\left(g^{i \alpha}\right)$ and $\mathbf{V}=\left(g^{i \beta}\right)$ are the respective normals to $x^{\alpha}=$ const. and $x^{\beta}=$ const. Therefore, by the definition (5.11)

$$
\cos \theta=\frac{U V}{\|U\|\|V\|}=\frac{g_{i j} g^{i \alpha} g^{j \beta}}{\sqrt{g_{p q} g^{p \alpha} g^{q \alpha}} \sqrt{g_{r s} g^{r \beta} g^{s \beta}}}=\frac{\delta_{j}^{\alpha} g^{j \beta}}{\sqrt{\delta_{q}^{\alpha} g^{q \alpha}} \sqrt{\delta_{s}^{\beta} g^{s \beta}}}=\frac{g^{\alpha \beta}}{\sqrt{g^{\alpha \alpha}} \sqrt{g^{\beta \beta}}}
$$

In consequence of (1), orthogonal coordinates are defined as those coordinate systems $\left(x^{i}\right)$ relative to which, at all points, $g^{i j}=0 \quad(i \neq j)$, or, equivalently, $g_{i j}=0 \quad(i \neq j)$. Obviously, orthogonal coordinates need not be rectangular: witness polar, cylindrical, and spherical coordinates.

\section*{Supplementary Problems}
5.21 Using the Euclidean metric for polar coordinates, compute the length of arc for the curve

$$
\mathscr{C}: x^{1}=2 a \cos t, \quad x^{2}=t \quad(0 \leqq t \leqq \pi / 2)
$$

and interpret geometrically.

5.22 Is the form $Q\left(u^{1}, u^{2}, u^{3}\right) \equiv 8\left(u^{1}\right)^{2}+\left(u^{2}\right)^{2}-6 u^{1} u^{3}+\left(u^{3}\right)^{2}$ positive definite?

5.23 Using the metric

$$
G=\left[\begin{array}{rrc}
12 & 4 & 0 \\
4 & 1 & 1 \\
0 & 1 & \left(x^{1}\right)^{2}
\end{array}\right]
$$

calculate the length of the curve given by $x^{1}=3-t, x^{2}=6 t+3, x^{3}=\ln t$, where $1 \leqq t \leqq e$.

5.24 A draftsman calculated several distances between points on his drawing using a set of vertical lines and his $\mathrm{T}$-square. He obtained the distance from $(1,2)$ to $(4,6)$ the usual way:

$$
\sqrt{(4-1)^{2}+(6-2)^{2}}=5
$$

Then he noticed his T-square was out several degrees, throwing off all measurements. An accurate reading showed his T-square measured $95.8^{\circ}$. Find, to three decimal places, the error committed in his calculations for the answer 5 obtained above. [Hint: Use Problem 5.9 in the special case $x_{1}^{3}=x_{2}^{3}=0$, with $\alpha=95.8^{\circ}$.]

5.25 In curvilinear coordinates $\left(x^{i}\right)$, show that the contravariant vectors

$$
\mathbf{U}=\left(-x^{1} / x^{2}, 1,0\right) \quad \mathbf{V}=\left(1 / x^{2}, 0,0\right)
$$

are an orthonormal pair, if $\left(x^{i}\right)$ is related to rectangular coordinates $\left(\bar{x}^{i}\right)$ through

$$
\bar{x}^{1}=x^{2} \quad \bar{x}^{2}=x^{3} \quad \bar{x}^{3}=x^{1} x^{2}
$$

where $x^{2} \neq 0$.

5.26 Express in $\left(x^{i}\right)$ the covariant vectors associated with $\mathbf{U}$ and $\mathbf{V}$ of Problem 5.25.

5.27 Even though $\left(g_{i j}\right)$ may define a non-Euclidean metric, prove that the norm (5.10) still obeys the following "Euclidean" laws: $(a)$ the law of cosines, $(b)$ the Pythagorean theorem.

5.28 (a) Solve system (1) of Problem 5.18. (b) Does the solution found in (a) include all curves orthogonal to the given pseudo-helix? Explain.

5.29 Find the family of orthogonal trajectories in polar coordinates for the family of spirals $x^{1}=c x^{2} \quad(c=$ const.). [Hint: Parameterize the family as $x^{1}=c e^{t}, x^{2}=e^{t}$.]

5.30 Find the condition for two curves, $z=f(\theta)$ and $z=g(\theta)$, on a right circular cylinder of radius $a$ to be orthogonal.

5.31 Let $\left(x^{i}\right)$ be any coordinate system and $\left(g_{i j}\right)$ any positive definite metric tensor realized in that system. Define the coordinate axes as the curves $\mathscr{C}_{\alpha}: x^{i}=t \delta_{\alpha}^{i} \quad(\alpha=1,2, \ldots n)$. Show that the angle $\phi$ between the coordinate axes $\mathscr{C}_{\alpha}$ and $\mathscr{C}_{\beta}$ satisfies the relation

$$
\cos \phi=\frac{g_{\alpha \beta}}{\sqrt{g_{\alpha \alpha}} \sqrt{g_{\beta \beta}}} \quad \text { (no sum) }
$$

and is thus distinct, in general, from the angle $\theta$ of Problem 5.20.

5.32 Refer to Problems 5.20 and 5.31. (a) What property must the metric tensor $\left(g_{i j}\right)$ possess in $\left(x^{i}\right)$ for the coordinate axis $\mathscr{C}_{\alpha}$ to be normal to the surface $x^{\alpha}=$ const. (in which case $\theta=\phi$ )? $(b)$ Show that the property of $(a)$ is equivalent to the mutual orthogonality of the coordinate axes.

5.33 Under the metric

$$
G=\left[\begin{array}{cc}
1 & \cos 2 x^{2} \\
\cos 2 x^{2} & 1
\end{array}\right] \quad\left(2 x^{2} / \pi \text { nonintegral }\right)
$$

compute the norm of the vector $\mathbf{V}=\left(d x^{i} / d t\right)$ evaluated along the curve $x^{1}=-\sin 2 t, x^{2}=t$, and use it to find the arc length between $t=0$ and $t=\pi / 2$.

5.34 Under the Euclidean metric for spherical coordinates, (5.4), determine a particular family of curves that intersect

$$
x^{1}=a \quad x^{2}=b t \quad x^{3}=t
$$

orthogonally. (Cf. Problem 5.28.)

\section*{Chapter 6}
\section*{The Derivative of a Tensor}
\subsection*{6.1 INADEQUACY OF ORDINARY DIFFERENTIATION}
Consider a contravariant tensor $\mathbf{T}=\left(T^{i}(\mathbf{x}(t))\right)$ defined on the curve $\mathscr{C}: \mathbf{x}=\mathbf{x}(t)$. Differentiating the transformation law

$$
\bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}
$$

with respect to $t$ gives

$$
\frac{d \bar{T}^{i}}{d t}=\frac{d T^{r}}{d t} \frac{\partial \bar{x}^{i}}{\partial x^{r}}+T^{r} \frac{\partial^{2} \bar{x}^{i}}{\partial x^{s} \partial x^{r}} \frac{d x^{s}}{d t}
$$

which shows that the ordinary derivative of $\mathbf{T}$ along the curve is a contravariant tensor when and only when the $\bar{x}^{i}$ are linear functions of the $x^{r}$.

Theorem 6.1: The derivative of a tensor is a tensor if and only if coordinate changes are restricted to linear transformations.

EXAMPLE 6.1 With $\mathbf{T}=d \mathbf{x} / d t$ the tangent field along $\mathscr{C}$ (under the choice $t=s=$ arc length), the classical formula for the curvature of $\mathscr{C}$,

$$
\kappa=\left\|\frac{d \mathbf{T}}{d t}\right\|
$$

will hold in affine coordinates but will fail to define an invariant in curvilinear coordinates, since $d \mathbf{T} / d t$ is not a general tensor. Clearly, to make the curvature of $\mathscr{C}$ an intrinsic property, we require a more general concept of tensor differentiation. This will entail the introduction of some complicated, nontensorial objects called Christoffel symbols.

\subsection*{6.2 CHRISTOFFEL SYMBOLS OF THE FIRST KIND}
\section*{Definition and Basic Properties}
The $n^{3}$ functions


\begin{equation*}
\Gamma_{i j k} \equiv \frac{1}{2}\left[\frac{\partial}{\partial x^{i}}\left(g_{j k}\right)+\frac{\partial}{\partial x^{j}}\left(g_{k i}\right)-\frac{\partial}{\partial x^{k}}\left(g_{i j}\right)\right] \tag{6.1a}
\end{equation*}


are the Christoffel symbols of the first kind. In order to simplify the notation here and elsewhere, we shall adopt the following convention: The partial derivative of a tensor with respect to $x^{k}$ will be indicated by a final subscript $k$. Thus,


\begin{equation*}
\Gamma_{i j k} \equiv \frac{1}{2}\left(-g_{i j k}+g_{j k i}+g_{k i j}\right) \tag{6.1b}
\end{equation*}


EXAMPLE 6.2 Compute the Christoffel symbols corresponding to the Euclidean metric for spherical coordinates:

$$
G=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & \left(x^{1}\right)^{2} \sin ^{2} x^{2}
\end{array}\right]
$$

Here, $g_{221}=2 x^{1}, g_{331}=2 x^{1} \sin ^{2} x^{2}, g_{332}=2\left(x^{1}\right)^{2} \sin x^{2} \cos x^{2}$, and all other $g_{i j k}$ are zero. Hence, $\Gamma_{i j k}=0$ unless the triplet $i j k$ includes precisely two $2 \mathrm{~s}$ (six cases) or precisely two $3 \mathrm{~s}$ (six cases):

$$
\begin{aligned}
-\Gamma_{221}=\frac{1}{2}\left(-g_{221}+g_{212}+g_{122}\right)=-x^{1} & \bullet \Gamma_{212}=\frac{1}{2}\left(-g_{212}+g_{122}+g_{221}\right)=x^{1} & \bullet \Gamma_{122}=\frac{1}{2}\left(-g_{122}+g_{221}+g_{212}\right)=x^{1} \\
\Gamma_{223}=\frac{1}{2}\left(-g_{223}+g_{232}+g_{322}\right)=0 & \Gamma_{232}=\frac{1}{2}\left(-g_{232}+g_{322}+g_{223}\right)=0 & \Gamma_{322}=\frac{1}{2}\left(-g_{322}+g_{223}+g_{233}\right)=0
\end{aligned}
$$

and

\begin{itemize}
  \item $\Gamma_{331}=\frac{1}{2}\left(-g_{331}+g_{313}+g_{133}\right)=-x^{1} \sin ^{2} x^{2}$
  \item $\Gamma_{323}=\frac{1}{2}\left(-g_{323}+g_{233}+g_{332}\right)=\left(x^{1}\right)^{2} \sin x^{2} \cos x^{2}$
  \item $\Gamma_{332}=\frac{1}{2}\left(-g_{332}+g_{323}+g_{233}\right)=-\left(x^{1}\right)^{2} \sin x^{2} \cos x^{2}$
  \item $\Gamma_{133}=\frac{1}{2}\left(-g_{133}+g_{331}+g_{313}\right)=x^{1} \sin ^{2} x^{2}$
  \item $\Gamma_{313}=\frac{1}{2}\left(-g_{313}+g_{133}+g_{331}\right)=x^{1} \sin ^{2} x^{2}$
  \item $\Gamma_{233}=\frac{1}{2}\left(-g_{233}+g_{332}+g_{323}\right)=\left(x^{1}\right)^{2} \sin x^{2} \cos x^{2}$
\end{itemize}

(The nine nonzero symbols are marked with bullets.)

The two basic properties of the Christoffel symbols of the first kind are:

(i) $\Gamma_{i j k}=\Gamma_{j i k}$ (symmetry in the first two indices)

(ii) all $\Gamma_{i j k}$ vanish if all $g_{i j}$ are constant

A useful formula results from simply permuting the subscripts in (6.1b) and summing:


\begin{equation*}
\frac{\partial g_{i k}}{\partial x^{j}}=\Gamma_{i j k}+\Gamma_{j k i} \tag{6.2}
\end{equation*}


The converse of property (ii) follows at once from (6.2); thus:

Lemma 6.2: In any particular coordinate system, the Christoffel symbols uniformly vanish if and only if the metric tensor has constant components in that system.

\section*{Transformation Law}
The transformation law for the $\Gamma_{i j k}$ can be inferred from that for the $g_{i j}$. By differentiation,

$$
\bar{g}_{i j k}=\frac{\partial}{\partial \bar{x}^{k}}\left(g_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}\right)=\frac{\partial g_{r s}}{\partial \bar{x}^{k}} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}+g_{r s} \frac{\partial^{2} x^{r}}{\partial \bar{x}^{k} \partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}+g_{r s} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial^{2} x^{s}}{\partial \bar{x}^{k} \partial \bar{x}^{j}}
$$

Use the chain rule on $\partial g_{r s} / \partial \bar{x}^{k}$ :

$$
\frac{\partial g_{r s}}{\partial \bar{x}^{\bar{k}}}=\frac{\partial g_{r s}}{\partial x^{t}} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \equiv g_{r s t} \frac{\partial x^{t}}{\partial \bar{x}^{k}}
$$

Then rewrite the expression with subscripts permuted cyclically, sum the three expressions (arrows couple terms which cancel out), and divide by 2 :

$$
\begin{aligned}
& -\bar{g}_{i j k}=-g_{r s t} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}+g_{r s}\left(-\frac{\partial^{2} x^{r}}{\partial \bar{x}^{k} \partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}-\frac{\partial^{2} x^{s}}{\partial \bar{x}^{k} \partial \bar{x}^{j}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}\right) \\
& \bar{g}_{j k i}=g_{s t r} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}+g_{s r}\left(\frac{\partial^{2} x^{s}}{\partial \bar{x}^{i} \partial \bar{x}^{j}} \frac{\partial \bar{x}^{r}}{\partial \bar{x}^{k}}+\frac{\partial^{2} x^{r}}{\partial \bar{x}^{i} \partial \bar{x}^{k}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}\right) \\
& \bar{g}_{k i j}=g_{t r s} \frac{\partial x^{t}}{\partial \bar{x}^{k}} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}+g_{s r}\left(\frac{\partial^{2} x^{s}}{\partial \bar{x}^{j} \partial \bar{x}^{k}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}+\frac{\partial^{2} x^{r}}{\partial \bar{x}^{j} \partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}\right)
\end{aligned}
$$

give


\begin{equation*}
\bar{\Gamma}_{i j k}=\Gamma_{r s t} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}+g_{r s} \frac{\partial^{2} x^{r}}{\partial \bar{x}^{i} \partial \bar{x}^{j}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \tag{6.3}
\end{equation*}


From the form of (6.3) it is clear that the set of Christoffel symbols is a third-order covariant affine tensor but is not a general tensor. Here again, conventional differentiation-this time, partial differentiation with respect to a coordinate-fails to produce more than an affine tensor (recall Problem 2.23).

\subsection*{6.3 CHRISTOFFEL SYMBOLS OF THE SECOND KIND}
\section*{Definition and Basic Properties}
The $n^{3}$ functions


\begin{equation*}
\Gamma_{j k}^{i}=g^{i r} \Gamma_{j k r} \tag{6.4}
\end{equation*}


are the Christoffel symbols of the second kind. It should be noted that formula (6.4) is simply the result of raising the third subscript of the Christoffel symbol of the first kind, although here we are not dealing with tensors.

EXAMPLE 6.3 Calculate the Christoffel symbols of the second kind for the Euclidean metric in polar coordinates.

Since

$$
G=\left[\begin{array}{cc}
1 & 0 \\
0 & \left(x^{1}\right)^{2}
\end{array}\right]
$$

we have:

$$
\begin{gathered}
\Gamma_{111}=\frac{1}{2} g_{111}=0 \quad \Gamma_{121}=\Gamma_{211}=\frac{1}{2}\left(-g_{121}+g_{211}+g_{112}\right)=0 \\
\text { - } \Gamma_{221}=\frac{1}{2}\left(-g_{221}+g_{212}+g_{122}\right)=-x^{1} \quad \Gamma_{112}=\frac{1}{2}\left(-g_{112}+g_{121}+g_{211}\right)=0 \\
\text { - } \Gamma_{122}=\Gamma_{212}=\frac{1}{2}\left(-g_{122}+g_{221}+g_{212}\right)=x^{1} \quad \Gamma_{222}=\frac{1}{2} g_{222}=0
\end{gathered}
$$

To continue,

$$
G^{-1}=\left[\begin{array}{cc}
1 & 0 \\
0 & \left(x^{1}\right)^{-2}
\end{array}\right]
$$

From $g_{12}=0=g_{21}$, it follows that $\Gamma_{j k}^{i}=g^{i r} \Gamma_{j k r}=g^{i i} \Gamma_{j k i}$ (no sum). Therefore, when $i=1$,

$$
\text { - } \Gamma_{22}^{1}=-x^{1} \quad \Gamma_{j k}^{1}=0 \quad \text { otherwise }
$$

and when $i=2$,

$$
\text { - } \Gamma_{12}^{2}=\Gamma_{21}^{2}=1 / x^{1} \quad \Gamma_{j k}^{2}=0 \quad \text { otherwise }
$$

The basic properties of $\Gamma_{i j k}$ carry over to $\Gamma_{j k}^{i}$ :

(i) $\Gamma_{j k}^{i}=\Gamma_{k j}^{i}$ (symmetry in the lower indices)

(ii) all $\Gamma_{j k}^{i}$ vanish if all $g_{i j}$ are constant

Furthermore, by Problem 6.25, Lemma 6.2 holds for both first and second kinds of Christoffel symbols.

\section*{Transformation Law}
Starting with

$$
\bar{\Gamma}_{j k}^{i}=\bar{g}^{i r} \bar{\Gamma}_{j k r}=\left(g^{s t} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial \bar{x}^{r}}{\partial x^{t}}\right) \bar{\Gamma}_{j k r}
$$

substitute for $\bar{\Gamma}_{j k r}$ from (6.3) to obtain

$$
\begin{aligned}
\bar{\Gamma}_{j k}^{i} & =\left(g^{s t} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial \bar{x}^{r}}{\partial x^{t}}\right)\left(\Gamma_{u v w} \frac{\partial x^{u}}{\partial \bar{x}^{j}} \frac{\partial x^{v}}{\partial \bar{x}^{k}} \frac{\partial x^{w}}{\partial \bar{x}^{r}}\right)+\left(g^{s t} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial \bar{x}^{r}}{\partial x^{t}}\right)\left(g_{u v} \frac{\partial^{2} x^{u}}{\partial \bar{x}^{j} \partial \bar{x}^{k}} \frac{\partial x^{v}}{\partial \bar{x}^{r}}\right) \\
& =g^{s t} \Gamma_{u v w} \delta_{t}^{w} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial x^{u}}{\partial \bar{x}^{j}} \frac{\partial x^{v}}{\partial \bar{x}^{k}}+g^{s t} g_{u v} \delta_{t}^{v} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial^{2} x^{u}}{\partial \bar{x}^{j} \partial \bar{x}^{k}} \\
& =g^{s t} \Gamma_{u v t} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial x^{u}}{\partial \bar{x}^{j}} \frac{\partial x^{v}}{\partial \bar{x}^{k}}+g^{s t} g_{u t} \frac{\partial \bar{x}^{i}}{\partial x^{s}} \frac{\partial^{2} x^{u}}{\partial \bar{x}^{j} \partial \bar{x}^{k}}
\end{aligned}
$$

Since $g^{s t} \Gamma_{u v t}=\Gamma_{u v}^{s}$ and $g^{s t} g_{u t}=\delta_{u}^{s}$, after changing indices this becomes


\begin{equation*}
\bar{\Gamma}_{j k}^{i}=\Gamma_{s t}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}+\frac{\partial^{2} x^{r}}{\partial \bar{x}^{j} \partial \bar{x}^{k}} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \tag{6.5}
\end{equation*}


The transformation law (6.5) shows that, like $\left(\Gamma_{i j k}\right),\left(\Gamma_{j k}^{i}\right)$ is merely an affine tensor.

An Important Formula


\begin{equation*}
\frac{\partial^{2} x^{r}}{\partial \bar{x}^{i} \partial \bar{x}^{j}}=\bar{\Gamma}_{i j}^{s} \frac{\partial x^{r}}{\partial \bar{x}^{s}}-\Gamma_{s t}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \frac{\partial x^{t}}{\partial \bar{x}^{j}} \tag{6.6}
\end{equation*}


See Problem 6.24. Needless to say, (6.6) holds when barred and unbarred coordinates are interchanged.

\subsection*{6.4 COVARIANT DIFFERENTIATION}
Of a Vector

Partial differentiation of the transformation law

of a covariant vector $\mathbf{T}=\left(T_{i}\right)$ yields

$$
\bar{T}_{i}=T_{r} \frac{\partial x^{r}}{\partial \bar{x}^{i}}
$$

$$
\frac{\partial \bar{T}_{i}}{\partial \bar{x}^{k}}=\frac{\partial T_{r}}{\partial \bar{x}^{k}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}+T_{r} \frac{\partial^{2} x^{r}}{\partial \bar{x}^{k} \partial \bar{x}^{i}}
$$

Using the chain rule on the first term on the right, and formula (6.6) on the second, results in the equations

$$
\begin{aligned}
\frac{\partial \bar{T}_{i}}{\partial \bar{x}^{k}} & =\frac{\partial T_{r}}{\partial x^{s}} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}+T_{r}\left(\bar{\Gamma}_{i k}^{s} \frac{\partial x^{r}}{\partial \bar{x}^{s}}-\Gamma_{s t}^{r} \frac{\partial x^{s}}{\partial \bar{x}^{i}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}\right) \\
& =\frac{\partial T_{r}}{\partial x^{s}} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}+\bar{\Gamma}_{i k}^{t} \bar{T}_{t}-\Gamma_{r s}^{t} T_{t} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}
\end{aligned}
$$

which rearrange to

$$
\frac{\partial \bar{T}_{i}}{\partial \bar{x}^{k}}-\bar{\Gamma}_{i k}^{t} \bar{T}_{t}=\left(\frac{\partial T_{r}}{\partial x^{s}}-\Gamma_{r s}^{t} T_{t}\right) \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}
$$

which is the defining law of a covariant tensor of order two. In other words, when the components of $\partial \mathbf{T} / \partial x^{k}$ are corrected by subtracting certain linear combinations of the components of $\mathbf{T}$ itself, the result is a tensor (and not just an affine tensor).

Definition 1: In any coordinate system $\left(x^{i}\right)$, the covariant derivative with respect to $x^{k}$ of a covariant vector $\mathbf{T}=\left(T_{i}\right)$ is the tensor

$$
\mathbf{T}_{, k}=\left(T_{i, k}\right) \equiv\left(\frac{\partial T_{i}}{\partial x^{k}}-\Gamma_{i k}^{t} T_{t}\right)
$$

Remark 1: The two covariant indices are notated $i$ and,$k$ to emphasize that the second index arose from an operation with respect to the $\mathrm{k}$ th coordinate.

Remark 2: From Lemma 6.2, the covariant derivative and the partial derivative coincide when the $g_{i j}$ are constants (as in a rectangular coordinate system).

A similar manipulation (Problem 6.7) of the contravariant vector law leads to

Definition 2: In any coordinate system $\left(x^{i}\right)$, the covariant derivative with respect to $x^{k}$ of a contravariant vector $\mathbf{T}=\left(T^{i}\right)$ is the tensor

$$
\mathbf{T}_{, k}=\left(T_{, k}^{i}\right) \equiv\left(\frac{\partial T^{i}}{\partial x^{k}}+\Gamma_{t k}^{i} T^{t}\right)
$$

\section*{Of Any Tensor}
In the general definition, each covariant index (subscript) gives rise to a linear "correction term" of the form given in Definition 1, and each contravariant index (superscript) gives rise to a term of the form given in Definition 2.

Definition 3: In any coordinate system $\left(x^{i}\right)$, the covariant derivative with respect to $x^{k}$ of a tensor $\mathbf{T}=\left(T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}\right)$ is the tensor $\mathbf{T}_{, k}=\left(T_{j_{1} j_{2} \ldots j_{q}, k}^{i_{1} i_{2} \ldots i_{p}}\right)$, where


\begin{gather*}
T_{j_{1} j_{2} \ldots j_{q}, k}^{i_{1} i_{2} \ldots i_{p}}=\frac{\partial T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}+\Gamma_{t k}^{i_{1}} T_{j_{1} j_{2} \ldots j_{q}}^{t i_{2} \ldots i_{p}}+\Gamma_{t k}^{i_{2}} T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} t \ldots i_{p}}+\cdots+\Gamma_{t k}^{i_{p}} T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots t}}{\partial x^{k}}+\Gamma_{j_{1} k}^{t} T_{t_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}-\Gamma_{j_{2} k}^{t} T_{j_{1} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p}}-\cdots-\Gamma_{j_{q} k}^{t} T_{j_{1} j_{2} \ldots t}^{i_{1} i_{2} \ldots i_{p}}
\end{gather*}


That $\mathbf{T}_{k}$ actually is a tensor must, of course, be proved. This can be accomplished basically as in Problem 6.8, by use of Theorem 4.2 and an induction on the number of indices.

Theorem 6.3: The covariant derivative of an arbitrary tensor is a tensor of which the covariant order exceeds that of the original tensor by exactly one.

\subsection*{6.5 ABSOLUTE DIFFERENTIATION ALONG A CURVE}
Because $\left(T_{, j}^{i}\right)$ is a tensor, the inner product of $\left(T_{, j}^{i}\right)$ with another tensor is also a tensor. Suppose that the other tensor is $\left(d x^{i} / d t\right)$, the tangent vector of the curve $\mathscr{C}: x^{i}=x^{i}(t)$. Then the inner product

$$
\left(T_{, r}^{i} \frac{d x^{r}}{d t}\right)
$$

is a tensor of the same type and order as the original tensor $\left(T^{i}\right)$. This tensor is known as the absolute derivative of $\left(T^{i}\right)$ along $\mathscr{C}$, with components written as


\begin{equation*}
\left(\frac{\delta T^{i}}{\delta t}\right) \equiv\left(\frac{d T^{i}}{d t}+\Gamma_{r s}^{i} T^{r} \frac{d x^{s}}{d t}\right) \quad \text { where } \quad T^{i}=T^{i}(\mathbf{x}(t)) \tag{6.8}
\end{equation*}


(see Problem 6.12). It is clear that, again, in coordinate systems in which the $g_{i j}$ are constant, absolute differentiation reduces to ordinary differentiation.

The definition (6.8) is not an arbitrary one; in Problem 6.18 is proved

Theorem 6.4 (Uniqueness of the Absolute Derivative): The only tensor derivable from a given tensor $\left(T^{i}\right)$ that coincides with the ordinary derivative ( $\left.d T^{i} / d t\right)$ along some curve in a rectangular coordinate system is the absolute derivative of $\left(T^{i}\right)$ along that curve.

Remark 3: Theorem 6.4 concerns tensors with a given form in rectangular coordinates. Thus it presumes the Euclidean metric (see Section 3.1).

\section*{Acceleration in General Coordinates}
In rectangular coordinates, the acceleration vector of a particle is the time derivative of its velocity vector, or the second time derivative of its position function $\mathbf{x}=\left(x^{i}(t)\right)$ :

$$
\mathbf{a}=\left(a^{i}\right) \equiv\left(\frac{d}{d t} \frac{d x^{i}}{d t}\right)=\left(\frac{d^{2} x^{i}}{d t^{2}}\right)
$$

The (Euclidean) length of this vector at time $t$ is the instantaneous acceleration of the particle:

$$
a=\sqrt{\delta_{i j} a^{i} a^{j}}
$$

Since derivatives are taken along the particle's trajectory, the natural generalization of $\frac{d}{d t}\left(\frac{d x^{i}}{d t}\right)$

is

$$
\frac{\delta}{\delta t}\left(\frac{d x^{i}}{d t}\right)=\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{r s}^{i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}
$$

Hence, in general coordinates, we take as the acceleration vector and the acceleration


\begin{gather*}
\mathbf{a}=\left(a^{i}\right) \equiv\left(\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{r s}^{i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}\right)  \tag{6.9}\\
a=\sqrt{\left|g_{i j} a^{i} a^{j}\right|} \tag{6.10}
\end{gather*}


Note that positive-definiteness of the metric is not assumed in (6.10).

\section*{Curvature in General Coordinates}
In Euclidean geometry an important role is played by the curvature of a curve $\mathscr{C}: x^{i}=x^{i}(t)$, commonly defined as the norm of the second derivative of $\left(x^{i}(s)\right)$ :

$$
\kappa(s)=\sqrt{\delta_{i j} \frac{d^{2} x^{i}}{d s^{2}} \frac{d^{2} x^{j}}{d s^{2}}}
$$

where $d s / d t=\sqrt{\delta_{i j}\left(d x^{i} / d t\right)\left(d x^{i} / d t\right)}$ gives the arc-length parameter. The obvious way to extend this concept as an invariant is again to use absolute differentiation. Writing


\begin{equation*}
\left(b^{i}\right) \equiv\left(\frac{\delta}{\delta s} \frac{d x^{i}}{d s}\right)=\left(\frac{d^{2} x^{i}}{d s^{2}}+\Gamma_{p q}^{i} \frac{d x^{p}}{d s} \frac{d x^{q}}{d s}\right) \tag{6.11}
\end{equation*}


where the arc-length parameter $s=s(t)$ is given by (5.6), we have:


\begin{equation*}
\kappa(s)=\sqrt{\left|g_{i j} b^{i} b^{j}\right|} \tag{6.12}
\end{equation*}


\section*{Geodesics}
An important application of (6.12) in curvilinear coordinates is the following. Suppose that we seek those curves for which $\kappa=0$ (that is, the "straight" lines or geodesics). For positive definite metrics, this condition is equivalent to requiring that


\begin{equation*}
b^{i}=\frac{d^{2} x^{i}}{d s^{2}}+\Gamma_{p q}^{i} \frac{d x^{p}}{d s} \frac{d x^{q}}{d s}=0 \quad(i=1,2, \ldots, n) \tag{6.13}
\end{equation*}


The solution of this system of second-order differential equations will define the geodesics $x^{i}=x^{i}(s)$.

EXAMPLE 6.4 In affine coordinates, where all $g_{i j}$ are constant and all Christoffel symbols vanish, integration of $(6.13)$ is immediate:

$$
x^{i}=\alpha^{i} s+\beta^{i} \quad(i=1,2, \ldots, n)
$$

where, $s$ being arc length, $g_{i j} \alpha^{i} \alpha^{j}=1$. Thus, from each point $\mathbf{x}=\boldsymbol{\beta}$ of space there emanates a geodesic ray in every direction (unit vector) $\boldsymbol{\alpha}$.

\subsection*{6.6 RULES FOR TENSOR DIFFERENTIATION}
Confidence in the preceding differentiation formulas should be considerably improved when it is learned (see Problem 6.15) that the same basic rules for differentiation from calculus carry over to covariant and absolute differentiation of tensors. For arbitrary tensors $\mathbf{T}$ and $\mathbf{S}$, we have:

\section*{Rules for Covariant Differentiation}
\$\$

\$\$

Since the absolute derivative along a curve is the inner product of the covariant derivative and the tangent vector, the above rules for differentiation repeat:

\section*{Rules for Absolute Differentiation}
$$
\begin{array}{ll} 
& \frac{\delta}{\delta t}(\mathbf{T}+\mathbf{S})=\frac{\delta \mathbf{T}}{\delta t}+\frac{\delta \mathbf{S}}{\delta t} \\
\text { outer product } & \frac{\delta}{\delta t}[\mathbf{T S}]=\left[\frac{\delta \mathbf{T}}{\delta t} \mathbf{S}\right]+\left[\mathbf{T} \frac{\delta \mathbf{S}}{\delta t}\right] \\
\text { inner product } & \frac{\delta}{\delta t}(\mathbf{T S})=\frac{\delta \mathbf{T}}{\delta t} \mathbf{S}+\mathbf{T} \frac{\delta \mathbf{S}}{\delta t}
\end{array}
$$

\section*{Solved Problems}
\section*{CHRISTOFFEL SYMBOLS OF THE FIRST KIND}
6.1 Verify that $\Gamma_{i j k}=\Gamma_{j i k}$.

By definition,

$$
\Gamma_{i j k}=\frac{1}{2}\left(-g_{i j k}+g_{j k i}+g_{k i j}\right) \quad \text { and } \quad \Gamma_{j i k}=\frac{1}{2}\left(-g_{j i k}+g_{i k j}+g_{k j i}\right)
$$

But $g_{i j k}=g_{j i k}, g_{j k i}=g_{k j i}$, and $g_{k i j}=g_{i k i}$, by symmetry of $g_{i j}$, and the result follows.

6.2 Show that if $\left(g_{i j}\right)$ is a diagonal matrix, then for all fixed subscripts $\alpha$ and $\beta \neq \alpha$ in the range $1,2, \ldots, n$,

(a) $\Gamma_{\alpha \alpha \alpha}=\frac{1}{2} g_{\alpha \alpha \alpha} \quad($ not summed on $\alpha$ )

(b) $-\Gamma_{\alpha \alpha \beta}=\Gamma_{\alpha \beta \alpha}=\Gamma_{\beta \alpha \alpha}=\frac{1}{2} g_{\alpha \alpha \beta} \quad$ (not summed on $\alpha$ )

(c) All remaining Christoffel symbols $\Gamma_{i j k}$ are zero.\\
(a) By definition, $\Gamma_{\alpha \alpha \alpha}=\frac{1}{2}\left(-g_{\alpha \alpha \alpha}+g_{\alpha \alpha \alpha}+g_{\alpha \alpha \alpha}\right)=\frac{1}{2} g_{\alpha \alpha \alpha}$.

(b) Since $\alpha \neq \beta$,

$$
\begin{aligned}
& -\Gamma_{\alpha \alpha \beta}=-\frac{1}{2}\left(-g_{\alpha \alpha \beta}+g_{\alpha \beta \alpha}+g_{\beta \alpha \alpha}\right)=-\frac{1}{2}\left(-g_{\alpha \alpha \beta}+0+0\right)=\frac{1}{2} g_{\alpha \alpha \beta} \\
& \Gamma_{\alpha \beta \alpha}=\Gamma_{\beta \alpha \alpha}=\frac{1}{2}\left(-g_{\alpha \beta \alpha}+g_{\beta \alpha \alpha}+g_{\alpha \alpha \beta}\right)=\frac{1}{2}\left(-0+0+g_{\alpha \alpha \beta}\right)=\frac{1}{2} g_{\alpha \alpha \beta}
\end{aligned}
$$

(c) Let $i, j, k$ be distinct subscripts. Then $g_{i j}=0$ and $g_{i j k}=0$, implying that $\Gamma_{i j k}=0$.

6.3 Is it true that if all $\Gamma_{i j k}$ vanish in some coordinate system, then the metric tensor has constant components in every coordinate system?

By Lemma 6.2, the conclusion would be valid if the $\Gamma_{i j k}$ vanished in every coordinate system. But $\left(\Gamma_{i j k}\right)$ is not a tensor, and the conclusion is false. For instance, all $\bar{\Gamma}_{i j k}=0$ for the Euclidean metric in rectangular coordinates, but $g_{22}=\left(x^{1}\right)^{2}$ in spherical coordinates.

\section*{CHRISTOFFEL SYMBOLS OF THE SECOND KIND}
6.4 If $\left(g_{i j}\right)$ is a diagonal matrix, show that for all fixed indices (no summation) in the range $1,2, \ldots, n$,

(a) $\Gamma_{\alpha \beta}^{\alpha}=\Gamma_{\beta \alpha}^{\alpha}=\frac{\partial}{\partial x^{\beta}}\left(\frac{1}{2} \ln \left|g_{\alpha \alpha}\right|\right)$

(b) $\Gamma_{\beta \beta}^{\alpha}=-\frac{1}{2 g_{\alpha \alpha}} g_{\beta \beta \alpha} \quad(\alpha \neq \beta)$

(c) All other $\Gamma_{j k}^{i}$ vanish.

(a) Both $\left(g_{i j}\right)$ and $\left(g_{i j}\right)^{-1}=\left(g^{i j}\right)$ are diagonal, with nonzero diagonal elements. Thus,


\begin{gather*}
\Gamma_{\alpha \beta}^{\alpha}=g^{\alpha j} \Gamma_{\alpha \beta j}=g^{\alpha \alpha} \Gamma_{\alpha \beta \alpha}=\frac{1}{g_{\alpha \alpha}}\left(\frac{1}{2} \frac{\partial g_{\alpha \alpha}}{\partial x^{\beta}}\right)=\frac{\partial}{\partial x^{\beta}}\left(\frac{1}{2} \ln \left|g_{\alpha \alpha}\right|\right) \\
\Gamma_{\beta \beta}^{\alpha}=g^{\alpha \alpha} \Gamma_{\beta \beta \alpha}=\frac{1}{g_{\alpha \alpha}}\left(-\frac{1}{2} g_{\beta \beta \alpha}\right) \tag{b}
\end{gather*}


(c) When $i, j, k$ are distinct, $\Gamma_{j k}^{i}=g^{i r} \Gamma_{j k r}=g^{i i} \Gamma_{j k i}=0$ (not summed on $i$ ).

6.5 Calculate the Christoffel symbols of the second kind for the Euclidean metric in spherical coordinates, using Problem 6.4.

We have $g_{11}=1, g_{22}=\left(x^{1}\right)^{2}$, and $g_{33}=\left(x^{1}\right)^{2} \sin ^{2} x^{2}$. Noting that $g_{11}$ is a constant and that all $g_{\alpha \alpha}$ are independent of $x^{3}$, we obtain the following nonzero symbols from Problem 6.4(a):

$$
\begin{aligned}
& \Gamma_{21}^{2}=\Gamma_{12}^{2}=\frac{\partial}{\partial x^{1}}\left(\frac{1}{2} \ln \left(x^{1}\right)^{2}\right)=\frac{1}{x^{1}} \\
& \Gamma_{31}^{3}=\Gamma_{13}^{3}=\frac{\partial}{\partial x^{1}}\left(\frac{1}{2} \ln \left(\left(x^{1}\right)^{2} \sin ^{2} x^{2}\right)\right)=\frac{1}{x^{1}} \\
& \Gamma_{32}^{3}=\Gamma_{23}^{3}=\frac{\partial}{\partial x^{2}}\left(\frac{1}{2} \ln \left(\left(x^{1}\right)^{2} \sin ^{2} x^{2}\right)\right)=\cot x^{2}
\end{aligned}
$$

Similarly, from Problem $6.4(b)$,

$$
\begin{aligned}
& \Gamma_{22}^{1}=-\frac{1}{2(1)} \frac{\partial}{\partial x^{1}}\left(x^{1}\right)^{2}=-x^{1} \\
& \Gamma_{33}^{1}=-\frac{1}{2(1)} \frac{\partial}{\partial x^{1}}\left(\left(x^{1}\right)^{2} \sin ^{2} x^{2}\right)=-x^{1} \sin ^{2} x^{2} \\
& \Gamma_{33}^{2}=-\frac{1}{2\left(x^{1}\right)^{2}} \frac{\partial}{\partial x^{2}}\left(\left(x^{1}\right)^{2} \sin ^{2} x^{2}\right)=-\sin x^{2} \cos x^{2}
\end{aligned}
$$

6.6 Use (6.6) to find the most general 3-dimensional transformation $x^{i}=x^{i}(\overline{\mathbf{x}})$ of coordinates such that $\left(x^{i}\right)$ is rectangular and $\left(\bar{x}^{i}\right)$ is any other coordinate system for which the Christoffel symbols are

$$
\bar{\Gamma}_{11}^{1}=1 \quad \bar{\Gamma}_{22}^{2}=2 \quad \bar{\Gamma}_{33}^{3}=3 \quad \text { all others }=0
$$

Since $\Gamma_{s t}^{r}=0$, (6.6) reduces to the system of linear partial differential equations with constant coefficients:


\begin{equation*}
\frac{\partial^{2} x^{r}}{\partial \bar{x}^{i} \partial \bar{x}^{j}}=\bar{\Gamma}_{i j}^{s} \frac{\partial x^{r}}{\partial \bar{x}^{s}} \tag{1}
\end{equation*}


It is simplest first to solve the intermediate, first-order system


\begin{equation*}
\frac{\partial \bar{u}_{j}^{r}}{\partial \bar{x}^{i}}=\bar{\Gamma}_{i j}^{s} \bar{u}_{s}^{r} \quad\left(\bar{u}_{s}^{r}=\frac{\partial x^{r}}{\partial \bar{x}^{s}}\right) \tag{2}
\end{equation*}


Since the systems (2) for $r=1,2,3$ are the same, temporarily replace $\bar{u}_{s}^{r}$ by $\bar{u}_{s}$, and $x^{r}$ by a single variable $x$; thus,

For $j=1$, (3) becomes


\begin{equation*}
\frac{\partial \bar{u}_{j}}{\partial \bar{x}^{i}}=\bar{\Gamma}_{i j}^{s} \bar{u}_{s} \tag{3}
\end{equation*}


$$
\begin{aligned}
& \frac{\partial \bar{u}_{1}}{\partial \bar{x}^{1}}=\bar{\Gamma}_{11}^{1} \bar{u}_{1}+\bar{\Gamma}_{11}^{2} \bar{u}_{2}+\bar{\Gamma}_{11}^{3} \bar{u}_{3}=\bar{u}_{1} \\
& \frac{\partial \bar{u}_{1}}{\partial \bar{x}^{2}}=\bar{\Gamma}_{21}^{1} \bar{u}_{1}+\bar{\Gamma}_{21}^{2} \bar{u}_{2}+\bar{\Gamma}_{21}^{3} \bar{u}_{3}=0 \\
& \frac{\partial \bar{u}_{1}}{\partial \bar{x}^{3}}=\bar{\Gamma}_{31}^{1} \bar{u}_{1}+\bar{\Gamma}_{31}^{2} \bar{u}_{2}+\bar{\Gamma}_{31}^{3} \bar{u}_{3}=0
\end{aligned}
$$

Hence $\bar{u}_{1}$ is a function of $\bar{x}^{1}$ alone, and the first differential equation integrates to give

$$
\bar{u}_{1}=b_{1} \exp \bar{x}^{1} \quad\left(b_{1}=\text { constant }\right)
$$

In the same way, we find for $j=2$ and $j=3$ :

$$
\begin{array}{ll}
\bar{u}_{2}=b_{2} \exp 2 \bar{x}^{2} & \left(b_{2}=\text { constant }\right) \\
\bar{u}_{3}=b_{3} \exp 3 \bar{x}^{3} & \left(b_{3}=\text { constant }\right)
\end{array}
$$

Now we return to the equations $\partial x / \partial \bar{x}^{i}=\bar{u}_{i}$ with the solutions just found for the $\bar{u}_{i}$.


\begin{equation*}
\frac{\partial x}{\partial \bar{x}^{1}}=b_{1} \exp \bar{x}^{1} \quad-\frac{\partial x}{\partial \bar{x}^{2}}=b_{2} \exp 2 \bar{x}^{2} \quad \frac{\partial x}{\partial \bar{x}^{3}}=b_{3} \exp 3 \bar{x}^{3} \tag{4}
\end{equation*}


Integration of the first equation (4) yields

$$
x=b_{1} \exp \bar{x}^{1}+\varphi\left(\bar{x}^{2}, \bar{x}^{3}\right)
$$

and then the second and third equations give:

$$
\begin{aligned}
& \frac{\partial \varphi}{\partial \bar{x}^{2}}=b_{2} \exp 2 \bar{x}^{2} \quad \text { or } \quad \varphi=a_{2} \exp 2 \bar{x}^{2}+\psi\left(\bar{x}^{3}\right) \\
& \frac{d \psi}{d \bar{x}^{3}}=b_{3} \exp 3 \bar{x}^{3} \quad \text { or } \quad \psi=a_{3} \exp 3 \bar{x}^{3}+a_{4}
\end{aligned}
$$

This means that, with $a_{1}=b_{1}$,

$$
x=a_{1} \exp \bar{x}^{1}+a_{2} \exp 2 \bar{x}^{2}+a_{3} \exp 3 \bar{x}^{3}+a_{4}
$$

so that the general solution of (1) is


\begin{equation*}
x^{r}=a_{1}^{r} \exp \bar{x}^{1}+a_{2}^{r} \exp 2 \bar{x}^{2}+a_{3}^{r} \exp 3 \bar{x}^{3}+a_{4}^{r} \tag{5}
\end{equation*}


for $r=1,2,3$.

The constants $a_{4}^{r}$ in (5) are unimportant; they merely allow any point in $\mathbf{R}^{3}$ to serve as the origin of the rectangular system $\left(x^{r}\right)$. The remaining constants may be chosen at will, subject to a single condition (see Problem 6.27).

\section*{COVARIANT DIFFERENTIATION}
6.7 Establish the tensor character of $\mathbf{T}_{, k}$ (Definition 2), where $\mathbf{T}$ is a contravariant vector.

Beginning with the transformation law

$$
\bar{T}^{i}=T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}
$$

take the partial derivative with respect to $\bar{x}^{k}$ and use the chain rule:

$$
\frac{\partial \bar{T}^{i}}{\partial \bar{x}^{k}}=\frac{\partial}{\partial x^{s}}\left(T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}}\right) \frac{\partial x^{s}}{\partial \bar{x}^{k}}=\frac{\partial T^{r}}{\partial x^{s}} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}+T^{r} \frac{\partial^{2} \bar{x}^{i}}{\partial x^{s} \partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}
$$

Now use (6.6), with barred and unbarred systems interchanged:

$$
\frac{\partial \bar{T}^{i}}{\partial \bar{x}^{k}}=\frac{\partial T^{r}}{\partial x^{s}} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}+T^{r}\left(\Gamma_{s r}^{t} \frac{\partial \bar{x}^{i}}{\partial x^{t}}-\bar{\Gamma}_{u v}^{i} \frac{\partial \bar{x}^{u}}{\partial x^{s}} \frac{\partial \bar{x}^{v}}{\partial x^{r}}\right) \frac{\partial x^{s}}{\partial \bar{x}^{k}}
$$

Since $\left(\partial \bar{x}^{u} / \partial x^{s}\right)\left(\partial x^{s} / \partial \bar{x}^{k}\right)=\delta_{k}^{u}$ and $T^{r}\left(\partial \bar{x}^{v} / \partial x^{r}\right)=\bar{T}^{\nu}$, this becomes

$$
\frac{\partial \bar{T}^{i}}{\partial \bar{x}^{k}}=\frac{\partial T^{r}}{\partial x^{s}} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}+\Gamma_{s r}^{t} T^{r} \frac{\partial \bar{x}^{i}}{\partial x^{t}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}-\bar{\Gamma}_{k v}^{i} \bar{T}^{v}
$$

or (by factoring and using the symmetry of the Christoffel symbols)

$$
\frac{\partial \bar{T}^{i}}{\partial \bar{x}^{k}}+\bar{\Gamma}_{t k}^{i} \bar{T}^{t}=\left(\frac{\partial T^{r}}{\partial x^{s}}+\Gamma_{t s}^{r} T^{t}\right) \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}} \quad \text { or } \quad \bar{T}_{, k}^{i}=T_{, s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{k}}
$$

6.8 Show that $\left(T_{j, k}^{i}\right)$, as defined by (6.7), is a tensor, using the previously proven facts that $T_{, k}^{i}$ and $T_{i, k}$ are tensorial for all tensors $\left(T^{i}\right)$ and $\left(T_{i}\right)$.

Let $\left(V_{i}\right)$ be any vector and set $U_{j}=T_{j}^{r} V_{r}$. The covariant derivative of the tensor $\left(U_{j}\right)$ is the tensor $\left(U_{j, k}\right)$, where

$$
U_{j, k}=\frac{\partial U_{j}}{\partial x^{k}}-\Gamma_{j k}^{r} U_{r}=\frac{\partial}{\partial x^{k}}\left(T_{j}^{r} V_{r}\right)-\Gamma_{j k}^{r}\left(T_{r}^{s} V_{s}\right)=\frac{\partial T_{j}^{s}}{\partial x^{k}} V_{s}+T_{j}^{r} \frac{\partial V_{r}}{\partial x^{k}}-\Gamma_{j k}^{r} T_{r}^{s} V_{s}
$$

But

$$
V_{r, k}=\frac{\partial V_{r}}{\partial x^{k}}-\Gamma_{r k}^{s} V_{s} \quad \text { or } \quad \frac{\partial V_{r}}{\partial x^{k}}=V_{r, k}+\Gamma_{r k}^{s} V_{s}
$$

When the above expression for $\partial V_{r} / \partial x^{k}$ is substituted into the preceding equation and the terms rearranged, the result is:

$$
\left(\frac{\partial T_{j}^{s}}{\partial x^{k}}+\Gamma_{r k}^{s} T_{j}^{r}-\Gamma_{j k}^{r} T_{r}^{s}\right) V_{s}=U_{j, k}-T_{j}^{r} V_{r, k}
$$

i.e.,

$$
T_{j, k}^{s} V_{s}=\text { tensor component }
$$

It follows at once from the Quotient Theorem (Theorem 4.2) that $\left(T_{j, k}^{i}\right)$ is a tensor.

6.9 Extend the notion of covariant differentiation so that it will apply to invariants.

First note that the partial derivative of an invariant is a tensor:

$$
\frac{\partial \bar{E}}{\partial \bar{x}^{i}}=\frac{\partial E}{\partial \bar{x}^{i}}=\frac{\partial E}{\partial x^{r}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}
$$

Now, under any reasonable definition, $\left(E_{, i}\right)$ must (1) be a tensor; (2) coincide with ( $\left.\partial E / \partial x^{i}\right)$ in rectangular coordinates. The obvious choice is therefore

$$
\left(E_{, i}\right) \equiv\left(\frac{\partial E}{\partial x^{i}}\right)
$$

6.10 Write the formula for the covariant derivative indicated by $T_{k, l}^{i j}$.

$$
T_{k, l}^{i j}=\frac{\partial T_{k}^{i j}}{\partial x^{l}}+\Gamma_{r l}^{i} T_{k}^{r j}+\Gamma_{r l}^{j} T_{k}^{i r}-\Gamma_{k l}^{r} T_{r}^{i j}
$$

6.11 Prove that the metric tensor behaves like a constant under covariant differentiation; i.e., $g_{i j, k}=0$ for all $i, j, k$.

By definition, since $\left(g_{i j}\right)$ is covariant of order 2 ,

$$
g_{i j, k}=\frac{\partial g_{i j}}{\partial x^{k}}-\Gamma_{i k}^{r} g_{r j}-\Gamma_{j k}^{r} g_{i r}=g_{i j k}-\Gamma_{i k j}-\Gamma_{j k i}=0
$$

by (6.2). (In a similar manner, it follows that $g^{i j}=0$; see Problem 6.34.)

Because of the above property of the metric tensor and its inverse, the operation of covariant differentiation commutes with those of raising and lowering indices. For example,

$$
T_{j, k}^{i}=\left(g^{i r} T_{r j}\right)_{, k}=g^{i r} T_{r j, k}
$$

\section*{ABSOLUTE DIFFERENTIATION}
6.12 Prove that (6.8) is the result of forming the inner product of the covariant derivative $\left(T_{, j}^{i}\right)$ with the tangent vector $\left(d x^{i} / d t\right)$ of the curve.

$$
T_{, j}^{i} \frac{d x^{j}}{d t}=\left(\frac{\partial T^{i}}{\partial x^{j}}+\Gamma_{r j}^{i} T^{r}\right) \frac{d x^{j}}{d t}=\frac{\partial T^{i}}{\partial x^{j}} \frac{d x^{j}}{d t}+\Gamma_{r j}^{i} T^{r} \frac{d x^{j}}{d t}=\frac{d T^{i}}{d t}+\Gamma_{r j}^{i} T^{r} \frac{d x^{j}}{d t}
$$

6.13 A particle is in motion along the circular arc given parametrically in spherical coordinates by $x^{1}=b, x^{2}=\pi / 4, x^{3}=\omega t \quad(t=$ time $)$. Find its acceleration using the formula (6.10) and compare with the result $a=r \omega^{2}$ from elementary mechanics.

From Problem 6.5, we have along the circle

$$
\begin{array}{cc}
\Gamma_{22}^{1}=-x^{1}=-b & \Gamma_{33}^{1}=-x^{1} \sin ^{2} x^{2}=-b \sin ^{2} \frac{\pi}{4}=-\frac{b}{2} \\
\Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{1}{x^{1}}=\frac{1}{b} & \Gamma_{33}^{2}=-\sin x^{2} \cos x^{2}=-\sin \frac{\pi}{4} \cos \frac{\pi}{4}=-\frac{1}{2} \\
\Gamma_{13}^{3}=\Gamma_{31}^{3}=\frac{1}{x^{1}}=\frac{1}{b} & \Gamma_{23}^{3}=\Gamma_{32}^{3}=\cot x^{2}=\cot \frac{\pi}{4}=1
\end{array}
$$

with all other symbols vanishing. The components of acceleration are, from (6.9),

$$
\begin{aligned}
& a^{1}=\frac{d^{2} x^{1}}{d t^{2}}+\Gamma_{r s}^{1} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}=0+\Gamma_{22}^{1}\left(\frac{d x^{2}}{d t}\right)^{2}+\Gamma_{33}^{1}\left(\frac{d x^{3}}{d t}\right)^{2}=0+\left(-\frac{b}{2}\right)(\omega)^{2}=-\frac{b \omega^{2}}{2} \\
& a^{2}=\frac{d^{2} x^{2}}{d t^{2}}+\Gamma_{r s}^{2} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}=0+2 \Gamma_{12}^{2} \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}+\Gamma_{33}^{2}\left(\frac{d x^{3}}{d t}\right)^{2}=0+\left(-\frac{1}{2}\right)(\omega)^{2}=-\frac{\omega^{2}}{2} \\
& a^{3}=\frac{d^{2} x^{3}}{d t^{2}}+\Gamma_{r s}^{3} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}=0+2 \Gamma_{13}^{3} \frac{d x^{1}}{d t} \frac{d x^{3}}{d t}+2 \Gamma_{23}^{3} \frac{d x^{2}}{d t} \frac{d x^{3}}{d t}=0
\end{aligned}
$$

Together with the metric components along the circle,

$$
g_{11}=1 \quad g_{22}=\left(x^{1}\right)^{2}=b^{2} \quad g_{33}=\left(x^{1}\right)^{2} \sin ^{2} x^{2}=\frac{b^{2}}{2}
$$

the acceleration components give, via (6.10),

$$
a=\sqrt{g_{i j} a^{i} a^{j}}=\sqrt{(1)\left(-b \omega^{2} / 2\right)^{2}+\left(b^{2}\right)\left(-\omega^{2} / 2\right)^{2}+0}=b \omega^{2} / \sqrt{2}
$$

Upon introducing the radius of the circle, using (3.4) with $\bar{x}^{1}=x=r$ and $x^{3}=0$,

we obtain $a=r \omega^{2}$.

$$
r=b \sin \frac{\pi}{4}=\frac{b}{\sqrt{2}}
$$

6.14 Verify that $x^{1}=a \sec x^{2}$ is a geodesic for the Euclidean metric in polar coordinates. [In rectangular coordinates $(x, y)$, the curve is $x=a$, a vertical line.]

First choose a parameterization for the curve; say,


\begin{align*}
& x^{1}=a \sec t  \tag{1}\\
& x^{2}=t
\end{align*} \quad(-\pi / 2<t<\pi / 2)


Parameter $t$ is related to the arc-length parameter $s$ via

$$
\begin{aligned}
\frac{d s}{d t} & =\sqrt{g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}}=\sqrt{\left(\frac{d x^{1}}{d t}\right)^{2}+(a \sec t)^{2}\left(\frac{d x^{2}}{d t}\right)^{2}}=\sqrt{a^{2} \sec ^{2} t \tan ^{2} t+a^{2} \sec ^{2} t} \\
& =(a \sec t) \sqrt{1+\tan ^{2} t}=a \sec ^{2} t
\end{aligned}
$$

or

$$
\frac{d t}{d s}=\frac{\cos ^{2} t}{a}
$$

so that for any function $x(t)$

$$
\begin{aligned}
& \frac{d x}{d s}=\frac{d x}{d t} \frac{d t}{d s}=\frac{\cos ^{2} t}{a} \frac{d x}{d t} \\
& \frac{d^{2} x}{d s^{2}}=\frac{d}{d t}\left(\frac{\cos ^{2} t}{a} \frac{d x}{d t}\right) \frac{d t}{d s}=\frac{\cos ^{4} t}{a^{2}} \frac{d^{2} x}{d t^{2}}-\frac{2 \sin t \cos ^{3} t}{a^{2}} \frac{d x}{d t}
\end{aligned}
$$

Now, taking the nonzero Christoffel symbols from Example 6.3, we can rewrite the geodesic equations (6.13) in terms of the independent variable $t$ :

$$
0=\frac{d^{2} x^{1}}{d s^{2}}+\Gamma_{22}^{1}\left(\frac{d x^{2}}{d s}\right)^{2}=\frac{\cos ^{4} t}{a^{2}} \frac{d^{2} x^{1}}{d t^{2}}-\frac{2 \sin t \cos ^{3} t}{a^{2}} \frac{d x^{1}}{d t}+\left(-x^{1}\right) \frac{\cos ^{4} t}{a^{2}}\left(\frac{d x^{2}}{d t}\right)^{2}
$$

or


\begin{equation*}
\frac{d^{2} x^{1}}{d t^{2}}-(2 \tan t) \frac{d x^{1}}{d t}-x^{1}\left(\frac{d x^{2}}{d t}\right)^{2}=0 \tag{2}
\end{equation*}


and

$$
0=\frac{d^{2} x^{2}}{d s^{2}}+2 \Gamma_{12}^{2} \frac{d x^{1}}{d s} \frac{d x^{2}}{d s}=\frac{\cos ^{4} t}{a^{2}} \frac{d^{2} x^{2}}{d t^{2}}-\frac{2 \sin t \cos ^{3} t}{a^{2}} \frac{d x^{2}}{d t}+2\left(\frac{1}{x^{1}}\right)\left(\frac{\cos ^{2} t}{a}\right)^{2} \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}
$$

or


\begin{equation*}
\frac{d^{2} x^{2}}{d t^{2}}-(2 \tan t) \frac{d x^{2}}{d t}+\left(\frac{2}{x^{1}}\right) \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}=0 \tag{3}
\end{equation*}


All that remains is to verify that the functions (1) satisfy the system (2)-(3). Substituting in (2):

$$
a\left(\sec t+2 \sec t \tan ^{2} t\right)-(2 \tan t)(a \sec t \tan t)-(a \sec t)(1)=0
$$

Substituting in (3):

$$
0-(2 \tan t)(1)+\left(\frac{2}{a \sec t}\right)(a \sec t \tan t)(1)=0 \quad \text { QED }
$$

\section*{DIFFERENTIATION RULES}
6.15 Prove the rules for covariant differentiation stated in Section 6.6.

(a) The sum rule obviously holds, as (6.7) is linear in the tensor components.

(b) Let $\mathbf{T}=\left(T_{j}^{i}\right)$ and $\mathbf{S}=\left(S_{j}^{i}\right)$ be two mixed tensors of order 2, with outer product $\mathbf{U}=\left(T_{r}^{p} S_{s}^{q}\right)$. Then,

$$
\begin{aligned}
& T_{r, k}^{p} S_{s}^{q}+T_{r}^{p} S_{s, k}^{q} \\
& \quad=\left(\frac{\partial T_{r}^{p}}{\partial x^{k}}+\Gamma_{t k}^{p} T_{r}^{t}-\Gamma_{r k}^{t} T_{r}^{p}\right) S_{s}^{q}+T_{r}^{p}\left(\frac{\partial S_{s}^{q}}{\partial x^{k}}+\Gamma_{t i k}^{q} S_{s}^{t}-\Gamma_{s k}^{t} S_{t}^{q}\right)
\end{aligned}
$$

$$
\begin{aligned}
& =\underbrace{\left.\frac{\partial T_{r}^{p}}{\partial x^{k}} S_{s}^{q}+T_{r}^{p} \frac{\partial S_{s}^{q}}{\partial x^{k}}\right)}_{\partial U_{r s}^{p q} / \partial x^{k}}+\Gamma_{t k}^{p} U_{r s}^{t q}+\Gamma_{t k}^{q} U_{r s}^{p t}-\Gamma_{r k}^{t} U_{t s}^{p q}-\Gamma_{s k}^{t} U_{r t}^{p q} \\
& \equiv U_{r s, k}^{p q}
\end{aligned}
$$

and this proof of the outer-product rule extends to arbitrary $\mathbf{T}$ and $\mathbf{S}$.

(c) The inner-product rule follows from the outer-product rule and the following useful result: Contraction of indices and covariant differentiation commute. To prove this last, let $\mathbf{R}=\left(R_{k}^{i j}\right)$. Then,

$$
\begin{aligned}
R_{k, l}^{i j} \delta_{j}^{k} & =\left(\frac{\partial R_{k}^{i j}}{\partial x^{l}}+\Gamma_{t l}^{i} R_{k}^{i j}+\Gamma_{t l}^{j} R_{k}^{i t}-\Gamma_{k l}^{t} R_{t}^{i j}\right) \delta_{j}^{k} \\
& =\frac{\partial R_{k}^{i k}}{\partial x^{l}}+\Gamma_{t l}^{i} R_{k}^{t k}+0=\left(R_{k}^{i k}\right)_{, l} \quad \text { QED }
\end{aligned}
$$

6.16 Instead of Problem 6.15, why not: "Each rule is a tensor equation that is valid in rectangular coordinates, where covariant differentiation reduces to partial differentiation. Therefore, each rule holds in every coordinate system.'?

If the space metric is non-Euclidean, there is no way to transform to a rectangular coordinate system (in which the rules would indeed hold).

6.17 Infer the outer-product rule for absolute differentation from the corresponding rule for covariant differentiation.

Let $\mathbf{x}=\mathbf{x}(t)$ be any curve, and $\mathbf{T}(\mathbf{x}(t))$ and $\mathbf{S}(\mathbf{x}(t))$ two tensors defined on the curve. Then,

$$
\frac{\delta}{\delta t}[\mathbf{T S}]=[\mathbf{T S}]_{, k} \frac{d x^{k}}{d t}=\left(\left[\mathbf{T}_{, k} \mathbf{S}\right]+\left[\mathbf{T S}_{, k}\right]\right) \frac{d x^{k}}{d t}=\left[\mathbf{T}_{, k} \frac{d x^{k}}{d t} \mathbf{S}\right]+\left[\mathbf{T ~}_{, k} \frac{d x^{k}}{d t}\right]=\left[\frac{\delta \mathbf{T}}{\delta t} \mathbf{S}\right]+\left[\mathbf{T} \frac{\delta \mathbf{S}}{\delta t}\right]
$$

\section*{UNIQUENESS OF THE ABSOLUTE DERIVATIVE}
6.18 Prove Theorem 6.4.

Denote by $\Delta \mathbf{T} / \Delta t$ any tensor that satisfies the hypothesis of the theorem. The tensor equation

$$
\frac{\Delta \mathbf{T}}{\Delta t}=\frac{\delta \mathbf{T}}{\delta t}
$$

is valid in rectangular coordinates $\left(x^{i}\right)$, since, in $\left(x^{i}\right)$, both sides coincide with $d \mathbf{T} / d t$. But then (Section 4.3) the equation holds in every coordinate system; i.e.,

$$
\frac{\Delta \mathbf{T}}{\Delta t} \equiv \frac{\delta \mathbf{T}}{\delta t}
$$

\section*{Supplementary Problems}
6.19 Find the general solution of the linear system

$$
\frac{\partial^{2} \bar{x}^{i}}{\partial x^{j} \partial x^{k}}=a_{j k}^{i}=\text { const. }
$$

with $a_{j k}^{i}$ symmetric in the two lower subscripts. [Hint: Set $y_{k}^{i}=\partial \bar{x}^{i} / \partial x^{k}-a_{r k}^{i} x^{r}$.]

6.20 A two-dimensional coordinate system $\left(x^{i}\right)$ is connected to a rectangular coordinate system $\left(\bar{x}^{i}\right)$ through

$$
\bar{x}^{1}=2\left(x^{1}\right)^{2}+x^{2} \quad \bar{x}^{2}=-x^{1}+3 x^{2}
$$

(a) Exhibit the metric tensor in $\left(x^{i}\right)$.

(b) Calculate the Christoffel symbols of the first kind for $\left(x^{i}\right)$ directly from the definition (6.1).

6.21 (a) Derive the formula

$$
\Gamma_{i j k}=\frac{\partial^{2} \bar{x}^{r}}{\partial x^{i} \partial x^{j}} \frac{\partial \bar{x}^{r}}{\partial x^{k}}
$$

when $\left(\bar{x}^{i}\right)$ is rectangular and $\left(x^{i}\right)$ is any other coordinate system. [Hint: Interchange barred and unbarred coordinate systems in (6.3)]. (b) Derive the analogous formula

$$
\Gamma_{j k}^{i}=\frac{\partial^{2} \bar{x}^{r}}{\partial x^{i} \partial x^{k}} \frac{\partial x^{i}}{\partial \bar{x}^{r}}
$$

when $\left(\bar{x}^{i}\right)$ is such that all $\bar{g}_{i j}$ arc constant. [Hint: Interchange barred and unbarred coordinate systems in $(6.5)$.]

6.22 Let the coordinate system $\left(x^{i}\right)$ be connected to a system of rectangular coordinates $\left(\bar{x}^{i}\right)$ via

$$
\bar{x}^{1}=\exp \left(x^{1}+x^{2}\right) \quad \bar{x}^{2}=\exp \left(x^{1}-x^{2}\right)
$$

Use Problem 6.21(b) to compute the nonzero Christoffel symbols of the second kind for $\left(x^{i}\right)$.

6.23 If

$$
\begin{aligned}
& \bar{x}^{1}=-\exp d_{1} x^{1}+\exp d_{2} x^{2}+\exp d_{3} x^{3} \\
& \bar{x}^{2}=2 \exp d_{1} x^{1}-\exp d_{2} x^{2}+\exp d_{3} x^{3} \\
& \bar{x}^{3}=\exp d_{1} x^{1}-2 \exp d_{2} x^{2}+3 \exp d_{3} x^{3}
\end{aligned}
$$

and if all $\bar{\Gamma}_{j k}^{i}=0$, find the $\Gamma_{j k}^{i}$.

6.24 Derive (6.6) by solving (6.5) for the second derivative and then changing indices.

6.25 Prove that all $\Gamma_{j k}^{i}$ vanish only if all $g_{i j}$ are constant.

6.26 Calculate the nonzero Christoffel symbols of both kinds for the Euclidean metric in cylindrical coordinates, (5.3).

6.27 Express the condition that the transformation (5) of Problem 6.6 be bijective (Section 2.6).

6.28 Show that if $\Gamma_{i j k}$ are constant, then $g_{i j}$ are linear in the variables $\left(x^{i}\right)$; but that this is not necessarily true if $\Gamma_{j k}^{i}$ are constant. (For a counterexample, use the metric $g_{11}=\exp 2 x^{1}, g_{12}=g_{21}=0, g_{22}=1$.)

6.29 What is the most general two-dimensional transformation $\bar{x}^{i}=\bar{x}^{i}(\mathbf{x})$ of coordinates such that $\left(\bar{x}^{i}\right)$ are rectangular and the Christoffel symbols $\Gamma_{j k}^{i}$ in $\left(x^{i}\right)$ are those for the metric of polar coordinates (Example 6.3)?

6.30 Is the covariant derivative of a tensor with constant components equal to zero as in ordinary differentiation? Explain your answer.

6.31 If $T_{j r s}^{i}$ are tensor components, write out the components of the covariant derivative, $T_{j r s, k}^{i}$.

6.32 Show that $\delta_{i, k}^{i}=0$ for all $i, j, k$.

6.33 For any tensor $\mathbf{T}$, verify that $(\mathbf{g} * \mathbf{T})_{, k}=\mathbf{g} * \mathbf{T}{ }_{, k}$, where $*$ denotes either an outer or inner product.

6.34 Use Problem 6.32 and $g_{j r} g^{r i}=\delta_{j}^{i}$ to show that the covariant derivative of $\mathbf{g}^{-1}$ is zero.

6.35 Use the recursive method of Problem 6.8 to verify that $\left(T_{i j, k}\right)$ is a tensor.

6.36 Using tensor methods in polar coordinates, find the curvature of the circle

$$
x^{1}=b \quad x^{2}=t
$$

6.37 If the metric for $\left(x^{i}\right)$ is

$$
G=\left[\begin{array}{cc}
\left(x^{1}\right)^{2} & 0 \\
0 & 1
\end{array}\right]
$$

(a) write the differential equations of the geodesics in terms of the dependent variables $u=\left(x^{1}\right)^{2}$ and $v=x^{2} ;(b)$ integrate these equations and eliminate the arc-length parameter from the solution.

6.38 Find the geodesics on the surface of a sphere of radius $a$ by $(a)$ writing the geodesic equations for the spherical coordinates $x^{2}$ and $x^{3}$ (the $x^{1}$-equation is trivial for $x^{1}=a=$ const. and may be ignored); $(b)$ exhibiting a particular solution of these two equations; and (c) generalizing on (b). Use Problem 6.5 for the Christoffel symbols.

\section*{Chapter 7}
\section*{Riemannian Geometry of Curves}
\subsection*{7.1 INTRODUCTION}
At this point, some new terminology is introduced, which commemorates the general formulation of $n$-dimensional geometry by Bernhard Riemann (1826-1866).

Definition 1: A Riemannian space is the space $\mathbf{R}^{n}$ coordinatized by $\left(x^{i}\right)$, together with a fundamental form or Riemannian metric, $g_{i j} d x^{i} d x^{j}$, where $\mathbf{g}=\left(g_{i j}\right)$ obeys conditions A-D of Section 5.3.

Thus, in our preliminary treatment of angles, tangents, normals, and geodesic curves, in Chapters 5 and 6 , we already entered Riemannian geometry-though largely restricted to familiar 3-dimensional coordinate systems and a positive definite (Euclidean) metric. The present chapter focuses on the theory of curves in a Riemannian space with an indefinite metric. It also takes up geodesics from a different viewpoint.

\subsection*{7.2 LENGTH AND ANGLE UNDER AN INDEFINITE METRIC}
Formulas (5.10) and (5.11) must be generalized to allow for changes in sign of the fundamental form.

Definition 2: The norm of an arbitrary (contravariant or covariant) vector $\mathbf{V}$ is

$$
\|\mathbf{V}\| \equiv \sqrt{\varepsilon \mathbf{V}^{2}}=\sqrt{\varepsilon V_{i} V^{i}} \quad(\varepsilon=\varepsilon(\mathbf{V}))
$$

where $\varepsilon(\quad)$ is the indicator function (Section 5.3).

Under this definition, $\|\mathbf{V}\| \geqq 0$, but it is possible that $\|\mathbf{V}\|=0$ for $\mathbf{V} \neq \mathbf{0}$; such a vector is called a null vector. Moreover, the triangle inequality is not necessarily obeyed by this norm (see Problem 7.8).

If $\mathbf{V}(t)$ is the tangent field to the curve $x^{i}=x^{i}(t) \quad(a \leqq t \leqq b)$, then the length formula (5.1a) may be written as


\begin{equation*}
L=\int_{a}^{b} \sqrt{\varepsilon g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}} d t=\int_{a}^{b}\|\mathbf{V}(t)\| d t \tag{7.1}
\end{equation*}


The angle between non-null contravariant vectors is still defined by (5.11), provided the new norm is understood:


\begin{equation*}
\cos \theta=\frac{\mathbf{U V}}{\|\mathbf{U}\|\|V\|}=\frac{g_{i j} U^{i} V^{j}}{\sqrt{\varepsilon_{1} g_{p q} U^{p} U^{q}} \sqrt{\varepsilon_{2} g_{r s} V^{r} V^{s}}} \tag{7.2}
\end{equation*}


where $\varepsilon_{1}=\varepsilon(\mathbf{U})$ and $\varepsilon_{2}=\varepsilon(\mathbf{V})$. Because of the indefiniteness of the metric, we must distinguish two possibilities in the application of (7.2).

Case 1: $|\mathbf{U V}| \leqq\|\mathbf{U}\|\|\mathbf{V}\|$ (the Cauchy-Schwarz inequality holds for $\mathbf{U}$ and $\mathbf{V}$ ). Then $\theta$ is uniquely determined as a real number in the interval $[0, \pi]$.

Case 2: $|\mathbf{U V}|>\|\mathbf{U}\|\|\mathbf{V}\|$ (the Cauchy-Schwarz does not hold). Then (7.2) takes the form

$$
\cos \theta=k \quad(|k|>1)
$$

which has an infinite number of solutions for $\theta$, all of them complex. By convention, we always choose the solution

$$
\theta= \begin{cases}i \ln \left(k+\sqrt{k^{2}-1}\right) & k>1 \\ \pi+i \ln \left(-k+\sqrt{k^{2}-1}\right) & k<-1\end{cases}
$$

that exhibits the proper limiting behavior as $k \rightarrow 1^{+}$or $k \rightarrow-1^{-}$.

EXAMPLE 7.1 At the points of intersection, find the angles between the curves (i.e., between their tangents)

$$
\mathscr{C}_{1}:\left(x_{1}^{i}\right)=\left(t, 0,0, t^{2}\right) \quad \mathscr{C}_{2}:\left(x_{2}^{i}\right)=\left(u, 0,0,2-u^{2}\right)
$$

( $t, u$ real), if the Riemannian metric is

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2}-\left(d x^{4}\right)^{2}
$$

[This is the metric of Special Relativity, with $x^{4} \equiv$ (speed of light) $\times($ time $)$.]

The curves meet in the two points $P(1,0,0,1)$ and $Q(-1,0,0,1)$. At $P$, (where $t=u=1$ ) the two tangent vectors are

$$
\begin{aligned}
& \mathbf{U}_{P}=\left(d x_{1}^{i} / d t\right)_{P}=(1,0,0,2 t)_{P}=(1,0,0,2) \\
& \mathbf{V}_{P}=\left(d x_{2}^{i} / d u\right)_{P}=(1,0,0,-2 u)_{P}=(1,0,0,-2)
\end{aligned}
$$

so that (7.2) gives

$$
\begin{aligned}
\cos \theta_{P} & =\frac{1(1)(1)+1(0)(0)+1(0)(0)-1(2)(-2)}{\sqrt{\varepsilon_{1}\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(2)^{2}\right]} \sqrt{\varepsilon_{2}\left[1(1)^{2}+1(0)^{2}+1(0)^{2}-1(-2)^{2}\right]}} \\
& =\frac{5}{\sqrt{+3} \sqrt{+3}}=\frac{5}{3}
\end{aligned}
$$

and $\theta_{P}=i \ln \left[(5 / 3)+\sqrt{(5 / 3)^{2}-1}\right]=i \ln 3$.

Similarly we calculate (for $-t=u=1$ )

so that $\theta_{Q}=\theta_{P}$.

$$
\mathbf{U}_{Q}=(1,0,0,-2)=\mathbf{V}_{P} \quad \mathbf{V}_{Q}=(1,0,0,2)=\mathbf{U}_{P}
$$

\subsection*{7.3 NULL CURVES}
If $\mathbf{g}$ is not required to be positive definite, a curve can have zero length.

EXAMPLE 7.2 In $\mathbf{R}^{4}$, under the metric of Example 7.1, consider the curve

$$
x^{1}=3 \cos t \quad x^{2}=3 \sin t \quad x^{3}=4 t \quad x^{4}=5 t
$$

for $0 \leqq t \leqq 1$. Along the curve,

$$
\begin{aligned}
\left(\frac{d x^{i}}{d t}\right) & =(-3 \sin t, 3 \cos t, 4,5) \\
\varepsilon\left(\frac{d s}{d t}\right)^{2} & =g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=(-3 \sin t)^{2}+(3 \cos t)^{2}+(4)^{2}-(5)^{2}=0
\end{aligned}
$$

and so the arc length is

$$
L=\int_{0}^{1} 0 d t=0
$$

A curve is null if it or any of its subarcs has zero length. Here, a subarc is understood to be nontrivial; that is, it consists of more than one point and corresponds to an interval $c \leqq t \leqq d$, where $c<d$. A curve is null at a point if for some value of the parameter $t$ the tangent vector is a null vector; i.e.,

$$
g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=0
$$

The set of $t$-values at which the curve is null is known as the null set of the curve.

Under the above definitions, a curve can be null without having zero length (if there is a subarc with zero length); but a curve having zero length is necessarily null at every point, and hence a null curve. Example 7.2 gives such a curve.

EXAMPLE 7.3 Under the Riemannian metric

$$
G=\left[\begin{array}{cc}
\left(x^{1}\right)^{2} & -1 \\
-1 & 0
\end{array}\right]
$$

the curve $\left(x^{1}, x^{2}\right)=\left(t,\left|t^{3}\right| / 6\right)$ possesses a null subarc that renders the length of the curve much smaller than might be expected. In fact, because $d x^{1} / d t=1$ and $d x^{2} / d t=\delta t^{2} / 2$, where $\delta= \pm 1$ and is positive if $t \geqq 0$,

$$
\varepsilon g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\varepsilon\left[\left(x^{1}\right)^{2}\left(\frac{d x^{1}}{d t}\right)^{2}-2 \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}\right]=\varepsilon\left[t^{2}(1)-2(1)\left(\delta t^{2} / 2\right)\right]=\varepsilon\left(t^{2}-\delta t^{2}\right)
$$

Since the quantity following the indicator is nonnegative, $\varepsilon=+1$ everywhere. But note that $t^{2}-\delta t^{2}=0$ if $t \geqq 0$. Hence,

$$
\begin{aligned}
L & =\int_{-1}^{999} \sqrt{t^{2}-\delta t^{2}} d t=\int_{-1}^{0} \sqrt{2 t^{2}} d t+\int_{0}^{999} 0 d t=\sqrt{2} \int_{-1}^{0}(-t) d t \\
& =-\sqrt{2} t^{2} /\left.2\right|_{-1} ^{0}=\sqrt{2} / 2 \approx 0.707
\end{aligned}
$$

The interpretation in rectangular coordinates $\left(x^{2}, x^{2}\right)$ is queer: As a particle travels less than a millimeter along the curve, its "shadow" on the $x^{1}$-axis travels a meter!

\section*{Nonexistence of an Arc-Length Parameter}
For a positive definite metric, the arc-length parameter $s$ is well-defined by (5.6) as a strictly increasing function of the curve parameter $t$. (Then it is also the case that $t$ is a strictly increasing function of $s$.) This fact allowed us freely to convert between the two parameterizations in the Solved Problems to Chapter 6. However, it is clear that on a null curve, which possesses at least one interval $t_{1}<t<t_{2}$ of null points, it is impossible to define arc length $s$. rule,

Indeed, even isolated points of nullity pose analytical problems. For if $s^{\prime}\left(t_{0}\right)=0$, then the chain


\begin{equation*}
\frac{d x^{i}}{d s}=\frac{d x^{i}}{d t} \frac{1}{s^{\prime}(t)} \tag{7.3}
\end{equation*}


breaks down at $s_{0}$, the image of $t_{0}$. When necessary, we get around the difficulty by restricting attention to curves that are regular.

Definition 3: A curve is regular if it has no null points (i.e., $d s / d t>0$ ).

It will be further assumed that all curves are of sufficiently high differentiability class to permit the theory considered; usually, this will require the assumption that curves are of class $C^{2}$.

\subsection*{7.4 REGULAR CURVES: UNIT TANGENT VECTOR}
Let a regular curve $\mathscr{C}: x^{i}=x^{i}(s)$ be given in terms of the arc-length parameter; the tangent field is $\mathbf{T} \equiv\left(d x^{i} / d s\right)$. By definition of arc length,

$$
s=\int_{0}^{s}\|\mathbf{T}(u)\| d u
$$

and differentiation gives $1=\|\mathbf{T}(s)\|$, showing that $\mathbf{T}$ has unit length at each point of $\mathscr{C}$.

When it is inconvenient or impossible to convert to the arc-length parameter, we can, by (7.3),\\
obtain $\mathbf{T}$ by normalizing the tangent vector $\mathbf{U}=\left(d x^{i} / d t\right)$ :


\begin{equation*}
\mathbf{T}=\frac{1}{\|\mathbf{U}\|} \mathbf{U}=\frac{1}{s^{\prime}(t)} \mathbf{U} \tag{7.4}
\end{equation*}


In Problem 7.20 is proved the useful

Theorem 7.1: The absolute derivative $\delta \mathbf{T} / \delta s$ of the unit tangent vector $\mathbf{T}$ is orthogonal to $\mathbf{T}$.

\subsection*{7.5 REGULAR CURVES: \\
 UNIT PRINCIPAL NORMAL AND CURVATURE}
Also associated with a regular curve $\mathscr{C}$ is a vector orthogonal to the tangent vector. It may be introduced in two ways: (1) as the normalized $\delta \mathbf{T} / \delta s$, if it exists; (2) as any differentiable unit vector orthogonal to $\mathbf{T}$ and proportional to $\delta \mathbf{T} / \delta s$ when $\|\delta \mathbf{T} / \delta s\| \neq 0$. The latter definition is global in nature, and it applies to a larger class of curves than does the former.

\section*{Analytical (Local) Approach}
At any point of $\mathscr{C}$ at which $\|\delta \mathbf{T} / \delta s\| \neq 0$, define the unit principal normal as the vector


\begin{equation*}
\mathbf{N}_{0} \equiv \frac{\delta \mathbf{T}}{\delta s} /\left\|\frac{\delta \mathbf{T}}{\delta s}\right\| \tag{7.5}
\end{equation*}


The absolute curvature is the scale factor in (7.5):


\begin{equation*}
\kappa_{0} \equiv\left\|\frac{\delta \mathbf{T}}{\delta s}\right\|=\sqrt{\varepsilon g_{i j} \frac{\delta T^{i}}{\delta s} \frac{\delta T^{j}}{\delta s}} \tag{7.6}
\end{equation*}


This notion of curvature was informally defined in (6.12).

Calling this quantity "curvature" is suggestive of the fact that in rectangular coordinates $\|\delta \mathbf{T} / \delta s\|=\|d \mathbf{T} / d s\|$ measures the rate of change of the tangent vector with respect to distance, or how sharply $\mathscr{C}$ "bends" at each point. Substitution of (7.6) into (7.5) yields one of the Frenet equations:


\begin{equation*}
\frac{\delta \mathbf{T}}{\delta s}=\kappa_{0} \mathbf{N}_{0} \quad\left(\kappa_{0} \neq 0\right) \tag{7.7}
\end{equation*}


While this approach is simple and concise, it does not apply to many curves we want to consider; for instance, a geodesic-as defined by (6.13) - will not possess a local normal $\mathbf{N}_{0}$ at any point. Even if there is only one point of zero curvature and the metric is Euclidean, $\mathbf{N}_{0}$ can have an essential point of discontinuity there.

EXAMPLE 7.4 The simple cubic $y=x^{3}$ has an inflection point at the origin, or $s=0$ (by arrangement). As shown in Fig. 7-1,

$$
\lim _{s \rightarrow-0} \mathbf{N}_{0}=(0,-1) \quad \lim _{s \rightarrow+0} \mathbf{N}_{0}=(0,1)
$$

To verify this analytically, make the parameterization $x=t, y=t^{3}$, and calculate $\mathbf{N}_{0}$ as a function of $t$ $\left(s^{\prime}(t)=\sqrt{1+9 t^{4}}\right)$.

$$
\begin{aligned}
& \mathbf{U}=\left(x^{\prime}(t), y^{\prime}(t)\right)=\left(1,3 t^{2}\right) \\
& \mathbf{T}=\frac{1}{s^{\prime}(t)} \mathbf{U}=\frac{1}{\sqrt{1+9 t^{4}}}\left(1,3 t^{2}\right) \\
& \frac{d \mathbf{T}}{d s}=\frac{1}{s^{\prime}(t)} \frac{d \mathbf{T}}{d t}=\frac{6 t}{\left(1+9 t^{4}\right)^{2}}\left(-3 t^{2}, 1\right) \\
& \kappa_{0}=\left\|\frac{d \mathbf{T}}{d s}\right\|=\frac{6|t|}{\left(1+9 t^{4}\right)^{3 / 2}}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-096}
\end{center}

Fig. 7-1

$$
\mathbf{N}_{0}=\frac{1}{\kappa_{0}} \frac{d \mathbf{T}}{d s}=\frac{t /|t|}{\sqrt{1+9 t^{4}}}\left(--3 t^{2}, 1\right) \quad(t \neq 0)
$$

The scalar factor $t /|t|$ accounts for the discontinuity in $\mathbf{N}_{0}$ at $t=0 \quad(s=0)$.

\section*{Geometric (Global) Approach}
A unit principal normal to a regular curve $\mathscr{C}$ is any contravariant vector $\mathbf{N}=\left(N^{i}(s)\right)$ such that, along $\mathscr{C}$,\\
A. $\quad N^{i}$ is continuously differentiable (class $C^{1}$ ) for each $i$;\\
B. $\|\mathbf{N}\|=1$;\\
C. $\mathbf{N}$ is orthogonal to the unit tangent vector $\mathbf{T}$, and is a scalar multiple of $\delta \mathbf{T} / \delta s$ wherever $\|\delta \mathbf{T} / \delta s\| \neq 0$.

The curvature under this development is defined as


\begin{equation*}
\kappa \equiv \varepsilon \mathbf{N} \frac{\delta \mathbf{T}}{\delta s}=\varepsilon g_{i j} N^{i} \frac{\delta T^{j}}{\delta s} \quad(\varepsilon=\varepsilon(\mathbf{N})) \tag{7.8}
\end{equation*}


If the metric is positive definite, the Frenet equation


\begin{equation*}
\frac{\delta \mathbf{T}}{\delta s}=\kappa \mathbf{N} \tag{7.9}
\end{equation*}


holds unrestrictedly along a regular curve (see Problem 7.13).

EXAMPLE 7.5 For the curve of Example 7.4, conditions A, B, and C allow precisely two possibilities for N:

$$
\mathbf{N}=\frac{+1}{\sqrt{1+9 t^{4}}}\left(-3 t^{2}, 1\right) \quad \text { or } \quad \mathbf{N}=\frac{-1}{\sqrt{1+9 t^{4}}}\left(-3 t^{2}, 1\right)
$$

for $-\infty<t<\infty$. Geometrically, these amount to reversing the normal arrows in either the left half or the right half of Fig. 7-1. The corresponding formulas for curvature are $(\varepsilon \equiv 1)$

$$
\kappa=\frac{6 t}{\left(1+9 t^{4}\right)^{3 / 2}} \quad \text { or } \quad \kappa=\frac{-6 t}{\left(1+9 t^{4}\right)^{3 / 2}}
$$

On curves having everywhere a non-null $\delta \mathbf{T} / \delta s$, either $\mathbf{N} \equiv \mathbf{N}_{0}$ (with $\kappa=\kappa_{0}$ ) or $\mathbf{N} \equiv-\mathbf{N}_{0}$ (with $\kappa=-\kappa_{0}$ ). Thus, the global concept applies to all curves covered by the local concept and, in addition, to all regular planar curves (see Problem 7.14) and all analytic curves (curves for which the $x^{i}$ are representable as convergent Taylor series in $s$ ).

\subsection*{7.6 GEODESICS AS SHORTEST ARCS}
When the metric is positive definite, a geodesic may be defined by the zero-curvature conditions (6.13), or, equivalently, by the condition that for any two of its points sufficiently close together, its length between the two points is least among all curves joining those points.

The minimum-length development employs a variational argument. We need to assume that all curves under consideration are class $C^{2}$ (that is, the parametric functions which represent them have continuous second-order derivatives). Let $x^{i}=x^{i}(t)$ represent a shortest curve (geodesic) passing through $A=\left(x^{i}(a)\right)$ and $B=\left(x^{i}(b)\right)$, where $b-a$ is as small as necessary. Embed the geodesic in a one-parameter family of $C^{2}$ curves passing through $A$ and $B$ :

$$
x^{i}=X^{i}(t, u) \equiv x^{i}(t)+(t-a)(b-t) u \phi^{i}(t)
$$

where the multipliers $\phi^{i}(t)$ are arbitrary twice-differentiable functions. The length of a curve in this family is given by

$$
L(u)=\int_{a}^{b} \sqrt{\varepsilon g_{i j} \frac{\partial X^{i}}{\partial t} \frac{\partial X^{j}}{\partial t}} d t \equiv \int_{a}^{b} \sqrt{w(t, u)} d t
$$

with $\varepsilon=1$ for a positive-definite metric. Since $X^{i}(t, 0)=x^{i}(t) \quad(i=1,2, \ldots, n)$, the function $L(u)$ must have a local minimum at $u=0$. Standard calculus techniques yield the following expression of the necessary condition $L^{\prime}(0)=0$ :


\begin{equation*}
\int_{a}^{b}\left[w^{-1 / 2} \frac{\partial g_{i j}}{\partial x^{k}} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}-\frac{d}{d t}\left(2 w^{-1 / 2} g_{i k} \frac{d x^{i}}{d t}\right)\right](t-a)(b-t) \phi^{k}(t) d t=0 \tag{7.10}
\end{equation*}


in which


\begin{equation*}
w \equiv w(t, 0)=g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t} \tag{7.11}
\end{equation*}


Since $(t-a)(b-t)>0$ on $(a, b)$ and $\phi^{k}(t)$ may be chosen arbitrarily, the bracketed expression in (7.10) must vanish identically over $(a, b)$, for $k=1,2, \ldots, n$; this leads to (Problem 7.21)


\begin{equation*}
\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{j k}^{i} \frac{d x^{j}}{d t} \frac{d x^{k}}{d t}=\frac{1}{2 w} \frac{d w}{d t} \frac{d x^{i}}{d t} \quad(i=1,2, \ldots, n) \tag{7.12}
\end{equation*}


System (7.12), with $w$ defined by (7.11), are the differential equations for the geodesics of Riemannian space, in terms of the arbitrary curve parameter $t$. Assuming that these geodesics will be regular curves, we may choose $t=s=\operatorname{arc}$ length. Then

$$
w=\left(\frac{d s}{d t}\right)^{2}=\left(\frac{d s}{d s}\right)^{2}=(1)^{2}=1 \quad \text { and } \quad \frac{d w}{d s}=0
$$

so that $(7.12)$ becomes


\begin{equation*}
\frac{d^{2} x^{i}}{d s^{2}}+\Gamma_{j k}^{i} \frac{d x^{j}}{d s} \frac{d x^{k}}{d s}=0 \quad(i=1,2, \ldots, n) \tag{7.13}
\end{equation*}


which is precisely (6.13).

It must be emphasized that $L^{\prime}(0)=0$ is only a necessary condition for minimum length, so that the geodesics are found among the solutions of (7.12) or (7.13).

\section*{Null Geodesics}
Consider the case of indefinite metrics and class $\mathscr{C}^{2}$ curves which may have one or more null points. Since, at a null point, $w=0$ in (7.11) the variational theory breaks down, because $L(u)$ fails\\
to be differentiable at such a point. Analogous to the zero-curvature approach, we consider the more general condition for geodesics


\begin{equation*}
\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{j k}^{i} \frac{d x^{j}}{d t} \frac{d x^{k}}{d t} \equiv \frac{\delta U^{i}}{\delta t}=0 \quad(i=1,2, \ldots, n) \tag{7.14}
\end{equation*}


where $U=U^{i}=\left(d x^{i} / d t\right)$ is the tangent vector field. By properties of absolute differentiation,

$$
\frac{d w}{d t}=\frac{d}{d t}\left(\varepsilon g_{i j} U^{i} U^{j}\right)=\frac{\delta}{\delta t}\left(\varepsilon g_{i j} U^{i} U^{j}\right)=2 \varepsilon g_{i j} U^{i} \frac{\delta U^{j}}{\delta t}=0
$$

along a solution curve to (7.14); so $w=$ const. along the curve. Since the curve has at least one point of nullity, $w=0$ at all points, whence the curve is a null curve-called a null geodesic. In summary, the following system of $n+1$ ordinary differential equations in the $n$ unknown functions $x^{i}(t)$ will determine the null geodesics:

$$
\begin{aligned}
& \frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{r s}^{i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}=0 \quad(i=1,2, \ldots, n) \\
& g_{r s} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}=0
\end{aligned}
$$

EXAMPLE 7.6 If the $g_{i j}$ are constants, (7.15) has the expected general solution

$$
x^{i}=x_{0}^{i}+\alpha^{i} t \quad \text { with } \quad g_{i j} \alpha^{i} \alpha^{j}=0
$$

Imagining the $x^{i}$ to be rectangular coordinates, we interpret the null geodesics as a bundle of straight lines issuing from the arbitrary point $\mathbf{x}_{0}$; each line is in the direction of some null vector $\boldsymbol{\alpha}$. By elimination of the $\alpha^{i}$ the equation of the bundle is found to be

$$
g_{i j}\left(x^{i}-x_{0}^{i}\right)\left(x^{j}-x_{0}^{j}\right)=0
$$

In particular, for the space of Special Relativity $\left(g_{11}=g_{22}=g_{33}=-g_{44}=1, g_{i j}=0\right.$ for $i \neq j$ ), the null geodesics compose the $45^{\circ}$ cone

-see Fig. 7-2.

$$
\left(x^{1}-x_{0}^{1}\right)^{2}+\left(x^{2}-x_{0}^{2}\right)^{2}+\left(x^{3}-x_{0}^{3}\right)^{2}=\left(x^{4}-x_{0}^{4}\right)^{2}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-098}
\end{center}

Fig. 7-2

\section*{Solved Problems}
\section*{LENGTH IN RIEMANNIAN SPACE}
7.1 Determine the indicator of the tangent vector $\mathrm{U}$ to the curve

$$
x^{1}=t^{3} \quad x^{2}=t^{2} \quad x^{3}=t
$$

$(-\infty<t<\infty)$ if the fundamental form is

(a) $\left(d x^{1}\right)^{2}+\left(x^{2}\right)^{2}\left(d x^{2}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{3}\right)^{2}-6 d x^{1} d x^{3}+2 x^{1} x^{2} d x^{2} d x^{3}$

(b) $\left(d x^{1}\right)^{2}+2\left(d x^{2}\right)^{2}+3\left(d x^{3}\right)^{2}$

(a) $\left(3 t^{2}\right)^{2}+t^{4}(2 t)^{2}+t^{6}(1)^{2}-6\left(3 t^{2}\right)(1)+2\left(t^{3}\right)\left(t^{2}\right)(2 t)(1)=9 t^{6}+9 t^{4}-18 t^{2}=9 t^{2}\left(t^{2}+2\right)\left(t^{2}-1\right)$

Since $t^{2}+2$ is always positive,

$$
\varepsilon(\mathbf{U})=\left\{\begin{array}{rc}
+1 & t \geqq 1 \\
-1 & 0<t<1 \\
+1 & t=0 \\
-1 & -1<t<0 \\
+1 & t \leqq-1
\end{array}\right.
$$

(b) $\varepsilon(\mathbf{U}) \equiv+1$, because the form is positive definite.

7.2 Show that the following matrix defines a Riemannian metric on $\mathbf{R}^{2}$ :

$$
G=\left[\begin{array}{rr}
x^{2} & -x^{1} \\
-x^{1} & x^{2}
\end{array}\right] \quad\left(x^{1}>0,-x^{1}<x^{2}<x^{1}\right)
$$

We must show that conditions A-D of Section 5.3 are satisfied.

A. Since each $g_{i j}$ is linear in the $x^{i}$, it is differentiable to any order.

B. By observation, the matrix is symmetric.\\
C. $\left|g_{i j}\right|=\left(x^{2}\right)^{2}-\left(x^{1}\right)^{2}<0$ over the given domain.

D. Extend the matrix to a tensor $\mathbf{g}$ by using the tensor transformation laws to define the $\bar{g}_{i j}$ in terms of the $g_{i j}$. This will then make the quadratic form $g_{i j} d x^{i} d x^{j}$, hence the distance formula, an invariant.

7.3 Find the null set of the curve $\mathscr{C}: x^{2}=\left(x^{1}\right)^{2} \quad\left(x^{1}>0\right)$ under the metric of Problem 7.2.

Let $\mathscr{C}$ be parameterized by $x^{1}=t, x^{2}=t^{2} \quad(t>0)$. Then, along $\mathscr{C}$,

$$
g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\left[\begin{array}{ll}
1 & 2 t
\end{array}\right]\left[\begin{array}{cc}
t^{2} & -t \\
-t & t^{2}
\end{array}\right]\left[\begin{array}{c}
1 \\
2 t
\end{array}\right]=\left[\begin{array}{ll}
1 & 2 t
\end{array}\right]\left[\begin{array}{c}
-t^{2} \\
-t+2 t^{3}
\end{array}\right]=t^{2}\left(4 t^{2}-3\right)
$$

which, for positive $t$, vanishes only at $t=\sqrt{3} / 2$.

7.4 Find the arc length of the curve $\mathscr{C}$ in Problem 7.3 from $x^{1}=0$ to $x^{1}=1$.

Again using $t=x^{1}$, observe that

Hence,

$$
g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}<0 \quad \text { for } \quad 0<t<\sqrt{3} / 2
$$

$$
\begin{aligned}
L & =\int_{0}^{1} \sqrt{\varepsilon t^{2}\left(4 t^{2}-3\right)} d t=\int_{0}^{\sqrt{3} / 2} t \sqrt{-\left(4 t^{2}-3\right)} d t+\int_{\sqrt{3} / 2}^{1} t \sqrt{4 t^{2}-3} d t \\
& =-\left.\frac{1}{12}\left(3-4 t^{2}\right)^{3 / 2}\right|_{0} ^{\sqrt{3} / 2}+\left.\frac{1}{12}\left(4 t^{2}-3\right)^{3 / 2}\right|_{\sqrt{3} / 2} ^{1}=\frac{3 \sqrt{3}+1}{12} \approx 0.516
\end{aligned}
$$

7.5 Write $g \equiv \operatorname{det} G$ for the determinant of a Riemannian metric. Prove that $|g|$ is a differentiable function of the coordinates.

Applying the chain rule to $|g|=\sqrt{g^{2}}$, we have


\begin{equation*}
\frac{\partial|g|}{\partial x^{i}}=\frac{g}{|g|} \frac{\partial g}{\partial x^{i}} \tag{1}
\end{equation*}


Since $\partial g / \partial x^{i}$ exists (Property A) and $|g| \neq 0$ (Property C), the right-hand side of (1) is well-defined.

7.6 Show that under the metric $\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2}-\left(d x^{3}\right)^{2}-\left(d x^{4}\right)^{2}$ (another version of the metric for Special Relativity), the curve

$$
x^{1}=A \sinh t \quad x^{2}=A \cosh t \quad x^{3}=B t \quad x^{4}=C t \quad(0 \leqq t \leqq 1)
$$

with $A^{2}=B^{2}+C^{2}$, is null at each of its points.

$$
\begin{aligned}
g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t} & =\left(\frac{d x^{1}}{d t}\right)^{2}-\left(\frac{d x^{2}}{d t}\right)^{2}-\left(\frac{d x^{3}}{d t}\right)^{2}-\left(\frac{d x^{4}}{d t}\right)^{2} \\
& =(A \cosh t)^{2}-(A \sinh t)^{2}-B^{2}-C^{2} \\
& =A^{2}\left(\cosh ^{2} t-\sinh ^{2} t\right)-B^{2}-C^{2}=A^{2}-B^{2}-C^{2} \equiv 0
\end{aligned}
$$

7.7 At the point of intersection $(0,0)$, find the angle between the curves

$$
\mathscr{C}_{1}:\left\{\begin{array}{l}
x^{1}=2 t-2 \\
x^{2}=t^{2}-1
\end{array} \quad \mathscr{C}_{2}:\left\{\begin{array}{l}
x^{1}=u^{4}-1 \\
x^{2}=25 u^{2}+50 u-75
\end{array}\right.\right.
$$

if the Riemannian metric is given by $\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-2 d x^{1} d x^{2}$.

At $t=1, \mathbf{T} \equiv\left(d x^{i} / d t\right)=(2,2)$; at $u=1, \mathbf{U} \equiv\left(d x^{i} / d u\right)=(4,100)$. Hence, using matrices,

$$
\begin{aligned}
& \mathbf{T U}=\left[\begin{array}{ll}
2 & 2
\end{array}\right]\left[\begin{array}{rr}
1 & -1 \\
-1 & 0
\end{array}\right]\left[\begin{array}{c}
4 \\
100
\end{array}\right]=-200 \\
& \|\mathbf{T}\|^{2}=\varepsilon_{1}\left[\begin{array}{ll}
2 & 2
\end{array}\right]\left[\begin{array}{rr}
1 & -1 \\
-1 & 0
\end{array}\right]\left[\begin{array}{l}
2 \\
2
\end{array}\right]=\left(\varepsilon_{1}\right)(-4)=4 \\
& \|\mathbf{U}\|^{2}=\varepsilon_{2}\left[\begin{array}{ll}
4 & 100
\end{array}\right]\left[\begin{array}{rr}
1 & -1 \\
-1 & 0
\end{array}\right]\left[\begin{array}{c}
4 \\
100
\end{array}\right]=\left(\varepsilon_{2}\right)(-784)=784
\end{aligned}
$$

$$
\text { and } \quad \cos \theta=\frac{-200}{\sqrt{4} \sqrt{784}}=-\frac{25}{7}
$$

This is Case 2 of Section 7.2; we have

$$
\theta=\pi+i \ln \left(\frac{25}{7}+\sqrt{\left(\frac{25}{7}\right)^{2}-1}\right)=\pi+i \ln 7
$$

7.8 Verify that the vectors of Problem 7.7 do not obey the triangle inequality.

As calculated, $\|\mathbf{T}\|+\|\mathbf{U}\|=2+28=30$. But

$$
\|\mathbf{T}+\mathbf{U}\|^{2}=\varepsilon_{3}\left[\begin{array}{ll}
6 & 102
\end{array}\right]\left[\begin{array}{rr}
1 & -1 \\
-1 & 0
\end{array}\right]\left[\begin{array}{c}
6 \\
102
\end{array}\right]=\varepsilon_{3}(-1188)=1188
$$

whence $\|\mathbf{T}+\mathbf{U}\| \approx 34.46>\|\mathbf{T}\|+\|\mathbf{U}\|$.

\section*{ARC-LENGTH PARAMETER, UNIT TANGENT VECTOR}
7.9 Let $\mathscr{C}: x^{i}=x^{i}(t)$ be any non-null curve. (a) Prove that $\operatorname{arc}$ length along $\mathscr{C}$ is defined as a strictly increasing function of $t$. (b) Exhibit the arc-length parameterization of $\mathscr{C}$.\\
(a)

For $t_{1}<t_{2}$, the Mean-Value Theorem of calculus gives

$$
s\left(t_{2}\right)-s\left(t_{1}\right)=\left(t_{2}-t_{1}\right) s^{\prime}(\tau) \quad\left(t_{1}<\tau<t_{2}\right)
$$

The right-hand side is nonnegative, so that $s\left(t_{1}\right) \leqq s\left(t_{2}\right)$. But, in view of the identity

$$
s\left(t_{2}\right)-s\left(t_{1}\right)=\left[s\left(t_{2}\right)-s\left(t_{3}\right)\right]+\left[s\left(t_{3}\right)-s\left(t_{1}\right)\right]
$$

where $t_{3}$ is any point in $\left(t_{1}, t_{2}\right)$, the equality $s\left(t_{1}\right)=c=s\left(t_{2}\right)$ would imply $s\left(t_{3}\right)=c$; i.e., $s(t)$ would be constant on $\left[t_{1}, t_{2}\right]$, making $s^{\prime}(t) \equiv 0$ on $\left(t_{1}, t_{2}\right)$ and thus making $\mathscr{C}$ a null curve. We conclude that

$$
s\left(t_{1}\right)<s\left(t_{2}\right) \quad \text { whenever } \quad t_{1}<t_{2}
$$

(b) The strictly increasing function $s(t)$ will possess a strictly increasing inverse; denote it as $t=\theta(s)$. Then $\mathscr{C}$ admits the parameterization $x^{i}=x^{i}(\theta(s))$.

7.10 (a) In rectangular coordinates $\left(x^{1}, x^{2}\right)$ but adopting the metric of Problem 7.7, find the null points of the parabola $\mathscr{C}: x^{1}=t, x^{2}=t^{2} \quad\left(0 \leqq t \leqq \frac{1}{2}\right)$. (b) Show that the arc-length parameterization of $\mathscr{C}$ is differentiable to all orders except at the null points. (c) Find the length of $\mathscr{C}$.


\begin{equation*}
\varepsilon\left(\frac{d s}{d t}\right)^{2}=\left(\frac{d x^{1}}{d t}\right)^{2}-2 \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}=1-4 t \tag{a}
\end{equation*}


so there is only one null point, at $t=1 / 4$.


\begin{equation*}
s=\int_{0}^{t} \sqrt{\varepsilon(1-4 u)} d u \tag{b}
\end{equation*}


Thus, for $0 \leqq t \leqq 1 / 4$,

$$
s=\int_{0}^{t} \sqrt{1-4 u} d u=\frac{1}{6}\left[1-(1-4 t)^{3 / 2}\right]
$$

and, for $1 / 4 \leqq t \leqq 1 / 2$,

$$
s=\int_{0}^{1 / 4} \sqrt{1-4 u} d u+\int_{1 / 4}^{t} \sqrt{4 u-1} d u=\frac{1}{6}\left[1+(4 t-1)^{3 / 2}\right]
$$

Inversion of these formulas gives

\[
t=\theta(s)=\left\{\begin{array}{lr}
\frac{1}{4}\left[1-(1-6 s)^{2 / 3}\right] & 0 \leqq s \leqq 1 / 6  \tag{1}\\
\frac{1}{4}\left[1+(6 s-1)^{2 / 3}\right] & 1 / 6 \leqq s \leqq 1 / 3
\end{array}\right.
\]

It is evident that $\theta(s)$ is infinitely differentiable except at the null point $s=1 / 6$ (the image of $t=1 / 4)$; the same will be true of the functions $x^{1}=\theta(s), x^{2}=\theta^{2}(s)$.

(c) Set $t=1 / 2$ in the applicable expression for $s$ :

$$
s=\frac{1}{6}\left[1+(2-1)^{3 / 2}\right]=\frac{1}{3}
$$

7.11 Find the arc length of the same curve $\mathscr{C}$ as in Problem 7.10, but with the normal Euclidean metric, $d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2}$.

Now

$$
\frac{d s}{d t}=\sqrt{\left(\frac{d x^{1}}{d t}\right)^{2}+\left(\frac{d x^{2}}{d t}\right)^{2}}=\sqrt{4 t^{2}+1}
$$

so that

$$
L=\int_{0}^{1 / 2} \sqrt{4 t^{2}+1} d t=\left[\frac{t}{2} \sqrt{4 t^{2}+1}+\frac{1}{4} \ln \left(2 t+\sqrt{4 t^{2}+1}\right)\right]_{0}^{1 / 2}=\frac{\sqrt{2}+\ln (1+\sqrt{2})}{4} \approx 0.574
$$

as compared to $L \approx 0.333$ in Problem 7.10 .

7.12 Using the arc-length parameterization found for the curve $\mathscr{C}$ in Problem 7.10(b), compute the components $T^{i}(s)$ of the tangent vector and verify that this vector has unit length for all $s \neq 1 / 6$.

We have $\left(T^{i}\right)=\left(\theta^{\prime}, 2 \theta \theta^{\prime}\right)$, where $\theta=\theta(s)$ is the function defined by (1) in Problem $7.10(b)$. Hence,

$$
\|\mathbf{T}\|^{2}=\varepsilon\left(\theta^{\prime 2}-4 \theta \theta^{\prime 2}\right)=\varepsilon(1-4 \theta) \theta^{\prime 2}
$$

But, by (1) of Problem 7.10(b),

$$
1-4 \theta=\left\{\begin{array}{rrr}
(1-6 s)^{2 / 3} & 0 \leqq s \leqq 1 / 6 \\
-(6 s-1)^{2 / 3} & 1 / 6 \leqq s \leqq 1 / 3
\end{array} \quad \theta^{\prime}=\left\{\begin{array}{rr}
(1-6 s)^{-1 / 3} & 0<s<1 / 6 \\
(6 s-1)^{-1 / 3} & 1 / 6<s<1 / 3
\end{array}\right.\right.
$$

Therefore, $\|\mathbf{T}\|^{2}=(\varepsilon)( \pm 1)=+1$, or $\|\mathbf{T}\| \equiv 1 \quad(s \neq 1 / 6)$.

\section*{UNIT PRINCIPAL NORMAL, CURVATURE}
7.13 Prove that the Frenet equation (7.9) holds at each point of a regular curve when the metric is positive definite.

At a point where $\|\delta \mathbf{T} / \delta s\| \neq 0$, we have (from property $\mathrm{C}$ of $\mathbf{N}$ ),


\begin{equation*}
\mathbf{N}=\lambda \frac{\delta \mathbf{T}}{\delta s} \tag{1}
\end{equation*}


from some real $\lambda$. Take the inner product with the vector $\mathbf{N}$ in (1); with $\varepsilon=\varepsilon(\mathbf{N})$,


\begin{equation*}
\varepsilon \mathbf{N}^{2}=\varepsilon \lambda \mathbf{N} \frac{\delta \mathbf{T}}{\delta s}=\lambda \kappa \quad \text { or } \quad 1=\lambda \kappa \tag{2}
\end{equation*}


Then $\lambda=1 / \kappa$, and substitution into (1) yields (7.9).

At a point where $\|\delta \mathbf{T} / \delta s\|=0$, both $\delta \mathbf{T} / \delta s=\mathbf{0}$ (because the metric is positive definite) and $\kappa=0$ (by $(7.8)$ ); the Frenet equation then holds trivially.

7.14 For any regular two-dimensional curve $\mathscr{C}: x^{i}=x^{i}(s)$, define the contravariant vector


\begin{equation*}
\mathbf{N}=\left(N^{i}\right) \equiv\left(-T_{2} / \sqrt{|g|}, T_{1} / \sqrt{|g|}\right) \tag{7.16}
\end{equation*}


where $\mathbf{T}=\left(T^{i}\right)$ is the unit tangent vector along $\mathscr{C}$ and $g=\operatorname{det}\left(g_{i j}\right)$. Show that $\mathbf{N}$ is a global unit normal for $\mathscr{C}$.

We must show that the three properties of Section 7.5 are possessed by the given vector (except possibly at null points).

A. Since $\mathscr{C}$ is regular, the $T^{i}$, and with them the $T_{i}=g_{i j} T^{j}$, are in $C^{1}$. The same is true of $|g|$ (Problem 7.5), which function is strictly positive. Therefore, the $N^{i}$ are also in $C^{1}$.

B. By (2.11), $g^{11}=g_{22} / g, g^{12}=g^{21}=-g_{12} / g$, and $g^{22}=g_{11} / g$. Hence,

$$
\begin{aligned}
\|\mathbf{N}\|^{2} & =\left|g_{11}\left(T_{2}^{2} /|g|\right)+2 g_{12}\left(-T_{1} T_{2} /|g|\right)+g_{22}\left(T_{1}^{2} /|g|\right)\right| \\
& =\frac{1}{|g|}\left|g g^{22} T_{2}^{2}+2 g g^{12} T_{1} T_{2}+g g^{11} T_{1}^{2}\right| \\
& =\frac{|g|}{|g|}\left|g^{i j} T_{i} T_{j}\right|=\left|T^{j} T_{j}\right|=\|\mathbf{T}\|^{2}
\end{aligned}
$$

and so $\|\mathbf{N}\|=\|\mathbf{T}\|=1$.\\
C. $\mathbf{N}$ is orthogonal to $\mathbf{T}$ :

$$
N^{i} T_{i}=-\frac{T_{2}}{\sqrt{|g|}} T_{1}+\frac{T_{1}}{\sqrt{|g|}} T_{2}=0
$$

Furthermore, when $\|\delta \mathbf{T} / \delta s\| \neq 0$, then $\mathbf{N}_{0}$ is defined and is also a vector orthogonal to $\mathbf{T}$ (by Theorem 7.1). In two dimensions this implies $\mathbf{N}= \pm \mathbf{N}_{0}=\lambda(\delta \mathbf{T} / \delta s)$.

7.15 For the curve and metric of Problem 7.10, determine the local normal $\mathbf{N}_{0}$ and, using Problem 7.14, a global normal $\mathbf{N}$. Verify that the two stand in the proper relationship.

We have $g_{11}=1, g_{12}=g_{21}=-1, g_{22}=0$ (all constants), and $\mathbf{T}=\left(\theta^{\prime}, 2 \theta \theta^{\prime}\right)$; therefore, for $s \neq 1 / 6$,

and

$$
\begin{aligned}
& \frac{\delta \mathbf{T}}{\delta s}=\frac{d \mathbf{T}}{d s}=\left(\theta^{\prime \prime}, 2 \theta^{\prime 2}+2 \theta \theta^{\prime \prime}\right) \\
& =\left\{\begin{array}{lr}
\left(2(1-6 s)^{-4 / 3},(1-6 s)^{-2 / 3}+(1-6 s)^{-4 / 3}\right) & 0<s<1 / 6 \\
\left(-2(6 s-1)^{-4 / 3},(6 s-1)^{-2 / 3}-(6 s-1)^{-4 / 3}\right) & 1 / 6<s<1 / 3
\end{array}\right. \\
& \left\|\frac{\delta \mathbf{T}}{\delta s}\right\|=\sqrt{\varepsilon g_{i j} \frac{d T^{i}}{d s} \frac{d T^{j}}{d s}}=\left\{\begin{array}{l}
2(1-6 s)^{-1} \\
2(6 s-1)^{-1}
\end{array}\right. \\
& \text { Thus, } \quad \mathbf{N}_{0}=\frac{\delta \mathbf{T}}{\delta s} /\left\|\frac{\delta \mathbf{T}}{\delta s}\right\|=\left\{\begin{array}{l}
\left((1-6 s)^{-1 / 3}, \frac{1}{2}(1-6 s)^{1 / 3}+\frac{1}{2}(1-6 s)^{-1 / 3}\right) \\
\left(-(6 s-1)^{-1 / 3}, \frac{1}{2}(6 s-1)^{1 / 3}-\frac{1}{2}(6 s-1)^{-1 / 3}\right)
\end{array}\right.
\end{aligned}
$$

With $g=-1$, Problem 7.14 gives $(s \neq 1 / 6)$ :

$$
\begin{aligned}
& T_{1}=g_{1 j} T^{j}=T^{1}-T^{2}=\theta^{\prime}(1-2 \theta)=\left\{\begin{array}{l}
\frac{1}{2}(1-6 s)^{-1 / 3}+\frac{1}{2}(1-6 s)^{1 / 3} \\
\frac{1}{2}(6 s-1)^{-1 / 3}-\frac{1}{2}(6 s-1)^{1 / 3}
\end{array}\right. \\
& T_{2}=g_{2 j} T^{j}=-T^{1}=-\theta^{\prime}=\left\{\begin{array}{l}
-(1-6 s)^{-1 / 3} \\
-(6 s-1)^{-1 / 3}
\end{array}\right. \\
& \mathbf{N}=\left(-T_{2}, T_{1}\right)=\left\{\begin{array}{l}
\left((1-6 s)^{-1 / 3}, \frac{1}{2}(1-6 s)^{-1 / 3}+\frac{1}{2}(1-6 s)^{1 / 3}\right) \\
\left((6 s-1)^{-1 / 3}, \frac{1}{2}(6 s-1)^{-1 / 3}-\frac{1}{2}(6 s-1)^{1 / 3}\right)
\end{array}\right.
\end{aligned}
$$

It is seen that, as expected, $\mathbf{N}=+\mathbf{N}_{0}$ for $s<1 / 6$ and $\mathbf{N}=-\mathbf{N}_{0}$ for $s>1 / 6$. neither $\mathbf{N}_{0}$ nor $\mathbf{N}$ is defined at the null point $s=1 / 6$. For comparison, recall the situation in Examples 7.4 and 7.5: there the discontinuity in $\mathbf{N}_{0}$ occurred at a regular point (the cubic has no null points under the Euclidean metric), and $\mathbf{N}$ (either choice) was defined everywhere.

7.16 Under the metric of Special Relativity (Example 7.1), a regular curve $\mathscr{C}$ is given by

$$
x^{1}=s^{2} \quad x^{2}=\frac{3 s}{5} \quad x^{3}=\frac{4 s}{5} \quad x^{4}=s^{2}
$$

for $0 \leqq s \leqq 1$. (a) Verify that $s$ is arc length for $\mathscr{C}$, and show that the absolute derivative, $\delta \mathbf{T} / \delta s$, of $\mathbf{T}$ is a null vector at every point of the curve (hence, a local principal normal $\mathbf{N}_{0}$ is nowhere defined on $\mathscr{C}$ ). Construct a global principal normal for $\mathscr{C}$ in such a manner that the corresponding curvature function is nonzero. Is more than one curvature function possible?

(a) We have $\left(T^{i}\right)=(2 s, 3 / 5,4 / 5,2 s)$ and

$$
\left|g_{i j} T^{i} T^{j}\right|=\left|4 s^{2}+(9 / 25)+(16 / 25)-4 s^{2}\right|=1
$$

hence, $s$ is an arc-length parameter. Also, since the $g_{i j}$ are constant, all Christoffel symbols vanish and

$$
\frac{\delta \mathbf{T}}{\delta s}=\frac{d \mathbf{T}}{d s}=(2,0,0,2) \quad\left\|\frac{\delta T}{\delta s}\right\|=\sqrt{\left|2^{2}+0^{2}+0^{2}-2^{2}\right|}=0
$$

for all $s$.

(b) Any differentiable unit vector orthogonal to $\mathbf{T}$ will do for $\mathbf{N}$, which then determines the curvature through (7.8). In the orthonormality conditions

$$
\begin{aligned}
& 2 s N^{1}+\frac{3}{5} N^{2}+\frac{4}{5} N^{3}-2 s N^{4}=0 \\
& \left(N^{1}\right)^{2}+\left(N^{2}\right)^{2}+\left(N^{3}\right)^{2}-\left(N^{4}\right)^{2}= \pm 1
\end{aligned}
$$

we may successively set $N^{1}=N^{4}=0, N^{3}=N^{4}=0$, and $N^{2}=N^{4}=0$ to obtain three candidate normals:

$$
\begin{gathered}
\mathbf{N}_{1}=\left(0,-\frac{4}{5}, \frac{3}{5}, 0\right) \quad \mathbf{N}_{2}=\frac{1}{\sqrt{(9 / 25)+4 s^{2}}}\left(-\frac{3}{5}, 2 s, 0,0\right) \\
\mathbf{N}_{3}=\frac{1}{\sqrt{(16 / 25)+4 s^{2}}}\left(-\frac{4}{5}, 0,2 s, 0\right)
\end{gathered}
$$

The constant $\mathbf{N}_{1}$ yields $\kappa_{1} \equiv 0$; but $\mathbf{N}_{2}$ and $\mathbf{N}_{3}$ yield the distinct curvature functions

$$
\kappa_{2}=\frac{-1}{\sqrt{\frac{1}{4}+\frac{25}{9} s^{2}}} \quad \kappa_{3}=\frac{-1}{\sqrt{\frac{1}{4}+\frac{25}{16} s^{2}}}
$$

Note that the Frenet equation is invalid for all these normals.

7.17 Refer to Problems 7.10 and 7.15. Calculate the curvature functions $\kappa_{0}$ and $\kappa$, and discuss the variability of $\kappa_{0}$ over the parabolic arc $0 \leqq s \leqq 1 / 3$.

Our previous results show that both curvatures are defined everywhere except $s=1 / 6$, with $\kappa=\kappa_{0}$ on $0 \leqq s<1 / 6$ and $\kappa=-\kappa_{0}$ on $1 / 6<s \leqq 1 / 3$ (cf. Problem 7.13). By Problem 7.15,

$$
\kappa_{0}=\left\|\frac{\delta \mathbf{T}}{\delta s}\right\|=\frac{2}{|1-6 s|} \quad(s \neq 1 / 6)
$$

whence

$$
\kappa=\frac{2}{1-6 s} \quad(s \neq 1 / 6)
$$

It is seen that $\kappa_{0}$ has the same value, 2, at $s=0$ (the vertex, or point of greatest Euclidean curvature) and the undistinguished point $s=1 / 3$. Moreover, near the ordinary (from the Euclidean viewpoint) point $s=1 / 6$, the absolute curvature becomes arbitrarily large.

7.18 (a) For any regular two-dimensional curve, derive the formula for absolute curvature


\begin{equation*}
\kappa_{0}=\sqrt{|g|}\left|T^{1} \frac{\delta T^{2}}{\delta s}-T^{2} \frac{\delta T^{1}}{\delta s}\right| \tag{7.17}
\end{equation*}


(b) Use (7.17) to check Problem 7.17.

(a) By (7.8) and the remarks made following Example 7.5,


\begin{equation*}
\kappa_{0}=|\kappa|=\left|N_{j} \frac{\delta T^{j}}{\delta s}\right|=\left|N_{1} \frac{\delta T^{1}}{\delta s}+N_{2} \frac{\delta T^{2}}{\delta s}\right| \tag{1}
\end{equation*}


Choosing the global normal $\left(N^{i}\right)$ established in Problem 7.14, we have:

$$
\begin{aligned}
N_{1} & =g_{11} N^{1}+g_{12} N^{2}=\left(g g^{22}\right)\left(\frac{-T_{2}}{\sqrt{|g|}}\right)+\left(-g g^{21}\right)\left(\frac{T_{1}}{\sqrt{|g|}}\right)=-\frac{\gamma}{\sqrt{|g|}}\left(g^{21} T_{1}+g^{22} T_{2}\right)=-\frac{\gamma}{\sqrt{|g|}} T^{2} \\
N_{2} & =g_{21} N^{1}+g_{22} N^{2}=\left(-g g^{12}\right)\left(\frac{-T_{2}}{\sqrt{|g|}}\right)+\left(g g^{11}\right)\left(\frac{T_{1}}{\sqrt{|g|}}\right) \\
& =\frac{\gamma}{\sqrt{|g|}}\left(g^{11} T_{1}+g^{12} T_{2}\right)=\frac{\gamma}{\sqrt{|g|}} T^{1}
\end{aligned}
$$

Substitution of these components in (1) yields (7.17).

(b) For the metric of Problem 7.10,

$$
G=\left[\begin{array}{rr}
1 & -1 \\
-1 & 0
\end{array}\right]
$$

$g=\operatorname{det} G=-1$ and absolute derivatives reduce to ordinary derivatives. Thus we can rewrite (7.17) in terms of the curve parameter $t$, as follows:

$$
\kappa_{0}=\left|T^{1} \frac{d T^{2}}{d t} \frac{d t}{d s}-T^{2} \frac{d T^{1}}{d t} \frac{d t}{d s}\right|=\frac{1}{s^{\prime}(t)}\left|T^{1} \frac{d T^{2}}{d t}-T^{2} \frac{d T^{1}}{d t}\right|=\frac{\left(T^{1}\right)^{2}}{s^{\prime}(t)}\left|\frac{d}{d t}\left(\frac{T^{2}}{T^{1}}\right)\right|
$$

Substitution of $s^{\prime}(t)=\sqrt{|1-4 t|} \quad(t \neq 1 / 4)$ and the components of the unit tangent vector,

$$
T^{1}=\frac{1}{s^{\prime}(t)} \frac{d x^{1}}{d t} \equiv \frac{1}{s^{\prime}(t)} \quad T^{2}=\frac{1}{s^{\prime}(t)} \frac{d x^{2}}{d t}=\frac{2 t}{s^{\prime}(t)}
$$

gives:

$$
\kappa_{0}=\frac{2}{\left(s^{\prime}(t)\right)^{3}}=\frac{2}{|1-4 t|^{3 / 2}} \quad(t \neq 1 / 4)
$$

From Problem 7.12,

$$
|1-4 t|=|1-4 \theta(s)|=|1-6 s|^{2 / 3}
$$

yielding exact agreement with Problem 7.17.

7.19 Compute the absolute curvature of the logarithmic curve $\mathscr{C}: x^{1}=t, x^{2}=a \ln t$, for $\frac{1}{2} \leqq t<a$, if the Riemannian metric is

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2}
$$

As the $g_{i j}$ are constants (with $g=-1$ ), we can proceed as in Problem 7.18(b). This time, the most convenient version of $(7.17)$ is

$$
\kappa_{0}=\sqrt{|g|} \frac{\left(T^{2}\right)^{2}}{s^{\prime}(t)}\left|\frac{d}{d t}\left(\frac{T^{1}}{T^{2}}\right)\right|
$$

Substituting

$$
\begin{aligned}
& s^{\prime}(t)=\sqrt{\left|\left(\frac{d x^{1}}{d t}\right)^{2}-\left(\frac{d x^{2}}{d t}\right)^{2}\right|}=\sqrt{\left|1-\frac{a^{2}}{t^{2}}\right|}=\frac{1}{t} \sqrt{a^{2}-t^{2}}(\neq 0) \\
& T^{1}=\frac{1}{s^{\prime}(t)} \frac{d x^{1}}{d t}=\frac{t}{\sqrt{a^{2}-t^{2}}} \\
& T^{2}=\frac{1}{s^{\prime}(t)} \frac{d x^{2}}{d t}=\frac{a}{\sqrt{a^{2}-t^{2}}}
\end{aligned}
$$

we find: $\kappa_{0}=a t\left(a^{2}-t^{2}\right)^{-3 / 2}$.

\subsection*{7.20 Prove Theorem 7.1.}
Along a regular curve we have

$$
\|\mathbf{T}\|^{2}=\varepsilon \mathbf{T} \mathbf{T}=1 \quad \text { or } \quad \mathbf{T T}=\varepsilon
$$

where the indicator $\varepsilon$ is constant, $|\varepsilon|=1$, on the curve. By the inner-product rule for absolute differentiation, and the fact that the absolute derivative of an invariant is the ordinary derivative,

$$
\frac{\delta \mathbf{T}}{\delta s} \mathbf{T}+\mathbf{T} \frac{\delta \mathbf{T}}{\delta s} \equiv 2 \mathbf{T} \frac{\delta \mathbf{T}}{\delta s}=\frac{d}{d s}(\varepsilon)=0 \quad \text { or } \quad \mathbf{T} \frac{\delta \mathbf{T}}{\delta s}=0
$$

\section*{GEODESICS}
\subsection*{7.21 Establish (7.12).}
Start with the conditions


\begin{equation*}
w^{-1 / 2} \frac{\partial g_{i j}}{\partial x^{k}} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\frac{d}{d t}\left(2 w^{-1 / 2} g_{i k} \frac{d x^{i}}{d t}\right) \tag{1}
\end{equation*}


By use of the product and chain rules, the expression on the right may be written

$$
-w^{-3 / 2} \frac{d w}{d t}\left(g_{i k} \frac{d x^{i}}{d t}\right)+2 w^{-1 / 2}\left(\frac{\partial g_{i k}}{\partial x^{j}} \frac{d x^{j}}{d t}\right) \frac{d x^{i}}{d t}+2 w^{-1 / 2} g_{i k} \frac{d^{2} x^{i}}{d t^{2}}
$$

Put this back in (1), multiply both sides by $w^{1 / 2}$, and go over to the notation $g_{i j k} \equiv \partial g_{i j} / \partial x^{k}$ :

$$
g_{i j k} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=-w^{-1} g_{i k} \frac{d w}{d t} \frac{d x^{i}}{d t}+2 g_{i k j} \frac{d x^{j}}{d t} \frac{d x^{i}}{d t}+2 g_{i k} \frac{d^{2} x^{i}}{d t^{2}}
$$

which rearranges to

$$
2 g_{i k} \frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}+2 g_{i k j} \frac{d x^{j}}{d t} \frac{d x^{i}}{d t}=\frac{1}{w} g_{i k} \frac{d w}{d t} \frac{d x^{i}}{d t}
$$

Making use of the symmetry of $g_{i j}$, the third term on the left may be split into two similar terms, yielding

$$
2 g_{i k} \frac{d^{2} x^{i}}{d t^{2}}-g_{i j k} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}+g_{j k i} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}+g_{k i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\frac{1}{w} g_{i k} \frac{d w}{d t} \frac{d x^{i}}{d t}
$$

Divide by 2 , multiply by $g^{p k}$, and sum on $k$ :

$$
\delta_{i}^{p} \frac{d^{2} x^{i}}{d t^{2}}-g^{p k} \Gamma_{i j k} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\frac{1}{2 w} \delta_{i}^{p} \frac{d w}{d t} \frac{d x^{i}}{d t} \quad \text { or } \quad \frac{d^{2} x^{p}}{d t^{2}}+\Gamma_{j k}^{p} \frac{d x^{j}}{d t} \frac{d x^{k}}{d t}=\frac{1}{2 w} \frac{d w}{d t} \frac{d x^{p}}{d t}
$$

which is $(7.12)$.

7.22 In a Riemannian 2-space with fundamental form $\left(d x^{1}\right)^{2}-\left(x^{2}\right)^{-2}\left(d x^{2}\right)^{2}$, determine $(a)$ the regular geodesics, $(b)$ the null geodesics.

Here $g_{11}=1, g_{12}=g_{21}=0, g_{22}=-\left(x^{2}\right)^{-2}$; Problem 6.4 gives

$$
\Gamma_{22}^{2}=\frac{d}{d x^{2}}\left[\frac{1}{2} \ln \left(x^{2}\right)^{-2}\right]=-\frac{1}{x^{2}}
$$

as the only nonvanishing Christoffel symbol.

(a) The system (7.13) becomes

$$
\frac{d^{2} x^{1}}{d s^{2}}=0 \quad \frac{d^{2} x^{2}}{d s^{2}}-\frac{1}{x^{2}}\left(\frac{d x^{2}}{d s}\right)^{2}=0
$$

The first equation integrates to $x^{1}=a s+x_{0}^{1}$. In the second, let $u \equiv d x^{2} / d s$ :

$$
u \frac{d u}{d x^{2}}-\frac{1}{x^{2}} u^{2}=0 \quad \text { or } \quad \frac{d u}{u}=\frac{d x^{2}}{x^{2}} \quad \text { or } \quad u=c x^{2}
$$

from which

$$
\frac{d x^{2}}{d s}=c x^{2} \quad \text { or } \quad \frac{d x^{2}}{x^{2}}=c d s \quad \text { or } \quad x^{2}=x_{0}^{2} e^{c s}
$$

As our notation indicates, an arbitrary point $\left(x_{0}^{1}, x_{0}^{2}\right)$ is the origin $(s=0)$ of a family of geodesics that seems to depend on two parameters, $a$ and $c$. However, $s$ must represent arc length, so that

$$
\pm 1=\left(\frac{d x^{1}}{d s}\right)^{2}-\left(x^{2}\right)^{-2}\left(\frac{d x^{2}}{d s}\right)^{2}=a^{2}-c^{2}
$$

Hence, either $a^{2}=c^{2}+1$ (the fundamental form is positive) or $c^{2}=a^{2}+1$ (the fundamental form is negative). Both cases may be accounted for by a single parameter, $\lambda$, if $s$ is eliminated between the parametric equations for $x^{1}$ and $x^{2}$ :

regular geodesics $\quad x^{2}=x_{0}^{2} \exp \left[\lambda\left(x^{1}-x_{0}^{1}\right)\right] \quad(|\lambda| \neq 1)$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-107}
\end{center}

Fig. 7-3

(b) System (7.14), in $t$, becomes

$$
\begin{gathered}
\frac{d^{2} x^{1}}{d t^{2}}=0 \quad \frac{d^{2} x^{2}}{d t^{2}}-\frac{1}{x^{2}}\left(\frac{d x^{2}}{d t}\right)^{2}=0 \\
\left(\frac{d x^{1}}{d t}\right)^{2}-\left(x^{2}\right)^{-2}\left(\frac{d x^{2}}{d t}\right)^{2}=0
\end{gathered}
$$

It is clear that the solution may be found by formally replacing $s$ by $t$ in part ( $a$ ) and setting $a^{2}=c^{2}$. Thus, the null geodesics through $\left(x_{0}^{1}, x_{0}^{2}\right)$ are given by

$$
\text { null geodesics } \quad x^{2}=x_{0}^{2} \exp \left[+\left(x^{1}-x_{0}^{1}\right)\right] \quad \text { and } \quad x^{2}=x_{0}^{2} \exp \left[-\left(x^{1}-x_{0}^{1}\right)\right]
$$

Note that the null geodesics correspond to the exceptional values $\lambda= \pm 1$ in part (a). Figure 7-3 is a sketch of the geodesics through the point $(1,-1)$ in cartesian coordinates.

7.23 Without converting to arc length, verify that in spherical coordinates, under the Euclidean metric

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1} d x^{2}\right)^{2}+\left(x^{1} \sin x^{2} d x^{3}\right)^{2}
$$

any curve of the form $\mathscr{C}: x^{1}=a \sec t, x^{2}=t+b, x^{3}=c(a, b, c$ constant) is a geodesic. (It should be apparent that $\mathscr{C}$ is a straight line.)

The equations (7.12) must be verified. The Christoffel symbols $\Gamma_{j k}^{i}$ for spherical coordinates are (Problem 6.5):

$$
i=1 \quad \Gamma_{22}^{1}=-x^{1}, \quad \Gamma_{33}^{1}=-x^{1} \sin ^{2} x^{2}
$$

$$
\begin{array}{lll}
i=2 & \Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{1}{x^{1}}, & \Gamma_{33}^{2}=-\sin x^{2} \cos x^{2} \\
i=3 & \Gamma_{13}^{3}=\Gamma_{31}^{3}=\frac{1}{x^{1}}, & \Gamma_{23}^{3}=\Gamma_{32}^{3}=\cot x^{2}
\end{array}
$$

The derivatives of the $x^{i}(t)$ are:

$$
\begin{gathered}
\frac{d x^{1}}{d t}=a \sec t \tan t, \quad \frac{d^{2} x^{1}}{d t^{2}}=(a \sec t)\left(\tan ^{2} t+\sec ^{2} t\right) \\
\frac{d x^{2}}{d t}=1, \quad \frac{d^{2} x^{2}}{d t^{2}}=0 \quad \text { and } \quad \frac{d x^{3}}{d t}=\frac{d^{2} x^{3}}{d t^{2}}=0
\end{gathered}
$$

With $\varepsilon \equiv 1,(7.11)$ gives

and

$$
\begin{aligned}
& w=g_{i j} \frac{d x^{i}}{d t} \frac{d x^{j}}{d t}=\left(\frac{d x^{1}}{d t}\right)^{2}+\left(x^{1}\right)^{2}\left(\frac{d x^{2}}{d t}\right)^{2}+\left(x^{1} \sin x^{2}\right)^{2}\left(\frac{d x^{3}}{d t}\right)^{2} \\
&=(a \sec t \tan t)^{2}+(a \sec t)^{2}(1)^{2}+0=a^{2} \sec ^{4} t \\
& \frac{1}{2 w} \frac{d w}{d t}=\frac{\left(4 a^{2} \sec ^{3} t\right)(\sec t \tan t)}{2 a^{2} \sec ^{4} t}=2 \tan t
\end{aligned}
$$

For convenience in the verification of (7.12), let LS denote the left side, and RS the right side, of the equation in question. We obtain:

$$
\begin{aligned}
i=\mathbf{1} \quad \mathrm{LS} & =\frac{d^{2} x^{1}}{d t^{2}}+\Gamma_{22}^{1}\left(\frac{d x^{2}}{d t}\right)^{2}+\Gamma_{33}^{1}\left(\frac{d x^{3}}{d t}\right)^{2} \\
& =(a \sec t)\left(\tan ^{2} t+\sec ^{2} t\right)-(a \sec t)(1)^{2}+0=2 a \sec t \tan ^{2} t \\
\mathrm{RS} & =(2 \tan t) \frac{d x^{1}}{d t}=(2 \tan t)(a \sec t \tan t)=2 a \sec t \tan ^{2} t=\mathrm{LS} \\
i=\mathbf{2} \quad \mathrm{LS} & =\frac{d^{2} x^{2}}{d t^{2}}+2 \Gamma_{12}^{2} \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}+\Gamma_{33}^{2}\left(\frac{d x^{3}}{d t}\right)^{2}=0+\frac{2}{a \sec t}(a \sec t \tan t)(1)+0=2 \tan t \\
\mathrm{RS} & =(2 \tan t) \frac{d x^{2}}{d t}=2 \tan t=\mathrm{LS} \\
\boldsymbol{i}=\mathbf{3} \quad \mathrm{LS} & =\frac{d^{2} x^{3}}{d t^{2}}+2 \Gamma_{13}^{3} \frac{d x^{1}}{d t} \frac{d x^{3}}{d t}+2 \Gamma_{23}^{3} \frac{d x^{2}}{d t} \frac{d x^{3}}{d t}=0 \\
\mathrm{RS} & =(2 \tan t) \frac{d x^{3}}{d t}=0=\mathrm{LS}
\end{aligned}
$$

\section*{Supplementary Problems}
7.24 Determine the fundamental indicator $\varepsilon(\mathbf{U})$ if $\left(U^{i}\right)=(2 t,-2 t, 1)$ at the point $\left(x^{i}\right)=\left(t^{2},-t^{2}, t\right)$. The Riemannian metric is given by

$$
\left(g_{i j}\right)=\left[\begin{array}{ccc}
2 x^{1} & x^{3} & 0 \\
x^{3} & 2 x^{2} & 0 \\
0 & 0 & 1
\end{array}\right] \quad\left(4 x^{1} x^{2} \neq\left(x^{3}\right)^{2}\right)
$$

7.25 Find the null points of the curve $\mathscr{C}: x^{1}=t, x^{2}=t^{4} \quad$ ( $t$ real), if the metric is

$$
\varepsilon d s^{2}=8\left(x^{1} d x^{1}\right)^{2}-2 d x^{1} d x^{2}
$$

7.26 Find the arc length of the curve in Problem 7.25 if $0 \leqq t \leqq 2$.

7.27 Find the null points of the curve $\mathscr{C}: x^{1}=t^{3}+1, x^{2}=t^{2}, x^{3}=t$, if the metric is

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2}-\left(x^{3} d x^{3}\right)^{2}
$$

7.28 Find the arc length of the curve in Problem 7.27 if $\frac{1}{2} \leqq t \leqq 1$.

7.29 Find the angle between the curves

$$
\mathscr{C}_{1}:\left\{\begin{array}{l}
x^{1}=5 t \\
x^{2}=2 \\
x^{3}=3 t
\end{array} \quad \mathscr{C}_{2}:\left\{\begin{array}{l}
x^{1}=u \\
x^{2}=2 \\
x^{3}=3 u^{2} / 25
\end{array}\right.\right.
$$

at each of the points of intersection, if the fundamental form is $\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2}-\left(d x^{3}\right)^{2}$.

7.30 If $\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2},(a)$ find the length $L$ of the curve $\mathscr{C}: x^{1}=12 t^{2}, x^{2}=8 t^{3}$, for $0 \leqq t \leqq 2$. (b) Find an arc-length parameterization, $x^{i}=x^{i}(s)$, for $\mathscr{C}$, with $s=0$ corresponding to $t=1$. (c) Show that the $x^{i}(s)$ are differentiable to all orders except at points of nullity.

7.31 Find the arc length of the curve of Problem.7.30, but with the Euclidean metric.

7.32 Compute $\mathbf{T}=\left(d x^{i} / d s\right)$ from the arc-length parameterization found in Problem 7.30 and verify that $\mathbf{T}$ has unit length at all points except $s=0$.

7.33 Calculate the components $N^{i}$ of the unit principal normal of the curve of Problem 7.30, using (7.16) (Problem 7.14).

7.34 Calculate both the curvature $\kappa$ and the absolute curvature $\kappa_{0}$ for the curve of Problem 7.30. Discuss the numerical behavior of $\kappa_{0}$ along the curve.

7.35 Use the formula of Problem 7.18(b) to confirm the value of $\kappa_{0}$ found in Problem 7.34.

7.36 Compute $\kappa_{0}$ under the Euclidean metric for the curve of Problem 7.30; compare with the result obtained in Problem 7.34. For convenience, let $t=0$ correspond to $s=8$.

7.37 Without calculating an arc-length parameter, find the vectors $\mathbf{T}$ and $\mathbf{N}$, and the curvature $\kappa$, for the "parabola" $x^{1}=t, x^{2}=t^{2} \quad\left(0 \leqq t \leqq \frac{1}{2}\right)$ under the Riemannian metric

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-2 d x^{1} d x^{2}
$$

7.38 Show that the first-quadrant portion $\left(x^{i}>0\right)$ of the hypocycloid $\mathscr{H}$ of four cusps

$$
\left(x^{1}\right)^{2 / 3}+\left(x^{2}\right)^{2 / 3}=a^{2 / 3} \quad(a>0)
$$

may be parameterized as $x^{1}=a \cos ^{3} t, x^{2}=a \sin ^{3} t$, with $0 \leqq t \leqq \pi / 2$. Find the arc length under the two metrics

$$
\text { (a) } \varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2} \quad \text { (b) } \quad d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2}
$$

(c) Without computing an arc-length parameter, find $\mathbf{T}$ and $\kappa_{0}$ for $\mathscr{H}$ under both metrics.

7.39 (a) Determine the Christoffel symbols of the second kind for the Riemannian metric $\varepsilon d s^{2}=x^{1}\left(d x^{1}\right)^{2}+$ $x^{2}\left(d x^{2}\right)^{2}$. (b) Without converting to an arc-length parameter, verify that all curves $x^{1}=t^{2}, x^{2}=$ $\left(a t^{3}+b\right)^{2 / 3}$, where $a$ and $b$ are arbitrary constants, are geodesics.

\section*{Chapter 8}
\section*{Riemannian Curvature}
\subsection*{8.1 THE RIEMANN TENSOR}
The Riemann tensor emerges from an analysis of a simple question. Starting with a covariant vector $\left(V_{i}\right)$ and taking the covariant derivative with respect to $x^{j}$ and then with respect to $x^{k}$ produces the third-order tensor

$$
\left(\left(V_{i}\right)_{, j}\right)_{, k} \equiv\left(V_{i, j k}\right)
$$

Does the order of differentiation matter, or does $V_{i, j k}=V_{i, k j}$ hold in general?

Standard hypotheses concerning differentiability suffice to guarantee that the partial derivative of order two is order-independent,

$$
\frac{\partial^{2} V_{i}}{\partial x^{j} \partial x^{k}}=\frac{\partial^{2} V_{i}}{\partial x^{k} \partial x^{j}}
$$

but due to the presence of Christoffel symbols, such hypotheses do not extend to covariant differentiation. The following formula is established in Problem 8.1:

where


\begin{gather*}
V_{j, k l}-V_{j, l k}=R_{j k l}^{i} V_{i}  \tag{8.1}\\
R_{j k l}^{i} \equiv \frac{\partial \Gamma_{j l}^{i}}{\partial x^{k}}-\frac{\partial \Gamma_{j k}^{i}}{\partial x^{l}}+\Gamma_{j l}^{r} \Gamma_{r k}^{i}-\Gamma_{j k}^{r} \Gamma_{r l}^{i} \tag{8.2}
\end{gather*}


The Quotient Theorem (covariant form) immediately implies

Theorem 8.1: The $n^{4}$ components defined by (8.2) are those of a fourth-order tensor, contravariant of order one, covariant of order three.

$\left(R_{j k l}^{i}\right)$ is called the Riemann (or Riemann-Christoffel) tensor of the second kind; lowering the contravariant index produces


\begin{equation*}
R_{i j k l} \equiv g_{i r} R_{j k l}^{r} \tag{8.3}
\end{equation*}


the Riemann tensor of the first kind.

In answer to our original question, we may now say that covariant differentiation is orderdependent unless the metric is such as to make the Riemann tensor (either kind) vanish.

\subsection*{8.2 PROPERTIES OF THE RIEMANN TENSOR}
\section*{Two Important Formulas}
The Riemann tensor of the first kind can be introduced independently via the following formula (see Problem 8.4):


\begin{equation*}
R_{i j k l}=\frac{\partial \Gamma_{j l i}}{\partial x^{k}}-\frac{\partial \Gamma_{j k i}}{\partial x^{l}}+\Gamma_{i l r} \Gamma_{j k}^{r}-\Gamma_{i k r} \Gamma_{j l}^{r} \tag{8.4}
\end{equation*}


From (8.4) there follows


\begin{equation*}
R_{i j k l}=\frac{1}{2}\left(\frac{\partial^{2} g_{i l}}{\partial x^{j} \partial x^{k}}+\frac{\partial^{2} g_{j k}}{\partial x^{i} \partial x^{l}}-\frac{\partial^{2} g_{i k}}{\partial x^{j} \partial x^{l}}-\frac{\partial^{2} g_{j l}}{\partial x^{i} \partial x^{k}}\right)+\Gamma_{i l r} \Gamma_{j k}^{r}-\Gamma_{i k r} \Gamma_{j l}^{r} \tag{8.5}
\end{equation*}


EXAMPLE 8.1 Calculate the components $R_{i j k l}$ of the Riemann tensor for the metric of Problem 7.22,

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(x^{2}\right)^{-2}\left(d x^{2}\right)^{2}
$$

The nonvanishing Christoffel symbols are $\Gamma_{22}^{2}=-\left(x^{2}\right)^{-1}$ and $\Gamma_{222}=g_{22} \Gamma_{22}^{2}=\left(x^{2}\right)^{-3}$. The partial-derivative terms in (8.4) vanish unless all indices are 2; but then the two terms cancel. Likewise the Christoffel-symbol terms either vanish or cancel. We conclude that all sixteen components $R_{i j k l}=0$.

\section*{Symmetry Properties}
Interchange of $k$ and $l$ in (8.2) shows that $R_{j k l}^{i}=-R_{j l k}^{i}$, whence $R_{i j k l}=-R_{i j l k}$. This and two other symmetry properties are easily established at this point; Bianchi's (first) identity will be demonstrated in Chapter 9.


\begin{align*}
\text { first skew symmetry } & R_{i j k l}=-R_{j i k l} \\
\text { second skew symmetry } & R_{i j k l}=-R_{i j l k}  \tag{8.6}\\
\text { block symmetry } & R_{i j k l}=R_{k l i j} \\
\text { Bianchi's identity } & R_{i j k l}+R_{i k l j}+R_{i l j k}=0
\end{align*}


\section*{Number of Independent Components}
We shall count the separate types of potentially nonzero components, using the above symmetry properties. The first two properties imply that $R_{a a c d}$ and $R_{a b c c}$ (not summed on $a$ or $c$ ) are zero. In the following list, we agree not to sum on repeated indices.

(A) Type $R_{a b a b}, a<b: \quad n_{A}={ }_{n} C_{2}=n(n-1) / 2$

(B) Type $R_{a b a c}, b<c: \quad n_{B}=3 \cdot{ }_{n} C_{3}=n(n-1)(n-2) / 2$

(C) Type $R_{a b c d}$ or $R_{a c b d}, a<b<c<d$ (for type $R_{a d b c}$, use Bianchi's identity): $n_{C}=2 \cdot{ }_{n} C_{4}=$ $n(n-1)(n-2)(n-3) / 12$

In (A) the count is of combinations of $n$ numbers two at a time (for $a$ and $b$ ). In (B) one partitions the index strings into the three groups (with ${ }_{n} C_{3}$ in each group) for which

$$
a<b<c \quad b<a<c \quad b<c<a
$$

Either subtype of (C) has as many members as there are combinations of $n$ numbers four at a time (for $a, b, c$, and $d$ ).

Summing $n_{A}, n_{B}$, and $n_{C}$, we prove

Theorem 8.2: There are a total of $n^{2}\left(n^{2}-1\right) / 12$ components of the Riemann tensor $\left(R_{i j k l}\right)$ that are not identically zero and that are independent from the rest.

Corollary 8.3: In two-dimensional Riemannian space, the only components of the Riemann tensor not identically zero are $R_{1212}=R_{2121}=-R_{1221}=-R_{2112}$.

EXAMPLE 8.2 For the metric of spherical coordinates,

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1} d x^{2}\right)^{2}+\left(x^{1} \sin x^{2} d x^{3}\right)^{2}
$$

list and calculate the nonzero components $R_{i j k l}$, if any.

By Theorem 8.2 with $n=3$, there are six potentially nonzero components:

(A) $R_{1212}, R_{1313}, R_{2323}$

(B) $R_{1213}, R_{1232}\left(=R_{2123}\right), R_{1323}\left(=R_{3132}\right)$

Because $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (diagonal metric tensor; no summation), we may instead compute the mixed components. From Problem 6.5,

$$
\begin{array}{ll}
i=1 & \Gamma_{22}^{1}=-x^{1}, \quad \Gamma_{33}^{1}=-x^{1} \sin ^{2} x^{2} \\
i=\mathbf{2} & \Gamma_{13}^{2}=\Gamma_{21}^{2}=\frac{1}{x^{1}}, \quad \Gamma_{33}^{2}=-\sin x^{2} \cos x^{2} \\
i=\mathbf{3} & \Gamma_{13}^{3}=\Gamma_{31}^{3}=\frac{1}{x^{1}}, \quad \Gamma_{23}^{3}=\Gamma_{32}^{3}=\cot x^{2}
\end{array}
$$

and (8.2) gives:

$$
\begin{aligned}
R_{212}^{1} & =\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{2}}+\Gamma_{22}^{r} \Gamma_{r 1}^{1}-\Gamma_{21}^{r} \Gamma_{r 2}^{1}=-1-\Gamma_{22}^{1} \Gamma_{11}^{1}-\Gamma_{21}^{2} \Gamma_{22}^{1}=0 \\
R_{313}^{1} & =\frac{\partial \Gamma_{33}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{31}^{1}}{\partial x^{1}}+\Gamma_{33}^{r} \Gamma_{r 1}^{1}-\Gamma_{31}^{r} \Gamma_{r 3}^{1}=-\sin ^{2} x^{2}+\Gamma_{33}^{1} \Gamma_{11}^{1}+\Gamma_{31}^{3} \Gamma_{33}^{1}=0 \\
R_{323}^{2} & =\frac{\partial \Gamma_{33}^{2}}{\partial x^{2}}-\frac{\partial \Gamma_{32}^{2}}{\partial x^{3}}+\Gamma_{33}^{r} \Gamma_{r 2}^{2}-\Gamma_{32}^{r} \Gamma_{r 3}^{2}=-\cos 2 x^{2}+\Gamma_{33}^{1} \Gamma_{12}^{2}-\Gamma_{32}^{3} \Gamma_{33}^{2}=-\cos 2 x^{2}-\sin ^{2} x^{2}+\cos ^{2} x^{2}=0 \\
R_{213}^{1} & =\frac{\partial \Gamma_{23}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{3}}+\Gamma_{23}^{r} \Gamma_{r 1}^{1}-\Gamma_{21}^{r} \Gamma_{r 3}^{1}=\Gamma_{23}^{3} \Gamma_{31}^{1}-\Gamma_{21}^{2} \Gamma_{23}^{1}=0 \\
R_{232}^{1} & =\frac{\partial \Gamma_{22}^{1}}{\partial x^{3}}-\frac{\partial \Gamma_{23}^{1}}{\partial x^{2}}+\Gamma_{22}^{r} \Gamma_{r 3}^{1}-\Gamma_{23}^{r} \Gamma_{r 2}^{1}=\Gamma_{22}^{1} \Gamma_{13}^{1}-\Gamma_{23}^{3} \Gamma_{32}^{1}=0 \\
R_{323}^{1} & =\frac{\partial \Gamma_{33}^{1}}{\partial x^{2}}-\frac{\partial \Gamma_{32}^{1}}{\partial x^{3}}+\Gamma_{33}^{r} \Gamma_{r 2}^{1}-\Gamma_{22}^{r} \Gamma_{r 3}^{1}=-2 x^{1} \sin x^{2} \cos x^{2}+\Gamma_{33}^{2} \Gamma_{22}^{1}-\Gamma_{32}^{3} \Gamma_{33}^{1} \\
& =-2 x^{1} \sin x^{2} \cos x^{2}+x^{1} \sin x^{2} \cos x^{2}+\left(\cot x^{2}\right)\left(x^{1} \sin ^{2} x^{2}\right)=0
\end{aligned}
$$

Therefore, $R_{i j k l}=0$ for all $i, j, k, l$.

\subsection*{8.3 RIEMANNIAN CURVATURE}
The Riemannian (or sectional) curvature relative to a given metric $\left(g_{i j}\right)$ is defined for each pair of (contravariant) vectors $\mathbf{U}=\left(U^{i}\right), \mathbf{V}=\left(V^{i}\right)$ as


\begin{equation*}
\mathrm{K}=\mathrm{K}(\mathbf{x} ; \mathbf{U}, \mathbf{V})=\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}} \quad\left(G_{p q r s} \equiv g_{p r} g_{q s}-g_{p s} g_{q r}\right) \tag{8.7}
\end{equation*}


This sort of curvature depends not only on position, but also on a pair of directions selected at each point (the vectors $\mathbf{U}$ and $\mathbf{V}$ ). By contrast, the curvature $\kappa$ of a curve depends only on the points along the curve. Although it would seem desirable for $\mathrm{K}$ to depend only on the points of space, to demand this would impose severe and unrealistic restrictions, as will become apparent in Chapter 9.

EXAMPLE 8.3 The numerator of (8.7) is an invariant, because $\left(R_{i j k l}\right)$ is a tensor. As for the denominator, the identity


\begin{equation*}
G_{p q r s} V_{(1)}^{p} V_{(2)}^{q} V_{(3)}^{r} V_{(4)}^{s}=\left(\mathbf{V}_{(1)} \mathbf{V}_{(3)}\right)\left(\mathbf{V}_{(2)} \mathbf{V}_{(4)}\right)-\left(\mathbf{V}_{(1)} \mathbf{V}_{(4)}\right)\left(\mathbf{V}_{(2)} \mathbf{V}_{(3)}\right) \tag{8.8}
\end{equation*}


implies that the denominator is an invariant and proves (Lemma 4.1) that $\left(G_{i j k l}\right)$ is also a tensor. It follows that $\mathrm{K}(\mathbf{x} ; \mathbf{U}, \mathbf{V})$ is an invariant, and thus it serves to generalize the Gaussian curvature of a surface to higher dimensions.

Helpful in the calculation of $\mathrm{K}$ is the fact that the $G_{i j k l}$ possess exactly the same symmetries as the $R_{i j k l}$ (see Problem 8.19). Moreover, if $g=\left(g_{i j}\right)$ is diagonal, all the nonzero $G_{i j k l}$ will be derivable from the type-A terms

$$
G_{a b a b}=g_{a a} g_{b b} \quad(a<b ; \text { no summation })
$$

EXAMPLE 8.4 Evaluate the Riemannian curvature at any point $\left(x^{i}\right)$ of Riemannian 3 -space in the directions (a) $\mathbf{U}=(1,0,0)$ and $\mathbf{V}=(0,1,1)$, and $(b) \mathbf{U}=(0,1,0)$ and $\mathbf{V}=(1,1,0)$, if the metric is given by

$$
g_{11}=1 \quad g_{22}=2 x^{1} \quad g_{33}=2 x^{2} \quad g_{i j}=0 \quad \text { if } \quad i \neq j
$$

From Problem 6.4, the nonzero Christoffel symbols are:

$$
\Gamma_{22}^{1}=-1 \quad \Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{1}{2 x^{1}} \quad \Gamma_{33}^{2}=-\frac{1}{2 x^{1}} \quad \Gamma_{23}^{3}=\Gamma_{32}^{3}=\frac{1}{2 x^{2}}
$$

Since $n=3$, only six (by Theorem 8.2) components of the Riemann tensor need be considered: $R_{1212}, R_{1313}$, $R_{2323}, R_{1213}, R_{2123}$ and $R_{3132}$. The metric being diagonal, we compute:

$$
\begin{gathered}
R_{212}^{1}=\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{2}}+\Gamma_{22}^{r} \Gamma_{r 1}^{1}-\Gamma_{21}^{r} \Gamma_{r 2}^{1}=0-0+0-\Gamma_{21}^{2} \Gamma_{22}^{1}=\frac{1}{2 x^{1}} \\
R_{313}^{1}=0 \quad R_{323}^{2}=\frac{1}{4 x^{1} x^{2}} \quad R_{213}^{1}=0 \quad R_{123}^{2}=0 \quad R_{132}^{3}=\frac{1}{4 x^{1} x^{2}}
\end{gathered}
$$

which yield the three terms

(A) $R_{1212}=g_{11} R_{212}^{1}=1 / 2 x^{1}, R_{2323}=g_{22} R_{323}^{2}=1 / 2 x^{2}$

(B) $R_{3132}=g_{33} R_{132}^{3}=1 / 2 x^{1}$

Theorem 8.2 also applies to the $G_{i j k l}$; but we may take the shortcut indicated in Example 8.3:

(A) $G_{1212}=g_{11} g_{22}=2 x^{1}, G_{1313}=g_{11} g_{33}=2 x^{2}, \quad G_{2323}=g_{22} g_{33}=4 x^{1} x^{2}$

Let us now give an expanded form of (8.7) in the case that type-C terms are absent. It is convenient to define the $n^{4}$ functions

\[
W_{i j k l} \equiv\left|\begin{array}{cc}
U^{i} & U^{j}  \tag{8.9}\\
V^{i} & V^{j}
\end{array}\right|\left|\begin{array}{ll}
U^{k} & U^{l} \\
V^{k} & V^{l}
\end{array}\right|
\]

of two vectors $\mathbf{U}=\left(U^{i}\right)$ and $\mathbf{V}=\left(V^{i}\right)$. Observe that the $W_{i j k l}$ possess all the symmetries of the $R_{i j k l}$ (or the $G_{i j k l}$ ). Looking at the numerator of (8.7), we see that a given type-A coefficient from the basic set generates, via its skew symmetries, the 4 terms

$$
R_{a b a b}\left(U^{a} V^{b} U^{a} V^{b}-U^{b} V^{a} U^{a} V^{b}-U^{a} V^{b} U^{b} V^{a}+U^{b} V^{a} U^{b} V^{a}\right)=R_{a b a b} W_{a b a b}
$$

and these precisely exhaust the $2 \times 2=4$ terms in which the coefficient $R_{i j k i}$ involves the same distinct integers, $a$ and $b$, in the first pair of indices as in the second pair. A given type-B coefficient from the basic set will generate, via its skew symmetries and block symmetry, the 8 terms

$$
R_{a b a c}\left(W_{a b a c}+W_{a c a b}\right)=2 R_{a b a c} W_{a b a c}
$$

and these exactly correspond to the $2^{2} \times 2=8$ ways of writing $R_{i j k l}$ such that the first and second index-pairs contain the common integer $a$ but are otherwise composed of distinct integers $b$ and $c$. Analyzing the denominator of (8.7) in the same fashion, we obtain as the desired formula:


\begin{equation*}
\mathrm{K}=\frac{\sum_{\text {type A }} R_{a b a b} W_{a b a b}+2 \sum_{\text {type B }} R_{a b a c} W_{a b a c}}{\sum_{\text {type A }} G_{a b a b} W_{a b a b}+2 \sum_{\text {type B }} G_{a b a c} W_{a b a c}} \tag{8.10}
\end{equation*}


It is understood that the summation convention does not operate in (8.10); the indicated summations are over all the nonzero, independent $R_{i j k l}\left(G_{i j k l}\right)$, according to type. Now to the problem at hand.

(a) For the data, (8.10) becomes

For

$$
\mathrm{K}=\frac{R_{1212} W_{1212}+R_{2323} W_{2323}+2 R_{3132} W_{3132}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}
$$

we have

$$
\left[\begin{array}{l}
\mathbf{U} \\
\mathbf{V}
\end{array}\right]=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 1
\end{array}\right]
$$

$$
\begin{array}{ll}
W_{1212}=\left|\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right|^{2}=1 & W_{2323}=\left|\begin{array}{ll}
0 & 0 \\
1 & 1
\end{array}\right|^{2}=0 \\
W_{3132}=\left|\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right|\left|\begin{array}{ll}
0 & 0 \\
1 & 1
\end{array}\right|=0 & W_{1313}=\left|\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right|^{2}=1
\end{array}
$$

and so

$$
\mathrm{K}=\frac{\left(1 / 2 x^{1}\right)(1)+\left(1 / 2 x^{2}\right)(0)+2\left(1 / 2 x^{1}\right)(0)}{\left(2 x^{1}\right)(1)+\left(2 x^{2}\right)(1)+\left(4 x^{1} x^{2}\right)(0)}=\frac{1}{4 x^{1}\left(x^{1}+x^{2}\right)}
$$

(b) For

$$
\left[\begin{array}{l}
\mathbf{U} \\
\mathbf{V}
\end{array}\right]=\left[\begin{array}{lll}
0 & 1 & 0 \\
1 & 1 & 0
\end{array}\right]
$$

we have

whence

$$
\begin{array}{ll}
W_{1212}=\left|\begin{array}{ll}
0 & 1 \\
1 & 1
\end{array}\right|^{2}=1 & W_{2323}=\left|\begin{array}{ll}
1 & 0 \\
1 & 0
\end{array}\right|^{2}=0 \\
W_{3132}=\left|\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right|\left|\begin{array}{ll}
0 & 1 \\
0 & 1
\end{array}\right|=0 & W_{1313}=\left|\begin{array}{ll}
0 & 0 \\
1 & 0
\end{array}\right|^{2}=0
\end{array}
$$

$$
\mathrm{K}=\frac{\left(1 / 2 x^{1}\right)(1)+0+0}{\left(2 x^{1}\right)(1)+0+0}=\frac{1}{4\left(x^{1}\right)^{2}}
$$

\section*{Observations on the Curvature Formula}
I. If $n=2,(8.7)$ reduces to


\begin{equation*}
\mathrm{K}=\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}} \equiv \frac{R_{1212}}{g} \tag{8.11}
\end{equation*}


(see Problem 8.7). Thus, at a given point in Riemannian 2-space, the curvature is determined by the $g_{i j}$ and their derivatives, and is independent of the directions $\mathbf{U}$ and $\mathbf{V}$.

II. The extension of (8.10) to include type-C terms is as follows:


\begin{equation*}
\mathrm{K}=\frac{\sum_{\text {type A }} R_{a b a b} W_{a b a b}+2 \sum_{\text {type B }} R_{a b a c} W_{a b a c}+2 \sum_{\text {type C }} R_{a b c d}\left(W_{a b c d}-W_{a d b c}\right)+2 \sum_{\text {type C }} R_{a c b d}\left(W_{a c b d}-W_{a d c b}\right)}{\sum_{\text {type A }} G_{a b a b} W_{a b a b}+2 \sum_{\text {type B }} G_{a b a c} W_{a b a c}+2 \sum_{\text {type C }} G_{a b c d}\left(W_{a b c d}-W_{a d b c}\right)+2 \sum_{\text {type C }} G_{a c b d}\left(W_{a c b d}-W_{a d c b}\right)} \tag{8.12}
\end{equation*}


(see Problem 8.9).

III. If linearly independent $\mathbf{U}$ and $\mathbf{V}$ are replaced by independent linear combinations of themselves, the curvature is unaffected; i.e.,


\begin{equation*}
\mathrm{K}(\mathbf{x} ; \lambda \mathbf{U}+\nu \mathbf{V}, \mu \mathbf{U}+\omega \mathbf{V})=\mathrm{K}(\mathbf{x} ; \mathbf{U}, \mathbf{V}) \tag{8.13}
\end{equation*}


Therefore, at a given point $\mathbf{x}$, the curvature will have a value, not for each pair of vectors $\mathbf{U}$ and $\mathbf{V}$, but for each 2-flat passing through $\mathbf{x}$.

\section*{Isotropic Points}
If the Riemannian curvature at $\mathbf{x}$ does not change with the orientation of a 2 -flat through $\mathbf{x}$, then $\mathbf{x}$ is called isotropic. From (8.11), we have

Theorem 8.4: All points of a two-dimensional Riemannian space are isotropic.

It is not immediately clear whether any metric ( $g_{i j}$ ) could lead to isotropic points in $\mathbf{R}^{n}, n \geqq 3$. But such is the case. Indeed, as is shown in Problem $8.12, \mathbf{R}^{3}$ under a hyperbolic metric is isotropic at any point.

\subsection*{8.4 THE RICCI TENSOR}
A brief look will be given a tensor that is of importance in Relativity. The Ricci tensor of the first kind is defined as a contraction of the Riemann tensor of the second kind:


\begin{equation*}
R_{i j} \equiv R_{i j k}^{k}=\frac{\partial \Gamma_{i k}^{k}}{\partial x^{j}}-\frac{\partial \Gamma_{i j}^{k}}{\partial x^{k}}+\Gamma_{i k}^{r} \Gamma_{r j}^{k}-\Gamma_{i j}^{r} \Gamma_{r k}^{k} \tag{8.14}
\end{equation*}


Raising an index yields the Ricci tensor of the second kind:


\begin{equation*}
R_{j}^{i} \equiv g^{i k} R_{k j} \tag{8.15}
\end{equation*}


By use of the following simple consequence of Laplace's expansion (2.5):

Lemma 8.5: Let $A=\left[a_{i j}(\mathbf{x})\right]_{n n}$ be a nonsingular matrix of multivariate functions, with inverse $B=\left[b_{i j}(\mathbf{x})\right]_{n n}$. Then

$$
\frac{\partial}{\partial x^{i}}(\ln |\operatorname{det} A|) \equiv \frac{1}{\operatorname{det} A} \frac{\partial}{\partial x^{i}}(\operatorname{det} A)=b_{s r} \frac{\partial a_{r s}}{\partial x^{i}}
$$

the definition (8.14) may be put into a form (Problem 8.14) that makes evident the symmetry of the $R_{i j}$.


\begin{equation*}
R_{i j}=\frac{\partial^{2}}{\partial x^{i} \partial x^{j}}(\ln \sqrt{|g|})-\frac{1}{\sqrt{|g|}} \frac{\partial}{\partial x^{r}}\left(\sqrt{|g|} \Gamma_{i j}^{r}\right)+\Gamma_{i s}^{r} \Gamma_{r j}^{s} \tag{8.16}
\end{equation*}


Here, as always, $g \equiv \operatorname{det} G$.

Theorem 8.6: The Ricci tensor is symmetric.

After raising a subscript to define the Ricci tensor of the second kind, $R_{j}^{i}=g^{i s} R_{s j}$, and then contracting on the remaining pair of indices, the important invariant $R \equiv R_{i}^{i}$ results, called the Ricci (or scalar) curvature. By (8.16),


\begin{equation*}
R=g^{i j}\left[\frac{\partial^{2}}{\partial x^{i} \partial x^{j}}(\ln \sqrt{|g|})-\frac{1}{\sqrt{|g|}} \frac{\partial}{\partial x^{r}}\left(\sqrt{|g|} \Gamma_{i j}^{r}\right)+\Gamma_{i s}^{r} \Gamma_{r j}^{s}\right] \tag{8.17}
\end{equation*}


\section*{Solved Problems}
\section*{THE RIEMANN TENSOR}
\subsection*{8.1 Prove (8.1).}
By definition of the covariant derivative,


\begin{equation*}
V_{i, j k}=\left(V_{i, j}\right)_{, k}=\frac{\partial}{\partial x^{k}}\left(V_{i, j}\right)-\Gamma_{i k}^{r}\left(V_{r, j}\right)-\Gamma_{j k}^{r}\left(V_{i, r}\right) \tag{1}
\end{equation*}


Substitute

$$
V_{i, j}=\frac{\partial V_{i}}{\partial x^{j}}-\Gamma_{i j}^{s} V_{s}
$$

in (1), carry out the differentiation, and remove parentheses:


\begin{equation*}
V_{i, j k}=\frac{\partial^{2} V_{i}}{\partial x^{k} \partial x^{j}}-\frac{\partial \Gamma_{i j}^{s}}{\partial x^{k}} V_{s}-\Gamma_{i j}^{s} \frac{\partial V_{s}}{\partial x^{k}}-\Gamma_{i k}^{r} \frac{\partial V_{r}}{\partial x^{j}}+\Gamma_{i k}^{r} \Gamma_{r j}^{s} V_{s}-\Gamma_{j k}^{r} \frac{\partial V_{i}}{\partial x^{r}}+\Gamma_{j k}^{r} \Gamma_{i r}^{s} V_{s} \tag{2}
\end{equation*}


Interchanging $j$ and $k$ yields


\begin{equation*}
V_{i, k j}=\frac{\partial^{2} V_{i}}{\partial x^{j} \partial x^{k}}-\frac{\partial \Gamma_{i k}^{s}}{\partial x^{j}} V_{s}-\Gamma_{i k}^{s} \frac{\partial V_{s}}{\partial x^{j}}-\Gamma_{i j}^{r} \frac{\partial V_{r}}{\partial x^{k}}+\Gamma_{i j}^{r} \Gamma_{r k}^{s} V_{s}-\Gamma_{k j}^{r} \frac{\partial V_{i}}{\partial x^{r}}+\Gamma_{k j}^{r} \Gamma_{i r}^{s} V_{s} \tag{3}
\end{equation*}


Subtracting (3) from (2), one sees that the first, third, fourth, sixth, and seventh terms on the right of (2) cancel with the first, fourth, third, sixth, and seventh terms on the right of (3), leaving

$$
\begin{aligned}
V_{i, j k}-V_{i, k j} & =-\frac{\partial \Gamma_{i j}^{s}}{\partial x^{k}} V_{s}+\Gamma_{i k}^{r} \Gamma_{r j}^{s} V_{s}+\frac{\partial \Gamma_{i k}^{s}}{\partial x^{j}} V_{s}-\Gamma_{i j}^{r} \Gamma_{r k}^{s} V_{s} \\
& =\left(\frac{\partial \Gamma_{i k}^{s}}{\partial x^{j}}-\frac{\partial \Gamma_{i j}^{s}}{\partial x^{k}}+\Gamma_{i k}^{r} \Gamma_{r j}^{s}-\Gamma_{i j}^{r} \Gamma_{r k}^{s}\right) V_{s}=R_{i j k}^{s} V_{s}
\end{aligned}
$$

8.2 Show that at any point where the Christoffel symbols vanish,

$$
R_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=0
$$

In this case the expression for $R_{j k l}^{i}$ reduces to just $\partial \Gamma_{j l}^{i} / \partial x^{k}-\partial \Gamma_{j k}^{i} / \partial x^{l}$. Therefore,

$$
R_{j k l}^{i}+R_{k l j}^{i}+R_{l j k}^{i}=\frac{\partial \Gamma_{j l}^{i}}{\partial x^{k}}-\frac{\partial \Gamma_{j k}^{i}}{\partial x^{l}}+\frac{\partial \Gamma_{k j}^{i}}{\partial x^{l}}-\frac{\partial \Gamma_{k l}^{i}}{\partial x^{j}}+\frac{\partial \Gamma_{l k}^{i}}{\partial x^{j}}-\frac{\partial \Gamma_{l j}^{i}}{\partial x^{k}}
$$

As all the terms cancel, the desired relationship is proved.

8.3 Prove that for an arbitrary second-order covariant tensor $\left(T_{i j}\right)$

$$
T_{i j, k l}-T_{i j, l k}=R_{i k l}^{s} T_{s j}+R_{j k l}^{s} T_{i s}
$$

(The general formula,


\begin{equation*}
T_{i_{1} i_{2} \ldots i_{p}, k l}-T_{i_{1} i_{2} \ldots i_{p}, l k}=\sum_{q=1}^{p} R_{i_{q} k l}^{s} T_{i_{1} \ldots i_{q-1} s i_{q+1} \ldots i_{p}} \tag{8.18}
\end{equation*}


which is credited to Ricci, is similarly established.)

A direct approach would be quite tedious; instead, first establish that


\begin{equation*}
V_{, j k}^{i}-V_{, k j}^{i}=-R_{s j k}^{i} V^{s} \tag{1}
\end{equation*}


for any contravariant vector $\left(V^{i}\right)$ (see Problem 8.16). Now observe that $\left(V^{q} T_{i q}\right)$ is a covariant vector, to which (8.1) applies. Thus,


\begin{equation*}
\left(V^{q} T_{i q}\right)_{, k l}-\left(V^{q} T_{i q}\right)_{, l k}=R_{i k l}^{s} V^{q} T_{s q} \tag{2}
\end{equation*}


By the inner-product rule for covariant differentiation,


\begin{align*}
& \left(V^{q} T_{i q}\right)_{, k}=V_{, k}^{q} T_{i q}+V^{q} T_{i q, k}  \tag{3}\\
& \left(V^{q} T_{i q}\right)_{, k l}=V_{, k l}^{q} T_{i q}+V_{, k}^{q} T_{i q, l}+V_{, l}^{q} T_{i q, k}+V^{q} T_{i q, k l}
\end{align*}


Interchange $k$ and $l$ :


\begin{equation*}
\left(V^{q} T_{i q}\right)_{, l k}=V_{, l k}^{q} T_{i q}+V_{, l}^{q} T_{i q, k}+V_{, k}^{q} T_{i q, l}+V^{q} T_{i q, l k} \tag{4}
\end{equation*}


Subtraction of (4) from (3) will cancel the middle two terms on the right-hand sides, leaving


\begin{equation*}
R_{i k l}^{s} V^{q} T_{s q}=\left(V_{, k l}^{q}-V_{, l k}^{q}\right) T_{i q}+\left(T_{i q, k l}-T_{i q, l k}\right) V^{q} \tag{5}
\end{equation*}


Now use (1) in the right member of (5):

$$
R_{i k l}^{s} V^{q} T_{s q}=-R_{q k l}^{s} V^{q} T_{i s}+\left(T_{i q, k l}-T_{i q, l k}\right) V^{q}
$$

which may be rearranged into

$$
\left[\left(T_{i q, k l}-T_{i q, l k}\right)-\left(R_{i k l}^{s} T_{s q}+R_{q k l}^{s} T_{i s}\right)\right] V^{q}=0
$$

But $\left(V^{i}\right)$ is arbitrary, so the bracketed expression must vanish. QED

\section*{PROPERTIES OF THE RIEMANN TENSOR}
8.4 Establish (8.4).

By definition,

$$
\begin{aligned}
R_{i j k l} & =g_{i s} R_{j k l}^{s}=g_{i s} \frac{\partial \Gamma_{j l}^{s}}{\partial x^{k}}-g_{i s} \frac{\partial \Gamma_{j k}^{s}}{\partial x^{l}}+g_{i s} \Gamma_{j l}^{r} \Gamma_{r k}^{s}-g_{i s} \Gamma_{j k}^{r} \Gamma_{r l}^{s} \\
& =\frac{\partial\left(g_{i s} \Gamma_{j l}^{s}\right)}{\partial x^{k}}-\frac{\partial g_{i s}}{\partial x^{k}} \Gamma_{j l}^{s}-\frac{\partial\left(g_{i s} \Gamma_{j k}^{s}\right)}{\partial x^{l}}+\frac{\partial g_{i s}}{\partial x^{l}} \Gamma_{j k}^{s}+\Gamma_{j l}^{r} \Gamma_{r k i}-\Gamma_{j k}^{r} \Gamma_{r l i}
\end{aligned}
$$

$$
=\frac{\partial \Gamma_{j l i}}{\partial x^{k}}-\frac{\partial \Gamma_{j k i}}{\partial x^{l}}+\Gamma_{j k}^{r}\left(\frac{\partial g_{i r}}{\partial x^{l}}-\Gamma_{r l i}\right)-\Gamma_{j l}^{r}\left(\frac{\partial g_{i r}}{\partial x^{k}}-\Gamma_{r k i}\right)
$$

Recall from (6.2) that for arbitrary index $l$,

$$
\frac{\partial g_{i r}}{\partial x^{l}}-\Gamma_{l r i}=\Gamma_{i l r}
$$

By substitution,

$$
R_{i j k l}=\frac{\partial \Gamma_{j l i}}{\partial x^{k}}-\frac{\partial \Gamma_{j k i}}{\partial x^{l}}+\Gamma_{i l r} \Gamma_{j k}^{r}-\Gamma_{i k r} \Gamma_{j l}^{r}
$$

8.5 Establish the first skew-symmetry property, $R_{i j k l}=-R_{j i k l}$.

To save writing, let

$$
G_{k l}^{i j} \equiv \frac{1}{2}\left(\frac{\partial^{2} g_{i j}}{\partial x^{k} \partial x^{l}}+\frac{\partial^{2} g_{k l}}{\partial x^{i} \partial x^{j}}\right) \quad \text { and } \quad H_{k l}^{i j} \equiv \Gamma_{i j r} \Gamma_{k l}^{r}
$$

Note the obvious symmetry properties

$$
G_{k l}^{i j}=G_{k l}^{j i}=G_{l k}^{i j} \quad \text { and } \quad H_{k l}^{i j}=H_{k l}^{j i}=H_{l k}^{i j}
$$

Also, it is clear that $G_{k l}^{i j}=G_{i j}^{k l}$; furthermore,

$$
H_{k l}^{i j}=\left(g_{r s} \Gamma_{i j}^{s}\right) \Gamma_{k l}^{r}=\Gamma_{i j}^{s}\left(g_{s r} \Gamma_{k l}^{r}\right)=\Gamma_{i j}^{s} \Gamma_{k l s}=H_{i j}^{k l}
$$

Now, by (8.5),

and

$$
\begin{aligned}
& R_{i j k l}=G_{j k}^{i l}-G_{j l}^{i k}+H_{j k}^{i l}-H_{j l}^{i k} \\
& R_{j i k l}=G_{i k}^{j l}-G_{i l}^{j k}+H_{i k}^{j l}-H_{i l}^{j k}=G_{j l}^{i k}-G_{j k}^{i l}+H_{j l}^{i k}-H_{j k}^{i l}=-R_{i j k l}
\end{aligned}
$$

8.6 List the independent, potentially nonzero components of $R_{i j k l}$ for $n=5$ and verify the formula of Theorem 8.2 in this case.

Type A: $R_{1212}, R_{1313}, R_{1414}, R_{1515}$

$R_{2323}, R_{2424}, R_{2525}$

$R_{3434}, R_{3535}$

$R_{4545}$

Type B: $R_{1213}, R_{1214}, R_{1215}, R_{1314}, R_{1315}, R_{1415}$

$R_{2123}, R_{2124}, R_{2125}, R_{2324}, R_{2325}, R_{2425}$

$R_{3132}, R_{3134}, R_{3135}, R_{3234}, R_{3235}, R_{3435}$

$R_{4142}, R_{4143}, R_{4145}, R_{4243}, R_{4245}, R_{4345}$

$R_{5152}, R_{5153}, R_{5154}, R_{5253}, R_{5254}, R_{5354}$

Type C: $R_{1234}, R_{1235}, R_{1245}, R_{1345}, R_{2345}$

$R_{1324}, R_{1325}, R_{1425}, R_{1435}, R_{2435}$

There are 10 components of types $\mathrm{A}$ and $\mathrm{C}$ each, and 30 of type B; or 50 altogether. From the formula,

$$
\frac{n^{2}\left(n^{2}-1\right)}{12}=\frac{5^{2}\left(5^{2}-1\right)}{12}=\frac{(25)(24)}{12}=50
$$

\section*{RIEMANNIAN CURVATURE}
8.7 Prove (8.11).

By Corollary 8.3 and the corresponding result for the $G_{i j k l}$,

$$
\mathrm{K}=\frac{R_{i j k l} U^{i} V^{j} U^{k} V^{l}}{G_{p q r s} U^{p} V^{q} U^{r} V^{s}}=\frac{R_{1212}\left[\left(U^{1}\right)^{2}\left(V^{2}\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\left(U^{2}\right)^{2}\left(V^{1}\right)^{2}\right]}{G_{1212}\left[\left(U^{1}\right)^{2}\left(V^{2}\right)^{2}-2 U^{1} V^{2} U^{2} V^{1}+\left(U^{2}\right)^{2}\left(V^{1}\right)^{2}\right]}=\frac{R_{1212}}{G_{1212}}=\frac{R_{1212}}{g_{11} g_{22}-g_{12}^{2}}
$$

8.8 Calculate $\mathrm{K}$ for the Riemannian metric $\varepsilon d s^{2}=\left(x^{1}\right)^{-2}\left(d x^{1}\right)^{2}-\left(x^{1}\right)^{-2}\left(d x^{2}\right)^{2}$, using the result of Problem 8.7. 6.4 ,

We have only to calculate $R_{1212}=g_{11} R_{212}^{1}$. The nonvanishing Christoffel symbols are, by Problem

$$
\Gamma_{11}^{1}=-\frac{1}{x^{1}} \quad \Gamma_{22}^{1}=-\frac{1}{x^{1}} \quad \Gamma_{12}^{2}=\Gamma_{21}^{2}=-\frac{1}{x^{1}}
$$

Consequently,

and

$$
\begin{gathered}
R_{212}^{1}=\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{2}}+\Gamma_{22}^{r} \Gamma_{r 1}^{1}-\Gamma_{21}^{r} \Gamma_{r 2}^{1}=\frac{1}{\left(x^{1}\right)^{2}}-0+\Gamma_{22}^{1} \Gamma_{11}^{1}-\Gamma_{21}^{2} \Gamma_{22}^{1} \\
=\frac{1}{\left(x^{1}\right)^{2}}-\frac{1}{x^{1}}\left(-\frac{1}{x^{1}}\right)-\left(-\frac{1}{x^{1}}\right)\left(-\frac{1}{x^{1}}\right)=\frac{1}{\left(x^{1}\right)^{2}} \\
\mathrm{~K}=\frac{g_{11} R_{212}^{1}}{g_{11} g_{22}}=\frac{R_{212}^{1}}{g_{22}}=\frac{\left(x^{1}\right)^{-2}}{-\left(x^{1}\right)^{-2}}=-1 .
\end{gathered}
$$

8.9 Derive the form (8.12) of the curvature equation.

We need only establish the summations over the type-C terms in the numerator; the rest of the work was done in Example 8.4.

First of all, let us verify that all $R_{i j k l}$ with $i j k l$ a permutation of $a b c d$, where $a<b<c<d$ are distinct integers, are generated by the skew and block symmetries of the three components $R_{a b c d}, R_{a c b d}$, and $R_{a d b c}$. Examination of Table 8-1, which uses an obvious notation for the symmetry operators, shows that all $4 !=24$ permutations are accounted for. Consequently, the type-C part of the numerator of (8.12) is [cf. the equation preceding $(8.10)$ ]


\begin{equation*}
2 \sum R_{a b c d} W_{a b c d}+2 \sum R_{a c b d} W_{a c b d}+2 \sum R_{a d b c} W_{a d b c} \tag{1}
\end{equation*}


Table 8-1

\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multirow{2}{*}{}\begin{tabular}{c}
Symmetry \\
Operator \\
\end{tabular} & \multicolumn{3}{|c|}{Subscript Chain} \\
\cline { 2 - 4 }
 & $\boldsymbol{a b c d}$ & $\boldsymbol{a} \boldsymbol{b} \boldsymbol{d}$ & $\boldsymbol{a d b} \boldsymbol{c}$ \\
\hline
$\mathrm{I}$ & $a b c d$ & $a c b d$ & $a d b c$ \\
$\mathrm{~S}_{1}$ & $b a c d$ & $c a b d$ & $d a b c$ \\
$\mathrm{~S}_{2}$ & $a b d c$ & $a c d b$ & $a d c b$ \\
$\mathrm{~S}_{1} \mathrm{~S}_{2}=\mathrm{S}_{2} \mathrm{~S}_{1}$ & $b a d c$ & $c a d b$ & $d a c b$ \\
\hline
$\mathrm{B}$ & $c d a b$ & $b \bar{d} a c$ & $b \bar{b} a \bar{d}$ \\
$\mathrm{BS}_{1}=\mathrm{S}_{2} \mathrm{~B}$ & $c d b a$ & $b d c a$ & $b c d a$ \\
$\mathrm{BS}_{2}=\mathrm{S}_{1} \mathrm{~B}$ & $d c a b$ & $d b a c$ & $c b a d$ \\
$\mathrm{BS}_{1} \mathrm{~S}_{2}=\mathrm{S}_{1} \mathrm{~S}_{2} \mathrm{~B}$ & $d c b a$ & $d b c a$ & $c b d a$ \\
\hline
\end{tabular}
\end{center}

The first summation is over all $a<b<c<d$ that yield a nonzero $R_{a b c d}$ in the basic set; similarly for the second summation. The third summation does not involve the basic set, but the symmetries of $R_{i j k l}$ (shared by $W_{i j k l}$ ) allow its absorption in the first two summations. Thus, by Bianchi's identity,


\begin{equation*}
2 R_{a d b c} W_{a d b c}=2\left(-R_{a b c d}-R_{a c d b}\right) W_{a d b c}=-2 R_{a b c d} W_{a d b c}-2 R_{a c b d} W_{a d c b} \tag{2}
\end{equation*}


and substitution of (2) in (1) produces the expression given in (8.12).

8.10 Prove (8.13).

We have

$$
\begin{aligned}
W_{i j k l}(\lambda \mathbf{U}+\nu \mathbf{V}, \mu \mathbf{U}+\omega \mathbf{V}) & =\left|\begin{array}{cc}
\lambda U^{i}+\nu V^{i} & \lambda U^{j}+\nu V^{j} \\
\mu U^{i}+\omega V^{i} & \mu U^{j}+\omega V^{j}
\end{array}\right|\left|\begin{array}{cc}
\lambda U^{k}+\nu V^{k} & \lambda U^{l}+\nu V^{l} \\
\mu U^{k}+\omega V^{k} & \mu U^{l}+\omega V^{l}
\end{array}\right| \\
& =\left|\begin{array}{cc}
\lambda & \nu \\
\mu & \omega
\end{array}\right|\left|\begin{array}{ll}
U^{i} & U^{j} \\
V^{i} & V^{j}
\end{array}\right|\left|\begin{array}{ll}
U^{k} & U^{l} \\
V^{k} & V^{l}
\end{array}\right|=(\lambda \omega-\nu \mu)^{2} W_{i j k l}(\mathbf{U}, \mathbf{V})
\end{aligned}
$$

so that the quantity $(\lambda \omega-\nu \mu)^{2}$ factors out of all terms in (8.12) for $\mathrm{K}(\mathbf{x} ; \lambda \mathbf{U}+\nu \mathbf{V}, \mu \mathbf{U}+\omega \mathbf{V})$, leaving $\mathrm{K}(\mathbf{x} ; \mathbf{U}, \mathbf{V})$.

8.11 Find the isotropic points in the Riemannian space $\mathbf{R}^{3}$ with metric

$$
g_{11}=1 \quad g_{22}=g_{33}=\left(x^{1}\right)^{2}+1 \quad g_{i j}=0 \quad(i \neq j)
$$

and calculate the curvature $\mathrm{K}$ at those points.

Follow Example 8.4. By Problem 6.4, the nonzero Christoffel symbols are

$$
\Gamma_{22}^{1}=-x^{1} \quad \Gamma_{33}^{1}=-x^{1} \quad \Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{x^{1}}{\left(x^{1}\right)^{2}+1} \quad \Gamma_{13}^{3}=\Gamma_{31}^{3}=\frac{x^{1}}{\left(x^{1}\right)^{2}+1}
$$

Then:

which give

$$
\begin{aligned}
& R_{212}^{1}=\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}+\Gamma_{22}^{1} \Gamma_{11}^{1}-\Gamma_{21}^{2} \Gamma_{22}^{1}=-1-\frac{x^{1}}{\left(x^{1}\right)^{2}+1}\left(-x^{1}\right)=-\frac{1}{\left(x^{1}\right)^{2}+1} \\
& R_{313}^{1}=\frac{\partial \Gamma_{33}^{1}}{\partial x^{1}}+\Gamma_{33}^{1} \Gamma_{11}^{1}-\Gamma_{31}^{3} \Gamma_{33}^{1}=-1-\frac{x^{1}}{\left(x^{1}\right)^{2}+1}\left(-x^{1}\right)=-\frac{1}{\left(x^{1}\right)^{2}+1} \\
& R_{323}^{2}=\Gamma_{33}^{1} \Gamma_{12}^{2}=-x^{1} \cdot \frac{x^{1}}{\left(x^{1}\right)^{2}+1}=-\frac{\left(x^{1}\right)^{2}}{\left(x^{1}\right)^{2}+1} \\
& R_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0
\end{aligned}
$$

(A) $\quad R_{1212}=g_{11} R_{212}^{1}=-\left[\left(x^{1}\right)^{2}+1\right]^{-1}, R_{1313}=g_{11} R_{313}^{1}=-\left[\left(x^{1}\right)^{2}+1\right]^{-1}, R_{2323}=g_{22} R_{323}^{2}=-\left(x^{1}\right)^{2}$

The corresponding terms for the denominator of $(8.10)$ are


\begin{align*}
G_{1212}=g_{11} g_{22} & =\left(x^{1}\right)^{2}+1, \quad G_{1313}=g_{11} g_{33}=\left(x^{1}\right)^{2}+1, \quad G_{2323}=g_{22} g_{33}=\left[\left(x^{1}\right)^{2}+1\right]^{2}  \tag{A}\\
\mathrm{~K} & =\frac{-\left[\left(x^{1}\right)^{2}+1\right]^{-1} W_{1212}-\left[\left(x^{1}\right)^{2}+1\right]^{-1} W_{1313}-\left(x^{1}\right)^{2} W_{2323}}{\left[\left(x^{1}\right)^{2}+1\right] W_{1212}+\left[\left(x^{1}\right)^{2}+1\right] W_{1313}+\left[\left(x^{1}\right)^{2}+1\right]^{2} W_{2323}} \\
& =-\left[\left(x^{1}\right)^{2}+1\right]^{-2} \frac{W_{1212}+W_{1313}+\left(x^{1}\right)^{2}\left[\left(x^{1}\right)^{2}+1\right] W_{2323}}{W_{1212}+W_{1313}+\left[\left(x^{1}\right)^{2}+1\right] W_{2323}}
\end{align*}


If $\mathrm{K}$ is to be independent of the $W_{i j k l}$ (which vary with ine direction of the 2 -flat), then $\left(x^{1}\right)^{2}=1$, or $x^{1}= \pm 1$. Therefore, the isotropic points compose two surfaces, on which the curvature has the value $\mathrm{K}=-[1+1]^{-2} \cdot 1=-1 / 4$.

8.12 Show that every point of $\mathbf{R}^{3}$ is isotropic for the metric

$$
d s^{2}=\left(x^{1}\right)^{-2}\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{-2}\left(d x^{2}\right)^{2}+\left(x^{1}\right)^{-2}\left(d x^{3}\right)^{2}
$$

Problem 6.4 gives as the nonvanishing Christoffel symbols:

$$
\begin{gathered}
\Gamma_{11}^{1}=-\frac{1}{x^{1}} \quad \Gamma_{22}^{1}=\frac{1}{x^{1}} \quad \Gamma_{33}^{1}=\frac{1}{x^{1}} \\
\Gamma_{12}^{2}=\Gamma_{21}^{2}=-\frac{1}{x^{1}} \quad \Gamma_{13}^{3}=\Gamma_{31}^{3}=-\frac{1}{x^{1}}
\end{gathered}
$$

As in earlier problems, we proceed to calculate a basic set of $R_{i j k l}$, via $R_{i j k l}=g_{i i} R_{j k l}^{i}$ (no sum).

$$
\begin{aligned}
R_{212}^{1} & =\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{2}}+\Gamma_{22}^{r} \Gamma_{r 1}^{1}-\Gamma_{21}^{r} \Gamma_{r 2}^{1}=-\frac{1}{\left(x^{1}\right)^{2}}-0+\Gamma_{22}^{1} \Gamma_{11}^{1}-\Gamma_{21}^{2} \Gamma_{22}^{1} \\
& =-\frac{1}{\left(x^{1}\right)^{2}}+\frac{1}{x^{1}}\left(-\frac{1}{x^{1}}\right)-\left(-\frac{1}{x^{1}}\right) \frac{1}{x^{1}}=-\frac{1}{\left(x^{1}\right)^{2}}
\end{aligned}
$$

Similarly, $R_{313}^{1}=-1 /\left(x^{1}\right)^{2}$. For the remainder, the partial-derivative terms all drop out, yielding

$$
\begin{gathered}
R_{323}^{2}=\Gamma_{33}^{r} \Gamma_{r 2}^{2}-\Gamma_{32}^{r} \Gamma_{r 3}^{2}=\Gamma_{33}^{1} \Gamma_{12}^{2}-0=-\frac{1}{\left(x^{1}\right)^{2}} \\
R_{213}^{1}=R_{123}^{2}=R_{132}^{3}=0
\end{gathered}
$$

Our basic set is thus

(A) $R_{1212}=R_{1313}=R_{2323}=-1 /\left(x^{1}\right)^{4}$

and, by Example 8.3,

(A) $G_{1212}=G_{1313}=G_{2323}=1 /\left(x^{1}\right)^{4}$

is a basic set of $G_{i j k l}$. Formula (8.10) or (8.12) now gives

$$
\mathrm{K}=\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{G_{1212} W_{1212}+G_{1313} W_{1313}+G_{2323} W_{2323}}=\frac{\left[-\left(x^{1}\right)^{-4}\right]\left(W_{1212}+W_{1313}+W_{2323}\right)}{\left[\left(x^{1}\right)^{-4}\right]\left(W_{1212}+W_{1313}+W_{2323}\right)}=-1
$$

It is seen that this Riemannian space is more than just isotropic; it is a space of constant curvature.

\section*{THE RICCI TENSOR}
8.13 For the metric of Example 8.4, calculate (a) $R_{i j},(b) R_{j}^{i}$, (c) $R$.

(a) From $R_{i j}=R_{i j k}^{k}=R_{i j 1}^{1}+R_{i j 2}^{2}+R_{i j 3}^{3}$ and the fact that $g_{i j}=0$ for $i \neq j$, it follows that


\begin{equation*}
R_{i j}=g^{11} R_{1 i j 1}+g^{22} R_{2 i j 2}+g^{33} R_{3 i j 3} \tag{1}
\end{equation*}


where $g^{11}=1, g^{22}=1 / 2 x^{1}, g^{33}=1 / 2 x^{2}$. Now, a basic set of the $R_{i j k l}$ was computed as

$$
R_{1221}\left(=-R_{1212}\right)=-\frac{1}{2 x^{1}} \quad R_{2332}\left(=-R_{2323}\right)=-\frac{1}{2 x^{2}} \quad R_{3123}\left(=-R_{3132}\right)=-\frac{1}{2 x^{1}}
$$

and the only other nonzero components of the form $R_{\text {aija }}$ generated by these are

$$
R_{2112}=-\frac{1}{2 x^{1}} \quad R_{3223}=-\frac{1}{2 x^{2}} \quad R_{3213}=-\frac{1}{2 x^{1}}
$$

Hence, the nonzero $R_{i j}$ may be read off from (1) as


\begin{gather*}
R_{11}=g^{22} R_{2112}=-\frac{1}{4\left(x^{1}\right)^{2}} \\
R_{22}=g^{11} R_{1221}+g^{33} R_{3223}=-\frac{1}{2 x^{1}}-\frac{1}{4\left(x^{2}\right)^{2}} \\
R_{33}=g^{22} R_{2332}=-\frac{1}{4 x^{1} x^{2}} \\
R_{12}=g^{33} R_{3123}=-\frac{1}{4 x^{1} x^{2}}=g^{33} R_{3213}=R_{21} \\
R_{j}^{i}=g^{i k} R_{k j}=g^{i i} R_{i j} \quad \text { (no summation on } i \text { ) }  \tag{b}\\
=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33} \\
=(1)\left[-\frac{1}{4\left(x^{1}\right)^{2}}\right]+\left(\frac{1}{2 x^{1}}\right)\left[-\frac{1}{2 x^{1}}-\frac{1}{4\left(x^{2}\right)^{2}}\right]+\left(\frac{1}{2 x^{2}}\right)\left(-\frac{1}{4 x^{1} x^{2}}\right)=-\frac{x^{1}+2\left(x^{2}\right)^{2}}{\left(2 x^{1} x^{2}\right)^{2}}
\end{gather*}


$$
R=R_{1}^{1}+R_{2}^{2}+R_{3}^{3}=g^{11} R_{11}+g^{22} R_{22}+g^{33} R_{33}
$$

8.14 Derive (8.16) from (8.14).

Formula (8.14) involves two summations of the form $\Gamma_{i s}^{s}$. By (6.4) and (6.1b),

$$
\begin{aligned}
\Gamma_{i s}^{s} & =g^{s r} \Gamma_{i s r}=\frac{1}{2} g^{s r}\left(-g_{i s r}+g_{s r i}+g_{r i s}\right)=-\frac{1}{2} g^{s r} g_{s i r}+\frac{1}{2} g^{s r} g_{s r i}+\frac{1}{2} g^{r s} g_{s i r} \\
& =\frac{1}{2} g^{s r} g_{r s i} \equiv \frac{1}{2} g^{s r} \frac{\partial g_{r s}}{\partial x^{i}}=\frac{\partial}{\partial x^{i}}(\ln \sqrt{|g|})
\end{aligned}
$$

where Lemma 8.5 was used in the last step. Now substitute in (8.14):

$$
\begin{aligned}
R_{i j} & =\frac{\partial^{2}(\ln \sqrt{|g|})}{\partial x^{i} \partial x^{j}}-\frac{\partial \Gamma_{i j}^{s}}{\partial x^{s}}+\Gamma_{i s}^{r} \Gamma_{r j}^{s}-\Gamma_{i j}^{r} \frac{\partial(\ln \sqrt{|g|})}{\partial x^{r}} \\
& =\frac{\partial^{2}(\ln \sqrt{|g|})}{\partial x^{i} \partial x^{j}}-\left(\frac{1}{\sqrt{|g|}} \sqrt{|g|} \frac{\partial \Gamma_{i j}^{s}}{\partial x^{s}}+\frac{1}{\sqrt{|g|}} \frac{\partial(\sqrt{|g|})}{\partial x^{s}} \Gamma_{i j}^{s}\right)+\Gamma_{i s}^{r} \Gamma_{r j}^{s} \\
& =\frac{\partial^{2}(\ln \sqrt{|g|})}{\partial x^{i} \partial x^{j}}-\frac{1}{\sqrt{|g|}} \frac{\partial}{\partial x^{s}}\left(\sqrt{|g|} \Gamma_{i j}^{s}\right)+\Gamma_{i s}^{r} \Gamma_{r j}^{s}
\end{aligned}
$$

\section*{Supplementary Problems}
8.15 The absolute partial derivatives of a tensor $\mathbf{T}=\left(T_{j}^{i} \ldots\right)$ defined on a 2-manifold $\mathscr{M}: x^{i}=x^{i}(u, v)$ are defined as

$$
\frac{\delta \mathbf{T}}{\delta u} \equiv\left(T_{j}^{i} \ldots, k, \frac{\partial x^{k}}{\partial u}\right) \quad \text { and } \quad \frac{\delta \mathbf{T}}{\delta v} \equiv\left(T_{j}^{i} \ldots, \frac{\partial x^{k}}{\partial v}\right)^{\dot{*}}
$$

Since $\left(\partial x^{i} / \partial u\right)$ and $\left(\partial x^{i} / \partial v\right)$ are vectors, the inner products produce a pair of tensors of the same type and order as $\mathbf{T}$; thus the operation of absolute partial differentiation may be repeated indefinitely. Prove that if $\left(V^{i}\right)$ is any contravariant vector defined on $\mathcal{M}$,

$$
\frac{\delta}{\delta u}\left(\frac{\delta V^{i}}{\delta v}\right)-\frac{\delta}{\delta v}\left(\frac{\delta V^{i}}{\delta u}\right)=R_{s k l}^{i} V^{s} \frac{\partial x^{k}}{\partial u} \frac{\partial x^{l}}{\partial v}
$$

[Hint: Expand the left side and use Problem 8.16.]

8.16 Prove that for any vector $\left(V^{i}\right), V_{, k l}^{i}-V_{, l k}^{i}=-R_{s k l}^{i} V^{s}$.

8.17 For an arbitrary second-order contravariant tensor $\left(T^{i j}\right)$, show that

$$
T_{, k l}^{i j}-T_{, l k}^{i j}=-R_{s k l}^{i} T^{s j}-R_{s k l}^{j} T^{i s}
$$

[Hint: Lower superscripts and use Problem 8.3.]

8.18 For an arbitrary mixed tensor $\left(T_{j}^{i}\right)$ show that

$$
T_{j, k l}^{i}-T_{j, l k}^{i}=-R_{s k l}^{i} T_{j}^{s}+R_{j k l}^{s} T_{s}^{i}
$$

8.19 Verify the symmetry properties (8.6) for the $G_{i j k l}[\operatorname{see}(8.7)]$ and for the $W_{i j k l}[\operatorname{see}(8.9)]$.

8.20 Derive (8.5) from (8.4). [Hint: It is helpful to adopt the notation $g_{i j k l}$ for $\partial^{2} g_{i j} / \partial x^{k} \partial x^{l}$.]

8.21 List the independent (nonzero) components of $R_{i j k i}$ when $n=4$ and verify Theorem 8.2 for this case.

8.22 Calculate the Riemannian curvature K for the metric $\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-2 x^{1}\left(d x^{2}\right)^{2}$.

8.23 Confirm that $\mathrm{K}=0$ for the Euclidean metric of polar coordinates,

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1} d x^{2}\right)^{2}
$$

$(a)$ by a calculation; $(b)$ by noting that $\mathrm{K}$ is an invariant.

8.24 Rework Example 8.4 for the pairs $(a) \mathbf{U}_{(1)}=(1,0,1), \mathbf{V}_{(1)}=(1,1,1)$ and $(b) \mathbf{U}_{(2)}=(0,1,0), \mathbf{V}_{(2)}=$ $(2,1,2)$. (c) Explain why the answers should be the same for $(a)$ and $(b)$.

8.25 Let the surface of the 3 -sphere of radius $a$ be metrized by setting $x^{1}=a$ in spherical coordinates and then allowing $x^{1}, x^{2}$ to replace $x^{2}, x^{3}$, respectively:

$$
d s^{2}=a^{2}\left(d x^{1}\right)^{2}+\left(a \sin x^{1}\right)^{2}\left(d x^{2}\right)^{2}
$$

Determine $\mathrm{K}$ for this non-Euclidean $\mathbf{R}^{2}$.

8.26 If the metric for Riemannian $\mathbf{R}^{3}$ is given by

$$
g_{11}=f\left(x^{2}\right) \quad g_{22}=g\left(x^{2}\right) \quad g_{33}=h\left(x^{2}\right)
$$

and $g_{i j}=0$ for $i \neq j$, write explicit formulas for $(a) \mathrm{K}\left(x^{2} ; \mathbf{U}, \mathbf{V}\right),(b) R$.

8.27 Specialize the results of Problem 8.26 to the case $f\left(x^{2}\right) \equiv g\left(x^{2}\right) \equiv h\left(x^{2}\right)$.

8.28 Find the isotropic points for the Riemannian metric

$$
d s^{2}=\left(\ln x^{2}\right)\left(d x^{1}\right)^{2}+\left(\ln x^{2}\right)\left(d x^{2}\right)^{2}+\left(\ln x^{2}\right)\left(d x^{3}\right)^{2} \quad\left(x^{2}>1\right)
$$

and find the curvature $\mathrm{K}$ at those points. [Hint: Use Problem 8.27.]

8.29 Show that $\mathbf{R}^{3}$ under the metric

$$
g_{11}=e^{x^{2}} \quad g_{22}=1 \quad g_{33}=e^{x^{2}} \quad g_{i j}=0 \quad(i \neq j)
$$

has constant Riemannian curvature with all points isotropic, and find that curvature.

8.30 Show that in a Riemannian 2-space [for which (8.11) holds]: (a) $R_{i j}=-g_{i j} \mathrm{~K},(b) R_{j}^{i}=-\delta_{j}^{i} \mathrm{~K}$, and (c) $R=-2 \mathrm{~K}$.

8.31 Calculate the Ricci tensor $R_{i j}$ for Problem 8.13 using (8.16), and compare your answers with those obtained earlier.

8.32 Use Problem 8.30 to calculate the Ricci tensors of both kinds and the curvature invariant for the spherical metric of Problem 8.25.

8.33 Calculate the Ricci tensors of both kinds and the curvature invariant for the (hyperbolic) metric of Problem 8.12. [Hint: Problem 8.27 can be used to good advantage here.]

8.34 Prove that for any tensor $\left(T^{i j}\right)$, symmetric or not, $T_{, i j}^{i j}=T_{, j i}^{i j}$. [Hint: Use Problem 8.17 and the symmetry of the Ricci tensor.]

8.35 Is identical vanishing equivalent for the Riemannian curvature and the Ricci curvature invariant? Can you find an example where one is zero everywhere but not the other?

8.36 Is constancy in space equivalent for the two curvatures $\mathrm{K}$ and $R$ ?

\section*{Chapter 9}
\section*{Spaces of Constant Curvature; Normal Coordinates}
\subsection*{9.1 ZERO CURVATURE AND THE EUCLIDEAN METRIC}
A fundamental question has run unanswered through preceding chapters: How can one tell whether a given metrization of $\mathbf{R}^{n}$ is Euclidean or not? To be sure that the meaning of "Euclidean" is clear, let us make the formal

Definition 1: A Riemannian metric $\mathbf{g}=\left(g_{i j}\right)$, specified in a coordinate system $\left(x^{i}\right)$, is the Euclidean metric if, under some permissible coordinate transformation (3.1), $\overline{\mathbf{g}}=\left(\delta_{i j}\right)$.

Now, a coordinate system $\left(\bar{x}^{i}\right.$ ) in which $\bar{g}_{i j}=\delta_{i j}$ is (by Definition 1 of Chapter 3) a rectangular system. Hence our question amounts to: Does a given Riemannian space admit rectangular coordinates or does it not?

Suppose that a rectangular system $\left(\bar{x}^{i}\right)$ does exist. Then $\overline{\mathrm{K}}=0$, since all Christoffel symbols vanish in $\left(\bar{x}^{i}\right)$. But Riemannian curvature is an invariant, so that $\mathrm{K}=0$ in the original coordinates $\left(x^{i}\right)$ as well. Moreover, by invariance,

$$
g_{i j} U^{i} U^{j}=\bar{U}^{i} \bar{U}^{i} \geqq 0
$$

Thus, the necessity part of the following theorem is immediate.

Theorem 9.1: A Riemannian metric $\left(g_{i j}\right)$ is the Euclidean metric if and only if the Riemannian curvature $\mathrm{K}$ is zero at all points and the metric is positive definite.

To prove the sufficiency portion, we set up a system of first-order partial differential equations for $n$ rectangular coordinates $\bar{x}^{i}$ as functions of the given coordinates $x^{j}(j=1,2, \ldots, n)$. The system that immediately comes to mind (Theorem 5.2) is $G=J^{T} J$, or


\begin{equation*}
\frac{\partial \bar{x}^{k}}{\partial x^{i}} \frac{\partial \bar{x}^{k}}{\partial x^{j}}=g_{i j}\left(x^{1}, x^{2}, \ldots, x^{n}\right) \tag{9.1}
\end{equation*}


But (9.1) is generally intractable because of its nonlinearity. Instead, we select the linear system that results when barred and unbarred coordinates are interchanged in (6.6) and then the $\bar{\Gamma}_{j k}^{i}$ are equated to zero:


\begin{equation*}
\frac{\partial^{2} \bar{x}^{k}}{\partial x^{i} \partial x^{j}}=\Gamma_{i j}^{r}(\mathbf{x}) \frac{\partial \bar{x}^{k}}{\partial x^{r}} \tag{9.2}
\end{equation*}


Setting $w \equiv \bar{x}^{k}$ and $u_{i} \equiv \partial \bar{x}^{k} / \partial x^{i}$ yields the desired first-order system


\begin{align*}
& \frac{\partial w}{\partial x^{i}}=u_{i} \\
& \frac{\partial u_{i}}{\partial x^{j}}=\Gamma_{i j}^{r} u_{r} \tag{9.3}
\end{align*}


EXAMPLE 9.1 It is proved in Problems 9.7 and 9.8 that when $\mathrm{K} \equiv 0,(9.3)$ is solvable for a coordinate system $\left(\bar{x}^{k}\right)$ for which all $\bar{g}_{i j}$ are constants (i.e., all $\bar{\Gamma}_{j k}^{i}=0$ ); from these coordinates, rectangular coordinates can be reached, provided $\left(g_{i j}\right)$ is positive definite. To make these results plausible, consider the two-dimensional metric

$$
g_{11}=1 \quad g_{12}=g_{21}=0 \quad g_{22}=\left(x^{2}\right)^{2}
$$

This metric is obviously positive definite and, because the only nonvanishing Christoffel symbol is $\Gamma_{22}^{2}=1 / x^{2}$, it has $R_{1212}=0=\mathrm{K}$. It is possible to solve (9.1) directly for the corresponding cartesian coordinates, and then to verify that that solution is contained in the general solution to (9.3).

Introduce the notation


\begin{equation*}
f_{1} \equiv \frac{\partial \bar{x}^{1}}{\partial x^{1}} \quad f_{2} \equiv \frac{\partial \bar{x}^{1}}{\partial x^{2}} \quad f_{3} \equiv \frac{\partial \bar{x}^{2}}{\partial x^{1}} \quad f_{4} \equiv \frac{\partial \bar{x}^{2}}{\partial x^{2}} \tag{1}
\end{equation*}


whereby (9.1) becomes the algebraic system


\begin{align*}
f_{1}^{2}+f_{3}^{2} & =1 \\
f_{1} f_{2}+f_{3} f_{4} & =0  \tag{2}\\
f_{2}^{2}+f_{4}^{2} & =\left(x^{2}\right)^{2}
\end{align*}


System (2) can be solved for three of the $f_{i}$ in terms of the fourth-say, $f_{1}$ :


\begin{equation*}
f_{1}=f_{1} \quad f_{2}=x^{2} \sqrt{1-f_{1}^{2}} \quad f_{3}=-\sqrt{1-f_{1}^{2}} \quad f_{4}=x^{2} f_{1} \tag{3}
\end{equation*}


Now (1) becomes two simple first-order systems in $\bar{x}^{1}$ alone and $\bar{x}^{2}$ alone:

$$
\text { I: }\left\{\begin{array} { l } 
{ \frac { \partial \overline { x } ^ { 1 } } { \partial x ^ { 1 } } = f _ { 1 } } \\
{ \frac { \partial \overline { x } ^ { 1 } } { \partial x ^ { 2 } } = x ^ { 2 } \sqrt { 1 - f _ { 1 } ^ { 2 } } }
\end{array} \quad \text { and } \quad \text { II: } \left\{\begin{array}{l}
\frac{\partial \bar{x}^{2}}{\partial x^{1}}=-\sqrt{1-f_{1}^{2}} \\
\frac{\partial \bar{x}^{2}}{\partial x^{2}}=x^{2} f_{1}
\end{array}\right.\right.
$$

The unknown function $f_{1}$ is determined by the requirements that the two equations I and the two equations II both be compatible:

$$
\frac{\partial f_{1}}{\partial x^{2}}=\frac{\partial}{\partial x^{1}}\left(x^{2} \sqrt{1-f_{1}^{2}}\right) \quad \text { and } \quad \frac{\partial}{\partial x^{2}}\left(-\sqrt{1-f_{1}^{2}}\right)=\frac{\partial}{\partial x^{1}}\left(x^{2} f_{1}\right)
$$

The only function satisfying these two compatibility conditions is

$$
f_{1}=\text { const. }=\cos \phi
$$

and I and II immediately integrate to give


\begin{align*}
& \bar{x}^{1}=x^{1} \cos \phi+\frac{1}{2}\left(x^{2}\right)^{2} \sin \phi+c \\
& \bar{x}^{2}=-x^{1} \sin \phi+\frac{1}{2}\left(x^{2}\right)^{2} \cos \phi+d \tag{4}
\end{align*}


We are, of course, free to set $\phi=c=d=0$ in (4).

Turning to $(9.3)$, we have to solve

$$
\begin{aligned}
& \text { (1) } \frac{\partial w}{\partial x^{1}}=u_{1}, \quad \frac{\partial w}{\partial x^{2}}=u_{2} \\
& \begin{array}{ll}
\text { (2) } \frac{\partial u_{1}}{\partial x^{1}}=0, \frac{\partial u_{1}}{\partial x^{2}}=0 & \text { (3) } \frac{\partial u_{2}}{\partial x^{1}}=0, \frac{\partial u_{2}}{\partial x^{2}}=u_{2} \Gamma_{22}^{2}=\frac{u_{2}}{x^{2}}
\end{array}
\end{aligned}
$$

Note that these equations include their own compatibility conditions! For instance, the second equation (2) and the first equation (3) ensure the compatibility of the two equations (1). The fact that system (9.3) is automatically compatible whenever $\mathrm{K}=0$ is crucial to the proof of Theorem 9.1. Integrating the above equations in the order (3)-(2)-(1), we get:

$$
w=a_{1} x^{1}+a_{2}\left(x^{2}\right)^{2}+a_{3} \quad\left(a_{1}, a_{2}, a_{3}=\text { const. }\right)
$$

or, replacing the index $k$,


\begin{equation*}
\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\left(x^{2}\right)^{2}+a_{3}^{k} \quad\left(a_{i}^{k}=\text { const. }\right) \tag{5}
\end{equation*}


As announced, (5) includes (4).

For subsequent use, the following compatibility theorem for quasilinear systems [which include linear systems such as (9.3)] is stated here, without proof:

Theorem 9.2: The quasilinear first-order system

$$
\frac{\partial u_{\lambda}}{\partial x^{j}}=F_{\lambda j}\left(u_{0}, u_{1}, \ldots, u_{m}, x^{1}, x^{2}, \ldots, x^{n}\right) \quad(\lambda=0,1, \ldots, m ; j=1,2, \ldots, n)
$$

where the functions $F_{\lambda j}$ are of differentiability class $C^{1}$, has a nontrivial solution for the $u_{\lambda}$, bounded over some region of $\mathbf{R}^{n}$, if and only if

$$
\frac{\partial F_{\lambda j}}{\partial u_{\nu}} F_{\nu k}+\frac{\partial F_{\lambda j}}{\partial x^{k}}=\frac{\partial F_{\lambda k}}{\partial u_{\nu}} F_{\nu j}+\frac{\partial F_{\lambda k}}{\partial x^{j}} \quad(\lambda=0,1, \ldots, m ; 1 \leqq j<k \leqq n)
$$

[The $\nu$-summations run from 0 to $m$.]

\subsection*{9.2 FLAT RIEMANNIAN SPACES}
A Riemannian space, or the determining metric, is termed flat if there is a transformation of coordinates $\bar{x}^{i}=\bar{x}^{i}(\mathbf{x})$ that puts the metric into the standard form


\begin{equation*}
\varepsilon d s^{2}=\varepsilon_{1}\left(d \bar{x}^{1}\right)^{2}+\varepsilon_{2}\left(d \bar{x}^{2}\right)^{2}+\cdots+\varepsilon_{n}\left(d \bar{x}^{n}\right)^{2} \tag{9.4}
\end{equation*}


where $\varepsilon_{i}= \pm 1$ for each $i$. This condition generalizes the concept of the Euclidean metric. The essential distinction between the two concepts revolves about positive-definiteness; the analogue to Theorem 9.1 with positive-definiteness removed is:

Theorem 9.3: A Riemannian space is flat if and only if $\mathrm{K}=0$ at all points.

Corollary 9.4: If $\mathrm{K}=0$, then $R=0$.

Proof: If $\mathrm{K}=0$, then by Theorem 9.3 the space is flat, and hence the $\bar{g}_{i j}$ are constant for some coordinate system $\left(\bar{x}^{i}\right)$. It follows that all $\bar{\Gamma}_{i j k}, \bar{\Gamma}_{j k}^{i}, \bar{R}_{j k l}^{i}, \bar{R}_{i j}$, and $\bar{R}_{j}^{i}$ vanish. Therefore, $\bar{R}=\bar{R}_{i}^{i}=0$, and since Ricci curvature is invariant, $R=0$.

Remark 1: Problem 8.35 shows that the converse of Corollary 9.4 does not hold.

EXAMPLE 9.2 Consider the Riemannian metric

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}+4\left(x^{2}\right)^{2}\left(d x^{2}\right)^{2}+4\left(x^{3}\right)^{2}\left(d x^{3}\right)^{2}-4\left(x^{4}\right)^{2}\left(d x^{4}\right)^{2}
$$

(a) Calculate the Riemannian curvature. (b) Find a solution of system (9.3) from which it may be inferred that the space is flat.

(a) Using Problem 6.4, we find as the nonvanishing Christoffel symbols

$$
\Gamma_{22}^{2}=\frac{1}{x^{2}} \quad \Gamma_{33}^{3}=\frac{1}{x^{3}} \quad \Gamma_{44}^{4}=\frac{1}{x^{4}}
$$

Because $\Gamma_{j k}^{i}=0$ unless $i=j=k$, the partial-derivative terms drop out of (8.2), leaving

$$
R_{j k l}^{i}=\Gamma_{j l}^{r} \Gamma_{r k}^{i}-\Gamma_{j k}^{r} \Gamma_{r l}^{i}=\Gamma_{i i}^{i} \Gamma_{i i}^{i}-\Gamma_{i i}^{i} \Gamma_{i i}^{i}=0 \quad \text { (not summed) }
$$

which in turn implies that $R_{i j k l}=0$ and $\mathrm{K}=0$.

(b) For the above-calculated Christoffel symbols

$$
\frac{\partial u_{1}}{\partial x^{1}}=0 \quad \frac{\partial u_{2}}{\partial x^{2}}=\frac{u_{2}}{x^{2}} \quad \frac{\partial u_{3}}{\partial x^{3}}=\frac{u_{3}}{x^{3}} \quad \frac{\partial u_{4}}{\partial x^{4}}=\frac{u_{4}}{x^{4}}
$$

with $\partial u_{i} / \partial x_{j}=0$ for $i \neq j$. Integrating,

$$
u_{1}=f_{1}\left(x^{2}, x^{3}, x^{4}\right) \quad u_{2}=x^{2} f_{2}\left(x^{1}, x^{3}, x^{4}\right) \quad u_{3}=x^{3} f_{3}\left(x^{1}, x^{2}, x^{4}\right) \quad u_{4}=x^{4} f_{4}\left(x^{1}, x^{2}, x^{3}\right)
$$

for arbitrary functions $f_{i}$. But the remaining equations (9.3), $\partial w / \partial x^{i}=u_{i}$, give rise to the compatibility relations

$$
\frac{\partial u_{i}}{\partial x^{j}}=\frac{\partial u_{j}}{\partial x^{i}}
$$

which are satisfied only if $f_{i}=c_{i}=$ const. Therefore,

$$
w=a_{1} x^{1}+a_{2}\left(x^{2}\right)^{2}+a_{3}\left(x^{3}\right)^{2}+a_{4}\left(x^{4}\right)^{2}+a_{5}
$$

and the transformation must be of the general form

$$
\bar{x}^{k}=a_{1}^{k} x^{1}+a_{2}^{k}\left(x^{2}\right)^{2}+a_{3}^{k}\left(x^{3}\right)^{2}+a_{4}^{k}\left(x^{4}\right)^{2}+a_{5}^{k} \quad\left(a_{i}^{k} \text { constants }\right)
$$

We wish to specialize the constants so that the covariant law $G=J^{T} \bar{G} J$ will hold, with $\bar{G}$ corresponding to (9.4). As a preliminary guess, set

$$
\left[a_{i}^{k}\right]_{45}=\left[\begin{array}{ccccc}
b_{1} & 0 & 0 & 0 & 0 \\
0 & b_{2} & 0 & 0 & 0 \\
0 & 0 & b_{3} & 0 & 0 \\
0 & 0 & 0 & b_{4} & 0
\end{array}\right]
$$

so that the covariant law becomes

$$
\left[\begin{array}{llll}
1 & & & \\
& 4\left(x^{2}\right)^{2} & & \\
& & 4\left(x^{3}\right)^{2} & \\
& & & -4\left(x^{4}\right)^{2}
\end{array}\right]=\left[\begin{array}{llll}
b_{1} & & & \\
& 2 b_{2} x^{2} & & \\
& & 2 b_{3} x^{3} & \\
& & & 2 b_{4} x^{4}
\end{array}\right]\left[\begin{array}{llll}
\varepsilon_{1} & & & \\
& \varepsilon_{2} & & \\
& & \varepsilon_{3} & \\
& & & \varepsilon_{4}
\end{array}\right]\left[\begin{array}{llll}
b_{1} & & \\
& 2 b_{2} x^{2} & & \\
& & 2 b_{3} x^{3} & \\
& & & 2 b_{4} x^{4}
\end{array}\right]
$$

By inspection, the choice $b_{1}=b_{2}=b_{3}=b_{4}=1$ will render $\varepsilon_{1}=\varepsilon_{2}=\varepsilon_{3}=-\varepsilon_{4}=1$.

In connection with (9.4) there is an interesting theorem (Sylvester's law of inertia). Define as the signature of a flat metric $\left(g_{i j}\right)$ the ordered $n$-tuple

$$
\left(\operatorname{sgn} \varepsilon_{1}, \operatorname{sgn} \varepsilon_{2}, \ldots, \operatorname{sgn} \varepsilon_{n}\right)
$$

composed of the signs of the coefficients in the standard form (i.e., the signs of $\bar{g}_{11}, \ldots, \bar{g}_{n n}$ ).

Theorem 9.5: The signature of a flat metric is uniquely determined up to order.

\subsection*{9.3 NORMAL COORDINATES}
It is possible to introduce local, quasirectangular coordinates in Riemannian space the use of which greatly simplifies the proofs of certain complicated tensor identities.

Let $O$ denote an arbitrary point of $\mathbf{R}^{n}$, and $\mathbf{p}=\left(p^{i}\right)$ an arbitrary direction (unit vector) at $O$. Assuming a positive-definite metric, consider the differential equations for geodesics,


\begin{equation*}
\frac{d^{2} x^{i}}{d s^{2}}+\Gamma_{j k}^{i} \frac{d x^{j}}{d s} \frac{d x^{k}}{d s}=0 \tag{9.5}
\end{equation*}


[cf. (7.13)], along with initial conditions


\begin{equation*}
\left.\frac{d x^{i}}{d s}\right|_{s=0}=p^{i} \tag{9.6}
\end{equation*}


Here the arc-length parameter is chosen to make $s=0$ at $O$.

Remark 2: Under an indefinite metric, there could exist directions at $O$ in which arc length could not be defined; see, e.g., Problem 7.22. There would then be no hope of satisfying (9.6) with $\left(p^{i}\right)$ arbitrary.

It can be shown that for a given $\mathbf{p}$, the system (9.5)-(9.6) has a unique solution; moreover, for each point $P$ in some neighborhood $\mathcal{N}$ of $O$, there is a unique choice of direction $\mathbf{p}$ at $O$ such that the solution curve $x^{i}=x^{i}(s)$ (a geodesic) passes through $P$. Accordingly, for each $P$ in $\mathcal{N}$, take as the coordinates of $P$


\begin{equation*}
y^{i}=s p^{i} \tag{9.7}
\end{equation*}


where $s$ is the distance along the geodesic from $O$ to $P$. The numbers $\left(y^{i}\right)$ are called the normal coordinates (or geodesic or Riemannian coordinates) of $P$.

EXAMPLE 9.3 Show that if the Riemannian metric $d s^{2}=g_{i i} d x^{i} d x^{j}$ for $\mathbf{R}^{2}$ is Euclidean and there is a point $O$\\
at which $g_{12}=0$, then normal coordinates $\left(y^{i}\right)$ with origin $O$ are constant multiples of $\left(z^{i}\right)$, for some rectangular coordinate system $\left(z^{i}\right)$.

Because $g_{12}=0$ at point $O$, the vectors $\mathbf{T}=\left(1 / \sqrt{g_{11}}, 0\right)$ and $\mathbf{S}=\left(0,1 / \sqrt{g_{22}}\right)$ are, at $O$, an orthonormal pair. The space, being Euclidean, admits a rectangular coordinate system; in particular, a system ( $z^{i}$ ) with origin $O$ and unit vectors $\mathbf{T}$ and $\mathbf{S}$ (Fig. 9-1). Again because the space is Euclidean, the straight line segment $O P$ is the unique geodesic connecting $O$ with the arbitrary point $P$. With $s=\overline{O P}$ and $\mathbf{p}$ the direction vector of $O P$, we have the vector equation

$$
z^{1} \mathbf{T}+z^{2} \mathbf{S}=s \mathbf{p}
$$

or componentwise,

$$
z^{1}\left(\frac{1}{\sqrt{g_{11}}}\right)=s p^{1} \equiv y^{1} \quad \text { and } \quad z^{2}\left(\frac{1}{\sqrt{g_{22}}}\right)=s p^{2} \equiv y^{2}
$$

QED.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-127}
\end{center}

Fig. 9-1

The chief value of Riemannian coordinates resides in the following theorem (Problem 9.10).

Theorem 9.6: If the metric tensor $\left(g_{i j}\right)$ is positive definite, then, at the origin of a Riemannian coordinate system $\left(y^{i}\right)$, all $\partial g_{i j} / \partial y^{k}, \partial g^{i j} / \partial y^{k}, \Gamma_{i j k}$, and $\Gamma_{j k}^{i}$ are zero.

Remark 3: Recall that neither the partial derivatives of the metric tensor nor the Christoffel symbols are tensorial. Thus, their $\left(y^{i}\right)$-representations can vanish at $O$ without their $\left(x^{i}\right)$-representations doing so. For instance, because the transformation between $\left(x^{i}\right)$ and $\left(y^{i}\right)$ has $J=I$ at $O,(6.5)$ gives:

$$
\left.\Gamma_{j k}^{i}(\mathbf{x})\right|_{O}=\left.\frac{\partial^{2} y^{i}}{\partial x^{j} \partial x^{k}}\right|_{O}
$$

The right side is generally nonzero, unless the coordinate transformation happens to be linear.

EXAMPLE 9.4 Prove Bianchi's first identity, $R_{i j k l}+R_{i k l j}+R_{i l j k}=0$.

Theorem 9.6 implies that at $O$, the origin of normal coordinates,

$$
R_{i j k l}=\frac{\partial \Gamma_{j l i}}{\partial y^{k}}-\frac{\partial \Gamma_{j k i}}{\partial y^{l}}
$$

If we use the notation $\Gamma_{i j k l}$ for $\partial \Gamma_{i j k} / \partial y^{l}$, for arbitrary $i, j, k, l$, then

$$
\begin{aligned}
& R_{i j k l}=\Gamma_{j l i k}-\Gamma_{j k i l} \\
& R_{i k l j}=\Gamma_{k j i l}-\Gamma_{k l i j} \\
& R_{i l j k}=\Gamma_{l k i j}-\Gamma_{l j i k}
\end{aligned}
$$

On summing these three relations and observing the cancellations which take place, we see that the desired identity holds at $O$ in the coordinates $\left(y^{i}\right)$. This tensor identity must therefore remain valid at $O$ in the alias coordinates $\left(x^{i}\right)$. But $O$ is any point of $\mathbf{R}^{n}$, and the proof is complete.

EXAMPLE 9.5 Prove Bianchi's second identity,


\begin{equation*}
R_{i j k l, u}+R_{i j l u, k}+R_{i j u k, l}=0 \tag{9.8}
\end{equation*}


Working with the Riemann tensor of the second kind, we have, at the origin $O$ of normal coordinates,

$$
\begin{aligned}
R_{j k l, u}^{i} & =\frac{\partial R_{j k l}^{i}}{\partial y^{u}}=\frac{\partial}{\partial y^{u}}\left(\frac{\partial \Gamma_{j l}^{i}}{\partial y^{k}}-\frac{\partial \Gamma_{j k}^{i}}{\partial y^{l}}+\Gamma_{j l}^{r} \Gamma_{r k}^{i}-\Gamma_{j k}^{r} \Gamma_{r l}^{i}\right) \\
& =\Gamma_{j l k u}^{i}-\Gamma_{j k l u}^{i}
\end{aligned}
$$

since terms like $\left(\partial \Gamma_{j l}^{r} / \partial y^{u}\right) \Gamma_{r k}^{i}$ vanish along with the $\Gamma_{r k}^{i}$ at $O$. From this, permutation of subscripts yields

$$
R_{j k l, u}^{i}+R_{j l u, k}^{i}+R_{j u k, l}^{i}=0
$$

at $O$, and the validity of $(9.8)$ at $O$ follows from the fact that covariant differentiation commutes with the lowering of a superscript (Problem 6.11). We conclude, as in Example 9.4, that (9.8) holds generally in $\left(x^{i}\right)$.

A positive definite metric has been tacitly assumed, both here and in Example 9.4. The assumption can be dropped; see Problem 9.13.

\subsection*{9.4 SCHUR'S THEOREM}
From Chapter 8 it is known that although every point of Riemannian two-space is isotropic, the curvature ( $=R_{1212} / g$ ) can still vary from one isotropic point to the next. However, Problems 8.11, $8.12,8.28$, and 8.29 suggest that a different situation prevails in $\mathbf{R}^{3}$. To prove the general theorem, known as Schur's theorem, it is necessary to establish a preliminary result, a generalization of (8.11).

Lemma 9.7: At an isotropic point of $\mathbf{R}^{n}$ the Riemannian curvature is given by


\begin{equation*}
\mathrm{K}=\frac{R_{a b c d}}{g_{a c} g_{b d}-g_{a d} g_{b c}} \equiv \frac{R_{a b c d}}{G_{a b c d}} \tag{9.9}
\end{equation*}


for any specific subscript string such that $G_{a b c d} \neq 0$. [If $G_{a b c d}=0$, then $R_{a b c d}=0$ also.] For a proof, see Problem 9.8.

Theorem 9.8 (Schur's Theorem): If all points in some neighborhood $\mathcal{N}$ in a Riemannian $\mathbf{R}^{n}$ are isotropic and $n \geqq 3$, then $\mathrm{K}$ is constant throughout that neighborhood.

For a proof, see Problem 9.14.

\subsection*{9.5 THE EINSTEIN TENSOR}
The Einstein tensor is defined in terms of the Ricci tensor $R_{i j}$ and the curvature invariant $R$ (Section 8.4):


\begin{equation*}
G_{j}^{i} \equiv R_{j}^{i}-\frac{1}{2} \delta_{j}^{i} R \tag{9.10}
\end{equation*}


It is clear that $\left(G_{j}^{i}\right)$ is in fact a mixed tensor of order two.

As a direct generalization of the notion of the divergence of a vector field $\mathbf{V}=\left(V^{i}\right)$ relative to\\
rectangular coordinates $\left(x^{i}\right)$,

$$
\operatorname{div} \mathbf{V}=\frac{\partial V^{1}}{\partial x^{1}}+\frac{\partial V^{2}}{\partial x^{2}}+\cdots+\frac{\partial V^{n}}{\partial x^{n}} \equiv \frac{\partial V^{r}}{\partial x^{r}}
$$

we define the divergence of the general tensor $\mathbf{T}=\left(T_{j_{1} j_{2} \ldots j_{q}}^{i_{1} i_{2} \ldots i_{p} \ldots i_{p}}\right)$ with respect to its $k$ th contravariant index to be the tensor


\begin{equation*}
\operatorname{div} \mathbf{T} \equiv\left(T_{j_{1} j_{2} \ldots j_{q}, r}^{i_{1} i_{2} \ldots r, i_{p}}\right) \tag{9.11}
\end{equation*}


In Problem 9.15 is proved

Theorem 9.9: For any Riemannian metric, the divergence of the Einstein tensor is zero at all points.

\section*{Solved Problems}
\section*{ZERO CURVATURE AND THE EUCLIDEAN METRIC}
9.1 Test the compatibility conditions (Theorem 9.2) for the system


\begin{equation*}
\frac{\partial u_{0}}{\partial x^{1}}=\frac{u_{0}}{x^{1}} \quad \frac{\partial u_{0}}{\partial x^{2}}=2 x^{2} u_{0} \tag{1}
\end{equation*}


If it is compatible, solve the system.

In the notation of Theorem 9.2, there is only the condition corresponding to $\lambda=0, j=1, k=2$ to be satisfied.

$$
\begin{gathered}
\frac{\partial F_{01}}{\partial u_{0}} F_{02}+\frac{\partial F_{01}}{\partial x^{2}} \stackrel{?}{=} \frac{\partial F_{02}}{\partial u_{0}} F_{01}+\frac{\partial F_{02}}{\partial x^{1}} \\
\frac{\partial}{\partial u_{0}}\left(\frac{u_{0}}{x^{1}}\right) \cdot 2 x^{2} u_{0}+\frac{\partial}{\partial x^{2}}\left(\frac{u_{0}}{x^{1}}\right) \stackrel{?}{=} \frac{\partial}{\partial u_{0}}\left(2 x^{2} u_{0}\right) \cdot \frac{u_{0}}{x^{1}}+\frac{\partial}{\partial x^{1}}\left(2 x^{2} u_{0}\right) \\
\frac{2 x^{2} u_{0}}{x^{1}}=\frac{2 x^{2} u_{0}}{x^{1}}
\end{gathered}
$$

Therefore, the system is compatible. The first equation (1) integrates to $u_{0}=x^{1} \phi\left(x^{2}\right)$; the second equation then gives

$$
x^{1} \phi^{\prime}=2 x^{2} x^{1} \phi \quad \text { whence } \quad \phi=c \exp \left(x^{2}\right)^{2}
$$

Hence the solution of (1) is $u_{0}=c x^{1} \exp \left(x^{2}\right)^{2}$.

9.2 Show that $\mathbf{R}^{3}$ under the metric $d s^{2}=\left[\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}\right]\left(d x^{1}\right)^{2}+\left[\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}\right]\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2}$ is Euclidean.

This metric has $g_{33}=$ const., and $g_{11}$ and $g_{22}$ independent of $x^{3}$. Problem 6.4 then shows that $\Gamma_{j k}^{i}=0$ whenever $i, j$, or $k$ equals 3 ; consequently, of the six independent components of the Riemann tensor, only $R_{1212}$ is possibly nonzero. But (from Problem 6.4), with $z \equiv\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}$,

$$
\begin{array}{lll}
\Gamma_{11}^{1}=\frac{x^{1}}{z} & \Gamma_{12}^{1}=\Gamma_{21}^{1}=\frac{x^{2}}{z} & \Gamma_{22}^{1}=-\frac{x^{1}}{z} \\
\Gamma_{11}^{2}=-\frac{x^{2}}{z} & \Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{x^{1}}{z} & \Gamma_{22}^{2}=\frac{x^{2}}{z}
\end{array}
$$

so that

$$
\begin{aligned}
R_{212}^{1} & =\frac{\partial \Gamma_{22}^{1}}{\partial x^{1}}-\frac{\partial \Gamma_{21}^{1}}{\partial x^{2}}+\Gamma_{22}^{1} \Gamma_{11}^{1}+\Gamma_{22}^{2} \Gamma_{21}^{1}-\Gamma_{21}^{1} \Gamma_{12}^{1}-\Gamma_{21}^{2} \Gamma_{22}^{1} \\
& \left.=\frac{-z+x^{1}\left(2 x^{1}\right)}{z^{2}}-\frac{z-x^{2}\left(2 x^{2}\right)}{z^{2}}+\left(-\frac{x^{1}}{z}\right)\left(\frac{x^{1}}{z}\right)+\frac{x^{2}}{z}\left(\frac{x^{2}}{z}\right)-\frac{x^{2}}{z}\left(\frac{x^{2}}{z}\right) \right\rvert\,-\frac{x^{1}}{z}\left(-\frac{x^{1}}{z}\right)=0
\end{aligned}
$$

Consequently, $R_{1212}=0=\mathrm{K}$. As the metric is clearly positive definite, Theorem 9.1 implies that the space is Euclidean.

9.3 For the Euclidean space of Problem 9.2, exhibit a transformation from the given coordinate system $\left(x^{i}\right)$ to a rectangular system $\left(\bar{x}^{i}\right)$.

Using the Christoffel symbols as calculated in Problem 9.2, we obtain from (9.3) the following system for the $u_{i}$ :

\[
\begin{array}{lll}
\frac{\partial u_{1}}{\partial x^{1}}=\frac{x^{1} u_{1}-x^{2} u_{2}}{z} & \frac{\partial u_{1}}{\partial x^{2}}=\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \frac{\partial u_{1}}{\partial x^{3}}=0 \\
\frac{\partial u_{2}}{\partial x^{1}}=\frac{x^{2} u_{1}+x^{1} u_{2}}{z} & \frac{\partial u_{2}}{\partial x^{2}}=\frac{-x^{1} u_{1}+x^{2} u_{2}}{z} & \frac{\partial u_{2}}{\partial x^{3}}=0 \\
\frac{\partial u_{3}}{\partial x^{1}}=0 & \frac{\partial u_{3}}{\partial x^{2}}=0 & \frac{\partial u_{3}}{\partial x^{3}}=0 \tag{3}
\end{array}
\]

Thus $u_{1}$ and $u_{2}$ are functions of $x^{1}, x^{2}$ alone, and $u_{3}=$ const. Since the $g_{i j}$ are all polynomials of degree 2 in $x^{1}, x^{2}$, use the method of undetermined coefficients, assuming polynomial forms

$$
u_{i}=a_{i}\left(x^{1}\right)^{2}+b_{i} x^{1} x^{2}+c_{i}\left(x^{2}\right)^{2}+d_{i} x^{1}+e_{i} x^{2}+f_{i} \quad(i=1,2)
$$

The (compatibility) relation $\partial u_{1} / \partial x^{2}=\partial u_{2} / \partial x^{1}$ implied by the second equation (1) and the first equation (2) requires

$$
b_{1}=2 a_{2} \quad 2 c_{1}=b_{2} \quad e_{1}=d_{2}
$$

Similarly, $\partial u_{1} / \partial x^{1}=-\partial u_{2} / \partial x^{2}$ implies

$$
2 a_{1}=-b_{2} \quad b_{1}=-2 c_{2} \quad d_{1}=-e_{2}
$$

Using the first equation (1), or $z\left(\partial u_{1} / \partial x^{1}\right)=x^{1} u_{1}-x^{2} u_{2}$, we get:

$$
a_{1}=0 \quad a_{2}=0 \quad c_{1}=b_{2} \quad b_{1}=-c_{2} \quad d_{1}=-e_{2} \quad f_{1}=0=-f_{2}
$$

It follows that $b_{1}=b_{2}=c_{1}=c_{2}=0$, and therefore (renotating $d_{1}$ and $e_{1}$ )

$$
u_{1}=a x^{1}+b x^{2} \quad u_{2}=b x^{1}-a x^{2} \quad u_{3}=c
$$

[Note: This solution of (1)-(2)-(3) may be obtained by the method of characteristics, without any prior assumptions.]

The first equations (9.3),

$$
\frac{\partial w}{\partial x^{1}}=a x^{1}+b x^{2} \quad \frac{\partial w}{\partial x^{2}}=b x^{1}-a x^{2} \quad \frac{\partial w}{\partial x^{3}}=c
$$

may now be integrated to give

$$
w=\frac{a}{2}\left(x^{1}\right)^{2}+b x^{1} x^{2}-\frac{a}{2}\left(x^{2}\right)^{2}+c x^{3}+d
$$

or, replacing $\bar{x}^{k}$ and corresponding superscripts, and with $d=0$,

$$
\bar{x}^{k}=\frac{a^{k}}{2}\left(x^{1}\right)^{2}+b^{k} x^{1} x^{2}-\frac{a^{k}}{2}\left(x^{2}\right)^{2}+c^{k}\left(x^{2}\right)^{2}
$$

It is clear that we may take $c^{1}=c^{2}=0=a^{3}=b^{3}$ and $c^{3}=1$ :

$$
\begin{aligned}
& \bar{x}^{1}=\frac{1}{2} a^{1}\left(x^{1}\right)^{2}+b^{1} x^{1} x^{2}-\frac{1}{2} a^{1}\left(x^{2}\right)^{2} \\
& \bar{x}^{2}=\frac{1}{2} a^{2}\left(x^{1}\right)^{2}+b^{2} x^{1} x^{2}-\frac{1}{2} a^{2}\left(x^{2}\right)^{2} \\
& \bar{x}^{3}=x^{3}
\end{aligned}
$$

The Jacobian matrix is

$$
J=\left[\begin{array}{ccc}
a^{1} x^{1}+b^{1} x^{2} & b^{1} x^{1}-a^{1} x^{2} & 0 \\
a^{2} x^{1}+b^{2} x^{2} & b^{2} x^{1}-a^{2} x^{2} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

Since $J^{T} J=G$, we must have

$$
\left(a^{1}\right)^{2}+\left(a^{2}\right)^{2}=1 \quad a^{1} b^{1}+a^{2} b^{2}=0 \quad\left(b^{1}\right)^{2}+\left(b^{2}\right)^{2}=1
$$

so take $a^{1}=0, a^{2}=1, b^{2}=0, b^{1}=1$. The transformation is, finally,

$$
\bar{x}^{1}=x^{1} x^{2} \quad \bar{x}^{2}=\frac{1}{2}\left[\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}\right] \quad \bar{x}^{3}=x^{3}
$$

\section*{FLAT RIEMANNIAN SPACES}
9.4 Determine whether the following metric is flat and/or Euclidean:

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(x^{2}\right)^{2}\left(d x^{2}\right)^{2} \quad(n=2)
$$

Since the metric is not positive definite, it cannot be Euclidean. To determine flatness, it suffices to examine $R_{1212}=g_{11} R_{212}^{1}$. But Problem 6.4 shows that $R_{212}^{1}=0$; hence the space is flat.

9.5 Show that if the metric tensor is constant, the space is flat and the coordinate transformation $\bar{x}=A x$, where $A$ is a rank- $n$ matrix of eigenvectors of $G=\left(g_{i j}\right)$, diagonalizes the metric (i.e., $\bar{g}_{i j}=0$ if $i \neq j$ ).

Since all partial derivatives of $g_{i j}$ are zero, all Christoffel symbols will vanish and all $R_{i j k l}=0$, making $\mathrm{K}=0$. Thus, by Theorem 9.3, the space is flat. By Chapters 2 and 3, if $\bar{x}=A x$, then $J=A$ and

$$
G=J^{T} \bar{G} J=A^{T} \bar{G} A
$$

However, since $G$ is real and symmetric, its eigenvectors form an orthogonal matrix which we now choose as $A$, with

$$
A G A^{-1}=A G A^{T}=D \quad \text { (diagonal matrix of eigenvalues of } G \text { ) }
$$

Hence, $\bar{G}=A G A^{T}=D \quad$ QED.

9.6 Find the signature of the flat metric

$$
\varepsilon d s^{2}=4\left(d x^{1}\right)^{2}+5\left(d x^{2}\right)^{2}-2\left(d x^{3}\right)^{2}+2\left(d x^{4}\right)^{2}-4 d x^{2} d x^{3}-4 d x^{2} d x^{4}-10 d x^{3} d x^{4}
$$

is

In view of Problem 9.5, it suffices to find the eigenvalues $\lambda$ of $G=\left(g_{i j}\right)$. The characteristic equation

$$
\begin{aligned}
|G-\lambda I| & =\left|\begin{array}{cccc}
4-\lambda & 0 & 0 & 0 \\
0 & 5-\lambda & -2 & -2 \\
0 & -2 & -2-\lambda & -5 \\
0 & -2 & -5 & 2-\lambda
\end{array}\right| \\
& =(4-\lambda)\left|\begin{array}{ccc}
5-\lambda & -2 & -2 \\
-2 & -2-\lambda & -5 \\
-2 & -5 & 2-\lambda
\end{array}\right|=-(4-\lambda)\left|\begin{array}{ccc}
5-\lambda & 2 & 0 \\
-2 & 2+\lambda & -3+\lambda \\
-2 & 5 & 7-\lambda
\end{array}\right| \\
& =-(4-\lambda)\left[(5-\lambda)\left(29-\lambda^{2}\right)+8(5-\lambda)\right]=-(4-\lambda)(5-\lambda)\left(37-\lambda^{2}\right)=0
\end{aligned}
$$

from which the eigenvalues are $\lambda=+4,+5,+\sqrt{37},-\sqrt{37}$. This means that there is a transformation which changes the metric into the form

$$
\varepsilon d s^{2}=4\left(d x^{1}\right)^{2}+5\left(d x^{2}\right)^{2}+\sqrt{37}\left(d x^{3}\right)^{2}-\sqrt{37}\left(d x^{4}\right)^{2}=\left(d \bar{x}^{1}\right)^{2}+\left(d \bar{x}^{2}\right)^{2}+\left(d \bar{x}^{3}\right)^{2}-\left(d \bar{x}^{4}\right)^{2}
$$

with the obvious change of coordinates. Hence, the signature is $(+++-)$, or some permutation thereof (Theorem 9.5).

9.7 Show that the conditions $R_{i j k l}=0$ are sufficient for the compatibility of (9.3).

In the notation of Theorem 9.2, (9.3) takes the form (with $m=n$ )

$$
\begin{array}{ll}
\boldsymbol{\lambda}=\mathbf{0} & \frac{\partial u_{0}}{\partial x^{j}}=F_{0 j} \equiv u_{j} \\
\lambda>0 & \frac{\partial u_{\lambda}}{\partial x^{j}}=F_{\lambda j} \equiv u_{r} \Gamma_{\lambda j}^{r}(\mathbf{x})
\end{array}
$$

The corresponding compatibility conditions are

$$
\boldsymbol{\lambda}=\mathbf{0} \quad \delta_{j}^{\nu} u_{r} \Gamma_{\nu k}^{r}=\delta_{k}^{\nu} u_{r} \Gamma_{\nu j}^{r}
$$

or $u_{r} \Gamma_{j k}^{r}=u_{r} \Gamma_{k j}^{r}$, which holds trivially, and

$$
\boldsymbol{\lambda}>\mathbf{0} \quad \delta_{r}^{\nu} \Gamma_{\lambda j}^{r} u_{s} \Gamma_{\nu k}^{s}+u_{r} \frac{\partial \Gamma_{\lambda j}^{r}}{\partial x^{k}}=\delta_{r}^{\nu} \Gamma_{\lambda k}^{r} u_{s} \Gamma_{\nu j}^{s}+u_{r} \frac{\partial \Gamma_{\lambda k}^{r}}{\partial x^{j}}
$$

which rearranges to

$$
(\underbrace{\frac{\partial \Gamma_{\lambda j}^{r}}{\partial x^{k}}-\frac{\partial \Gamma_{\lambda k}^{r}}{\partial x^{j}}+\Gamma_{\lambda j}^{s} \Gamma_{s k}^{r}-\Gamma_{\lambda k}^{s} \Gamma_{s j}^{r}}_{R_{\lambda k j}^{r}}) u_{r}=0
$$

Thus, $R_{r \lambda k j}=0$ forces $R_{\lambda k j}^{r}=0$ and compatibility.

\subsection*{9.8 Prove Lemma 9.7.}
As $\left(R_{i j k l}\right)$ and $\left(G_{i j k l}\right)$ are tensors [see Example 8.3] and $\mathrm{K}$ is an invariant,

$$
\left(T_{i j k l}\right) \equiv\left(R_{i j k l}-\mathrm{K} G_{i j k l}\right)
$$

is a tensor of the same type and order. It must be proved that all $T_{i j k l}=0$ at an isotropic point $P$. Since $\mathrm{K}$ is independent of direction at $P$, so are the $T_{i j k l}$; and (8.7) gives


\begin{equation*}
T_{i j k l} U^{i} V^{j} U^{k} V^{l}=0 \quad\left(T_{i j k l}=T_{i j k l}(P)\right) \tag{1}
\end{equation*}


If we define the second-order tensor $\left(S_{i k}\right) \equiv\left(T_{i j k l} V^{j} V^{l}\right)$, we find that $S_{i k}=S_{k i}$, and by (1), $S_{i k} U^{i} U^{k}=0$ at $P$ for any $\left(U^{i}\right)$. It follows that all $S_{i k}=0$ at $P$. Now set $V^{i}=\delta_{a}^{i}$. Then, at $P$,

$$
0=S_{i k}=T_{i j k l} \delta_{a}^{j} \delta_{a}^{l}=T_{i a k a}
$$

for arbitrary (fixed) index $a$. Next set $V^{i}=\delta_{a}^{i}+\delta_{b}^{i}$ for arbitrary fixed indices $a$ and $b$ :

$$
0=T_{i j k l} V^{j} V^{l}=T_{i j k l}\left(\delta_{a}^{j}+\delta_{b}^{j}\right)\left(\delta_{a}^{l}+\delta_{b}^{l}\right)=T_{i a k a}+T_{i a k b}+T_{i b k a}+T_{i b k b}
$$

or $T_{i a k b}+T_{i b k a}=0$. Therefore, since $T_{i j k l}$ obeys the same symmetry laws as $R_{i j k l}$ and $G_{i j k l}$,


\begin{align*}
T_{i j k l}-T_{i l j k} & =0  \tag{2}\\
T_{i j k l}+T_{i k l j}+T_{i l j k} & =0 \tag{3}
\end{align*}


Adding (2) and (3),


\begin{equation*}
2 T_{i j k l}+T_{i k l j}=0 \tag{4}
\end{equation*}


But, from (2), $T_{i k l j}=T_{i j k l}$, so that (4) implies $T_{i j k l}=0$, as desired.

\subsection*{9.9 Prove Theorems 9.1 and 9.3.}
We already know that if the space is either Euclidean or flat, $K \equiv 0$. Suppose, conversely, that $K \equiv 0$; then every point is isotropic, and Lemma 9.7 implies that all $R_{i j k l}$ vanish. It then follows from Problem 9.7 that there exists a coordinate system $\left(\bar{x}^{i}\right)$ for which $\bar{\Gamma}_{j k}^{i}=0$ or $\bar{g}_{i j}=$ const. By Problem 9.5 , there exists another coordinate system, $\left(y^{i}\right)$, in which the metric takes the form (for real constants $a_{i}$ )

$$
\varepsilon d s^{2}=\varepsilon_{1} a_{1}^{2}\left(d y^{1}\right)^{2}+\varepsilon_{2} a_{2}^{2}\left(d y^{2}\right)^{3}+\cdots+\varepsilon_{n} a_{n}^{2}\left(d y^{n}\right)^{2}
$$

The transformation $\bar{y}^{1}=a_{1} y^{1}, \bar{y}^{2}=a_{2} y^{2}, \ldots, \bar{y}^{n}=a_{n} y^{n}$ now reduces the metric to


\begin{equation*}
\varepsilon d s^{2}=\varepsilon_{1}\left(d \bar{y}^{1}\right)^{2}+\varepsilon_{2}\left(d \bar{y}^{2}\right)^{2}+\cdots+\varepsilon_{n}\left(d \bar{y}^{n}\right)^{2} \tag{1}
\end{equation*}


and the space is flat. This proves Theorem 9.3. If the given metric is positive definite, then in $(1), \varepsilon_{i}=1$ for each $i$. In this case the metric is Euclidean, proving Theorem 9.1.

\section*{NORMAL COORDINATES}
\subsection*{9.10 Prove Theorem 9.6.}
If $\left(y^{i}\right)$ are normal coordinates, then the geodesic through $O$ and any point $P$ in some neighborhood $\mathcal{N}$ of $O$ has the parametric form

$$
y^{i}=s p^{i} \quad\left(p^{i}=\text { const. }\right)
$$

This geodesic thus obeys the differential equations

$$
\frac{d y^{i}}{d s}=p^{i} \quad \text { and } \quad \frac{d^{2} y^{i}}{d s^{2}}=0
$$

But it must also satisfy (9.5), $\delta \mathbf{T} / \delta s=0$, in the coordinates $\left(y^{i}\right)$ :

$$
\frac{d^{2} y^{i}}{d s^{2}}+\Gamma_{j k}^{i} \frac{d y^{j}}{d s} \frac{d y^{k}}{d s}=0
$$

Thus, by substittion, $\Gamma_{j k}^{i} p^{j} p^{k}=0$ for all directions $\left(p^{i}\right)$ at $O$. But $\Gamma_{j k}^{i}$ is symmetric for each $i$; hence, $\Gamma_{j k}^{i}=0$ at $O$ for all $i, j, k$. Also, $\Gamma_{i j k}=g_{k r} \Gamma_{i j}^{r}=0$; hence, $\partial g_{i j} / \partial y^{k}=0$ at $O$, by (6.2). Finally, since $g^{i j} g_{j r}=\delta_{r}^{i}$, the product rule for differentiation yields $\partial g^{i j} / \partial y^{k}=0$ at $O$.

9.11 Prove that at the origin of a Riemannian coordinate system $\left(y^{i}\right)$,

$$
\frac{\partial \Gamma_{j i}^{i}}{\partial y^{k}}=\frac{\partial \Gamma_{k i}^{i}}{\partial y^{j}} \quad(\text { all } j \text { and } k ; \text { summed on } i)
$$

Since $\Gamma_{j k}^{i}$ and $\partial g^{i j} / \partial y^{k}$ all vanish at the origin $O$ of the Riemannian coordinate system,


\begin{equation*}
\frac{\partial \Gamma_{j i}^{i}}{\partial y^{k}}=\frac{\partial}{\partial y^{k}}\left(g^{i r} \Gamma_{j i r}\right)=g^{i r} \frac{\partial}{\partial y^{k}}\left[\frac{1}{2}\left(-g_{j i r}+g_{i r j}+g_{r j i}\right)\right]=\frac{1}{2} g^{i r}\left(-g_{j i r k}+g_{i r j k}+g_{r j i k}\right) \tag{1}
\end{equation*}


at $O$, with $g_{i j k l} \equiv \partial^{2} g_{i j} / \partial y^{k} \partial y^{l}$. But, since $g^{i r}=g^{r i}$,

$$
g^{i r} g_{j i r k}=g^{r i} g_{j i r k}=g^{i r} g_{j r i k}=g^{i r} g_{r j i k}
$$

and (1) becomes

$$
\frac{\partial \Gamma_{j i}^{i}}{\partial y^{k}}=\frac{1}{2} g^{i r} g_{i r j k}=\frac{1}{2} g^{i r} g_{i r k j}=\frac{\partial \Gamma_{k i}^{i}}{\partial y^{j}}
$$

9.12 Prove the identity $R_{i j k l, u}+R_{i l j k, u}=R_{i k u l, j}+R_{i k j u, l}$.

Covariant differentiation of Bianchi's first identity, (8.6), gives $R_{i j k l, u}+R_{i k l j, u}+R_{i l j k, u}=0$. Then the second identity, $(9.8)$, yields

$$
R_{i j k l, u}+R_{i l j k, u}=-R_{i k l j, u}=R_{i k j u, l}+R_{i k u l, j}
$$

9.13 Show that Bianchi's identities remain valid under an indefinite metric.

One can appeal to the topological fact that, at a given point $P$ of $\mathbf{R}^{n}$, the directions for which a given metric $\left(g_{i j}\right)$ is indefinite span, at worst, a hyperplane. Hence, normal coordinates are possible along geodesics whose tangent vectors $\left(p^{i}\right)$ at $P$ do not lie in the hyperplane; Problem 9.10 gives $\Gamma_{j k}^{i} p^{j} p^{k}=0$ for these directions. But the $\Gamma_{i k}^{i}$ are continuous, and any direction in the hyperplane is the limit of a sequence of directions not in the hyperplane. It follows that $\Gamma_{j k}^{i} p^{i} p^{j}=0$ for all $\left(p^{i}\right)$, yielding Theorem 9.6 and the Bianchi identities.

\section*{SCHUR'S THEOREM}
9.14 Prove Schur's theorem (Theorem 9.8).

By Lemma 9.7, $R_{i j k l}=G_{i j k l} \mathrm{~K}$ throughout $\mathcal{N}$. Take the covariant derivative of both sides with respect to $x^{u}$, then permute indices ( $G_{i j k l, u}=0$ because $g_{i j, u}=0$ in general):

$$
R_{i j k l, u}=G_{i j k l} \mathrm{~K}_{, u} \quad R_{i j l u, k}=G_{i j l u} \mathrm{~K}_{, k} \quad R_{i j u k, l}=G_{i j u k} \mathrm{~K}_{, l}
$$

Add the three equations and apply $(9.8)$ :


\begin{equation*}
G_{i j k l} \mathrm{~K}_{, u}+G_{i j l u} \mathrm{~K}_{, k}+G_{i j u k} \mathrm{~K}_{, l}=0 \tag{1}
\end{equation*}


Multiply both sides of (1) by $g^{i k} g^{j l}$ and sum. Since

$$
\begin{aligned}
& g^{i k} g^{j l} G_{i j k l}=g^{i k} g^{j l}\left(g_{i k} g_{j l}-g_{i l} g_{j k}\right)=\delta_{k}^{k} \delta_{l}^{l}-\delta_{l}^{k} \delta_{k}^{l}=n^{2}-n \\
& g^{i k} g^{j l} G_{i j l u}=g^{i k} g^{j l}\left(g_{i l} g_{j u}-g_{i u} g_{j l}\right)=\delta_{l}^{k} \delta_{u}^{l}-\delta_{u}^{k} \delta_{l}^{l}=\delta_{u}^{k}-n \delta_{u}^{k} \\
& g^{i k} g^{j l} G_{i j u k}=g^{i k} g^{j l}\left(g_{i u} g_{j k}-g_{i k} g_{j u}\right)=\delta_{u}^{k} \delta_{k}^{l}-\delta_{k}^{k} \delta_{u}^{l}=\delta_{u}^{l}-n \delta_{u}^{l}
\end{aligned}
$$

that summation yields the relation

$$
\begin{aligned}
0 & =\left(n^{2}-n\right) \mathrm{K}_{, u}+\left(\delta_{u}^{k}-n \delta_{u}^{k}\right) \mathrm{K}_{, k}+\left(\delta_{u}^{l}-n \delta_{u}^{l}\right) \mathrm{K}_{, t} \\
& =\left(n^{2}-n\right) \mathrm{K}_{, u}+(1-n) \mathrm{K}_{, u}+(1-n) \mathrm{K}_{, u}=(n-2)(n-1) \mathrm{K}_{, u}
\end{aligned}
$$

For $n \geqq 3, \mathrm{~K}_{, u}=\partial \mathrm{K} / \partial x^{u}=0$. Since $u$ was arbitrary, $\mathrm{K}$ must be constant over $\mathcal{N}$. QED

\section*{THE EINSTEIN TENSOR}
\subsection*{9.15 Prove Theorem 9.9.}
We must prove that $G_{i, r}^{r}=0$. Multiply both sides of $(9.8)$ by $g^{i l} g^{j k}$ and sum:

$$
\begin{aligned}
0 & =g^{i l} g^{j k} R_{i j k l, u}-g^{i l} g^{j k} R_{i j u l, k}-g^{i l} g^{j k} R_{j i u k, l} \\
& =g^{j k} R_{j k l, u}^{l}-g^{j k} R_{j u l, k}^{l}-g^{i l} R_{i u k, l}^{k}=g^{j k} R_{j k, u}-g^{j k} R_{j u, k}-g^{i l} R_{i u, l} \\
& =R_{k, u}^{k}-R_{u, k}^{k}-R_{u, l}^{l}=2\left(\frac{1}{2} R_{, u}-R_{u, k}^{k}\right)
\end{aligned}
$$

or, changing $u$ to $i$ and $k$ to $r, \frac{1}{2} \delta_{i}^{r} R_{, r}-R_{i, r}^{r}=0$. But, by Problem 6.32, $\delta_{i, j}^{r}=0$ for all $i, j, r$; hence,

$$
\left(R_{i}^{r}-\frac{1}{2} \delta_{i}^{r} R\right)_{, r}=0 \quad \text { or } \quad G_{i, r}^{r}=0
$$

9.16 Show that $G_{i j}$, the associated Einstein tensor obtained by lowering the index $i$ in $G_{j}^{i}$, is symmetric.

By definition,

$$
G_{i j}=g_{i k} G_{j}^{k}=g_{i k}\left(R_{j}^{k}-\frac{1}{2} \delta_{j}^{k} R\right)=R_{i j}-\frac{1}{2} g_{i j} R
$$

which is obviously symmetric (by symmetry of the Ricci tensor).

\section*{Supplementary Problems}
9.17 Solve, if compatible, the 'system $\partial u_{\lambda} / \partial x^{j}=F_{\lambda j}$, with

$$
\begin{aligned}
& F_{01}=x^{2} / 2 u_{0} \quad F_{02}=x^{1} / 2 u_{0} \\
& F_{01}=u_{0} x^{1} \quad F_{02}=u_{1} x^{2} \quad F_{11}=u_{0} x^{1} \quad F_{12}=u_{1} x^{2}
\end{aligned}
$$

9.18 Verify that $d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2}$ represents the Euclidean metric (in polar coordinates).

9.19 Consider the metric $\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(x^{1} d x^{2}\right)^{2}-\left(x^{1} d x^{3}\right)^{2}$. Show that $R_{1212}=2$ and that, therefore, the space is not flat.

9.20 Determine whether the following metric is flat and/or Euclidean:

$$
\varepsilon d s^{2}=\left(d x^{1}\right)^{2}-\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2} \quad(n=2)
$$

9.21 Determine whether the following metric is flat and/or Euclidean:

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(x^{3}\right)^{2}\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2}
$$

9.22 Find the signature of the metric for $\mathbf{R}^{3}$ given by

$$
\varepsilon d s^{2}=2\left(d x^{1}\right)^{2}+2\left(d x^{2}\right)^{2}+5\left(d x^{3}\right)^{2}-8 d x^{1} d x^{2}-4 d x^{1} d x^{3}-d x^{2} d x^{3}
$$

9.23 Prove that $R_{i j k}^{i}=0$. [Hint: Use the first of (8.6).]

9.24 Use Problem 9.11 to obtain a simplified proof for Problem 8.34.

9.25 Show that the Einstein invariant, $G \equiv G_{i}^{i}$, vanishes if the space is flat. [Hint: Use Corollary 9.4.]

9.26 In the general theory of relativity one encounters the Schwarzschild metric,

$$
\varepsilon d s^{2}=e^{\varphi}\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left[\left(d x^{2}\right)^{2}+\left(\sin ^{2} x^{2}\right)\left(d x^{3}\right)^{2}\right]-e^{\psi}\left(d x^{4}\right)^{2}
$$

where both $\varphi$ and $\psi$ are functions of $x^{1}$ and $x^{4}$ only. Calculate the nonzero components of the Einstein tensor.

\section*{Chapter 10}
\section*{Tensors in Euclidean Geometry}
\subsection*{10.1 INTRODUCTION}
There exists a starting correlation between formulas of differential geometry, developed to answer questions about curves and surfaces in Euclidean 3-space, and tensor identities previously introduced to handle changes of coordinate systems. Differential geometry was used to great advantage by Einstein in his development of relativity.

The metric will be assumed to be the Euclidean metric, and to emphasize this fact we shall designate the space by $\mathbf{E}^{3}$, which means $\mathbf{R}^{3}$ with the metric

$$
d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2}+\left(d x^{3}\right)^{2}
$$

Moreover, we shall use the familiar notation $(x, y, z)$ in place of $\left(x^{1}, x^{2}, x^{3}\right)$.

\subsection*{10.2 CURVE THEORY; THE MOVING FRAME}
A curve $\mathscr{C}$ in $\mathbf{E}^{3}$ is the image of a class $C^{3}$ mapping, $\mathbf{r}$, from an interval $\mathscr{I}$ of real numbers into $\mathbf{E}^{3}$, as indicated in Fig. 10-1. The image of the real number $t$ in $\mathscr{I}$ will be denoted


\begin{equation*}
\mathbf{r}(t) \equiv(x(t), y(t), z(t)) \tag{10.1}
\end{equation*}


a vector field of class $C^{3}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-136}
\end{center}

Fig. 10-1

\section*{Regular Curves}
The tangent vector of $\mathscr{C}$ is given by


\begin{equation*}
\frac{d \mathbf{r}}{d t} \equiv \dot{\mathbf{r}}=\left(\frac{d x}{d t}, \frac{d y}{d t}, \frac{d z}{d t}\right) \tag{10.2}
\end{equation*}


$\mathscr{C}$ is said to be regular if $\mathbf{r}(t) \neq \mathbf{0}$ for each $t$ in $\mathscr{I}$.

Remark 1: This corresponds to the definition of regularity, given in Section 7.3, in the case of a positive definite metric.

EXAMPLE 10.1 An elliptical helix (Fig. 10-2) is a helix lying on an elliptical cylinder $x^{2} / a^{2}+y^{2} / b^{2}=1$ in $x y z$-space; it is given by $\mathscr{C}: x=a \cos t, \quad y=b \sin t, \quad z=c t$, with $\mathscr{F}$ the entire real line. The pitch is defined as the number $c$. If $a=b$, the helix is called circular, with radius $a$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137}
\end{center}

Fig. 10-2

EXAMPLE 10.2 The space curve $\mathscr{C}: x=t, \quad y=a t^{2}, \quad z=b t^{3} \quad(\mathscr{I}=\mathbf{R})$ captures the salient local features of all curves; it is known as the twisted cubic. As indicated in Fig. 10-3, the projection of $\mathscr{C}$ in the $x y$-plane is a parabola, $y=a x^{2}$; its projection in the $x z$-plane is a standard cubic curve, $z=b x^{3}$; in the $y z$-plane, the semicubical parabola $(y / a)^{3}=(z / b)^{2}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-137(1)}
\end{center}

Fig. 10-3

\section*{Arc Length}
Since the Euclidean metric is positive definite, every regular curve has an arc-length parameterization $\mathbf{r}=\mathbf{r}(s)$, such that


\begin{equation*}
s=\int_{a}^{t}\left\|\frac{d \mathbf{r}}{d u}\right\| d u \quad \text { or } \quad \frac{d s}{d t}=\|\dot{\mathbf{r}}\| \tag{10.3}
\end{equation*}


(The dot, as in $\dot{\mathbf{r}}$, is used to denote differentiation with respect to $t$, and a prime, as in $\mathbf{r}^{\prime}$, denotes differentiation with respect to $s$.) The mapping $t \rightarrow s$ defined by (10.3) has the inverse relation $s \rightarrow t$ given explicitly by $t=\varphi(s)$, where $\varphi$ is also differentiable:


\begin{equation*}
\frac{d t}{d s}=\varphi^{\prime}(s)=\frac{1}{\|\dot{\mathbf{r}}\|} \tag{10.4}
\end{equation*}


\section*{The Moving Frame}
Three vectors of fundamental importance to curve theory will now be discussed. Two of them were introduced in Chapter 7: the unit tangent vector-the (unique) vector

$$
\mathbf{T} \equiv \mathbf{r}^{\prime}=\left(\frac{d x}{d s}, \frac{d y}{d s}, \frac{d z}{d s}\right)
$$

\begin{itemize}
  \item and the unit principal normal-any unit, class $C^{1}$ vector $\mathbf{N}$ that is orthogonal to $\mathbf{T}$ and is parallel to $\mathbf{T}^{\prime}$ wherever $\mathbf{T}^{\prime} \neq \mathbf{0}$. The binormal vector associated with a curve is the unit vector $\mathbf{B} \equiv \mathbf{T} \times \mathbf{N}$ [for the cross product, see (2.10)]; B is uniquely determined once $\mathbf{N}$ has been chosen.
\end{itemize}

Not all regular curves have a principal normal vector (see Problem 10.1). However, it was proved in Problem 7.14 that all planar curves possess a principal normal, of the form

$$
\mathbf{N}=(-\sin \theta, \cos \theta, 0) \quad(\text { plane } z=0)
$$

if $\mathbf{T}=(\cos \theta, \sin \theta, 0)$. The following result provides further information.

Theorem 10.1: Every planar curve has a principal normal vector. If a space curve has a principal normal vector, that vector lies in the plane of the curve for any nonstraight planar segment of the curve. Along any straight-line segment, the principal normal can be chosen as any class $C^{1}$ vector orthogonal to the unit tangent vector.

At each point of $\mathscr{C}$ where $\mathbf{N}$ can be defined, the mutually orthogonal triplet of unit vectors $\mathbf{T}, \mathbf{N}$, B constitutes a right-handed system of basis elements for $\mathbf{E}^{3}$. This triad, which changes continuously along $\mathscr{C}$ (Fig. 10-4), is often called the moving frame or moving triad; the plane of $\mathbf{T}$ and $\mathbf{N}$ is known as the osculating plane.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-138}
\end{center}

Fig. 10-4

The moving frame has been defined for the arc-length parameterization. When it is necessary to use the original parameter $t$ instead, the following expressions may be established (Problem 10.4) for any point at which $\dot{\mathbf{r}} \neq \mathbf{0}$ and $\dot{\mathbf{r}} \times \ddot{\mathbf{r}} \neq \mathbf{0}$ :


\begin{equation*}
\mathbf{T}=\frac{\dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|} \quad \mathbf{N}=\varepsilon \frac{(\dot{\mathbf{r}} \dot{\mathbf{r}}) \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|} \quad \mathbf{B}=\varepsilon \frac{\dot{\mathbf{r}} \times \ddot{\mathbf{r}}}{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|} \tag{10.5}
\end{equation*}


Here, $\varepsilon= \pm 1$, the choice of sign depending on the choice of $\mathbf{N}$ as a class $C^{1}$ vector.

\subsection*{10.3 CURVATURE AND TORSION}
Two important numbers, or more accurately, scalar fields, are associated with space curves.

Definition 1: The curvature $\kappa$ and torsion $\tau$ of a curve $\mathscr{C}: \mathbf{r}=\mathbf{r}(s) \quad$ in $\mathbf{E}^{3}$ are, respectively, the real numbers


\begin{equation*}
\kappa \equiv \mathbf{N T}^{\prime} \quad \text { and } \quad \tau \equiv-\mathbf{N B}^{\prime} \tag{10.6}
\end{equation*}


The sign of $\kappa$ will depend on that chosen for $\mathbf{N}$; however, since $\mathbf{B}$ and $\mathbf{B}^{\prime}$ change in sign together with $\mathbf{N}, \tau$ is uniquely determined.

It foliows (cf. Problem 7.13) that the absolute values of curvature and torsion are given by


\begin{equation*}
\kappa_{0} \equiv|\kappa|=\left\|\mathbf{T}^{\prime}\right\| \quad \text { and } \quad \tau_{0} \equiv|\tau|=\left\|\mathbf{B}^{\prime}\right\| \tag{10.7}
\end{equation*}


Thus, $\kappa_{0}$ measures the absolute rate of change of the unit tangent vector and the amount of "bending" a curve possesses at any given point, while $\tau_{0}$ measures the absolute rate of change of the binormal and the tendency of the curve to "twist" out of its osculating plane at each point. The significance of negative values for $\kappa$ and $\tau$ will become apparent later.

Remark 2: It can be shown that the two functions $\kappa=\kappa(s)$ and $\tau=\tau(s)$ determine the curve $\mathscr{C}$ up to a rigid motion in $\mathbf{E}^{3}$.

In the $t$-parameterization of $\mathscr{C}$, we have (Problem 10.7):


\begin{equation*}
\kappa=\frac{\varepsilon\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|}{\|\dot{\mathbf{r}}\|^{3}} \quad \text { and } \quad \tau=\frac{\operatorname{det}[\dot{\mathbf{r}} \ddot{\mathbf{r}} \ddot{\mathbf{r}}]}{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2}} \tag{10.8}
\end{equation*}


where $\varepsilon= \pm 1$ and $[\dot{\mathbf{r}} \ddot{\mathbf{r}} \ddot{\mathbf{r}}]$ represents the $3 \times 3$ matrix having as row vectors $\dot{\mathbf{r}}, \ddot{\mathbf{r}}$, and $\ddot{\mathbf{r}}$. [Recall the identity

$$
\mathbf{a} \cdot(\mathbf{b} \times \mathbf{c})=\operatorname{det}[\mathbf{a} \mathbf{b} \mathbf{c}]
$$

for the triple scalar product of three vectors.]

\section*{Serret-Frenet Formulas}
The derivatives of the vectors composing the moving triad are given by

\[
\left.\begin{array}{l}
\mathbf{T}^{\prime}=\kappa \mathbf{N}  \tag{10.9}\\
\mathbf{N}^{\prime}=-\kappa \mathbf{T}+\tau \mathbf{B} \quad \text { or } \quad\left[\begin{array}{l}
\mathbf{T} \\
\mathbf{N} \\
\mathbf{B}
\end{array} \mathbf{B}^{\prime}=-\tau \mathbf{N}\right.
\end{array}\right]^{\prime}=\left[\begin{array}{rrr}
0 & \kappa & 0 \\
-\kappa & 0 & \tau \\
0 & -\tau & 0
\end{array}\right]\left[\begin{array}{l}
\mathbf{T} \\
\mathbf{N} \\
\mathbf{B}
\end{array}\right]
\]

Note the skew-symmetry of the coefficient matrix. The first of these formulas was established in Problem 7.13; the other two are derived in Problem 10.8.

\subsection*{10.4 REGULAR SURFACES}
Surfaces are generally encountered in the calculus in the form $z=F(x, y)$; that is, as graphs of two-variable functions in three-dimensional space. Here, however, it is more convenient to adopt the

Definition 2: A surface $\mathscr{S}$ in $\mathbf{E}^{2}$ is the image of a $C^{3}$ vector function,

$$
\mathbf{r}\left(x^{1}, x^{2}\right)=\left(f\left(x^{1}, x^{2}\right), g\left(x^{1}, x^{2}\right), h\left(x^{1}, x^{2}\right)\right)
$$

which maps some region $\mathscr{V}$ of $\mathbf{E}^{2}$ into $\mathbf{E}^{3}$.

(See Fig. 10-5; in general, primes will designate objects in the parameter plane ( $x^{i}$ ) corresponding to those on the surface in $x y z$-space.) The coordinate breakdown of the mapping $\mathbf{r}$,


\begin{equation*}
x=f\left(x^{1}, x^{2}\right) \quad y=g\left(x^{1}, x^{2}\right) \quad z=h\left(x^{1}, x^{2}\right) \tag{10.10}
\end{equation*}


is called the Gaussian form or representation of $\mathscr{S}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-140}
\end{center}

Fig. 10-5

Point $P$ is a regular point of $\mathscr{S}$ if

\[
\frac{\partial \mathbf{r}}{\partial x^{1}} \times \frac{\partial \mathbf{r}}{\partial x^{2}} \equiv\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k}  \tag{10.11}\\
\frac{\partial f}{\partial x^{1}} & \frac{\partial g}{\partial x^{1}} & \frac{\partial h}{\partial x^{1}} \\
\frac{\partial f}{\partial x^{2}} & \frac{\partial g}{\partial x^{2}} & \frac{\partial h}{\partial x^{2}}
\end{array}\right| \neq \mathbf{0}
\]

at $P^{\prime}$; otherwise, $P$ is a singular point. If every point of $\mathscr{S}$ is a regular point, then $\mathscr{S}$ is a regular surface.

Remark 3: Condition (10.11) is tantamount to the linear independence of the two vectors $\left(\partial \mathbf{r} / \partial x^{1}\right)_{P}$ and $\left(\partial \mathbf{r} / \partial x^{2}\right)_{P}$. Equivalently, and of more geometrical interest, the condition ensures that every curve in $\mathscr{S}$ through $P$ which we take to be the image under $\mathbf{r}$ of a regular curve in $\mathscr{V}$ through $P^{\prime}$, is, in a neighborhood of $P$, regular in the sense of Section 10.2 .

EXAMPLE 10.3 For a $C^{3}$ function $F$, show that the graph $z=F(x, y)$ is a regular surface.

The surface has the Gaussian representation

$$
x=x^{1} \quad y=x^{2} \quad z=F\left(x^{1}, x^{2}\right)
$$

and thus

$$
\frac{\partial \mathbf{r}}{\partial x^{1}} \times \frac{\partial \mathbf{r}}{\partial x^{2}}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
1 & 0 & \partial F / \partial x^{1} \\
0 & 1 & \partial F / \partial x^{2}
\end{array}\right|=\left(-\frac{\partial F}{\partial x^{1}},-\frac{\partial F}{\partial x^{2}}, 1\right) \neq \mathbf{0}
$$

at an arbitrary surface point $P$. (This would be true if $F$ were merely class $C^{1}$.)

\section*{Subscript Notation for Partial Derivatives}
From now on, write

$$
\frac{\partial \mathbf{r}}{\partial x^{1}} \equiv \mathbf{r}_{1} \quad \frac{\partial \mathbf{r}}{\partial x^{2}} \equiv \mathbf{r}_{2} \quad \frac{\partial^{2} \mathbf{r}}{\partial x^{1} \partial x^{1}}=\mathbf{r}_{11} \quad \text { etc. }
$$

so that, e.g., (10.11) takes the compact form $\mathbf{r}_{1} \times \mathbf{r}_{2} \neq \mathbf{0}$.

\subsection*{10.5 PARAMETRIC LINES; TANGENT SPACE}
Let $\left(x^{i}\right)$ be taken as coordinates-for the moment, rectangular coordinates-in the parameter plane $\mathbf{E}^{2}$, yielding two (orthogonal) families of coordinate lines:

$$
\left\{\begin{array} { l } 
{ x ^ { 1 } = t } \\
{ x ^ { 2 } = d }
\end{array} \quad \text { and } \quad \left\{\begin{array}{l}
x^{1}=c \\
x^{2}=\sigma
\end{array}\right.\right.
$$

If $(c, d)$ runs over $\mathscr{V}$ (the pre-image of surface $\mathscr{S}$ ), then the images under $\mathbf{r}$ of these two families are the two sets of parametric lines (or coordinate curves) on $\mathscr{S}$ :

$$
\underbrace{\mathbf{r}=\mathbf{r}(t, d) \equiv \mathbf{p}(t)}_{\boldsymbol{x}^{1} \text {-curves }} \quad \underbrace{\mathbf{r}=\mathbf{r}(c, \sigma) \equiv \mathbf{q}(\sigma)}_{x^{2} \text {-curves }}
$$

Figure 10-6 suggests that the net of parametric lines is orthogonal also. This is not, of course, true in general. In fact, since the tangent fields to the $x^{1}$-curves and the $x^{2}$-curves are respectively $d \mathbf{p} / d t=\mathbf{r}_{1}$ and $d \mathbf{q} / d \sigma=\mathbf{r}_{2}$, the net is orthogonal if and only if $\mathbf{r}_{1} \mathbf{r}_{2}=0$ at every point of $\mathscr{S}$.\\
\includegraphics[max width=\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-141}

Fig. 10-6

For surface curves in general, the tangent vector of a curve passing through $\mathbf{r}(c, d)$ is a linear combination of the vectors $\mathbf{r}_{1}$ and $\mathbf{r}_{2}$, as the following analysis shows. Let the curve be given in the parameter plane as $\mathscr{C}^{\prime}: x^{1}=x^{1}(t), x^{2}=x^{2}(t)$; then the corresponding curve on the surface is

$$
\mathscr{C}: \mathbf{r}=\mathbf{r}\left(x^{1}(t), x^{2}(t)\right) \equiv \mathbf{r}(t)
$$

with tangent vector


\begin{equation*}
\dot{\mathbf{r}}=\frac{\partial \mathbf{r}}{\partial x^{1}} \frac{d x^{1}}{d t}+\frac{\partial \mathbf{r}}{\partial x^{2}} \frac{d x^{2}}{d t} \equiv u^{1} \mathbf{r}_{1}+u^{2} \mathbf{r}_{2} \equiv u^{i} \mathbf{r}_{i} \tag{10.12}
\end{equation*}


Here, $u^{1} \equiv d x^{1} / d t, u^{2} \equiv d x^{2} / d t$, so that the vector $\left(u^{i}\right)$ in the parameter plane is the tangent to $\mathscr{C}^{\prime}$ at $P^{\prime}$ (see Fig. 10-6).

Definition 3: The collection of linear combinations of the vectors $\mathbf{r}_{1}(P)$ and $\mathbf{r}_{2}(P)$ is called the tangent space of $\mathscr{S}$ at $P$. The unit surface normal is the unit vector $\mathbf{n}$ in the direction of $\mathbf{r}_{1} \times \mathbf{r}_{2}$ :


\begin{equation*}
\mathbf{n}=\frac{1}{E}\left(\mathbf{r}_{1} \times \mathbf{r}_{2}\right) \quad\left(E \equiv\left\|\mathbf{r}_{1} \times \mathbf{r}_{2}\right\|>0\right) \tag{10.13}
\end{equation*}


The geometric realization of the tangent space is obviously the tangent plane at $P$, and the surface normal can be identified with a line segment through $P$ perpendicular to this tangent plane; that is, orthogonal to the surface at $P$, as indicated in Fig. 10-6.

To summarize this whole affair, the linearly independent (by regularity) triad of vectors $\mathbf{r}_{1}, \mathbf{r}_{2}, \mathbf{n}$ forms a moving frame for the surface, as shown in Fig. 10-7, much in the manner that a moving triad exists for a regular curve having a principal normal.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-142}
\end{center}

Fig. 10-7

\subsection*{10.6 FIRST FUNDAMENTAL FORM}
Consider a curve on the regular surface $\mathscr{S}: \mathbf{r}=\mathbf{r}\left(x^{1}, x^{2}\right)$ given by $\mathscr{C}: \mathbf{r}=\mathbf{r}\left(x^{1}(t), x^{2}(t)\right) \equiv \mathbf{r}(t)$, with pre-image $\mathscr{C}^{\prime}: x^{i}=x^{i}(t)$ in the parameter plane. Using (10.12) and recalling that the (Euclidean) inner product is distributive over linear combinations of vectors, arc length along $\mathscr{C}$ is calculated as


\begin{equation*}
\left(\frac{d s}{d t}\right)^{2}=\|\dot{\mathbf{r}}\|^{2}=\dot{\mathbf{r}} \dot{\mathbf{r}}=\left(u^{i} \mathbf{r}_{i}\right)\left(u^{j} \mathbf{r}_{j}\right) \equiv g_{i j} u^{i} u^{j} \tag{10.14a}
\end{equation*}


in which


\begin{equation*}
g_{i j}=\mathbf{r}_{i} \mathbf{r}_{j} \quad(1 \leqq i, j \leqq 2) \tag{10.15}
\end{equation*}


and, as above, $u^{i}=d x^{i} / d t$. In the equivalent differential form,


\begin{equation*}
d s^{2}=g_{i j} d x^{i} d x^{j} \equiv \mathrm{I} \tag{10.14b}
\end{equation*}


the arc-length formula is known as the First Fundamental Form (abbreviated FFF) of the surface $\mathscr{S}$. In view of (10.12) and the regularity of $\mathscr{S},\|\dot{\mathbf{r}}\|=0$ if and only if $u^{1}=u^{2}=0$; this proves

Lemma 10.2: The FFF of a regular surface is positive definite.

Lemma 10.2 implies that $g \equiv \operatorname{det}\left(g_{i j}\right)>0$; in fact, we can use Lagrange's identity,

$$
\left(\mathbf{r}_{1} \times \mathbf{r}_{2}\right)^{2}=\left(\mathbf{r}_{1}^{2}\right)\left(\mathbf{r}_{2}^{2}\right)-\left(\mathbf{r}_{1} \mathbf{r}_{2}\right)^{2}
$$

to establish that


\begin{equation*}
g=E^{2} \tag{10.16}
\end{equation*}


cf. (10.13).

EXAMPLE 10.4 Compute the FFF for the right helicoid (Fig. 10-8),

$$
\mathbf{r}=\left(x^{1} \cos x^{2}, x^{1} \sin x^{2}, a x^{2}\right)
$$

We have:

whence

$$
\mathbf{r}_{1}=\left(\cos x^{2}, \sin x^{2}, 0\right) \quad \mathbf{r}_{2}=\left(-x^{1} \sin x^{2}, x^{1} \cos x^{2}, a\right)
$$

whence

$$
g_{11}=\mathbf{r}_{1}^{2}=\cos ^{2} x^{2}+\sin ^{2} x^{2}+0^{2}=1
$$

$$
\begin{aligned}
& g_{12}=\mathbf{r}_{1} \mathbf{r}_{2}=\left(\cos x^{2}\right)\left(-x^{1} \sin x^{2}\right)+\left(\sin x^{2}\right)\left(x^{1} \cos x^{2}\right)=0 \\
& g_{22}=\mathbf{r}_{2}^{2}=\left(-x^{1}\right)^{2}\left(\sin ^{2} x^{2}\right)+\left(x^{1}\right)^{2}\left(\cos ^{2} x^{2}\right)+a^{2}=\left(x^{1}\right)^{2}+a^{2}
\end{aligned}
$$

and

$$
\mathbf{I}=\left(d x^{1}\right)^{2}+\left[\left(x^{1}\right)^{2}+a^{2}\right]\left(d x^{2}\right)^{2}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-143}
\end{center}

Fig. 10-8

Along with the FFF, tensor calculus enters the picture. For the intrinsic properties of a particular surface $\mathscr{S}$ in $\mathbf{E}^{3}$ (the properties defined by measurements of distance on the surface) are all implicit in $(10.14 b)$, which can be interpreted as a particular Riemannian metrization of the parameter plane. Thus, the study of intrinsic properties of surfaces becomes the tensor analysis of Riemannian metrics in $\mathbf{R}^{2}$ - and this may be conducted without any reference to $\mathbf{E}^{3}$ whatever. Observe that the metrics under consideration will all be positive definite (Lemma 10.2) but not necessarily Euclidean (see Theorem 9.1). Accordingly, we shall drop the designation $\mathbf{E}^{2}$ for the parameter plane, which shall henceforth be referred to general coordinates $\left(x^{i}\right)$.

EXAMPLE 10.5 The metric for $\mathbf{R}^{2}$ corresponding to the right helicoid (Example 10.4) is non-Euclidean, as is demonstrated in Problem 10.27. Now the parameters $x^{1}$ and $x^{2}$, which are actual polar coordinates in the $x y$-plane of $\mathbf{E}^{3}$ (see Fig. 10-8), formally keep that significance when the plane is considered abstractly as parameter space. This is an instance of the formal use of a familiar coordinate system in a non-Euclidean space, as mentioned in Section 3.1.

\section*{Unit Tangent Vector}
If $\mathscr{C}: \mathbf{r}=\mathbf{r}\left(x^{1}(t), x^{2}(t)\right)$ is any curve on $\mathscr{S}$, then by (10.12) and (10.14a),


\begin{equation*}
\mathbf{T}=\frac{\dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|}=\frac{u^{i} \mathbf{r}_{i}}{\sqrt{g_{j k} u^{j} u^{k}}} \tag{10.17}
\end{equation*}


\section*{Angle Between Two Curves}
Let $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$ be two intersecting curves on $\mathscr{S}$ that correspond to $x^{i}=\phi^{i}(t)$ and $x^{i}=\psi^{i}(\sigma) \quad(i=$ $1,2)$ in the parameter plane. Writing $u^{i} \equiv d \phi^{i} / d t$ and $v^{i} \equiv d \psi^{i} / d \sigma$, we have for the angle $\theta$ between $\mathbf{T}_{1}$ of $\mathscr{C}_{1}$ and $\mathbf{T}_{2}$ of $\mathscr{C}_{2}$ :


\begin{equation*}
\cos \theta=\mathbf{T}_{1} \mathbf{T}_{2}=\frac{u^{i} \mathbf{r}_{i}}{\sqrt{g_{p q} u^{p} u^{q}}} \cdot \frac{v^{j} \mathbf{r}_{j}}{\sqrt{g_{r s} v^{r} v^{s}}}=\frac{g_{i j} u^{i} v^{j}}{\sqrt{g_{p q} u^{p} u^{q}} \sqrt{g_{r s} v^{r} v^{s}}} \tag{10.18}
\end{equation*}


Compare (5.11).

Theorem 10.3: The angle between the two parametric lines through a surface point is


\begin{equation*}
\cos \theta=\frac{g_{12}}{\sqrt{g_{11}} \sqrt{g_{22}}} \tag{10.19}
\end{equation*}


Corollary 10.4: The two families of parametric lines form an orthogonal net if and only if $g_{12}=0$ at every point of $\mathscr{S}$.

\subsection*{10.7 GEODESICS ON A SURFACE}
A further link with tensors is provided by the concept of geodesics for regular surfaces. One can intuitively imagine stretching a string between two points on a surface, and pulling it tight: on a sphere this would lead to a great circular arc, and on a right circular cylinder, a helical arc. Since from our point of view the surface is disregarded and $\left(g_{i j}\right)$ is taken as a metric for the parameter plane, the problem has already been worked out (Section 7.6).

Relative to the FFF of $\mathscr{S}$, define the Christoffel symbols through formulas (6.1) and (6.4), $n=2$. [Problem 10.48 gives an equivalent "extrinsic" definition, in terms of the vector $\mathbf{r}$.] Then a geodesic on $\mathscr{S}$ is any curve $\mathbf{r}=\mathbf{r}\left(x^{1}(t), x^{2}(t)\right)$ in the surface whose pre-image in the parameter $\mathbf{R}^{2}$ satisfies the system of differential equations (7.11)-(7.12); if $t=s=$ arc length, the governing system is (7.13). [Remember that the (non-Euclidean) distance measured by $s$ in $\mathbf{R}^{2}$ is the Euclidean distance along the geodesic as a curve in $\mathbf{E}^{3}$.]

Similarly, harking back to Section 6.5, the intrinsic curvature of a curve $\mathscr{C}$ in $\mathscr{S}$ is the function


\begin{equation*}
\tilde{\kappa}(s)=\sqrt{g_{i j} b^{i} b^{j}} \tag{10.20}
\end{equation*}


-cf. (6.12)-where the intrinsic curvature vector ( $b^{i}$ ) (in $\mathbf{R}^{2}$ ) is given by (6.11).

Remark 4: Intrinsic curvature can be shown to be the instantaneous rate of change of the angle between the tangent vector of $\mathscr{C}$ and another vector in the tangent space that is "transported parallelly" along the curve. Here, the term "parallel" refers to a certain generalization of Euclidean parallelism (see Problem 10.22).

Theorem 10.5: A curve on a surface is a geodesic if and only if its intrinsic curvature $\tilde{\kappa}$ is identically zero.

In contrast to the above intrinsic characterization of geodesics, there is an interesting and useful extrinsic characterization, proved as Problem 10.18. It adds a visual dimension that often allows the immediate identification of a geodesic.

Theorem 10.6: A curve on a regular surface is a geodesic if and only if a principal normal $\mathbf{N}$ of the curve can be chosen that coincides with the surface normal $\mathbf{n}$ at all points along the curve.

\subsection*{10.8 SECOND FUNDAMENTAL FORM}
By taking the dot product of the surface normal with the second partial derivatives of $\mathbf{r}$ with respect to $x^{1}$ and $x^{2}$,


\begin{equation*}
f_{i j} \equiv \mathbf{n} \mathbf{r}_{i j} \tag{10.21}
\end{equation*}


we generate the coefficients of the Second Fundamental Form (SFF) of a surface:


\begin{equation*}
f_{i j} d x^{i} d x^{j} \equiv \mathrm{II} \tag{10.22}
\end{equation*}


\section*{Curvature of a Normal Section}
If $\mathscr{F}$ is a plane containing the surface normal $\mathbf{n}$ at some point $P$ of $\mathscr{S}$ (Fig. 10-9), the curvature of the normal section of $\mathscr{S}$ (the curve of intersection of $\mathscr{S}$ and $\mathscr{F}$ ), denoted $\mathscr{C}_{\mathscr{F}}$, is given at point $P$ by the formula


\begin{equation*}
\kappa_{\mathscr{F}}=\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}=\frac{\mathrm{II}}{\mathrm{I}} \tag{10.23}
\end{equation*}


where $\left(u^{i}\right)=\left(d x^{i} / d t\right)$ gives the direction, at $P^{\prime}$, of the curve corresponding to $C_{\mathscr{F}}$ in the parameter plane; see Problem 10.23.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-145}
\end{center}

Fig. 10-9

As $\mathscr{F}$ rotates about $\mathbf{n}$, the curvature $\kappa_{\mathscr{F}}$ of $\mathscr{C}_{\mathscr{F}}$ at $P$ is periodic and will reach an absolute maximum and an absolute minimum; let


\begin{equation*}
\max \kappa_{\mathscr{F}} \equiv \kappa_{1} \quad \min \kappa_{\mathscr{F}} \equiv \kappa_{2} \tag{10.24}
\end{equation*}


The two section curves having these two extremal curvatures are called principal curves, and their directions are the principal directions. If $\kappa_{1}=\kappa_{2}$ at $P$, all the normal sections at $P$ have the same curvature and unique principal directions do not exist. (In this case, $P$ is called an umbilical point of the surface.)

\section*{Surface Curvature}
Two measures of the curvature of a surface $\mathscr{S}$ are commonly used.

Definition 4: The Gaussian curvature of $\mathscr{S}$ at point $P$ is the number $\mathrm{K}=\kappa_{1} \kappa_{2}$; the mean curvature is the number $\mathrm{H}=\kappa_{1}+\kappa_{2}$.

It will be proved in Problem 10.25 that the extreme curvatures $\kappa_{1}$ and $\kappa_{2}$ are the roots of the following quadratic equation in $\lambda$ :


\begin{equation*}
\left(g_{11} g_{22}-g_{12}^{2}\right) \lambda^{2}-\left(f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}\right) \lambda+\left(f_{11} f_{22}-f_{12}^{2}\right)=0 \tag{10.25}
\end{equation*}


The relations between the roots and the coefficients of a polynomial equation then give:


\begin{equation*}
\mathbf{K}=\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}} \quad \mathbf{H}=\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}} \tag{10.26}
\end{equation*}


\subsection*{10.9 STRUCTURE FORMULAS FOR SURFACES}
Two fundamental sets of relationships involve the parts of the moving triad of a surface, $\left(\mathbf{r}_{1}, \mathbf{r}_{2}, \mathbf{n}\right)$.

\section*{Equations of Weingarten}
Since $\mathbf{n}^{2}=1, \partial\left(\mathbf{n}^{2}\right) / \partial x^{i}=2 \mathbf{n n}_{i}=0 \quad(i=1,2)$. Hence, for each $i, \mathbf{n}_{i}$ lies in the tangent space: $\mathbf{n}_{i}=u_{i}^{1} \mathbf{r}_{1}+u_{i}^{2} \mathbf{r}_{2}$, for certain scalars $u_{i}^{k}$. Similarly, from orthogonality,

$$
0=\left(\mathbf{n r}_{i}\right)_{j}=\mathbf{n}_{j} \mathbf{r}_{i}+\mathbf{n} \mathbf{r}_{i j} \quad \text { or } \quad \mathbf{n}_{j} \mathbf{r}_{i}=-f_{i j}
$$

It follows (Problem 10.28) that


\begin{equation*}
\mathbf{n}_{i}=-g^{j k} f_{i j} \mathbf{r}_{k} \tag{10.27a}
\end{equation*}


for $i=1,2$. From the explicit form of the inverse metric matrix $\left(g^{i j}\right),(10.27 a)$ may be spelled out as follows:


\begin{align*}
& \mathbf{n}_{1}=\frac{g_{12} f_{12}-g_{22} f_{11}}{g} \mathbf{r}_{1}+\frac{g_{12} f_{11}-g_{11} f_{12}}{g} \mathbf{r}_{2} \\
& \mathbf{n}_{2}=\frac{g_{12} f_{22}-g_{22} f_{12}}{g} \mathbf{r}_{1}+\frac{g_{12} f_{12}-g_{11} f_{22}}{g} \mathbf{r}_{2} \tag{10.27b}
\end{align*}


\section*{Equations of Gauss}
Since $\left(\mathbf{r}_{1}, \mathbf{r}_{2}, \mathbf{n}\right)$ is a basis for $\mathbf{E}^{3}$, we can write $\mathbf{r}_{i j}=u_{i j}^{1} \mathbf{r}_{1}+u_{i j}^{2} \mathbf{r}_{2}+u_{i j}^{3} \mathbf{n}$. Evaluation of the coefficients (Problem 10.29) leads to


\begin{equation*}
\mathbf{r}_{i j}=\Gamma_{i j}^{k} \mathbf{r}_{k}+f_{i j} \mathbf{n} \tag{10.28}
\end{equation*}


\section*{An Identity Between FFF and SFF}
Since $\mathbf{r}_{i j k}=\mathbf{r}_{i k j}$, (10.28) implies $\left(\Gamma_{i j}^{s} \mathbf{r}_{s}+f_{i j} \mathbf{n}\right)_{k}=\left(\Gamma_{i k}^{s} \mathbf{r}_{s}+f_{i k} \mathbf{n}\right)_{j}$, or

$$
\left(\Gamma_{i j}^{s}\right)_{k} \mathbf{r}_{s}+\Gamma_{i j}^{s} \mathbf{r}_{s k}+f_{i j k} \mathbf{n}+f_{i j} \mathbf{n}_{k}=\left(\Gamma_{i k}^{s}\right)_{j} \mathbf{r}_{s}+\Gamma_{i k}^{s} \mathbf{r}_{s j}+f_{i k j} \mathbf{n}+f_{i k} \mathbf{n}_{j}
$$

Dot both sides with $\mathbf{r}_{l}$ and use the definition $\mathbf{r}_{l} \mathbf{r}_{s} \equiv g_{l s}$ and the relations $\mathbf{r}_{l} \mathbf{r}_{s k}=\Gamma_{s k l}$ (Problem 10.48) and $\mathbf{r}_{l} \mathbf{n}=0$ :

$$
\left(\Gamma_{i j}^{s}\right)_{k} g_{s l}+\Gamma_{i j}^{s} \Gamma_{s k l}+f_{i j} \mathbf{n}_{k} \mathbf{r}_{l}=\left(\Gamma_{i k}^{s}\right)_{j} g_{s l}+\Gamma_{i k}^{s} \Gamma_{s j l}+f_{i k} \mathbf{n}_{j} \mathbf{r}_{l}
$$

Now substitute for the $\mathbf{n}_{i}$ from (10.27a) and use $\mathbf{r}_{t} \mathbf{r}_{l} \equiv g_{t l}$ and $g^{s t} g_{t l}=\delta_{l}^{s}$ to simplify the result:

$$
-f_{i j} f_{k l}+f_{i k} f_{j l}=g_{s l}\left(\frac{\partial \Gamma_{i k}^{s}}{\partial x^{j}}-\frac{\partial \Gamma_{i j}^{s}}{\partial x^{k}}+\Gamma_{i k}^{r} \Gamma_{r j}^{s}-\Gamma_{i j}^{r} \dot{\Gamma}_{r k}^{s}\right)
$$

Finally, introducing the Riemann tensor via (8.2) and (8.3), we obtain


\begin{equation*}
R_{i i k l}=f_{i k} f_{i l}-f_{i l} f_{i k} \tag{10.29}
\end{equation*}


The left member of (10.29) depends only on the coefficients of I together with their first and second derivatives; the right member depends only on the coefficients of II. This essential compatibility relation between the two fundamental forms must hold at every point of a regular surface.

\section*{The 'Most Excellent Theorem' of Gauss}
By (10.26) and (10.29),


\begin{equation*}
\mathrm{K}=\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\frac{R_{1212}}{g} \tag{10.30}
\end{equation*}


Thus, the numerator of $\mathrm{K}$ can be derived entirely from the FFF. Since the denominator is also obviously from the FFF, we have:

Theorem 10.7 (Theorema Egregium): The Gaussian curvature is an intrinsic property of a surface, depending only on the First Fundamental Form and its derivatives.

Remark 5: The motive for the definition (8.7) of Riemannian curvature is now apparent.

\subsection*{10.10 ISOMETRIES}
The practical question of whether inhabitants of a fog-enshrouded planet could, solely by measuring distances on the surface of the planet, determine its curvature, is answered in the affirmative by Theorem 10.7. A further important conclusion can be drawn.

Suppose that two surfaces, $\mathscr{S}^{(1)}: \mathbf{r}^{(1)}=\mathbf{r}^{(1)}\left(x^{1}, x^{2}\right)$ and $\mathscr{P}^{(2)}: \mathbf{r}^{(2)}=\mathbf{r}^{(2)}\left(x^{1}, x^{2}\right)$, are defined over the same region $\mathscr{V}$ of the plane and that the First Fundamental Forms agree on $\mathscr{V}$. This will obviously set up a correspondence between $\mathscr{S}^{(1)}$ and $\mathscr{S}^{(2)}$ in $\mathbf{E}^{3}$ that is bijective between small patches (induced by the neighborhoods of $\mathscr{V}$ over which both $\mathbf{r}^{(i)}$ are bijective) of the two surfaces. This correspondence is called a local isometry between $\mathscr{S}^{(1)}$ and $\mathscr{S}^{(2)}$ because the two surfaces are, patch for patch, metrically identical. But then (Theorem 10.7) the Gaussian curvatures $K^{(1)}$ and $K^{(2)}$ must be equal at corresponding points.

Theorem 10.8: If two surfaces are locally isometric, their Gaussian curvatures are identical.

In the case of constant Gaussian curvature K, Beltrami's theorem tells us that there is a parameterization for $\mathscr{S}$ for which the FFF takes on the form:

$$
\begin{array}{ll}
d s^{2}=a^{2}\left(d x^{1}\right)^{2}+\left(a^{2} \sinh ^{2} x^{1}\right)\left(d x^{2}\right)^{2} & \text { if } \mathbf{K}=-1 / a^{2}<0 \\
d s^{2}=\left(d x^{1}\right)^{2}+\left(d x^{2}\right)^{2} & \text { if } \mathbf{K}=0 \\
d s^{2}=a^{2}\left(d x^{1}\right)^{2}+\left(a^{2} \sin ^{2} x^{1}\right)\left(d x^{2}\right)^{2} & \text { if } \mathbf{K}=1 / a^{2}>0
\end{array}
$$

EXAMPLE 10.6 The plane and the sphere are surfaces of constant zero curvature and constant positive curvature, respectively. For a surface of constant negative curvature, see Problem 10.49.

Beltrami's theorem implies a partial converse of Theorem 10.8:

Theorem 10.9 (Minding's Theorem): If two surfaces are of the same constant Gaussian curvature, they are locally isometric.

Remark 6: A proof of Theorem 10.9 for zero curvature was given in Problem 9.9.

\section*{Solved Problems}
\section*{CURVE THEORY; THE MOVING FRAME}
10.1 The curve

$$
\mathscr{C}:\left\{\begin{array} { l } 
{ x = t } \\
{ y = t ^ { 4 } } \\
{ z = 0 }
\end{array} \quad ( t < 0 ) \quad \left\{\begin{array}{l}
x=t \\
y=0 \\
z=t^{4}
\end{array} \quad(t \geqq 0)\right.\right.
$$

lies partly in the $x y$-plane and partly in the $x z$-plane (Fig. 10-10). Show that it is regular of class $C^{3}$, but that it possesses no principal normal vector.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-148}
\end{center}

Fig. 10-10

The component functions for $\mathbf{r}(t)$ are

$$
x(t)=t \quad y(t)=\left\{\begin{array}{ll}
t^{4} & t<0 \\
0 & t \geqq 0
\end{array} \quad z(t)= \begin{cases}0 & t<0 \\
t^{4} & t \geqq 0\end{cases}\right.
$$

When $t<0, \dot{y}(t)=4 t^{3}$. As $t \rightarrow 0$,

$$
\lim _{t \rightarrow-0} \frac{y(t)-y(0)}{t-0}=\lim _{t \rightarrow-0} \frac{t^{4}}{t}=0 \quad \lim _{t \rightarrow+0} \frac{y(t)-y(0)}{t-0}=\lim _{t \rightarrow+0} \frac{0}{t}=0
$$

hence, $y(t)$ is differentiable at $t=0$. Clearly, $\dot{y}(t)=0$ for $t>0$. A similar analysis applies to $z(t)$. Hence:

$$
\dot{y}(t)=\left\{\begin{array}{ll}
4 t^{3} & t<0 \\
0 & t \geqq 0
\end{array} \quad \dot{z}(t)= \begin{cases}0 & t<0 \\
4 t^{3} & t \geqq 0\end{cases}\right.
$$

which are continuous functions. Continuing the analysis up to the third derivatives:

$$
\dddot{y}(t)=\left\{\begin{array}{ll}
24 t & t<0 \\
0 & t \geqq 0
\end{array} \quad \dddot{z}(t)= \begin{cases}0 & t<0 \\
24 t & t \geqq 0\end{cases}\right.
$$

Hence, $x(t)$ being differentiable to all orders, $\mathbf{r}(t)$ is of class $C^{3}$. Furthermore, because $\dot{x}(t) \equiv 1, \dot{\mathbf{r}}(t) \neq 0$ for all $t$ and $\mathscr{C}$ is regular. However, the principal normal, which exists for the separate parts of $\mathscr{C}$ (lying in the $x y$-plane for $t<0$ and in the $x z$-plane for $t>0$ ), cannot possibly be continuous at $t=0$, let alone differentiable. Hence, $\mathscr{C}$ does not possess a principal normal.

10.2 (a) Describe the curve $\mathbf{r}=\left(\cos t, \sin t, \tan ^{-1} t\right)$, where $0 \leqq t$ and where the principal value of the arctangent is understood. (b) Find the arc length between the points $\mathbf{r}(0)$ and $\mathbf{r}(1)$.\\
(a) This is a form of the circular helix, except that the pitch decreases with increasing $t$. The curve lies on the right circular cylinder $x^{2}+y^{2}=1$; beginning at $(1,0,0)$, it winds around the cylinder and approaches the circle $x^{2}+y^{2}=1, z=\pi / 2$ asymptotically as $t \rightarrow \infty$.

(b)

$$
\dot{\mathbf{r}}=\left(-\sin t, \cos t, \frac{1}{t^{2}+1}\right) \quad \text { or } \quad \frac{d s}{d t}=\sqrt{\sin ^{2} t+\cos ^{2} t+\frac{1}{\left(t^{2}+1\right)^{2}}}
$$

A numerical method of integration is required. Using Simpson's rule on a programmable calculator, one obtains

$$
L=\int_{0}^{1} \frac{\sqrt{t^{4}+2 t^{2}+2}}{t^{2}+1} d t \approx 1.27797806
$$

10.3 Find the moving frame for the curve

$$
\mathscr{C}: \mathbf{r}=\left(\frac{3-3 t^{3}}{5}, \frac{4+4 t^{3}}{5}, 3 t\right) \quad(t \text { real })
$$

Show that the binormal vector $\mathbf{B}$ is constant, so that the curve is actually planar.

Making the calculations required in (10.5):

and

$$
\dot{\mathbf{r}}=\left(\frac{-9 t^{2}}{5}, \frac{12 t^{2}}{5}, 3\right) \quad\|\dot{\mathbf{r}}\|=\sqrt{\frac{81}{25} t^{4}+\frac{144}{25} t^{4}+9}=3 \sqrt{t^{4}+1}
$$

$$
\mathbf{T}=\frac{\left(-9 t^{2} / 5,12 t^{2} / 5,3\right)}{3 \sqrt{t^{4}+1}}=\frac{\left(-3 t^{2}, 4 t^{2}, 5\right)}{5 \sqrt{t^{4}+1}}
$$

$$
\begin{aligned}
& \ddot{\mathbf{r}}=\left(\frac{-18 t}{5}, \frac{24 t}{5}, 0\right) \\
& \dot{\mathbf{r}} \times \ddot{\mathbf{r}}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
-\frac{9}{5} t^{2} & \frac{12}{5} t^{2} & 3 \\
-\frac{18 t}{5} & \frac{24 t}{5} & 0
\end{array}\right|=\left(-\frac{72 t}{5},-\frac{54 t}{5}, 0\right)=\frac{-18 t}{5}(4,3,0) \\
& \|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|=\frac{18|t|}{5} \sqrt{4^{2}+3^{2}+0^{2}}=18|t| \\
& (\dot{\mathbf{r}} \dot{\mathbf{r}}) \ddot{\mathbf{r}}=\left(9 t^{4}+9\right)\left(-\frac{18}{5} t, \frac{24}{5} t, 0\right)=\left(-\frac{162}{5} t^{5}-\frac{162}{5} t, \frac{216}{5} t^{5}+\frac{216}{5} t, 0\right) \\
& (\ddot{\mathbf{r}}) \dot{\mathbf{r}}=\left(\frac{9 \cdot 18}{25} t^{3}+\frac{24 \cdot 12}{25} t^{3}+0\right)\left(-\frac{9}{5} t^{2}, \frac{12}{5} t^{2}, 3\right)=\left(-\frac{162}{5} t^{5}, \frac{216}{5} t^{5}, 54 t^{3}\right) \\
& (\dot{\mathbf{r}}) \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}=\left(-\frac{162}{5} t, \frac{216}{5} t,-54 t^{3}\right)=18 t\left(-\frac{9}{5}, \frac{12}{5},-3 t^{2}\right) \\
& \quad \mathbf{N}=\varepsilon \frac{-18 t\left(9 / 5,-12 / 5,3 t^{2}\right)}{\left(3 \sqrt{t^{4}+1}\right)(18|t|)}=-\frac{\varepsilon t}{|t|} \frac{\left(3,-4,5 t^{2}\right)}{5 \sqrt{t^{4}+1}}
\end{aligned}
$$

and

Now choose $\varepsilon=+1$ when $t<0$ and -1 otherwise, making

$$
\mathbf{N}=\frac{\left(3,-4,5 t^{2}\right)}{5 \sqrt{t^{4}+1}} \quad \mathbf{B}=\varepsilon \frac{(-18 t / 5)(4,3,0)}{18|t|}=\frac{-\varepsilon t}{|t|}\left(\frac{4}{5}, \frac{3}{5}, 0\right)=\left(\frac{4}{5}, \frac{3}{5}, 0\right)
$$

10.4 Establish the general formulas (10.5) for the moving frame of the curve $\mathscr{C}: \mathbf{r}=\mathbf{r}(t)$, with an arbitrary parameter $t$.

By definition,

$$
\frac{d s}{d t}=\|\dot{\mathbf{r}}\| \equiv(\dot{\mathbf{r}} \dot{\mathbf{r}})^{1 / 2} \quad \text { or } \quad \frac{d t}{d s}=\|\dot{\mathbf{r}}\|^{-1}
$$

and we have at once for the unit tangent vector

$$
\mathbf{T}=\mathbf{r}^{\prime}=\dot{\mathbf{r}} \frac{d t}{d s}=\frac{\dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|}
$$

To obtain a principal normal, first calculate

$$
\frac{d}{d t}\|\dot{\mathbf{r}}\| \equiv \frac{d}{d t}(\dot{\mathbf{r}})^{1 / 2}=\frac{1}{2}(\dot{\mathbf{r}} \dot{\mathbf{r}})^{-1 / 2}(\ddot{\mathbf{r}} \dot{\mathbf{r}}+\dot{\mathbf{r}} \ddot{\mathbf{r}})=\frac{\dot{\mathbf{r}} \ddot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|}
$$

(Note the general formula $d\|\mathbf{u}\| / d t=\mathbf{u \dot { u }} /\|\mathbf{u}\|$.) Hence,

and

$$
\begin{gathered}
\frac{d}{d t}\|\dot{\mathbf{r}}\|^{-1}=-\|\dot{\mathbf{r}}\|^{-2} \frac{d}{d t}\|\dot{\mathbf{r}}\|=-\frac{\dot{\mathbf{r}} \ddot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{3}} \\
\dot{\mathbf{T}}=\ddot{\mathbf{r}} \frac{d t}{d s}+\dot{\mathbf{r}} \frac{d}{d t}\|\dot{\mathbf{r}}\|^{-1}=\frac{\ddot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|}-\frac{(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{3}}=\frac{\|\dot{\mathbf{r}}\|^{2} \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{3}} \\
\mathbf{T}^{\prime}=\dot{\mathbf{T}} \frac{d t}{d s}=\frac{(\dot{\mathbf{r}} \dot{\mathbf{r}}) \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{4}}=-\frac{\dot{\mathbf{r}} \times(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})}{\|\dot{\mathbf{r}}\|^{4}}
\end{gathered}
$$

where the last step used the vector identify $\mathbf{u} \times(\mathbf{v} \times \mathbf{w})=(\mathbf{u w}) \mathbf{v}-(\mathbf{u v}) \mathbf{w}$. It follows that $\mathbf{N}$ can be constructed by normalizing the vector

$$
\mathbf{N}^{*} \equiv-\dot{\mathbf{r}} \times(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})
$$

Since $\dot{\mathbf{r}}$ and $\dot{\mathbf{r}} \times \ddot{\mathbf{r}}$ are orthogonal, $\left\|\mathbf{N}^{*}\right\|=\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|$ and so, provided $\dot{\mathbf{r}} \times \ddot{\mathbf{r}} \neq \mathbf{0}$,

$$
\mathbf{N}=\varepsilon \frac{(\dot{\mathbf{r}} \times \ddot{\mathbf{r}}) \times \dot{\mathbf{r}}}{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|\|\dot{\mathbf{r}}\|}=\varepsilon \frac{(\dot{\mathbf{r}} \dot{\mathbf{r}}) \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|}
$$

Finally, for the binormal vector, with $\mathbf{v} \equiv \dot{\mathbf{r}} \times \ddot{\mathbf{r}} \neq \mathbf{0}$,

$$
\mathbf{B}=\mathbf{T} \times \mathbf{N}=\frac{\dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|} \times \varepsilon \frac{\mathbf{v} \times \dot{\mathbf{r}}}{\|\mathbf{v}\|\|\dot{\mathbf{r}}\|}=\varepsilon \frac{(\dot{\mathbf{r}} \dot{\mathbf{r}}) \mathbf{v}-(\dot{\mathbf{r}} \mathbf{v}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{2}\|\mathbf{v}\|}=\varepsilon \frac{\|\dot{\mathbf{r}}\|^{2} \mathbf{v}-(0) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|^{2}\|\mathbf{v}\|}=\varepsilon \frac{\mathbf{v}}{\|\mathbf{v}\|}
$$

\section*{CURVATURE AND TORSION}
10.5 Find the curvature and the torsion of the circular helix

$$
\mathbf{r}=\left(a \cos \frac{s}{c}, a \sin \frac{s}{c}, \frac{b s}{c}\right) \quad\left(c=\sqrt{a^{2}+b^{2}}\right)
$$

where $s$ is arc length.

By differentiation with respect to arc length,

$$
\mathbf{T}=\mathbf{r}^{\prime}=\left(-\frac{a}{c} \sin \frac{s}{c}, \frac{a}{c} \cos \frac{s}{c}, \frac{b}{c}\right) \quad \mathbf{T}^{\prime}=\left(-\frac{a}{c^{2}} \cos \frac{s}{c},-\frac{a}{c^{2}} \sin \frac{s}{c}, 0\right)
$$

Normalizing $\mathbf{T}^{\prime}$, choose

$$
\mathbf{N}=\left(-\cos \frac{s}{c},-\sin \frac{s}{c}, 0\right)
$$

and, correspondingly,

$$
\mathbf{B}=\mathbf{T} \times \mathbf{N}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
-\frac{a}{c} \sin \frac{s}{c} & \frac{a}{c} \cos \frac{s}{c} & \frac{b}{c} \\
-\cos \frac{s}{c} & -\sin \frac{s}{c} & 0
\end{array}\right|=\left(\frac{b}{c} \sin \frac{s}{c},-\frac{b}{c} \cos \frac{s}{c}, \frac{a}{c}\right)
$$

$$
\mathbf{B}^{\prime}=\left(\frac{b}{c^{2}} \cos \frac{s}{c}, \frac{b}{c^{2}} \sin \frac{s}{c}, 0\right)
$$

Then, by (10.6),

$$
\kappa=\frac{a}{c^{2}} \cos ^{2} \frac{s}{c}+\frac{a}{c^{2}} \sin ^{2} \frac{s}{c}+0^{2}=\frac{a}{c^{2}} \quad \tau=\frac{b}{c^{2}} \cos ^{2} \frac{s}{c}+\frac{b}{c^{2}} \sin ^{2} \frac{s}{c}+0^{2}=\frac{b}{c^{2}}
$$

[If we introduce the "time" parameter $t=c s$, we then have:

$$
\frac{d z}{d t}=\frac{b}{c^{2}}=\tau
$$

i.e. the rate at which the helix rises out of the $x y$-plane (its osculating plane at $t=0$ ) is given by its (constant) torsion.]

10.6 Find the curvature and torsion of the curve $\mathbf{r}=\left(t^{2}+t \sqrt{2}, t^{2}-t \sqrt{2}, 2 t^{3} / 3\right) \quad(t$ real $)$.

Use the formulas $(10.8)$ :

$$
\begin{gathered}
\dot{\mathbf{r}}=\left(2 t+\sqrt{2}, 2 t-\sqrt{2}, 2 t^{2}\right) \quad\|\dot{\mathbf{r}}\|=\sqrt{(2 t+\sqrt{2})^{2}+(2 t-\sqrt{2})^{2}+4 t^{4}}=2\left(t^{2}+1\right) \\
\ddot{\mathbf{r}}=(2,2,4 t) \quad \ddot{\mathbf{r}}=(0,0,4) \\
\dot{\mathbf{r}} \times \ddot{\mathbf{r}}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
2 t+\sqrt{2} & 2 t-\sqrt{2} & 2 t^{2} \\
2 & 2 & 4 t
\end{array}\right|=4\left(t^{2}-t \sqrt{2},-\left(t^{2}+t \sqrt{2}\right), \sqrt{2}\right) \\
(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})^{2}=16\left[\left(t^{2}-t \sqrt{2}\right)^{2}+\left(t^{2}+t \sqrt{2}\right)^{2}+2\right]=32\left(t^{2}+1\right)^{2} \\
\operatorname{det}[\dot{\mathbf{r}} \ddot{\mathbf{r}} \ddot{\mathbf{r}}]=\ddot{\mathbf{r}} \cdot(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})=(0,0,4) \cdot 4\left(t^{2}-t \sqrt{2},-t^{2}-t \sqrt{2}, \sqrt{2}\right)=16 \sqrt{2}
\end{gathered}
$$

Hence

$$
\kappa=\frac{\varepsilon \sqrt{32\left(t^{2}+1\right)^{2}}}{8\left(t^{2}+1\right)^{3}}=\frac{\varepsilon}{\sqrt{2}\left(t^{2}+1\right)^{2}} \quad \tau=\frac{16 \sqrt{2}}{32\left(t^{2}+1\right)^{2}}=\frac{1}{\sqrt{2}\left(t^{2}+1\right)^{2}}
$$

\subsection*{10.7 Prove (10.8).}
Using the results of Problem 10.4, we have

$$
\begin{aligned}
\kappa=\mathbf{N T}^{\prime} & =\left(\varepsilon \frac{(\dot{\mathbf{r}} \times \ddot{\mathbf{r}}) \times \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|}\right) \cdot\left(-\frac{\dot{\mathbf{r}} \times(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})}{\|\dot{\mathbf{r}}\|^{4}}\right)=\varepsilon \frac{\|\dot{\mathbf{r}} \times(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})\|^{2}}{\|\dot{\mathbf{r}}\|^{5}\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|} \\
& =\varepsilon \frac{\|\dot{\mathbf{r}}\|^{2}\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2} \sin ^{2}(\pi / 2)}{\|\dot{\mathbf{r}}\|^{5} \mid \dot{\mathbf{r}} \times \ddot{\mathbf{r}} \|}=\varepsilon \frac{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|}{\|\dot{\mathbf{r}}\|^{3}}
\end{aligned}
$$

The torsion requires the computation of $\mathbf{B}^{\prime}$. By (10.5),

$$
\varepsilon \mathbf{B}=\frac{\dot{\mathbf{r}} \times \ddot{\mathbf{r}}}{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|} \equiv \frac{\mathbf{v}}{\|\mathbf{v}\|}
$$

whence

$$
\varepsilon_{\varepsilon} \dot{\mathbf{B}}=\frac{d}{d t}\left(\frac{\mathbf{v}}{\|\mathbf{v}\|}\right)=\frac{1}{\|\mathbf{v}\|} \dot{\mathbf{v}}+\frac{d}{d t}\left(\frac{1}{\|\mathbf{v}\|}\right) \mathbf{v}=\frac{\dot{\mathbf{v}}}{\|\mathbf{v}\|}-\frac{(\mathbf{v} \dot{\mathbf{v}}) \mathbf{v}}{\|\mathbf{v}\|^{3}}=\frac{\|\mathbf{v}\|^{2} \dot{\mathbf{v}}-(\mathbf{v} \dot{\mathbf{v}}) \mathbf{v}}{\|\mathbf{v}\|^{3}}
$$

But $\dot{\mathbf{r}}=d(\dot{\mathbf{r}} \times \ddot{\mathbf{r}}) / d t=(\ddot{\mathbf{r}} \times \ddot{\mathbf{r}})+(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})=\dot{\mathbf{r}} \times \ddot{\mathbf{r}}$; hence,

$$
\varepsilon \mathbf{B}^{\prime}=\frac{\varepsilon \dot{\mathbf{B}}}{\|\dot{\mathbf{r}}\|}=\frac{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2}(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})-[(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})](\dot{\mathbf{r}} \times \ddot{\mathbf{r}})}{\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{3}}
$$

Dot this with

$$
\varepsilon \mathbf{N}=\frac{(\dot{\mathbf{r}} \dot{\mathbf{r}}) \ddot{\mathbf{r}}-(\dot{\mathbf{r}} \ddot{\mathbf{r}}) \dot{\mathbf{r}}}{\|\dot{\mathbf{r}}\|\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|}
$$

from (10.5), and use $\mathbf{u} \cdot(\mathbf{u} \times \mathbf{w})=0$ :

$$
\begin{gathered}
\mathbf{N B}^{\prime}=\frac{(\dot{\mathbf{r}} \dot{\mathbf{r}})\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2}[\ddot{\mathbf{r}} \cdot(\dot{\mathbf{r}} \times \ddot{\mathbf{r}})]-0-0+0}{\|\dot{\mathbf{r}}\|^{2}\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{4}}=\frac{\|\dot{\mathbf{r}}\|^{2}(-\operatorname{det}[\ddot{\mathbf{r}} \dot{\mathbf{r}} \ddot{\mathbf{r}}])}{\|\dot{\mathbf{r}}\|^{2}\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2}} \\
\tau=\frac{\operatorname{det}[\dot{\mathbf{r}} \ddot{\mathbf{r}} \ddot{\mathbf{r}}]}{\|\dot{\mathbf{r}} \times \ddot{\mathbf{r}}\|^{2}}
\end{gathered}
$$

or

10.8 Prove $(a) \mathbf{N}^{\prime}=-\kappa \mathbf{T}+\tau \mathbf{B},(b) \mathbf{B}^{\prime}=-\tau \mathbf{N}$.

(a) Since $\mathbf{N N}=1, \mathbf{N}^{\prime}$ is orthogonal to $\mathbf{N}$, which puts it in the plane of $\mathbf{T}$ and $\mathbf{B}$. Therefore, for certain real $\lambda$ and $\mu$,


\begin{equation*}
\mathbf{N}^{\prime}=\lambda \mathbf{T}+\mu \mathbf{B} \tag{1}
\end{equation*}


Dot both sides by $\mathbf{T}$, then by $\mathbf{B}$, and use $\mathbf{T N}=0, \kappa=\mathbf{N T}^{\prime}$, and $\tau=-\mathbf{N B}^{\prime}$ :

$$
\begin{aligned}
& \mathbf{T} \mathbf{N}^{\prime}=\lambda \mathbf{T}^{2}+\mu \mathbf{T B}=\lambda \quad \text { or } \quad \lambda=-\mathbf{T}^{\prime} \mathbf{N}=-\kappa \\
& \mathbf{B N}^{\prime}=\tau=\lambda \mathbf{B T}+\mu \mathbf{B}^{2}=\mu
\end{aligned}
$$

Substitution for $\lambda$ and $\mu$ in (1) then yields the desired results.

(b) From $\mathbf{B}=\mathbf{T} \times \mathbf{N}$ and part (a),

$$
\begin{aligned}
\mathbf{B}^{\prime} & =\mathbf{T}^{\prime} \times \mathbf{N}+\mathbf{T} \times \mathbf{N}^{\prime}=(\kappa \mathbf{N}) \times \mathbf{N}+\mathbf{T} \times(-\boldsymbol{\kappa} \mathbf{T}+\tau \mathbf{B}) \\
& =0+0+\tau(\mathbf{T} \times \mathbf{B})=\tau(-\mathbf{N})=-\tau \mathbf{N}
\end{aligned}
$$

10.9 Prove that if a curve has $\kappa^{\prime}=0$ at some point, then $\mathbf{N}^{\prime \prime}$ is orthogonal to $\mathbf{T}$ at that point.

From $\mathbf{N}^{\prime}=-\kappa \mathbf{T}+\tau \mathbf{B}$, it follows that $\mathbf{N}^{\prime \prime}=-\kappa^{\prime} \mathbf{T}-\kappa \mathbf{T}^{\prime}+\tau^{\prime} \mathbf{B}+\tau \mathbf{B}^{\prime}$. But $\kappa^{\prime}=0$; and from the Serret-Frenet formulas for $\mathbf{T}^{\prime}$ and $\mathbf{B}^{\prime}$ we obtain

$$
\mathbf{N}^{\prime \prime}=-\kappa(\kappa \mathbf{N})+\tau^{\prime} \mathbf{B}+\tau(-\tau \mathbf{N})=\left(-\kappa^{2}-\tau^{2}\right) \mathbf{N}+\tau^{\prime} \mathbf{B}
$$

As $\mathbf{N}^{\prime \prime}$ is in the plane of $\mathbf{N}$ and $\mathbf{B}$, it is orthogonal to $\mathbf{T}$.

\section*{SURFACES IN EUCLIDEAN SPACE}
10.10 Show that a surface of revolution is regular and exhibit the unit surface normal.

The Gaussian form of a surface of revolution about the $z$-axis (Fig. 10-11) is

$$
\mathbf{r}=\left(f\left(x^{1}\right) \cos x^{2}, f\left(x^{1}\right) \sin x^{2}, g\left(x^{1}\right)\right) \quad\left(f\left(x^{1}\right)>0\right)
$$

so

$$
\mathbf{r}_{1}=\left(f^{\prime}\left(x^{1}\right) \cos x^{2}, f^{\prime}\left(x^{1}\right) \sin x^{2}, g^{\prime}\left(x^{1}\right)\right) \quad \mathbf{r}_{2}=\left(-f\left(x^{1}\right) \sin x^{2}, f\left(x^{1}\right) \cos x^{2}, 0\right)
$$

and $\mathbf{r}_{1} \times \mathbf{r}_{2}=\left(-f g^{\prime} \cos x^{2},-f g^{\prime} \sin x^{2}, f f^{\prime}\left(\cos ^{2} x^{2}+\sin ^{2} x^{2}\right)\right)$, with norm

$$
E=\sqrt{f^{2} g^{\prime 2} \cos ^{2} x^{2}+f^{2} g^{\prime 2} \sin ^{2} x^{2}+f^{2} f^{\prime 2}}=f \sqrt{f^{\prime 2}+g^{\prime 2}}
$$

Now $f=f\left(x^{1}\right) \neq 0$; further, the generating curve is regular, which means that, with $t=x^{1}$, the tangent vector of that curve,

$$
\left(\frac{d x}{d t}, 0, \frac{d z}{d t}\right)=\left(f^{\prime}, 0, g^{\prime}\right)
$$

is non-null and $f^{\prime 2}+g^{\prime 2} \neq 0$. Therefore, $E \neq 0$ and the surface is regular.

The unit surface normal is

$$
\mathbf{n}=\frac{1}{E}\left(\mathbf{r}_{1} \times \mathbf{r}_{2}\right)=\left(-\frac{g^{\prime}}{\sqrt{f^{\prime 2}+g^{\prime 2}}} \cos x^{2},-\frac{g^{\prime}}{\sqrt{f^{\prime 2}+g^{\prime 2}}} \sin x^{2}, \frac{f^{\prime}}{\sqrt{f^{\prime 2}+g^{\prime 2}}}\right)
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153(1)}
\end{center}

Fig. 10-11

10.11 Identify the $x^{1}$ - and $x^{2}$-curves for the right helicoid (Example 10.4) and describe the behavior of the unit surface normal along an $x^{1}$-curve.

The $x^{1}$-curves $\left(x^{2}=\right.$ const.) are given by

$$
\mathbf{r}=\left(0,0, a x^{2}\right)+x^{1}\left(\cos x^{2}, \sin x^{2}, 0\right) \quad\left(x^{1} \geqq 0\right)
$$

thus, they are rays parallel to the $x y$-plane. The $x^{2}$-curves $\left(x^{1}=\right.$ const.) are given by

$$
\sqrt{x^{2}+y^{2}}=x^{1} \quad z=a x^{2}
$$

i.e., circular helices of radii $x^{1}$.

We have:

$$
\mathbf{r}_{1}=\left(\cos x^{2}, \sin x^{2}, 0\right) \quad \mathbf{r}_{2}=\left(-x^{1} \sin x^{2}, x^{1} \cos x^{2}, a\right)
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-153}
\end{center}

Fig. 10-12

$$
\begin{aligned}
& \mathbf{r}_{1} \times \mathbf{r}_{2}=\left|\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
\cos x^{2} & \sin x^{2} & 0 \\
-x^{1} \sin x^{2} & x^{1} \cos x^{2} & a
\end{array}\right|=\left(a \sin x^{2},-a \cos x^{2}, x^{1}\right) \\
& \mathbf{n}=\frac{\mathbf{r}_{1} \times \mathbf{r}_{2}}{\left\|\mathbf{r}_{1} \times \mathbf{r}_{2}\right\|}=\left(\frac{a \sin x^{2}}{\sqrt{a^{2}+\left(x^{1}\right)^{2}}}, \frac{-a \cos x^{2}}{\sqrt{a^{2}+\left(x^{1}\right)^{2}}}, \frac{x^{1}}{\sqrt{a^{2}+\left(x^{1}\right)^{2}}}\right) \\
&=(\cos \omega) \mathbf{u}+(\sin \omega) \mathbf{v}
\end{aligned}
$$

and

where $\omega \equiv \tan ^{-1}\left(x^{1} / a\right), \mathbf{u} \equiv\left(\sin x^{2},-\cos x^{2}, 0\right), \mathbf{v} \equiv(0,0,1)$. On an $\mathbf{x}^{1}$-ray, $\mathbf{u}$ and $\mathbf{v}$ are fixed unit vectors, while $\omega$ increases from 0 to a $\pi / 2$ as $x^{1}$ increases from 0 to $\infty$. Thus, $\mathbf{n}$ traces out a quarter-circle as the ray is described (see Fig. 10-12).

10.12 Find the FFF for any surface of revolution, and specialize to a right circular cone.

With $\mathbf{r}_{1}$ and $\mathbf{r}_{2}$ as obtained in Problem 10.10,

$$
\begin{aligned}
& g_{11}=\mathbf{r}_{1} \mathbf{r}_{1}=\left(f^{\prime} \cos x^{2}\right)^{2}+\left(f^{\prime} \sin x^{2}\right)^{2}+\left(g^{\prime}\right)^{2}=f^{\prime 2}+g^{\prime 2} \\
& g_{12}=g_{21}=\mathbf{r}_{1} \mathbf{r}_{2}=-f^{\prime} f \cos x^{2} \sin x^{2}+f^{\prime} f \sin x^{2} \cos x^{2}+\left(g^{\prime}\right)(0)=0 \\
& g_{22}=\mathbf{r}_{2} \mathbf{r}_{2}=\left(-f \sin x^{2}\right)^{2}+\left(f \cos x^{2}\right)^{2}+0^{2}=f^{2}
\end{aligned}
$$

and


\begin{equation*}
\mathrm{I}=\left(f^{\prime 2}+g^{\prime 2}\right)\left(d x^{1}\right)^{2}+f^{2}\left(d x^{2}\right)^{2} \tag{1}
\end{equation*}


For a right circular cone (Fig. 10-13), $f=x^{1}$ and $g=a x^{1}$; hence,


\begin{equation*}
\mathrm{I}=\left(1+a^{2}\right)\left(d x^{1}\right)^{2}+\left(x^{1}\right)^{2}\left(d x^{2}\right)^{2} \tag{2}
\end{equation*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154}
\end{center}

Fig. 10-13

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-154(1)}
\end{center}

Fig. 10-14

10.13 Find the FFF for the catenoid (Fig. 10-14) and compute the length of the curve given by $x^{1}=t, x^{2}=t \quad(0 \leqq t \leqq \ln (1+\sqrt{2}))$.

Here $f\left(x^{1}\right)=a \cosh x^{1}, g\left(x^{1}\right)=a x^{1}$, and (1) of Problem 10.12 gives, along the curve,

$$
\left(\frac{d s}{d t}\right)^{2}=\left(a^{2} \cosh ^{2} x^{1}\right)\left(\frac{d x^{1}}{d t}\right)^{2}+\left(a^{2} \cosh ^{2} x^{1}\right)\left(\frac{d x^{2}}{d t}\right)^{2}=2 a^{2} \cosh ^{2} t
$$

and

$$
L=a \sqrt{2} \int_{0}^{\ln (1+\sqrt{2})} \cosh t d t=a \sqrt{2} \sinh [\ln (1+\sqrt{2})]=a \sqrt{2}
$$

10.14 Let $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$ be two curves on the right circular cone $\mathbf{r}=\left(x^{1} \cos x^{2}, x^{1} \sin x^{2}, 2 x^{1}\right)$ whose pre-images in the parameter plane are

$$
\mathscr{C}_{1}:\left\{\begin{array}{l}
x^{1}=3-t \\
x^{2}=t / 2
\end{array} \quad \mathscr{C}_{2}:\left\{\begin{array}{l}
x^{1}=\sigma>0 \\
x^{2}=\sigma^{2}
\end{array}\right.\right.
$$

At the point of intersection, find the angle between $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$, and show that orthogonality in the $x^{1} x^{2}$-plane does not carry over to the cone.

The intersection point $P^{\prime}$ of the two pre-image curves is determined by the simultaneous equations

$$
3-t=\sigma \quad \text { and } \quad \frac{t}{2}=\sigma^{2}
$$

which give $t=2, \tau=1$, and $P^{\prime}=(1,1)$. Thus, the two tangent vectors at $P^{\prime}$ are:

$$
\left(u^{i}\right)=\left.\left(\frac{d x^{i}}{d t}\right)\right|_{t=2}=\left(-1, \frac{1}{2}\right) \quad\left(v^{i}\right)=\left.\left(\frac{d x^{i}}{d \sigma}\right)\right|_{\sigma=1}=(1,2)
$$

Considered in the Euclidean sense, $\left(u^{i}\right)$ and $\left(v^{i}\right)$ are orthogonal.

To express the angle between tangents at the image of $P^{\prime}$, we adopt the metric (2) of Problem 10.12 (with $a=2$ ) and apply (10.18) for $x^{1}=1, x^{2}=1$ :

$$
\cos \theta=\frac{\left(1+2^{2}\right)(-1)(1)+(1)^{2}\left(\frac{1}{2}\right)(2)}{D}=\frac{-4}{D} \neq 0
$$

Therefore, the curves are not orthogonal at the image of $P^{\prime}$.

10.15 Prove Theorem 10.3 and verify Corollary 10.4 geometrically for the right helicoid (Example 10.4) and for any surface of revolution (Problem 10.12).

The proof consists merely in taking $\left(u^{i}\right)=(1,0)$ and $\left(v^{i}\right)=(0,1)$ in (10.18). (Compare Problem 5.31.)

As is clear from Problem 10.11, the right helicoid is a ruled surface, generated by a half-line (an $x^{1}$-curve), pivoted on the $z$-axis, that rotates parallel to the $x y$-plane while the pivot point travels up the $z$-axis. A given point $P$ of the generator thus describes a helical $x^{2}$-curve (Fig. 10-8), which is necessarily everywhere orthogonal to the generator (i.e., to the $x^{1}$-curves). As for surfaces of revolution, it is clear that the parameter lines that match the revolved planar curve ( $x^{1}$-curves, or meridians) and the circles traced by individual points of the planar curve ( $x^{2}$-curves, or parallels of latitude) are mutually orthogonal. By previous computations, $g_{12}=0$ for both the helicoid and for the general surface of revolution.

10.16 Show that under a change of coordinates $x^{1}=x^{1}\left(\bar{x}^{1}, \bar{x}^{2}\right), x^{2}=x^{2}\left(\bar{x}^{1}, \bar{x}^{2}\right)$ in the plane, the surface metric $\left(g_{i j}\right)$ transforms as a second-order covariant tensor.

We have by substitution $\mathbf{r}\left(x^{1}, x^{2}\right)=\mathbf{r}\left(x^{1}\left(\bar{x}^{1}, \bar{x}^{2}\right), x^{2}\left(\bar{x}^{1}, \bar{x}^{2}\right)\right) \equiv \overline{\mathbf{r}}\left(\bar{x}^{1}, \bar{x}^{2}\right)$, the latter being the "new" parameterization for $\mathscr{S}$. To compute the metric under this parameterization, write (by the chain rule for partial derivatives and the bilinearity of the inner product)

$$
\begin{aligned}
\bar{g}_{i j} & =\overline{\mathbf{r}}_{i} \overline{\mathbf{r}}_{j} \equiv \frac{\partial \overline{\mathbf{r}}}{\partial \bar{x}^{i}} \frac{\partial \overline{\mathbf{r}}}{\partial \bar{x}^{j}}=\left(\frac{\partial \mathbf{r}}{\partial x^{p}} \frac{\partial x^{p}}{\partial \bar{x}^{i}}\right) \cdot\left(\frac{\partial \mathbf{r}}{\partial x^{q}} \frac{\partial x^{q}}{\partial \bar{x}^{j}}\right)=\left(\mathbf{r}_{p} \frac{\partial x^{p}}{\partial \bar{x}^{i}}\right) \cdot\left(\mathbf{r}_{q} \frac{\partial x^{q}}{\partial \bar{x}^{j}}\right) \\
& =\mathbf{r}_{p} \mathbf{r}_{q} \frac{\partial x^{p}}{\partial \bar{x}^{i}} \frac{\partial x^{q}}{\partial \bar{x}^{j}} \equiv g_{p q} \frac{\partial x^{p}}{\partial \bar{x}^{i}} \frac{\partial x^{q}}{\partial \bar{x}^{j}}
\end{aligned}
$$

which is the correct formula for tensor character.

\section*{GEODESICS}
10.17 (a) Find the Christoffel symbols of the second kind for the sphere of radius $a$. (b) Verify that the great circles passing through the north and south poles (i.e., the $x^{1}$-curves) are geodesics.\\
(a) The FFF for the sphere of radius $a$ may be calculated from Problem 10.12:

$$
g_{11}=a^{2} \quad g_{12}=0=g_{21} \quad g_{22}=a^{2} \sin ^{2} x^{1}
$$

The formulas from Problem 6.4 can be used, since $\left(g_{i j}\right)$ is diagonal; the nonzero Christoffel symbols are found to be:

$$
\Gamma_{22}^{1}=-\sin x^{1} \cos x^{1} \quad \Gamma_{12}^{2}=\Gamma_{21}^{2}=\cot x^{1}
$$

(b) We want to show that the family of curves $x^{1}=t, x^{2}=d=$ const. are integral curves of the differential system (7.11)-(7.12), which may be conveniently written as

$$
\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{j k}^{i} \frac{d x^{j}}{d t} \frac{d x^{k}}{d t}=\frac{1}{2} \frac{d x^{i}}{d t}\left[\frac{d}{d t} \ln \left(g_{j k} \frac{d x^{j}}{d t} \frac{d x^{k}}{d t}\right)\right]
$$

or, for the given metric,

$$
\begin{array}{ll}
i=1 & \frac{d^{2} x^{1}}{d t^{2}}-\left(\sin x^{1} \cos x^{1}\right)\left(\frac{d x^{2}}{d t}\right)^{2}=\frac{1}{2} \frac{d x^{1}}{d t}\left[\frac{d}{d t} \ln \left(a^{2}\left(\frac{d x^{1}}{d t}\right)^{2}+\left(a^{2} \sin ^{2} x^{1}\right)\left(\frac{d x^{2}}{d t}\right)^{2}\right)\right] \\
i=2 & \frac{d^{2} x^{2}}{d t^{2}}+\left(2 \cot x^{1}\right) \frac{d x^{1}}{d t} \frac{d x^{2}}{d t}=\frac{1}{2} \frac{d x^{2}}{d t}\left[\frac{d}{d t} \ln \mathrm{I}\right.
\end{array}
$$

Since $d x^{1} / d t=1$ and $d x^{2} / d t=0$, both equations reduce to $0=0$, and the verification is complete.

10.18 Prove Theorem 10.6: A curve on a regular surface is a geodesic if and only if, by proper choice of the principal normal, $\mathbf{N}=\mathbf{n}$.

Let any curve on the surface be given by $\mathscr{C}: \mathbf{r}=\mathbf{r}\left(x^{1}(s), x^{2}(s)\right)$, where $s=\operatorname{arc}$ length, Then,

$$
\mathbf{T}=\mathbf{r}_{i} \frac{d x^{i}}{d s}
$$

and the first formula (10.9) gives


\begin{equation*}
\kappa \mathbf{N}=\mathbf{T}^{\prime}=\frac{d^{2} x^{i}}{d s^{2}} \mathbf{r}_{i}+\frac{d x^{i}}{d s}\left(\frac{\partial \mathbf{r}_{i}}{\partial x^{j}} \frac{d x^{j}}{d s}\right) \equiv \frac{d^{2} x^{i}}{d s^{2}} \mathbf{r}_{i}+\frac{d x^{i}}{d s} \frac{d x^{j}}{d s} \mathbf{r}_{i j} \tag{1}
\end{equation*}


Dot both sides of (1) by the vector $\mathbf{r}_{k}$ and use the resuit of Problem 10.48:


\begin{equation*}
\kappa \mathbf{r}_{k} \mathbf{N}=\frac{d^{2} x^{i}}{d s^{2}} \mathbf{r}_{i} \mathbf{r}_{k}+\frac{d x^{i}}{d s} \frac{d x^{j}}{d s} \mathbf{r}_{i j} \mathbf{r}_{k} \equiv \frac{d^{2} x^{i}}{d s^{2}} g_{i k}+\frac{d x^{i}}{d s} \frac{d x^{j}}{d s} \Gamma_{i j k} \tag{2}
\end{equation*}


Multiply both sides of (2) by $g^{k l}$ and sum on $k$ :


\begin{equation*}
g^{k l} \kappa \mathbf{r}_{k} \mathbf{N}=\frac{d^{2} x^{i}}{d s^{2}} \delta_{i}^{l}+\frac{d x^{i}}{d s} \frac{d x^{j}}{d s} g^{k l} \Gamma_{i j k}=\frac{d^{2} x^{l}}{d s^{2}}+\Gamma_{i j}^{l} \frac{d x^{i}}{d s} \frac{d x^{j}}{d s} \tag{3}
\end{equation*}


Now if $\mathscr{C}$ is a geodesic, the right side of (3) vanishes, and this implies that $\kappa \mathbf{r}_{k} \mathbf{N}=0$ for $k=1,2$. If $\kappa \neq 0$, then $\mathbf{r}_{1} \mathbf{N}=0=\mathbf{r}_{2} \mathbf{N}$; so that $\mathbf{N}$ is orthogonal to both $\mathbf{r}_{1}$ and $\mathbf{r}_{2}$ (thus to the tangent plane). Therefore, but for orientation, $\mathbf{N}=\mathbf{n}$. If $\kappa=0$ at some point $P$ and there is a sequence of points along the curve approaching $P$ for which $\kappa \neq 0$, then, by continuity, $\mathbf{N}=\mathbf{n}$. Otherwise, $\kappa=0$ on an interval and the curve is a straight line on that interval, in which case its principal normal $\mathbf{N}$ can be chosen to agree with $\mathbf{n}$. Conversely, if the curve has the property that $\mathbf{N}=\mathbf{n}$ at all points, then $\mathbf{r}_{k} \mathbf{N}=\mathbf{r}_{k} \mathbf{n}=0$ and the left side of (3) vanishes, showing that the pre-image of $\mathscr{C}$ satisfies the differential equations for a geodesic.

10.19 Apply Theorem 10.6 to the plane sections of a torus.

In Fig. 10-15 is shown a torus and various examples of plane sections. In (a), an elliptical-shaped vertical section, the section cannot be a geodesic because the surface normal at $P$ does not lie in the plane of the curve (which contains the curve normal). In (b), a horizontal section that is a circle, again the surface normals do not lie in the plane, and this section is not a geodesic. The circles shown in $(c)$ and $(d)$ are geodesics, since the surface normal will coincide with a correctly chosen principal normal of the curve.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157}
\end{center}

Fig. 10-15

10.20 Demonstrate that the intrinsic curvature $\tilde{\kappa}$ of a curve on a surface can be different from its curvature $\kappa$ as a curve in $\mathbf{E}^{3}$.

One example is a circle on a sphere of radius $a$. If the circle also has radius $a$, it is a great circle and, hence, a geodesic with zero intrinsic curvature. But its curvature as a (planar) curve in $\mathbf{E}^{3}$ is $1 / a$. Another example is the circular helix: its curvature is nonzero as a curve in $\mathbf{E}^{3}$, but as a geodesic on a circular cylinder its intrinsic curvature is zero.

\section*{SECOND FUNDAMENTAL FORM}
10.21 (a) Find the SFF for the right circular cone of Problem 10.12. (b) At the point $P(1,0, a)$, calculate the curvature of the normal section having the direction $\mathbf{j}$ at $P$ (see Fig. 10-16).

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-157(1)}
\end{center}

Fig. 10-16\\
(a) From $\mathbf{r}=\left(x^{1} \cos x^{2}, x^{1} \sin x^{2}, a x^{1}\right) \quad\left(x^{1}>0\right)$, we obtain:

$$
\begin{array}{ccc}
\mathbf{r}_{1}=\left(\cos x^{2}, \sin x^{2}, a\right) & \mathbf{r}_{2}=\left(-x^{1} \sin x^{2}, x^{1} \cos x^{2}, 0\right) \\
\mathbf{r}_{11}=(0,0,0) & \mathbf{r}_{12}=\mathbf{r}_{21}=\left(-\sin x^{2}, \cos x^{2}, 0\right) & \mathbf{r}_{22}=\left(-x^{1} \cos x^{2},-x^{1} \sin x^{2}, 0\right)
\end{array}
$$

and by Problem 10.10, $\mathbf{n}=\left(a^{2}+1\right)^{-1 / 2}\left(-a \cos x^{2},-a \sin x^{2}, 1\right)$. The coefficients in II are thus $f_{12} \equiv \mathbf{n r}_{11}=0, f_{12}=f_{21} \equiv \mathbf{n} \mathbf{r}_{12}=0, f_{22} \equiv \mathbf{n r}_{22}=\left(a^{2}+1\right)^{-1 / 2} a x^{1}$.

(b) The direction $\mathbf{j}$ at $P$ corresponds to the direction $\left(u^{1}, u^{2}\right)$ at $P^{\prime}=(1,0)$ in the parameter plane, where

$$
\begin{aligned}
\mathbf{j} & =u^{1} \mathbf{r}_{1}(P)+u^{2} \mathbf{r}_{2}(P) \\
(0,1,0) & =u^{1}(1,0, a)+u^{2}(0,1,0) \\
(0,1,0) & =\left(u^{1}, u^{2}, a u^{1}\right)
\end{aligned}
$$

Thus, $u^{1}=0$ and $u^{2}=1$. Appropriating I from Problem 10.12, we have

$$
\kappa_{g f}=\frac{\mathrm{II}\left(u^{1}, u^{2}\right)}{\mathrm{I}\left(u^{1}, u^{2}\right)}=\frac{f_{22}(P)\left(u^{2}\right)^{2}}{\left(a^{2}+1\right)\left(u^{1}\right)^{2}+(1)^{2}\left(u^{2}\right)^{2}}=f_{22}(P)=\frac{a}{\sqrt{a^{2}+1}}
$$

10.22 Develop geometrically a notion of "parallel transport" of a vector along a curve $\mathscr{C}$ on a regular surface $\mathscr{S}$.

Imagine $\mathscr{C}$, and with it $\mathscr{S}$, as being rolled without slipping onto a fixed plane $\mathscr{F}$, in such fashion that the point of contact is aways on $\mathscr{C}$ and the tangent plane to $\mathscr{S}$ at the point of contacts always coincides with $\mathscr{F}$. This maps $\mathscr{C}$ to a (planar) curve $\mathscr{C}^{*}$ in $\mathscr{F}$ that has the same arc-length parameter and the same tangent vector. Then, any vector in $\mathscr{F}$ that is attached to the point of contact and that remains parallel to itself (in the ordinary Euclidean sense) as the contact point describes $\mathscr{C}^{*}$ may-under the inverse mapping $\mathscr{C}^{*} \rightarrow \mathscr{C}$-be considered as undergoing parallel transport along $\mathscr{C}$. In general, parallel transport of a given vector around a closed curve on a surface does not reproduce the initial vector.

10.23 Prove (10.23).

Start with the formula for the unit tangent vector of any curve $\mathscr{C}$ on $\mathscr{S}$ :

$$
\mathbf{T}=\frac{u^{i} \mathbf{r}_{i}}{\sqrt{g_{k l} u^{k} u^{i}}} \quad\left(u^{i} \equiv \frac{d x^{i}}{d t}\right)
$$

Then


\begin{equation*}
\dot{\mathbf{T}}=\frac{d}{d t}\left(\frac{u^{i}}{\sqrt{g_{k l} u^{k} u^{l}}}\right) \mathbf{r}_{i}+\frac{u^{i}}{\sqrt{g_{k l} u^{k} u^{l}}} \dot{\mathbf{r}}_{i}=Q^{i} \mathbf{r}_{i}+\frac{u^{i}}{\sqrt{g_{k l} u^{k} u^{l}}} \mathbf{r}_{i j} u^{j} \tag{1}
\end{equation*}


where $Q^{i}$ is an abbreviation for the scalar coefficient of $\mathbf{r}_{i}$. Now the Frenet formula gives $\kappa \mathbf{N}=\mathbf{T}^{\prime}=$ $\dot{\mathbf{T}} / \sqrt{g_{k l} u^{k} u^{\prime}}$; together with (1), this yields:


\begin{equation*}
\kappa \mathbf{N}=\frac{Q_{i}}{\sqrt{g_{k l} u^{k} u^{l}}} \mathbf{r}_{i}+\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \mathbf{r}_{i j} \tag{2}
\end{equation*}


Dot both sides of (2) with $\mathbf{n}$ (the surface normal) and use the fact that $\mathbf{r}_{i} \mathbf{n}=0$ for each $i$ :


\begin{equation*}
\kappa \mathbf{n} \mathbf{N}=\frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} \mathbf{r}_{i j} \mathbf{n} \equiv \frac{u^{i} u^{j}}{g_{k l} u^{k} u^{l}} f_{i j} \tag{3}
\end{equation*}


If $\mathscr{C}$ is a normal section $\mathscr{C}_{\mathscr{F}}$ at $P$, and $\kappa, \mathbf{N}$, and the right side of (3) are all evaluated at $P$, then $\kappa=\kappa_{\mathscr{F}}$, $\mathbf{n N}=\mathbf{n}^{2}=1$, and (3) becomes the desired expression

$$
\kappa_{\mathscr{F}}=\frac{f_{i j} u^{i} u^{j}}{g_{k l} u^{k} u^{l}}
$$

\section*{CURVATURE OF SURFACES}
10.24 Show that the maximum and minimum values of the function

$$
F(\mathbf{u})=\frac{a_{i j} u^{i} u^{j}}{b_{k l} u^{k} u^{l}}=\frac{\mathbf{u}^{T} A \mathbf{u}}{\mathbf{u}^{T} B \mathbf{u}}
$$

where $A=\left[a_{i j}\right]_{22}, B=\left[b_{i j}\right]_{22}, \mathbf{u}=\left(u^{1}, u^{2}\right) \neq(0,0)$, with $B$ positive definite, are the two roots of the quadratic equation in $\lambda$

\[
\operatorname{det}(A-\lambda B) \equiv\left|\begin{array}{ll}
a_{11}-\lambda b_{11} & a_{12}-\lambda b_{12}  \tag{1}\\
a_{21}-\lambda b_{21} & a_{22}-\lambda b_{22}
\end{array}\right|=0
\]

(hence, eigenvalues of $B^{-1} A$ ), and that the extreme values of $F$ occur for vectors $\mathbf{u}$ satisfying $(A-x B) \mathbf{u}=\mathbf{0}$, where $x$ takes on the two eigenvalues of $B^{-1} A$ (hence, eigenvectors of $B^{-1} A$ ).

We may assume without loss of generality that $A$ and $B$ are symmetric. Let $\mathscr{G}$ be any simple closed curve in the $u^{1} u^{2}$-plane having the origin in its interior. The Weierstrass theorem guarantees that $F(\mathbf{u})$ asumes a largest value on $\mathscr{G}$; say, $F(\mathbf{w})=M$. Because $F$ is constant on rays emanating from the origin $(F(\lambda \mathbf{u})=F(\mathbf{u})$ for any $\lambda \neq 0)$, the absolute maximum on $\mathscr{G}$ is both an absolute and a relative maximum in the $u^{1} u^{2}$-plane; hence, the gradient of $F$ must vanish at $\mathbf{w}$. We have:

$$
\begin{gathered}
\frac{\partial F(\mathbf{u})}{\partial u^{P}}=\frac{\left(b_{k l} u^{k} u^{l}\right)\left(2 a_{p j} u^{j}\right)-\left(a_{i j} u^{i} u^{j}\right)\left(2 b_{p l} u^{l}\right)}{\left(b_{k l} u^{k} u^{l}\right)^{2}}=\frac{2}{b_{k l} u^{k} u^{l}}\left[a_{p j} u^{j}-F(\mathbf{u})\left(b_{p l} u^{l}\right)\right] \\
\nabla F(\mathbf{u})=\frac{2}{\mathbf{u}^{T} B \mathbf{u}}[A \mathbf{u}-F(\mathbf{u}) B \mathbf{u}]
\end{gathered}
$$

or

Therefore, $A \mathbf{w}-M B \mathbf{w}=\mathbf{0}$, which shows (i) that $M$ is an eigenvalue of $B^{-1} A$ and thus is a root of the characteristic equation (1); (ii) that $\mathbf{w}$ is an eigenvector belonging to $M$.

A like consideration of the minimum value, $m$, of $F$ on $\mathscr{G}$ leads to the other eigenvalue and associated eigenvector.

10.25 Prove that the extreme normal curvatures $\kappa_{1}, \kappa_{2}$ are the two roots of the quadratic equation $(10.25)$.

In Problem 10.24, take $a_{i j}=f_{i j}$ and $b_{k l}=g_{k l}$; expand (1) to obtain (10.25).

10.26 Prove that the two normal section curves through $P$ on $\mathscr{S}$ giving rise to $\max \kappa_{\mathscr{F}}=\kappa_{1}$ and $\min \kappa_{\mathscr{F}}=\kappa_{2}$ are orthogonal when $\kappa_{1} \neq \kappa_{2}$ (that is, when $P$ is not an umbilical point on $\mathscr{S}$ ).

Let us prove the general result, in the notation of Problem 10.24. We have:

$$
A \mathbf{w}-M B \mathbf{w}=\mathbf{0} \quad A \mathbf{v}-m B \mathbf{v}=\mathbf{0}
$$

With the inner product of column vectors defined as $\mathbf{p} \cdot \mathbf{q} \equiv \mathbf{p}^{T} B \mathbf{q}$, multiply the first equation by $\mathbf{v}^{T}$ and the second by $\mathbf{w}^{T}$, and subtract:

$$
(m-M) \mathbf{v} \cdot \mathbf{w}=0
$$

Hence, if $m \neq M, \mathbf{v}$ and $\mathbf{w}$ are orthogonal.

10.27 Calculate $\mathrm{K}$ and $\mathrm{H}$ for the right helicoid. Show that as $x^{1} \rightarrow \infty, \mathrm{K}$ tends to zero (the surface becomes "flatter" as the distance from its axis increases without bound).

From Problem 10.11 and Example 10.4,

$$
\begin{gathered}
\mathbf{n}=\frac{1}{\sqrt{\left(x^{1}\right)^{2}+a^{2}}}\left(a \sin x^{2},-a \cos x^{2}, x^{1}\right) \\
\mathbf{r}_{11}=(0,0,0) \quad \mathbf{r}_{12}=\mathbf{r}_{21}=\left(-\sin x^{2}, \cos x^{2}, 0\right) \quad \mathbf{r}_{22}=\left(-x^{1} \cos x^{2},-x^{1} \sin x^{2}, 0\right)
\end{gathered}
$$

so that

and

$$
\begin{gathered}
f_{11}=\mathbf{n r}_{11}=0 \quad f_{12}=f_{21}=-a / \sqrt{\left(x^{1}\right)^{2}+a^{2}} \quad f_{22}=0 \\
\mathrm{~K}=\frac{f_{11} f_{22}-f_{12}^{2}}{g_{11} g_{22}-g_{12}^{2}}=\frac{0-a^{2} /\left[\left(x^{1}\right)^{2}+a^{2}\right]}{\left[\left(x^{1}\right)^{2}+a^{2}\right]}=-\frac{a^{2}}{\left[\left(x^{1}\right)^{2}+a^{2}\right]^{2}} \rightarrow 0 \quad \text { as } x^{1} \rightarrow \infty \\
\mathrm{H}=\frac{f_{11} g_{22}+f_{22} g_{11}-2 f_{12} g_{12}}{g_{11} g_{22}-g_{12}^{2}}=\frac{0+0-2(0)}{g}=0
\end{gathered}
$$

\section*{STRUCTURE FORMULAS; ISOMETRIES}
10.28 Complete the proof of $(10.27 a)$.

The relations $\mathbf{n}_{i}=u_{i}^{k} \mathbf{r}_{k}$ and $\mathbf{n}_{j} \mathbf{r}_{i}=-f_{i j}$ imply

$$
-f_{i j}=u_{j}^{k} g_{k i}
$$

Multiply both sides by $g^{i s}$ and sum over $i$, obtaining $u_{j}^{s}=-g^{l s} f_{l j}$. Hence,

$$
\mathbf{n}_{i}=u_{i}^{k} \mathbf{r}_{k}=-g^{l k} f_{l i} \mathbf{r}_{k}=-g^{l k} f_{i l} \mathbf{r}_{k}
$$

10.29 Prove (10.28).

Let the equation $\mathbf{r}_{i j}=u_{i j}^{1} \mathbf{r}_{1}+u_{i j}^{2} \mathbf{r}_{2}+u_{i j}^{3} \mathbf{n}$ be rewritten as $\mathbf{r}_{i j}=u_{i j}^{s} \mathbf{r}_{s}+u_{i j}^{3} \mathbf{n}$. Dot with $\mathbf{n}$, obtaining $u_{i j}^{3}=f_{i j}$; therefore,


\begin{equation*}
\mathbf{r}_{i j}=u_{i j}^{s} \mathbf{r}_{s}+f_{i j} \mathbf{n} \tag{1}
\end{equation*}


Dot (1) with $\mathbf{r}_{k}$ and use Problem 10.48:

$$
\mathbf{r}_{i j} \mathbf{r}_{k}=u_{i j}^{s} \mathbf{r}_{s} \mathbf{r}_{k}+0 \quad \text { or } \quad \Gamma_{i j k}=u_{i j}^{s} g_{s k}
$$

Solve for $u_{i j}^{s}:$

$$
g^{k t} \Gamma_{i j k}=u_{i j}^{s} g^{k t} g_{s k} \quad \text { or } \quad \Gamma_{i j}^{t}=u_{i j}^{s} \delta_{s}^{t}=u_{i j}^{t}
$$

Substitute back into (1):

$$
\mathbf{r}_{i j}=\Gamma_{i j}^{t} \mathbf{r}_{t}+f_{i j} \mathbf{n}
$$

\section*{Supplementary Problems}
10.30 (a) Describe geometrically the curve whose parametric vector equation is

$$
\mathbf{r}=\left(\cos t, \sin t,(1-t)^{-1}\right) \quad(0 \leqq t<1)
$$

What happens as $t \rightarrow 1$ ? (b) Use a programmable calculator and Simpson's rule to find the arc length for $0 \leqq t \leqq 1 / 2$ accurate to 6 places.

10.31 Find the exact length of the space curve $\mathbf{r}=\left(t^{2}+t \sqrt{2}, t^{2}-t \sqrt{2}, 2 t^{3} / 3\right) \quad(-1 \leqq t \leqq 1)$.

10.32 (a) Using the arc-length parameterization of the right circular helix,

$$
\mathbf{r}=\left(a \cos \frac{s}{c}, a \sin \frac{s}{c}, \frac{b s}{c}\right) \quad\left(c \equiv \sqrt{a^{2}+b^{2}}\right)
$$

find the coordinate equations of the tangent line to the helix at any point $P \equiv \mathbf{r}(s)$. (b) Show that the tangent line intersects the $x y$-plane at a point $Q \equiv \mathbf{r}^{*}(s)$ such that $P Q=s$. (c) By thinking of a string wound along the helix, interpret the result of $(b)$.

10.33 Show that for the curve $y=x^{5}$ in the $x y$-plane, parameterized as $\mathbf{r}=\left(t, t^{5}, 0\right)$, the vector $\mathbf{T}^{\prime} /\left\|\mathbf{T}^{\prime}\right\|$ has an essential point of discontinuity at $t=0$.

10.34 Find the curvature and the torsion of the curve $\mathbf{r}=\left(t, t^{5}+a, t^{5}-a\right)$.

10.35 Prove that a curve is planar if and only if its torsion vanishes.

10.36 Prove that a planar curve with constant nonzero curvature $\kappa$ is a circle. [Hint: $\mathbf{T}=(\cos \theta, \sin \theta, 0)$ and $\mathbf{N}=(-\sin \theta, \cos \theta, 0)$ imply $\kappa=\theta^{\prime}$ or $\theta=\kappa s+a$; show that the radius is $\left.1 / \kappa.\right]$

10.37 Verify the Serret-Frenet formulas for the circular helix.

10.38 Show that if included in the range of the map $\mathbf{r}\left(x^{1}, x^{2}\right)$, the vertex of the right circular cone is a singular point.

10.39 Calculate the unit normal for the catenoid as parameterized in Fig. 10-14 and show that the surface is regular.

10.40 Find the length of the curve on the right helicoid (Example 10.4) given by $x^{1}=t^{2}, x^{2}=\ln t$, with $1 \leqq t \leqq 2$, in the special case when the parameter $a=1$.

10.41 Find the two possible directions for a curve $\mathscr{C}$ ' in the parameter plane whose image on the paraboloid of Fig. 10-17 meets the circle $x^{2}+y^{2}=4, z=4$ at $P(0,2,4)$ at an angle of $\pi / 3$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-161}
\end{center}

Fig. 10-17

10.42 Use Theorem 10.6 to show that an elliptical helix is not in general a geodesic on an elliptical cylinder.

10.43 Calculate the Christoffel symbols of the second kind for the right helicoid (Example 10.4). Show that circular helices on the surface are geodesics.

10.44 Exhibit the SFF for the general surface of revolution (Problem 10.10).

10.45 Establish the formulas below for any surface of revolution, with $G \equiv g^{\prime} / f^{\prime}$ (see Problems 10.12 and $10.44)$ :

$$
\mathrm{K}=\frac{G G^{\prime}}{f f^{\prime}\left(1+G^{2}\right)^{2}} \quad \text { and } \quad \mathrm{H}=\frac{f G^{\prime}+g^{\prime}\left(1+G^{2}\right)}{f\left|f^{\prime}\right|\left(1+G^{2}\right)^{3 / 2}}
$$

Use these formulas to verify that a sphere of radius $a$ has Gaussian curvature $1 / a^{2}$ and mean curvature $-2 / a$.

10.46 (a) Calculate $\mathrm{K}$ and $\mathrm{H}$ for two different parameterizations of the paraboloid $z=a\left(x^{2}+y^{2}\right)$ : (i) as the surface of revolution for which $f=x^{1}, g=a\left(x^{1}\right)^{2}$; (ii) as the surface $\mathbf{r}=\left(\bar{x}^{1}, \bar{x}^{2}, a\left(\bar{x}^{1}\right)^{2}+a\left(\bar{x}^{2}\right)^{2}\right) .(b)$ Interpret the results of $(a)$.

10.47 Infer from Problem 10.45 that $\mathrm{H} \equiv 0$ for any catenoid. [A surface with $\mathrm{H}=0$ at all points is called a minimal surface. Among minimal surfaces are those that solve "soap-bubble" problems, which require a minimum in surface area.]

10.48 Prove that $\Gamma_{i j k}=\mathbf{r}_{i j} \mathbf{r}_{k}$. Hint: $\left.\quad\left(\mathbf{r}_{i} \mathbf{r}_{j}\right)_{k}=\mathbf{r}_{i k} \mathbf{r}_{j}+\mathbf{r}_{i} \mathbf{r}_{j k}.\right]$

10.49 Surfaces for which the Gaussian curvature is a negative constant are very rare. One such surface can be constructed as follows. (a) A tractrix is the involute of a catenary (see Problem 10.32(c)). Write the vector equation for the involute of the catenary $\mathbf{r}=\left(a \cosh x^{1}, 0, a x^{1}\right)$ (see Fig. 10-14). (b) Using Problem 10.45, show that $\mathrm{K}=-1 / a^{2}$ for the tractroid generated by revolving the tractrix of $(a)$ about the $z$-axis.

10.50 Prove that the catenoid,

$$
I=\left(a^{2} \cosh ^{2} x^{1}\right)\left(d x^{1}\right)^{2}+\left(a^{2} \cosh ^{2} x^{1}\right)\left(d x^{2}\right)^{2}
$$

and the right helicoid,

$$
\mathrm{I}=\left(d \bar{x}^{1}\right)^{2}+\left[\left(\bar{x}^{1}\right)^{2}+a^{2}\right]\left(d \bar{x}^{2}\right)^{2}
$$

are locally isometric.

\section*{Chapter 11}
\section*{Tensors in Classical Mechanics}
\subsection*{11.1 INTRODUCTION}
Classical mechanics originated with the work of Galileo and was developed extensively by Newton (it is often called Newtonian mechanics). It deals with the motion of particles in a fixed frame of reference (rectangular coordinate system). The basic premise of Newtonian mechanics is the concept of absolute time measurement between two reference frames at constant velocity relative to each other (called Galilean frames). Within those frames, other coordinate systems may be used so long as the metric remains Euclidean. This means that some of the theory of tensors can be brought to bear on this study.

\subsection*{11.2 PARTICLE KINEMATICS IN RECTANGULAR COORDINATES}
Let $P$ be a particle whose path in $\mathbf{E}^{3}$ is given by


\begin{equation*}
\mathscr{C}: \mathbf{x}=\left(x^{i}(t)\right) \tag{11.1}
\end{equation*}


where $t$ represents time. The velocity vector of $P$ is defined as


\begin{equation*}
\mathbf{v} \equiv \frac{d \mathbf{x}}{d t} \equiv\left(\frac{d x^{1}}{d t}, \frac{d x^{2}}{d t}, \frac{d x^{3}}{d t}\right) \equiv\left(\dot{x}^{1}, \dot{x}^{2}, \dot{x}^{3}\right) \tag{11.2}
\end{equation*}


and the (instantaneous) velocity or speed as the scalar


\begin{equation*}
v \equiv\|\mathbf{v}\| \equiv \sqrt{\left(\dot{x}^{1}\right)^{2}+\left(\dot{x}^{2}\right)^{2}+\left(\dot{x}^{3}\right)^{2}} \tag{11.3}
\end{equation*}


Further, define the acceleration vector as


\begin{equation*}
\mathbf{a} \equiv \frac{d \mathbf{v}}{d t} \equiv\left(\frac{d^{2} x^{1}}{d t^{2}}, \frac{d^{2} x^{2}}{d t^{2}}, \frac{d^{2} x^{3}}{d t^{2}}\right) \equiv\left(\ddot{x}^{1}, \ddot{x}^{2}, \ddot{x}^{3}\right) \tag{11.4}
\end{equation*}


and the acceleration as


\begin{equation*}
a=\|\mathbf{a}\| \equiv \sqrt{\left(\ddot{x}^{1}\right)^{2}+\left(\ddot{x}^{2}\right)^{2}+\left(\ddot{x}^{3}\right)^{2}} \tag{11.5}
\end{equation*}


If $\mathbf{v}=\left(v^{i}\right)$ and $\mathbf{a}=\left(a^{i}\right)$, the preceding formulas have the component forms


\begin{equation*}
v^{i}=\frac{d x^{i}}{d t} \quad v=\sqrt{v^{i} v^{i}} \quad a^{i}=\frac{d^{2} x^{i}}{d t^{2}} \quad a=\sqrt{a^{i} a^{i}} \tag{11.6}
\end{equation*}


In terms of the geometry of the curve $\mathscr{C}$ we have $\mathbf{v}=v \mathbf{T}$, with $v=d s / d t$. Hence,

$$
\mathbf{a}=\frac{d}{d t}(v \mathbf{T})=\dot{v} \mathbf{T}+v \dot{\mathbf{T}}=\dot{v} \mathbf{T}+v \frac{d \mathbf{T}}{d s} \frac{d s}{d t}=\dot{v} \mathbf{T}+v^{2} \mathbf{T}^{\prime}
$$

But $\mathbf{T}^{\prime}=\kappa \mathbf{N}$, and so


\begin{equation*}
\mathbf{a}=\dot{v} \mathbf{T}+\kappa v^{2} \mathbf{N} \tag{11.7}
\end{equation*}


Via the Pythagorean theorem, (11.7) implies


\begin{equation*}
a=\sqrt{\dot{v}^{2}+\kappa^{2} v^{4}} \tag{11.8}
\end{equation*}


EXAMPLE 11.1 Formula (11.7) serves to define the tangential and normal accelerations of $P$ as

$$
\dot{v}=\frac{d^{2} s}{d t^{2}} \quad \text { and } \quad \kappa v^{2}=\frac{v^{2}}{\rho} \quad(\rho \equiv \text { radius of curvature })
$$

respectively. For a particle with constant velocity, $a=\left\|\kappa v^{2} \mathbf{N}\right\|=|\kappa| v^{2}$; i.e., the acceleration is proportional to the absolute curvature.

\subsection*{11.3 PARTICLE KINEMATICS IN CURVILINEAR COORDINATES}
There emerges the problem of expressing the preceding formulas in nonrectangular coordinate systems. This is not merely an academic consideration, for there are important situations in which the differential equations of motion can be solved only in polar or spherical coordinates (cf. Example 11.3), not to mention applications in relativistic mechanics.

Let us start with the definitions of the velocity and acceleration vectors in a barred rectangular system:

$$
\bar{v}^{i}=\frac{d \bar{x}^{i}}{d t} \quad \text { and } \quad \bar{a}^{i}=\frac{d \bar{v}^{i}}{d t}
$$

Because the tangent field of $\mathscr{C}$ is a tensor, the velocity components in an arbitrary coordinate system $\left(x^{i}\right)$ are just $v^{i}=d x^{i} / d t$. However, as we found in Chapter 6 , the acceleration components must be written as absolute derivatives along $\mathscr{C}: a^{i}=\delta v^{i} / \delta t$. Hence, in an arbitrary coordinate system $\left(x^{i}\right)$, with $\left(g_{i j}\right)$ representing the Euclidean metric, we have:


\begin{equation*}
v^{i}=\frac{d x^{i}}{d t} \quad a^{i}=\frac{d v^{i}}{d t}+\Gamma_{r s}^{i} v^{r} v^{s}=\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{r s}^{i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t} \tag{11.9}
\end{equation*}


and the speed and acceleration scalars are the invariants


\begin{equation*}
v=\sqrt{g_{i j} v^{i} v^{j}} \quad a=\sqrt{g_{i j} a^{i} a^{j}} \tag{11.10}
\end{equation*}


EXAMPLE 11.2 Formulas (11.9) give the contravariant components of velocity and acceleration. These are not the components used in classical physics and vector analysis. There, the metric for an orthogonal curvilinear coordinate system is written as

$$
d s^{2}=h_{1}^{2}\left(d x_{1}\right)^{2}+h_{2}^{2}\left(d x_{2}\right)^{2}+h_{3}^{2}\left(d x_{3}\right)^{2}
$$

and one defines the physical components of the velocity vector as

$$
v_{(1)}=h_{1} \frac{d x_{1}}{d t} \quad v_{(2)}=h_{2} \frac{d x_{2}}{d t} \quad v_{(3)}=h_{3} \frac{d x_{3}}{d t}
$$

Thus, the physical components are related to the contravariant components via


\begin{equation*}
v_{(\alpha)}=\sqrt{g_{\alpha \alpha}} v^{\alpha} \quad(\alpha=1,2,3 ; \text { no summation }) \tag{1}
\end{equation*}


Likewise for acceleration and force vectors. (See Problem 11.24.)

Let us illustrate the distinction by calculating the components of acceleration in cylindrical coordinates, $\left(x^{1}, x^{2}, x^{3}\right)=\left(x_{1}, x_{2}, x_{3}\right)=(r, \theta, z)$, for which $g_{11}=1, g_{22}=\left(x^{1}\right)^{2}, g_{33}=1$. By Problem 6.26,

$$
\left.\Gamma_{22}^{1}=-x^{1} \quad \Gamma_{12}^{2}=\Gamma_{21}^{2}=1 / x^{1} \quad \text { (all others zero }\right)
$$

so that (11.9) gives for the contravariant components


\begin{equation*}
a^{1}=\frac{d^{2} r}{d t^{2}}-r\left(\frac{d \theta}{d t}\right)^{2} \quad a^{2}=\frac{d^{2} \theta}{d t^{2}}+\frac{2}{r} \frac{d r}{d t} \frac{d \theta}{d t} \quad a^{3}=\frac{d^{2} z}{d t^{2}} \tag{2}
\end{equation*}


The physical components are then obtained from (1) as


\begin{equation*}
a_{(r)}=\frac{d^{2} r}{d t^{2}}-r\left(\frac{d \theta}{d t}\right)^{2} \quad a_{(\theta)}=r \frac{d^{2} \theta}{d t^{2}}+2 \frac{d r}{d t} \frac{d \theta}{d t} \quad a_{(z)}=\frac{d^{2} z}{d t^{2}} \tag{3}
\end{equation*}


Only the $\theta$-component differs between (2) and (3); but the difference is significant. For instance, the coriolis acceleration of a particle is $2 \dot{r} \dot{\theta}$, as in (3).

\subsection*{11.4 NEWTON'S SECOND LAW IN CURVILINEAR COORDINATES}
The momentum vector of a particle of mass $m$ is defined as $\mathbf{M}=m \mathbf{v}$. Relative to a rectangular coordinate system (with the property of being an inertial frame), Newton's second law of motion effectively defines the force vector acting on a particle as $\mathbf{F}=d \mathbf{M} / d t$. Accordingly, in curvilinear coordinates $\left(x^{i}\right)$, the law reads:


\begin{equation*}
\mathbf{F}=\frac{\delta \mathbf{M}}{\delta t}=m \frac{\delta \mathbf{v}}{\delta t}=m \mathbf{a} \tag{11.11}
\end{equation*}


assuming a constant mass. Therefore, the contravariant components of force are given by


\begin{equation*}
F^{i}=m a^{i}=m\left(\frac{d^{2} x^{i}}{d t^{2}}+\Gamma_{r s}^{i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}\right) \tag{11.12a}
\end{equation*}


and the covariant components by


\begin{equation*}
F_{i}=g_{i r} F^{r}=m\left(g_{i r} \frac{d^{2} x^{r}}{d t^{2}}+\Gamma_{r s i} \frac{d x^{r}}{d t} \frac{d x^{s}}{d t}\right) \tag{11.12b}
\end{equation*}


By introducing a scalar invariant called the kinetic energy of the particle,

$$
T \equiv \frac{1}{2} m v^{2}=\frac{1}{2} m g_{i j} v^{i} v^{j}
$$

one can (see Problem 11.5) put (11.12b) into the equivalent Lagrangian form


\begin{equation*}
F_{i}=\frac{d}{d t}\left(\frac{\partial T}{\partial v^{i}}\right)-\frac{\partial T}{\partial x^{i}} \tag{11.13}
\end{equation*}


The partial derivatives in (11.13) are taken with $T$ considered as a function of six independent variables, the $x^{i}$ (via the $g_{i j}$ ) and the $v^{i}$.

EXAMPLE 11.3 (Motion under a Central Force) (a) Obtain the differential equation for the trajectory of a particle acted on by a force that is always directed from (or toward) a fixed point $O$. (b) Solve the equation of (a) when the central force is gravitational, thus determining the orbit of a satellite.

(a) By Problem 11.18, the motion will be confined to a plane through $O$. Take $O$ as the origin of polar coordinates $\left(x^{1}, x^{2}\right)=(r, \theta)$ in the plane; the force field then has the form $\mathbf{F}=\left(F^{1}, 0\right)$. Taking the acceleration components from (2) of Example 11.2, we have as the equations of motion:

$$
\begin{aligned}
& F^{1}=m a^{1}=m\left[\frac{d^{2} r}{d t^{2}}-r\left(\frac{d \theta}{d t}\right)^{2}\right] \\
& 0=m a^{2}=m\left[\frac{d^{2} \theta}{d t^{2}}+\frac{2}{r} \frac{d r}{d t} \frac{d \theta}{d t}\right] \equiv \frac{m}{r^{2}} \frac{d}{d t}\left(r^{2} \frac{d \theta}{d t}\right)
\end{aligned}
$$

The $\theta$-equation has the first integral

$$
r^{2} \frac{d \theta}{d t}=q=\text { const. }
$$

(conservation of angular momentum), which can be used to change the parameter of the trajectory from $t$ to $\theta$. Thus, writing $u=1 / r$, we have:

$$
\begin{aligned}
& \frac{d r}{d t}=\frac{d \theta}{d t} \frac{d u^{-1}}{d \theta}=\left(q u^{2}\right)\left(-u^{-2} \frac{d u}{d \theta}\right)=-q \frac{d u}{d \theta} \\
& \frac{d^{2} r}{d t^{2}}=-q \frac{d^{2} u}{d \theta^{2}}\left(q u^{2}\right)=-q^{2} u^{2} \frac{d^{2} u}{d \theta^{2}}
\end{aligned}
$$

and the $r$-equation becomes


\begin{equation*}
\frac{d^{2} u}{d \theta^{2}}+u=g(u, \theta) \tag{1}
\end{equation*}


in which $g(u, \theta)=-F^{1}\left(u^{-1}, \theta\right) / m q^{2} u^{2}$.\\
(b) For the gravitational field, $F^{1}=-k / r^{2}=-k u^{2}$ ( $k>0$; attractive force), so that $g(u, \theta)=Q=$ const. and the solution of (1) is $u=P \cos \theta+Q$, or


\begin{equation*}
r=\frac{1 / Q}{1+e \cos \theta} \tag{2}
\end{equation*}


which is a conic having eccentricity $e=P / Q$ and focus at $O$, the classical result.

\subsection*{11.5 DIVERGENCE, LAPLACIAN, CURL}
The divergence of a contravariant vector $\mathbf{u}=\left(u^{i}\right)$ on $\mathbf{E}^{3}$ is defined by (9.11), with use of Problem 8.14:

$$
\operatorname{div} \mathbf{u}=u_{, i}^{i}=\frac{\partial u^{i}}{\partial x^{i}}+\Gamma_{r i}^{i} u^{r}=\frac{\partial u^{i}}{\partial x^{i}}+u^{r} \frac{\partial}{\partial x^{r}}(\ln \sqrt{g})=\frac{\partial u^{i}}{\partial x^{i}}+u^{i} \frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}(\sqrt{g})
$$

The product rule for partial differentiation yields the compact form


\begin{equation*}
\operatorname{div} \mathbf{u}=\frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}\left(\sqrt{g} u^{i}\right) \tag{11.14}
\end{equation*}


Another notation for the divergence is $\boldsymbol{\nabla} \cdot \mathbf{u}$.

The Laplacian of a scalar field $f$ is given by $\nabla^{2} f \equiv \operatorname{div}(\operatorname{grad} f)$. Since in general coordinates divergence is defined for contravariant tensors only, while grad $f=\left(\partial f / \partial x^{i}\right)$ is a covariant tensor (Example 3.5), we first raise the subscript and then find the divergence by (11.14):


\begin{equation*}
\nabla^{2} f=\operatorname{div}\left(g^{i j} \frac{\partial f}{\partial x^{j}}\right)=\frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}\left(\sqrt{g} g^{i j} \frac{\partial f}{\partial x^{j}}\right) \tag{11.15}
\end{equation*}


The Laplacian figures importantly in electromagnetic theory via the scalar wave equation,


\begin{equation*}
\frac{\partial^{2} f}{\partial t^{2}}=k^{2} \nabla^{2} f \quad(k=\text { const. }=\text { wave speed }) \tag{11.16a}
\end{equation*}


In cartesian coordinates only, one defines the Laplacian of a vector field as $\nabla^{2} \mathbf{u} \equiv\left(\nabla^{2} u^{i}\right)$, where $\nabla^{2} u^{i}=u_{x x}^{i}+u_{y y}^{i}+u_{z z}^{i}$, and writes the vector wave equation,


\begin{equation*}
\frac{\partial^{2} \mathbf{u}}{\partial t^{2}}=k^{2} \nabla^{2} \mathbf{u} \tag{11.16b}
\end{equation*}


as an abbreviation for three scalar wave equations.

EXAMPLE 11.4 Write the Laplacian for cvlindrical coordinates.

Using $g^{11}=1, g^{22}=1 /\left(x^{1}\right)^{2}, g^{33}=1$, and $g=\left(x^{1}\right)^{2}$ in (11.15),

$$
\begin{aligned}
\nabla^{2} f & =\frac{1}{x^{1}}\left[\frac{\partial}{\partial x^{1}}\left(x^{1} \frac{\partial f}{\partial x^{1}}\right)+\frac{\partial}{\partial x^{2}}\left(\frac{1}{x^{1}} \frac{\partial f}{\partial x^{2}}\right)+\frac{\partial}{\partial x^{3}}\left(x^{1} \frac{\partial f}{\partial x^{3}}\right)\right] \quad\left(x^{1}>0\right) \\
& =f_{11}+\frac{1}{\left(x^{1}\right)^{2}} f_{22}+f_{33}+\frac{1}{x^{1}} f_{1}
\end{aligned}
$$

the last line employing subscript notation for the partial derivatives.

The curl of a vector field $\mathbf{u}=\left(u^{i}\right)-$ symbolized curl $\mathbf{u}, \boldsymbol{\nabla} \times \mathbf{u}$, or rot $\mathbf{u}-$ is given in a rectangular coordinate system $\left(x^{i}\right)$ by


\begin{equation*}
\operatorname{curl} \mathbf{u} \equiv\left(e_{i j k} \frac{\partial u^{k}}{\partial x^{j}}\right) \tag{11.17a}
\end{equation*}


where $e_{i j k}$ is the permutation symbol (Chapter 3). The definition may be rewritten as a determinantal operator:

\[
\operatorname{curl} \mathbf{u} \equiv\left|\begin{array}{ccc}
\mathbf{e}_{1} & \mathbf{e}_{2} & \mathbf{e}_{3}  \tag{11.17b}\\
\frac{\partial}{\partial x^{1}} & \frac{\partial}{\partial x^{2}} & \frac{\partial}{\partial x^{3}} \\
u^{1} & u^{2} & u^{3}
\end{array}\right|
\]

in which $\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{3}\right)=(\mathbf{i}, \mathbf{j}, \mathbf{k})$ is the standard orthonormal basis. Unlike the gradient and the divergence, the curl cannot be extended to curvilinear coordinate systems by a tensor formula.

Remark 1: Not everything in mathematical physics is a tensor. Problem 11.11 shows that (11.17) defines a direct cartesian tensor, but that is all. This is not to say that the curl operator cannot be formulated and used in curvilinear coordinates (see any text in vector analysis). It is only that the curl in spherical coordinates (say) and the curl in rectangular coordinates are not related tensorwise.

\section*{Nonrelativistic Maxwell's Equations}
Let $\quad \mathbf{E}=$ electric field strength

$\mathbf{D}=$ electric displacement

$\mathbf{H}=$ magnetic field strength

$\mathbf{B}=$ magnetic induction

$\mathbf{J}=$ current density

$\rho=$ charge density

$\epsilon=$ dielectric constant

$\mu=$ magnetic permeability

$c=$ velocity of light

Then the famous Maxwell's equations may be written as follows:

\[
\begin{array}{ll}
\operatorname{curl} \mathbf{E}+\frac{1}{c} \frac{\partial \mathbf{B}}{\partial t}=0 & \operatorname{div} \mathbf{B}=0  \tag{11.18}\\
\operatorname{curl} \mathbf{H}-\frac{1}{c} \frac{\partial \mathbf{D}}{\partial t}=\frac{1}{c} \mathbf{J} & \operatorname{div} \mathbf{D}=\rho
\end{array}
\]

From standard formulas in electromagnetic theory, $\mathbf{D}=\epsilon \mathbf{E}, \mathbf{B}=\mu \mathbf{H}$, and $\mathbf{J}=\rho \mathbf{u}$, where $\mathbf{u}$ denotes the velocity field of the charge distribution; (11.18) becomes

$$
\begin{array}{ll}
\operatorname{curl} \mathbf{E}=-\frac{\mu}{c} \frac{\partial \mathbf{H}}{\partial t} & \operatorname{div} \mathbf{H}=0 \\
\operatorname{curl} \mathbf{H}=\frac{\epsilon}{c} \frac{\partial \mathbf{E}}{\partial t}+\frac{\rho}{c} \mathbf{u} & \operatorname{div} \mathbf{E}=\frac{\rho}{\epsilon}
\end{array}
$$

If the charge distribution is in free space $\left(\epsilon=\epsilon_{0}, \mu=\mu_{0}\right)$, a proper choice of units brings the equations into the form

\[
\begin{array}{ll}
\operatorname{curl} \mathbf{E}=-\frac{1}{c} \frac{\partial \mathbf{H}}{\partial t} & \operatorname{div} \mathbf{H}=0  \tag{11.19}\\
\operatorname{curl} \mathbf{H}=\frac{1}{c} \frac{\partial \mathbf{E}}{\partial t}+\frac{\rho}{c} \mathbf{u} & \operatorname{div} \mathbf{E}=\rho
\end{array}
\]

Work with Maxwell's equations requires the vector identities listed below (see Problems 11.10 and 11.21).


\begin{align*}
& \boldsymbol{\nabla} \cdot(\boldsymbol{\nabla} \times \mathbf{u})=0 \quad \text { (for any } \mathbf{u})  \tag{11.20}\\
& \boldsymbol{\nabla} \times(\boldsymbol{\nabla} \times \mathbf{u})=\nabla(\boldsymbol{\nabla} \cdot \mathbf{u})-\nabla^{2} \mathbf{u}  \tag{11.21}\\
& \frac{\partial}{\partial t}(\boldsymbol{\nabla} \cdot \mathbf{u})=\boldsymbol{\nabla} \cdot \frac{\partial \mathbf{u}}{\partial t}  \tag{11.22}\\
& \frac{\partial}{\partial t}(\boldsymbol{\nabla} \times \mathbf{u})=\boldsymbol{\nabla} \times \frac{\partial \mathbf{u}}{\partial t} \tag{11.23}
\end{align*}


\section*{Solved Problems}
\section*{VELOCITY AND ACCELERATION}
11.1 Find the velocity and acceleration vectors and the scalars $v$ and $a$ for a particle whose equation of motion (along a twisted cubic) is $\mathbf{x}=\left(t, t^{2}, t^{3}\right) \quad(-1 \leqq t \leqq 1)$. Determine the extreme values of $v$ and $a$, and where they are assumed.

$$
\begin{array}{lll}
\mathbf{v}=\left(1,2 t, 3 t^{2}\right) & \text { and } & v=\sqrt{1+4 t^{2}+9 t^{4}} \\
\mathbf{a}=(0,2,6 t) & \text { and } & a=\sqrt{4+36 t^{2}}
\end{array}
$$

Hence $v$ and $a$ have maxima at $t= \pm 1$, where $v=\sqrt{14}$ and $a=\sqrt{40}$. They have minima at $t=0$, where $v=1$ and $a=2$.

\section*{PARTICLE DYNAMICS}
11.2 A particle travels at constant speed $v$ on a curve with positive curvature. Show that its acceleration is greatest where the curvature is greatest.

By (11.8) with $\dot{v}=0, a=\kappa v^{2}$ or $a / \kappa=$ const.

11.3 Compute the contravariant acceleration components in a coordinate system $\left(x^{i}\right)$ connected to a rectangular coordinate system $\left(\bar{x}^{i}\right)$ by $\bar{x}^{1}=\left(x^{1}\right)^{2}, \bar{x}^{2}=x^{2}, \bar{x}^{3}=x^{3}$.

Use (5.7):

$$
G=J^{T} J=\left[\begin{array}{ccc}
2 x^{1} & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
2 x^{1} & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
4\left(x^{1}\right)^{2} & 0 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$

Hence, the Christoffel symbols are given by

$$
\Gamma_{11}^{1}=\frac{\partial}{\partial x^{1}}\left[\frac{1}{2} \ln 4\left(x^{1}\right)^{2}\right]=\frac{1}{x^{1}} \quad \text { (all others zero) }
$$

and (11.9) gives

$$
a^{1}=\frac{d^{2} x^{1}}{d t^{2}}+\frac{1}{x^{1}}\left(\frac{d x^{1}}{d t}\right)^{2} \quad a^{2}=\frac{d^{2} x^{2}}{d t^{2}} \quad a^{3}=\frac{d^{2} x^{3}}{d t^{2}}
$$

\section*{NEWTON'S SECOND LAW}
11.4 Show that Newton's second law is consistent with Newton's first law: A particle that is not acted upon by an outside force is at rest or is in motion along a straight line at constant velocity. Assume a rectangular coordinate system.

$\mathbf{F}=\mathbf{0}$ implies $d \mathbf{v} / d t=\mathbf{0}$, or $\mathbf{v}=\mathbf{d}$ (constant). Then,

$$
\frac{d \mathbf{x}}{d t}=\mathbf{d} \quad \text { or } \quad \mathbf{x}=t \mathbf{d}+\mathbf{x}_{0}
$$

which is the parametric equation for a point (if $\mathbf{d}=\mathbf{0}$ ) or for a straight line (if $\mathbf{d} \neq \mathbf{0}$ ), along which $v=\|\mathbf{d}\|=$ const.

11.5 Prove the equivalence of (11.13) and (11.12b).

For simplicity, take $m=1$ in (11.13). By the chain rule and the symmetry of $\left(g_{i j}\right)$,

$$
\frac{d}{d t}\left(\frac{\partial T}{\partial v^{i}}\right)-\frac{\partial T}{\partial x^{i}}=\frac{d}{d t}\left(g_{i r} v^{r}\right)-\frac{\partial T}{\partial g_{r s}} \frac{\partial g_{r s}}{\partial x^{i}}=g_{i r} \frac{d v^{r}}{d t}-\frac{\partial T}{\partial g_{r s}} \frac{\partial g_{r s}}{\partial x^{i}}+\frac{d g_{i r}}{d t} v^{r}
$$

$$
\begin{aligned}
& =g_{i r} \frac{d v^{r}}{d t}-g_{r s i}\left(\frac{1}{2} v^{r} v^{s}\right)+\frac{\partial g_{i r}}{\partial x^{s}} \frac{d x^{s}}{d t} v^{r}=g_{i r} \frac{d v^{r}}{d t}-\frac{1}{2} g_{r s i} v^{r} v^{s}+g_{i r s} v^{s} v^{r} \\
& =g_{i r} \frac{d v^{r}}{d t}-\frac{1}{2} g_{r s i} v^{r} v^{s}+\frac{1}{2} g_{s i r} v^{s} v^{r}+\frac{1}{2} g_{i r s} v^{s} v^{r}=g_{i r} \frac{d v^{r}}{d t}+\Gamma_{r s i} v^{r} v^{s}
\end{aligned}
$$

The final expression is exactly the right-hand side of $(11.12 b)$ (for $m=1$ ).

11.6 Solve (1) of Example 11.3 when the force field is of the form

$$
g(u, \theta)=A u+h(\theta)
$$

where $A$ is a constant and $h(\theta)$ is periodic of period $2 \pi$.

With primes denoting $\theta$-derivatives, we must solve

$$
u^{\prime \prime}+u=A u+h(\theta) \quad \text { or } \quad u^{\prime \prime}+(1-A) u=h(\theta)
$$

The general solution to the homogeneous equation is

$$
u= \begin{cases}P \cos (\sqrt{1-A} \theta+\alpha) & A<1 \\ \alpha \theta+\beta & A=1 \\ Q \exp (\sqrt{A-1} \theta)+R \exp (-\sqrt{A-1} \theta) & A>1\end{cases}
$$

A particular solution of the nonhomogeneous equation may be obtained in the form $u=u_{H} w$, where $u_{H}$ is any particular solution of the homogeneous equation. In fact, substitution in the differential equation yields

$$
2 u_{H}^{\prime} w^{\prime}+u_{H} w^{\prime \prime}=h \quad \text { or } \quad\left(u_{H}^{2} w^{\prime}\right)^{\prime}=u_{H} h
$$

and this last equation can be solved by two quadratures:

$$
w^{\prime}(\theta)=\frac{1}{u_{H}^{2}(\theta)} \int_{0}^{\theta} u_{H}(\phi) h(\phi) d \phi \quad \text { and } \quad w(\theta)=\int_{0}^{\theta} \frac{d \psi}{u_{H}^{2}(\psi)} \int_{0}^{\psi} u_{H}(\phi) h(\phi) d \phi
$$

The integrals are easily evaluated when $h(\phi)$ is represented as a Fourier series.

11.7 If $h(\theta)=0$ in Problem 11.6, identify the orbits corresponding to (a) $A=0,(b) A=1,(c)$ $A=5 / 4$.

(a) The curve $1 / r=P \cos (\theta+\alpha)$, or $r \cos (\theta+\alpha)=1 / P$, is a straight line (Fig. 11-1).

(b) The curve $1 / r=\alpha \theta+\beta$ is a hyperbolic spiral that degenerates into a circle for $\alpha=0$.

(c) The curve $1 / r=Q e^{\theta / 2}+R e^{-\theta / 2}$ is a complex spiral which, in the case $Q=0, R=1$, reduces to the simple logarithmic spiral $r=e^{\theta / 2}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-169}
\end{center}

Fig. 11-1

\section*{DIFFERENTIAL OPERATORS}
11.8 Calculate the Laplacian for spherical coordinates by the tensor formula. (The calculation is very tedious by other methods.)

We have

$$
G=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{2} & 0 \\
0 & 0 & \left(x^{1} \sin x^{2}\right)^{2}
\end{array}\right] \quad G^{-1}=\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & \left(x^{1}\right)^{-2} & 0 \\
0 & 0 & \left(x^{1} \sin x^{2}\right)^{-2}
\end{array}\right]
$$

and $g=\left(x^{1}\right)^{4} \sin ^{2} x^{2}$, so that in (11.15),

Therefore,

$$
\sqrt{g} g^{i j} \frac{\partial f}{\partial x^{j}}=\left(x^{1}\right)^{2}\left(\sin x^{2}\right)\left(g^{i 1} \frac{\partial f}{\partial x^{1}}+g^{i 2} \frac{\partial f}{\partial x^{2}}+g^{i 3} \frac{\partial f}{\partial x^{3}}\right)
$$

$$
\begin{aligned}
& \sqrt{g} g^{1 j} \frac{\partial f}{\partial x^{j}}=\left(x^{1}\right)^{2}\left(\sin x^{2}\right) \frac{\partial f}{\partial x^{1}} \\
& \sqrt{g} g^{2 j} \frac{\partial f}{\partial x^{j}}=\left(x^{1}\right)^{2}\left(\sin x^{2}\right) \frac{1}{\left(x^{1}\right)^{2}} \frac{\partial f}{\partial x^{2}}=\left(\sin x^{2}\right) \frac{\partial f}{\partial x^{2}} \\
& \sqrt{g} g^{3 j} \frac{\partial f}{\partial x^{j}}=\left(x^{1}\right)^{2}\left(\sin x^{2}\right) \frac{1}{\left(x^{1} \sin x^{2}\right)^{2}} \frac{\partial f}{\partial x^{3}}=\left(\csc x^{2}\right) \frac{\partial f}{\partial x^{3}}
\end{aligned}
$$

and so

$$
\begin{aligned}
& \frac{\partial}{\partial x^{i}}\left[\sqrt{g} g^{i j} \frac{\partial f}{\partial x^{j}}\right]=\frac{\partial}{\partial x^{1}} {\left[\left(x^{1}\right)^{2}\left(\sin x^{2}\right) \frac{\partial f}{\partial x^{1}}\right]+\frac{\partial}{\partial x^{2}}\left[\left(\sin x^{2}\right) \frac{\partial f}{\partial x^{2}}\right]+\frac{\partial}{\partial x^{3}}\left[\left(\csc x^{2}\right) \frac{\partial f}{\partial x^{3}}\right] } \\
&=2 x^{1}\left(\sin x^{2}\right) \frac{\partial f}{\partial x^{1}}+\left(x^{1}\right)^{2}\left(\sin x^{2}\right) \frac{\partial^{2} f}{\left(\partial x^{1}\right)^{2}}+\left(\cos x^{2}\right) \frac{\partial f}{\partial x^{2}}+\left(\sin x^{2}\right) \frac{\partial^{2} f}{\left(\partial x^{2}\right)^{2}} \\
&+\left(\csc x^{2}\right) \frac{\partial^{2} f}{\left(\partial x^{3}\right)^{2}}
\end{aligned}
$$

In writing the final steps we convert to $\rho=x^{1}, \varphi=x^{2}$, and $\theta=x^{3}$ :

$$
\begin{aligned}
\nabla^{2} f & =\frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}\left[\sqrt{g} g^{i j} \frac{\partial f}{\partial x^{j}}\right] \\
& =\frac{1}{\rho^{2} \sin \varphi}\left[(2 \rho \sin \varphi) \frac{\partial f}{\partial \rho}+\left(\rho^{2} \sin \varphi\right) \frac{\partial^{2} f}{\partial \rho^{2}}+(\cos \varphi) \frac{\partial f}{\partial \varphi}+(\sin \varphi) \frac{\partial^{2} f}{\partial \varphi^{2}}+(\csc \varphi) \frac{\partial^{2} f}{\partial \theta^{2}}\right] \\
& =\frac{\partial^{2} f}{\partial \rho^{2}}+\frac{1}{\rho^{2}} \frac{\partial^{2} f}{\partial \varphi^{2}}+\frac{1}{\rho^{2} \sin ^{2} \varphi} \frac{\partial^{2} f}{\partial \theta^{2}}+\frac{2}{\rho} \frac{\partial f}{\partial \rho}+\frac{\cot \varphi}{\rho^{2}} \frac{\partial f}{\partial \varphi}
\end{aligned}
$$

11.9 Calculate the divergence in spherical coordinates $(\rho, \varphi, \theta)$ of $(a)$ a contravariant vector, $\mathbf{u}=\left(u^{i}\right) ;(b)$ a vector specified by its physical components, $\mathbf{u}=u_{(1)} \mathbf{e}_{1}+u_{(2)} \mathbf{e}_{2}+u_{(3)} \mathbf{e}_{3}$.

(a) We plug into the formula (11.14):

$$
\begin{aligned}
\operatorname{div} \mathbf{u} & =\frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}\left(\sqrt{g} u^{i}\right)=\frac{\partial u^{i}}{\partial x^{i}}+u^{i} \frac{1}{\sqrt{g}} \frac{\partial}{\partial x^{i}}(\sqrt{g}) \\
& =\frac{\partial u^{i}}{\partial x^{i}}+u^{i} \frac{1}{\left(x^{1}\right)^{2} \sin x^{2}} \frac{\partial}{\partial x^{i}}\left[\left(x^{1}\right)^{2} \sin x^{2}\right] \\
& =\frac{\partial u^{i}}{\partial x^{i}}+u^{1}\left(\frac{2}{x^{1}}\right)+u^{2}\left(\frac{\cos x^{2}}{\sin x^{2}}\right)+u^{3}(0)
\end{aligned}
$$

Thus

$$
\operatorname{div} \mathbf{u}=\frac{\partial u^{1}}{\partial \rho}+\frac{\partial u^{2}}{\partial \varphi}+\frac{\partial u^{3}}{\partial \theta}+\frac{2}{\rho} u^{1}+(\cot \varphi) u^{2}
$$

(b) By Example 11.2, we apply (11.4) to the contravariant vector having components

$$
u^{1}=\frac{u_{(1)}}{1} \quad u^{2}=\frac{u_{(2)}}{x^{1}} \quad u^{3}=\frac{u_{(3)}}{x^{1} \sin x^{2}}
$$

Hence, from (a),

$$
\begin{aligned}
\operatorname{div} \mathbf{u} & =\frac{\partial}{\partial x^{1}} u_{(1)}+\frac{\partial}{\partial x^{2}}\left(\frac{u_{(2)}}{x^{1}}\right)+\frac{\partial}{\partial x^{3}}\left(\frac{u_{(3)}}{x^{1} \sin x^{2}}\right)+u_{(1)}\left(\frac{2}{x^{1}}\right)+\frac{u_{(2)}}{x^{1}}\left(\cot x^{2}\right) \\
& =\frac{\partial u_{(\rho)}}{\partial \rho}+\frac{1}{\rho} \frac{\partial u_{(\varphi)}}{\partial \varphi}+\frac{1}{\rho \sin \varphi} \frac{\partial u_{(\theta)}}{\partial \theta}+\frac{2}{\rho} u_{(\rho)}+\frac{\cot \varphi}{\rho} u_{(\varphi)}
\end{aligned}
$$

It is in this last form that "the divergence in spherical coordinates" is generally encountered in reference books.

11.10 Establish in rectangular coordinates the identity


\begin{equation*}
\boldsymbol{\nabla} \times(\boldsymbol{\nabla} \times \mathbf{u})=\nabla(\boldsymbol{\nabla} \cdot \mathbf{u})-\nabla^{2} \mathbf{u} \tag{1}
\end{equation*}


("curl curl equals grad div minus del-square").

Both sides of (1) are (cartesian) vectors; we shall show that they are componentwise equal.

By (11.7), the $i$ th component of curl $\mathbf{u}$ is $e_{i j k}\left(\partial u^{k} / \partial x^{j}\right)$. Therefore, the $i$ th component of curl (curl $\mathbf{u}$ ) is [use (3.23)]:

$$
\begin{aligned}
e_{i r s} \frac{\partial}{\partial x^{r}}\left(e_{s j k} \frac{\partial u^{k}}{\partial x^{i}}\right) & =e_{i r s} e_{s j k} \frac{\partial^{2} u^{k}}{\partial x^{r} \partial x^{j}}=e_{s i r} e_{s j k} \frac{\partial^{2} u^{k}}{\partial x^{r} \partial x^{j}} \\
& =\left(\delta_{i j} \delta_{r k}-\delta_{i k} \delta_{r j}\right) \frac{\partial^{2} u^{k}}{\partial x^{r} \partial x^{j}}=\frac{\partial^{2} u^{r}}{\partial x^{r} \partial x^{i}}-\frac{\partial^{2} u^{i}}{\partial x^{r} \partial x^{r}} \\
& \equiv \frac{\partial}{\partial x^{i}}(\operatorname{div} \mathbf{u})-\nabla^{2} u^{i}
\end{aligned}
$$

The first term on the right is recognized as the $i$ th component of grad (div $\mathbf{u}$ ), and the second term is (by definition) the $i$ th component of the Laplacian of the vector $\mathbf{u}$. QED

11.11 Prove that the array represented in rectangular coordinates $\left(x^{i}\right)$ by

$$
\operatorname{curl} \mathbf{u}=\left(e_{i j k} \frac{\partial u^{k}}{\partial x^{j}}\right)
$$

is a direct cartesian tensor.

It suffices to show that $\left(e_{i j k}\right)$ is a direct cartesian tensor, since $\left(\partial u^{i} / \partial x^{j}\right)$ is known to be a (direct) cartesian tensor. Therefore, given the orthogonal transformation $\bar{x}^{i}=a_{j}^{i} x^{j}$, with $\left|a_{j}^{i}\right|=+1$, define the $3^{3}=27$ quantities

$$
\tau_{i j k} \equiv e_{r s t} \frac{\partial x^{r}}{\partial \bar{x}^{i}} \frac{\partial x^{s}}{\partial \bar{x}^{j}} \frac{\partial x^{t}}{\partial \bar{x}^{k}}=e_{r s t} a_{r}^{i} a_{s}^{j} a_{t}^{k}
$$

We observe that:

(i) $\tau_{i j k}=0$ when two subscripts have the same value; e.g.,


\begin{align*}
\tau_{i 22}=e_{r s t} a_{r}^{i} a_{s}^{2} a_{t}^{2} & =-e_{r t s} a_{r}^{i} a_{s}^{2} a_{t}^{2}=-e_{r s t} a_{r}^{i} a_{t}^{2} a_{s}^{2}=-\tau_{i 22} \\
\tau_{123} & =e_{r s t} a_{r}^{1} a_{s}^{2} a_{t}^{3}=\left|a_{j}^{i}\right|=+1 \tag{ii}
\end{align*}


(iii) $\tau_{i j k}$ changes sign when any two subscripts are interchanged; e.g.

$$
\tau_{k j i}=e_{r s t} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-e_{t s r} a_{r}^{k} a_{s}^{j} a_{t}^{i}=-\tau_{i j k}
$$

But these three properties identify $\tau_{i j k}$ with $\bar{e}_{i j k}$, and the proof is complete.

11.12 Show that in a vacuum with zero charge $(\rho=0)$, the electric field $\mathbf{E}$ satisfies the vector wave equation

$$
\frac{\partial^{2} \mathbf{E}}{\partial t^{2}}=c^{2} \nabla^{2} \mathbf{E}
$$

From Maxwell's equations (11.19), along with the identity (11.23),

$$
\boldsymbol{\nabla} \times(\boldsymbol{\nabla} \times \mathbf{E})=-\frac{1}{c} \frac{\partial}{\partial t}(\boldsymbol{\nabla} \times \mathbf{H})=-\frac{1}{c}\left(\frac{1}{c} \frac{\partial^{2} \mathbf{E}}{\partial t^{2}}\right)=-\frac{1}{c^{2}} \frac{\partial^{2} \mathbf{E}}{\partial t^{2}}
$$

But $\boldsymbol{\nabla} \cdot \mathbf{E}=0$ and Problem 11.10 imply $\boldsymbol{\nabla} \times(\boldsymbol{\nabla} \times \mathbf{E})=-\nabla^{2} \mathbf{E}$, and the wave equation follows.

\section*{Supplementary Problems}
11.13 Show that if $v$ is constant, a particle describes equal lengths of arc in equal periods of time.

11.14 (a) Show that a particle whose path is given by $\mathbf{x}=(\cos t, \sin t, \cot t)$, for $\pi / 4 \leqq t<\pi / 2$, has velocity decreasing to $\sqrt{2}$ as $t \rightarrow \pi / 2$. (b) What is the behavior of the acceleration as $t \rightarrow \pi / 2$ ? (c) Find the extreme values of $v$ and $a$ for this particle.

11.15 For what kind of motion, if any, is $a=d v / d t$ ?

11.16 Develop a formula for  for a particle that has constant speed $v$.

11.17 Calculate the acceleration components (contravariant) in spherical coordinates $(\rho, \varphi, \theta)$.

11.18 Prove that motion under a central force is planar.

11.19 Calculate the Laplacian for cylindrical coordinates $(r, \theta, z)$.

11.20 Show that $\nabla^{2} f=g^{i j} f_{, i j}$. [Hint: Write (11.15) at the origin of Riemannian coordinates.]

11.21 Prove (11.22) and (11.23).

11.22 Prove that $\operatorname{curl}(\operatorname{grad} f)=\mathbf{0}$ for any $C^{2}$ scalar field $f$.

11.23 Show that in a charge-free vacuum, $\mathbf{H}$ also satisfies the vector wave equation.

11.24 Show that, relative to an orthogonal curvilinear coordinate system $\left(x^{1}, x^{2}, x^{3}\right)$, an arbitrary contravariant vector $\mathbf{v}=\left(v^{i}\right)$ has the representation

$$
\mathbf{v}=v_{(1)} \mathbf{e}_{1}+v_{(2)} \mathbf{e}_{2}+v_{(3)} \mathbf{e}_{3}
$$

where $v_{(\alpha)}$ is the physical component and $\mathbf{e}_{\alpha}$ is the unit normal to the surface $x^{\alpha}=$ const. [Hint: Use Problems 5.19 and 5.20].

\section*{Chapter 12}
\section*{Tensors in Special Relativity}
\subsection*{12.1 INTRODUCTION}
If the motion of a light pulse were an ordinary phenomenon, its velocity $c$ to one observer would appear to a second observer, moving at velocity $v$ relative to the first, to have the value $c-v$. This hypothetical property of light depends on the concept of absolute time measurement for all observers. However, beginning with the landmark Michelson-Morley experiment in 1880, all experimental data force us to abandon this reasonable hypothesis and to accept instead the now undisputed fact that the velocity of light, rather than the measurement of time, is an absolute of nature. Light is observed to have a single velocity, $c=2.9979 \times 10^{8} \mathrm{~m} / \mathrm{s}$, independent of the observer's motion away from or towards its source. This calls for adjustments to the equations of Newtonian mechanics which become major when high-velocity particles are involved.

\subsection*{12.2 EVENT SPACE}
It is first necessary to wed the concepts of time and space. Thus, each event (atomic collision, flash of lightning, etc.) is assigned four coordinates $(t, x, y, z)$, where $t$ is the time (in seconds) of the event and $(x, y, z)$ is the location (in meters) of the event in ordinary rectangular coordinates. Such coordinates are called space-time coordinates.

Definition 1: An event space is an $\mathbf{R}^{4}$ whose points are events, coordinatized by $\left(x^{i}\right)=\left(x^{0}, x^{1}, x^{2}, x^{3}\right)$, where $x^{0}=c t$ is the temporal coordinate, and $\left(x^{1}, x^{2}, x^{3}\right)=(x, y, z)$ the rectangular positional coordinates, of an event. Two events $E_{1}\left(\mathbf{x}_{1}\right)$ and $E_{2}\left(\mathbf{x}_{2}\right)$ are identical if $x_{1}^{i}=x_{2}^{i}$ for all $i$; simultaneous, if $x_{1}^{0}=x_{2}^{0}$; and copositional, if $x_{1}^{i}=x_{2}^{i}$ for $i=1,2,3$. The spatial distance between $E_{1}$ and $E_{2}$ is the number


\begin{equation*}
d=\sqrt{\left(\Delta x^{1}\right)^{2}+\left(\Delta x^{2}\right)^{2}+\left(\Delta x^{3}\right)^{2}} \tag{12.1}
\end{equation*}


where $\Delta x^{i} \equiv x_{2}^{i}-x_{1}^{i}$ for $i=1,2,3$.

\section*{Inertial Reference Frames}
The general setting for Einstein's Special Theory of Relativity (henceforth abbreviated SR) consists of two or more observers $O, \bar{O}, \overline{\bar{O}}, \ldots$, moving at constant velocities relative to each other, who set up space-time coordinate systems $\left(x^{i}\right),\left(\bar{x}^{i}\right),\left(\bar{x}^{i}\right), \ldots$ to record events and make calculations for experiments they conduct. Such coordinate systems in uniform relative motion are called inertial frames provided Newton's first law is valid in each system. All the systems are assumed to have a common origin at some instant, which is taken as $t=\bar{t}=\overline{\bar{t}}=\cdots=0$.

\section*{Light Cone}
A flash of light at position $(0,0,0)$ and time $t=0$ sends out an expanding spherical wave front, with equation $x^{2}+y^{2}+z^{2}=c^{2} t^{2}$, or


\begin{equation*}
\left(x^{0}\right)^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}-\left(x^{3}\right)^{2}=0 \tag{12.2}
\end{equation*}


(12.2) is the equation of the light cone in event space, relative to the inertial frame $\left(x^{i}\right)$. Figure 12-1 shows the projection of the light cone onto the hyperplane $x^{3}=0$. In any other inertial frame, $\left(\bar{x}^{i}\right)$, the equation of the light cone is exactly the same (since all observers measure the velocity of light as c):

$$
\left(\bar{x}^{0}\right)^{2}-\left(\bar{x}^{1}\right)^{2}-\left(\bar{x}^{2}\right)^{2}-\left(\bar{x}^{3}\right)^{2}=0
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-174}
\end{center}

Fig. 12-1

In Example 7.6 (using slightly different notation), we identified the light cone with the null geodesics of $\mathbf{R}^{4}$ under the metric of SR.

\section*{Relativistic Length}
For an arbitrary event $E(\mathbf{x})$, the quantity $\left(x^{0}\right)^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}-\left(x^{3}\right)^{2}$ may be positive, negative, or zero. The relativistic distance from $E(\mathbf{x})$ to the origin $E_{0}(\mathbf{0})$ is the real number $s \geqq 0$ such that

$$
\varepsilon s^{2}=\left(x^{0}\right)^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}-\left(x^{3}\right)^{2} \quad(\varepsilon= \pm 1)
$$

More generally, the length of interval or relativistic distance between $E_{1}\left(\mathbf{x}_{1}\right)$ and $E_{2}\left(\mathbf{x}_{2}\right)$ is the unique real number $\Delta s \geqq 0$ such that


\begin{equation*}
\varepsilon(\Delta s)^{2}=\left(\Delta x^{0}\right)^{2}-\left(\Delta x^{1}\right)^{2}-\left(\Delta x^{2}\right)^{2}-\left(\Delta x^{3}\right)^{2} \quad(\varepsilon= \pm 1) \tag{12.3}
\end{equation*}


where $\Delta x^{i} \equiv x_{2}^{i}-x_{1}^{i}$ for $i=0,1,2,3$. The chief significance of this length-concept is to be found in Theorem 12.1: Relativistic distance is an invariant across all inertial frames.

For a proof, see Problem 12.6.

\section*{Interval Types}
The interval between $E_{1}\left(\mathbf{x}_{1}\right)$ and $E_{2}\left(\mathbf{x}_{2}\right)$ is

(1) spacelike if $\left(\Delta x^{1}\right)^{2}+\left(\Delta x^{2}\right)^{2}+\left(\Delta x^{3}\right)^{2}>\left(\Delta x^{0}\right)^{2} \quad$ (or $\varepsilon=-1$; predominance of distance over time);

(2) lightlike if $\left(\Delta x^{0}\right)^{2}=\left(\Delta x^{1}\right)^{2}+\left(\Delta x^{2}\right)^{2}+\left(\Delta x^{3}\right)^{2} \quad$ (equality of time and distance);

(3) timelike if $\left(\Delta x^{0}\right)^{2}>\left(\Delta x^{1}\right)^{2}+\left(\Delta x^{2}\right)^{2}+\left(\Delta x^{3}\right)^{2} \quad$ (or $\varepsilon=+1$; predominance of time over distance).

By Theorem 12.1, the categorization is independent of the particular inertial frame.

\subsection*{12.3 THE LORENTZ GROUP AND THE METRIC OF SR}
Imagine two observers, $\dot{O}$ and $\bar{O}$, in uniform relative motion at speed $v$. They approach each other during negative time, coincide at zero time, and then recede from each other during positive time (Fig. 12-2(a)). Let $O$ and $\bar{O}$ set up independent reference frames $\left(x^{i}\right)$ and $\left(\bar{x}^{i}\right)$ by means of identical but separate clocks, with $t=\bar{t}=0$ when $O=\bar{O}$, and identical metersticks. Newton's first law will be assumed to hold in both frames, making them inertial frames.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175(1)}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-175}
\end{center}

(b)

Fig. 12-2

Common observation of events sets up a correspondence


\begin{equation*}
\mathscr{T}: \bar{x}^{i}=F^{i}\left(x^{0}, x^{1}, x^{2}, x^{3}\right) \tag{12.4}
\end{equation*}


that is bijective, since each event is assigned a unique set of coordinates. In most of what follows, it will be assumed that $O$ and $\bar{O}$ perform a simplifying maneuver at the instant of coincidence, whereby their $x$-axes point in the same direction along the line of motion and their $y$-axes and their $z$-axes coincide. In the ensuing translation, the $y$-axes and the $z$-axes remain parallel (Fig. 12-2(b)).

\section*{Postulates of SR}
(1) Principle of Relativity: The laws of physics are the same in all inertial frames.

(2) Invariance of Uniform Motion: A particle with constant velocity in one inertial frame will have constant velocity in all inertial frames.

(3) Invariance of Light Speed: The speed of light is invariant across all inertial frames.

Postulate 2 requires that the bijective transformation (12.4) be such as to map straight lines into straight lines. Consequently, each $F^{i}$ must be a linear function. Since $F^{i}(\mathbf{0})=\mathbf{0}$, constants $a_{j}^{i}$ exist such that


\begin{equation*}
T: \bar{x}^{i}=a_{j}^{i} x^{j} \tag{12.5}
\end{equation*}


\section*{Lorentz Matrices and Transformations}
The invariance of the equation of the light cone (in consequence of Postulate 3) may be expressed as


\begin{equation*}
g_{i j} x^{i} x^{j}=0=g_{i j} \bar{x}^{i} \bar{x}^{j} \tag{12.6}
\end{equation*}


where $g_{00}=1, g_{11}=g_{22}=g_{33}=-1$, and $g_{i j}=0$ for $i \neq j$. Substitution of (12.5) in (12.6) yields (Problem 12.4):


\begin{equation*}
g_{i j} a_{r}^{i} a_{s}^{j}=g_{r s} \tag{12.7a}
\end{equation*}


or, in matrix form,

or, written out,


\begin{equation*}
A^{T} G A=G \tag{12.7b}
\end{equation*}


\[
\begin{array}{rlrl}
\left(a_{0}^{0}\right)^{2}-\left(a_{0}^{1}\right)^{2}-\left(a_{0}^{2}\right)^{2}-\left(a_{0}^{3}\right)^{2} & =1 & \\
\left(a_{j}^{0}\right)^{2}-\left(a_{j}^{1}\right)^{2}-\left(a_{j}^{2}\right)^{2}-\left(a_{j}^{3}\right)^{2}=-1 & & (j=1,2,3)  \tag{12.7c}\\
a_{i}^{0} a_{j}^{0}-a_{i}^{1} a_{j}^{1}-a_{i}^{2} a_{j}^{2}-a_{i}^{3} a_{j}^{3} & =0 & & (i \neq j)
\end{array}
\]

It is easy to see (Problem 12.8) that requiring $g_{i j} x^{i} x^{j}=0$ to be invariant is tantamount to requiring the invariance of $g_{i j} x^{i} x^{j}=q$ for every value of $q$. Thus, (12.7) is a criterion for the quadratic form $g_{i j} x^{i} x^{j}$ to be invariant.

Definition 2: Any $4 \times 4$ matrix (or corresponding linear transformation) that preserves the quadratic form $\mathbf{x}^{T} G \mathbf{x}$ is called Lorentz.

In Problem 12.10 it is shown that the set of Lorentz matrices constitutes a group (the Lorentz group) under matrix multiplication.

\section*{Metric of SR}
If the terms $\bar{g}_{i j} \equiv g_{i j}$ are defined for the $\left(\bar{x}^{i}\right)$-system, then (12.7a) becomes $g_{r s}=\bar{g}_{i j} a_{r}^{i} a_{s}^{j}$, which makes $\left(g_{i j}\right)$ a covariant tensor of the second order under Lorentz transformations of coordinates. Accordingly, the metric for $\mathbf{R}^{4}$ is chosen as


\begin{equation*}
\varepsilon d s^{2}=g_{i j} d x^{i} d x^{j} \equiv\left(d x^{0}\right)^{2}-\left(d x^{1}\right)^{2}-\left(d x^{2}\right)^{2}-\left(d x^{3}\right)^{2} \tag{12.8}
\end{equation*}


\subsection*{12.4 SIMPLE LORENTZ MATRICES}
Let us suppose that $O$ and $\bar{O}$ have performed an alignment of their $x y z$-axes. Then any right circular cylinder with axis along the line of relative motion must have the same equation in the two systems; i.e., $\left(x^{2}\right)^{2}+\left(x^{3}\right)^{2}$ is invariant. It follows (see Problem 12.11) that the Lorentz transformation for this situation has the form

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{0}=a_{0}^{0} x^{0}+a_{1}^{0} x^{1} \equiv a x^{0}+b x^{1}  \tag{12.9}\\
\bar{x}^{1}=a_{0}^{1} x^{0}+a_{1}^{1} x^{1} \equiv d x^{0}+e x^{1} \\
\bar{x}^{2}=x^{2} \\
\bar{x}^{3}=x^{3}
\end{array}\right.
\]

By (12.7),


\begin{equation*}
a^{2}-d^{2}=1 \quad b^{2}-e^{2}=-1 \quad a b-d e=0 \tag{12.10}
\end{equation*}


By considering the coordinates which $O$ and $\bar{O}$ would assign to each other's origin (see Problem 12.12), we find that


\begin{equation*}
d=-(v / c) a \equiv-\beta a \quad \text { and } \quad a=e \tag{12.11}
\end{equation*}


(The notation $\beta=v / c$ is standard in SR.) From (12.10) and the fact that $a>0$ (since both clocks can be assumed to run in the same sense), it follows that


\begin{equation*}
a=\left(1-\beta^{2}\right)^{-1 / 2}=e \quad b=-\beta\left(1-\beta^{2}\right)^{-1 / 2}=d \tag{12.12}
\end{equation*}


Therefore, the coordinate transformation takes on the simplified form

\[
\mathscr{T}:\left\{\begin{array}{l}
\bar{x}^{0}=\frac{x^{0}-\beta x^{1}}{\sqrt{1-\beta^{2}}}  \tag{12.13}\\
\bar{x}^{1}=\frac{-\beta x^{0}+x^{1}}{\sqrt{1-\beta^{2}}} \\
\bar{x}^{2}=x^{2} \\
\bar{x}^{3}=x^{3}
\end{array} \quad \text { or } \quad A=\left[\begin{array}{cccc}
\frac{1}{\sqrt{1-\beta^{2}}} & \frac{-\beta}{\sqrt{1-\beta^{2}}} & 0 & 0 \\
\frac{-\beta}{\sqrt{1-\beta^{2}}} & \frac{1}{\sqrt{1-\beta^{2}}} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]\right.
\]

Any $4 \times 4$ matrix (linear transformation) of the form

$$
A=\left[\begin{array}{llll}
a & b & 0 & 0 \\
b & a & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

where $a^{2}-b^{2}=1$, will be termed simple Lorentz. The relative velocity in the physical situation modeled by $A$ is recovered as $\beta=-b / a$.

EXAMPLE 12.1 By Problem 12.9, the inverse of a simple Lorentz matrix is

$$
A^{-1}=\left[\begin{array}{cccc}
a & -b & 0 & 0 \\
-b & a & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

which is itself a simple Lorentz matrix, corresponding to a reversal in sign of $\beta$. [If the velocity of $\bar{O}$ with respect to $O$ is $v$, then the velocity of $O$ with respect to $\bar{O}$ is $-v$.]

\section*{A Decomposition Theorem}
The possibility of simplifying an arbitrary Lorentz matrix by a suitable rotation of axes can be expressed in purely mathematical terms. (See Problems 12.14 and 12.15.)

Theorem 12.2: An arbitrary Lorentz matrix $L=\left(a_{j}^{i}\right)$ has the representation

$$
L=R_{1} L^{*} R_{2}
$$

where $L^{*}$ is a simple Lorentz matrix with parameters $a=\left|a_{0}^{0}\right|=\varepsilon a_{0}^{0}$ and $b=$ $-\sqrt{\left(a_{0}^{0}\right)^{2}-1}$, and $R_{1}$ and $R_{2}$ are orthogonal Lorentz matrices defined by

\[
R_{1}=L R_{2}^{T}\left(L^{*}\right)^{-1} \quad \text { and } \quad R_{2}=\left[\begin{array}{llll}
\mathbf{e}_{1} & \mathbf{r}^{\prime} & \mathbf{s}^{\prime} & \mathbf{t}^{\prime} \tag{12.14}
\end{array}\right]^{T}
\]

Here, $\mathbf{e}_{1}=(1,0,0,0), \mathbf{r}^{\prime}=(\varepsilon / b)\left(0, a_{1}^{0}, a_{2}^{0}, a_{3}^{0}\right) \equiv(0, \mathbf{r}), \mathbf{s}^{\prime}=(0, \mathbf{s}), \mathbf{t}^{\prime}=(0, \mathbf{t})$, with $\mathbf{s}$ and $\mathbf{t}$ chosen to complete a $3 \times 3$ orthogonal matrix $\left[\begin{array}{lll}\mathbf{r} & \mathbf{s} & \mathbf{t}\end{array}\right]$.

Corollary 12.3: If $L=\left(a_{j}^{i}\right)$ connects two inertial frames, then the relative velocity between the frames is


\begin{equation*}
v=c \sqrt{1-\left(a_{0}^{0}\right)^{-2}} \tag{12.15}
\end{equation*}


\subsection*{12.5 PHYSICAL IMPLICATIONS OF THE SIMPLE LORENTZ TRANSFORMATION}
\section*{Length Contraction}
For any fixed $x^{0},(12.13)$ gives

$$
\Delta \bar{x}^{1}=\frac{1}{\sqrt{1-\beta^{2}}} \Delta x^{1} \quad \text { or } \quad \Delta x^{1}=\sqrt{1-\beta^{2}} \Delta \bar{x}^{1}<\Delta \bar{x}^{1}
$$

If the frame $\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, distances in $\bar{O}$ appear to observer $O$ to be foreshortened in the direction of the motion by the factor $\sqrt{1-\beta^{2}}$.

\section*{Time Dilation}
For any fixed $\bar{x}^{1},(12.13)$, inverted, gives

$$
\Delta x^{0}=\frac{1}{\sqrt{1-\beta^{2}}} \Delta \bar{x}^{0} \quad \text { or } \quad \Delta t=\frac{1}{\sqrt{1-\beta^{2}}} \Delta \bar{t}>\Delta \bar{t}
$$

If the frame $\bar{O}$ is moving at a uniform velocity $v$ relative to $O$, the clock of observer $\bar{O}$ appears to observer $O$ to run slow by the factor $\sqrt{1-\beta^{2}}$.

\section*{Composition of Velocities}
If $\bar{O}$ has velocity $v_{1}$ relative to $O$ and $\overline{\bar{O}}$ has velocity $v_{2}$ relative to $\bar{O}$, then the Newtonian composition of velocities predicts that $\bar{O}$ has velocity $v_{3}=v_{1}+v_{2}$ relative to $O$. Although the error does not show up unless $v_{1}$ and $v_{2}$ are substantial fractions of the velocity of light, SR shows the Newtonian theory to be incorrect. The correct formula (Problem 12.20) is


\begin{equation*}
v_{3}=\frac{v_{1}+v_{2}}{1+v_{1} v_{2} / c^{2}} \tag{12.16}
\end{equation*}


\subsection*{12.6 RELATIVISTIC KINEMATICS}
\section*{4-Vectors}
We begin with ordinary velocity and acceleration of a particle within a single inertial frame $\left(x^{i}\right)$. By introducing the concept of proper time, we shall be able to obtain velocity and acceleration as contravariant vectors with respect to Lorentz transformations (to be called 4-vectors from now on). In general, $\left(V^{i}\right)$ is a 4-vector if it transforms according to the law $\bar{V}^{i}=a_{j}^{i} V^{j}$, where $\left(a_{j}^{i}\right)$ is the Lorentz matrix of (12.5). It is customary to use the following notation for 4-vectors:

$$
\left(V^{i}\right) \equiv\left(V^{0}, \mathbf{V}\right) \quad \text { where } \quad V^{0} \equiv V_{t} \quad \text { and } \quad \mathbf{V} \equiv\left(V^{1}, V^{2}, V^{3}\right) \equiv\left(V_{x}, V_{y}, V_{z}\right)
$$

$V^{0}$ is referred to as the time component of the vector and $\left(V_{x}, V_{y}, V_{z}\right)$ are the usual space components. All indices are understood to range over the values $0,1,2,3$, unless specifically noted otherwise.

\section*{Nonrelativistic Velocity and Acceleration}
In the inertial frame $\left(x^{i}\right)=(x, y, z)$ let a particle describe the class $C^{2}$ curve

$$
\mathscr{K}:\left(x^{i}\right)=(c t, \mathbf{r}(t))=(c t, x(t), y(t), z(t))
$$

Then we have the classical formulas


\begin{equation*}
\left(v_{i}\right)=\left(\frac{d x^{i}}{d t}\right) \equiv(c, \mathbf{v}) \tag{12.17}
\end{equation*}


where $\mathbf{v}=d \mathbf{r} / d t$ and $\hat{v} \equiv\|\mathbf{v}\|=\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}$;


\begin{equation*}
\left(a_{i}\right)=\left(\frac{d^{2} x^{i}}{d t^{2}}\right) \equiv(0, \mathbf{a}) \tag{12.18}
\end{equation*}


where $\mathbf{a}=d \mathbf{v} / d t$ and $\hat{a}=\|\mathbf{a}\|=\sqrt{a_{x}^{2}+a_{y}^{2}+a_{z}^{2}}$.

As defined, neither the velocity nor the acceleration is a tensor under Lorentz transformations. In fact (Problem 12.22), if $\left(\bar{v}_{i}\right)$ and $\left(\bar{a}_{i}\right)$ are the like quantities in $\left(\bar{x}^{i}\right)$, then (12.13) yields the relations

\[
\begin{array}{cc}
\bar{v}_{0}=c=v_{0} & \bar{v}_{x}=\frac{v_{x}-v}{1-v_{x} v / c^{2}} \quad \bar{v}_{y}=\frac{v_{y} \sqrt{1-\beta^{2}}}{1-v_{x} v / c^{2}} \quad \bar{v}_{z}=\frac{v_{z} \sqrt{1-\beta^{2}}}{1-v_{x} v / c^{2}} \\
\bar{a}_{0}=0=a_{0} \quad \bar{a}_{x}=\frac{a_{x}\left(1-\beta^{2}\right)^{3 / 2}}{\left(1-v_{x} v / c^{2}\right)^{3}} \quad \bar{a}_{y}=\frac{\left[a_{y}+\left(v_{y} a_{x}-v_{x} a_{y}\right)\left(v / c^{2}\right)\right]\left(1-\beta^{2}\right)}{\left(1-v_{x} v / c^{2}\right)^{3}} \\
\bar{a}_{z}=\frac{\left[a_{z}+\left(v_{z} a_{x}-v_{x} a_{z}\right)\left(v / c^{2}\right)\right]\left(1-\beta^{2}\right)}{\left(1-v_{x} v / c^{2}\right)^{3}} \tag{12.20}
\end{array}
\]

The inverse relations can be quickly obtained by replacing $v$ by $-v$ and interchanging barred and unbarred terms throughout. For example, the second formula in (12.19) inverts to

$$
v_{x}=\frac{\bar{v}_{x}+v}{1+\bar{v}_{x} v / c^{2}}
$$

which is just (12.16) as applied to $v_{1}=\bar{v}_{x}$ and $v_{2}=v$.

\section*{Proper Time; Velocity and Acceleration 4-Vectors}
Let us reparameterize the curve $\mathscr{K}$, choosing now the quantity


\begin{equation*}
\tau=\frac{s}{c}=\frac{1}{c} \int_{t_{0}}^{t} \sqrt{\varepsilon g_{i j} \frac{d x^{i}}{d u} \frac{d x^{j}}{d u}} d u \quad \text { or } \quad \frac{d \tau}{d t}=\sqrt{1-\hat{v}^{2} / c^{2}} \tag{12.21}
\end{equation*}


where, as always, $\hat{v}<c$. The new parameter $\tau$ (a distance divided by a velocity) is known as the proper time for the particle; by Problem 12.23, a clock attached to the particle (and thus accelerating and decelerating along with it) reads $\tau$.

When $\tau$-derivatives replace $t$-derivatives, velocity and acceleration become tensorial; i.e., the components


\begin{equation*}
u^{i} \equiv \frac{d x^{i}}{d \tau} \quad b^{i} \equiv \frac{d u^{i}}{d \tau}=\frac{d^{2} x^{i}}{d \tau^{2}} \tag{12.22}
\end{equation*}


are taken by (12.13) into

\[
\begin{array}{llll}
\bar{u}^{0}=\frac{u^{0}-\beta u^{1}}{\sqrt{1-\beta^{2}}} & \bar{u}^{1}=\frac{-\beta u^{0}+u^{1}}{\sqrt{1-\beta^{2}}} & \bar{u}^{2}=u^{2} & \bar{u}^{3}=u^{3} \\
\bar{b}^{0}=\frac{b^{0}-\beta b^{1}}{\sqrt{1-\beta^{2}}} & \bar{b}^{1}=\frac{-\beta b^{0}+b^{1}}{\sqrt{1-\beta^{2}}} & \bar{b}^{2}=b^{2} & \bar{b}^{3}=b^{3} \tag{12.24}
\end{array}
\]

The important identities


\begin{equation*}
u_{i} u^{i}=c^{2} \quad u_{i} b^{i}=0 \tag{12.25}
\end{equation*}


are proved in Problem 12.24, and Problem 12.25 establishes the following connecting formulas between the numerical values of the relativistic and the nonrelativistic components:


\begin{equation*}
u^{i}=\frac{v_{i}}{\sqrt{1-\hat{v}^{2} / c^{2}}} \quad b^{i}=\frac{a_{i}}{1-\hat{v}^{2} / c^{2}}+\frac{(\mathbf{v a}) v_{i}}{c^{2}\left(1-\hat{v}^{2} / c^{2}\right)^{2}} \tag{12.26}
\end{equation*}


\section*{Instantaneous Rest Frame}
At time $t=t_{1}$, the particle moving along $\mathscr{K}$ has instantaneous position $P_{1}=\mathbf{r}\left(t_{1}\right)$ and instantaneous speed $\hat{v}\left(t_{1}\right)$. An instantaneous rest frame for the particle is an inertial frame that translates at speed $\hat{v}\left(t_{1}\right)$ along the tangent to $\mathscr{K}$ at $P_{1}$, in such manner that its origin coincides with $P_{1}$ at $t=t_{1}$. See Fig. 12-3.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-180}
\end{center}

Fig. 12-3

We shall say that a particle's motion (with respect to frame $O$ ) is uniformly accelerated if its spatial acceleration relative to an instantaneous rest frame $\bar{O}$,

$$
\alpha=\hat{\bar{a}}=\sqrt{\bar{a}_{x}^{2}+\bar{a}_{y}^{2}+\bar{a}_{z}^{2}}
$$

does not vary along the trajectory $\mathscr{K}$.

EXAMPLE 12.2 An electron fired at right angles into a uniform magnetic field undergoes uniformly accelerated motion (in a circle).

\subsection*{12.7 RELATIVISTIC MASS, FORCE, AND ENERGY}
The appropriate SR version of Newton's second law depends on the concept of mass to be adopted.

\section*{Rest Mass and Relativistic Mass}
The rest mass of a particle is its mass as measured or as inferred from Newtonian mechanics in any instantaneous rest frame for that particle.

The relativistic mass (in $O$ ) of a particle with spatial velocity $\mathbf{v}$ (with respect to $O$ ) is


\begin{equation*}
\hat{m}=\frac{m}{\sqrt{1-\hat{v}^{2} / c^{2}}} \tag{12.27}
\end{equation*}


where $m$ is the rest mass of the particle. As is shown in Problem 12.27, (12.27) is a necessary consequence of conservation of momentum.

\section*{Relativistic Momentum and Force}
The 4-momentum of SR is defined by


\begin{equation*}
\left(p^{i}\right) \equiv\left(p^{0}, \mathbf{p}\right)=\left(m u^{i}\right)=\left(\hat{m} v_{i}\right) \tag{12.28}
\end{equation*}


and the (nontensorial) Lorentz force $\left(F_{0}, \mathbf{F}\right)$ is defined as the time derivative of the 4-momentum:


\begin{equation*}
F_{0} \equiv \frac{d p^{0}}{d t}=\frac{d}{d t}\left(\frac{m c}{\sqrt{1-\hat{v}^{2} / c^{2}}}\right) \quad \mathbf{F} \equiv \frac{d}{d t}\left(\frac{m \mathbf{v}}{\sqrt{1-\hat{v}^{2} / c^{2}}}\right) \tag{12.29}
\end{equation*}


Like velocity, force becomes tensorial when proper time is introduced. Thus, the 4-force (Minkowski force) of SR is defined as

From (12.21), we have the connecting formula


\begin{equation*}
\left(K^{i}\right) \equiv\left(\frac{d p^{i}}{d \tau}\right) \tag{12.30}
\end{equation*}



\begin{equation*}
K^{i}=\frac{F_{i}}{\sqrt{1-\hat{v}^{2} / c^{2}}} \tag{12.31}
\end{equation*}


The following identities for the Lorentz and Minkowski forces are proved in Problem 12.29:


\begin{equation*}
u_{i} K^{i}=0 \quad K^{0}=\frac{1}{c} \mathbf{v K} \quad F_{0}=\frac{1}{c} \mathbf{v F} \quad \mathbf{v F}=\frac{d}{d t}\left(\frac{m c^{2}}{\sqrt{1-\hat{v}^{2} / c^{2}}}\right) \tag{12.32}
\end{equation*}


\section*{Relativistic Energy}
According to the classical work-energy theorem, the rate at which work is performed on a particle, (vF), equals the rate of increase of the particle's kinetic energy. Thus, the last identity (12.32) suggests the definition for $\mathrm{SR}$


\begin{equation*}
\hat{E}=\frac{m c^{2}}{\sqrt{1-\hat{v}^{2} / c^{2}}} \equiv \hat{m} c^{2} \tag{12.33}
\end{equation*}


for the energy of a particle moving at speed $\hat{v}$. In the limit as $\hat{v} \rightarrow 0,(12.33)$ becomes


\begin{equation*}
E=m c^{2} \tag{12.34}
\end{equation*}


This is Einstein's famous formula for the rest energy $E$ of a particle with rest mass $m$.

\subsection*{12.8 MAXWELL'S EQUATIONS IN SR}
It is helpful to look briefly at the way the metric for Special Relativity affects the formulas for the divergence and the Laplacian, and to consider a new kind of matrix that will be useful in formulating Maxwell's equations.

\section*{Vector Calculus and Lorentz Transformations}
For the metric of SR, all Christoffel symbols vanish, so that


\begin{equation*}
\operatorname{div} \mathbf{u}=\frac{\partial u^{i}}{\partial x^{i}} \tag{12.35}
\end{equation*}


and


\begin{align*}
\operatorname{div}(\operatorname{grad} f) \equiv \square f & =\frac{\partial}{\partial x^{i}}\left(g^{i j} \frac{\partial f}{\partial x^{j}}\right)=g^{i j} \frac{\partial^{2} f}{\partial x^{i} \partial x^{j}} \\
& =\frac{\partial^{2} f}{\left(\partial x^{0}\right)^{2}}-\frac{\partial^{2} f}{\left(\partial x^{1}\right)^{2}}-\frac{\partial^{2} f}{\left(\partial x^{2}\right)^{2}}-\frac{\partial^{2} f}{\left(\partial x^{3}\right)^{2}} \\
& =\frac{1}{c^{2}} \frac{\partial^{2} f}{\partial t^{2}}-\nabla^{2} f \tag{12.36}
\end{align*}


Note that in SR the Laplacian operator is notated $\square$, with $\nabla^{2}$ reserved for its spatial part. It is verified in Problem 12.31 that $\square f$ is an invariant under Lorentz transformations, which means that the scalar wave equation has the same form, $\square f=0$, in every inertial frame.

If we introduce the differential operator


\begin{equation*}
\partial^{i} \equiv g^{i j} \frac{\partial}{\partial x^{j}} \tag{12.37}
\end{equation*}


then we can express the equation of continuity for a vector $\left(w_{i}\right) \equiv\left(w_{0}, \mathbf{w}\right)$ as


\begin{equation*}
\partial^{i} w_{i}=0 \tag{12.38}
\end{equation*}


(12.38) is equivalent to $\partial w_{0} / \partial t=c \operatorname{div} \mathbf{w}$.

\section*{Maxwell's Equations in Event Space}
First, introduce for any two 3-vectors $\mathbf{U}=\left(U^{i}\right)$ and $\mathbf{V}=\left(V^{i}\right)$ the two antisymmetric matrices

\[
\left[f^{i j}\right]_{44} \equiv\left[\begin{array}{cccc}
0 & -V^{1} & -V^{2} & -V^{3}  \tag{12.39a}\\
V^{1} & 0 & U^{3} & -U^{2} \\
V^{2} & -U^{3} & 0 & U^{1} \\
V^{3} & U^{2} & -U^{1} & 0
\end{array}\right] \quad\left[\tilde{f}_{i j}\right]_{44} \equiv\left[\begin{array}{cccc}
0 & U^{1} & U^{2} & U^{3} \\
-U^{1} & 0 & V^{3} & -V^{2} \\
-U^{2} & -V^{3} & 0 & V^{1} \\
-U^{3} & V^{2} & -V^{1} & 0
\end{array}\right]
\]

The second matrix may be obtained from the first by making the replacements $\mathbf{V} \rightarrow-\mathbf{U}$ and $\mathbf{U} \rightarrow \mathbf{V}$; because these replacements constitute an anti-involution-i.e., $\tilde{f}_{i j}=-f^{i j}-$ the two matrices are said to be dual to each other. In terms of individual components $\left(e_{p q r}\right.$ denotes the permutation symbol of order 3):

\[
\begin{array}{lll}
f^{i j}=-f^{j i} & f^{0 q}=-V^{q} & f^{p q}=e_{p q r} U^{r}  \tag{12.39b}\\
\tilde{f}_{i j}=-\tilde{f}_{j i} & \tilde{f}_{0 q}=U^{q} & \tilde{f}_{p q}=e_{p q r} V^{r}
\end{array}
\]

in which $i, j \geqq 0$ and $p, q \geqq 1$.

By their concoction, these matrices turn out to be tensors under Lorentz transformations (provided the row-divergences vanish in all inertial frames); a proof is given in Problem 12.32. Moreover, these tensors have the properties (Problem 12.33)


\begin{equation*}
\frac{\partial f^{0 j}}{\partial x^{j}}=-\operatorname{div} \mathbf{V} \quad \frac{\partial \tilde{f}_{0 j}}{\partial x^{j}}=\operatorname{div} \mathbf{U} \tag{12.40}
\end{equation*}


and


\begin{equation*}
\left(\frac{\partial f^{1 j}}{\partial x^{j}}, \frac{\partial f^{2 j}}{\partial x^{j}}, \frac{\partial f^{3 j}}{\partial x^{j}}\right)=\operatorname{curl} \mathbf{U}+\frac{1}{c} \frac{\partial \mathbf{V}}{\partial t} \quad\left(\frac{\partial \tilde{f}_{1 j}}{\partial x^{j}}, \frac{\partial \tilde{f}_{2 j}}{\partial x^{j}}, \frac{\partial \tilde{f}_{3 j}}{\partial x^{j}}\right)=\operatorname{curl} \mathbf{V}-\frac{1}{c} \frac{\partial \mathbf{U}}{\partial t} \tag{12.41}
\end{equation*}


We now show how Maxwell's equations in vacuum, (11.19), may be extended to space-time via dual tensors of this sort. The equations are:

\[
\begin{array}{ll}
\operatorname{div} \mathbf{H}=0 & \operatorname{curl} \mathbf{E}+\frac{1}{c} \frac{\partial \mathbf{H}}{\partial t}=\mathbf{0} \\
\operatorname{div} \mathbf{E}=\rho & \operatorname{curl} \mathbf{H}-\frac{1}{c} \frac{\partial \mathbf{E}}{\partial t}=\frac{\rho}{c} \mathbf{v} \tag{12.43}
\end{array}
\]

in the last of which $\mathbf{v}$ is the classical spatial velocity (12.17) of the charge-cloud $\rho$. Define per (12.39) the tensors

\[
\mathscr{F}=\left[F^{i j}\right]_{44} \equiv\left[\begin{array}{cccc}
0 & -H_{1} & -H_{2} & -H_{3}  \tag{12.44}\\
H_{1} & 0 & E_{3} & -E_{2} \\
H_{2} & -E_{3} & 0 & E_{1} \\
H_{3} & E_{2} & -E_{1} & 0
\end{array}\right] \quad \tilde{\mathscr{F}}=\left[\tilde{F}^{i j}\right]_{44} \equiv\left[\begin{array}{ccccc}
0 & E_{1} & E_{2} & E_{3} \\
-E_{1} & 0 & H_{3} & -H_{2} \\
-E_{2} & -H_{3} & 0 & H_{1} \\
-E_{3} & H_{2} & -H_{1} & 0
\end{array}\right]
\]

(with $\mathbf{U}=\mathbf{E}$ and $\mathbf{V}=\mathbf{H}$ ). In view of the first equations (12.40) and (12.41), (12.42) may be written as


\begin{equation*}
\frac{\partial F^{i j}}{\partial x^{j}}=0 \tag{12.45a}
\end{equation*}


Similarly, if we make a 4-vector out of $\mathbf{v}$ and $\rho$ by the prescription


\begin{equation*}
\left(s^{i}\right) \equiv\left(\rho, \frac{\rho}{c} \mathbf{v}\right) \tag{12.46}
\end{equation*}


(see Problem 12.52), then the remaining Maxwell's equations, (12.43), are rendered tensorial as


\begin{equation*}
\frac{\partial \tilde{F}^{i j}}{\partial x^{j}}=s^{i} \tag{12.45b}
\end{equation*}


Equations (12.45) are the relativistic Maxwell's equations, valid in every inertial frame. Because $\tilde{\mathbf{F}}$ is antisymmetric, we have from $(12.45 \mathrm{~b})$ :

$$
\frac{\partial s^{i}}{\partial x^{i}}=\frac{\partial^{2} \tilde{F}^{i j}}{\partial x^{i} \partial x^{j}}=0 \quad \text { or } \quad \frac{\partial}{\partial x^{i}}\left(g^{i j} s_{j}\right)=\left(g^{j i} \frac{\partial}{\partial x^{i}}\right) s_{j} \equiv \partial^{j} s_{j}=0
$$

so that the covariant vector $\left(s_{j}\right)$ obeys the equation of continuity (12.38).

\section*{Solved Problems}
\section*{EVENT SPACE}
12.1 Calculate $\varepsilon$ and $\Delta s$ for the event pairs: (a) $E_{1}(5,1,-2,0)$ and $E_{2}(0,3,1,-3),(b)$ $E_{1}(5,1,3,3)$ and $E_{2}(2,-1,1,1),(c) E_{1}(7,2,4,4)$ and $E_{2}(4,1,2,6),(d) E_{1} \equiv$ flash of light in Chicago at 7 p.m. and $E_{2} \equiv$ flash of light in St. Louis ( 400 miles away) at 7.00000061 p.m. (e) Determine the interval type in each case.

(a) $\varepsilon(\Delta s)^{2}=5^{2}-(-2)^{2}-(-3)^{2}-3^{2}=25-4-9-9=3$, or $\Delta s=\sqrt{3}$ and $\varepsilon=1$.

(b) $\varepsilon(\Delta s)^{2}=9-4-4-4=-3$, or $\Delta s=\sqrt{3}$ and $\varepsilon=-1$.

(c) $\varepsilon(\Delta s)^{2}=9-1-4-4=0$, or $\Delta s=0$ and $\varepsilon=1$.

(d) With $c=186300 \mathrm{mi} / \mathrm{sec}, \varepsilon(\Delta s)^{2}=(0.002196 c)^{2}-(400)^{2} \approx 7375 \mathrm{mi}^{2}$, or $\Delta s \approx 85.8 \mathrm{mi}$ and $\varepsilon=1$.

(e) Timelike, spacelike, lightlike, and timelike, respectively.

12.2 Show that (a) simultaneous events have a spacelike interval; (b) copositional events have a timelike interval; $(c)$ the interval between two light flashes is lightlike if they are simultaneous to an observer who is present at the site of one of the flashes.


\begin{gather*}
\varepsilon(\Delta s)^{2}=0^{2}-\left(\Delta x^{1}\right)^{2}-\left(\Delta x^{2}\right)^{2}-\left(\Delta x^{3}\right)^{2}<0  \tag{a}\\
\varepsilon(\Delta s)^{2}=\left(\Delta x^{0}\right)^{2}-0>0 \tag{b}
\end{gather*}


$$
\varepsilon(\Delta s)-(\Delta x)-0<0
$$

(c) Let the observer measure the proximate flash as $E_{1}(0,0,0,0)$. The distant flash $E_{2}\left(c \Delta t, \Delta x^{1}, \Delta x^{2}, \Delta x^{3}\right)$ will be registered simultaneously, at $x^{0}=0$, if

$$
\Delta t=-\frac{\sqrt{\left(\Delta x^{1}\right)^{2}+\left(\Delta x^{2}\right)^{2}+\left(\Delta x^{3}\right)^{2}}}{c}
$$

But then $\varepsilon(\Delta s)^{2}=0$ and the interval is lightlike. (Note that the (negative) time coordinate of $E_{2}$ is calculated, not measured.)

\section*{THE LORENTZ GROUP}
12.3 Prove the following lemma involving the metric of SR, $g_{i j}$, as given by (12.6).

Lemma 12.4: If $C=\left(c_{i j}\right)$ is a symmetric $4 \times 4$ matrix such that $c_{i j} x^{i} x^{j}=0$ for all $\left(x^{i}\right)$ such that $g_{i j} x^{i} x^{j}=0$, there exists a fixed real number $\lambda$ for which $c_{i j}=\lambda g_{i j}(C=\lambda G)$.

Observe that the vector $(1, \pm 1,0,0)$ satisfies $g_{i j} x^{i} x^{j}=0$. Hence, substituting these components into the equation $c_{i j} x^{i} x^{j}=0$ yields

$$
c_{00} \pm c_{01} \pm c_{10}+c_{11}=0 \quad \text { or } \quad c_{00}+c_{11}=0=c_{01}=c_{10}
$$

(by symmetry of $C$ ). Similarly, using the vectors $(1,0, \pm 1,0)$ and $(1,0,0, \pm 1)$, we get

$$
c_{00}=-c_{11}=-c_{22}=-c_{33}=\lambda \quad c_{i j}=0 \quad(i=0 \text { or } j=0)
$$

Finally, employing the vectors $(\sqrt{2}, 1,1,0),(\sqrt{2}, 1,0,1)$, and $(\sqrt{2}, 0,1,1)$, we obtain $c_{12}=c_{13}=c_{23}=$ 0 .

12.4 Establish the transformation (12.7) between inertial frames under the postulates for SR.

From (12.6) and (12.5),

$$
g_{i j} x^{i} x^{j}=0=g_{i j} \bar{x}^{i} \bar{x}^{j}=g_{i j}\left(a_{r}^{i} x^{r}\right)\left(a_{s}^{j} x^{s}\right)=g_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}
$$

that is,


\begin{equation*}
g_{r s} a_{i}^{r} a_{j}^{s} x^{i} x^{j}=0 \quad \text { whenever } \quad g_{i j} x^{i} x^{j}=0 \tag{1}
\end{equation*}


Now apply Lemma 12.4 to (1), with $g_{r s} a_{i}^{r} a_{j}^{s}=c_{i j}$, where $C=\left(c_{i j}\right)=A^{T} G A$ is symmetric. We obtain


\begin{equation*}
g_{r s} a_{i}^{r} a_{j}^{s}=\lambda g_{i j} \quad \text { or } \quad A^{T} G A=\lambda G \tag{2}
\end{equation*}


It remains to show that $\lambda=1$. Since $G^{2}=I$, multiplication of (2) by the matrix $\lambda^{-1} G$ gives $\left(G\left(\lambda^{-1} A^{T}\right) G\right) A=I$, which shows that the inverse of $A$ is

\[
B=\frac{1}{\lambda} G A^{T} G=\left[\begin{array}{rrrr}
a_{0}^{0} / \lambda & -a_{0}^{1} / \lambda & -a_{0}^{2} / \lambda & -a_{0}^{3} / \lambda  \tag{3}\\
-a_{1}^{0} / \lambda & a_{1}^{1} / \lambda & a_{1}^{2} / \lambda & a_{1}^{3} / \lambda \\
-a_{2}^{0} / \lambda & a_{2}^{1} / \lambda & a_{2}^{2} / \lambda & a_{2}^{3} / \lambda \\
-a_{3}^{0} / \lambda & a_{3}^{1} / \lambda & a_{3}^{2} / \lambda & a_{3}^{3} / \lambda
\end{array}\right] \equiv\left[b_{j}^{i}\right]_{44}
\]

In particular, $b_{0}^{0}=a_{0}^{0} / \lambda$. Now since observers $O$ and $\bar{O}$ are receding from each other at constant velocity $v$ and are using identical measuring devices, it is clear that each views the other in the sam. way. It follows that $a_{0}^{0}=b_{0}^{0}$ and $\lambda=a_{0}^{0} / b_{0}^{0}=1$ (see Problem 12.5).

12.5 With reference to Problem 12.4, give a "thought-experiment" which leads to the conclusion that $a_{0}^{0}=b_{0}^{0}$.

Consider the motion of $O$ in $\bar{O}$ 's frame: Transform the point $(c t, 0,0,0)$ under $\mathscr{T}$ to get

$$
\bar{x}^{0}=c \bar{t}=a_{0}^{0} c t \quad \text { or } \quad \bar{t}=a_{0}^{0} t
$$

Thus, 1 second on $O$ 's clock is $a_{0}^{0}$ seconds on $\bar{O}$ 's; reciprocally, 1 second on $\bar{O}$ 's clock is $b_{0}^{0}$ seconds on $O$ 's. Thus, $a_{0}^{0}=b_{0}^{0}$.

12.6 Prove Theorem 12.1 from (12.7). [Note that Problem 12.4 did not make use of Theorem 12.1, so the proof will be logically correct.]

By (12.7), $\left(g_{i j}\right)$ is a covariant tensor under Lorentz transformations, so that $g_{i j} \Delta x^{i} \Delta x^{j}$ is an invariant (under Lorentz transformations).

12.7 Verify that the following matrix is Lorentz:

$$
\left[\begin{array}{cccc}
\sqrt{3} & \sqrt{2} & 0 & 0 \\
1 & \frac{\sqrt{6}}{2} & \frac{1}{2} & \frac{1}{2} \\
1 & \frac{\sqrt{6}}{2} & -\frac{1}{2} & -\frac{1}{2} \\
0 & 0 & -\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}
\end{array}\right]
$$

We verify directly the conditions $(12.7 c)$ :

$$
\begin{array}{cc}
(\sqrt{3})^{2}-1^{2}-1^{2}-0^{2}=3-2=1 & (\sqrt{2})^{2}-\left(\frac{\sqrt{6}}{2}\right)^{2}-\left(\frac{\sqrt{6}}{2}\right)^{2}-0^{2}=2-\frac{3}{2}-\frac{3}{2}=-1 \\
0^{2}-\left(\frac{1}{2}\right)^{2}-\left(-\frac{1}{2}\right)^{2}-\left( \pm \frac{\sqrt{2}}{2}\right)^{2}=-1 & (\sqrt{3})(\sqrt{2})-(1)\left(\frac{\sqrt{6}}{2}\right)-(1)\left(\frac{\sqrt{6}}{2}\right)-0=0 \\
(\sqrt{3})(0)-(1)\left(\frac{1}{2}\right)-(1)\left(-\frac{1}{2}\right)-(0)\left( \pm \frac{\sqrt{2}}{2}\right)=0 & (\sqrt{2})(0)-\left(\frac{\sqrt{6}}{2}\right)\left(\frac{1}{2}\right)-\left(\frac{\sqrt{6}}{2}\right)\left(-\frac{1}{2}\right)-(0)\left( \pm \frac{1}{2}\right)=0 \\
0-\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)+\left(\frac{1}{2}\right)\left(-\frac{1}{2}\right)+\left(\frac{\sqrt{2}}{2}\right)\left(\frac{\sqrt{2}}{2}\right)=0-\frac{1}{4}-\frac{1}{4}+\frac{1}{2}=0
\end{array}
$$

12.8 Show that a matrix $A$ which preserves $\mathbf{x}^{T} G \mathbf{x}=0$ necessarily preserves $\mathbf{x}^{T} G \mathbf{x}=q$.

This is really Problem 12.6 in another guise. By Problem 12.4, $A$ must satisfy $A^{T} G A=G$. But then

$$
(A \mathbf{x})^{T} G(A \mathbf{x})=\mathbf{x}^{T}\left(A^{T} G A\right) \mathbf{x}=\mathbf{x}^{T} G \mathbf{x}=q
$$

12.9 (a) Exhibit the inverse, $B$, of a given Lorentz matrix, $A$. (b) If we define a matrix $A$ to be pseudo-orthogonal when there exists a matrix $J$ whose square is the identity and $A^{T} J A=J$, show that all Lorentz matrices are pseudo-orthogonal.

(a) Set $\lambda=1$ in (3) of Problem 12.4.

(b) If $A$ is a Lorentz matrix, then $G$ clearly fills the role of $J$ in the definition of pseudo-orthogonal matrix.

12.10 Prove that the Lorentz matrices compose a group under matrix multiplication.

We are required to show that (a) the product of two Lorentz matrices is Lorentz, $(b)$ the inverse of a Lorentz matrix is Lorentz.

(a)

$$
(P Q)^{T} G(P Q)=Q^{T}\left(P^{T} G P\right) Q=Q^{T} G Q=G
$$

(b) Using Problem 12.4 with $\lambda=1, B=A^{-1}=G A^{T} G$, and

$$
B^{T} G B=\left(G A^{T} G\right)^{T} G B=G A G^{2} B=G A B=G
$$

\section*{SIMPLE LORENTZ MATRICES}
12.11 Derive the simple form (12.9) of the transformation equations for SR by considering how observers $O$ and $\bar{O}$ will view events occurring on a circular cylinder about their common $x$-axis.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-186}
\end{center}

Fig. 12-4

At any time $t$, let $E_{1}$ and $E_{2}$ be two events taking place at the points of space $(q, 1,0)$ and $(q, 0,1)$, respectively, which lie on a unit cylinder about $O$ 's $x$-axis (Fig. 12-4). Thus, with $p=c t$, we have space-time coordinates $E_{1}(p, q, 1,0)$ and $E_{2}(p, q, 0,1)$. Since the axes of $O$ are not turning with respect to $\bar{O}$ 's, these two events will be viewed by observer $\bar{O}$ as $E_{1}(\bar{p}, \bar{q}, 1,0)$ and $E_{2}\left(\bar{p}^{*}, \bar{q}^{*}, 0,1\right)$, respectively. The transformation equations (12.5) give:

$$
\text { (I) }\left\{\begin{array} { l } 
{ \overline { p } = a _ { 0 } ^ { 0 } p + a _ { 1 } ^ { 0 } q + a _ { 2 } ^ { 0 } } \\
{ \overline { q } = a _ { 0 } ^ { 1 } p + a _ { 1 } ^ { 1 } q + a _ { 2 } ^ { 1 } } \\
{ 1 = a _ { 0 } ^ { 2 } p + a _ { 1 } ^ { 2 } q + a _ { 2 } ^ { 2 } } \\
{ 0 = a _ { 0 } ^ { 3 } p + a _ { 1 } ^ { 3 } q + a _ { 2 } ^ { 3 } }
\end{array} \quad \text { (II) } \left\{\begin{array}{l}
\bar{p}^{*}=a_{0}^{0} p+a_{1}^{0} q+a_{3}^{0} \\
\bar{q}^{*}=a_{0}^{1} p+a_{1}^{1} q+a_{3}^{1} \\
0=a_{0}^{2} p+a_{1}^{2} q+a_{3}^{2} \\
1=a_{0}^{3} p+a_{2}^{3} q+a_{3}^{3}
\end{array}\right.\right.
$$

Observing just the last equation of (I) and the third equation of (II), we may, since $p$ and $q$ are arbitrary, take $p=q=0$, then $p=1, q=0$, and $p=0, q=1$. It follows that all six of the coefficients vanish: $a_{0}^{2}=a_{1}^{2}=a_{3}^{2}=a_{0}^{3}=a_{1}^{3}=a_{2}^{3}=0$. Using the third equation of (I) and the last equation of (II), we find that $a_{2}^{2}=a_{3}^{3}=1$. It follows that the last two equations of $\mathscr{T}$ reduce to $\bar{x}^{2}=x^{2}$ and $\bar{x}^{3}=x^{3}$. Now to concentrate on the first two: If $p=q=0$, then event $E_{1}$ is $(0,0,1,0)$-occurring when $t=\bar{t}=0$ at $x^{1}=0$, the instant when $\bar{x}^{1}=0$. That is, $\bar{p}=\bar{q}=0$, with the result $a_{2}^{0}=a_{2}^{1}=0$. Similarly, using $E_{2}$, $p=q=0$ implies $\bar{p}^{*}=\bar{q}^{*}=0$ and $a_{3}^{0}=a_{3}^{1}=0$.

12.12 Consider event $E_{1}$, a lightning flash at the point $(v, 0,0)$ at time $t=1 \mathrm{~s}$ in $O$ 's frame, and event $E_{2}$, a lightning flash at $(-v, 0,0)$ at time $\bar{t}=1 \mathrm{~s}$ in $\bar{O}$ 's frame. By determining the corresponding events in the opposing frames of reference, deduce (12.11).

Since at $t=1$ observer $\bar{O}$ has reached the point $(v, 0,0)$, the lightning strikes $\bar{O}$ 's origin at time $\bar{t}$. Hence, $E_{1}$ has coordinates $(c, v, 0,0)$ in $O$ and $(c \bar{t}, 0,0,0)$ in $\bar{O}$. Substituting these into $\mathscr{T}$ we obtain

$$
c \bar{t}=a c+b v \quad 0=d c+e v
$$

The second equation gives $d=-\beta e$.

Since $O$ has progressed backwards to the point $(-v, 0,0)$ in $\bar{O}$ at the time $\bar{t}=1$ at which $E_{2}$ occurs, this event has coordinates $(c t, 0,0,0)$ in $O$ and $(c,-v, 0,0)$ in $\bar{O}$. Substituting these into $\mathscr{T}$ yields

$$
c=a c t+b(0) \quad-v=d c t+e(0)
$$

which upon division give $d=-\beta a$. Hence, $a=e$.

12.13 Show that a $4 \times 4$ matrix is both Lorentz and orthogonal if and only if it has the form

\[
R=\left[\begin{array}{rrrr} 
\pm 1 & 0 & 0 & 0  \tag{1}\\
0 & r_{1} & s_{1} & t_{1} \\
0 & r_{2} & s_{2} & t_{2} \\
0 & r_{3} & s_{3} & t_{3}
\end{array}\right]
\]

where the $3 \times 3$ matrix $\left[\begin{array}{lll}\mathbf{r} & \mathbf{s} & \mathbf{t}\end{array}\right]$ is orthogonal.

A Lorentz matrix $A=\left(a_{j}^{i}\right)$ is also orthogonal if and only if its inverse $B$, as obtained in Problem 12.4 (with $\lambda=1$ ), is equal to $A^{T}$ and is itself orthogonal. This observation immediately yields the form (1).

\subsection*{12.14 Prove Theorem 12.2.}
Since $\|\mathbf{r}\|^{2}=b^{-2}\left[\left(a_{1}^{0}\right)^{2}+\left(a_{2}^{0}\right)^{2}+\left(a_{3}^{0}\right)^{2}\right]=b^{-2}\left[\left(a_{0}^{0}\right)^{2}-1\right]=1$ (using Problem 12.36), the matrix $\left[\begin{array}{lll}\mathrm{r} & \mathbf{s} & \mathbf{t}\end{array}\right]$ is orthogonal and $R_{2}^{T}$ has the form of the matrix in Problem 12.13, making it Lorentz and orthogonal. It follows that $R_{2}$ is orthogonal (and Lorentz), with $R_{2}^{-1}=R_{2}^{T}$; hence, $L=R_{1} L^{*} R_{2}$.

Now, as the product of Lorentz matrices, $R_{1}$ is Lorentz; to show it is orthogonal, consider $L R_{2}^{T}\left(L^{*}\right)^{-1}$, which may be written as

$$
\begin{aligned}
& {\left[\begin{array}{llll}
a_{0} & b_{0} & c_{0} & d_{0} \\
a_{1} & b_{1} & c_{1} & d_{1} \\
a_{2} & b_{2} & c_{2} & d_{2} \\
a_{3} & b_{3} & c_{3} & d_{3}
\end{array}\right]\left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & r_{1} & s_{1} & t_{1} \\
0 & r_{2} & s_{2} & t_{2} \\
0 & r_{3} & s_{3} & t_{3}
\end{array}\right]\left[\begin{array}{rrrr}
a & -b & 0 & 0 \\
-b & a & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-187}
\end{center}

[The omitted rows have the form $\left(a_{i}, b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}, b_{i} s_{1}+c_{i} s_{2}+d_{i} s_{3}, b_{i} t_{1}+c_{i} t_{2}+d_{i} t_{3}\right.$ ), with $i=1,2,3$.] We first concentrate on proving that the top row and first column of this product are $( \pm 1,0,0,0)$. The 00 -element of the product is

$$
a_{0} a+\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\right)(-b)=\varepsilon a_{0}^{2}+\frac{\varepsilon}{b}\left(b_{0}^{2}+c_{0}^{2}+d_{0}^{2}\right)(-b)=\varepsilon\left(a_{0}^{2}-b_{0}^{2}-c_{0}^{2}-d_{0}^{2}\right)=\varepsilon
$$

again using the fact that the transpose of a Lorentz matrix is Lorentz. The next element in the top row of the product is

$$
-a_{0} b+\left(b_{0} r_{1}+c_{0} r_{2}+d_{0} r_{3}\right) a=-a_{0} b+\frac{b}{\varepsilon}\left(r^{2}\right) \varepsilon a_{0}=-a_{0} b+b a_{0}=0
$$

For the third and fourth elements,

$$
b_{0} s_{1}+c_{0} s_{2}+d_{0} s_{3}=\frac{b}{\varepsilon} \mathbf{r s}=0 \quad \text { and } \quad b_{0} t_{1}+c_{0} t_{2}+d_{0} t_{3}=\frac{b}{\varepsilon} \mathbf{r t}=0
$$

Now for the first column of the product; its elements, beginning with the second, are (for $i=1,2,3$ )

$$
a_{i} a+\left(b_{i} r_{1}+c_{i} r_{2}+d_{i} r_{3}\right)(-b)=\varepsilon a_{i} a_{0}-\varepsilon\left(b_{i} b_{0}+c_{i} c_{0}+d_{i} d_{0}\right)=0
$$

Hence, the product matrix becomes

$$
R_{1}=\left[\begin{array}{llll}
\varepsilon & 0 & 0 & 0 \\
0 & & & \\
0 & & R & \\
0 & & &
\end{array}\right]
$$

and the $3 \times 3$ matrix $R$ must be orthogonal, since $R_{1}$ is Lorentz.

12.15 Apply Theorem 12.2 to the Lorentz matrix of Problem 12.7, and demonstrate the physical significance of this matrix by computing the velocity $v$ between the two observers involved.

We proceed to calculate $a, b$, and the vectors $\mathbf{r}, \mathbf{s}$, and $\mathbf{t}$ :

$$
a=\sqrt{3} \quad \varepsilon=1 \quad b=-\sqrt{3-1}=-\sqrt{2} \quad \mathbf{r}=-\frac{1}{\sqrt{2}}(\sqrt{2}, 0,0)=(-1,0,0)
$$

Hence, we may take $\mathbf{s}=(0,1,0)$ and $\mathbf{t}=(0,0,1)$, yielding

$$
R_{2}=\left[\begin{array}{rrrr}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

and

$$
\begin{aligned}
R_{1} & =\left[\begin{array}{cccc}
\sqrt{3} & \sqrt{2} & 0 & 0 \\
1 & \sqrt{6} / 2 & 1 / 2 & 1 / 2 \\
1 & \sqrt{6} / 2 & -1 / 2 & -1 / 2 \\
0 & 0 & -\sqrt{2} / 2 & \sqrt{2} / 2
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
\sqrt{3} & \sqrt{2} & 0 & 0 \\
\sqrt{2} & \sqrt{3} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right] \\
& =\left[\begin{array}{cccc}
\sqrt{3} & -\sqrt{2} & 0 & 0 \\
1 & -\sqrt{6} / 2 & 1 / 2 & 1 / 2 \\
1 & -\sqrt{6} / 2 & -1 / 2 & -1 / 2 \\
0 & 0 & -\sqrt{2} / 2 & \sqrt{2} / 2
\end{array}\right]\left[\begin{array}{cccc}
\sqrt{3} & \sqrt{2} & 0 & 0 \\
\sqrt{2} & \sqrt{3} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]=\left[\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & -\sqrt{2} / 2 & 1 / 2 & 1 / 2 \\
0 & -\sqrt{2} / 2 & -1 / 2 & -1 / 2 \\
0 & 0 & -\sqrt{2} / 2 & \sqrt{2} / 2
\end{array}\right]
\end{aligned}
$$

By Corollary 12.3,

$$
v=c \sqrt{1-(\sqrt{3})^{-2}}=\sqrt{\frac{2}{3}} c
$$

\section*{LENGTH CONTRACTION, TIME DILATION}
12.16 A pole-vaulter runs at the rate $(\sqrt{3} / 2) c$ (in $\mathrm{m} / \mathrm{s}$ ) and carries a pole that is $20 \mathrm{~m}$ long in his reference frame [the rest length of the pole is $20 \mathrm{~m}$ ]. He approaches a barn that is open at both ends and is $10 \mathrm{~m}$ long, as measured by a ground observer. To the ground observer, will the pole fit inside the barn? What is the pole-vaulter's conclusion?

To the ground observer, the pole undergoes length contraction with the factor $\sqrt{1-\beta^{2}}$, where $\beta=\sqrt{3} / 2$. Hence, the length of the pole in the frame of the ground observer is

$$
20 \sqrt{1-(\sqrt{3} / 2)^{2}}=10 \mathrm{~m}
$$

and so, for her, the pole exactly fits inside the barn (instantaneously). To the runner, however, the barn is $10(1 / 2)=5 \mathrm{~m}$ long, so that the $20-\mathrm{m}$ pole does not fit.

This example shows that order relations are not preserved under the Lorentz transformation.

12.17 (the Twin Paradox) One of a pair of twins embarks on a journey into outer space, taking one year (earth time) to accelerate to $(3 / 4) c$, then spends the next 20 years cruising to reach a galaxy 15 light-years away. An additional year is spend in decelerating in order to explore one of its solar systems. After one year of exploration $(\beta=0)$, the twin returns to earth by the same schedule-one year of acceleration, 20 years of cruising, and one year of deceleration. Estimate the difference in the ages of the twins after the journey has ended.

In order to apply SR, replace the four periods of acceleration or deceleration by four periods of uniform motion at speed (3/8)c (the time-average speed under constant acceleration). These account for 4 years by the earth clock; but to the space twin, who measures proper (shortest) time intervals, the time lapse is $(\beta=3 / 8)$

$$
4 \sqrt{1-(3 / 8)^{2}} \approx 3.71 \text { years }
$$

Similarly, the 40 earth-years of cruising at $\beta=3 / 4$ corresponds to a proper-time interval of

$$
40 \sqrt{1-(3 / 4)^{2}} \approx 26.46 \text { years }
$$

Thus, the space twin has aged $3.71+26.46+1 \approx 31$ years while the earth twin has aged $4+40+1=$ 45 years.

The space twin returns biologically younger by some 14 years. While the accelerations and decelerations between the two twins were reciprocal, the forces in the situation acted on the space twin alone.

12.18 Prove the basic integrity of (12.16) by solving algebraically for $v_{2}$ as a function of $v_{1}$ and $v_{3}$ to verify that $v_{2}$ follows the correct format for composition of velocities.

Solving,

$$
v_{2}=\frac{-v_{1}+v_{3}}{1-v_{1} v_{3} / c^{2}}
$$

which is precisely (12.16) under the substitution $\left(v_{1}, v_{2}, v_{3}\right) \rightarrow\left(-v_{1}, v_{3}, v_{2}\right)$.

12.19 A light source at $O$ sends a spherical wavefront (Fig. 12-5(a)) advancing in all directions at velocity $c$; it reaches the ends of a diameter $A B$ centered at $O$ simultaneously, as determined by $O$. But as far as $\bar{O}$ is concerned, the spherical wave, centered at $\bar{O}$, moves with him (invariance of the light cone) and therefore reaches point $B$ before it reaches point $A$. Calculate the time difference on $\bar{O}$ 's clock for these two events (light reaching $B$ and light reaching $A$ ) if $\beta=1 / 2$ and if $A B=6 \mathrm{~m}$.

Since $A B=6 \mathrm{~m}$ and $O$ is the midpoint of segment $A B, O$ assigns spatial coordinates $B(3,0,0)$ and $A(-3,0,0)$ to the endpoints. It takes $3 / c$ seconds for light to reach $A$ and $B$, so $O$ calculates the time coordinate as $x^{0}=c(3 / c)=3 \mathrm{~m}$. The space-time coordinates of the two events are thus

$$
E_{B}(3,3,0,0) \quad \text { and } \quad E_{A}(3,-3,0,0)
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189}
\end{center}

(a)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-189(1)}
\end{center}

(b)

Fig. 12-5

Substitute these values and $\beta=1 / 2$ into the first equation (12.13) to obtain $\bar{t}_{B}=\sqrt{3} / c, \bar{t}_{A}=3 \sqrt{3} / c$. Hence, $\Delta \bar{t}=2 \sqrt{3} / c$ (in s), while $\Delta t=0$.

It is seen that simultaneity is not an invariant of Lorentz transformations.

12.20 Derive the composition of velocities formula, (12.16).

According to Section 12.4, we must have $v_{i}=-b_{i} c / a_{i}$ for $i=1,2,3$. Composing the simple Lorentz transformations, we have

$$
\left[\begin{array}{cccc}
a_{1} & b_{1} & 0 & 0 \\
b_{1} & a_{1} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
a_{2} & b_{2} & 0 & 0 \\
b_{2} & a_{2} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]=\left[\begin{array}{cccc}
a_{1} a_{2}+b_{1} b_{2} & a_{1} b_{2}+a_{2} b_{1} & 0 & 0 \\
a_{2} b_{1}+a_{1} b_{2} & b_{1} b_{2}+a_{1} a_{2} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

whence $a_{3}=a_{1} a_{2}+b_{1} b_{2}, b_{3}=a_{1} b_{2}+a_{2} b_{1}$, and

$$
v_{3}=-\frac{\left(a_{1} b_{2}+a_{2} b_{1}\right) c}{a_{1} a_{2}+b_{1} b_{2}}=\frac{-\frac{a_{1} b_{2} c}{a_{1} a_{2}}-\frac{a_{2} b_{1} c}{a_{1} a_{2}}}{\frac{a_{1} a_{2}}{a_{1} a_{2}}+\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\frac{-\frac{b_{2} c}{a_{2}}-\frac{b_{1} c}{a_{1}}}{1+\frac{b_{1} b_{2}}{a_{1} a_{2}}}=\frac{v_{2}+v_{1}}{1+v_{1} v_{2} / c^{2}}
$$

12.21 A physicist wants to compose two equal velocities $v=v_{1}=v_{2}$ to produce a resultant velocity that is $90 \%$ of the velocity of light. What velocity must he use?

From (12.16),

$$
0.90 c=\frac{2 v}{1+v^{2} / c^{2}} \quad \text { or } \quad 0.90=\frac{2 \beta}{1+\beta^{2}}
$$

Solving the quadratic, $\beta \approx 0.627$ (as compared to the Newtonian value 0.45 ).

\section*{VELOCITY AND ACCELERATION IN RELATIVITY}
12.22 Establish (12.19) and (12.20), the Lorentz transformations of velocity and acceleration, that define how $\bar{O}$ tracks the motion of a particle in $O$ 's frame.

To simplify notation, let $\gamma \equiv\left(1-\beta^{2}\right)^{-1 / 2}$. Then $\mathscr{T}$ is

$$
c \bar{t}=\gamma(c t-\beta x) \quad \bar{x}=\gamma(-\beta c t+x) \quad \bar{y}=y \quad \bar{z}=z
$$

Differentiate the first equation with respect to $\bar{t}$ and use the chain rule:

$$
c=\gamma\left(c-\beta v_{x}\right) \frac{d t}{d \bar{t}} \quad \text { or } \quad \frac{d t}{d \bar{t}}=\frac{1}{\gamma\left(1-v v_{x} / c^{2}\right)}
$$

Now differentiate the last three equations:

$$
\begin{aligned}
& \bar{v}_{x}=\gamma\left(-\beta c+v_{x}\right) \frac{d t}{d \bar{t}}=\frac{\gamma\left(-v+v_{x}\right)}{\gamma\left(1-v v_{x} / c^{2}\right)}=\frac{v_{x}-v}{1-v_{x} v / c^{2}} \\
& \bar{v}_{y}=v_{y} \frac{d t}{d \bar{t}}=\frac{v_{y}}{\gamma\left(1-v v_{x} / c^{2}\right)}=\frac{v_{y} \sqrt{1-\beta^{2}}}{1-v_{x} v / c^{2}} \\
& \bar{v}_{z}=v_{z} \frac{d t}{d \bar{t}}=\frac{v_{z} \sqrt{1-\beta^{2}}}{1-v_{x} v / c^{2}}
\end{aligned}
$$

By differentiation of the velocity components just found,

$$
\begin{aligned}
\bar{a}_{x} & =\frac{d \bar{v}_{x}}{d t} \frac{d t}{d \bar{t}}=\frac{\left(a_{x}-0\right)\left(1-v_{x} v / c^{2}\right)-\left(v_{x}-v\right)\left(0-a_{x} v / c^{2}\right)}{\left(1-v_{x} v / c^{2}\right)^{2}} \frac{1}{\gamma\left(1-v_{x} v / c^{2}\right)} \\
& =\frac{a_{x}-a_{x} v_{x} v / c^{2}+v_{x} a_{x} v / c^{2}-a_{x} v^{2} / c^{2}}{\gamma\left(1-v_{x} v / c^{2}\right)^{3}}=\frac{a_{x}\left(1-\beta^{2}\right)^{3 / 2}}{\left(1-v_{x} v / c^{2}\right)^{3}}
\end{aligned}
$$

$$
\bar{a}_{y}=\frac{a_{y}\left(1-v_{x} v / c^{2}\right)-v_{y}\left(0-a_{x} v / c^{2}\right)}{\left(1-v_{x} v / c^{2}\right)^{2}} \frac{1-\beta^{2}}{1-v v_{x} / c^{2}}=\frac{a_{y}+\left(a_{x} v_{y}-v_{x} a_{y}\right)\left(v / c^{2}\right)}{\left(1-v_{x} / c^{2}\right)^{3}}\left(1-\beta^{2}\right)
$$

The formula for $\bar{a}_{z}$ is derived as that for $\bar{a}_{y}$, with $z$ replacing $y$ throughout.

12.23 Show that if the curve of motion in $O$ 's frame is the path of $\bar{O}$ itself, the clock in $\bar{O}$ 's frame (the clock moving with the particle) measures proper time.

By (3) of Problem 12.4,

$$
\begin{aligned}
x^{0} & =a_{0}^{0} \bar{x}^{0}-a_{0}^{1} \bar{x}^{1}-a_{0}^{2} \bar{x}^{2}-a_{0}^{3} \bar{x}^{3} \\
x^{i} & =-a_{i}^{0} \bar{x}^{0}+a_{i}^{1} \bar{x}^{1}+a_{i}^{2} \bar{x}^{2}+a_{i}^{3} \bar{x}^{3} \quad(i=1,2,3)
\end{aligned}
$$

Now the motion of $\bar{O}$ relative to itself is obviously $\bar{x}^{1}=\bar{x}^{2}=\bar{x}^{3}=0$. Hence,

$$
x^{0}=a_{0}^{0} c u \quad x^{1}=-a_{1}^{0} c u \quad x^{2}=-a_{2}^{0} c u \quad x^{3}=-a_{3}^{0} c u
$$

give the trajectory of $\bar{O}$ in $O$ 's frame, with parameter $u=\bar{t}$. Therefore, the tangent field to the trajectory is

$$
\left(\frac{d x^{i}}{d u}\right)=\left(a_{0}^{0} c,-a_{1}^{0} c,-a_{2}^{0} c,-a_{3}^{0} c\right)
$$

so that the proper time parameter for this curve is defined as

$$
\tau=\frac{1}{c} \int_{0}^{\bar{t}} \sqrt{\left|\left(a_{0}^{0} c\right)^{2}-\left(a_{1}^{0} c\right)^{2}-\left(a_{2}^{0} c\right)^{2}-\left(a_{3}^{0} c\right)^{2}\right|} d u=\sqrt{\left|\left(a_{0}^{0}\right)^{2}-\left(a_{1}^{0}\right)^{2}-\left(a_{2}^{0}\right)^{2}-\left(a_{3}^{0}\right)^{2}\right|} \int_{0}^{\bar{t}} d u
$$

Because the inverse transformation is Lorentz, the factor in front of the integral sign equals 1 , and so $\tau=\bar{t}$.

12.24 Derive the identities (12.25).

By (12.21),

$$
\begin{aligned}
u_{i} u^{i} & =g_{i j} u^{i} u^{j} \equiv\left(u^{0}\right)^{2}-\left(u^{1}\right)^{2}-\left(u^{2}\right)^{2}-\left(u^{3}\right)^{2} \\
& =\left[\left(v_{t}\right)^{2}-\left(v_{x}\right)^{2}-\left(v_{y}\right)^{2}-\left(v_{z}\right)^{2}\right]\left(\frac{d t}{d \tau}\right)^{2}=\left[c^{2}-\hat{v}^{2}\right] \frac{1}{1-\hat{v}^{2} / c^{2}}=c^{2}
\end{aligned}
$$

and from this,

$$
0=\frac{d}{d \tau}\left(c^{2}\right)=\frac{d}{d \tau}\left(u_{i} u^{i}\right)=2 u_{i} b^{i}
$$

12.25 Establish the formulas in (12.26).

$$
\begin{aligned}
u^{i} & =\frac{d x^{i}}{d \tau}=\frac{d x^{i}}{d t} \frac{d t}{d \tau}=\frac{v_{i}}{\sqrt{1-\hat{v}^{2} / c^{2}}} \\
b^{i} & =\frac{d u^{i}}{d \tau}=\left[\frac{d}{d t}\left(\frac{v_{i}}{\sqrt{1-\hat{v}^{2} / c^{2}}}\right)\right] \frac{d t}{d \tau} \\
& =\frac{a_{i}\left(1-\hat{v}^{2} / c^{2}\right)^{1 / 2}-v_{i}(1 / 2)\left(1-\hat{v}^{2} / c^{2}\right)^{-1 / 2}\left(-2 a_{x} v_{x}-2 a_{y} v_{y}-2 a_{z} v_{z}\right) / c^{2}}{1-\hat{v}^{2} / c^{2}} \frac{d t}{d \tau} \\
& =\frac{a_{i}\left(1-\hat{v}^{2} / c^{2}\right)+v_{i}\left(a_{x} v_{x}+a_{y} v_{y}+a_{z} v_{z}\right) / c^{2}}{\left(1-\hat{v}^{2} / c^{2}\right)^{2}}=\frac{a_{i}}{1-\hat{v}^{2} / c^{2}}+\frac{(\mathbf{v a}) v_{i}}{c^{2}\left(1-\hat{v}^{2} / c^{2}\right)^{2}}
\end{aligned}
$$

12.26 Derive the equation for uniformly accelerated motion along the $x$-axis of an inertial frame:

$$
x^{2}-c^{2} t^{2}=\frac{c^{4}}{\alpha^{2}}
$$

Let $\bar{O}$ be an instantaneous rest frame at some point $t_{1}$, and let $O$ be a given (stationary) frame in which the motion curve is traced. Since the motion is along the $x$-axis of $O$,

$$
v_{y}=v_{z}=a_{y}=a_{z}=0 \quad \text { and } \quad \bar{v}_{y}=\bar{v}_{z}=\bar{a}_{y}=\bar{a}_{z}=0
$$

whereby $\bar{a}_{x}=\alpha=$ const. (assuming $\bar{a}_{x}>0$ ). At $t=t_{1}, v=v_{x}$ (the constant velocity of $\bar{O}$ is by definition equal to the instantaneous velocity of the particle); thus, from (12.20),


\begin{equation*}
\alpha=\frac{a_{x}\left(1-v^{2} / c^{2}\right)^{3 / 2}}{\left(1-v_{x} v / c^{2}\right)^{3}}=\frac{a_{x}\left(1-v_{x}^{2} / c^{2}\right)^{3 / 2}}{\left(1-v_{x}^{2} / c^{2}\right)^{3}}=\frac{a_{x}}{\left(1-v_{x}^{2} / c^{2}\right)^{3 / 2}} \tag{1}
\end{equation*}


Since $t_{1}$ is arbitrary, (1) must hold for all $t$. Writing $\dot{x}, \ddot{x}$ for the derivatives of $x(t)$, we have from (1):


\begin{equation*}
c^{3} \ddot{x}=\alpha\left(c^{2}-\dot{x}^{2}\right)^{3 / 2} \tag{2}
\end{equation*}


Make the substitution $y=\dot{x}$ and (2) becomes


\begin{equation*}
c^{3} \frac{d y}{d t}=\alpha\left(c^{2}-y^{2}\right)^{3 / 2} \quad \text { or } \quad \int \frac{c^{3} d y}{\left(c^{2}-y^{2}\right)^{3 / 2}}=\int \alpha d t \tag{3}
\end{equation*}


Standard techniques of integration yield the first integral


\begin{equation*}
\frac{c y}{\sqrt{c^{2}-y^{2}}}=\alpha t \tag{4}
\end{equation*}


(where we have taken the initial velocity to be zero). Solving (4) for $y$ (assumed positive for positive $t$ ) and then integrating the equation $\dot{x}=y(t)$, we obtain

$$
x=c \sqrt{c^{2}+\alpha^{2} t^{2}} / \alpha \quad \text { or } \quad x^{2}-c^{2} t^{2}=c^{4} / \alpha^{2}
$$

(where we also take the initial position as zero). This is the desired equation, which represents a hyperbola in the $x t$ plane. By contrast, the Newtonian equation is the parabola $x=\frac{1}{2} \alpha t^{2}$.

\section*{RELATIVISTIC MASS, FORCE, AND ENERGY}
12.27 Show that the observed mass of a particle with rest mass $m$, moving at velocity $v$, is $\hat{m}=m\left(1-v^{2} / c^{2}\right)^{-1 / 2}$, by considering the following experiment. Let each observer $O$ and $\bar{O}$ carry a ball with rest mass $m$ near his origin and so situated as to collide obliquely at $t=\bar{t}=0$ (when their origins coincide). See Fig. 12-6. Suppose this collision imparts reciprocal velocities of $\varepsilon$ in the positive $x$-direction and negative $\bar{x}$-direction. Calculate the momentum of the system before and after collision (which is preserved), and what each observer sees based on the equations of SR; then take the limit as $\varepsilon \rightarrow 0$.

The velocity vectors $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$ of balls $B_{1}$ and $B_{2}$ before collision are, as seen by $O,(0,0,0)=\mathbf{0}$ and $(v, 0,0)=v$ i. Observer $\bar{O}$ calculates these vectors as $\overline{\mathbf{v}}_{1}=(-v, 0,0)$ and $\overline{\mathbf{v}}_{2}=(0,0,0)$ (either by

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192}
\end{center}

(a) Before impact

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-192(1)}
\end{center}

(b) After impact

Fig. 12-6\\
reciprocity or by use of (12.19). After collision, observer $O$ calculates the velocity of $B_{1}$ as $\mathbf{v}_{1}=$ $(\varepsilon, \delta, 0)=\varepsilon \mathbf{i}+\delta \mathbf{j}$, assuming $B_{1}$ has the proper alignment with $B_{2}$. Reciprocally, observer $\bar{O}$ calculates the velocity of $B_{2}$ as $\overline{\mathbf{v}}_{2}=(-\varepsilon,-\delta, 0)$. To find $\mathbf{v}_{2}$, use the inverse of (12.19), with $\bar{v}_{x}=-\varepsilon$ and $\bar{v}_{y}=-\delta$ :

$$
\mathbf{v}_{2}=v_{x} \mathbf{i}+v_{y} \mathbf{j}=\left(\frac{-\varepsilon+v}{1-\varepsilon v / c^{2}}\right) \mathbf{i}+\left(\frac{-\delta \sqrt{1-\beta^{2}}}{1-\varepsilon v / c^{2}}\right) \mathbf{j}
$$

Thus, observer $O$ calculates the net momentum vector of the system as follows, using the rest mass $m$ of $B_{1}$ for $m_{1}$ and the "perceived mass" $\hat{m}$ of $B_{2}$ for $m_{2}$ :

$$
\text { before impact } \quad \begin{aligned}
m_{1} \mathbf{v}_{1}+m_{2} \mathbf{v}_{2} & =m_{1}(\mathbf{0})+m_{2}(v \mathbf{i})=\hat{m} v \mathbf{i} \\
\text { after impact } \quad m_{1} \mathbf{v}_{1}+m_{2} \mathbf{v}_{2} & =m(\varepsilon \mathbf{i}+\delta \mathbf{j})+\hat{m}\left[\left(\frac{-\varepsilon+v}{1-\varepsilon v / c^{2}}\right) \mathbf{i}+\left(\frac{-\delta \sqrt{1-\beta^{2}}}{1-\varepsilon v / c^{2}}\right) \mathbf{j}\right] \\
& =\left(m \varepsilon+\hat{m} \frac{v-\varepsilon}{1-\varepsilon v / c^{2}}\right) \mathbf{i}+\left(m \delta-\hat{m} \frac{\delta \sqrt{1-\beta^{2}}}{1-\varepsilon v / c^{2}}\right) \mathbf{j}
\end{aligned}
$$

Since $O$ is using the universal laws of physics as they apply to his frame (Postulate 1 of SR), the two momentum vectors above must be the same. Hence,

$$
\hat{m} v=m \varepsilon+\hat{m} \frac{v-\varepsilon}{1-\varepsilon v / c^{2}} \quad \text { and } \quad 0=m-\hat{m} \frac{\sqrt{1-\beta^{2}}}{1-\varepsilon v / c^{2}}
$$

(after division by $\delta$ ). Now take the limit as $\varepsilon \rightarrow 0$ :

$$
\hat{m} v=\hat{m} v \quad \text { and } \quad 0=m-\hat{m} \sqrt{1-\beta^{2}}
$$

The right-hand equation is the connection between $m$ and $\hat{m}$.

12.28 Show that the Minkowski force is a 4-vector.

We must show that $\bar{K}^{i}=a_{j}^{i} K^{j}$, if $\bar{x}^{i}=a_{j}^{i} x^{j}$, where $\left(a_{j}^{i}\right)$ is any Lorentz matrix. Since $\tau$ is invariant and $a_{i}^{i}=$ const. we may differentiate the coordinate transformation with respect to $\tau$ across the equal sign:

$$
\frac{d}{d \tau}\left(\bar{x}^{i}\right)=\frac{d}{d \tau}\left(a_{j}^{i} x^{j}\right) \quad \text { or } \quad \bar{u}^{i}=a_{j}^{i} u^{j}
$$

(proving that $\left(u^{i}\right)$ is a 4-vector). Multiply both sides by $m$ and differentiate again, using the fact that the rest mass of a particle is invariant:

$$
\bar{K}^{i}=\frac{d}{d \tau}\left(\bar{m} \bar{u}^{i}\right)=\frac{d}{d \tau}\left(m \bar{u}^{i}\right)=\frac{d}{d \tau}\left(a_{j}^{i} m u^{j}\right)=a_{j}^{i} \frac{d}{d \tau}\left(m u^{j}\right)=a_{j}^{i} K^{j}
$$

\subsection*{12.29 Establish (12.32).}
Definition (12.30), $K^{i}=d\left(m u^{i}\right) / d \tau=m b^{i}$, along with the second identity (12.25), gives at once $u_{i} K^{i}=0$.

From $u_{i} K^{i}=g_{i j} u^{i} K^{j}=u^{0} K^{0}-u^{q} K^{q}=0$ and the first formula (12.26), we have

$$
\frac{v_{0} K^{0}}{\sqrt{1-\hat{v}^{2} / c^{2}}}-\frac{v_{q} K^{q}}{\sqrt{1-\hat{v}^{2} / c^{2}}}=0 \quad \text { or } \quad c K^{0}=\mathbf{v K}
$$

By (12.31) and $c K^{0}=\mathbf{v K}$,

$$
\frac{1}{c} \mathbf{v F}=\frac{1}{c} \mathbf{v K} \sqrt{1-\hat{v}^{2} / c^{2}}=K^{0} \sqrt{1-\hat{v}^{2} / c^{2}}=F_{0}
$$

Using the first definition (12.29),

$$
\mathbf{v F}=c F_{0}=\frac{d}{d t}\left(\frac{m c^{2}}{\sqrt{1-\hat{v}^{2} / c^{2}}}\right)
$$

12.30 Show that as $\hat{v} \rightarrow 0, \hat{E}=m c^{2}+\frac{1}{2} m \hat{v}^{2}+O\left(\hat{v}^{4} / c^{2}\right)$. Interpret this result.

The expression for relativistic energy, $\hat{E}=m c^{2}\left(1-\hat{v}^{2} / c^{2}\right)^{-1 / 2}$ may be expanded by the binomial theorem:

$$
(1+x)^{\alpha}=1+\alpha x+\frac{\alpha(\alpha-1)}{2 !} x^{2}+\cdots \quad(-1<x<1)
$$

The result is

$$
\hat{E}=m c^{2}+\frac{1}{2} m \hat{v}^{2}+\frac{3 m \hat{v}^{4}}{8 c^{2}}+\cdots
$$

Thus, at low speeds, the total energy of particle is very nearly the sum of its rest energy (which includes all sorts of potential energy) and its classical kinetic energy.

\section*{MAXWELL'S EQUATIONS IN SR}
12.31 Prove that $\bar{\square} \bar{f}=\square f$.

As the $g_{i j}$ are constants, $\square f \equiv g^{i j} f_{, i j}=$ invariant.

12.32 Prove that if $\left(F^{i j}\right)$ is any matrix of functions of the 3-vectors $\mathbf{U}$ and $\mathbf{V}$ such that $\partial F^{i j} / \partial x^{j}=$ $0 \quad(i=0,1,2,3)$ for all inertial frames and $F^{i j}(\mathbf{0}, \mathbf{0})=0$ for all $i, j$, where $\mathbf{0}=(0,0,0)$, then $\left(F^{i j}\right)$ is a second-order contravariant tensor under Lorentz transformations.

Let $\left(u_{i}\right)$ be any constant, covariant vector under Lorentz transformations [hence, $\left(\bar{u}_{i}\right)=\left(b_{i}^{k} u_{k}\right)$ is also constant]. Define

$$
S^{i} \equiv u_{k} F^{k i} \quad \bar{S}^{i} \equiv \bar{u}_{k} \bar{F}^{k i}
$$

By the given conditions $\partial \bar{F}^{i j} / \partial \bar{x}^{j}=0$,

$$
\frac{\partial \bar{S}^{i}}{\partial \bar{x}^{i}}=\bar{u}_{k} \frac{\partial \bar{F}^{k i}}{\partial \bar{x}^{i}}=0=\frac{\partial S^{i}}{\partial x^{i}}
$$

Suppose that at some point $\left(x_{0}^{i}\right), \bar{S}^{i}=h^{i}\left(S^{0}, S^{1}, S^{2}, S^{3}\right)$; then,

$$
\frac{\partial \bar{S}^{i}}{\partial \bar{x}^{i}}=0=\frac{\partial h^{i}}{\partial S^{j}} \frac{\partial S^{j}}{\partial x^{k}} \frac{\partial x^{k}}{\partial \bar{x}^{i}} \quad \text { or } \quad\left(b_{i}^{k} \frac{\partial h^{i}}{\partial S^{j}}\right) \frac{\partial S^{j}}{\partial x^{k}}=0
$$

for an arbitrary matrix $\left(\partial S^{j} / \partial x^{k}\right)$ having $\partial S^{i} / \partial x^{i}=0$. By a well-known lemma (Problem 12.57), there exists a real number $\lambda=\lambda\left(S^{0}, S^{1}, S^{2}, S^{3}\right)$ such that


\begin{equation*}
b_{i}^{k} \frac{\partial h^{i}}{\partial S^{j}}=\lambda \delta_{j}^{k} \tag{1}
\end{equation*}


Now differentiate both sides of (1) with respect to $S^{l}$ :


\begin{equation*}
b_{i}^{k} \frac{\partial^{2} h^{i}}{\partial S^{j} \partial S^{l}}=\frac{\partial \lambda}{\partial S^{l}} \delta_{j}^{k} \tag{2}
\end{equation*}


which is symmetrical in $j$ and $l$; therefore,


\begin{equation*}
\frac{\partial \lambda}{\partial S^{l}} \delta_{j}^{k}=\frac{\partial \lambda}{\partial S^{j}} \delta_{l}^{k} \tag{3}
\end{equation*}


for all $j, k, l$. Let $k=l \neq j$ in (3):

$$
\frac{\partial \lambda}{\partial S^{k}} \cdot 0=\frac{\partial \lambda}{\partial S^{j}} \cdot 1 \quad \text { or } \quad \frac{\partial \lambda}{\partial S^{j}}=0
$$

Hence $\lambda$ is constant with respect to the $S^{i}$ and (1) inverts to give


\begin{equation*}
\frac{\partial h^{i}}{\partial S^{j}}=\lambda a_{j}^{i} \tag{4}
\end{equation*}


Integrating (4),


\begin{equation*}
h^{i} \equiv \bar{S}^{i}=\lambda a_{j}^{i} S^{j}+T^{i} \tag{5}
\end{equation*}


For the special assignment $\mathbf{U}=\mathbf{V}=\mathbf{0}$, we have (since $\overline{\mathbf{0}}=\mathbf{0}$ ):


\begin{align*}
& S^{i}=\left(u_{1}\right)(0)+\left(u_{2}\right)(0)+\left(u_{3}\right)(0)+\left(u_{4}\right)(0)=0  \tag{6}\\
& \bar{S}^{i}=\left(\bar{u}_{1}\right)(0)+\left(\bar{u}_{2}\right)(0)+\left(\bar{u}_{3}\right)(0)+\left(\bar{u}_{4}\right)(0)=0
\end{align*}


Together, (5) and (6) imply $T^{i}=0 \quad(i=0,1,2,3)$; consequently,


\begin{equation*}
\bar{S}^{i}=\lambda a_{j}^{i} S^{j} \tag{7}
\end{equation*}


Similarly, there exists a real number $\mu$ such that


\begin{equation*}
S^{i}=\mu b_{j}^{i} \bar{S}^{j} \tag{8}
\end{equation*}


It follows that $\bar{S}^{i}=\lambda a_{i}^{i} \mu b_{k}^{j} \bar{S}^{k}=\lambda \mu \bar{S}^{i}$, or $\lambda \mu=1$. But we can exploit the reciprocal relationship between observers $O$ and $\bar{O}$, as in Problem 12.4; to show that $\lambda=\mu$. Therefore, $\lambda=\mu=1$ and (7) or (8) becomes the transformation law of a (contravariant) 4-vector. Finally, we conclude from the Quotient Theorem that if $F^{k i} u_{k} \equiv S^{i}$ is a tensor for an arbitrary covariant vector $\left(u_{i}\right),\left(F^{i j}\right)$ is a second-order contravariant tensor.

12.33 Prove the relations (12.40) and (12.41).

By (12.39) and the constancy of the $g_{i j}$,

$$
\frac{\partial f^{0 j}}{\partial x^{j}}=\frac{\partial f^{00}}{\partial x^{0}}+\frac{\partial f^{0 q}}{\partial x^{q}}=\frac{\partial}{\partial x^{q}}\left(-V^{q}\right)=-\frac{\partial V^{q}}{\partial x^{q}}=-\operatorname{div} \mathbf{V}
$$

and, for $p=1,2,3$,

$$
\frac{\partial f^{p j}}{\partial x^{j}}=\frac{\partial f^{p 0}}{\partial x^{0}}+\frac{\partial f^{p q}}{\partial x^{q}}=-\frac{\partial f^{0 p}}{\partial x^{0}}+\frac{\partial}{\partial x^{q}}\left(\varepsilon_{p q r} U^{r}\right)=\frac{\partial V^{p}}{\partial x^{0}}-\varepsilon_{p r q} \frac{\partial U^{r}}{\partial x^{q}}=\left(\frac{1}{c} \frac{\partial \mathbf{V}}{\partial t}+\operatorname{curl} \mathbf{U}\right)_{p}
$$

The other two formulas are derived from these by replacing $\mathbf{U}, \mathbf{V}$ by $\mathbf{V},-\mathbf{U}$.

\section*{Supplementary Problems}
12.34 Suppose two events consist of light signals, and an observer sends one of the signals himself. Classify the space-time interval between the events if the observer sees the distant light signal $(a)$ before he sends his own signal, (b) after he sends his own signal, (c) at the same time he sends his own signal.

12.35 Assuming that any velocity less than $c$ is attainable, suppose that a concert in Los Angeles begins at 8:0508 p.m. and one in New York City, 3000 miles away (consider this the accurate distance), begins at 8:0506 p.m. could a person physically attend both events (opening measures only)? Is the pair of events timelike or spacelike?

12.36 Show that the transpose of a Lorentz matrix is Lorentz.

12.37 Verify the expressions (12.12).

12.38 An event occurs at $\bar{O}$ 's origin at some time $\bar{t}$. (a) How does $O$ view this event? (b) What is the significance of $a_{0}^{0}>0$ ?

12.39 Write out the simple Lorentz transformation connecting inertial frames $O$ and $\bar{O}$ that move apart at $80 \%$ of the velocity of light.

12.40 (a) Confirm that a photon (a particle with the velocity of light in some inertial frame) will be viewed as having the velocity of light in all other inertial frames. (b) What must be the rest mass of such a particle?

12.41 Show that the following matrix is Lorentz, and use Theorem 12.2 to find the matrices $L^{*}, R_{1}$, and $R_{2}$, and the velocity $v$ between the two observers.

$$
L=\left[\begin{array}{crcr}
5 / 4 & 1 / 2 & 1 / 4 & -1 / 2 \\
-3 / 4 & -5 / 6 & -5 / 12 & 5 / 6 \\
0 & 2 / 3 & 2 / 15 & 11 / 15 \\
0 & -1 / 3 & 14 / 15 & 2 / 15
\end{array}\right]
$$

12.42 Verify that the following matrix is Lorentz and calculate the velocity between the two observers without finding the simple Lorentz matrix $L^{*}$.

$$
L=\left[\begin{array}{cccc}
3 / \sqrt{3} & 1 / \sqrt{3} & 2 / \sqrt{3} & -1 / \sqrt{3} \\
1 & 1 & 1 & 0 \\
1 & 0 & 1 & -1 \\
0 & 1 / \sqrt{3} & -1 / \sqrt{3} & -1 / \sqrt{3}
\end{array}\right]
$$

12.43 Show that by definition the proper-time parameter $\tau$ is an invariant with respect to all Lorentz transformations

12.44 Verify the formula for the composition of velocities by (i) multiplying the two simple Lorentz matrices below; (ii) calculating from (12.15) the velocities belonging to the two matrices and to their product; (iii) showing that the three velocities obey (12.16).

$$
L_{1}=\left[\begin{array}{cccc}
13 / 12 & 5 / 12 & 0 & 0 \\
5 / 12 & 13 / 12 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right] \quad L_{2}=\left[\begin{array}{cccc}
17 / 8 & -15 / 8 & 0 & 0 \\
-15 / 8 & 17 / 8 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$

12.45 An electron gun shoots particles in opposite directions at one-half the velocity of light. At what relative velocity are the particles receding from each other?

12.46 Show that the composition of two velocities less than $c$ is also less than $c$.

12.47 How slow would your watch run relative to a stationary clock if you were moving at $2 / 3$ the velocity of light?

12.48 At the age of 20 , an astronaut left her twin brother on earth to go exploring in outer space. The first two years the spaceship gradually accelerated to a cruising speed 95 percent of the velocity of light. Traveling at that speed for 25 years, it reached a distant galaxy (23.75 light years away) and then decelerated for two years. Two years were spent exploring the galaxy before the journey back home, which followed the schedule of the trip outward. How old is the astronaut when she rejoins her 80-year-old brother? (Use an average rate for clock-retardation during the 8 years in acceleration/deceleration.)

12.49 How fast would a pole-vaulter have to run for his 20 -foot pole to fit (instantaneously) inside a barn, in the judgment of a ground observer for whom the barn is 19 feet 11 inches long?

12.50 An alternate definition of uniformly accelerated motion is motion under a constant Lorentz force. Verify that the two definitions are equivalent for one-dimensional motion.

12.51 Show that $g^{r s} a_{r}^{i} a_{s}^{j}=g^{i j}$.

12.52 Prove that the array (12.46) is a 4-vector.

12.53 Show that the matrices $\tilde{\mathscr{F}}$ and $\mathscr{F}$ of (12.44) are connected via $\tilde{F}^{i j}=\frac{1}{2} e_{i j k l} g_{k r} g_{l s} F^{r s}$. [Hint: First evaluate the matrix product $G F G \equiv P$.]

12.54 Define Faraday's two-form by

$$
\Phi \equiv G \tilde{\mathscr{F}} G=\left[\begin{array}{cccc}
0 & -E_{1} & -E_{2} & -E_{3} \\
E_{1} & 0 & H_{3} & -H_{2} \\
E_{2} & -H_{3} & 0 & H_{1} \\
E_{3} & H_{2} & -H_{1} & 0
\end{array}\right]=\left[\Phi_{i j}\right]_{44}
$$

or, inversely, $\tilde{\mathscr{F}}=G \Phi G$. Show that $(a) \mathscr{F}$ is related to $\Phi$ through $F^{i j}=-\frac{1}{2} e_{i j k l} \Phi_{k l} ;(b)$ Maxwell's equations can be written in terms of the single matrix $\Phi$ as

$$
\frac{\partial \Phi_{i j}}{\partial x^{k}}+\frac{\partial \Phi_{k i}}{\partial x^{j}}+\frac{\partial \Phi_{j k}}{\partial x^{i}}=0 \quad g_{i k} g_{i l} \frac{\partial \Phi_{k l}}{\partial x^{j}}=s^{i}
$$

12.55 The energy flux in an electromagnetic field is specified by the Poynting vector, $\mathbf{p}=\mathbf{E} \times \mathbf{H}$. By direct matrix multiplication or otherwise, derive the formula

$$
\frac{1}{2}(\tilde{\mathscr{F}} \mathscr{F}-\tilde{F} \tilde{\mathscr{F}})=\left[\begin{array}{cccc}
0 & 0 & 0 & 0 \\
* & 0 & p_{3} & -p_{2} \\
* & * & 0 & p_{1} \\
* & * & * & 0
\end{array}\right] \text { (antisymmetric matrix) }
$$

12.56 Verify that for simple Lorentz matrices $A$ (hence, no rotation of axes allowed): (a) $\tilde{\mathscr{F}}(\mathbf{U}, \mathbf{V})=$ $G \mathscr{F}(\mathbf{V}, \mathbf{U}) G ;(b) \overline{\mathscr{F}}(\overline{\mathbf{V}}, \overline{\mathbf{U}})=B^{T} \mathscr{F}(\mathbf{V}, \mathbf{U}) B$, where $B=A^{-1} ;$ and $(c) \tilde{\mathscr{F}}(\overline{\mathbf{U}}, \overline{\mathbf{V}})=A \tilde{\mathscr{F}}(\mathbf{U}, \mathbf{V}) A^{T}$ (thereby proving that $\left(F^{i j}\right)$ is a contravariant tensor under simple Lorentz transformations).

12.57 Prove that if $A \equiv\left[A_{i j}\right]_{n n}$ satisfies $A_{i j} B_{i j}=0$ for every $B \equiv\left[B_{i j}\right]_{n n}$ that has zero trace $\left(B_{i i}=0\right)$, then $A=\lambda I$, for some real $\lambda$. [Hint: First take $B$ as having all elements zero, except for one off-diagonal element. Then choose $B_{\alpha \alpha}=-B_{\beta \beta}=1 \quad(\alpha \neq \beta ;$ no summation $)$, with all other $B_{i j}$ zero.]

\section*{Chapter 13}
\section*{Tensor Fields on Manifolds}
\subsection*{13.1 INTRODUCTION}
The modern, noncoordinate approach to tensors will be introduced as an important alternative to the coordinate-component approach employed exclusively in the previous chapters. This will entail somewhat more sophisticated mathematics.

\subsection*{13.2 ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}
Linear algebra provides a means of systematically studying the algebraic interplay between real numbers (scalars) and a wide variety of different types of objects (vectors). Vectors can be matrices, $n$-tuples of real numbers, functions, differential operators, etc. In this chapter, we shall adopt the convention of using uppercase boldface characters for sets (of points, of real numbers, of elements of a group, etc.), and lowercase boldface for vectors (as in the preceding chapters). However, the latter will be gradually phased out in favor of light uppercase characters, not only for easier reading, but also in conformity with notation used in many standard textbooks.

The concept of vector spaces requires a careful distinction between the scalars $a, b, c, \ldots$, and the objects of study (vectors), $\mathbf{u}, \mathbf{v}, \mathbf{w}, \ldots$ We shall always identify the scalars with the field of real numbers, although any field could serve for the construction of an abstract vector space.

\section*{Algebraic Properties of a Vector Space}
In terms of two binary operations, the axioms for a vector space are as follows.

\section*{Addition Axioms}
\begin{enumerate}
  \item $\mathbf{u}+\mathbf{v}$ is always a vector

  \item $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$

  \item $(\mathbf{u}+\mathbf{v})+\mathbf{w}=\mathbf{u}+(\mathbf{v}+\mathbf{w})$

  \item There is a vector $\mathbf{0}$ such that $\mathbf{u}+\mathbf{0}=\mathbf{u}$.

  \item For each $\mathbf{u}$ there is a vector $-\mathbf{u}$ such that $\mathbf{u}+(-\mathbf{u})=\mathbf{0}$.

\end{enumerate}

\section*{Scalar Multiplication Axioms}
\begin{enumerate}
  \setcounter{enumi}{5}
  \item $a \cdot \mathbf{u} \equiv a \mathbf{u}$ is always a vector
  \item $a(\mathbf{u}+\mathbf{v})=a \mathbf{u}+a \mathbf{v}$
  \item $(a+b) \mathbf{u}=a \mathbf{u}+b \mathbf{u}$
  \item $(a b) \mathbf{u}=a(b \mathbf{u})$
  \item $1 \mathbf{u}=\mathbf{u}$
\end{enumerate}

EXAMPLE 13.1 We give notation for four familiar vector spaces.

(a) $\mathbf{R}^{n} \equiv$ the $n$-tuples of reals under componentwise addition and scalar multiplication.

(b) $\quad \mathbf{P}^{n} \equiv$ the polynomials (in a variable $t$ ) of degree $n$ or less. If $p(t) \equiv a_{i} t^{i}, q(t) \equiv b_{i} t^{i}$, let $p(t)+q(t)=$ $\left(a_{i}+b_{i}\right) t^{i}$ and $r \cdot p(t)=\left(r a_{i}\right) t^{i}$.

(c) $\quad C^{k}(\mathbf{R}) \equiv$ the continuously $k$-times differentiable functions (of $t$ ), $f \vdots \mathbf{R} \rightarrow \mathbf{R}$ (mapping the reals into the reals). To define + and $\cdot$, write $f(t)+g(t)=(f+g)(t)$ and $r \cdot f(t)=(r f)(t)$.

(d) $\mathbf{M}^{n}(\mathbf{R}) \equiv$ the $n \times n$ matrices over $\mathbf{R}$. If $A=\left(a_{i j}\right)$ and $B=\left(b_{i j}\right)$, addition and scalar multiplication are defined by $A+B=\left(a_{i j}+b_{i j}\right)$ and $r A=\left(r a_{i j}\right)$.

\section*{Algebraic Properties of a Group}
Axioms 1-5 make a vector space an abelian (commutative) group under addition. In the general definition of a group, the binary operation is designated as "multiplication" and the commutative requirement is dropped.

\section*{Multiplication Axioms}
\begin{enumerate}
  \item $u v$ belongs to the group.

  \item $(u v) w=u(v w)$.

  \item There is an identity element $e$ such that $e u=u e=u$.

  \item For each $u$ there is an inverse element $u^{-1}$ such that $u u^{-1}=u^{-1} u=e$.

\end{enumerate}

EXAMPLE 13.2 Some frequently encountered groups follow.

(a) The reals $\mathbf{R}$ over ordinary addition; the reals over ordinary multiplication if 0 is removed from the set.

(b) The cube roots of unity, $\mathbf{C}^{3}=\left\{1, \omega, \omega^{2}\right\}$, over ordinary multiplication of complex numbers, where $\omega=\frac{1}{2}\left(-1+i \sqrt{3}\right.$ ). Groups of this type are called cyclic and are generally denoted by $\mathbf{C}^{k}$ (the cyclic group of order $k$ ). $\mathbf{C}^{k}$ is necessarily abelian.

(c) The 4-group $\{e, u, s, b\}$, under the rules $u^{2}=s^{2}=b^{2}=e, b=u s$, and the associative law of multiplication. The 4-group is abelian, but it is not equivalent to the cyclic group on four elements, $\mathbf{C}^{4}$.

(d) $\mathbf{M}^{n}(\mathbf{R})$, under matrix addition.

(e) $\mathbf{G L}(n, \mathbf{R}) \equiv$ the real, nonsingular $n \times n$ matrices under matrix multiplication; this is the general linear group (nonabelian). GL $(n, \mathbf{R})$ contains many very important smaller groups (called subgroups). Some of these are: $\mathbf{S L}(n, \mathbf{R}) \equiv$ the real $n \times n$ matrices with determinant $+1 ; \mathbf{S O}(n) \equiv$ the $n \times n$ orthogonal matrices; and $\mathbf{L}(n) \equiv$ the $n \times n$ Lorentz matrices [see the definition of $\mathbf{L}(4)$ in Section 12.3].

(f) $\mathbf{G L}(n, \mathbf{C}) \equiv$ the complex, nonsingular $n \times n$ matrices under matrix multiplication. An important subgroup is the unitary group, $\mathbf{U}(n)$, consisting of all $n \times n$ Hermitian matrices (such that $A^{-1}=\bar{A}^{T}$, where the bar denotes complex conjugation).

\subsection*{13.3 IMPORTANT CONCEPTS FOR VECTOR SPACES}
Basis

A basis for a vector space is a maximal, linearly independent set of vectors $\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots$ If this set is finite, possessing $n$ elements, the vector space is finite-dimensional, of dimension $n$. Otherwise, the space is said to be infinite-dimensional.

EXAMPLE 13.3 (1) It is obvious that a basis for $\mathbf{R}^{n}$ is the set of vectors

$$
\mathbf{e}_{1}=(1,0,0, \ldots, 0), \quad \mathbf{e}_{2}=(0,1,0, \ldots, 0), \ldots, \quad \mathbf{e}_{n}=(0,0, \ldots, 0,1)
$$

called the standard basis. (2) $\mathbf{P}^{n}$ is finite-dimensional, of dimension $n+1$; one basis is $\left\{t^{i}\right\}, 0 \leqq i \leqq n$. (3) The vector space of all polynomials is infinite-dimensional, as is the vector space $C^{k}(\mathbf{R})$. See Problem 13.4.

\section*{Isomorphisms, Linear Mappings}
Two mathematical systems of the same type (such as two vector spaces or two groups) are called isomorphic if they are structurally identical and differ only in nomenclature. In the case of two vector spaces, an isomorphism is a one-to-one (bijective) linear mapping $\varphi$ from one space to the other, where the term linear refers to the properties (for all vectors $\mathbf{u}, \mathbf{v}$, and scalars $a$ ):


\begin{equation*}
\varphi(\mathbf{u}+\mathbf{v})=\varphi(\mathbf{u})+\varphi(\mathbf{v}) \quad \text { and } \quad \varphi(a \mathbf{u})=a \varphi(\mathbf{u}) \tag{13.1}
\end{equation*}


For groups, an isomorphism would be a bijection $\psi$ with the property $\psi(u v)=\psi(u) \psi(v)$, for all elements of the group. A more general mapping that is important for groups is a homomorphism, which merely requires that $\psi(u v)=\psi(u) \psi(v)$ for all $u$ and $v$, without necessarily requiring one-tooneness.

\section*{Product of Vector Spaces}
If $\mathbf{U}$ and $\mathbf{V}$ are any two vector spaces, the ordinary cartesian product $\mathbf{U} \times \mathbf{V}$, the set of ordered pairs $(\mathbf{u}, \mathbf{v})$ with $\mathbf{u}$ in $\mathbf{U}$ and $\mathbf{v}$ in $\mathbf{V}$, can be made into a vector space by defining addition and scalar multiplication of pairs via

$$
(\mathbf{p}, \mathbf{q})+(\mathbf{r}, \mathbf{s})=(\mathbf{p}+\mathbf{r}, \mathbf{q}+\mathbf{s}) \quad \text { and } \quad a(\mathbf{p}, \mathbf{q})=(a \mathbf{p}, a \mathbf{q})
$$

Such a product space is denoted $\mathbf{U} \otimes \mathbf{V}$; if $\mathbf{U}=\mathbf{V}$, write $\mathbf{U} \otimes \mathbf{V}$ as $\mathbf{U}^{2}$. More generally, the product of any number of vector spaces $\mathbf{V}_{1}, \mathbf{V}_{2}, \ldots, \mathbf{V}_{k}$ may be easily defined as above; this product is denoted $\mathbf{V}_{1} \otimes \mathbf{V}_{2} \otimes \mathbf{V}_{3} \otimes \cdots \otimes \mathbf{V}_{k}$. If $\mathbf{V}_{1}=\mathbf{V}_{2}=\cdots=\mathbf{V}_{k}=\mathbf{V}$, the product space is written $\mathbf{V}^{k}$. (This notation is also often used for the tensor product of two vector spaces, a concept which will not be treated here.)

\subsection*{13.4 THE ALGEBRAIC DUAL OF A VECTOR SPACE}
If a vector space $\mathbf{V}$ be mapped linearly into the reals $\mathbf{R}$, satisfying (13.1), the mapping is called a linear functional, or one-form. As in Example 13.1(c), we can make the set of all linear functionals on $\mathbf{V}$ into a vector space itself, with the zero functional as that mapping which takes every vector in $\mathbf{V}$ into 0 in $\mathbf{R}$.

Definition 1: The algebraic dual of a vector space $\mathbf{V}$ is the set $\mathbf{V}^{*}$ of all linear functionals made into a vector space under ordinary pointwise addition and scalar multiplication:

$$
(f+g)(\mathbf{v})=f(\mathbf{v})+g(\mathbf{v}) \quad(\lambda f)(\mathbf{v})=\lambda f(\mathbf{v})
$$

Since any linear functional on $\mathbf{R}^{n}$ can be expressed as a linear function of the coordinates,

$$
\mathbf{v}=v^{1} \mathbf{e}_{1}+v^{2} \mathbf{e}_{2}+\cdots+v^{n} \mathbf{e}_{n} \rightarrow f(\mathbf{v})=a_{1} v^{1}+a_{2} v^{2}+\cdots+a_{n} v^{n}
$$

where $a_{i}=f\left(\mathbf{e}_{i}\right)$ for each $i$, the functional is completely determined by the $n$-tuple $\left(a_{1}, a_{2}, \ldots, a_{n}\right)$.

\section*{Differential Notation: One-Forms}
Thus, different functionals correspond to different $n$-tuples, as

$$
f \leftrightarrow\left(a_{1}, a_{2}, \ldots, a_{n}\right) \quad g \leftrightarrow\left(b_{1}, b_{2}, \ldots, b_{n}\right) \quad \ldots
$$

and it has become customary to represent linear functionals by the compact notation of one-forms:

$$
\boldsymbol{\omega}=a_{1} d x^{1}+a_{2} d x^{2}+\cdots+a_{n} d x^{n} \quad \boldsymbol{\sigma}=b_{1} d x^{1}+b^{2} d x^{2}+\cdots+b_{n} d x^{n} \quad \cdots
$$

But why $d x^{i}$ for the coordinates? The motivation comes from differential geometry. Recall that any class $C^{1}$ multivariate function $F\left(x^{1}, x^{2}, \ldots, x^{n}\right)$ on $\mathbf{R}^{n}$ has the gradient $\nabla F=\left(\partial F / \partial x^{i}\right)$ and the directional derivative (in the direction $\left(d x^{1}, d x^{2}, \ldots, d x^{n}\right)$ )

$$
d F=\frac{\partial F}{\partial x^{1}} d x^{1}+\frac{\partial F}{\partial x^{2}} d x^{2}+\cdots+\frac{\partial F}{\partial x^{n}} d x^{n}
$$

which, at a specific point in space, is a one-form that defines a linear functional on $\mathbf{R}^{n}$ (i.e., the set of all directions). Recall too that, just as in ordinary one-dimensional calculus,

$$
d x=\Delta x \equiv \text { an unspecified real number }
$$

not necessarily small.

EXAMPLE 13.4 (a) In $\mathbf{R}^{3}$, find the image of $\mathbf{v}=(1,3,5)$ under the one-forms (linear functionals)

$$
\boldsymbol{\omega}=4 d x^{1}-d x^{2} \quad \boldsymbol{\sigma}=2 d x^{1}+3 d x^{2}-d x^{3} \quad \boldsymbol{\omega}+\boldsymbol{\sigma}=6 d x^{1}+2 d x^{2}-d x^{3}
$$

(b) What is the relationship among $\boldsymbol{\omega}(\mathbf{v}), \boldsymbol{\sigma}(\mathbf{v})$, and $(\boldsymbol{\omega}+\boldsymbol{\sigma})(\mathbf{v})$ ?\\
(a)


\begin{align*}
& \boldsymbol{\omega}(\mathbf{v})=4 \cdot 1-1 \cdot 3+0 \cdot 5=4-3+0=1 \\
& \boldsymbol{\sigma}(\mathbf{v})=2 \cdot 1+3 \cdot 3-1 \cdot 5=2+9-5=6 \\
& (\boldsymbol{\omega}+\boldsymbol{\sigma})(\mathbf{v})=6 \cdot 1+2 \cdot 3-1 \cdot 5=6+6-5=7 \\
& \quad \boldsymbol{\omega}(\mathbf{v})+\boldsymbol{\sigma}(\mathbf{v})=1+6=7=(\boldsymbol{\omega}+\boldsymbol{\sigma})(\mathbf{v}) \tag{b}
\end{align*}


For vector spaces different from $\mathbf{R}^{n}$ we agree to use the procedure of Example 13.4 on the components of vectors relative to an arbitrary basis. That is, to evaluate the image of $\mathbf{v}=$ $v^{1} \mathbf{b}_{1}+v^{2} \mathbf{b}_{2}+\cdots+v^{n} \mathbf{b}_{n} \equiv v^{i} \mathbf{b}_{i}$ under the one-form $\boldsymbol{\omega}=a_{i} d x^{i}$, simply write


\begin{equation*}
\boldsymbol{\omega}(\mathbf{v})=\boldsymbol{\omega}\left(v^{j} \mathbf{b}_{j}\right) \equiv\left(a_{i} d x^{i}\right)\left(v^{j} \mathbf{b}_{j}\right)=a_{i} v^{i} \tag{13.2}
\end{equation*}


A dual reading of (13.2) gives a better understanding of the relationship between $\mathbf{V}$ and $\mathbf{V}^{*}$ (between vectors and one-forms). If we regard the $a_{i}$ as fixed (tantamount to fixing a basis in $\mathbf{V}$ ) while the $v^{i}$ vary-the "normal" situation-then a linear map from $\mathbf{V}$ to $\mathbf{R}$ is uniquely defined. If, on the other hand, the vector components $v^{i}$ are held fixed and the coefficients $a_{i}$ are allowed to vary (this amounts to fixing a basis in $\mathbf{V}^{*}$ ), a linear map from $\mathbf{V}^{*}$ to $\mathbf{R}$ is defined (the latter map is actually an element of the space $\mathrm{V}^{* *}$ ). The expression $a_{i} v^{i}$ is bilinear in the two vector variables $\mathrm{v}$ and $\omega$.

Theorem 13.1: If $\mathbf{V}$ is a finite-dimensional vector space, then $\mathrm{V}^{*}$ is finite-dimensional, of the same dimension, and is isomorphic to $\mathbf{V}$.

A proof is given in Problem 13.6.

\section*{Dual Basis}
$A$ basis $\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}$ for $\mathbf{V}$ determines one for the dual space $\mathbf{V}^{*}$ in a very natural way. Each $\mathbf{v}$ in $\mathbf{V}$ has a representation $\mathbf{v}=v^{j} \mathbf{b}_{j}$ and thus defines a linear functional


\begin{equation*}
\varphi(\mathbf{v})=v^{1} d x^{1}+v^{2} d x^{2}+\cdots+v^{n} d x^{n} \tag{13.3}
\end{equation*}


Then the $n$ linear functionals (vectors in $\mathbf{V}^{*}$ ) defined by


\begin{equation*}
\varphi\left(\mathbf{b}_{i}\right) \equiv \boldsymbol{\beta}^{i} \quad(i=1,2, \ldots, n) \tag{13.4a}
\end{equation*}


form a basis for $\mathbf{V}^{*}$ (see Problem 13.6); we say that the basis $\left\{\boldsymbol{\beta}^{i}\right\}$ in $\mathbf{V}^{*}$ is the dual of the basis $\left\{\mathbf{b}_{i}\right\}$ in $\mathbf{V}$. The evaluation rule (13.2) provides a simpler characterization of the dual basis:


\begin{equation*}
\boldsymbol{\beta}^{i}(\mathbf{v})=\left(\varphi\left(\mathbf{b}_{i}\right)\right)\left(v^{j} \mathbf{b}_{j}\right)=\left(0 \cdot d x^{1}+0 \cdot d x^{2}+\cdots+1 \cdot d x^{i}+\cdots+0 \cdot d x^{n}\right)\left(v^{j} \mathbf{b}_{j}\right)=v^{i} \tag{13.4b}
\end{equation*}


Thus, $\boldsymbol{\beta}^{i}=d x^{i}$ is the linear functional that picks out the $i$ th component relative to $\left\{\mathbf{b}_{k}\right\}$ of any vector in V. A special application of (13.4b) gives


\begin{equation*}
\boldsymbol{\beta}^{i}\left(\mathbf{b}_{j}\right)=\delta_{j}^{i} \tag{13.5}
\end{equation*}


for all $i, j$.

EXAMPLE 13.5 The standard basis $\mathbf{e}=\left\{\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{n}\right\}$ for $\mathbf{R}^{n}$ generates the standard basis for $\left(\mathbf{R}^{n}\right)^{*}$, given in terms of one-forms as

$$
\boldsymbol{\beta}^{1}(\mathbf{e})=d x^{1} \quad \boldsymbol{\beta}^{2}(\mathbf{e})=d x^{2} \quad \ldots \quad \boldsymbol{\beta}^{n}(\mathbf{e})=d x^{n}
$$

Suppose, then, that $\mathbf{R}^{3}$ is referred to the (nonstandard) basis

$$
b_{1}=(1,1,0) \quad b_{2}=(1,0,1) \quad b_{3}=(0,1,1)
$$

This may be written in terms of the standard basis $\left\{\mathbf{e}_{i}\right\}$ through a formal matrix multiplication:

$$
\left[\begin{array}{l}
\mathbf{b}_{1} \\
\mathbf{b}_{2} \\
\mathbf{b}_{3}
\end{array}\right]=\left[\begin{array}{lll}
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1
\end{array}\right]\left[\begin{array}{l}
\mathbf{e}_{1} \\
\mathbf{e}_{2} \\
\mathbf{e}_{3}
\end{array}\right]
$$

Express the dual basis $\left\{\boldsymbol{\beta}^{i}\right\}$ for $\left(\mathbf{R}^{3}\right)^{*}$ in terms of its standard basis $\left(d x^{i}\right)$ as a similar matrix product.

Let $\boldsymbol{\beta}^{i}=a_{1}^{i} d x^{1}+a_{2}^{i} d x^{2}+a_{3}^{i} d x^{3}$; we must solve for the components $a_{j}^{i}$. For $i=1$, we have from (13.5):

$$
\begin{array}{r}
\boldsymbol{\beta}^{1}\left(\mathbf{b}_{1}\right)=\boldsymbol{\beta}^{1}(1,1,0)=a_{1}^{1} \cdot 1+a_{2}^{1} \cdot 1+a_{3}^{1} \cdot 0=a_{1}^{1}+a_{2}^{1} \equiv x+y=1 \\
\boldsymbol{\beta}^{1}\left(\mathbf{b}_{2}\right)=\boldsymbol{\beta}^{1}(1,0,1)=x \cdot 1+y \cdot 0+z \cdot 1=x+z=0 \\
\boldsymbol{\beta}^{1}\left(\mathbf{b}_{3}\right)=\boldsymbol{\beta}^{1}(0,1,1)=x \cdot 0+y \cdot 1+z \cdot 1=y+z=0
\end{array}
$$

(where $x \equiv a_{1}^{1}, y \equiv a_{2}^{1}, z \equiv a_{3}^{1}$ ). Solving, $x=\frac{1}{2}=y, z=-\frac{1}{2}$. A similar analysis may be used to determine the $a_{j}^{2}$ and $a_{j}^{3}$. The final result is

$$
\begin{aligned}
& \boldsymbol{\beta}^{1}=\frac{1}{2} d x^{1}+\frac{1}{2} d x^{2}-\frac{1}{2} d x^{3} \\
& \boldsymbol{\beta}^{2}=\frac{1}{2} d x^{1}-\frac{1}{2} d x^{2}+\frac{1}{2} d x^{3} \\
& \boldsymbol{\beta}^{3}=-\frac{1}{2} d x^{1}+\frac{1}{2} d x^{2}+\frac{1}{2} d x^{3}
\end{aligned} \quad \text { or } \quad\left[\begin{array}{l}
\boldsymbol{\beta}^{1} \\
\boldsymbol{\beta}^{2} \\
\boldsymbol{\beta}^{3}
\end{array}\right]=\left[\begin{array}{rrr}
\frac{1}{2} & \frac{1}{2} & -\frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\
-\frac{1}{2} & \frac{1}{2} & \frac{1}{2}
\end{array}\right]\left[\begin{array}{l}
d x^{1} \\
d x^{2} \\
d x^{3}
\end{array}\right]
$$

Observe that the two basis-connecting matrices are formal inverses of each other.

\section*{Change of Basis in $\mathbf{V}$ and $\mathbf{V}^{*}$}
The result of Example 13.5 can be generalized. Let $\left\{\mathbf{b}_{i}\right\}$ and $\left\{\overline{\mathbf{b}}_{i}\right\}$ be two bases for $\mathbf{V}$, and let $\left\{\boldsymbol{\beta}^{i}\right\}$ and $\left\{\overline{\boldsymbol{\beta}}^{i}\right\}$ be the respective dual bases for $\mathbf{V}^{*}$. Then


\begin{equation*}
\overline{\mathbf{b}}_{i}=A_{i}^{j} \mathbf{b}_{j} \rightarrow \overline{\boldsymbol{\beta}}^{i}=\bar{A}_{j}^{i} \boldsymbol{\beta}^{j} \quad \text { with } \quad\left(\bar{A}_{j}^{i}\right)=\left(A_{j}^{i}\right)^{-1} \tag{13.6}
\end{equation*}


the arrow denoting implication. (See Problem 13.7.)

\subsection*{13.5 TENSORS ON VECTOR SPACES}
The concept of a multilinear functional is needed: If $f\left(\mathbf{v}^{1}, \mathbf{v}^{2}, \ldots, \mathbf{v}^{m}\right)$ represents a mapping of $m$ vector variables into the reals such that the restricted mapping obtained by holding all but one of the variables fixed is a linear functional, then $f$ is said to be multilinear in all its variables.

Definition 2: A type- $\left(\begin{array}{c}p \\ q\end{array}\right)$ tensor is any multilinear functional $T:\left(\mathbf{V}^{*}\right)^{p} \otimes \mathbf{V}^{q} \rightarrow \mathbf{R}$ mapping $p$ one-forms and $q$ vectors into the reals; the real image is denoted

$$
T\left(\boldsymbol{\omega}^{1}, \ldots, \boldsymbol{\omega}^{p} ; \mathbf{v}^{1}, \ldots, \mathbf{v}^{q}\right)
$$

EXAMPLE 13.6 Let $T$ represent a linear functional in what follows. A type-(1) tensor takes on real values $T(\boldsymbol{\omega})$ for all one-forms $\boldsymbol{\omega}$ as argument. As we shall see later, such a tensor can be identified with a contravariant vector. A type- $\left(\begin{array}{l}0 \\ 1\end{array}\right)$ tensor takes on real values $T(\mathbf{v})$ for all vectors $\mathbf{v}$ as argument; it can be shown to correspond to a covariant vector. A type- $\left(\begin{array}{c}1 \\ 1\end{array}\right)$ tensor takes on real values $U(\boldsymbol{\omega} ; \mathbf{v})$ for all ordered pairs in $\mathbf{V}^{*} \otimes \mathbf{V}$ as argument, with $U$ a bilinear functional.

EXAMPLE 13.7 For $n$-dimensional vectors, the ordinary scalar product $\mathbf{u} \cdot \mathbf{v} \equiv \mathbf{u v}$ defines a type- $\left(\begin{array}{l}0 \\ 2\end{array}\right)$ tensor, in the form $G(\mathbf{u}, \mathbf{v})=\mathbf{u v}$, since the elementary properties of the scalar product make $G$ a bilinear mapping of a vector pair into the reals. More generally, an inner product defined arbitrarily by the quadratic form

$$
G(\mathbf{u}, \mathbf{v})=\mathbf{u}^{T} E \mathbf{v}
$$

where $E$ is an $n \times n$ matrix, defines a type- $\left(\begin{array}{l}0 \\ 2\end{array}\right)$ tensor.

Definition 3: A type- $\left(\begin{array}{l}0 \\ 2\end{array}\right)$ tensor $G(\mathbf{u}, \mathbf{v})$ is (i) symmetric if, for every $\mathbf{u}$ and $\mathbf{v}$,

(ii) nonsingular if

$$
G(\mathbf{u}, \mathbf{v})=G(\mathbf{v}, \mathbf{u})
$$

$$
[G(\mathbf{u}, \mathbf{v})=0, \text { identically in } \mathbf{u}] \rightarrow \mathbf{v}=\mathbf{0}
$$

and (iii) positive definite if, for any nonzero vector $\mathbf{u}$,

$$
G(\mathbf{u}, \mathbf{u})>0
$$

A type- $\left(\begin{array}{l}0 \\ 2\end{array}\right)$ tensor that is symmetric and nonsingular is called a metric tensor. (A positive definite tensor is necessarily nonsingular.)

EXAMPLE 13.8 Let $C=\left[C_{j}^{i}\right]_{n n}$ be a square matrix and let $\left(a_{i}\right)$ and $\left(v^{i}\right)$ be the respective components of $\omega$ and $\mathbf{v}$ relative to the standard basis in $\mathbf{R}^{n}$ and its dual. Then the matrix product

$$
T(\boldsymbol{\omega} ; \mathbf{v})=\boldsymbol{\omega} C \mathbf{v} \equiv a_{i} C_{j}^{i} v^{j} \quad \text { (a bilinear form) }
$$

defines a type- $\left(\begin{array}{l}1 \\ 1\end{array}\right)$ tensor over the vector space $\mathbf{R}^{n}$.

\section*{Tensor Components}
In the three types of tensors considered in Examples 13.6-13.8, we may define tensor components in the following manner, which may be generalized to arbitrary tensors in an obvious way. Let $\mathbf{b}_{1}, \ldots, \mathbf{b}_{n}$ be a basis for $\mathbf{V}$, and $\boldsymbol{\beta}^{1}, \ldots, \boldsymbol{\beta}^{n}$ its dual in $\mathbf{V}^{*}$. Then, for each $i$, write

$$
\begin{array}{ll}
\text { type }\left(\begin{array}{l}
\mathbf{1} \\
\mathbf{0}
\end{array}\right) & T^{i}=T\left(\boldsymbol{\beta}^{i}\right) \\
\text { type }\left(\begin{array}{l}
\mathbf{0} \\
1
\end{array}\right) & T_{i}=T\left(\mathbf{b}_{i}\right) \\
\text { type }\left(\begin{array}{l}
\mathbf{1} \\
1
\end{array}\right) & T_{j}^{i}=T\left(\boldsymbol{\beta}^{i} ; \mathbf{b}_{j}\right)
\end{array}
$$

EXAMPLE 13.9 Find the components, relative to the standard basis for $\mathbf{V}=\mathbf{R}^{n}$, of a type- $\left(\begin{array}{l}1 \\ 1\end{array}\right)$ tensor on $\mathbf{V}$ constructed by the recipe of Example 13.8.

By construction, $T(\boldsymbol{\omega} ; \mathbf{v})=a_{i} C_{j}^{i} v^{j}$, for all $\boldsymbol{\omega}$ and $\mathbf{v}$. Substituting $\boldsymbol{\omega}=\boldsymbol{\beta}^{p}=d x^{p}$ and $\mathbf{v}=\mathbf{b}=\mathbf{e}_{q}$, we find

$$
T_{q}^{p} \equiv T\left(d x^{p} ; \mathbf{e}_{q}\right)=\delta_{i}^{p} C_{j}^{i} \delta_{q}^{j}=C_{q}^{p}
$$

Thus, the components of $T$ are independent of those of the arguments, $\boldsymbol{\omega}$ and $\mathbf{v}$, and depend only on the components of the matrix $C$.

\section*{Effect of Change of Basis on Tensor Components}
Under a change of basis, (13.6),

$$
\begin{aligned}
& \text { type }\left(\begin{array}{l}
\mathbf{1} \\
\mathbf{0}
\end{array}\right) \quad \bar{T}^{i}=T\left(\overline{\boldsymbol{\beta}}^{i}\right)=T\left(\bar{A}_{r}^{i} \boldsymbol{\beta}^{r}\right)=\bar{A}_{r}^{i} T\left(\boldsymbol{\beta}^{r}\right)=T^{r} \bar{A}_{r}^{i} \\
& \text { type }\left(\begin{array}{l}
0 \\
\mathbf{1}
\end{array}\right) \quad \bar{T}_{i}=T\left(\overline{\mathbf{b}}_{i}\right)=T\left(A_{i}^{r} \mathbf{b}_{r}\right)=A_{i}^{r} T\left(\mathbf{b}_{r}\right)=T_{r} A_{i}^{r}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-203}
\end{center}

EXAMPLE 13.10 If $\mathbf{V}$ is Euclidean $\mathbf{R}^{n}$, the change of basis $\overline{\mathbf{b}}_{i}=A_{i}^{j} \mathbf{b}_{j}$ induces the change of coordinates $x^{i}=A_{j}^{i} \bar{x}^{j}$, for which

$$
\bar{J} \equiv\left(\frac{\partial x^{i}}{\partial \bar{x}^{j}}\right)=A \quad \text { and so } \quad J=\bar{J}^{-1}=\bar{A}
$$

The above transformation formulas then reduce to the classical laws for affine tensors-compare (3.21).

\subsection*{13.6 THEORY OF MANIFOLDS}
A manifold is the natural extension of a surface to higher dimensions, and also to spaces more general than $\mathbf{R}^{n}$. It is helpful at first to think of a manifold as just a hypersurface in $\mathbf{R}^{n}$.

By the term neighborhood of a point we shall understand either the set of all points in $\mathbf{R}^{n}$ within some fixed distance from the given point, or any set containing these points. A neighborhood of $p$ will be denoted $\mathbf{U}_{p}$. If the concept used for distance in $\mathbf{R}^{m}$ is Euclidean, then every neighborhood $\mathbf{U}_{p}$ contains a solid, spherical ball (or "hyperball," if $n>3$ ), having some positive radius and centered at $p$. A neighborhood of a point $p$ in a set is the intersection of a neighborhood $\mathbf{U}_{p}$ and the set. A set is open if each of its points has a neighborhood completely composed of points of the set. An open neighborhood is simply a neighborhood that is also an open set (in the case of a solid-ball neighborhood, the outer boundary of the ball would have to be removed in order to make it an open neighborhood).

\section*{Descriptive Definition of a Manifold}
A manifold is a set which has the property that each point can serve as the origin of local coordinates that are valid in an open neighborhood of the point, which neighborhood is an exact "copy" of an open neighborhood of a point in $\mathbf{R}^{n}$. Though such a definition allows the manifold to lie in a metric space, topological space, Banach space, or other abstract mathematical system, it is best that we begin with manifolds in a simpler space, like $\mathbf{R}^{m}$. Accordingly:

Definition 4: A manifold is any set $\mathbf{M}$ in $\mathbf{R}^{m}$ which has the property that for each point $p$ in the manifold there exists an open neighborhood $\mathbf{U}_{p}$ in $\mathbf{M}$ and a mapping $\varphi_{p}$ which carries $\mathbf{U}_{p}$ into a neighborhood in $\mathbf{R}^{n}$. The mapping is required to be a homeomorphism; i.e.

(1) $\varphi_{p}$ is continuous.

(2) $\varphi_{p}$ is bijective from $\mathbf{U}_{p}$ onto its range, $\varphi_{p}\left(\mathbf{U}_{p}\right)$.

(3) $\varphi_{p}^{-1}$ is continuous.

See Fig. 13-1.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-204}
\end{center}

Fig. 13-1

\section*{Coordinate Patches, Atlas}
The neighborhoods $\mathbf{U}_{p}$ for $p$ in $\mathbf{M}$ provide a means for locally ascribing coordinates to $\mathbf{M}$ which have the correct dimension (e.g., a plane lying in 3-space is actually 2-dimensional and, as a manifold, has a coordinatization by pairs of reals instead of triples, as in Section 10.4). For any point $p$ in $\mathbf{M}$, the pair $\left(\mathbf{U}_{p}, \varphi_{p}\right)$ is called a coordinate patch (also chart, or local coordinatization) for $\mathbf{M}$, while any collection of such pairs for which the neighborhoods $\mathbf{U}_{p}$ together cover $\mathbf{M}$ is called an atlas for $\mathbf{M}$. Since the coordinate patches make $\mathbf{M} n$-dimensional at each point, $\mathbf{M}$ is sometimes referred to as an $n$-manifold.

Often a finite number of charts is sufficient for an atlas (Example 13.11). It can be proved that if a manifold in $\mathbf{R}^{m}$ is closed, and bounded in terms of the distance in $\mathbf{R}^{m}$, a finite number of charts will always be sufficient.

\section*{EXAMPLE 13.11}
(a) The 2-sphere, denoted $\mathbf{S}^{2}$, is the ordinary sphere in 3-dimensional space $\left(y^{i}\right)$, centered at $(0,0,0)$ having radius $a$. It may be coordinatized by an atlas of only two charts, as follows. [Note that the usual spherical coordinates $(\varphi, \theta)$ fail to give a one-one mapping at the poles, where $\theta$ is indeterminate.]

$$
\begin{aligned}
& y^{1}=\frac{2 a^{2} x^{1}}{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}+a^{2}} \\
& y^{2}=\frac{2 a^{2} x^{2}}{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}+a^{2}} \\
& y^{3}=\varepsilon a \frac{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}-a^{2}}{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}+a^{2}} \quad(\varepsilon= \pm 1)
\end{aligned}
$$

As illustrated in Fig. 13-2, the chart corresponding to $\varepsilon=+1$ has $\mathbf{U}_{p}$ centered on the south pole (whose coordinates are $x^{1}=x^{2}=0$ ) and including every point of the sphere except the north pole. The other chart $(\varepsilon=-1)$ is the mirror image of the first chart in the equatorial plane. For a derivation of this atlas, see Problem 13.15.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205}
\end{center}

Fig. 13-2

(b) The $n$-sphere $\mathbf{S}^{n}$ in $\mathbf{R}^{n+1}$ may be defined as the set of points $\left(y^{i}\right)$ in $\mathbf{R}^{n+1}$ such that

$$
\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}+\cdots+\left(y^{n+1}\right)^{2}=a^{2}
$$

(centered at $(0,0,0, \ldots, 0)$, radius $a)$. A coordinate patch for a neighborhood of $(0,0, \ldots, a)$ is:

$$
y^{1}=x^{1} \quad y^{2}=x^{2} \quad \cdots \quad y^{n}=x^{n} \quad y^{n+1}=\sqrt{a^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}-\cdots-\left(x^{n}\right)^{2}}
$$

where the mapping is into the $n$-dimensional neighborhood $\left(x^{1}\right)^{2}+\cdots+\left(x^{n}\right)<a^{2} \quad$ (the interior of $\mathbf{S}^{n-1}$ ). Establishing the analogous patches around the other "diametrically opposite" endpoints, we obtain an atlas of $2 n+2$ charts. (A smaller atlas requires a more clever approach.)

\section*{Differentiable Manifolds}
Inevitably, there will exist pairs $\left(\mathbf{U}_{p}, \varphi_{p}\right)$ and $\left(\mathbf{U}_{q}, \varphi_{q}\right)$ whose neighborhoods overlap in $\mathbf{M}$ (Fig. 13-3); so the common region $\mathbf{U}_{p} \cap \mathbf{U}_{q} \equiv \mathbf{W}$, called an overlapping set, generates a map $\varphi$ between the images of $\mathbf{W}$ under $\varphi_{p}$ and $\varphi_{q}$. Explicitly (trace the circuit in Fig. 13-3), $\varphi=\varphi_{q}{ }^{\circ} \varphi_{p}^{-1}$.

It is clear that $\varphi$ and $\varphi^{-1}$ are both continuous. If $\varphi$ and $\varphi^{-1}$ are of class $C^{k}$ (have continuous partial derivatives of order $k$ at each point) then the overlapping set $\mathbf{W}$ is said to be of class $C^{k}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-205(1)}
\end{center}

Fig. 13-3

Definition 5: A differentiable manifold is a manifold which possesses an atlas such that all overlapping sets are of class $C^{1}$. A $C^{k}\left(C^{\infty}\right.$ or $\left.C^{\omega}\right)$ manifold has an atlas whose overlapping sets are of class $C^{k}\left(C^{\infty}\right.$ or $\left.C^{\omega}\right)$.

Remark 1: Recall the distinction between infinitely differentiable $\left(C^{\infty}\right)$ and analytic $\left(C^{\omega}\right)$.

One way to ensure that a manifold be $C^{k}$ in the present context is to demand that each $\varphi_{p}$ and $\varphi_{p}^{-1}$ be class $C^{k}$. As a matter of convenience, we assume from now on that all manifolds are $C^{\infty}$ manifolds.

EXAMPLE 13.12 In the case of the spherical manifolds of Example 13.11, the mapping functions $\varphi_{P}^{-1}$ are either rational with nonvanishing denominators or square roots of positive polynomials. These are certainly $C^{\infty}$ manifolds (in fact, $C^{\omega}$ ).

To bring the notation closer to that of differential geometry (cf. Section 10.4), we now redesignate the maps $\varphi_{p}^{-1}$ linking $\mathbf{M}$ with coordinates $\left(x^{i}\right)$ : let

$$
\varphi_{p}^{-1}\left(x^{1}, x^{2}, \ldots, x^{n}\right) \equiv \mathbf{r}\left(x^{1}, x^{2}, \ldots, x^{n}\right) \equiv\left(y^{j}\left(x^{1}, x^{2}, \ldots, x^{n}\right)\right)
$$

for $1 \leqq j \leqq m$. (See Fig. 13-4.)

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-206}
\end{center}

Fig. 13-4

\subsection*{13.7 TANGENT SPACE; VECTOR FIELDS ON MANIFOLDS}
Intuitively expressed, a vector field $V$ on a manifold $\mathbf{M}$ is simply a tangent vector to $\mathbf{M}$ which varies in some continuous (and differentiable) manner from point to point (Fig. 13-5). More precisely, it is a rule that gives a tangent vector at every point of $\mathbf{M}$. One way to obtain a vector field (if we are in $\mathbf{R}^{3}$ ) is to take the variable normal vector $\mathbf{n}$ and cross it with some fixed vector $\mathbf{a}$; thus $V=\mathbf{n} \times \mathbf{a}$ is a differentiable vector field. But this definition takes us outside the manifold (is extrinsic). We seek a way to remain on the manifold itself (which is immediately applicable to abstract manifolds not imbeddable in a familiar space); such endeavors are called intrinsic methods.

The clue is to consider some curve on $\mathbf{M}$ and to define $V$ as the tangent field of that curve. If the curve is defined through a coordinate patch by

$$
\mathbf{c}=\mathbf{r}\left(x^{1}(t), x^{2}(t), \ldots, x^{n}(t)\right)=\left(y^{j}\left(x^{1}, x^{2}, \ldots, x^{n}\right)\right) \quad(1 \leqq j \leqq m)
$$

then the chain rule gives

$$
\frac{d \mathbf{c}}{d t}=\frac{d \mathbf{r}}{d t}=\frac{\partial \mathbf{r}}{\partial x^{i}} \frac{d x^{i}}{d t}=V \quad \text { or } \quad V=V^{i} \mathbf{r}_{i}
$$

where the vectors $\mathbf{r}_{i} \equiv \partial \mathbf{r} / \partial x^{i}$ are themselves tangent to $\mathbf{M}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-207}
\end{center}

Fig. 13-5

Definition 6: For any (differentible) manifold $\mathbf{M}$ having coordinate patch $\mathbf{U}_{p}$ at any point $p$, the span of the vectors $\mathbf{r}_{1}, \mathbf{r}_{2}, \ldots, \mathbf{r}_{n}$ [evaluated at $\varphi_{p}(p)$ ] is the tangent space at $p$, denoted $T_{p}(\mathbf{M})$. The union of all tangent spaces $T_{p}(\mathbf{M})$, for ail $p$ in $\mathbf{M}$, is called the tangent bundle of $\mathbf{M}$, denoted $T(\mathbf{M})$.

Although each $T_{p}(\mathbf{M})$ is a vector space, this does not guarantee that $T(\mathbf{M})$ is a vector space. For example, the sum of a vector in $T_{p}(\mathbf{M})$ and a vector in $T_{q}(\mathbf{M})$ will not generally be tangent to $\mathbf{M}$.

Definition 7: A vector field $V$ on a manifold $\mathbf{M}$ is any $C^{\infty}$ function that maps $\mathbf{M}$ to its tangent bundle $T(\mathbf{M})$. That is, for each point $p$ in $\mathbf{M}$, the image $\mathbf{V}(p)=\mathbf{V}_{p}$ is a vector belonging to the tangent space $T_{p}(\mathbf{M})$ at $p$. Explicitly, for certain scalar functions $V^{i}$,


\begin{equation*}
V=V^{i} \mathbf{r}_{i}=\left(V^{i} \frac{\partial y^{j}}{\partial x^{i}}\right) \quad(j=1,2, \ldots, m) \tag{13.7a}
\end{equation*}


A fundamental theorem regarding vector fields on manifolds may be proved from the basic theory of systems of ordinary differential equations.

Theorem 13.2: Every vector field on a manifold $\mathbf{M}$ possesses a system of flow curves or integral curves on $\mathbf{M}$, defined as curves for which the tangent vector at each point coincides with the given vector field at that point.

\section*{Notation}
To de-emphasize the particular choice of coordinatization map $\varphi_{p}: \mathbf{U}_{p} \rightarrow \mathbf{R}^{n}$, it is customary to omit the vector $\mathbf{r}$ from the above description of the basis for $T_{p}(\mathbf{M})$, and to write

$$
\frac{\partial}{\partial x^{1}}, \frac{\partial}{\partial x^{2}}, \ldots, \frac{\partial}{\partial x^{n}} \text { in place of } \frac{\partial \mathbf{r}}{\partial x^{1}}, \frac{\partial \mathbf{r}}{\partial x^{2}}, \ldots, \frac{\partial \mathbf{r}}{\partial x^{n}}
$$

or, even more cursorily, $\partial_{1}, \partial_{2}, \ldots, \partial_{n}$. Many textbooks use this last notation exclusively, and write $(13.7 a)$ as


\begin{equation*}
V=V^{i} \partial_{i} \tag{13.7b}
\end{equation*}


Such shorthand is particularly convenient when the coordinate maps $\varphi_{p}^{-1}$ for a particular manifold are unspecified (for example, when the manifold is defined by an equation $F\left(y^{1}, y^{2}, \ldots, y^{m}\right)=0$ for some real-valued function $F$ ). In this situation, since $\mathbf{r}_{1}, \mathbf{r}_{2}, \ldots, \mathbf{r}_{n}$ are not explicitly defined, we use the notation $E_{1}, E_{2}, \ldots, E_{n}$ to denote the coordinate frame on $\mathbf{M}$, whose restriction to $T_{p}(\mathbf{M})$, for each $p$ in $\mathbf{M}$, is a basis for $T_{p}(\mathbf{M})$. We make the identifications $E_{i} \equiv \partial_{i} \quad(i=1,2, \ldots, n)$, giving


\begin{equation*}
V=V^{i} E_{i} \tag{13.7c}
\end{equation*}


\section*{Extrinsic Representation of Vector Fields}
It is possible to represent a vector field $V$ on a manifold without reference to coordinate patches (which often have the disadvantage of being complicated or difficult to construct); one can stay entirely in the $\left(y^{i}\right)$ system, which we assume to be rectangular. Suppose $\mathbf{M}$ is given by a single equation $F\left(y^{1}, y^{2}, \ldots, y^{m}\right)=0$, for some $C^{k}$ function $F$. A one-form $\sigma=\omega_{i} d y^{i}$, where $\omega_{i}=$ $\omega_{i}\left(y^{1}, y^{2}, \ldots, y^{m}\right)$, is said to be restricted to $\mathbf{M}$ if the point $\left(y^{i}\right)$ is required to lie on $\mathbf{M}$; that is, $F\left(y^{1}, y^{2}, \ldots, y^{m}\right)=0$. As is well known from multidimensional calculus, the gradient $\nabla F=\left(\partial F / \partial y^{i}\right)$ is normal to $\mathbf{M}$, so that if we further require that the restriction of $\sigma$ map $\nabla F$ into zero,

$$
\omega_{i} \frac{\partial F}{\partial y^{i}}=0
$$

then $V=\sigma$ is a vector field on $\mathbf{M}$ (having components $d y^{i}$ ).

EXAMPLE 13.13 Consider the paraboloid $\mathbf{P}$ in $\mathbf{R}^{3}$ given by

$$
F\left(y^{1}, y^{2}, y^{3}\right)=\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}-y^{3}=0
$$

Show that the restriction of $\sigma=y^{1} y^{2} d y^{1}+\left(y^{2}\right)^{2} d y^{2}+2 y^{2} y^{3} d y^{3}$ to $\mathbf{P}$ is a vector field on $\mathbf{P}$.

We must show that the scalar product of $\left(y^{1} y^{2},\left(y^{2}\right)^{2}, 2 y^{2} y^{3}\right)$ and $\nabla F=\left(2 y^{1}, 2 y^{2},-1\right)$ is zero:

$$
\left(y^{1} y^{2}\right)\left(2 y^{1}\right)+\left(y^{2}\right)^{2}\left(2 y^{2}\right)+\left(2 y^{2} y^{3}\right)(-1)=\left[\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}-y^{3}\right] 2 y^{2}=0
$$

\subsection*{13.8 TENSOR FIELDS ON MANIFOLDS}
\section*{Dual Tangent Bundle}
At each $p$ in $\mathbf{M}$, let $T_{p}^{*}(\mathbf{M})$ denote the dual of the vector space $T_{p}(\mathbf{M})$, and denote by $T^{*}(\mathbf{M})$ the union of all spaces $T_{p}^{*}(\mathbf{M})$. The set $T^{*}(\mathbf{M})$, called the dual tangent bundle of $\mathbf{M}$, is not necessarily a vector space (just as $T(\mathbf{M})$ was not).

We need to make explicit certain elements of $T^{*}(\mathbf{M})$.

\section*{Differentials on $M$}
The differential of a function $f: \mathbf{R}^{n} \rightarrow \mathbf{R}$ is rigorously defined as a two-vector function $d f:\left(\mathbf{R}^{n}\right)^{2} \rightarrow \mathbf{R}$ which maps each pair $(\mathbf{x}, \mathbf{v})$ - where $\mathbf{x}$ is a point in $\mathbf{R}^{n}$ and $\mathbf{v}=\left(d x^{1}, d x^{2}, \ldots, d x^{n}\right)$ is a direction in $\mathbf{R}^{n}$-to the real number


\begin{equation*}
d f(\mathbf{x}, \mathbf{v}) \equiv \frac{\partial f}{\partial x^{1}} d x^{1}+\frac{\partial f}{\partial x^{2}} d x^{2}+\cdots+\frac{\partial f}{\partial x^{n}} d x^{n}=f_{i} d x^{i} \tag{13.8}
\end{equation*}


Here, the $f_{i}$ are evaluated at $\mathbf{x}$. If $f$ is any real-valued $C^{k}$ function on $\mathbf{M}$, then the differential of

$$
f\left(x^{1}, x^{2}, \ldots, x^{n}\right) \equiv f\left(y^{1}\left(x^{1}, \ldots, x^{n}\right), y^{2}\left(x^{1}, \ldots, x^{n}\right), \ldots, y^{m}\left(x^{1}, \ldots, x^{n}\right)\right)
$$

called a differential field on $\mathbf{M}$, is

$$
d f=\frac{\partial}{\partial x^{i}}(f(\mathbf{r}(x))) d x^{i}=\frac{\partial f}{\partial y^{k}} \frac{\partial y^{k}}{\partial x^{i}} d x^{i}=\left(\nabla f \cdot \mathbf{r}_{i}\right) d x^{i}
$$

\begin{itemize}
  \item a one-form. Thus, $d f$ may be thought of as a mapping from $\mathbf{M}$ to $T^{*}(\mathbf{M})$ : we agree that the evaluation of $d f$ at $p$ is the one-form $\left(\nabla f(p) \cdot r_{i}(p)\right) d x^{i}$ in $T_{p}^{*}(\mathbf{M})$.
\end{itemize}

Let us now compare the two kinds of fields on $\mathbf{M}$.

vector field

differential field

$$
\begin{aligned}
& V: \frac{\text { Mapping }}{} \quad \frac{\text { Restricted to } \mathbf{U}_{P}}{V=V^{i} E_{i}} \\
& d f: \mathbf{M} \rightarrow T^{*}(\mathbf{M}) \quad \omega=\left(\nabla f \cdot E_{i}\right) d x^{i}
\end{aligned}
$$

Definition 8: A tensor field of type $\left(\begin{array}{c}r \\ s\end{array}\right)$ on a manifold $\mathbf{M}$ is a mapping $T:\left[T^{*}(\mathbf{M})\right]^{r} \otimes[T(\mathbf{M})]^{s} \rightarrow C^{k}\left(\mathbf{R}^{m}\right)$ taking $r$ differential fields and $s$ vector fields on $\mathbf{M}$ to real-valued $C^{k}$-functions $f$ on $\mathbf{R}^{m}$. It is assumed that the evaluation of $T$ at a point $p$ on $\mathbf{M}$ is given by

$$
T_{p}\left(\omega^{1}, \ldots, \omega^{r} ; V_{1}, \ldots, V_{s}\right)=T\left(\omega_{p}^{1}, \ldots, \omega_{p}^{r} ; V_{1 p}, \ldots, V_{s p}\right) \equiv f(p)
$$

and that each map $T_{p}$ is multilinear.

EXAMPLE 13.14 (a) At each fixed $p$ on M, the mapping $T_{p}$ is a tensor [on the vector space $\left[T_{p}^{*}(\mathbf{M})\right]^{r} \otimes\left[T_{p}(\mathbf{M})\right]^{s}$, of type $\left.\left(\begin{array}{c}r \\ s\end{array}\right)\right]$, per Definition 2. (b) Any vector field $V$ on $\mathbf{M}$ can be interpreted as a type- $\left(\begin{array}{l}1 \\ 0\end{array}\right)$ tensor field via a mapping $T(\omega)=\omega(V)$; compare Problem 13.20.

\section*{Solved Problems}
\section*{ABSTRACT VECTOR SPACES AND THE GROUP CONCEPT}
13.1 (a) Show that the set of polynomials

$$
p_{1}(t)=1+t \quad p_{2}(t)=t+t^{2} \quad p_{3}(t)=t^{2}+t^{3} \quad p_{4}(t)=t^{3}-1
$$

is a basis for the vector space $\mathbf{P}^{3}$ (polynomials of degree $\leqq 3$ ). (b) Find the components of the polynomial $p(t)=t^{3}$ relative to this basis.

(a) Since the dimension of $\mathbf{P}^{3}$ is 4 and there are 4 vectors, it suffices to show they are linearly independent. Suppose that, for all $t$,

or

$$
\begin{gathered}
\lambda^{1}(1+t)+\lambda^{2}\left(t+t^{2}\right)+\lambda^{3}\left(t^{2}+t^{3}\right)+\lambda^{4}\left(t^{3}-1\right)=0 \\
\left(\lambda^{1}-\lambda^{4}\right) \cdot 1+\left(\lambda^{1}+\lambda^{2}\right) t+\left(\lambda^{2}+\lambda^{3}\right) t^{2}+\left(\lambda^{3}+\lambda^{4}\right) t^{3}=0
\end{gathered}
$$

Since this is an identity, we must have

$$
0=\lambda^{1}-\lambda^{4}=\lambda^{1}+\lambda^{2}=\lambda^{2}+\lambda^{3}=\lambda^{3}+\lambda^{4}
$$

Thus $\lambda^{1}=\lambda^{4}, \lambda^{1}=-\lambda^{2}=\lambda^{3}$; so the last equation gives $\lambda^{1}+\lambda^{1}=0$ or $\lambda^{1}=0$, and all $\lambda^{i}$ vanish, thus proving linear independence.

(b) To find the linear combination yielding $p(t)=t^{3}$, write

$$
\begin{aligned}
& \qquad \lambda^{1}(1+t)+\lambda^{2}\left(t+t^{2}\right)+\lambda^{3}\left(t^{2}+t^{3}\right)+\lambda^{4}\left(t^{3}-1\right)=t^{3} \\
& \text { i.e. } \quad\left(\lambda^{1}-\lambda^{4}\right) \cdot 1+\left(\lambda^{1}+\lambda^{2}\right) t+\left(\lambda^{2}+\lambda^{3}\right) t^{2}+\left(\lambda^{3}+\lambda^{4}-1\right) t^{3}=0 \\
& \text { or } \quad \lambda^{1}=\lambda^{4} \quad \lambda^{1}=-\lambda^{2}=\lambda^{3} \quad \lambda^{1}+\lambda^{1}-1=0
\end{aligned}
$$

Hence, $\lambda^{1}=-\lambda^{2}=\lambda^{3}=\lambda^{4}=1 / 2$.

13.2 (a) Model the 4-group by manipulating an ordinary $8 \frac{1}{2}$ by 11 sheet of paper, in the following way: Let $s$ be the operation of turning the sheet over sideways (as in a book) and setting it on its original location; $u$, the operation of turning the sheet upside down (end-for-end); $b$, both operations ( $s$ followed by $u$, resulting in a $180^{\circ}$ rotation of the page, face up); and $e$, doing nothing (identity). Interpret the group operation (multiplication) as one operation followed by another (thus, for example, by the above definitions, $b=s u$, reading from left to right). $(b)$ Show that the 4-group cannot be isomorphic to the cyclic group on four elements, $\mathbf{C}^{4}$.\\
(a) This is one of those problems in mathematics that is best handled without formulas or equations. By simple observation, the operation $u s$ also results in a $180^{\circ}$ rotation; hence, $b=s u=u s$. It is also clear that if we apply $s$ twice, or $u$ twice, the sheet is left in its original state; $s^{2}=u^{2}=e$. Next, observe that the associative law is valid, so long as we keep the order of the operations intact. It follows that

$$
b^{2}=(s u)(u s)=s(u)^{2} s=s^{2}=e
$$

When we multiply all the group elements by $b$, we obtain:

$$
b e=b \quad b b=e \quad b u=(s u) u=s u^{2}=s \quad b s=(s u) s=(u s) s=u
$$

Hence, the multiplication table for this group may be displayed and may be seen to coincide with that for the 4-group:

\begin{center}
\begin{tabular}{c|cccc}
$\cdot$ & $e$ & $s$ & $u$ & $b$ \\
\hline
$e$ & $e$ & $s$ & $u$ & $b$ \\
$s$ & $s$ & $e$ & $b$ & $u$ \\
$u$ & $u$ & $b$ & $e$ & $s$ \\
$b$ & $b$ & $u$ & $s$ & $e$ \\
\end{tabular}
\end{center}

(b) For the cyclic group, $\left\{e, z, z^{2}, z^{3}\right\}$, with $z^{4}=e$ for some $z$, we could not have $z^{2}=e$, which is the characteristic property of all elements of the 4-group.

13.3 The simple Lorentz group can be studied by compressing the $4 \times 4$ matrices down to $2 \times 2$ matrices:

$$
\left[\begin{array}{llll}
a & b & 0 & 0 \\
b & a & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right] \rightarrow\left[\begin{array}{ll}
a & b \\
b & a
\end{array}\right] \quad\left(a^{2}-b^{2}=1\right)
$$

Show explicitly that all real $2 \times 2$ matrices of the above form constitute an abelian group (the group $\mathbf{L}(2)$ ) under matrix multiplication, and that $\mathbf{L}(2)$ is a subgroup of the following two larger groups:

$$
\begin{aligned}
& \mathbf{G L}(2, \mathbf{R}) \text { : matrices of the form }\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right], \quad a d \neq b c \\
& \mathbf{S U}(2) \text { : matrices of the form }\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right], \quad a d-b c=1
\end{aligned}
$$

Since for a matrix in $\mathbf{L}(2)$,

$$
a d-b c=a^{2}-b^{2}=1
$$

all such matrices belong to $\mathbf{S U}(2)$, which, in turn, is a $\operatorname{subgroup}$ of $\mathbf{G L}(2, \mathbf{R})$. Now verify the group properties:

(1) $u v$ belongs to the group for all $u, v$.

If

$$
\begin{gathered}
A=\left[\begin{array}{ll}
a & b \\
b & a
\end{array}\right] \quad B=\left[\begin{array}{ll}
c & d \\
d & c
\end{array}\right] \\
A B=\left[\begin{array}{ll}
a & b \\
b & a
\end{array}\right]\left[\begin{array}{ll}
c & d \\
d & c
\end{array}\right]=\left[\begin{array}{ll}
a c+b d & a d+b c \\
b c+a d & b d+a c
\end{array}\right] \equiv\left[\begin{array}{ll}
x & y \\
y & x
\end{array}\right] \\
x^{2}-y^{2}=\operatorname{det} A B=(\operatorname{det} A)(\operatorname{det} B)=(1)(1)=1
\end{gathered}
$$

$$
\begin{aligned}
& \text { then } \\
& \text { and }
\end{aligned}
$$

(2) $(u v) w=u(v w)$. Yes: matrix multiplication is associative.

(3) For some $e$ and all $u$, $e u=u e=u$. Yes: the identity matrix has $1^{2}-0^{2}=1$, so is a member of $\mathbf{L}(2)$.

(4) Given $u, u^{-1} u=u u^{-1}=e$ for some $u^{-1}$.

$$
\left[\begin{array}{ll}
a & b \\
b & a
\end{array}\right]^{-1}=\frac{1}{a^{2}-b^{2}}\left[\begin{array}{rr}
a & -b \\
-b & a
\end{array}\right]=\left[\begin{array}{rr}
a & -b \\
-b & a
\end{array}\right]
$$

which is in $\mathbf{L}(2)$.\\
$u v=v u \quad$ (abelian group).

\[
B A=\left[\begin{array}{ll}
c & d  \tag{5}\\
d & c
\end{array}\right]\left[\begin{array}{ll}
a & b \\
b & a
\end{array}\right]=\left[\begin{array}{ll}
c a+d b & c b+d a \\
d a+c b & d b+c a
\end{array}\right]=\left[\begin{array}{ll}
x & y \\
y & x
\end{array}\right]=A B
\]

\section*{VECTOR SPACE CONCEPTS}
13.4 (a) Show that the space $\mathbf{P}$ of all real-valued polynomials in a real variable $x$ is infinitedimensional. (b) Conclude that $C^{k}(\mathbf{R})$ is infinite-dimensional.

(a) Suppose that $\mathbf{P}$ had the finite basis $\left\{p_{1}, p_{2}, \ldots, p_{n}\right\}$. Then, for any real polynomial $p(x)$, there exist constants $a_{1}, \ldots, a_{n}$ such that


\begin{equation*}
a_{1} p_{1}(x)+a_{2}(x)+\cdots+a_{n} p_{n}(x)=p(x) \tag{1}
\end{equation*}


Write (1) for the $n+1$ values $x_{1}<x_{2}<\cdots<x_{n+1}$ as a matrix equation:

\[
a_{1}\left[\begin{array}{c}
p_{1}\left(x_{1}\right)  \tag{2}\\
p_{1}\left(x_{2}\right) \\
\cdots \\
p_{1}\left(x_{n+1}\right)
\end{array}\right]+a_{2}\left[\begin{array}{c}
p_{2}\left(x_{1}\right) \\
p_{2}\left(x_{2}\right) \\
\cdots \\
p_{2}\left(x_{n+1}\right)
\end{array}\right]+\cdots+a_{n}\left[\begin{array}{c}
p_{n}\left(x_{1}\right) \\
p_{n}\left(x_{2}\right) \\
\cdots \\
p_{n}\left(x_{n+1}\right)
\end{array}\right]=\left[\begin{array}{c}
p\left(x_{1}\right) \\
p\left(x_{2}\right) \\
\cdots \\
p\left(x_{n+1}\right)
\end{array}\right]
\]

The column vectors on the left are elements of $\mathbf{R}^{n+1}$, and as there are $n$ of them, they do not span $\mathbf{R}^{n+1}$ (see Problem 13.5). To finish the proof, we have only to choose a vector on the right of (2) that is not in the span of those on the left-say, $\left(z_{1}, z_{2}, \ldots, z_{n+1}\right)$-and then to exhibit a polynomial $p$ that takes on those values at $x_{1}, x_{2}, \ldots, x_{n+1}$. The polynomial provided by Lagrange's interpolation formula does the job.

(b) For any $k$, the vector space $C^{k}(\mathbf{R})$ contains the infinite-dimensional subspace $\mathbf{P}$; thus, it too is infinite-dimensional.

13.5 The set $\mathbf{S}$ of all linear combinations of a fixed set of $n$ vectors, $\left\{\mathbf{b}_{1}, \mathbf{b}_{2}, \ldots, \mathbf{b}_{n}\right\}$, is called the span of the given vectors; it is obviously a vector space. Prove that this space has dimension $m \leqq n$, with equality if and only if the given vectors are linearly independent.

First we show that any $n+1$ vectors in $\mathbf{S}$ are linearly dependent. Suppose, on the contrary, that $\left\{\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{n+1}\right\}$ are linearly independent. Then, because the sequence of vectors

$$
\begin{array}{lllll}
\mathbf{u}_{1} & \mathbf{b}_{1} & \mathbf{b}_{2} & \ldots & \mathbf{b}_{n}
\end{array}
$$

is necessarily dependent, the well-known exchange lemma tells us that a sequence

$$
\begin{array}{lllllll}
\mathbf{u}_{1} & \mathbf{b}_{1} & \ldots & \mathbf{b}_{j-1} & \mathbf{b}_{j+1} & \ldots & \mathbf{b}_{n}
\end{array}
$$

also spans $\mathbf{S}$. Repeating the argument $n-1$ times, we arrive at the result that the vectors

$$
\begin{array}{lllll}
\mathbf{u}_{n} & \mathbf{u}_{n-1} & \ldots & \mathbf{u}_{2} & \mathbf{u}_{1}
\end{array}
$$

span $\mathbf{S}$, making $\mathbf{u}_{n+1}$ dependent on them-a contradiction.

If, therefore, $\left\{\mathbf{b}_{i}\right\}$ is linearly independent, it constitutes a basis for $\mathbf{S}$, and $m=n$. On the other hand, if only $m<n$ of the $\mathbf{b}_{i}$ are linearly independent, the above argument shows that any basis consists of exactly $m$ vectors.

\section*{DUAL SPACE}
\subsection*{13.6 Prove Theorem 13.1.}
It is almost trivial that any two vector spaces of dimension $n$ are isomorphic [if $\left\{\mathbf{b}_{i}^{(1)}\right\}$ and $\left\{\mathbf{b}_{i}^{(2)}\right\}$ are bases, set up the correspondence $\left.v^{i} \mathbf{b}_{i}^{(1)} \leftrightarrow v^{i} \mathbf{b}_{i}^{(2)}\right]$. Thus it is necessary to prove only that $\mathbf{V}^{*}$ is $n$-dimensional if $\mathbf{V}$ is; in other words, to prove that the set of vectors $\left\{\boldsymbol{\beta}^{i}\right\}$ defined by (13.4) (i) is linearly independent and (ii) has $\mathbf{V}^{*}$ as its span. (Problem 13.5 will then immediately yield Theorem 13.1.)

Proof of (i): By (13.5), for $j=1,2, \ldots, n$,

$$
\lambda_{i} \boldsymbol{\beta}^{i}(\mathbf{v})=0 \quad \rightarrow \quad \lambda_{i} \boldsymbol{\beta}^{i}\left(\mathbf{b}_{j}\right)=0 \rightarrow \lambda_{i} \delta_{j}^{i}=0 \quad \rightarrow \quad \lambda_{j}=0
$$

Proof of (ii): If $\boldsymbol{\beta}(\mathbf{v})$ is an arbitrary element of $\mathbf{V}^{*}$, then, by $(13.4 b)$,

$$
\boldsymbol{\beta}(\mathbf{v})=\boldsymbol{\beta}\left(v^{i} \mathbf{b}_{i}\right)=\boldsymbol{\beta}\left(\mathbf{b}_{i}\right) v^{i}=\boldsymbol{\beta}\left(\mathbf{b}_{i}\right) \boldsymbol{\beta}^{i}(\mathbf{v})
$$

that is, $\boldsymbol{\beta}$ is a linear combination of the $\boldsymbol{\beta}^{i}$.

13.7 Prove the inverse relation between the matrices $A$ and $\bar{A}$ of (13.6).

By definition $\overline{\mathbf{b}}_{i}=A_{i}^{j} \mathbf{b}_{j}$ and $\overline{\boldsymbol{\beta}}^{j}=\bar{A}_{k}^{j} \boldsymbol{\beta}^{k}$, so look at (13.5): $\overline{\boldsymbol{\beta}}^{j}\left(\overline{\mathbf{b}}_{i}\right)=\delta_{i}^{j}$. By the algebra of mappings and the fact that each $\overline{\boldsymbol{\beta}}^{j}$ and $\boldsymbol{\beta}^{k}$ is linear, we have

$$
\begin{aligned}
\delta_{i}^{j}=\overline{\boldsymbol{\beta}}^{j}\left(\overline{\mathbf{b}}_{i}\right) & =\left(\bar{A}_{k}^{j} \boldsymbol{\beta}^{k}\right)\left(\overline{\mathbf{b}}_{i}\right)=\bar{A}_{k}^{j} \boldsymbol{\beta}^{k}\left(\overline{\mathbf{b}}_{i}\right)=\bar{A}_{k}^{j} \boldsymbol{\beta}^{k}\left(A_{i}^{r} \mathbf{b}_{r}\right) \\
& =\bar{A}_{k}^{j}{ }^{j} A_{i}^{r} \boldsymbol{\beta}^{k}\left(\mathbf{b}_{r}\right)=\bar{A}_{k}^{j} A_{i}^{r} \delta_{r}^{k}=\bar{A}_{k}^{j} A_{i}^{k}
\end{aligned}
$$

that is, $\bar{A} A=I$.

\section*{TENSORS ON VECTOR SPACES}
13.8 Which of the following represent linear mappings of $\left(\mathbf{R}^{3}\right)^{*}$ (taking the one-forms on $\mathbf{R}^{3}$ into the reals), and so constitute (contravariant) tensors of type $\left(\begin{array}{l}1 \\ 0\end{array}\right)$ ?\\
(a) $T\left(a_{1} d x^{1}+a_{2} d x^{2}+a_{3} d x^{3}\right)=a_{1} a_{2} a_{3}$\\
(b) $T\left(a_{i} d x^{i}\right)=a_{1}-a_{3}$\\
(c) $T\left(a_{i} d x^{i}\right)=1$\\
(d) $T\left(a_{i} d x^{i}\right)=0$

(b) and (d)-the only linear mappings.

13.9 Associated with a particular basis $\left\{\mathbf{b}_{i}\right\}$ of a vector space of dimension $n$, we are given some set of numbers $\left\{C_{k}^{i j} ; \quad i, j, k=1, \ldots, n\right\}$. Then we define another set of numbers (and assume a similar definition for all changes of bases), $\left\{\bar{C}_{k}^{i j} ; i, j, k=1, \ldots, n\right\}$, such that

$$
\bar{C}_{k}^{i j}=\bar{A}_{r}^{i} \bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s}
$$

and call these numbers the components of the "tensor" $C$ on the new basis $\left\{\overline{\mathbf{b}}_{i}\right\}$. Show that this "tensor" is indeed a tensor per Definition 2.

We have only to define the functional

$$
T\left(\boldsymbol{\omega}_{1}, \boldsymbol{\omega}_{2} ; \mathbf{v}\right)=T\left(a_{i} \boldsymbol{\beta}^{i}, b_{j} \boldsymbol{\beta}^{j} ; v^{k} \mathbf{b}_{k}\right)=a_{i} b_{j} C_{k}^{i j} v^{k}
$$

which, by inspection, is a type $\left(\begin{array}{l}2 \\ 1\end{array}\right)$ tensor. We have:

$$
\begin{aligned}
& T_{k}^{i j}=T\left(\boldsymbol{\beta}^{i}, \boldsymbol{\beta}^{j} ; \mathbf{b}_{k}\right)=T\left(\delta_{r}^{i} \boldsymbol{\beta}^{r}, \delta_{s}^{j} \boldsymbol{\beta}^{s} ; \delta_{k}^{t} \mathbf{b}_{t}\right)=\delta_{r}^{i} \delta_{s}^{j} C_{t}^{r s} \delta_{k}^{t}=C_{k}^{i j} \\
& \bar{T}_{k}^{i j}=T\left(\overline{\boldsymbol{\beta}}^{i}, \overline{\boldsymbol{\beta}}^{j} ; \overline{\mathbf{b}}_{k}\right)=T\left(\bar{A}_{r}^{i} \boldsymbol{\beta}^{r}, \bar{A}_{s}^{j} \overline{\boldsymbol{\beta}}^{s} ; A_{k}^{t} \mathbf{b}_{t}\right)=\bar{A}_{r}^{i} \bar{A}_{s}^{j} A_{k}^{t} T\left(\beta^{r}, \beta^{s} ; b_{t}\right)=\bar{A}_{r}^{i} \bar{A}_{s}^{j} A_{k}^{t} C_{t}^{r s} \equiv \bar{C}_{k}^{i j}
\end{aligned}
$$

which show that $T$ and $C$ coincide in all coordinate systems.

13.10 In terms of the components $g_{i j}$ of a metric tensor $G(\mathbf{u}, \mathbf{v})$, show that:

(a) $G$ is symmetric if and only if $g_{i j}=g_{j i}$ for all $i, j$.

(b) $G$ is nonsingular if and only if $\left|g_{i j}\right| \neq 0$.

(c) $G$ is positive definite if, for all vectors $\left(u^{i}\right) \neq \mathbf{0}, g_{i j} u^{i} u^{j} \neq 0$ and $g_{11}>0$.

By Section 13.5, $g_{i j}=G\left(\mathbf{b}_{i}, \mathbf{b}_{j}\right)$ where $\left\{\mathbf{b}_{i}\right\}$ is some basis for $\mathbf{V}$. Then, if $\mathbf{u}=u^{i} \mathbf{b}_{i}$ and $\mathbf{v}=v^{i} \mathbf{b}_{i}$ are any two vectors in $\mathbf{V}$,

$$
G(\mathbf{u}, \mathbf{v})=u^{i} v^{j} G\left(\mathbf{b}_{i}, \mathbf{b}_{j}\right)=g_{i j} u^{i} v^{j}
$$

(a) $G(\mathbf{u}, \mathbf{v})=G(\mathbf{v}, \mathbf{u})$, for all $\mathbf{u}, \mathbf{v}$, if and only if

$$
g_{i j} u^{i} v^{j}=g_{i j} v^{i} u^{j}=g_{j i} u^{i} v^{j} \quad \text { or } \quad\left(g_{i j}-g_{j i}\right) u^{i} v^{j}=0
$$

for all real $u^{i}, v^{j}$, which is true if and only if $g_{i j}=g_{j i}$.

(b) In matrix form, the nonsingularity criterion reads:

$$
\left[u^{T} G v=0, \text { for all } u\right] \rightarrow v=0
$$

But $u^{T} G v$ vanishes for all $u$ if and only if $G v$ is the zero vector. Hence the criterion takes the form

$$
G v=0 \rightarrow v=0
$$

which defines $G$ as a nonsingular matrix (a matrix with nonvanishing determinant).

(c) For each fixed $\mathbf{u}$ and a scalar parameter $\lambda$, we have

$$
g_{i j}\left(u^{i}+\lambda \mathbf{b}_{1}^{i}\right)\left(u^{j}+\lambda \mathbf{b}_{1}^{j}\right)=g_{i j}\left(u^{i}+\lambda \delta_{1}^{i}\right)\left(u^{j}+\lambda \delta_{1}^{j}\right)=G(\mathbf{u}, \mathbf{u})+b \lambda+g_{11} \lambda^{2} \equiv P(\lambda)
$$

where $b \equiv\left(g_{1 j}+g_{j 1}\right) u^{j}$. If $\mathbf{u}$ is not in the span of $\mathbf{b}_{1}$, the quadratic form is, by hypothesis, nonzero. Hence, the discriminant of $P(\lambda)$ is negative:

$$
b^{2}-4 g_{11} G(\mathbf{u}, \mathbf{u})<0 \quad \text { or } \quad G(\mathbf{u}, \mathbf{u})>\frac{b^{2}}{4 g_{11}} \geqq 0
$$

It only remains to note that if $\mathbf{u}=\kappa \mathbf{b}_{1} \quad(\kappa \neq 0)$, then $G(\mathbf{u}, \mathbf{u})=\kappa^{2} g_{11}$, which is again positive.

13.11 Show that positive-definiteness of a type- $\left(\begin{array}{l}0 \\ 2\end{array}\right)$ tensor $G$ implies its nonsingularity.

If $G(\mathbf{u}, \mathbf{v})=0$ for all $\mathbf{u}$ and some $\mathbf{v}$, then $G(\mathbf{v}, \mathbf{v})=0$; and so, by positive-definiteness, $\mathbf{v}=0$.

13.12 A covariant tensor $A(\mathbf{u}, \mathbf{v})$ is antisymmetric if and only if $A(\mathbf{u}, \mathbf{v})--A(\mathbf{v}, \mathbf{u})$, for all $\mathbf{u}, \mathbf{v}$. Show that a criterion for antisymmetry is:

$$
A(\mathbf{u}, \mathbf{u})=0 \quad(\text { all } \mathbf{u})
$$

By bilinearity,

$$
A(\mathbf{u}+\mathbf{v}, \mathbf{u}+\mathbf{v})=A(\mathbf{u}, \mathbf{u})+A(\mathbf{u}, \mathbf{v})+A(\mathbf{v}, \mathbf{u})+A(\mathbf{v}, \mathbf{v})
$$

Thus, if $A(\mathbf{u}, \mathbf{u})=0$ for all $\mathbf{u}$,

$$
0=0+A(\mathbf{u}, \mathbf{v})+A(\mathbf{v}, \mathbf{u})+0 \quad \text { or } \quad A(\mathbf{u}, \mathbf{v})=-A(\mathbf{v}, \mathbf{u})
$$

Conversely, suppose $A(\mathbf{u}, \mathbf{v})=-A(\mathbf{v}, \mathbf{u})$, for all $\mathbf{u}$ and $\mathbf{v}$. Then, with $\mathbf{u}=\mathbf{v}$, we have $A(\mathbf{u}, \mathbf{u})=-A(\mathbf{u}, \mathbf{u})$, or $A(\mathbf{u}, \mathbf{u})=0$.

\section*{MANIFOLDS}
13.13 (a) Show that the 1 -sphere $\mathbf{S}^{1}$ (a circle in $\mathbf{R}^{2}$ ) can be made into a $C^{\infty} 1$-manifold by constructing an atlas with two charts. (b) Show that a one-chart atlas does not exist [thus, a circle is not homeomorphic to a line or interval].

(a) The standard parameterization of the circle,

$$
\varphi^{-1}:\left\{\begin{array}{l}
y^{1}=a \cos \theta \\
y^{2}=a \sin \theta
\end{array} \quad(0 \leq \theta<2 \pi)\right.
$$

is insufficient, since the inverse map $\varphi$ is discontinuous at point $p$ (Fig. 13-6). But if we define

$$
\varphi_{p}^{-1}:\left\{\begin{array}{l}
y^{1}=a \cos x^{1} \\
y^{2}=a \sin x^{1}
\end{array} \quad\left(-\pi<x^{1}<\pi\right) \quad \varphi_{q}^{-1}:\left\{\begin{array}{l}
y^{1}=a \cos x^{1} \\
y^{2}=a \sin x^{1}
\end{array} \quad\left(0<x^{1}<2 \pi\right)\right.\right.
$$

then $\left(\mathbf{S}^{1}-q, \varphi_{p}\right)$ and $\left(\mathbf{S}^{1}-p, \varphi_{q}\right)$ will constitute an atlas. Since there are no 'singular' points involved, it is clear that $\varphi_{p}, \varphi_{q}$ and their inverses are $C^{\infty}$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-214}
\end{center}

Fig. 13-6

(b) Suppose (U, $\phi$ ) covered $\mathbf{S}^{1}\left(\mathbf{U}=\mathbf{S}^{1}\right)$ and $\phi$ mapped $\mathbf{S}^{1}$ to the real line $\left(x^{1}\right)$, with both $\phi$ and $\boldsymbol{\phi}^{-1}$ continuous. It is not too difficult to see that $\phi$ maps the circle to a closed interval I: for continuous maps take bounded, closed sets to bounded, closed sets, and connected sets to connected sets; and the only bounded, closed, connected subsets of the real line are closed finite intervals. For any point $P$ on the circle let $P^{\prime}$ be its diametrically opposite point. The map $g(t) \equiv \phi\left[\left(\phi^{-1}(t)\right)^{\prime}\right]$ takes a real number $t$ in $\mathbf{I}$, maps it to a unique point $P$ on $\mathbf{S}^{1}$, goes to the (unique) diametrically opposite point $P^{\prime}$, and returns to a unique real number $t^{\prime}$ in $\mathbf{I}$; it is thus a continuous map from $\mathbf{I}$ to $\mathbf{I}$. As such, it must (by a familiar theorem of analysis) have a fixed point:

$$
g\left(t_{0}\right)=t_{0} \text { for some } t_{0} \text { in } \mathbf{I}
$$

But this means that $\phi$ sends some pair of diametrically opposite points on $\mathbf{S}^{1}$ to the same real number, denying one-oneness of $\phi$.

13.14 A manifold in $\mathbf{R}^{4}$ is defined by the charts $\left(k=\ldots,-2,-1,0,1,2, \ldots ; x^{1}>0\right)$

$$
\mathbf{r}_{(k)}:\left\{\begin{array}{l}
y^{1}=x^{1} \cos x^{2} \cos x^{3} \\
y^{2}=x^{1} \cos x^{2} \sin x^{3} \\
y^{3}=x^{1} \sin x^{2} \\
y^{4}=a\left(x^{2}+x^{3}\right)
\end{array} \quad\left((k-1) \frac{\pi}{2}<x^{3}<(k+1) \frac{\pi}{2}\right)\right.
$$

(a) Show that on each coordinate patch, the mapping $\mathbf{r}_{(k)}$ is one-to-one; hence, $\varphi_{(k)}=$ $\mathbf{r}_{(k)}^{-1}: U_{(k)} \rightarrow \mathbf{R}^{3}$ exists. (b) Show that both $\varphi_{(k)}$ and $\varphi_{(k)}^{-1}$ are continuous. (c) Show that the manifold is generated by a line in $\mathbf{R}^{4}$ moving along an axis orthogonal to it, with the axis, in turn, orthogonal to the hyperplane $y^{4}=0$ (use vector geometry in $\mathbf{R}^{4}$ ). Verify that the parameter $x^{1}$ measures the distance from a given point on the manifold to the axis. (d) Show that the parametric section $x^{3}=0$ is a right helicoid (Example 10.4), lying in the hyperplane $y^{2}=0$ ( $\mathbf{R}^{3}$ coordinatized by $\left.y^{1}, y^{3}, y^{4}\right)$.

(a) Assume that $\mathbf{r}_{(k)}\left(x^{i}\right)=\mathbf{r}_{(k)}\left(u^{i}\right)$; we want to show that $\left(x^{1}, x^{2}, x^{3}\right)=\left(u^{1}, u^{2}, u^{3}\right)$. Now,

$$
\left.\begin{array}{c}
x^{1} \cos x^{2} \cos x^{3}=u^{1} \cos u^{2} \cos u^{2} \\
x^{1} \cos x^{2} \sin x^{3}=u^{1} \cos u^{2} \sin u^{3}
\end{array}\right\} \rightarrow \tan x^{3}=\tan u^{3}
$$

But, for $\mathrm{U}_{(k)}$, the argument of the tangent function is restricted to a range of $\pi$ units; so $x^{3}=u^{3}$. It follows that

$$
a\left(x^{2}+x^{3}\right)=a\left(u^{2}+u^{3}\right) \rightarrow x^{2}=u^{2}
$$

Finally, from $x^{1} \sin x^{2}=u^{1} \sin u^{2}$, we obtain $x^{1}=u^{1}$.\\
(b) From the form of $\varphi_{(k)}^{-1} \equiv \mathbf{r}_{(k)}$, this function is $C^{\infty}$. To solve for $\left(x^{i}\right)$ in terms of $\left(y^{i}\right)$ (to find $\left.\varphi_{(k)}\right)$, write

$$
\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}=\left(x^{1}\right)^{2}\left(\cos ^{2} x^{2}\right)\left(\cos ^{2} x^{3}+\sin ^{2} x^{3}\right)+\left(x^{1}\right)^{2}\left(\sin ^{2} x^{2}\right)=\left(x^{1}\right)^{2}
$$

or $x^{1}=\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}} \quad\left(\right.$ since $\left.x^{1}>0\right)$. Then

$$
\sin x^{2}=\frac{y^{3}}{x^{1}}=\frac{y^{3}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}}}
$$

or, for a suitable branch of the function $\sin ^{-1}$,

$$
x^{2}=\sin ^{-1}\left(\frac{y^{3}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}}}\right)
$$

It is seen that

$$
\varphi_{(k)}:\left\{\begin{array}{l}
x^{1}=\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}} \\
x^{2}=\sin ^{-1}\left(\frac{y^{3}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}}}\right) \\
x^{3}=\frac{y^{4}}{a}-\sin ^{-1}\left(\frac{y^{3}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}}}\right)
\end{array}\right.
$$

is continuous (in fact, $C^{\infty}$ ).

(c) The axis orthogonal to $y^{4}=0$ is the vector $\mathbf{e}_{4}$ in $\mathbf{R}^{4}$. At the point $y^{4}=a\left(x^{2}+x^{3}\right)=$ const. on the manifold, we have (with $x^{2}, x^{3}$ constants and $x^{1}=t$ )

$$
y^{1}=t \cos x^{2} \cos x^{3} \quad y^{2}=t \cos x^{2} \sin x^{3} \quad y^{3}=t \sin x^{2} \quad y^{4}=\text { const } .
$$

-a straight line with direction vector orthogonal to $\mathbf{e}_{4}$. A previous calculation gives the distance from $\left(y^{1}, y^{2}, y^{3}, y^{4}\right)$ on $\mathbf{M}$ to $\left(0,0,0, a\left(x^{2}+x^{3}\right)\right)$ as

$$
\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}}=x^{1}
$$

(d) Set $x^{3}=0$ and the map reduces to

$$
y^{1}=x^{1} \cos x^{2} \quad y^{2}=0 \quad y^{3}=x^{1} \sin x^{2} \quad y^{4}=a x^{2}
$$

13.15 Derive the charts of Example 13.11(a), using stereographic projection (Fig. 13-7).

As $P$ is a "convex" combination of $Q$ and $\mathrm{N}$,


\begin{equation*}
\left(y^{1}, y^{2}, y^{3}\right)=\lambda\left(x^{1}, x^{2}, 0\right)+(1-\lambda)(0,0, a) \tag{1}
\end{equation*}


\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-215}
\end{center}

Fig. 13-7

To determine $\lambda(\lambda>0)$, write

$$
a^{2}=\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}=\left(\lambda x^{1}\right)^{2}+\left(\lambda x^{2}\right)^{2}+[(1-\lambda) a]^{2}
$$

and solve, obtaining


\begin{equation*}
\lambda=\frac{2 a^{2}}{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}+a^{2}} \tag{2}
\end{equation*}


[Note that $\lambda$ is less than or greater than 1 according as $P$ lies in the northern or southern hemisphere; $\lambda \neq 0$, so this patch omits the north pole.] Together, (1) and (2) yield the chart $\varepsilon=+1$; the chart $\varepsilon=-1$ is obtained by changing $a$ to $-a$ in the above (stereographic projection from the south pole).

\section*{VECTOR FIELDS ON MANIFOLDS}
13.16 The hyperboloid of one sheet $4\left(y^{1}\right)^{2}+4\left(y^{2}\right)^{2}-\left(y^{3}\right)^{2}=16$ is a $C^{\infty} 2$-manifold $\mathbf{M}$, by the coordinatization $(k=1,2)$

$$
\varphi_{(k)}^{-1}:\left\{\begin{array}{l}
y^{1}=2 \cos x^{1} \cosh x^{2} \\
y^{2}=2 \sin x^{1} \cosh x^{2} \\
y^{3}=4 \sinh x^{2}
\end{array} \quad\left((k-2) \pi<x^{1}<k \pi\right)\right.
$$

with $\mathbf{U}_{(1)}=\mathbf{U}_{p}$ and $p=(2,0,0), \mathbf{U}_{(2)}=\mathbf{U}_{q}$ and $q=(-2,0,0)$. Represent the vector field on $\mathbf{M}$ given by

$$
\left(V^{i}\right)=\left(4 \sinh x^{2}, 4 \cosh x^{2}\right)
$$

in terms of $(a)$ a vector basis for the tangent space $T_{p}(\mathbf{M})$, and $(b)$ extrinsically. (c) Describe this field geometrically.

(a) By the usual tools of surface theory (Section 10.5):

$$
\begin{aligned}
& \mathbf{r}=\left(2 \cos x^{1} \cosh x^{2}, 2 \sin x^{1} \cosh x^{2}, 4 \sinh x^{2}\right) \\
& E_{1}=\mathbf{r}_{1}=\left(-2 \sin x^{1} \cosh x^{2}, 2 \cos x^{1} \cosh x^{2}, 0\right) \\
& E_{2}=\mathbf{r}_{2}=\left(2 \cos x^{1} \sinh x^{2}, 2 \sin x^{1} \sinh x^{2}, 4 \cosh x^{2}\right) \\
& V=V^{i} E_{i}=\left(-8 \sin x^{1} \sinh x^{2} \cosh x^{2}, 8 \cos x^{1} \sinh x^{2} \cosh x^{2}, 0\right) \\
& \quad+\left(8 \cos x^{1} \sinh x^{2} \cosh x^{2}, 8 \sin x^{1} \sinh x^{2} \cosh x^{2}, 16 \cosh ^{2} x^{2}\right) \\
& \quad=\left(4\left(\cos x^{1}-\sin x^{1}\right) \sinh 2 x^{2}, 4\left(\cos x^{1}+\sin x^{1}\right) \sinh 2 x^{2}, 16 \cosh ^{2} x^{2}\right)
\end{aligned}
$$

(b) From the equations for $y^{1}, y^{2}, y^{3}$ we may calculate:

$$
\begin{array}{ll}
\cosh x^{2}=\frac{1}{2} \sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}} & \sinh x^{2}=\frac{1}{4} y^{3} \\
\cos x^{1}=\frac{y^{1}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}} & \sin x^{1}=\frac{y^{2}}{\sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}}
\end{array}
$$

so that

$$
\begin{aligned}
& E_{1}=\left(-y^{2}, y^{1}, 0\right) \\
& E_{2}=\left(\frac{y^{1} y^{3}}{2 \sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}}, \frac{y^{2} y^{3}}{2 \sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}}, 2 \sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}\right) \\
& \left(V^{i}\right)=\left(y^{3}, 2 \sqrt{\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}}\right) \\
& V=V^{i} E_{i}=\left(-y^{2} y^{3}, y^{1} y^{3}, 0\right)+\left(y^{1} y^{3}, y^{2} y^{3}, 4\left(y^{1}\right)^{2}+4\left(y^{2}\right)^{2}\right) \\
& \quad=\left(y^{3}\left(y^{1}-y^{2}\right), y^{3}\left(y^{1}+y^{2}\right),\left(y^{3}\right)^{2}+16\right)
\end{aligned}
$$

(using the equation of the hyperboloid). Hence, in terms of the coordinates $\left(y^{i}\right)$,

$$
V=\sigma=y^{3}\left(y^{1}-y^{2}\right) d y^{1}+y^{3}\left(y^{1}+y^{2}\right) d y^{2}+\left[\left(y^{3}\right)^{2}+16\right] d y^{3}
$$

(c) See Fig. 13-8 and note that the first component is zero in the plane $y^{1}=y^{2}$. Hence, along the curve of intersection, the field is always parallel to the $y^{2} y^{3}$-plane. Similarly, along $y^{1}=-y^{2}$, the field is parallel to the $y^{1} y^{3}$-plane. On the circle $y^{3}=0$ the field is $(0,0,16)$, or vertical. Since the third component is $\geqq 16$, there is always a vertical component.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217}
\end{center}

Fig. 13-8

13.17 Show that the restrictions of (a) $\sigma_{1}=y^{1} d y^{2}-y^{2} d y^{1}$ and (b) $\sigma_{2}=\left(y^{2}-y^{3}\right) d y^{1}-\left(y^{1}+\right.$ $\left.y^{3}\right) d y^{2}+\left(y^{1}+y^{2}\right) d y^{3}$ to the sphere $\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}=a^{2}$ are vector fields. (See Fig. 13-9 for a graph of selected values of $\sigma_{1}$.) By the well-known "Hairy-Ball Theorem" (every head of hair has a cowlick), every continuous vector field on $\mathbf{S}^{2}$ (and also on $\mathbf{S}^{n}$, for all even integers $n$ ) is zero at some point on the sphere. In fact, the field must vanish at some point of an arbitrarily selected, open hemisphere. (c) Find the zero points explicitly.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-217(1)}
\end{center}

Fig. 13-9\\
(a) The normal vector to $\mathbf{S}^{2}$ is $\omega=2 y^{1} d y^{1}+2 y^{2} d y^{2}+2 y^{3} d y^{3}$ and


\begin{gather*}
\sigma_{1} \cdot \frac{1}{2} \omega=\left(-y^{2}\right)\left(y^{1}\right)+\left(y^{1}\right)\left(y^{2}\right)+(0)\left(y^{3}\right)=0 \\
\sigma_{2} \cdot \frac{1}{2} \omega=\left(y^{2}-y^{3}\right)\left(y^{1}\right)-\left(y^{1}+y^{3}\right)\left(y^{2}\right)+\left(y^{1}+y^{2}\right)\left(y^{3}\right)  \tag{b}\\
=y^{1} y^{2}-y^{1} y^{3}-y^{1} y^{2}-y^{2} y^{3}+y^{1} y^{3}+y^{2} y^{3}=0
\end{gather*}


(c) If $\sigma_{1}=0,-y^{2}=y^{1}=0$ and $0^{2}=0^{2}+\left(y^{3}\right)^{2}=a^{2}$, or $y^{3}= \pm a$. Thus, the zero points are $(0,0, \pm a)$. For $\sigma_{2}=0$,

$$
y^{2}-y^{3}=y^{1}+y^{3}=y^{1}+y^{2}=0 \quad \rightarrow \quad y^{2}=y^{3}=-y^{1}
$$

and $\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}=a^{2}=3\left(y^{1}\right)^{2}$, or $y^{1}= \pm a / \sqrt{3}$. Hence, the zero points are $\pm(a / \sqrt{3}$, $-a / \sqrt{3},-a / \sqrt{3})$.

13.18 Consider a manifold whose coordinatization is not easily determined ( $\mathbf{S O}(n)$, of Example 13.2(e), is such a manifold in $\mathbf{R}^{n^{2}}$ ) for which, therefore, base vectors $\mathbf{r}_{i}=E_{i}$ for $T_{p}(\mathbf{M})$ are unavailable. Develop a reasonable definition of $T_{p}(\mathbf{M})$ in this situation, which possesses the salient properties of a "tangent space" at point $p$.

To get an idea of what may be desirable, examine the case when the vectors $\mathbf{r}_{1}, \mathbf{r}_{2}, \ldots, \mathbf{r}_{n}$ are available. Each tangent vector has the form $V=V^{i} \mathbf{r}_{i}$, and when $V$ is the tangent vector of a curve $\mathscr{C}$ on M-the image of $\mathscr{C}^{\prime}: x^{i}=x^{i}(t)$ in the coordinate space $\mathbf{R}^{n}-$ then

$$
V=\frac{d x^{i}}{d t} \mathbf{r}_{i} \quad \text { or } \quad V^{i}=\frac{d x^{i}}{d t}
$$

Thus $\left(V^{i}\right)$ is a direction vector. Recall that

$$
\begin{gathered}
\mathbf{r}=\mathbf{r}\left(x^{1}, \ldots, x^{n}\right) \equiv \mathbf{r}\left(y^{1}\left(x^{1}, \ldots, x^{n}\right), \dot{y}^{2}\left(x^{1}, \ldots, x^{n}\right), \ldots, y^{m}\left(x^{1}, \ldots, x^{n}\right)\right) \\
\mathbf{r}_{i}=\left(\frac{\partial y^{1}}{\partial x^{i}}, \frac{\partial y^{2}}{\partial x^{i}}, \ldots, \frac{\partial y^{m}}{\partial x^{i}}\right)
\end{gathered}
$$

whence

Thus when we write $V^{i} \mathbf{r}_{i}$ we are actually indicating the $m$ directional derivatives

$$
V^{i} \frac{\partial y^{j}}{\partial x^{i}} \equiv \nabla y^{j} \cdot V \quad(1 \leqq j \leqq m)
$$

with each $y^{j}\left(x^{1}, x^{2}, \ldots, x^{n}\right)$ a $C^{\infty}$ real-valued function (defined on $\mathbf{M}$ if we identify the points of $\mathbf{M}$ with their coordinates $\left(x^{i}\right)$ in $\mathbf{R}^{n}$ ). It is customary to let the directional derivative of a function $f: \mathbf{R}^{n} \rightarrow \mathbf{R}$ in the direction $V$ be denoted

$$
V(f) \equiv \nabla f \cdot V
$$

Thus, each vector $V$ maps a differentiable, real-valued function $f$ to its directional derivative in the direction $V$. The properties of this mapping are immediate: If $f$ and $g$ denote any two differentiable functions from $\mathbf{M}$ to $\mathbf{R}$, with $f g$ denoting the ordinary product of two functions, and if $a$ and $b$ are two scalar constants, then

$$
\begin{aligned}
\text { linearity } & V(a f+b g)=a V(f)+b V(g) \\
\text { Leibniz' rule } & V(f g)=V(f) g+f V(g)
\end{aligned}
$$

With this in mind, and armed with the knowledge that the directional derivatives of all functions on $\mathbf{M}$ would be enough information to construct the basis $\left\{\mathbf{r}_{i}\right\}$ when $\mathbf{r}$ is known, we frame

Definition 9: By $\mathbf{C}^{\infty}(p)$ will be understood the real-valued $C^{\infty}$ functions on $\mathbf{U}_{p}$, such that any two functions that agree on some neighborhood of $p$ are identified.

Definition 10: The tangent space $T_{p}(\mathbf{M})$ at $p$ is the set of all mappings $V_{p}: \mathbf{C}^{\infty}(p) \rightarrow \mathbf{R}$ that satisfy for all $a, b$ in $\mathbf{R}$ and $f, g$ in $\mathbf{C}^{\infty}(p)$ the two conditions\\
(i) $V_{p}(a f+b g)=a V_{p}(f)+b V_{p}(g)$

(ii) $V_{p}(f g)=V_{p}(f) g+f V_{p}(g)$

with the vector-space operations in $T_{p}(\mathbf{M})$ defined by

$$
\begin{aligned}
\left(U_{p}+V_{p}\right)(f) & \equiv U_{p}(f)+V_{p}(f) \\
\left(a V_{p}\right)(f) & \equiv a V_{p}(f)
\end{aligned}
$$

Any $V_{p}$ in $T_{p}(\mathbf{M})$ will be called a tangent vector to $\mathbf{M}$ at $p$. This definition has the advantage not only of dispensing with coordinates, but of enabling one to extend naturally a mapping $F: \mathbf{M} \rightarrow \mathbf{N}$ (from one manifold to another) to a mapping $F_{*}: T_{p}(\mathbf{M}) \rightarrow T_{p^{\prime}}(\mathbf{N})$ at each point $p$ in $\mathbf{M}$, where $p^{\prime}=F(p)$. Such an extension cannot be accomplished using the more elementary definition.

Remark 2: The vectors of $T_{p}(\mathbf{M})$ as originally defined, if regarded as mappings on $\mathbf{C}^{\infty}(p)$, are members of the abstract $T_{p}(\mathbf{M})$ (Definition 10). In more advanced treatments it is shown that the reverse is true and that $\operatorname{dim} T_{p}(\mathbf{M})=\operatorname{dim} \mathbf{M}=n$. Hence, the two approaches to tangent spaces are equivalent.

\section*{TENSOR FIELDS}
13.19 Show that tensor fields always have the property of being bilinear with respect to scalar functions (as well as to scalar constants), unlike differential operators.

We must show that for any scalar function $f$ on $\mathbf{M}$ and any tensor $T$ of type $\left(\begin{array}{c}0 \\ r\end{array}\right)$,

$$
T\left(V_{1}, \ldots, f V_{i}, \ldots, V_{r}\right)=f T\left(V_{1}, \ldots, V_{i}, \ldots, V_{r}\right)
$$

This is true, since it is true at each point $p$ of $\mathbf{M}$ :

$$
\begin{aligned}
T_{p}\left(V_{1}, \ldots, f V_{i}, \ldots, V_{r}\right) & \equiv T\left(V_{1 p}, \ldots, f(p) V_{i p}, \ldots, V_{r p}\right)=f(p) T\left(V_{1 p}, \ldots, V_{i p}, \ldots, V_{r p}\right) \\
& =f T_{p}\left(V_{1}, \ldots, V_{i}, \ldots, V_{r}\right)
\end{aligned}
$$

13.20 Show how to interpret the tangent vector to a curve on a surface $S$ as a (contravariant) tensor of type $\left(\begin{array}{l}1 \\ 0\end{array}\right)$.

Let $\mathbf{c}=\mathbf{c}(t)$ be a given curve on $\mathbf{M}=\mathbf{S}$, with

$$
\mathbf{c}_{*}(t)=\frac{d \mathbf{c}}{d t}=\frac{\partial \mathbf{y}}{\partial x^{i}} \frac{d x^{i}}{d t}
$$

Define for any one-form $\omega=a_{i} d z^{i}$ the linear mapping from $T^{*}(\mathbf{M})$ to $\mathbf{R}$ :

$$
T(\omega)=a_{i} \frac{d x^{i}}{d t} \equiv \omega\left(\frac{d \mathbf{x}}{d t}\right)
$$

Under the standard basis $\left\{d z^{1}, d z^{2}, \ldots, d z^{n}\right\}$ of $T_{p}^{*}(\mathbf{M})$, and with $\omega=d z^{i} \equiv \delta_{j}^{i} d z^{i}$,

$$
T^{i}=T\left(d z^{i}\right)=\delta_{j}^{i} \frac{d x^{j}}{d t}=\frac{d x^{i}}{d t}
$$

(We saw earlier that the $d x^{i} / d t$ were contravariant components.)

13.21 Show how to interpret the gradient of a function as a tensor of type $\left(\begin{array}{l}0 \\ 1\end{array}\right)$.

Let $f$ have gradient $\nabla f \equiv\left(\partial f / \partial x^{i}\right)$. Define the linear mapping

$$
T(V)=V^{i} \frac{\partial f}{\partial x^{i}} \quad\left(\frac{\partial f}{\partial x^{i}} \text { fixed }\right)
$$

Use the basis $\left\{E_{1}, E_{2}, \ldots, E_{n}\right\}$ for $T_{p}(\mathbf{M})$; then with $V=E_{i} \equiv \delta_{i}^{j} E_{j}$,

$$
T_{i}=T\left(E_{i}\right)=\delta_{i}^{j} \frac{\partial f}{\partial x^{j}}=\frac{\partial f}{\partial x^{i}}
$$

\section*{Supplementary Problems}
13.22 The set of all $2 \times 2$ matrices of the form

$$
\left[\begin{array}{rr} 
\pm 1 & 0 \\
0 & \pm 1
\end{array}\right]
$$

where all possible combinations of signs are taken, forms a four-element subset of $\mathbf{G L}(2, \mathbf{R})$. Is it a subgroup?

13.23 Prove that $\mathrm{SU}(n)$, the set of all $n \times n$ matrices over the complex numbers having determinant +1 , is a subgroup of $\mathbf{G L}(n, \mathbf{C})$. [Hint: $\operatorname{det} A B=(\operatorname{det} A)(\operatorname{det} B)$ holds for complex matrices.]

13.24 Show that the operator $L(f)=\int_{0}^{1} f(x) d x$ is a linear functional over the set of continuous, real-valued functions on $[0,1]$.

13.25 In terms of the standard basis $\left\{d x^{i}\right\}$ of $\left(\mathbf{R}^{3}\right)^{*}$, a new basis is defined by

$$
\boldsymbol{\beta}^{1}=d x^{1}-2 d x^{3} \quad \boldsymbol{\beta}^{2}=2 d x^{1}+d x^{2} \quad \boldsymbol{\beta}^{3}=d x^{1}+d x^{3}
$$

Find the corresponding dual basis $\left\{\mathbf{b}_{i}\right\}$ for $\mathbf{R}^{3}$ in terms of $\left(\mathbf{e}_{i}\right)$, using (13.6). Check your answer by making several calculations of the form $\omega(v)=\bar{\omega}(\bar{v})$ (a change of basis does not affect the value a linear functional assigns to a vector).

13.26 Consider a tensor $T(\boldsymbol{\omega} ; \mathbf{v})$ over a vector space of dimension $n$ and its dual, with components $T_{j}^{i}$. (a) Show that the trace $\tau(T) \equiv T_{i}^{i}$ is invariant under changes of bases. (b) Find $\tau(T)$ for the tensor defined by $T(\boldsymbol{\omega} ; \mathbf{v})=\boldsymbol{\omega}(\mathbf{v})$.

13.27 Show that every metric tensor $G$ induces a one-to-one mapping (which is an isomorphism, since it is linear) $\hat{G}: \mathbf{V} \rightarrow \mathbf{V}^{*}$ from a vector space to its dual, under the definition: For each fixed $\mathbf{u}$ in $\mathbf{V}$, let $\hat{G}(\mathbf{u})$ be the linear functional $\hat{G}(\mathbf{u})(\mathbf{v})=G(\mathbf{u}, \mathbf{v})$, for all $\mathbf{v}$ in $\mathbf{V}$. This proves for vector spaces of arbitrary dimension:

Theorem 13.3: If $\mathbf{V}$ possesses a metric tensor, then $\mathbf{V}$ is isomorphic to its dual $\mathbf{V}^{*}$.

13.28 Find a convenient atlas showing that the set in $\mathbf{R}^{4}$ given by the equation

$$
\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}-\left(y^{4}\right)^{2}=a^{2}
$$

can be made into a $C^{\infty} 3$-manifold. [Hint: Use radicals, as in Example 13.11(b); here, 6 charts will suffice.]

13.29 Show that the restriction of $\sigma=y^{1} d y^{2}-y^{2} d y^{1}+y^{3} d y^{4}-y^{4} d y^{3}$ on $\mathbf{R}^{4}$ to the sphere $\mathbf{S}^{3}$ is a nonzero vector field on $\mathbf{S}^{3}$.

13.30 Extend Problem 13.29 to the sphere $\mathbf{S}^{2 k-1}(k \geqq 2)$.

13.31 Show that if there are only two points, $p_{1}$ and $p_{2}$, on $\mathbf{S}^{2}$ where a vector field is zero, those points must be antipodal (endpoints of a diameter).

13.32 Show, by geometric reasoning, that there exists a continuous, nonzero vector field on the torus.

13.33 Show that the restrictions of the following one-forms to $\mathbf{S}^{4},\left(y^{1}\right)^{2}+\cdots+\left(y^{5}\right)^{2}=1$, are vector fields on $\mathbf{S}^{4}$, and find the points where they are zero:

(a) $\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}$

(b) $\sigma=\left(y^{2}-y^{3}-y^{4}\right) d y^{1}+\left(y^{3}-y^{1}\right) d y^{2}+\left(y^{1}-y^{2}+y^{5}\right) d y^{3}+y^{1} d y^{4}-y^{3} d y^{5}$

B3.3 Although no nonvanishing continuous vector field exists on the 2-sphere $\mathbf{S}^{2}$, there are three, mutually orthogonal, unit vector fields on $\mathbf{S}^{3} \subset \mathbf{R}^{4}$. These are, in the extrinsic representation of $\mathbf{S}^{3}$,

$$
\begin{aligned}
& \sigma_{1}=-y^{1} d y^{1}+y^{2} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4} \\
& \sigma_{2}=-y^{3} d y^{1}-y^{4} d y^{2}+y^{1} d y^{3}+y^{2} d y^{4} \\
& \sigma_{3}=-y^{4} d y^{1}+y^{3} d y^{2}-y^{2} d y^{3}+y^{1} d y^{4}
\end{aligned}
$$

Show this. [Note: Manifolds with such vector-field bases are called parallelizable. The manifolds $\mathbf{S}^{1}, \mathbf{S}^{3}$, $\mathbf{S}^{7}$ - and no other $n$-spheres-and the torus are examples.]

13.35 Without resorting to coordinate patches, express extrinsically the collection of tangent spaces $T(\mathbf{M})$, if $\mathbf{M}$ is the hyperboloid of one sheet $\left(y^{1}\right)^{2}-4\left(y^{2}\right)^{2}+4\left(y^{3}\right)^{2}=4$.

13.36 For the manifold $\mathbf{M}$ of Problem 13.35, consider the coordinate patch

$$
y^{1}=x^{1} \quad y^{2}=x^{2} \quad y^{3}=\sqrt{1-\left(x^{1} / 2\right)^{2}+\left(x^{2}\right)^{2}}
$$

valid for $y^{3}>0$. Find an expression for an arbitrary vector in $T_{p}(\mathbf{M})$.

13.37 One way to show that two surfaces meet at right angles is to show that along the curve of intersection the normal vector to one lies in the tangent space of the other. Illustrate this idea for the sphere $\left(y^{1}\right)^{2}+\left(y^{2}\right)^{2}+\left(y^{3}\right)^{2}=16$ and the cone $\left(y^{3}\right)^{2}=9\left(y^{1}\right)^{2}+9\left(y^{2}\right)^{2}$, the latter coordinatized by

$$
y^{1}=x^{1} \quad y^{2}=x^{2} \quad y^{3}=3 \sqrt{\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}}
$$

\section*{Answers to Supplementary Problems}
\section*{CHAPTER 1}
$1.15 a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}+a_{4} b_{4}+a_{5} b_{5}+a_{6} b_{6}$

1.16 $R_{j k 1}^{1}+R_{j k 2}^{2}+R_{j k 3}^{3}+R_{j k 4}^{4}$. The index $i$ is a dummy index, while $j$ and $k$ are free indices; there are 16 summations.

$1.17 x_{j}$

1.18 (a) $n$; (b) $\delta_{i j} \delta_{i j}=\delta_{i i}=n ;(c) \delta_{i j} c_{i j}=c_{i i}=c_{11}+c_{22}+c_{33}+\cdots+c_{n n}$

$1.19 a_{i 3} b_{i 3} \quad(n=3)$

$1.20 \quad a_{i j} x_{i} x_{j} \quad(n=3)$

$1.21 y_{i}=c_{i j} x_{j} \quad(n=2)$

$1.22 a_{1 k} \quad(k=2,3)$

$1.23 \frac{\partial}{\partial x_{k}}\left(a_{i j} x_{j}\right)=a_{i j} \frac{\partial}{\partial x_{k}}\left(x_{j}\right)=a_{i j} \delta_{j k}=a_{i k}$

$1.24 a_{i k}\left[\left(x_{i}\right)^{2}+2 x_{i} x_{k}\right]$ [not summed on $k$ ]

$1.25\left(a_{l i j}+a_{i l j}+a_{i j l}\right) x_{i} x_{j}$

$1.26 a_{k l}+a_{l k}$

1.27 (a) $b_{j}^{i} T_{i}^{r r}$; (b) $a_{i j} b_{j r} x_{r}$; (c) $a_{i j k} b_{i r} b_{j s} b_{k t} x_{r} x_{s} x_{t}$

1.28 (c) $a_{i j}\left(x_{i}+x_{j}\right)=a_{i j}\left(\varepsilon_{j} x_{i}+\varepsilon_{i} x_{j}\right)=a_{i j} \varepsilon_{j} x_{i}+a_{i j} \varepsilon_{i} x_{j}=a_{j i} \varepsilon_{j} x_{i}+a_{i j} \varepsilon_{i} x_{j}=2 a_{i j} \varepsilon_{i} x_{j}$

\section*{CHAPTER 2}
(a) and $(b)\left[\begin{array}{lllll}u^{11} & u^{12} & u^{13} & u^{14} & u^{15} \\ u^{21} & u^{22} & u^{23} & u^{24} & u^{25} \\ u^{31} & u^{32} & u^{33} & u^{34} & u^{35}\end{array}\right]$

(c) $\left[\begin{array}{lll}u^{11} & u^{12} & u^{13} \\ u^{21} & u^{22} & u^{23} \\ u^{31} & u^{32} & u^{33} \\ u^{41} & u^{42} & u^{43} \\ u^{51} & u^{52} & u^{53}\end{array}\right]$

(d) $\left[\begin{array}{llllll}1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0\end{array}\right]$

2.25

(a) $\left[\begin{array}{l}5 \\ 0 \\ 5\end{array}\right]$

(b) $\left[\begin{array}{lll}1 & 2 & -4 \\ 2 & 2 & -2\end{array}\right]$

2.29 (a) $17 ;(b) 0 ;(c)-1$

2.30 (a) $-a_{12} a_{21} a_{33} a_{44}+a_{12} a_{21} a_{34} a_{43}+a_{12} a_{23} a_{31} a_{44}-a_{12} a_{23} a_{34} a_{41}-a_{12} a_{24} a_{31} a_{43}+a_{12} a_{24} a_{33} a_{41}$

(b) $-a_{12}\left|\begin{array}{lll}a_{21} & a_{23} & a_{24} \\ a_{31} & a_{33} & a_{34} \\ a_{41} & a_{43} & a_{44}\end{array}\right| \equiv a_{12} A_{12}$

2.32 (a) $\left[\begin{array}{rr}2 & -1 \\ -5 & 3\end{array}\right] \quad$ (b) $\frac{1}{7}\left[\begin{array}{rrr}1 & 3 & 2 \\ 1 & -4 & 2 \\ 3 & 2 & -1\end{array}\right]$

2.33 One need verify only that (i) interchanging a pair of consecutive indices changes the sign of a single factor in the product; (ii)

$$
\prod_{p>q} \frac{p-q}{|p-q|}=\prod 1=1
$$

$2.342 \pi / 3$

2.35 One pair are $(2,3,0)$ and $(-3,-2,5)$.

$2.36\left[\begin{array}{l}x \\ y\end{array}\right]=\left[\begin{array}{r}-1 \\ 5\end{array}\right]$

2.37 $Q=x_{1}^{2}+2 x_{2}^{2}-x_{3}^{2}+8 x_{1} x_{2}+6 x_{1} x_{3}$

$2.38 \quad A=\left[\begin{array}{rrrr}-3 & -\frac{1}{2} & -\frac{1}{2} & 3 \\ -\frac{1}{2} & -1 & 0 & 0 \\ -\frac{1}{2} & 0 & 1 & 0 \\ 3 & 0 & 0 & 0\end{array}\right]$

$2.39 \bar{c}_{i}=c_{r} b_{r i}$, where $\left(b_{i j}\right)=\left(a_{i j}\right)^{-1}$.

$2.40 \quad g_{11}=13 / 49, g_{12}=g_{21}=4 / 49, g_{22}=5 / 49$

$2.41 d(\overline{\mathbf{x}}, \overline{\mathbf{y}})=3=d(\mathbf{x}, \mathbf{y})$

\section*{CHAPTER 3}
3.23 (a) $\mathscr{I}=-2 \exp \left(2 x^{1}\right)<0$

(b) $\mathscr{T}^{-1}:\left\{\begin{array}{l}x^{1}=\frac{1}{2} \ln \left(\bar{x}^{-1} \bar{x}^{2}\right) \\ x^{2}=\frac{1}{2} \ln \left(\bar{x}^{1} / \bar{x}^{2}\right)\end{array} \quad\left(\bar{x}^{1}, \bar{x}^{2}>0\right)\right.$

(c) $\bar{J}=\left[\begin{array}{rr}1 / 2 \bar{x}^{1} & 1 / 2 \bar{x}^{2} \\ 1 / 2 \bar{x}^{1} & -1 / 2 \bar{x}^{2}\end{array}\right]=\left[\begin{array}{rr}\exp \left(x^{1}+x^{2}\right) & \exp \left(x^{1}+x^{2}\right) \\ \exp \left(x^{1}-x^{2}\right) & -\exp \left(x^{1}-x^{2}\right)\end{array}\right]^{-1}$

$3.26 \frac{\partial \bar{f}}{\partial \theta}=0$, so that $f(x, y)=\bar{f}(r)=\bar{f}\left(\sqrt{x^{2}+y^{2}}\right)=g\left(x^{2}+y^{2}\right)$.

$3.29 \delta_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=\frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{r}}{\partial \bar{x}^{j}}=\delta^{i}{ }_{j}=\bar{\delta}^{i}{ }_{j}$

3.30 The inverse Jacobian matrix at $(1,2)$ is

$$
\bar{J}=\left[\begin{array}{cc}
\bar{x}^{2} & \bar{x}^{1} \\
0 & 1
\end{array}\right]=\left[\begin{array}{ll}
2 & 1 \\
0 & 1
\end{array}\right]
$$

By Problem 3.14(a), covariance of the matrix

would imply the matrix equation

$$
E \equiv\left[e_{i j}\right]_{22}=\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]
$$

$$
\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]=\left[\begin{array}{ll}
2 & 0 \\
1 & 1
\end{array}\right]\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]\left[\begin{array}{ll}
2 & 1 \\
0 & 1
\end{array}\right]
$$

or

which is patently false.

$$
\left[\begin{array}{rr}
0 & 1 \\
-1 & 0
\end{array}\right]=\left[\begin{array}{rr}
0 & 2 \\
-2 & 0
\end{array}\right]
$$

3.32 (a) $\left(T_{j}^{i}+T_{i}^{j}\right)$ represents a tensor if and only if

$$
\left(T_{s}^{r}+T_{r}^{s}\right) \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}=T_{s}^{r} \frac{\partial \bar{x}^{i}}{\partial x^{r}} \frac{\partial x^{s}}{\partial \bar{x}^{j}}+T_{r}^{s} \frac{\partial \bar{x}^{j}}{\partial x^{s}} \frac{\partial x^{r}}{\partial \bar{x}^{i}}
$$

which requires that $J T \bar{J}=\bar{J}^{T} T J^{T}$. This last relation, in turn, generally requires that $\bar{J}=J^{T}$; i.e., $J$ must be an orthogonal matrix.

(b) $\bar{T}=J T \bar{J}$, so that $\bar{T}^{T}=\bar{T}$ if $\bar{J}=J^{T}$.

3.35 As $\mathbf{T}$ is a tensor (Example 3.4), it is an affine tensor: $\bar{T}^{i}=a_{r}^{i} T^{r}$. Thus,

$$
\frac{d \bar{T}^{i}}{d t}=a_{r}^{i} \frac{d T^{r}}{d t}
$$

showing $d \mathbf{T} / d t$ also to be an affine tensor. Any affine tensor is a fortiori a cartesian tensor.


\begin{equation*}
\bar{u}_{i} \bar{u}_{i}=\left(a_{i r} u_{r}\right)\left(a_{i s} u_{s}\right)=a_{i r} a_{i s} u_{r} u_{s}=\delta_{r s} u_{r} u_{s}=u_{r} u_{r} \tag{a}
\end{equation*}


(b) No, because distance and angles are not preserved under arbitrary linear transformations. Specifically, consider $\bar{x}^{1}=3 x^{1}, \bar{x}^{2}=x^{1}+x^{2}$. A scalar product in $\left(\bar{x}^{i}\right)$ is

$$
\bar{u}_{i} \bar{v}_{i}=\left(3 u_{1}, u_{1}+u_{2}\right) \cdot\left(3 v_{1}, v_{1}+v_{2}\right)=10 u_{1} v_{1}+u_{1} v_{2}+u_{2} v_{1}+u_{2} v_{2}
$$

This clearly will not coincide with $u_{1} v_{1}+u_{2} v_{2}$.

\section*{CHAPTER 4}
4.19 Write $[\mathbf{S T}]=\left(U_{l m n}^{i j k}\right)$. There are $\left(\begin{array}{c}3 \\ 2\end{array}\right)$ ways of choosing locations for the contraction indices $u$ and $v$ among the contravariant indices, and, for each of these, $\left(\begin{array}{l}3 \\ 2\end{array}\right)$ ways of choosing locations among the covariant indices. A given quartet of locations can be filled in 2 inequivalent ways. Thus, the desired number is

$$
\left(\begin{array}{l}
3 \\
2
\end{array}\right) \cdot\left(\begin{array}{l}
3 \\
2
\end{array}\right) \cdot 2=18
$$

4.23 First, use the device of Problem 4.11 to establish that $T_{j k l}^{i} U^{k} V^{l}$ are tensor components for all $\left(U^{i}\right)$ and $\left(V^{i}\right)$; then apply the Quotient Theorem twice.

\section*{CHAPTER 5}
5.21 $L=a \pi$; semicircle of radius $a$.

5.22 No: $Q(1,0,3)=-1$.

5.23 $L=2+e$

5.24 The true distance formula, $\overline{P_{1} P_{2}}=\sqrt{\left(x_{1}^{1}-x_{2}^{1}\right)^{2}+\left(x_{1}^{2}-x_{2}^{2}\right)^{2}-0.2021125\left(x_{1}^{1}-x_{2}^{1}\right)\left(x_{1}^{2}-x_{2}^{2}\right)}$, yields 4.751. for an error of +0.249 .

5.25

$$
G=\left[\begin{array}{ccc}
\left(x^{2}\right)^{2} & x^{1} x^{2} & 0 \\
x^{1} x^{2} & 1+\left(x^{1}\right)^{2} & 0 \\
0 & 0 & 1
\end{array}\right]
$$

$5.26\left(U_{i}\right)=(0,1,0),\left(V_{i}\right)=\left(x^{2}, x^{1}, 0\right)$

5.27 (a) $\|\mathbf{U}+\mathbf{V}\|^{2}=(\mathbf{U}+\mathbf{V})^{2}=\mathbf{U}^{2}+\mathbf{V}^{2}+2 \mathbf{U V}=\|\mathbf{U}\|^{2}+\left\|\mathbf{V}^{2}\right\|+2\|\mathbf{U}\|\|\mathbf{V}\| \cos \theta$

(b) Take $\theta=\pi / 2$ in (a).

5.28 (a) $x^{2}=C \exp \left(-2 b x^{3} / a^{2}\right)$ (a one-parameter family of spirals on the cylinder $x^{1}=a$ )\\
(b) No: the curves of (a) have tangent field $\mathbf{V}$ all along their length; but, for orthogonality, it is necessary only that the tangent at intersections with the pseudo-helix be V. For example, the curve $x^{2}=x^{3}$ on $x^{1}=a$ is also orthogonal to the pseudo-helix at the point $x^{2}=-a^{2} / 2 b, x^{3}=a^{4} / 4 b$.

$5.29 \quad x^{1}=d \exp \left(-\left(x^{2}\right)^{2} / 2\right) \quad(d=$ const. $)$

$5.30 f^{\prime}\left(\theta_{0}\right) g^{\prime}\left(\theta_{0}\right)=-a^{2}$ at intersection points.

5.32 (a) $g^{i \alpha}=\lambda(\alpha) \delta_{\alpha}^{i}$, which is tantamount to $g^{i j}=g_{i j}=0$ for $i \neq j$.

$5.33\|\mathbf{V}\|=1, L=\pi / 2$

$5.34 x^{1}=a, x^{3}=b \cot x^{2}+c(c=$ const. $)$

\section*{CHAPTER 6}
6.19 $\bar{x}^{i}=\frac{1}{2} a_{r s}^{i} x^{r} x^{s}+b_{r}^{i} x^{r}+c^{i} \quad$ (the $b_{j}^{i}$ and $c^{i}$ constants)

$6.20(a)$

$$
G=\left[\begin{array}{cc}
16\left(x^{1}\right)^{2}+1 & 4 x^{1}-3 \\
4 x^{1}-3 & 10
\end{array}\right]
$$

(b) $\Gamma_{111}=16 x^{1}, \Gamma_{112}=4$, all others 0

6.22 The values, in $\left(x^{i}\right)$, of the $\partial x^{i} / \partial \bar{x}^{j}$ are easiest found by inverting $J \equiv\left(\partial \bar{x}^{i} / \partial x^{j}\right)$. Final results are:

$$
\Gamma_{11}^{1}=\Gamma_{22}^{1}-\Gamma_{12}^{2}=\Gamma_{21}^{2}=1
$$

6.23 From Problem 6.21(b), $\Gamma_{j k}^{i}=0$ for $j \neq k$; while, for $j=k=\alpha$ (no summation on $\alpha$ ),

$$
\Gamma_{\alpha \alpha}^{i}=\frac{\partial}{\partial x^{\alpha}}\left(\frac{\partial \bar{x}^{r}}{\partial x^{\alpha}}\right) \frac{\partial x^{i}}{\partial \bar{x}^{r}}=\left(d_{\alpha} \frac{\partial \bar{x}^{r}}{\partial x^{\alpha}}\right) \frac{\partial x^{i}}{\partial \bar{x}^{r}}=d_{\alpha} \delta_{\alpha}^{i}
$$

$6.26-\Gamma_{221}=\Gamma_{212}=\Gamma_{122}=x^{1} ; \Gamma_{21}^{2}=\Gamma_{12}^{2}=1 / x^{1}, \Gamma_{22}^{1}=-x^{1}$

6.27

$$
\overline{\mathscr{J}}=\left|\begin{array}{lll}
a_{1}^{1} \exp \bar{x}^{1} & 2 a_{2}^{1} \exp 2 \bar{x}^{2} & 3 a_{3}^{1} \exp 3 \bar{x}^{3} \\
a_{1}^{2} \exp \bar{x}^{-1} & 2 a_{2}^{2} \exp 2 \bar{x}^{2} & 3 a_{3}^{2} \exp 3 \bar{x}^{3} \\
a_{1}^{3} \exp \bar{x}^{1} & 2 a_{2}^{3} \exp 2 \bar{x}^{2} & 3 a_{3}^{3} \exp 3 \bar{x}^{3}
\end{array}\right|=\left[6 \exp \left(\bar{x}^{1}+2 \bar{x}^{2}+3 \bar{x}^{3}\right)\right] \operatorname{det}\left(a_{j}^{i}\right) \neq 0
$$

Hence the condition is $\operatorname{det}\left(a_{j}^{i}\right) \neq 0$.

6.29 $\bar{x}^{i}=A^{i} x^{1} \sin x^{2}+B^{i} x^{1} \cos x^{2}+C^{i} \quad(i=1,2)$, with

$$
x^{1}\left|\begin{array}{ll}
A^{1} & B^{1} \\
A^{2} & B^{2}
\end{array}\right| \neq 0
$$

for a bijection.

6.30 No, because of the presence of Christoffel symbols in (6.7).

$6.31 \quad T_{j r s, k}^{i}=\frac{\partial T_{j r s}^{i}}{\partial x^{k}}+\Gamma_{u k}^{i} T_{j r s}^{u}-\Gamma_{j k}^{u} T_{u r s}^{i}-\Gamma_{r k}^{u} T_{j u s}^{i}-\Gamma_{s k}^{u} T_{j r u}^{i}$

$6.36 \kappa=1 / b$

6.37 (a) $\frac{d^{2} u}{d s^{2}}=\frac{d^{2} v}{d s^{2}}=0$

(b) $x^{2}=p\left(x^{1}\right)^{2}+q$ (a two-parameter family of "parabolas")\\
$6.38(a)$

$$
\begin{aligned}
& \frac{d^{2} x^{2}}{d s^{2}}-\left(\sin x^{2} \cos x^{2}\right)\left(\frac{d x^{3}}{d s}\right)^{2}=0 \\
& \frac{d^{2} x^{3}}{d s^{2}}+\left(2 \cot x^{2}\right) \frac{d x^{2}}{d s} \frac{d x^{3}}{d s}=0
\end{aligned}
$$

(b) $x^{2}=\frac{1}{a} s \quad x^{3}=0$

(c) The solution (b) represents an arc of a particular great circle $\left(x^{2}+z^{2}=a^{2}\right.$, in the usual cartesian coordinates) on the sphere. By symmetry of the sphere, all great-circular arcs, and only these, will be geodesics.

\section*{CHAPTER 7}
$7.24 \varepsilon= \begin{cases}+1 & 0<|t| \leqq 1 / 2 \\ -1 & |t|>1 / 2\end{cases}$

$7.25 t=0,1$

$7.26 L=8 \sqrt{2} / 3$

$7.27 t=\sqrt{5} / 3\left[t=0\right.$, which makes $\gamma \equiv\left|g_{i j}\right|=0$, is disallowed $]$

$7.28 \quad L=(64+11 \sqrt{11}) / 216 \approx 0.465$

$7.29 \quad \theta=i \ln 2$ at $(0,2,0) ; \theta=\cos ^{-1}(7 / 4 \sqrt{11})$ at $(5,2,3)$

7.30 (a) $L=8(1+3 \sqrt{3}) \approx 49.57$

(b) $x^{1}=3\left(\sigma s^{2 / 3}+4\right), x^{2}=\left(\sigma s^{2 / 3}+4\right)^{3 / 2}$, where

$$
\sigma=\left\{\begin{array}{lr}
+1 & -8 \leqq s>0 \\
-1 & 0 \leqq s \leqq 24 \sqrt{3}
\end{array}\right.
$$

(c) The null points are $t=0(s=-8)$ and $t=1(s=0)$.

7.31 $L=8(5 \sqrt{5}-1) \approx 81.44$

7.32 For $s \neq 0, \mathbf{T}=\left(2|s|^{-1 / 3}, \sqrt{\sigma+4 s^{-2 / 3}}\right)$ and $\|\mathbf{T}\|^{2}=|-\sigma|=+1$

$7.33 \quad N^{1}=T^{2}, N^{2}=T^{1}$

7.34 For $s \neq-8,0$,

7.36

$$
\begin{gathered}
\kappa=\frac{-2}{3 s \sqrt{\sigma s^{2 / 3}+4}} \quad \kappa_{0}=|\kappa| \\
\kappa_{0}=|\kappa|=\frac{2}{3|s|\left(s^{2 / 3}-4\right)^{1 / 2}} \quad(s \neq 8)
\end{gathered}
$$

At the null point $(0,0)$, both Euclidean and Riemannian absolute curvatures become infinite; but at the null point $(12,8)$, only the Riemannian curvature becomes infinite.

$7.37 \mathbf{T}=|1-4 t|^{-1 / 2}(1,2 t), \mathbf{N}=|1-4 t|^{-1 / 2}(1,1-2 t), \kappa=2|1-4 t|^{-3 / 2}$

$7.38 \quad$ (a) $L=a$. (b) $L=3 a / 2$. (c) Riemannian: $\mathbf{T}=|\cos 2 t|^{-1 / 2}(-\cos t, \sin t), \kappa_{0}=(2 / 3 a)(\csc 2 t)|\cos 2 t|^{-3 / 2}$; Euclidean: $\mathbf{T}=(-\cos t, \sin t), \kappa_{0}=(2 / 3 a) \csc 2 t$

7.39 (a) $\Gamma_{11}^{1}=1 / 2 x^{1}, \Gamma_{22}^{2}=1 / 2 x^{2}$, others zero

\section*{CHAPTER 8}
8.16 By Problem 6.34 and $(8: 1)$,

$$
\begin{aligned}
V_{, k l}^{i}-V_{, l k}^{i} & =g^{i r}\left(V_{r, k l}-V_{r, l k}\right)=g^{i r} R_{r k l}^{s} V_{s} \\
& =g^{i r}\left(g_{s t} R_{r k l}^{s}\right) V^{t}=\left(g^{i r} R_{t r k l}\right) V^{t}=-R_{t k l}^{i} V^{t}
\end{aligned}
$$

8.22 $\mathrm{K}=1 / 4\left(x^{1}\right)^{2}$

8.24 (a) and (b) $\mathrm{K}=\frac{x^{1}+x^{2}}{4\left(x^{1}\right)^{2} x^{2}\left(1+2 x^{2}\right)}$

(c) $\mathbf{U}_{(2)}=-\mathbf{U}_{(1)}+\mathbf{V}_{(1)}, \mathbf{V}_{(2)}=\mathbf{U}_{(1)}+\mathbf{V}_{(1)}$

8.25 $\mathrm{K}=1 / a^{2}$

8.26 Basic sets of nonvanishing terms are:

$$
\text { (A) } R_{1212}=-\frac{1}{4}\left(2 f^{\prime \prime}-\frac{f^{\prime 2}}{f}-\frac{f^{\prime} g^{\prime}}{g}\right), R_{1313}=-\frac{1}{4} \frac{f^{\prime} h^{\prime}}{g}, R_{2323}=-\frac{1}{4}\left(2 h^{\prime \prime}-\frac{h^{\prime 2}}{h}-\frac{h^{\prime} g^{\prime}}{g}\right)
$$

and

(A) $G_{1212}=f g, G_{1313}=f h, G_{2323}=g h$

so that

(a) $\mathrm{K}\left(x^{2} ; \mathbf{U}, \mathbf{V}\right)=\frac{R_{1212} W_{1212}+R_{1313} W_{1313}+R_{2323} W_{2323}}{f g W_{1212}+f h W_{1313}+g h W_{2323}}$

(b) $\quad R=-\frac{2}{f g h}\left(h R_{1212}+g R_{1313}+f R_{2323}\right)$

8.27

(a) $\mathrm{K}\left(x^{2} ; \mathbf{U}, \mathbf{V}\right)=\frac{-2(\ln |f|)^{\prime \prime}\left(W_{1212}+W_{2323}\right)-(\ln |f|)^{\prime 2} W_{1313}}{4 f\left(W_{1212}+W_{1313}+W_{2323}\right)}$

(b) $R=\frac{4 f^{\prime \prime} f-3 f^{\prime 2}}{2 f^{3}}$

8.28 Isotropic points compose the surface $x^{2}=e^{-3 / 2}$, over which $\mathrm{K}=2 e^{3 / 27}$.

8.29 $\mathrm{K}=-1 / 4$

8.32 $R_{11}=-1, R_{12}=R_{21}=0, R_{22}=\sin ^{2} x^{1} ; R_{1}^{1}=-1 / a^{2}=R_{2}^{2}, R_{2}^{1}=0=R_{1}^{2} ; R=-2 / a^{2}$

8.33 $\quad R_{11}=R_{22}=R_{33}=2 /\left(x^{1}\right)^{2}$, others $0 ; R_{1}^{1}=R_{2}^{2}=R_{3}^{3}=2$, others $0 ; R=6$

8.35 $g_{i j}=\left(x^{1}\right)^{4} \delta_{i j}$ has $R=0, \mathrm{~K} \neq 0$ (use Problem 8.27).

8.36 No implication either way.

\section*{CHAPTER 9}
9.17 (a) $u_{0}= \pm \sqrt{x^{1} x^{2}+a} \quad(a=$ const. $) ;(b)$ incompatible

9.20 flat, non-Euclidean

9.21 Euclidean

$9.22(++-)$

9.26 With the notation $f_{i} \equiv \partial f / \partial x^{i}$, for any function $f$ :

$$
\begin{aligned}
& G_{1}^{1}=\frac{1}{\left(x^{1}\right)^{2}}+e^{-\varphi}\left[-\frac{\psi_{1}}{x^{1}}-\frac{1}{\left(x^{1}\right)^{2}}\right] \\
& G_{2}^{2}=e^{-\varphi}\left(-\frac{\psi_{11}}{2}-\frac{\psi_{1}^{2}}{4}+\frac{\varphi_{1} \psi_{1}}{4}+\frac{\varphi_{1}}{2 x^{1}}-\frac{\psi_{i}}{2 x^{1}}\right)+e^{-\psi}\left(\frac{\varphi_{44}}{2}+\frac{\varphi_{4}^{2}}{4}-\frac{\varphi_{4} \psi_{4}}{4}\right)=G_{3}^{3} \\
& G_{4}^{4}=\frac{1}{\left(x^{1}\right)^{2}}+e^{-\varphi}\left[\frac{\varphi_{1}}{x^{1}}-\frac{1}{\left(x^{1}\right)^{2}}\right] \quad G_{4}^{1}=-\varphi_{4} e^{-\varphi / x^{1}} \quad G_{1}^{4}=\varphi_{4} e^{-\psi / x^{1}}
\end{aligned}
$$

\section*{CHAPTER 10}
10.30 (a) The curve lies on a right circular cylinder of unit radius, beginning at the point $(1,0,1)$ and rising in helix fashion, approaching $\infty$ asymptotic to the vertical line $x=\cos 1, y=\sin 1$, as $t \rightarrow 1$.


\begin{equation*}
L=\int_{0}^{1 / 2} \frac{\sqrt{(1-t)^{4}+1}}{(1-t)^{2}} d t \approx 1.13209039 \tag{b}
\end{equation*}


$10.3116 / 3$

10.32 (a) $\mathbf{T}=(-(a / c) \sin (s / c),(a / c) \cos (s / c), b / c)$. Hence the tangent line, $\mathbf{r}(t) \equiv \mathbf{r}+t \mathbf{T}$, has the coordinate equations

$$
x=a \cos \frac{s}{c}-\frac{a t}{c} \sin \frac{s}{c} \quad y=a \sin \frac{s}{c}+\frac{a t}{c} \cos \frac{s}{c} \quad z=\frac{b s}{c}+\frac{b t}{c}
$$

(b) $Q$ corresponds to $t=-s$ and $P Q=\|-s \mathbf{T}\|=s$.

(c) The interpretation is that $Q$ can be thought of as the free end of the taut string as it is unwound from the helix. [The locus of $Q, \mathbf{r}^{*}=\mathbf{r}(s)-s \mathbf{r}^{\prime}(s)$, is called an involute of the helix.]

10.33

$$
\begin{gathered}
\frac{\mathbf{T}^{\prime}}{\left\|\mathbf{T}^{\prime}\right\|}=\frac{t /|t|}{\left(1+25 t^{8}\right)^{1 / 2}}\left(-5 t^{4}, 1,0\right) \\
\kappa=\frac{20 t^{3} \sqrt{2}}{\left(1+50 t^{8}\right)^{3 / 2}} \quad \tau=0
\end{gathered}
$$

10.35 Let the curve $\mathbf{r}=\mathbf{r}(s)$ lie in the plane $\mathbf{b r}=$ const., where $\mathbf{b}=$ const. and $\|\mathbf{b}\|=1$. Differentiate twice with respect to $s: \mathbf{b T}=0$ and $\mathbf{b T}^{\prime}=0$; hence, $\mathbf{b}(\kappa \mathbf{N})=0$ or $\mathbf{b N}=0$. It follows that $\mathbf{b}=\mathbf{B}$, the binormal vector, so that $\mathbf{B}^{\prime}=\mathbf{0}$ and $\tau=-\mathbf{B}^{\prime} \mathbf{N}=0$. Conversely, if $\tau=0$ for a curve $\mathbf{r}=\mathbf{r}(s)$, then $\mathbf{B}^{\prime}=-\tau \mathbf{N}=\mathbf{0}$ and $\mathbf{B}$ is a constant unit vector. Define the function $Q(s) \equiv \mathbf{B} \cdot(\mathbf{r}(s)-\mathbf{r}(0))$; we have

$$
Q^{\prime}=\mathbf{B r}^{\prime}=\mathbf{B} \mathbf{T}=0
$$

whence $Q=$ const. $=Q(0)=0$. Therefore, the curve lies in the plane

$$
\mathbf{B r}=\mathbf{B r}(0)=\text { const. }
$$

10.38 $E=\left|x^{1}\right| \sqrt{a^{2}+1}=0$ at $x^{1}=0$.

10.39 $E=a^{2} \cosh ^{2} x^{1}>0, \mathbf{n}=\frac{1}{\cosh x^{1}}\left(-\cos x^{2},-\sin x^{2}, \sinh x^{1}\right)$

10.40

$$
L=\int_{1}^{2} \frac{\sqrt{5 t^{4}+1}}{t} d t=\frac{1}{2}\left[9-\sqrt{6}+\ln \frac{2}{5}(\sqrt{6}+1)\right]
$$

$10.41\left(v^{1}, v^{2}\right)=(\sqrt{12 / 29}, \sqrt{17 / 29})$ or $(-\sqrt{12 / 29}, \sqrt{17 / 29})$

10.43 $\Gamma_{12}^{2}=\Gamma_{21}^{2}=\frac{2 x^{1}}{\left(x^{1}\right)^{2}+a^{2}} ;$ all others zero

10.44

$$
\mathrm{II}=\frac{f^{\prime} g^{\prime \prime}-f^{\prime \prime} g^{\prime}}{\sqrt{f^{\prime 2}+g^{\prime 2}}}\left(d x^{1}\right)^{2}+\frac{f g^{\prime}}{\sqrt{f^{\prime 2}+g^{\prime 2}}}\left(d x^{2}\right)^{2}
$$

$10.46(a)$\\
(i) $\mathrm{K}=\frac{4 a^{2}}{\left[1+4 a^{2}\left(x^{1}\right)^{2}\right]^{2}}$\\
$\mathrm{H}=\frac{4 a\left[1+2 a^{2}\left(x^{1}\right)^{2}\right]}{\left[1+4 a^{2}\left(x^{1}\right)^{2}\right]^{3 / 2}}$\\
(ii) $\mathrm{K}=\frac{4 a^{2}}{\left[1+4 a^{2}\left(\bar{x}^{1}\right)^{2}+4 a^{2}\left(\bar{x}^{2}\right)^{2}\right]^{2}}$,\\
$\mathrm{H}=\frac{4 a\left[1+2 a^{2}\left(\bar{x}^{1}\right)^{2}+2 a^{2}\left(\bar{x}^{2}\right)^{2}\right]}{\left[1+4 a^{2}\left(\bar{x}^{1}\right)^{2}+4 a^{2}\left(\bar{x}^{2}\right)^{2}\right]^{3 / 2}}$

(b) Consistent with the invariance of $\mathrm{K}$ and $\mathrm{H}$, the change of parameters $\bar{x}^{1}=x^{1} \cos x^{2}, \bar{x}^{2}=$ $x^{1} \sin x^{2}-$ i.e., the transformation from polar to rectangular coordinates in the parameter planetakes the forms (i) into the forms (ii).

10.49 (a) $\mathbf{r}^{*}=\left(a \operatorname{sech} x^{1}, 0, a x^{1}-a \tanh x^{1}\right)$

10.50 The two FFFs correspond under the mapping $\bar{x}^{1}=a \sinh x^{1}, \bar{x}^{2}=x^{2}$.

\section*{CHAPTER 11}
\includegraphics[max width=\textwidth, center]{2024_04_03_41f90be4f896e21f0dc9g-229}\\
(b) $\quad a=\sqrt{1+4 \csc ^{4} t \cot ^{2} t} \rightarrow 1$;\\
(c) $\max v=\sqrt{5}, \quad \max a=\sqrt{17}$

11.15 Rectilinear motion [use (11.8) to show that $\kappa$ must vanish].

11.16 From (11.7) and (10.9), $\dot{\mathbf{a}}=-\kappa^{2} v^{3} \mathbf{T}+\dot{\kappa} v^{2} \mathbf{N}+\kappa \tau v^{3} \mathbf{B}$.

$11.17 \quad a^{1}=\frac{d^{2} \rho}{d t^{2}}-\left(\rho \sin ^{2} \varphi\right)\left(\frac{d \theta}{d t}\right)^{2}-\rho\left(\frac{d \varphi}{d t}\right)^{2}, \quad a^{2}=\frac{d^{2} \varphi}{d t^{2}}+\frac{2}{\rho} \frac{d \rho}{d t} \frac{d \varphi}{d t}-(\sin \varphi \cos \varphi)\left(\frac{d \theta}{d t}\right)^{2}$, $a^{3}=\frac{d^{2} \theta}{d t^{2}}+\frac{2}{\rho} \frac{d \rho}{d t} \frac{d \theta}{d t}+(2 \cot \varphi) \frac{d \theta}{d t} \frac{d \varphi}{d t}$

11.18 Let the center of force be the origin of rectangular coordinates for $\mathbf{E}^{3}$, with the particle's path given by $\mathbf{r}=\mathbf{r}(t)$. By Newton's second law, $f \mathbf{r}=m \ddot{\mathbf{r}}$, so that

$$
\frac{d}{d t}(\mathbf{r} \times \dot{\mathbf{r}})=\mathbf{r} \times \ddot{\mathbf{r}}=\mathbf{r} \times\left(\frac{f}{m} \mathbf{r}\right)=\mathbf{0}
$$

and $\mathbf{r} \times \dot{\mathbf{r}}=\mathbf{p}=$ const. It follows that $\mathbf{p} \cdot \mathbf{r}=0$.

$11.19 \nabla^{2} f=\frac{\partial^{2} f}{\partial r^{2}}+\frac{1}{r^{2}} \frac{\partial^{2} f}{\partial \theta^{2}}+\frac{\partial^{2} f}{\partial z^{2}}+\frac{1}{r} \frac{\partial f}{\partial r}$

\section*{CHAPTER 12}
12.34 (a) timelike; (b) spacelike; (c) lightlike

12.35 Yes: travel at $4167 \mathrm{mi} / \mathrm{sec} \ll c$. Timelike interval.

12.36 Premultiply $A^{T} G A=G$ by $A G$, and postmultiply by $A^{-1} G$. 12.38 (a) $t=a_{0}^{0} \bar{t}, x^{1}=-a_{0}^{1} c \bar{t}, x^{2}=-a_{0}^{2} c \bar{c}, x^{3}=-a_{0}^{3} c \bar{c}$. (b) $a_{0}^{0}>0$ if $t$ and $\bar{t}$ have the same sign; that is, if the\\
clocks of the two observers are both turning clockwise or both counterclockwise.

$$
\bar{x}^{0}=\frac{5}{3} x^{0}-\frac{4}{3} x^{1} \quad \bar{x}^{1}=-\frac{4}{3} x^{0}+\frac{5}{3} x^{1} \quad \bar{x}^{2}=x^{2} \quad \bar{x}^{3}=x^{3}
$$

12.40

(b) zero

12.41

$$
\begin{aligned}
L^{*} & =\left[\begin{array}{cccc}
5 / 4 & -3 / 4 & 0 & 0 \\
-3 / 4 & 5 / 4 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right] \\
v & =(3 / 5) c
\end{aligned}
$$

$12.42 v=\sqrt{\frac{2}{3}} c$

$12.45 v=(4 / 5) c$

12.47 Approximately $25 \%$ slow.

12.48 About 45 years old.

$12.49 \approx 17000 \mathrm{mi} / \mathrm{sec}$

12.50 For constants $\hat{F}$ and $\hat{\bar{a}} \equiv \hat{F} / m$, and with $\mathbf{F}=(\hat{F}, 0,0)$ and $\mathbf{v}=\left(v_{x}, 0,0\right),(12.29)$ becomes identical with (1) of Problem 12.26.

12.52 Since $\partial \bar{s}^{i} / \partial \bar{x}^{i}=0=\partial s^{i} / \partial x^{i}$ (the equation of continuity), $\left(s^{i}\right)$ may be identified with the vector $\left(S^{i}\right)$ of Problem 12.32.

12.54 (a) By analogy with the evaluation of $\frac{1}{2} e_{i j k l} P_{k l}$ in Problem 12.53,

$$
\left[\frac{1}{2} e_{i j k l}\left(-\Phi_{k l}\right)\right]_{44}=\left[\begin{array}{cccc}
0 & -\Phi_{23} & \Phi_{13} & -\Phi_{12} \\
* & 0 & -\Phi_{03} & \Phi_{02} \\
* & * & 0 & -\Phi_{01} \\
* & * & * & 0
\end{array}\right]=\left[\begin{array}{cccc}
0 & -H_{1} & -H_{2} & -H_{3} \\
* & 0 & E_{3} & -E_{2} \\
* & * & 0 & E_{1} \\
* & * & * & 0
\end{array}\right]=\left[F^{i j}\right]_{44}
$$

(b) Let $(a b c d)$ denote a permutation of (0123). Then $\Phi_{a b}=-e_{a b c d} F^{c d} \quad$ (no summation) and

$$
\begin{aligned}
\frac{\partial \Phi_{a b}}{\partial x^{c}}+\frac{\partial \Phi_{c a}}{\partial x^{b}}+\frac{\partial \Phi_{b c}}{\partial x^{a}} & =-e_{a b c d} \frac{\partial F^{c d}}{\partial x^{c}}-e_{c a b d} \frac{\partial F^{b d}}{\partial x^{b}}-e_{b c a d} \frac{\partial F^{a d}}{\partial x^{a}} \\
& =-e_{a b c d}\left(\frac{\partial F^{c d}}{\partial x^{c}}+\frac{\partial F^{b d}}{\partial x^{b}}+\frac{\partial F^{a d}}{\partial x^{d}}\right)= \pm \frac{\partial F^{j d}}{\partial x^{j}}=0
\end{aligned}
$$

The second set of equations is derivable directly from $(12.45 b)$, the definition of $\Phi$, and the fact that the $g_{i j}$ are constants.

\section*{CHAPTER 13}
13.22 Yes; it is isomorphic to the 4-group.

13.26 (a) By (13.6),

$$
\bar{T}_{i}^{i}=T\left(\overline{\boldsymbol{\beta}}^{i}, \overline{\mathbf{b}}_{i}\right)=T\left(\bar{A}_{j}^{i} \boldsymbol{\beta}^{j}, A_{i}^{k} \mathbf{b}_{k}\right)=\bar{A}_{j}^{i} A_{i}^{k} T\left(\boldsymbol{\beta}^{j}, \mathbf{b}_{k}\right)=\delta_{j}^{k} T_{k}^{j}=T_{j}^{j}
$$

(b) $\tau(T)=n$

13.27 Suppose that $\hat{G}\left(\mathbf{u}_{1}\right)=\hat{G}\left(\mathbf{u}_{2}\right)$. Then $G\left(\mathbf{u}_{1}, \mathbf{v}\right)=G\left(\mathbf{u}_{2}, \mathbf{v}\right)$, or by symmetry, $G\left(\mathbf{v}, \mathbf{u}_{1}\right)=G\left(\mathbf{v}, \mathbf{u}_{2}\right)$ for all $\mathbf{v}$ By nonsingularity, $\mathbf{u}_{1}=\mathbf{u}_{2}$.

13.28

$$
\begin{aligned}
& \varphi_{p}^{-1}: \begin{cases}y^{1}=\sqrt{a^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}+\left(x^{3}\right)^{2}} \\
y^{2}=x^{1} & \\
y^{3}=x^{2} & \mathbf{U}_{p}: y^{1}>0 \\
y^{4}=x^{3} & p=(a, 0,0,0)\end{cases}
\end{aligned}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231}
\end{center}

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(2)}
\end{center}

$$
\begin{aligned}
& \varphi_{ \pm r}^{-1}:\left\{\begin{array}{l|l}
y^{1}=x^{1} & \mathbf{U}_{r}: y^{3}>0 \\
y^{2}=x^{2} & \begin{array}{c}
\mathbf{U}_{-r}: y^{3}<0 \\
r=(0,0, a, 0)
\end{array} \\
y^{3}= \pm \sqrt{a^{2}-\left(x^{1}\right)^{2}-\left(x^{2}\right)^{2}+\left(x^{3}\right)^{2}} \\
y^{4}=x^{3}
\end{array}\right.
\end{aligned}
$$

13.30

$$
\sigma=y^{2} d y^{1}-y^{1} d y^{2}+y^{4} d y^{3}-y^{3} d y^{4}+y^{6} d y^{5}-y^{5} d y^{6}+\cdots+y^{2 k} d y^{2 k-1}-y^{2 k-1} d y^{2 k}
$$

13.31 If $p_{1}$ and $p_{2}$ are not antipodal, there exists a closed hemisphere containing neither one, on which the given (continuous) vector field is nonzero-an impossibility by Problem 13.17(b).

13.32 As shown in Fig. 13-10, let a unit tangent vector be constructed to a generating circle; as the circle is revolved to generate the torus, the tangent vector is obviously propagated continuously to all points of the torus.

\begin{center}
\includegraphics[max width=\textwidth]{2024_04_03_41f90be4f896e21f0dc9g-231(1)}
\end{center}

Fig. 13-10

13.33 Zero points are: $(a)(0,0,0,0, \pm a) ;(b) \pm(0, a / \sqrt{3}, 0, a / \sqrt{3}, a / \sqrt{3})$.

13.35 With $\omega \equiv 2\left(y^{1} d y^{1}-4 y^{2} d y^{2}+4 y^{3} d y^{3}\right), \sigma=f d y^{1}+g d y^{2}+h d y^{3}$ must be orthogonal to $\omega$, for $C^{\infty}$ functions $f, g, h$. Hence,

$$
y^{1} f-4 y^{2} g+4 y^{3} h=0
$$

Replace $f, g$ by $4 y^{3} F, y^{3} G$, and solve for $h$. Similarly, replace $g$ by $y^{1} G$ and $h$ by $y_{1} H$ and solve for $f$; etc. All possible tangent vectors are given by one of three distinct types $\left(F, G, H\right.$ denote arbitrary $C^{\infty}$ functions of $y^{1}, y^{2}, y^{3}$ ):

(1) $\sigma=4 y^{3} F d y^{1}+y^{3} G d y^{2}+\left(y^{2} G-y^{1} F\right) d y^{3}$

(2) $\sigma=\left(4 y^{2} G-4 y^{3} H\right) d y^{1}+y^{1} G d y^{2}+y^{1} H d y^{3}$

(3) $\sigma=4 y^{2} F d y^{1}+\left(y^{1} F+y^{3} H\right) d y^{2}+y^{2} H d y^{3}$

13.36

$$
U=U_{1}^{i} \mathbf{r}_{i} \equiv\left(2 U^{1} \sqrt{4-\left(x^{1}\right)^{2}+4\left(x^{2}\right)^{2}}, 2 U^{2} \sqrt{4-\left(x^{1}\right)^{2}+4\left(x^{2}\right)^{2}},-x^{1} U^{1}+4 x^{2} U^{2}\right)
$$

for any two $C^{\infty}$ functions $U^{1}, U^{2}$ on $\left(x^{1}, x^{2}\right)$.

13.37 The normal vector to the sphere is represented by $\sigma=y^{1} d y^{1}+y^{2} d y^{2}+y^{3} d y^{3}$; the tangent space of the cone is given by

$$
\left(U_{1}, U_{2},\left(3 x^{1} U^{1}+3 x^{2} U^{2}\right)\left(\left(x^{1}\right)^{2}+\left(x^{2}\right)^{2}\right)^{-1 / 2}\right)
$$

Set $U^{1}=y^{1}$ and $U^{2}=y^{2}$.


\end{document}