{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c11609-2a8b-404a-81e5-db456fca9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb932f10-b19f-49a9-b883-3a14d4058e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_structured_contents(book_url, file_contents):\n",
    "    contents = []\n",
    "    if book_url == \"books/Schaum's Outlines - Tensor Calculus/2024_04_03_41f90be4f896e21f0dc9g/2024_04_03_41f90be4f896e21f0dc9g.tex\":\n",
    "        sections = re.split(r'(?=\\\\section..Chapter)', file_contents)\n",
    "        contents = []\n",
    "        for section in sections:\n",
    "            print(section.split('\\n', 1)[0],len(section))\n",
    "            section_parts = {}\n",
    "            section_parts[\"all\"]=section.split('\\n', 1)\n",
    "            \n",
    "            subsections = re.split(r'(?=\\\\section)', section)\n",
    "            for subsection in subsections:\n",
    "                if \"solved problems\" in subsection.split('\\n', 1)[0].lower():\n",
    "                    print(\"-->\"+subsection.split('\\n', 1)[0])\n",
    "                if \"supplementary problems\" in subsection.split('\\n', 1)[0].lower():\n",
    "                    print(\"-->\"+subsection.split('\\n', 1)[0])\n",
    "                    \n",
    "            # Splitting the section into three parts\n",
    "            parts = section.split(\"Solved Problems\", 1)\n",
    "            part_one = parts[0]\n",
    "            part_two_and_three = parts[1] if len(parts) > 1 else \"\"\n",
    "            \n",
    "            parts = part_two_and_three.split(\"Supplementary Problems\", 1)\n",
    "            part_two = \"Solved Problems\" + parts[0] if part_two_and_three else \"\"\n",
    "            part_three_and_four = \"Supplementary Problems\" + parts[1] if len(parts) > 1 else \"\"\n",
    "    \n",
    "            #extract answers to supplementary problems\n",
    "            parts = part_three_and_four.split(\"Answers to Supplementary Problems\", 1)\n",
    "            part_three = \"Supplementary Problems\" + parts[0] if part_two_and_three else \"\"\n",
    "            part_four = \"Answers to Supplementary Problems\" + parts[1] if len(parts) > 1 else \"\"\n",
    "    \n",
    "            # Optionally, print or process the parts\n",
    "            print(\"lesson:\", len(part_one), \"solved problems:\", len(part_two), \"supplementary:\", len(part_three), \", total\", len(part_one)+len(part_two)+ len(part_three))\n",
    "            \n",
    "            section_parts[\"lesson\"]=part_one\n",
    "            section_parts[\"solved_problems\"]=part_two\n",
    "            section_parts[\"supplementary_problems\"]=part_three\n",
    "            section_parts[\"answers_to_supplementary_problems\"]=part_four\n",
    "        \n",
    "            if (len(part_one)>0 and len(part_two)>0):\n",
    "                contents.append(section_parts)\n",
    "            else:\n",
    "                print(\"dropped a section from further analysis\")\n",
    "                \n",
    "    elif book_url == \"books/Schaum's Outlines - Linear Algebra,Fourth Edition/2024_04_03_de2bde501961f6000cc6g/2024_04_03_de2bde501961f6000cc6g.tex\":\n",
    "        contents = []\n",
    "        sections = re.split(r'(?=section.*.Introduction)', file_contents)\n",
    "        for i, section in enumerate(sections):\n",
    "            if i<len(sections)-1:\n",
    "                #print('\\n'.join(section.split('\\n')[-3:]),\"(length:\",len(section),\")\")\n",
    "                #make these last three lines before the Indtroduction part of the next chapter\n",
    "                sections[i+1] = '\\n'.join(section.split('\\n')[-3:])+sections[i+1]\n",
    "            #and remove them from the last\n",
    "            sections[i] = '\\n'.join(section.split('\\n')[:-3])\n",
    "        \n",
    "        for i, section in enumerate(sections[1:]):#skip the stuff before chapter 1\n",
    "            section_parts = {}\n",
    "            section_parts[\"all\"]=section\n",
    "            subsections = re.split(r'(?=\\\\section)', section)\n",
    "            print(\"processing section\",i)\n",
    "            for j, subsection in enumerate(subsections):\n",
    "                if \"solved problems\" in subsection.split('\\n', 1)[0].lower():\n",
    "                    print(\"-s->\"+subsection.split('\\n', 1)[0], j)\n",
    "                    section_parts[\"lesson\"] = \"\\n\".join(subsections[0:j])\n",
    "                    end_of_lesson = j\n",
    "                if \"supplementary problems\" in subsection.split('\\n', 1)[0].lower():\n",
    "                    if \"answers to supplementary problems\" in subsection.split('\\n', 1)[0].lower():\n",
    "                        print(\"-p->\"+subsection.split('\\n', 1)[0],j)\n",
    "                        section_parts[\"supplementary_problems\"] = \"\\n\".join(subsections[end_of_solved_problems:j])\n",
    "                        section_parts[\"answers_to_supplementary_problems\"] = \"\\n\".join(subsections[j:])\n",
    "                    else:\n",
    "                        print(\"-a->\"+subsection.split('\\n', 1)[0],j)\n",
    "                        section_parts[\"solved_problems\"] = \"\\n\".join(subsections[end_of_lesson:j])\n",
    "                        end_of_solved_problems = j\n",
    "            if \"lesson\" in section_parts and \"supplementary_problems\" in section_parts and \"answers_to_supplementary_problems\" in section_parts:\n",
    "                contents.append(section_parts)\n",
    "            else:\n",
    "                print(\"dropped a section from further analysis\")\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38516fe5-9dad-4f3d-bff8-39a73fc10847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books/Schaum's Outlines - Linear Algebra,Fourth Edition/2024_04_03_de2bde501961f6000cc6g/2024_04_03_de2bde501961f6000cc6g.tex\n",
      "processing section 0\n",
      "-s->\\section*{SOLVED PROBLEMS} 21\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 27\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 34\n",
      "processing section 1\n",
      "-s->\\section*{SOLVED PROBLEMS} 18\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 29\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 35\n",
      "processing section 2\n",
      "-s->\\section*{SOLVED PROBLEMS} 42\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 51\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 59\n",
      "processing section 3\n",
      "-s->\\section*{SOLVED PROBLEMS} 28\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 41\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 51\n",
      "processing section 4\n",
      "-s->\\section*{SOLVED PROBLEMS} 20\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 27\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 34\n",
      "processing section 5\n",
      "-s->\\section*{SOLVED PROBLEMS} 8\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 14\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 6\n",
      "-s->\\section*{SOLVED PROBLEMS} 23\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 31\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 39\n",
      "processing section 7\n",
      "-s->\\section*{SOLVED PROBLEMS} 13\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 20\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 28\n",
      "processing section 8\n",
      "-s->\\section*{SOLVED PROBLEMS} 16\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 22\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 27\n",
      "processing section 9\n",
      "-s->\\section*{SOLVED PROBLEMS} 3\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 10\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 19\n",
      "processing section 10\n",
      "-s->\\section*{SOLVED PROBLEMS} 3\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 7\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 12\n",
      "processing section 11\n",
      "-s->\\section*{SOLVED PROBLEMS} 9\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 15\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 12\n",
      "-s->\\section*{SOLVED PROBLEMS} 7\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 13\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 13\n",
      "dropped a section from further analysis\n",
      "processing section 14\n",
      "dropped a section from further analysis\n",
      "processing section 15\n",
      "dropped a section from further analysis\n",
      "books/SCHAUM's Outlines - Advanced Calculus, 3rd Edition_2010/2024_04_03_ffb6ac533fe0a53b3ceeg/2024_04_03_ffb6ac533fe0a53b3ceeg.tex\n",
      "books/Schaum's Outlines - Tensor Calculus/2024_04_03_41f90be4f896e21f0dc9g/2024_04_03_41f90be4f896e21f0dc9g.tex\n",
      "\\documentclass[10pt]{article} 386\n",
      "lesson: 386 solved problems: 0 supplementary: 0 , total 386\n",
      "dropped a section from further analysis\n",
      "\\section*{Chapter 1} 19218\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 8913 solved problems: 8080 supplementary: 2247 , total 19240\n",
      "\\section*{Chapter 2} 46088\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 18735 solved problems: 22706 supplementary: 4669 , total 46110\n",
      "\\section*{Chapter 3} 64587\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 30932 solved problems: 29896 supplementary: 3781 , total 64609\n",
      "\\section*{Chapter 4} 28058\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 11872 solved problems: 14301 supplementary: 1907 , total 28080\n",
      "\\section*{Chapter 5} 46129\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 19967 solved problems: 22209 supplementary: 3975 , total 46151\n",
      "\\section*{Chapter 6} 49218\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 22268 solved problems: 22400 supplementary: 4572 , total 49240\n",
      "\\section*{Chapter 7} 54408\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 7641 solved problems: 42821 supplementary: 3968 , total 54430\n",
      "\\section*{Chapter 8} 45043\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 19338 solved problems: 20745 supplementary: 4982 , total 45065\n",
      "\\section*{Chapter 9} 42707\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 20990 solved problems: 19771 supplementary: 1968 , total 42729\n",
      "\\section*{Chapter 10} 80197\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 33342 solved problems: 41807 supplementary: 5070 , total 80219\n",
      "\\section*{Chapter 11} 32983\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 17080 solved problems: 14262 supplementary: 1663 , total 33005\n",
      "\\section*{Chapter 12} 76938\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "lesson: 30474 solved problems: 39909 supplementary: 6577 , total 76960\n",
      "\\section*{Chapter 13} 113804\n",
      "-->\\section*{Solved Problems}\n",
      "-->\\section*{Supplementary Problems}\n",
      "-->\\section*{Answers to Supplementary Problems}\n",
      "lesson: 42369 solved problems: 37296 supplementary: 5332 , total 84997\n",
      "books/Differential quation/2024_04_03_5bb5b4275a64cb9887d1g/2024_04_03_5bb5b4275a64cb9887d1g.tex\n",
      "books/Schaum's_Outlines_-_Discrete_Mathematics,_3rd_Ed._by_Seymour_Lipschutz/2024_04_03_e2bc10318661343af903g/2024_04_03_e2bc10318661343af903g.tex\n",
      "books/Schaum's Outlines - Calculus, 5th Edition/2024_04_03_1accc0f8242883d76c43g/2024_04_03_1accc0f8242883d76c43g.tex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk('books'):\n",
    "    for file in files:\n",
    "        if file.endswith('.tex'):\n",
    "            book_url = os.path.join(root, file)\n",
    "            # Now you have the path of a .tex file, you can do something with it\n",
    "            print(book_url)\n",
    "\n",
    "            with open(book_url, 'r') as file:\n",
    "                # Read the contents of the file into a string\n",
    "                file_contents = file.read()\n",
    "            \n",
    "            contents = get_structured_contents(book_url, file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "618aac3c-a98c-438d-91b3-be0b7504c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solved_problems(latex_content):\n",
    "    # Regex to match section numbers, allowing for whitespace, LaTeX command prefixes, and looking ahead to ensure it's likely a title following.\n",
    "    # Adjust this pattern to fit the specificities of your LaTeX files.\n",
    "    pattern = r'(^|\\\\section|\\\\subsection|\\\\subsubsection)\\s*{?\\s*(\\d+\\.\\d+)(\\.?\\s)(?=\\s*[^%\\\\].*?$)'\n",
    "\n",
    "    # Split the content based on the pattern\n",
    "    sections = []\n",
    "    start = 0\n",
    "\n",
    "    for chapter in latex_content:\n",
    "        if \"solved_problems\" in chapter:\n",
    "            for match in re.finditer(pattern, chapter[\"solved_problems\"], flags=re.MULTILINE):\n",
    "                end = match.start()\n",
    "                if start < end:\n",
    "                    sections.append(chapter[\"solved_problems\"][start:end])\n",
    "                start = end\n",
    "            sections.append(chapter[\"solved_problems\"][start:])  # Add the last section\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f0be03f5-3fca-4407-99ba-b08e4f955834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supplementary_problems_answers_in_chapters(latex_content):\n",
    "    # Regex to match section numbers, allowing for whitespace, LaTeX command prefixes, and looking ahead to ensure it's likely a title following.\n",
    "    # Adjust this pattern to fit the specificities of your LaTeX files.\n",
    "    pattern = r'(^|\\\\section|\\\\subsection|\\\\subsubsection)\\s*{?\\s*(\\d+\\.\\d+)(\\.?\\s)(?=\\s*[^%\\\\].*?$)'\n",
    "\n",
    "    # Split the content based on the pattern\n",
    "    questions = []\n",
    "    answers = []\n",
    "    start = 0\n",
    "\n",
    "    for chapter in latex_content:\n",
    "        if \"solved_problems\" in chapter:\n",
    "            for match in re.finditer(pattern, chapter[\"supplementary_problems\"], flags=re.MULTILINE):\n",
    "                end = match.start()\n",
    "                if start < end:\n",
    "                    questions.append(chapter[\"supplementary_problems\"][start:end])\n",
    "                start = end\n",
    "            questions.append(chapter[\"supplementary_problems\"][start:])  # Add the last section\n",
    "        if \"answers_to_supplementary_problems\" in chapter:\n",
    "            for match in re.finditer(pattern, chapter[\"answers_to_supplementary_problems\"], flags=re.MULTILINE):\n",
    "                end = match.start()\n",
    "                if start < end:\n",
    "                    answers.append(chapter[\"answers_to_supplementary_problems\"][start:end])\n",
    "                start = end\n",
    "            answers.append(chapter[\"answers_to_supplementary_problems\"][start:])  # Add the last section\n",
    "\n",
    "    sections = []\n",
    "    unsanswered = []\n",
    "    for question in questions:\n",
    "        start_string = question.split(\" \")[0].strip()\n",
    "        found_answer = False\n",
    "        for answer in answers:\n",
    "            if start_string in answer and answer.strip().index(start_string)==0:\n",
    "                sections.append(question+\" SOLUTION:\"+answer)\n",
    "                found_answer = True\n",
    "        if not found_answer:\n",
    "            unsanswered.append(question)\n",
    "        \n",
    "    return sections, unsanswered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8fdad28-6a16-46e9-b4ab-973315da31a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291 222\n"
     ]
    }
   ],
   "source": [
    "answered, unanswered = get_supplementary_problems_answers_in_chapters(contents)\n",
    "print(len(answered), len(unanswered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b5a083d-3e45-48eb-9d3b-cc960198c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "lesson\n",
      "solved_problems\n",
      "supplementary_problems\n",
      "answers_to_supplementary_problems\n"
     ]
    }
   ],
   "source": [
    "for v in contents[0]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6f91ccb5-ea04-4067-9f67-f0de724fdcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing section 0\n",
      "-s->\\section*{SOLVED PROBLEMS} 21\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 27\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 34\n",
      "processing section 1\n",
      "-s->\\section*{SOLVED PROBLEMS} 18\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 29\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 35\n",
      "processing section 2\n",
      "-s->\\section*{SOLVED PROBLEMS} 42\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 51\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 59\n",
      "processing section 3\n",
      "-s->\\section*{SOLVED PROBLEMS} 28\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 41\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 51\n",
      "processing section 4\n",
      "-s->\\section*{SOLVED PROBLEMS} 20\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 27\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 34\n",
      "processing section 5\n",
      "-s->\\section*{SOLVED PROBLEMS} 8\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 14\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 6\n",
      "-s->\\section*{SOLVED PROBLEMS} 23\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 31\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 39\n",
      "processing section 7\n",
      "-s->\\section*{SOLVED PROBLEMS} 13\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 20\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 28\n",
      "processing section 8\n",
      "-s->\\section*{SOLVED PROBLEMS} 16\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 22\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 27\n",
      "processing section 9\n",
      "-s->\\section*{SOLVED PROBLEMS} 3\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 10\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 19\n",
      "processing section 10\n",
      "-s->\\section*{SOLVED PROBLEMS} 3\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 7\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 12\n",
      "processing section 11\n",
      "-s->\\section*{SOLVED PROBLEMS} 9\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 15\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 12\n",
      "-s->\\section*{SOLVED PROBLEMS} 7\n",
      "-a->\\section*{SUPPLEMENTARY PROBLEMS} 13\n",
      "-p->\\section*{ANSWERS TO SUPPLEMENTARY PROBLEMS} 21\n",
      "processing section 13\n",
      "dropped a section from further analysis\n",
      "processing section 14\n",
      "dropped a section from further analysis\n",
      "processing section 15\n",
      "dropped a section from further analysis\n",
      "502 solved problems separated out\n"
     ]
    }
   ],
   "source": [
    "#for each book separately, go through all functions above, saving the intermediate outputs\n",
    "book_url = \"books/Schaum's Outlines - Linear Algebra,Fourth Edition/2024_04_03_de2bde501961f6000cc6g/2024_04_03_de2bde501961f6000cc6g.tex\"\n",
    "# Open the file with read permission\n",
    "with open(book_url, 'r') as file:\n",
    "    # Read the contents of the file into a string\n",
    "    file_contents = file.read()\n",
    "\n",
    "contents = get_structured_contents(book_url, file_contents)\n",
    "\n",
    "#Check for this manually before continuing:\n",
    "#contents must:\n",
    "#have consecutive chapter names on the first line\n",
    "#contains lesson, solved_problems, supplementary_problems\n",
    "#the last chapter must contain answers_to_supplementary_problems\n",
    "\n",
    "#create a json file url from the book url\n",
    "json_url = book_url.rsplit('.', 1)[0] + '.json'\n",
    "\n",
    "solved_problems_this_book = get_solved_problems(contents)\n",
    "print(len(solved_problems_this_book),\"solved problems separated out\")\n",
    "\n",
    "#get supplementary problems and their solutions\n",
    "supplementary_problems_this_book, unanswered = get_supplementary_problems_answers_in_chapters(contents)\n",
    "\n",
    "#print(supplementary_problems_this_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f0d8bca3-6955-443d-a2d3-cbe964988cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\section*{SOLVED PROBLEMS}\\n\\n\\\\section*{Vectors in $\\\\mathbf{R}^{\\\\boldsymbol{n}}$}\\n',\n",
       " '1.1. Determine which of the following vectors are equal:\\n\\n$$\\nu_{1}=(1,2,3), \\\\quad u_{2}=(2,3,1), \\\\quad u_{3}=(1,3,2), \\\\quad u_{4}=(2,3,1)\\n$$\\n\\nVectors are equal only when corresponding entries are equal; hence, only $u_{2}=u_{4}$.\\n',\n",
       " '\\n1.2. Let $u=(2,-7,1), v=(-3,0,4), w=(0,5,-8)$. Find:\\n\\n(a) $3 u-4 v$,\\n\\n(b) $2 u+3 v-5 w$.\\n\\nFirst perform the scalar multiplication and then the vector addition.\\n\\n(a) $3 u-4 v=3(2,-7,1)-4(-3,0,4)=(6,-21,3)+(12,0,-16)=(18,-21,-13)$\\n\\n(b) $2 u+3 v-5 w=(4,-14,2)+(-9,0,12)+(0,-25,40)=(-5,-39,54)$\\n',\n",
       " '\\n1.3. Let $u=\\\\left[\\\\begin{array}{r}5 \\\\\\\\ 3 \\\\\\\\ -4\\\\end{array}\\\\right], v=\\\\left[\\\\begin{array}{r}-1 \\\\\\\\ 5 \\\\\\\\ 2\\\\end{array}\\\\right], w=\\\\left[\\\\begin{array}{r}3 \\\\\\\\ -1 \\\\\\\\ -2\\\\end{array}\\\\right]$. Find:\\n\\n(a) $5 u-2 v$,\\n\\n(b) $-2 u+4 v-3 w$.\\n\\nFirst perform the scalar multiplication and then the vector addition:\\n\\n(a) $5 u-2 v=5\\\\left[\\\\begin{array}{r}5 \\\\\\\\ 3 \\\\\\\\ -4\\\\end{array}\\\\right]-2\\\\left[\\\\begin{array}{r}-1 \\\\\\\\ 5 \\\\\\\\ 2\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}25 \\\\\\\\ 15 \\\\\\\\ -20\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{r}2 \\\\\\\\ -10 \\\\\\\\ -4\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}27 \\\\\\\\ 5 \\\\\\\\ -24\\\\end{array}\\\\right]$\\n\\n(b) $-2 u+4 v-3 w=\\\\left[\\\\begin{array}{r}-10 \\\\\\\\ -6 \\\\\\\\ 8\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{r}-4 \\\\\\\\ 20 \\\\\\\\ 8\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{r}-9 \\\\\\\\ 3 \\\\\\\\ 6\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}-23 \\\\\\\\ 17 \\\\\\\\ 22\\\\end{array}\\\\right]$\\n',\n",
       " '\\n1.4. Find $x$ and $y$, where: (a) $(x, 3)=(2, x+y)$, (b) $(4, y)=x(2,3)$.\\n\\n(a) Because the vectors are equal, set the corresponding entries equal to each other, yielding\\n\\n$$\\nx=2, \\\\quad 3=x+y\\n$$\\n\\nSolve the linear equations, obtaining $x=2, y=1$.\\n\\n(b) First multiply by the scalar $x$ to obtain $(4, y)=(2 x, 3 x)$. Then set corresponding entries equal to each other to obtain\\n\\n$$\\n4=2 x, \\\\quad y=3 x\\n$$\\n\\nSolve the equations to yield $x=2, y=6$.\\n',\n",
       " '\\n1.5. Write the vector $v=(1,-2,5)$ as a linear combination of the vectors $u_{1}=(1,1,1), u_{2}=(1,2,3)$, $u_{3}=(2,-1,1)$.\\n\\nWe want to express $v$ in the form $v=x u_{1}+y u_{2}+z u_{3}$ with $x, y, z$ as yet unknown. First we have\\n\\n$$\\n\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-2 \\\\\\\\\\n5\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\nx+y+2 z \\\\\\\\\\nx+2 y-z \\\\\\\\\\nx+3 y+z\\n\\\\end{array}\\\\right]\\n$$\\n\\n(It is more convenient to write vectors as columns than as rows when forming linear combinations.) Set corresponding entries equal to each other to obtain\\n\\n$$\\n\\\\begin{aligned}\\n& x+y+2 z=1 \\\\quad x+y+2 z=1 \\\\quad x+y+2 z=1 \\\\\\\\\\n& x+2 y-z=-2 \\\\quad \\\\text { or } \\\\quad y-3 z=-3 \\\\quad \\\\text { or } \\\\quad y-3 z=-3 \\\\\\\\\\n& x+3 y+z=5 \\\\quad 2 y-z=4 \\\\quad 5 z=10\\n\\\\end{aligned}\\n$$\\n\\nThis unique solution of the triangular system is $x=-6, y=3, z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.\\n',\n",
       " '\\n1.6. Write $v=(2,-5,3)$ as a linear combination of\\n\\n$$\\nu_{1}=(1,-3,2), u_{2}=(2,-4,-1), u_{3}=(1,-5,7) \\\\text {. }\\n$$\\n\\nFind the equivalent system of linear equations and then solve. First,\\n\\n$$\\n\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-5 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-3 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-4 \\\\\\\\\\n-1\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-5 \\\\\\\\\\n7\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\nx+2 y+z \\\\\\\\\\n-3 x-4 y-5 z \\\\\\\\\\n2 x-y+7 z\\n\\\\end{array}\\\\right]\\n$$\\n\\nSet the corresponding entries equal to each other to obtain\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y+z=2 \\\\quad x+2 y+z=2 \\\\quad x+2 y+z=2 \\\\\\\\\\n& -3 x-4 y-5 z=-5 \\\\quad \\\\text { or } \\\\quad 2 y-2 z=1 \\\\quad \\\\text { or } \\\\quad 2 y-2 z=1 \\\\\\\\\\n& 2 x-y+7 z=3 \\\\quad-5 y+5 z=-1 \\\\quad 0=3\\n\\\\end{aligned}\\n$$\\n\\nThe third equation, $0 x+0 y+0 z=3$, indicates that the system has no solution. Thus, $v$ cannot be written as a linear combination of the vectors $u_{1}, u_{2}, u_{3}$.\\n\\n\\n\\\\section*{Dot (Inner) Product, Orthogonality, Norm in $\\\\mathbf{R}^{n}$}\\n',\n",
       " '1.7. Find $u \\\\cdot v$ where:\\n\\n(a) $u=(2,-5,6)$ and $v=(8,2,-3)$,\\n\\n(b) $u=(4,2,-3,5,-1)$ and $v=(2,6,-1,-4,8)$.\\n\\nMultiply the corresponding components and add:\\n\\n(a) $u \\\\cdot v=2(8)-5(2)+6(-3)=16-10-18=-12$\\n\\n(b) $u \\\\cdot v=8+12+3-20-8=-5$\\n',\n",
       " '\\n1.8. Let $u=(5,4,1), v=(3,-4,1), w=(1,-2,3)$. Which pair of vectors, if any, are perpendicular (orthogonal)?\\n\\nFind the dot product of each pair of vectors:\\n\\n$$\\nu \\\\cdot v=15-16+1=0, \\\\quad v \\\\cdot w=3+8+3=14, \\\\quad u \\\\cdot w=5-8+3=0\\n$$\\n\\nThus, $u$ and $v$ are orthogonal, $u$ and $w$ are orthogonal, but $v$ and $w$ are not.\\n',\n",
       " '\\n1.9. Find $k$ so that $u$ and $v$ are orthogonal, where:\\n\\n(a) $u=(1, k,-3)$ and $v=(2,-5,4)$,\\n\\n(b) $u=(2,3 k,-4,1,5)$ and $v=(6,-1,3,7,2 k)$.\\n\\nCompute $u \\\\cdot v$, set $u \\\\cdot v$ equal to 0 , and then solve for $k$ :\\n\\n(a) $u \\\\cdot v=1(2)+k(-5)-3(4)=-5 k-10$. Then $-5 k-10=0$, or $k=-2$.\\n\\n(b) $u \\\\cdot v=12-3 k-12+7+10 k=7 k+7$. Then $7 k+7=0$, or $k=-1$.\\n',\n",
       " '\\n1.10. Find $\\\\|u\\\\|$, where: (a) $u=(3,-12,-4)$,\\n\\n(b) $u=(2,-3,8,-7)$.\\n\\nFirst find $\\\\|u\\\\|^{2}=u \\\\cdot u$ by squaring the entries and adding. Then $\\\\|u\\\\|=\\\\sqrt{\\\\|u\\\\|^{2}}$.\\n\\n(a) $\\\\|u\\\\|^{2}=(3)^{2}+(-12)^{2}+(-4)^{2}=9+144+16=169$. Then $\\\\|u\\\\|=\\\\sqrt{169}=13$.\\n\\n(b) $\\\\|u\\\\|^{2}=4+9+64+49=126$. Then $\\\\|u\\\\|=\\\\sqrt{126}$.\\n',\n",
       " '\\n1.11. Recall that normalizing a nonzero vector $v$ means finding the unique unit vector $\\\\hat{v}$ in the same direction as $v$, where\\n\\n$\\\\hat{v}=\\\\frac{1}{\\\\|v\\\\|} v$\\n\\nNormalize: (a) $u=(3,-4), \\\\quad$ (b) $v=(4,-2,-3,8), \\\\quad$ (c) $\\\\quad w=\\\\left(\\\\frac{1}{2}, \\\\frac{2}{3},-\\\\frac{1}{4}\\\\right)$.\\n\\n(a) First find $\\\\|u\\\\|=\\\\sqrt{9+16}=\\\\sqrt{25}=5$. Then divide each entry of $u$ by 5 , obtaining $\\\\hat{u}=\\\\left(\\\\frac{3}{5},-\\\\frac{4}{5}\\\\right)$.\\n\\n(b) Here $\\\\|v\\\\|=\\\\sqrt{16+4+9+64}=\\\\sqrt{93}$. Then\\n\\n$$\\n\\\\hat{v}=\\\\left(\\\\frac{4}{\\\\sqrt{93}}, \\\\frac{-2}{\\\\sqrt{93}}, \\\\frac{-3}{\\\\sqrt{93}}, \\\\frac{8}{\\\\sqrt{93}}\\\\right)\\n$$\\n\\n(c) Note that $w$ and any positive multiple of $w$ will have the same normalized form. Hence, first multiply $w$ by 12 to \"clear fractions\"-that is, first find $w^{\\\\prime}=12 w=(6,8,-3)$. Then\\n\\n$$\\n\\\\left\\\\|w^{\\\\prime}\\\\right\\\\|=\\\\sqrt{36+64+9}=\\\\sqrt{109} \\\\text { and } \\\\hat{w}=\\\\widehat{w^{\\\\prime}}=\\\\left(\\\\frac{6}{\\\\sqrt{109}}, \\\\frac{8}{\\\\sqrt{109}}, \\\\frac{-3}{\\\\sqrt{109}}\\\\right)\\n$$\\n',\n",
       " '\\n1.12. Let $u=(1,-3,4)$ and $v=(3,4,7)$. Find:\\n\\n(a) $\\\\cos \\\\theta$, where $\\\\theta$ is the angle between $u$ and $v$;\\n\\n(b) $\\\\operatorname{proj}(u, v)$, the projection of $u$ onto $v$;\\n\\n(c) $d(u, v)$, the distance between $u$ and $v$.\\n\\nFirst find $u \\\\cdot v=3-12+28=19,\\\\|u\\\\|^{2}=1+9+16=26,\\\\|v\\\\|^{2}=9+16+49=74$. Then\\n\\n(a) $\\\\cos \\\\theta=\\\\frac{u \\\\cdot v}{\\\\|u\\\\|\\\\|v\\\\|}=\\\\frac{19}{\\\\sqrt{26} \\\\sqrt{74}}$,\\n\\n(b) $\\\\operatorname{proj}(u, v)=\\\\frac{u \\\\cdot v}{\\\\|v\\\\|^{2}} v=\\\\frac{19}{74}(3,4,7)=\\\\left(\\\\frac{57}{74}, \\\\frac{76}{74}, \\\\frac{133}{74}\\\\right)=\\\\left(\\\\frac{57}{74}, \\\\frac{38}{37}, \\\\frac{133}{74}\\\\right)$,\\n\\n(c) $d(u, v)=\\\\|u-v\\\\|=\\\\|(-2,-7-3)\\\\|=\\\\sqrt{4+49+9}=\\\\sqrt{62}$.\\n',\n",
       " '\\n1.13. Prove Theorem 1.2: For any $u, v, w$ in $\\\\mathbf{R}^{n}$ and $k$ in $\\\\mathbf{R}$ :\\\\\\\\\\n(i) $(u+v) \\\\cdot w=u \\\\cdot w+v \\\\cdot w$,\\\\\\\\\\n(ii) $(k u) \\\\cdot v=k(u \\\\cdot v)$,\\\\\\\\\\n(iii) $u \\\\cdot v=v \\\\cdot u$,\\n\\n(iv) $u \\\\cdot u \\\\geq 0$, and $u \\\\cdot u=0$ iff $u=0$.\\n\\nLet $u=\\\\left(u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right), v=\\\\left(v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right), w=\\\\left(w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right)$.\\n\\n(i) Because $u+v=\\\\left(u_{1}+v_{1}, u_{2}+v_{2}, \\\\ldots, u_{n}+v_{n}\\\\right)$,\\n\\n$$\\n\\\\begin{aligned}\\n(u+v) \\\\cdot w & =\\\\left(u_{1}+v_{1}\\\\right) w_{1}+\\\\left(u_{2}+v_{2}\\\\right) w_{2}+\\\\cdots+\\\\left(u_{n}+v_{n}\\\\right) w_{n} \\\\\\\\\\n& =u_{1} w_{1}+v_{1} w_{1}+u_{2} w_{2}+\\\\cdots+u_{n} w_{n}+v_{n} w_{n} \\\\\\\\\\n& =\\\\left(u_{1} w_{1}+u_{2} w_{2}+\\\\cdots+u_{n} w_{n}\\\\right)+\\\\left(v_{1} w_{1}+v_{2} w_{2}+\\\\cdots+v_{n} w_{n}\\\\right) \\\\\\\\\\n& =u \\\\cdot w+v \\\\cdot w\\n\\\\end{aligned}\\n$$\\n\\n(ii) Because $k u=\\\\left(k u_{1}, k u_{2}, \\\\ldots, k u_{n}\\\\right)$,\\n\\n$$\\n(k u) \\\\cdot v=k u_{1} v_{1}+k u_{2} v_{2}+\\\\cdots+k u_{n} v_{n}=k\\\\left(u_{1} v_{1}+u_{2} v_{2}+\\\\cdots+u_{n} v_{n}\\\\right)=k(u \\\\cdot v)\\n$$\\n\\n(iii) $u \\\\cdot v=u_{1} v_{1}+u_{2} v_{2}+\\\\cdots+u_{n} v_{n}=v_{1} u_{1}+v_{2} u_{2}+\\\\cdots+v_{n} u_{n}=v \\\\cdot u$\\n\\n(iv) Because $u_{i}^{2}$ is nonnegative for each $i$, and because the sum of nonnegative real numbers is nonnegative,\\n\\n$$\\nu \\\\cdot u=u_{1}^{2}+u_{2}^{2}+\\\\cdots+u_{n}^{2} \\\\geq 0\\n$$\\n\\nFurthermore, $u \\\\cdot u=0$ iff $u_{i}=0$ for each $i$, that is, iff $u=0$.\\n',\n",
       " '\\n1.14. Prove Theorem 1.3 (Schwarz): $|u \\\\cdot v| \\\\leq\\\\|u\\\\|\\\\|v\\\\|$.\\n\\nFor any real number $t$, and using Theorem 1.2, we have\\n\\n$$\\n0 \\\\leq(t u+v) \\\\cdot(t u+v)=t^{2}(u \\\\cdot u)+2 t(u \\\\cdot v)+(v \\\\cdot v)=\\\\|u\\\\|^{2} t^{2}+2(u \\\\cdot v) t+\\\\|v\\\\|^{2}\\n$$\\n\\nLet $a=\\\\|u\\\\|^{2}, b=2(u \\\\cdot v), c=\\\\|v\\\\|^{2}$. Then, for every value of $t, a t^{2}+b t+c \\\\geq 0$. This means that the quadratic polynomial cannot have two real roots. This implies that the discriminant $D=b^{2}-4 a c \\\\leq 0$ or, equivalently, $b^{2} \\\\leq 4 a c$. Thus,\\n\\n$$\\n4(u \\\\cdot v)^{2} \\\\leq 4\\\\|u\\\\|^{2}\\\\|v\\\\|^{2}\\n$$\\n\\nDividing by 4 gives us our result.\\n',\n",
       " '\\n1.15. Prove Theorem 1.4 (Minkowski): $\\\\|u+v\\\\| \\\\leq\\\\|u\\\\|+\\\\|v\\\\|$.\\n\\nBy the Schwarz inequality and other properties of the dot product,\\n\\n$$\\n\\\\|u+v\\\\|^{2}=(u+v) \\\\cdot(u+v)=(u \\\\cdot u)+2(u \\\\cdot v)+(v \\\\cdot v) \\\\leq\\\\|u\\\\|^{2}+2\\\\|u\\\\|\\\\|v\\\\|+\\\\|v\\\\|^{2}=(\\\\|u\\\\|+\\\\|v\\\\|)^{2}\\n$$\\n\\nTaking the square root of both sides yields the desired inequality.\\n\\n\\n\\\\section*{Points, Lines, Hyperplanes in $\\\\mathbf{R}^{\\\\mathbf{n}}$}\\nHere we distinguish between an $n$-tuple $P\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right)$ viewed as a point in $\\\\mathbf{R}^{n}$ and an $n$-tuple $u=\\\\left[c_{1}, c_{2}, \\\\ldots, c_{n}\\\\right]$ viewed as a vector (arrow) from the origin $O$ to the point $C\\\\left(c_{1}, c_{2}, \\\\ldots, c_{n}\\\\right)$.\\n',\n",
       " '\\n1.16. Find the vector $u$ identified with the directed line segment $\\\\overrightarrow{P Q}$ for the points:\\\\\\\\\\n(a) $P(1,-2,4)$ and $Q(6,1,-5)$ in $\\\\mathbf{R}^{3}$,\\\\\\\\\\n(b) $P(2,3,-6,5)$ and $Q(7,1,4,-8)$ in $\\\\mathbf{R}^{4}$.\\\\\\\\\\n(a) $u=\\\\overrightarrow{P Q}=Q-P=[6-1,1-(-2),-5-4]=[5,3,-9]$\\\\\\\\\\n(b) $u=\\\\overrightarrow{P Q}=Q-P=[7-2,1-3,4+6,-8-5]=[5,-2,10,-13]$\\n',\n",
       " '\\n1.17. Find an equation of the hyperplane $H$ in $\\\\mathbf{R}^{4}$ that passes through $P(3,-4,1,-2)$ and is normal to $u=[2,5,-6,-3]$.\\n\\nThe coefficients of the unknowns of an equation of $H$ are the components of the normal vector $u$. Thus, an equation of $H$ is of the form $2 x_{1}+5 x_{2}-6 x_{3}-3 x_{4}=k$. Substitute $P$ into this equation to obtain $k=-26$. Thus, an equation of $H$ is $2 x_{1}+5 x_{2}-6 x_{3}-3 x_{4}=-26$.\\n',\n",
       " '\\n1.18. Find an equation of the plane $H$ in $\\\\mathbf{R}^{3}$ that contains $P(1,-3,-4)$ and is parallel to the plane $H^{\\\\prime}$ determined by the equation $3 x-6 y+5 z=2$.\\n\\nThe planes $H$ and $H^{\\\\prime}$ are parallel if and only if their normal directions are parallel or antiparallel (opposite direction). Hence, an equation of $H$ is of the form $3 x-6 y+5 z=k$. Substitute $P$ into this equation to obtain $k=1$. Then an equation of $H$ is $3 x-6 y+5 z=1$.\\n',\n",
       " '\\n1.19. Find a parametric representation of the line $L$ in $\\\\mathbf{R}^{4}$ passing through $P(4,-2,3,1)$ in the direction of $u=[2,5,-7,8]$.\\n\\nHere $L$ consists of the points $X\\\\left(x_{i}\\\\right)$ that satisfy\\n\\n$$\\nX=P+t u \\\\quad \\\\text { or } \\\\quad x_{i}=a_{i} t+b_{i} \\\\quad \\\\text { or } \\\\quad L(t)=\\\\left(a_{i} t+b_{i}\\\\right)\\n$$\\n\\nwhere the parameter $t$ takes on all real values. Thus we obtain\\n\\n$$\\nx_{1}=4+2 t, x_{2}=-2+2 t, x_{3}=3-7 t, x_{4}=1+8 t \\\\quad \\\\text { or } \\\\quad L(t)=(4+2 t,-2+2 t, 3-7 t, 1+8 t)\\n$$\\n',\n",
       " '\\n1.20. Let $C$ be the curve $F(t)=\\\\left(t^{2}, 3 t-2, t^{3}, t^{2}+5\\\\right)$ in $\\\\mathbf{R}^{4}$, where $0 \\\\leq t \\\\leq 4$.\\n\\n(a) Find the point $P$ on $C$ corresponding to $t=2$.\\n\\n(b) Find the initial point $Q$ and terminal point $Q^{\\\\prime}$ of $C$.\\n\\n(c) Find the unit tangent vector $\\\\mathbf{T}$ to the curve $C$ when $t=2$.\\n\\n(a) Substitute $t=2$ into $F(t)$ to get $P=f(2)=(4,4,8,9)$.\\n\\n(b) The parameter $t$ ranges from $t=0$ to $t=4$. Hence, $Q=f(0)=(0,-2,0,5)$ and $Q^{\\\\prime}=F(4)=(16,10,64,21)$.\\n\\n(c) Take the derivative of $F(t)$-that is, of each component of $F(t)$-to obtain a vector $V$ that is tangent to the curve:\\n\\n$$\\nV(t)=\\\\frac{d F(t)}{d t}=\\\\left[2 t, 3,3 t^{2}, 2 t\\\\right]\\n$$\\n\\nNow find $V$ when $t=2$; that is, substitute $t=2$ in the equation for $V(t)$ to obtain $V=V(2)=[4,3,12,4]$. Then normalize $V$ to obtain the desired unit tangent vector $\\\\mathbf{T}$. We have\\n\\n$$\\n\\\\|V\\\\|=\\\\sqrt{16+9+144+16}=\\\\sqrt{185} \\\\quad \\\\text { and } \\\\quad \\\\mathbf{T}=\\\\left[\\\\frac{4}{\\\\sqrt{185}}, \\\\frac{3}{\\\\sqrt{185}}, \\\\frac{12}{\\\\sqrt{185}}, \\\\frac{4}{\\\\sqrt{185}}\\\\right]\\n$$\\n\\n\\n\\\\section*{Spatial Vectors (Vectors in $\\\\mathbf{R}^{\\\\mathbf{3}}$ ), ijk Notation, Cross Product}\\n',\n",
       " '1.21. Let $u=2 \\\\mathbf{i}-3 \\\\mathbf{j}+4 \\\\mathbf{k}, v=3 \\\\mathbf{i}+\\\\mathbf{j}-2 \\\\mathbf{k}, w=\\\\mathbf{i}+5 \\\\mathbf{j}+3 \\\\mathbf{k}$. Find:\\\\\\\\\\n(a) $u+v$\\\\\\\\\\n(b) $2 u-3 v+4 w$\\\\\\\\\\n(c) $u \\\\cdot v$ and $u \\\\cdot w$,\\\\\\\\\\n(d) $\\\\|u\\\\|$ and $\\\\|v\\\\|$.\\n\\nTreat the coefficients of $\\\\mathbf{i}, \\\\mathbf{j}, \\\\mathbf{k}$ just like the components of a vector in $\\\\mathbf{R}^{3}$.\\n\\n(a) Add corresponding coefficients to get $u+v=5 \\\\mathbf{i}-2 \\\\mathbf{j}-2 \\\\mathbf{k}$.\\n\\n(b) First perform the scalar multiplication and then the vector addition:\\n\\n$$\\n\\\\begin{aligned}\\n2 u-3 v+4 w & =(4 \\\\mathbf{i}-6 \\\\mathbf{j}+8 \\\\mathbf{k})+(-9 \\\\mathbf{i}+3 \\\\mathbf{j}+6 \\\\mathbf{k})+(4 \\\\mathbf{i}+20 \\\\mathbf{j}+12 \\\\mathbf{k}) \\\\\\\\\\n& =-\\\\mathbf{i}+17 \\\\mathbf{j}+26 \\\\mathbf{k}\\n\\\\end{aligned}\\n$$\\n\\n(c) Multiply corresponding coefficients and then add:\\n\\n$$\\nu \\\\cdot v=6-3-8=-5 \\\\quad \\\\text { and } \\\\quad u \\\\cdot w=2-15+12=-1\\n$$\\n\\n(d) The norm is the square root of the sum of the squares of the coefficients:\\n\\n$$\\n\\\\|u\\\\|=\\\\sqrt{4+9+16}=\\\\sqrt{29} \\\\quad \\\\text { and } \\\\quad\\\\|v\\\\|=\\\\sqrt{9+1+4}=\\\\sqrt{14}\\n$$\\n',\n",
       " '\\n1.22. Find the (parametric) equation of the line $L$ :\\n\\n(a) through the points $P(1,3,2)$ and $Q(2,5,-6)$;\\n\\n(b) containing the point $P(1,-2,4)$ and perpendicular to the plane $H$ given by the equation $3 x+5 y+7 z=15$.\\n\\n(a) First find $v=\\\\overrightarrow{P Q}=Q-P=[1,2,-8]=\\\\mathbf{i}+2 \\\\mathbf{j}-8 \\\\mathbf{k}$. Then\\n\\n$$\\nL(t)=(t+1,2 t+3,-8 t+2)=(t+1) \\\\mathbf{i}+(2 t+3) \\\\mathbf{j}+(-8 t+2) \\\\mathbf{k}\\n$$\\n\\n(b) Because $L$ is perpendicular to $H$, the line $L$ is in the same direction as the normal vector $\\\\mathbf{N}=3 \\\\mathbf{i}+5 \\\\mathbf{j}+7 \\\\mathbf{k}$ to $H$. Thus,\\n\\n$$\\nL(t)=(3 t+1,5 t-2,7 t+4)=(3 t+1) \\\\mathbf{i}+(5 t-2) \\\\mathbf{j}+(7 t+4) \\\\mathbf{k}\\n$$\\n',\n",
       " '\\n1.23. Let $S$ be the surface $x y^{2}+2 y z=16$ in $\\\\mathbf{R}^{3}$.\\n\\n(a) Find the normal vector $\\\\mathbf{N}(x, y, z)$ to the surface $S$.\\n\\n(b) Find the tangent plane $H$ to $S$ at the point $P(1,2,3)$.\\\\\\\\\\n(a) The formula for the normal vector to a surface $F(x, y, z)=0$ is\\n\\n$$\\n\\\\mathbf{N}(x, y, z)=F_{x} \\\\mathbf{i}+F_{y} \\\\mathbf{j}+F_{z} \\\\mathbf{k}\\n$$\\n\\nwhere $F_{x}, F_{y}, F_{z}$ are the partial derivatives. Using $F(x, y, z)=x y^{2}+2 y z-16$, we obtain\\n\\n$$\\nF_{x}=y^{2}, \\\\quad F_{y}=2 x y+2 z, \\\\quad F_{z}=2 y\\n$$\\n\\nThus, $\\\\mathbf{N}(x, y, z)=y^{2} \\\\mathbf{i}+(2 x y+2 z) \\\\mathbf{j}+2 y \\\\mathbf{k}$.\\n\\n(b) The normal to the surface $S$ at the point $P$ is\\n\\n$$\\n\\\\mathbf{N}(P)=\\\\mathbf{N}(1,2,3)=4 \\\\mathbf{i}+10 \\\\mathbf{j}+4 \\\\mathbf{k}\\n$$\\n\\nHence, $\\\\mathbf{N}=2 \\\\mathbf{i}+5 \\\\mathbf{j}+2 \\\\mathbf{k}$ is also normal to $S$ at $P$. Thus an equation of $H$ has the form $2 x+5 y+2 z=c$. Substitute $P$ in this equation to obtain $c=18$. Thus the tangent plane $H$ to $S$ at $P$ is $2 x+5 y+2 z=18$.\\n',\n",
       " '\\n1.24. Evaluate the following determinants and negative of determinants of order two:\\n\\n(a) (i) $\\\\left|\\\\begin{array}{ll}3 & 4 \\\\\\\\ 5 & 9\\\\end{array}\\\\right|$, (ii) $\\\\left|\\\\begin{array}{rr}2 & -1 \\\\\\\\ 4 & 3\\\\end{array}\\\\right|$, (iii) $\\\\left|\\\\begin{array}{ll}4 & -5 \\\\\\\\ 3 & -2\\\\end{array}\\\\right|$\\n\\n(b) (i) $-\\\\left|\\\\begin{array}{ll}3 & 6 \\\\\\\\ 4 & 2\\\\end{array}\\\\right|$, (ii) $-\\\\left|\\\\begin{array}{rr}7 & -5 \\\\\\\\ 3 & 2\\\\end{array}\\\\right|$, (iii) $-\\\\left|\\\\begin{array}{ll}4 & -1 \\\\\\\\ 8 & -3\\\\end{array}\\\\right|$\\n\\nUse $\\\\left|\\\\begin{array}{ll}a & b \\\\\\\\ c & d\\\\end{array}\\\\right|=a d-b c$ and $-\\\\left|\\\\begin{array}{ll}a & b \\\\\\\\ c & d\\\\end{array}\\\\right|=b c-a d$. Thus,\\\\\\\\\\n(a) (i) $27-20=7$,\\\\\\\\\\n(ii) $6+4=10$, (iii) $-8+15=7$.\\\\\\\\\\n(b) (i) $24-6=18$,\\\\\\\\\\n(ii) $-15-14=-29$, (iii) $-8+12=4$.\\n',\n",
       " '\\n1.25. Let $u=2 \\\\mathbf{i}-3 \\\\mathbf{j}+4 \\\\mathbf{k}, v=3 \\\\mathbf{i}+\\\\mathbf{j}-2 \\\\mathbf{k}, w=\\\\mathbf{i}+5 \\\\mathbf{j}+3 \\\\mathbf{k}$.\\n\\nFind: (a) $u \\\\times v$, (b) $u \\\\times w$\\n\\n(a) Use $\\\\left[\\\\begin{array}{rrr}2 & -3 & 4 \\\\\\\\ 3 & 1 & -2\\\\end{array}\\\\right]$ to get $u \\\\times v=(6-4) \\\\mathbf{i}+(12+4) \\\\mathbf{j}+(2+9) \\\\mathbf{k}=2 \\\\mathbf{i}+16 \\\\mathbf{j}+11 \\\\mathbf{k}$.\\n\\n(b) Use $\\\\left[\\\\begin{array}{rrr}2 & -3 & 4 \\\\\\\\ 1 & 5 & 3\\\\end{array}\\\\right]$ to get $u \\\\times w=(-9-20) \\\\mathbf{i}+(4-6) \\\\mathbf{j}+(10+3) \\\\mathbf{k}=-29 \\\\mathbf{i}-2 \\\\mathbf{j}+13 \\\\mathbf{k}$.\\n',\n",
       " '\\n1.26. Find $u \\\\times v$, where: (a) $\\\\quad u=(1,2,3), v=(4,5,6)$; (b) $\\\\quad u=(-4,7,3), v=(6,-5,2)$.\\n\\n(a) Use $\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 4 & 5 & 6\\\\end{array}\\\\right]$ to get $u \\\\times v=[12-15,12-6,5-8]=[-3,6,-3]$.\\n\\n(b) Use $\\\\left[\\\\begin{array}{rrr}-4 & 7 & 3 \\\\\\\\ 6 & -5 & 2\\\\end{array}\\\\right]$ to get $u \\\\times v=[14+15,18+8,20-42]=[29,26,-22]$.\\n',\n",
       " '\\n1.27. Find a unit vector $u$ orthogonal to $v=[1,3,4]$ and $w=[2,-6,-5]$.\\n\\nFirst find $v \\\\times w$, which is orthogonal to $v$ and $w$.\\n\\nThe array $\\\\left[\\\\begin{array}{rrr}1 & 3 & 4 \\\\\\\\ 2 & -6 & -5\\\\end{array}\\\\right]$ gives $v \\\\times w=[-15+24,8+5,-6-61]=[9,13,-12]$.\\n\\nNormalize $v \\\\times w$ to get $u=[9 / \\\\sqrt{394}, 13 / \\\\sqrt{394},-12 / \\\\sqrt{394}]$.\\n',\n",
       " \"\\n1.28. Let $u=\\\\left(a_{1}, a_{2}, a_{3}\\\\right)$ and $v=\\\\left(b_{1}, b_{2}, b_{3}\\\\right)$ so $u \\\\times v=\\\\left(a_{2} b_{3}-a_{3} b_{2}, a_{3} b_{1}-a_{1} b_{3}, a_{1} b_{2}-a_{2} b_{1}\\\\right)$. Prove:\\n\\n(a) $u \\\\times v$ is orthogonal to $u$ and $v$ [Theorem 1.5(a)].\\n\\n(b) $\\\\|u \\\\times v\\\\|^{2}=(u \\\\cdot u)(v \\\\cdot v)-(u \\\\cdot v)^{2}$ (Lagrange's identity).\\\\\\\\\\n(a) We have\\n\\n$$\\n\\\\begin{aligned}\\nu \\\\cdot(u \\\\times v) & =a_{1}\\\\left(a_{2} b_{3}-a_{3} b_{2}\\\\right)+a_{2}\\\\left(a_{3} b_{1}-a_{1} b_{3}\\\\right)+a_{3}\\\\left(a_{1} b_{2}-a_{2} b_{1}\\\\right) \\\\\\\\\\n& =a_{1} a_{2} b_{3}-a_{1} a_{3} b_{2}+a_{2} a_{3} b_{1}-a_{1} a_{2} b_{3}+a_{1} a_{3} b_{2}-a_{2} a_{3} b_{1}=0\\n\\\\end{aligned}\\n$$\\n\\nThus, $u \\\\times v$ is orthogonal to $u$. Similarly, $u \\\\times v$ is orthogonal to $v$.\\n\\n(b) We have\\n\\n\\n\\\\begin{gather*}\\n\\\\|u \\\\times v\\\\|^{2}=\\\\left(a_{2} b_{3}-a_{3} b_{2}\\\\right)^{2}+\\\\left(a_{3} b_{1}-a_{1} b_{3}\\\\right)^{2}+\\\\left(a_{1} b_{2}-a_{2} b_{1}\\\\right)^{2}  \\\\tag{1}\\\\\\\\\\n(u \\\\cdot u)(v \\\\cdot v)-(u \\\\cdot v)^{2}=\\\\left(a_{1}^{2}+a_{2}^{2}+a_{3}^{2}\\\\right)\\\\left(b_{1}^{2}+b_{2}^{2}+b_{3}^{2}\\\\right)-\\\\left(a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}\\\\right)^{2} \\\\tag{2}\\n\\\\end{gather*}\\n\\n\\nExpansion of the right-hand sides of (1) and (2) establishes the identity.\\n\\n\\n\\\\section*{Complex Numbers, Vectors in $\\\\mathbf{C}^{n}$}\\n\",\n",
       " '1.29. Suppose $z=5+3 i$ and $w=2-4 i$. Find: (a) $z+w$, (b) $z-w$, (c) $z w$.\\n\\nUse the ordinary rules of algebra together with $i^{2}=-1$ to obtain a result in the standard form $a+b i$.\\n\\n(a) $z+w=(5+3 i)+(2-4 i)=7-i$\\n\\n(b) $z-w=(5+3 i)-(2-4 i)=5+3 i-2+4 i=3+7 i$\\n\\n(c) $z w=(5+3 i)(2-4 i)=10-14 i-12 i^{2}=10-14 i+12=22-14 i$\\n',\n",
       " '\\n1.30. Simplify: (a) $(5+3 i)(2-7 i)$, (b) $(4-3 i)^{2}$, (c) $(1+2 i)^{3}$.\\n\\n(a) $(5+3 i)(2-7 i)=10+6 i-35 i-21 i^{2}=31-29 i$\\n\\n(b) $(4-3 i)^{2}=16-24 i+9 i^{2}=7-24 i$\\n\\n(c) $(1+2 i)^{3}=1+6 i+12 i^{2}+8 i^{3}=1+6 i-12-8 i=-11-2 i$\\n',\n",
       " '\\n1.31. Simplify: (a) $i^{0}, i^{3}, i^{4}$, (b) $i^{5}, i^{6}, i^{7}, i^{8}$, (c) $i^{39}, i^{174}, i^{252}, i^{317}$.\\n\\n(a) $i^{0}=1, \\\\quad i^{3}=i^{2}(i)=(-1)(i)=-i, \\\\quad i^{4}=\\\\left(i^{2}\\\\right)\\\\left(i^{2}\\\\right)=(-1)(-1)=1$\\n\\n(b) $i^{5}=\\\\left(i^{4}\\\\right)(i)=(1)(i)=i, \\\\quad i^{6}=\\\\left(i^{4}\\\\right)\\\\left(i^{2}\\\\right)=(1)\\\\left(i^{2}\\\\right)=i^{2}=-1, \\\\quad i^{7}=i^{3}=-i, \\\\quad i^{8}=i^{4}=1$\\n\\n(c) Using $i^{4}=1$ and $i^{n}=i^{4 q+r}=\\\\left(i^{4}\\\\right)^{q} i^{r}=1^{q} i^{r}=i^{r}$, divide the exponent $n$ by 4 to obtain the remainder $r$ :\\n\\n$$\\ni^{39}=i^{4(9)+3}=\\\\left(i^{4}\\\\right)^{9} i^{3}=1^{9} i^{3}=i^{3}=-i, \\\\quad i^{174}=i^{2}=-1, \\\\quad i^{252}=i^{0}=1, \\\\quad i^{317}=i^{1}=i\\n$$\\n',\n",
       " '\\n1.32. Find the complex conjugate of each of the following:\\\\\\\\\\n(a) $6+4 i, 7-5 i, 4+i,-3-i$,\\\\\\\\\\n(b) $6,-3,4 i,-9 i$.\\n\\n(a) $\\\\overline{6+4 i}=6-4 i, \\\\overline{7-5 i}=7+5 i, \\\\overline{4+i}=4-i, \\\\overline{-3-i}=-3+i$\\n\\n(b) $\\\\overline{6}=6, \\\\overline{-3}=-3, \\\\overline{4 i}=-4 i, \\\\overline{-9 i}=9 i$\\n\\n(Note that the conjugate of a real number is the original number, but the conjugate of a pure imaginary number is the negative of the original number.)\\n',\n",
       " '\\n1.33. Find $z \\\\bar{z}$ and $|z|$ when $z=3+4 i$.\\n\\nFor $z=a+b i$, use $z \\\\bar{z}=a^{2}+b^{2}$ and $z=\\\\sqrt{z \\\\bar{z}}=\\\\sqrt{a^{2}+b^{2}}$.\\n\\n$$\\nz \\\\bar{z}=9+16=25, \\\\quad|z|=\\\\sqrt{25}=5\\n$$\\n',\n",
       " '\\n1.34. Simpify $\\\\frac{2-7 i}{5+3 i}$.\\n\\nTo simplify a fraction $z / w$ of complex numbers, multiply both numerator and denominator by $\\\\bar{w}$, the conjugate of the denominator:\\n\\n$$\\n\\\\frac{2-7 i}{5+3 i}=\\\\frac{(2-7 i)(5-3 i)}{(5+3 i)(5-3 i)}=\\\\frac{-11-41 i}{34}=-\\\\frac{11}{34}-\\\\frac{41}{34} i\\n$$\\n',\n",
       " '\\n1.35. Prove: For any complex numbers $z, w \\\\in \\\\mathbf{C}$, (i) $\\\\overline{z+w}=\\\\bar{z}+\\\\bar{w}$, (ii) $\\\\overline{z w}=\\\\bar{z} \\\\bar{w}$, (iii) $\\\\bar{z}=z$.\\n\\nSuppose $z=a+b i$ and $w=c+d i$ where $a, b, c, d \\\\in \\\\mathbf{R}$.\\n\\n(i)\\n\\n$$\\n\\\\begin{aligned}\\n\\\\overline{z+w} & =\\\\overline{(a+b i)+(c+d i)}=\\\\overline{(a+c)+(b+d) i} \\\\\\\\\\n& =(a+c)-(b+d) i=a+c-b i-d i \\\\\\\\\\n& =(a-b i)+(c-d i)=\\\\bar{z}+\\\\bar{w}\\n\\\\end{aligned}\\n$$\\n\\n(ii) $\\\\overline{z w}=\\\\overline{(a+b i)(c+d i)}=\\\\overline{(a c-b d)+(a d+b c) i}$\\n\\n$$\\n=(a c-b d)-(a d+b c) i=(a-b i)(c-d i)=\\\\bar{z} \\\\bar{w}\\n$$\\n\\n(iii) $\\\\bar{z}=a+b i=\\\\overline{\\\\overline{a-b i}}=a-(-b) i=a+b i=z$\\n',\n",
       " '\\n1.36. Prove: For any complex numbers $z, w \\\\in \\\\mathbf{C},|z w|=|z||w|$.\\n\\nBy (ii) of Problem 1.35,\\n\\n$$\\n|z w|^{2}=(z w)(\\\\overline{z w})=(z w)(\\\\bar{z} \\\\bar{w})=(z \\\\bar{z})(w \\\\bar{w})=|z|^{2}|w|^{2}\\n$$\\n\\nThe square root of both sides gives us the desired result.\\n',\n",
       " \"\\n1.37. Prove: For any complex numbers $z, w \\\\in \\\\mathbf{C},|z+w| \\\\leq|z|+|w|$.\\n\\nSuppose $z=a+b i$ and $w=c+d i$ where $a, b, c, d \\\\in \\\\mathbf{R}$. Consider the vectors $u=(a, b)$ and $v=(c, d)$ in $\\\\mathbf{R}^{2}$. Note that\\n\\n$$\\n|z|=\\\\sqrt{a^{2}+b^{2}}=\\\\|u\\\\|, \\\\quad|w|=\\\\sqrt{c^{2}+d^{2}}=\\\\|v\\\\|\\n$$\\n\\nand\\n\\n$$\\n|z+w|=|(a+c)+(b+d) i|=\\\\sqrt{(a+c)^{2}+(b+d)^{2}}=\\\\|(a+c, b+d)\\\\|=\\\\|u+v\\\\|\\n$$\\n\\nBy Minkowski's inequality (Problem 1.15), $\\\\|u+v\\\\| \\\\leq\\\\|u\\\\|+\\\\|v\\\\|$, and so\\n\\n$$\\n|z+w|=\\\\|u+v\\\\| \\\\leq\\\\|u\\\\|+\\\\|v\\\\|=|z|+|w|\\n$$\\n\",\n",
       " '\\n1.38. Find the dot products $u \\\\cdot v$ and $v \\\\cdot u$ where: (a) $u=(1-2 i, 3+i), v=(4+2 i, 5-6 i)$, (b) $u=(3-2 i, 4 i, 1+6 i), v=(5+i, 2-3 i, 7+2 i)$.\\n\\nRecall that conjugates of the second vector appear in the dot product\\n\\n$$\\n\\\\left(z_{1}, \\\\ldots, z_{n}\\\\right) \\\\cdot\\\\left(w_{1}, \\\\ldots, w_{n}\\\\right)=z_{1} \\\\bar{w}_{1}+\\\\cdots+z_{n} \\\\bar{w}_{n}\\n$$\\n\\n(a) $u \\\\cdot v=(1-2 i)(\\\\overline{4+2 i})+(3+i)(\\\\overline{5-6 i})$\\n\\n$$\\n=(1-2 i)(4-2 i)+(3+i)(5+6 i)=-10 i+9+23 i=9+13 i\\n$$\\n\\n$v \\\\cdot u=(4+2 i)(\\\\overline{1-2 i})+(5-6 i)(\\\\overline{3+i})$\\n\\n$$\\n=(4+2 i)(1+2 i)+(5-6 i)(3-i)=10 i+9-23 i=9-13 i\\n$$\\n\\n(b) $u \\\\cdot v=(3-2 i)(\\\\overline{5+i})+(4 i)(\\\\overline{2-3 i})+(1+6 i)(\\\\overline{7+2 i})$\\n\\n$$\\n=(3-2 i)(5-i)+(4 i)(2+3 i)+(1+6 i)(7-2 i)=20+35 i\\n$$\\n\\n$$\\n\\\\begin{aligned}\\nv \\\\cdot u & =(5+i)(\\\\overline{3-2 i})+(2-3 i)(\\\\overline{4 i})+(7+2 i)(\\\\overline{1+6 i}) \\\\\\\\\\n& =(5+i)(3+2 i)+(2-3 i)(-4 i)+(7+2 i)(1-6 i)=20-35 i\\n\\\\end{aligned}\\n$$\\n\\nIn both cases, $v \\\\cdot u=\\\\overline{u \\\\cdot v}$. This holds true in general, as seen in Problem 1.40.\\n',\n",
       " '\\n1.39. Let $u=(7-2 i, 2+5 i)$ and $v=(1+i,-3-6 i)$. Find:\\\\\\\\\\n(a) $u+v$\\\\\\\\\\n(b) $2 i u$,\\\\\\\\\\n(c) $(3-i) v$,\\\\\\\\\\n(d) $u \\\\cdot v$,\\\\\\\\\\n(e) $\\\\|u\\\\|$ and $\\\\|v\\\\|$.\\\\\\\\\\n(a) $u+v=(7-2 i+1+i, 2+5 i-3-6 i)=(8-i,-1-i)$\\\\\\\\\\n(b) $2 i u=\\\\left(14 i-4 i^{2}, 4 i+10 i^{2}\\\\right)=(4+14 i,-10+4 i)$\\\\\\\\\\n(c) $(3-i) v=\\\\left(3+3 i-i-i^{2},-9-18 i+3 i+6 i^{2}\\\\right)=(4+2 i,-15-15 i)$\\\\\\\\\\n(d) $u \\\\cdot v=(7-2 i)(\\\\overline{1+i})+(2+5 i)(\\\\overline{-3-6 i})$\\n\\n$$\\n=(7-2 i)(1-i)+(2+5 i)(-3+6 i)=5-9 i-36-3 i=-31-12 i\\n$$\\n\\n(e) $\\\\|u\\\\|=\\\\sqrt{7^{2}+(-2)^{2}+2^{2}+5^{2}}=\\\\sqrt{82}$ and $\\\\|v\\\\|=\\\\sqrt{1^{2}+1^{2}+(-3)^{2}+(-6)^{2}}=\\\\sqrt{47}$\\n',\n",
       " '\\n1.40. Prove: For any vectors $u, v \\\\in \\\\mathbf{C}^{n}$ and any scalar $z \\\\in \\\\mathbf{C}$, (i) $u \\\\cdot v=\\\\overline{v \\\\cdot u}$, (ii) $(z u) \\\\cdot v=z(u \\\\cdot v)$, (iii) $u \\\\cdot(z v)=\\\\bar{z}(u \\\\cdot v)$.\\n\\nSuppose $u=\\\\left(z_{1}, z_{2}, \\\\ldots, z_{n}\\\\right)$ and $v=\\\\left(w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right)$.\\n\\n(i) Using the properties of the conjugate,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\overline{v \\\\cdot u} & =\\\\overline{w_{1} \\\\bar{z}_{1}+w_{2} \\\\bar{z}_{2}+\\\\cdots+w_{n} \\\\bar{z}_{n}}=\\\\overline{w_{1} \\\\bar{z}_{1}}+\\\\overline{w_{2} \\\\bar{z}_{2}}+\\\\cdots+\\\\overline{w_{n} \\\\bar{z}_{n}} \\\\\\\\\\n& =\\\\bar{w}_{1} z_{1}+\\\\bar{w}_{2} z_{2}+\\\\cdots+\\\\bar{w}_{n} z_{n}=z_{1} \\\\bar{w}_{1}+z_{2} \\\\bar{w}_{2}+\\\\cdots+z_{n} \\\\bar{w}_{n}=u \\\\cdot v\\n\\\\end{aligned}\\n$$\\n\\n(ii) Because $z u=\\\\left(z z_{1}, z z_{2}, \\\\ldots, z z_{n}\\\\right)$,\\n\\n$$\\n(z u) \\\\cdot v=z z_{1} \\\\bar{w}_{1}+z z_{2} \\\\bar{w}_{2}+\\\\cdots+z z_{n} \\\\bar{w}_{n}=z\\\\left(z_{1} \\\\bar{w}_{1}+z_{2} \\\\bar{w}_{2}+\\\\cdots+z_{n} \\\\bar{w}_{n}\\\\right)=z(u \\\\cdot v)\\n$$\\n\\n(Compare with Theorem 1.2 on vectors in $\\\\mathbf{R}^{n}$.)\\n\\n(iii) Using (i) and (ii),\\n\\n$$\\nu \\\\cdot(z v)=\\\\overline{(z v) \\\\cdot u}=z(\\\\overline{v \\\\cdot u})=\\\\bar{z}(\\\\overline{v \\\\cdot u})=\\\\bar{z}(u \\\\cdot v)\\n$$\\n\\n',\n",
       " '2.1 Given $A=\\\\left[\\\\begin{array}{rrr}1 & -2 & 3 \\\\\\\\ 4 & 5 & -6\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{rrr}3 & 0 & 2 \\\\\\\\ -7 & 1 & 8\\\\end{array}\\\\right]$, find:\\n\\n(a) $A+B$, (b) $2 A-3 B$.\\n\\n(a) Add the corresponding elements:\\n\\n$$\\nA+B=\\\\left[\\\\begin{array}{rrr}\\n1+3 & -2+0 & 3+2 \\\\\\\\\\n4-7 & 5+1 & -6+8\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrr}\\n4 & -2 & 5 \\\\\\\\\\n-3 & 6 & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) First perform the scalar multiplication and then a matrix addition:\\n\\n$$\\n2 A-3 B=\\\\left[\\\\begin{array}{rrr}\\n2 & -4 & 6 \\\\\\\\\\n8 & 10 & -12\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rrr}\\n-9 & 0 & -6 \\\\\\\\\\n21 & -3 & -24\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrr}\\n-7 & -4 & 0 \\\\\\\\\\n29 & 7 & -36\\n\\\\end{array}\\\\right]\\n$$\\n\\n(Note that we multiply $B$ by -3 and then add, rather than multiplying $B$ by 3 and subtracting. This usually prevents errors.)\\n',\n",
       " '\\n2.2. Find $x, y, z, t$ where $3\\\\left[\\\\begin{array}{cc}x & y \\\\\\\\ z & t\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}x & 6 \\\\\\\\ -1 & 2 t\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{cc}4 & x+y \\\\\\\\ z+t & 3\\\\end{array}\\\\right]$.\\n\\nWrite each side as a single equation:\\n\\n$$\\n\\\\left[\\\\begin{array}{cc}\\n3 x & 3 y \\\\\\\\\\n3 z & 3 t\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\nx+4 & x+y+6 \\\\\\\\\\nz+t-1 & 2 t+3\\n\\\\end{array}\\\\right]\\n$$\\n\\nSet corresponding entries equal to each other to obtain the following system of four equations:\\n\\n$$\\n\\\\begin{array}{cccc} \\n& 3 x=x+4, \\\\quad 3 y=x+y+6, & 3 z=z+t-1, & 3 t=2 t+3 \\\\\\\\\\n\\\\text { or } \\\\quad 2 x=4, \\\\quad 2 y=6+x, \\\\quad 2 z=t-1, & t=3\\n\\\\end{array}\\n$$\\n\\nThe solution is $x=2, y=4, z=1, t=3$.\\n',\n",
       " '\\n2.3. Prove Theorem 2.1 (i) and (v): (i) $(A+B)+C=A+(B+C)$, (v) $k(A+B)=k A+k B$.\\n\\nSuppose $A=\\\\left[a_{i j}\\\\right], B=\\\\left[b_{i j}\\\\right], C=\\\\left[c_{i j}\\\\right]$. The proof reduces to showing that corresponding $i j$-entries in each side of each matrix equation are equal. [We prove only (i) and (v), because the other parts of Theorem 2.1 are proved similarly.]\\\\\\\\\\n(i) The $i j$-entry of $A+B$ is $a_{i j}+b_{i j}$; hence, the $i j$-entry of $(A+B)+C$ is $\\\\left(a_{i j}+b_{i j}\\\\right)+c_{i j}$. On the other hand, the $i j$-entry of $B+C$ is $b_{i j}+c_{i j}$; hence, the $i j$-entry of $A+(B+C)$ is $a_{i j}+\\\\left(b_{i j}+c_{i j}\\\\right)$. However, for scalars in $K$,\\n\\n$$\\n\\\\left(a_{i j}+b_{i j}\\\\right)+c_{i j}=a_{i j}+\\\\left(b_{i j}+c_{i j}\\\\right)\\n$$\\n\\nThus, $(A+B)+C$ and $A+(B+C)$ have identical $i j$-entries. Therefore, $(A+B)+C=A+(B+C)$.\\n\\n(v) The $i j$-entry of $A+B$ is $a_{i j}+b_{i j}$; hence, $k\\\\left(a_{i j}+b_{i j}\\\\right)$ is the $i j$-entry of $k(A+B)$. On the other hand, the $i j$ entries of $k A$ and $k B$ are $k a_{i j}$ and $k b_{i j}$, respectively. Thus, $k a_{i j}+k b_{i j}$ is the $i j$-entry of $k A+k B$. However, for scalars in $K$,\\n\\n$$\\nk\\\\left(a_{i j}+b_{i j}\\\\right)=k a_{i j}+k b_{i j}\\n$$\\n\\nThus, $k(A+B)$ and $k A+k B$ have identical $i j$-entries. Therefore, $k(A+B)=k A+k B$.\\n\\n\\n\\\\section*{Matrix Multiplication}\\n',\n",
       " '2.4. Calculate: (a) $[8,-4,5]\\\\left[\\\\begin{array}{r}3 \\\\\\\\ 2 \\\\\\\\ -1\\\\end{array}\\\\right], \\\\quad$ (b) $[6,-1,7,5]\\\\left[\\\\begin{array}{r}4 \\\\\\\\ -9 \\\\\\\\ -3 \\\\\\\\ 2\\\\end{array}\\\\right], \\\\quad$ (c) $[3,8,-2,4]\\\\left[\\\\begin{array}{r}5 \\\\\\\\ -1 \\\\\\\\ 6\\\\end{array}\\\\right]$\\n\\n(a) Multiply the corresponding entries and add:\\n\\n$$\\n[8,-4,5]\\\\left[\\\\begin{array}{r}\\n3 \\\\\\\\\\n2 \\\\\\\\\\n-1\\n\\\\end{array}\\\\right]=8(3)+(-4)(2)+5(-1)=24-8-5=11\\n$$\\n\\n(b) Multiply the corresponding entries and add:\\n\\n$$\\n[6,-1,7,5]\\\\left[\\\\begin{array}{r}\\n4 \\\\\\\\\\n-9 \\\\\\\\\\n-3 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]=24+9-21+10=22\\n$$\\n\\n(c) The product is not defined when the row matrix and the column matrix have different numbers of elements.\\n',\n",
       " '\\n2.5. Let $(r \\\\times s)$ denote an $r \\\\times s$ matrix. Find the sizes of those matrix products that are defined:\\\\\\\\\\n(a) $(2 \\\\times 3)(3 \\\\times 4)$,\\\\\\\\\\n(c) $(1 \\\\times 2)(3 \\\\times 1)$,\\\\\\\\\\n(e) $(4 \\\\times 4)(3 \\\\times 3)$\\\\\\\\\\n(b) $(4 \\\\times 1)(1 \\\\times 2)$,\\\\\\\\\\n(d) $(5 \\\\times 2)(2 \\\\times 3)$,\\\\\\\\\\n(f) $(2 \\\\times 2)(2 \\\\times 4)$\\n\\nIn each case, the product is defined if the inner numbers are equal, and then the product will have the size of the outer numbers in the given order.\\\\\\\\\\n(a) $2 \\\\times 4$,\\\\\\\\\\n(c) not defined,\\\\\\\\\\n(e) not defined\\\\\\\\\\n(b) $4 \\\\times 2$,\\\\\\\\\\n(d) $5 \\\\times 3$,\\\\\\\\\\n(f) $2 \\\\times 4$\\n',\n",
       " '\\n2.6. Let $A=\\\\left[\\\\begin{array}{rr}1 & 3 \\\\\\\\ 2 & -1\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{rrr}2 & 0 & -4 \\\\\\\\ 3 & -2 & 6\\\\end{array}\\\\right]$. Find: (a) $A B$, (b) $B A$.\\n\\n(a) Because $A$ is a $2 \\\\times 2$ matrix and $B$ a $2 \\\\times 3$ matrix, the product $A B$ is defined and is a $2 \\\\times 3$ matrix. To obtain the entries in the first row of $A B$, multiply the first row $[1,3]$ of $A$ by the columns $\\\\left[\\\\begin{array}{l}2 \\\\\\\\ 3\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{r}0 \\\\\\\\ -2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{r}-4 \\\\\\\\ 6\\\\end{array}\\\\right]$ of $B$, respectively, as follows:\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{rr}\\n1 & 3 \\\\\\\\\\n2 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rrr}\\n2 & 0 & -4 \\\\\\\\\\n3 & -2 & 6\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll}\\n2+9 & 0-6 & -4+18\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll}\\n11 & -6 & 14\\n\\\\end{array}\\\\right]\\n$$\\n\\nTo obtain the entries in the second row of $A B$, multiply the second row $[2,-1]$ of $A$ by the columns of $B$ :\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{rr}\\n1 & 3 \\\\\\\\\\n2 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rrr}\\n2 & 0 & -4 \\\\\\\\\\n3 & -2 & 6\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ccc}\\n11 & -6 & 14 \\\\\\\\\\n4-3 & 0+2 & -8-6\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus,\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{rrr}\\n11 & -6 & 14 \\\\\\\\\\n1 & 2 & -14\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) The size of $B$ is $2 \\\\times 3$ and that of $A$ is $2 \\\\times 2$. The inner numbers 3 and 2 are not equal; hence, the product $B A$ is not defined.\\n',\n",
       " '\\n2.7. Find $A B$, where $A=\\\\left[\\\\begin{array}{rrr}2 & 3 & -1 \\\\\\\\ 4 & -2 & 5\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{rrrr}2 & -1 & 0 & 6 \\\\\\\\ 1 & 3 & -5 & 1 \\\\\\\\ 4 & 1 & -2 & 2\\\\end{array}\\\\right]$.\\n\\nBecause $A$ is a $2 \\\\times 3$ matrix and $B$ a $3 \\\\times 4$ matrix, the product $A B$ is defined and is a $2 \\\\times 4$ matrix. Multiply the rows of $A$ by the columns of $B$ to obtain\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{cccc}\\n4+3-4 & -2+9-1 & 0-15+2 & 12+3-2 \\\\\\\\\\n8-2+20 & -4-6+5 & 0+10-10 & 24-2+10\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrrr}\\n3 & 6 & -13 & 13 \\\\\\\\\\n26 & -5 & 0 & 32\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n2.8. Find: (a) $\\\\left[\\\\begin{array}{rr}1 & 6 \\\\\\\\ -3 & 5\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}2 \\\\\\\\ -7\\\\end{array}\\\\right], \\\\quad$ (b) $\\\\left[\\\\begin{array}{r}2 \\\\\\\\ -7\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & 6 \\\\\\\\ -3 & 5\\\\end{array}\\\\right], \\\\quad$ (c) $[2,-7]\\\\left[\\\\begin{array}{rr}1 & 6 \\\\\\\\ -3 & 5\\\\end{array}\\\\right]$.\\n\\n(a) The first factor is $2 \\\\times 2$ and the second is $2 \\\\times 1$, so the product is defined as a $2 \\\\times 1$ matrix:\\n\\n$$\\n\\\\left[\\\\begin{array}{rr}\\n1 & 6 \\\\\\\\\\n-3 & 5\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{c}\\n2-42 \\\\\\\\\\n-6-35\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n-40 \\\\\\\\\\n-41\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) The product is not defined, because the first factor is $2 \\\\times 1$ and the second factor is $2 \\\\times 2$.\\n\\n(c) The first factor is $1 \\\\times 2$ and the second factor is $2 \\\\times 2$, so the product is defined as a $1 \\\\times 2$ (row) matrix:\\n\\n$$\\n[2,-7]\\\\left[\\\\begin{array}{rr}\\n1 & 6 \\\\\\\\\\n-3 & 5\\n\\\\end{array}\\\\right]=[2+21,12-35]=[23,-23]\\n$$\\n',\n",
       " \"\\n2.9. Clearly, $0 A=0$ and $A 0=0$, where the 0 's are zero matrices (with possibly different sizes). Find matrices $A$ and $B$ with no zero entries such that $A B=0$.\\n\\n$$\\n\\\\text { Let } A=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n2 & 4\\n\\\\end{array}\\\\right] \\\\text { and } B=\\\\left[\\\\begin{array}{rr}\\n6 & 2 \\\\\\\\\\n-3 & -1\\n\\\\end{array}\\\\right] \\\\text {. Then } A B=\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right] \\\\text {. }\\n$$\\n\",\n",
       " '\\n2.10. Prove Theorem 2.2(i): $(A B) C=A(B C)$.\\n\\nLet $A=\\\\left[a_{i j}\\\\right], \\\\quad B=\\\\left[b_{j k}\\\\right], \\\\quad C=\\\\left[c_{k l}\\\\right], \\\\quad$ and let $A B=S=\\\\left[s_{i k}\\\\right], \\\\quad B C=T=\\\\left[t_{j}\\\\right]$. Then\\n\\n$$\\ns_{i k}=\\\\sum_{j=1}^{m} a_{i j} b_{j k} \\\\quad \\\\text { and } \\\\quad t_{j l}=\\\\sum_{k=1}^{n} b_{j k} c_{k l}\\n$$\\n\\nMultiplying $S=A B$ by $C$, the $i l$-entry of $(A B) C$ is\\n\\n$$\\ns_{i 1} c_{1 l}+s_{i 2} c_{2 l}+\\\\cdots+s_{i n} c_{n l}=\\\\sum_{k=1}^{n} s_{i k} c_{k l}=\\\\sum_{k=1}^{n} \\\\sum_{j=1}^{m}\\\\left(a_{i j} b_{j k}\\\\right) c_{k l}\\n$$\\n\\nOn the other hand, multiplying $A$ by $T=B C$, the $i l$-entry of $A(B C)$ is\\n\\n$$\\na_{i 1} t_{1 l}+a_{i 2} t_{2 l}+\\\\cdots+a_{i n} t_{n l}=\\\\sum_{j=1}^{m} a_{i j} t_{j l}=\\\\sum_{j=1}^{m} \\\\sum_{k=1}^{n} a_{i j}\\\\left(b_{j k} c_{k l}\\\\right)\\n$$\\n\\nThe above sums are equal; that is, corresponding elements in $(A B) C$ and $A(B C)$ are equal. Thus, $(A B) C=A(B C)$.\\n',\n",
       " '\\n2.11. Prove Theorem 2.2(ii): $A(B+C)=A B+A C$.\\n\\nLet $A=\\\\left[a_{i j}\\\\right], B=\\\\left[b_{j k}\\\\right], C=\\\\left[c_{j k}\\\\right]$, and let $D=B+C=\\\\left[d_{j k}\\\\right], E=A B=\\\\left[e_{i k}\\\\right], F=A C=\\\\left[f_{i k}\\\\right]$. Then\\n\\n$$\\nd_{j k}=b_{j k}+c_{j k}, \\\\quad e_{i k}=\\\\sum_{j=1}^{m} a_{i j} b_{j k}, \\\\quad f_{i k}=\\\\sum_{j=1}^{m} a_{i j} c_{j k}\\n$$\\n\\nThus, the $i k$-entry of the matrix $A B+A C$ is\\n\\n$$\\ne_{i k}+f_{i k}=\\\\sum_{j=1}^{m} a_{i j} b_{j k}+\\\\sum_{j=1}^{m} a_{i j} c_{j k}=\\\\sum_{j=1}^{m} a_{i j}\\\\left(b_{j k}+c_{j k}\\\\right)\\n$$\\n\\nOn the other hand, the $i k$-entry of the matrix $A D=A(B+C)$ is\\n\\n$$\\na_{i 1} d_{1 k}+a_{i 2} d_{2 k}+\\\\cdots+a_{i m} d_{m k}=\\\\sum_{j=1}^{m} a_{i j} d_{j k}=\\\\sum_{j=1}^{m} a_{i j}\\\\left(b_{j k}+c_{j k}\\\\right)\\n$$\\n\\nThus, $A(B+C)=A B+A C$, because the corresponding elements are equal.\\n\\n\\n\\\\section*{Transpose}\\n',\n",
       " '2.12. Find the transpose of each matrix:\\\\\\\\\\n$A=\\\\left[\\\\begin{array}{rrr}1 & -2 & 3 \\\\\\\\ 7 & 8 & -9\\\\end{array}\\\\right]$\\\\\\\\\\n$B=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 2 & 4 & 5 \\\\\\\\ 3 & 5 & 6\\\\end{array}\\\\right]$,\\\\\\\\\\n$C=[1,-3,5,-7]$,\\\\\\\\\\n$D=\\\\left[\\\\begin{array}{r}2 \\\\\\\\ -4 \\\\\\\\ 6\\\\end{array}\\\\right]$\\n\\nRewrite the rows of each matrix as columns to obtain the transpose of the matrix:\\n\\n$$\\nA^{T}=\\\\left[\\\\begin{array}{rr}\\n1 & 7 \\\\\\\\\\n-2 & 8 \\\\\\\\\\n3 & -9\\n\\\\end{array}\\\\right], \\\\quad B^{T}=\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 3 \\\\\\\\\\n2 & 4 & 5 \\\\\\\\\\n3 & 5 & 6\\n\\\\end{array}\\\\right], \\\\quad C^{T}=\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-3 \\\\\\\\\\n5 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right], \\\\quad D^{T}=[2,-4,6]\\n$$\\n\\n(Note that $B^{T}=B$; such a matrix is said to be symmetric. Note also that the transpose of the row vector $C$ is a column vector, and the transpose of the column vector $D$ is a row vector.)\\n',\n",
       " '\\n2.13. Prove Theorem 2.3(iv): $(A B)^{T}=B^{T} A^{T}$.\\n\\nLet $A=\\\\left[a_{i k}\\\\right]$ and $B=\\\\left[b_{k j}\\\\right]$. Then the $i j$-entry of $A B$ is\\n\\n$$\\na_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\\\cdots+a_{i m} b_{m j}\\n$$\\n\\nThis is the $j i$-entry (reverse order) of $(A B)^{T}$. Now column $j$ of $B$ becomes row $j$ of $B^{T}$, and row $i$ of $A$ becomes column $i$ of $A^{T}$. Thus, the $i j$-entry of $B^{T} A^{T}$ is\\n\\n$$\\n\\\\left[b_{1 j}, b_{2 j}, \\\\ldots, b_{m j}\\\\right]\\\\left[a_{i 1}, a_{i 2}, \\\\ldots, a_{i m}\\\\right]^{T}=b_{1 j} a_{i 1}+b_{2 j} a_{i 2}+\\\\cdots+b_{m j} a_{i m}\\n$$\\n\\nThus, $(A B)^{T}=B^{T} A^{T}$ on because the corresponding entries are equal.\\n\\n\\n\\\\section*{Square Matrices}\\n',\n",
       " '2.14. Find the diagonal and trace of each matrix:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rrr}1 & 3 & 6 \\\\\\\\ 2 & -5 & 8 \\\\\\\\ 4 & -2 & 9\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrr}2 & 4 & 8 \\\\\\\\ 3 & -7 & 9 \\\\\\\\ -5 & 0 & 2\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rrr}1 & 2 & -3 \\\\\\\\ 4 & -5 & 6\\\\end{array}\\\\right]$.\\n\\n(a) The diagonal of $A$ consists of the elements from the upper left corner of $A$ to the lower right corner of $A$ or, in other words, the elements $a_{11}, a_{22}, a_{33}$. Thus, the diagonal of $A$ consists of the numbers $1,-5$, and 9 . The trace of $A$ is the sum of the diagonal elements. Thus,\\n\\n$$\\n\\\\operatorname{tr}(A)=1-5+9=5\\n$$\\n\\n(b) The diagonal of $B$ consists of the numbers $2,-7$, and 2 . Hence,\\n\\n$$\\n\\\\operatorname{tr}(B)=2-7+2=-3\\n$$\\n\\n(c) The diagonal and trace are only defined for square matrices.\\n',\n",
       " '\\n2.15. Let $A=\\\\left[\\\\begin{array}{rr}1 & 2 \\\\\\\\ 4 & -3\\\\end{array}\\\\right]$, and let $f(x)=2 x^{3}-4 x+5$ and $g(x)=x^{2}+2 x+11$. Find\\\\\\\\\\n(a) $A^{2}$,\\\\\\\\\\n(b) $A^{3}$,\\\\\\\\\\n(c) $f(A)$,\\\\\\\\\\n(d) $g(A)$.\\n\\n(a) $A^{2}=A A=\\\\left[\\\\begin{array}{rr}1 & 2 \\\\\\\\ 4 & -3\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & 2 \\\\\\\\ 4 & -3\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}1+8 & 2-6 \\\\\\\\ 4-12 & 8+9\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}9 & -4 \\\\\\\\ -8 & 17\\\\end{array}\\\\right]$\\n\\n(b) $A^{3}=A A^{2}=\\\\left[\\\\begin{array}{rr}1 & 2 \\\\\\\\ 4 & -3\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}9 & -4 \\\\\\\\ -8 & 17\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}9-16 & -4+34 \\\\\\\\ 36+24 & -16-51\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}-7 & 30 \\\\\\\\ 60 & -67\\\\end{array}\\\\right]$\\n\\n(c) First substitute $A$ for $x$ and $5 I$ for the constant in $f(x)$, obtaining\\n\\n$$\\nf(A)=2 A^{3}-4 A+5 I=2\\\\left[\\\\begin{array}{rr}\\n-7 & 30 \\\\\\\\\\n60 & -67\\n\\\\end{array}\\\\right]-4\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n4 & -3\\n\\\\end{array}\\\\right]+5\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow perform the scalar multiplication and then the matrix addition:\\n\\n$$\\nf(A)=\\\\left[\\\\begin{array}{rr}\\n-14 & 60 \\\\\\\\\\n120 & -134\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}\\n-4 & -8 \\\\\\\\\\n-16 & 12\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{ll}\\n5 & 0 \\\\\\\\\\n0 & 5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n-13 & 52 \\\\\\\\\\n104 & -117\\n\\\\end{array}\\\\right]\\n$$\\n\\n(d) Substitute $A$ for $x$ and 11I for the constant in $g(x)$, and then calculate as follows:\\n\\n$$\\n\\\\begin{aligned}\\ng(A) & =A^{2}+2 A-11 I=\\\\left[\\\\begin{array}{rr}\\n9 & -4 \\\\\\\\\\n-8 & 17\\n\\\\end{array}\\\\right]+2\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n4 & -3\\n\\\\end{array}\\\\right]-11\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =\\\\left[\\\\begin{array}{rr}\\n9 & -4 \\\\\\\\\\n-8 & 17\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}\\n2 & 4 \\\\\\\\\\n8 & -6\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}\\n-11 & 0 \\\\\\\\\\n0 & -11\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nBecause $g(A)$ is the zero matrix, $A$ is a root of the polynomial $g(x)$.\\n',\n",
       " '\\n2.16. Let $A=\\\\left[\\\\begin{array}{rr}1 & 3 \\\\\\\\ 4 & -3\\\\end{array}\\\\right]$. (a) Find a nonzero column vector $u=\\\\left[\\\\begin{array}{l}x \\\\\\\\ y\\\\end{array}\\\\right]$ such that $A u=3 u$.\\n\\n(b) Describe all such vectors.\\n\\n(a) First set up the matrix equation $A u=3 u$, and then write each side as a single matrix (column vector) as follows:\\n\\n$$\\n\\\\left[\\\\begin{array}{rr}\\n1 & 3 \\\\\\\\\\n4 & -3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right]=3\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right], \\\\quad \\\\text { and then } \\\\quad\\\\left[\\\\begin{array}{c}\\nx+3 y \\\\\\\\\\n4 x-3 y\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n3 x \\\\\\\\\\n3 y\\n\\\\end{array}\\\\right]\\n$$\\n\\nSet the corresponding elements equal to each other to obtain a system of equations:\\n\\n$$\\n\\\\begin{aligned}\\nx+3 y & =3 x \\\\\\\\\\n4 x-3 y & =3 y\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n2 x-3 y & =0 \\\\\\\\\\n4 x-6 y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad 2 x-3 y=0\\n$$\\n\\nThe system reduces to one nondegenerate linear equation in two unknowns, and so has an infinite number of solutions. To obtain a nonzero solution, let, say, $y=2$; then $x=3$. Thus, $u=(3,2)^{T}$ is a desired nonzero vector.\\n\\n(b) To find the general solution, set $y=a$, where $a$ is a parameter. Substitute $y=a$ into $2 x-3 y=0$ to obtain $x=\\\\frac{3}{2} a$. Thus, $u=\\\\left(\\\\frac{3}{2} a, a\\\\right)^{T}$ represents all such solutions.\\n\\n\\n\\\\section*{Invertible Matrices, Inverses}\\n',\n",
       " '2.17. Show that $A=\\\\left[\\\\begin{array}{rrr}1 & 0 & 2 \\\\\\\\ 2 & -1 & 3 \\\\\\\\ 4 & 1 & 8\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{rrr}-11 & 2 & 2 \\\\\\\\ -4 & 0 & 1 \\\\\\\\ 6 & -1 & -1\\\\end{array}\\\\right]$ are inverses.\\n\\nCompute the product $A B$, obtaining\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{lll}\\n-11+0+12 & 2+0-2 & 2+0-2 \\\\\\\\\\n-22+4+18 & 4+0-3 & 4-1-3 \\\\\\\\\\n-44-4+48 & 8+0-8 & 8+1-8\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]=I\\n$$\\n\\nBecause $A B=I$, we can conclude (Theorem 3.16) that $B A=I$. Accordingly, $A$ and $B$ are inverses.\\n',\n",
       " '\\n2.18. Find the inverse, if possible, of each matrix:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{ll}5 & 3 \\\\\\\\ 4 & 2\\\\end{array}\\\\right]$\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rr}2 & -3 \\\\\\\\ 1 & 3\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $\\\\left[\\\\begin{array}{rr}-2 & 6 \\\\\\\\ 3 & -9\\\\end{array}\\\\right]$.\\n\\nUse the formula for the inverse of a $2 \\\\times 2$ matrix appearing in Section 2.9.\\n\\n(a) First find $|A|=5(2)-3(4)=10-12=-2$. Next interchange the diagonal elements, take the negatives of the nondiagonal elements, and multiply by $1 /|A|$ :\\n\\n$$\\nA^{-1}=-\\\\frac{1}{2}\\\\left[\\\\begin{array}{rr}\\n2 & -3 \\\\\\\\\\n-4 & 5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n-1 & \\\\frac{3}{2} \\\\\\\\\\n2 & -\\\\frac{5}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) First find $|B|=2(3)-(-3)(1)=6+3=9$. Next interchange the diagonal elements, take the negatives of the nondiagonal elements, and multiply by $1 /|B|$ :\\n\\n$$\\nB^{-1}=\\\\frac{1}{9}\\\\left[\\\\begin{array}{rr}\\n3 & 3 \\\\\\\\\\n-1 & 2\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{3} & \\\\frac{1}{3} \\\\\\\\\\n-\\\\frac{1}{9} & \\\\frac{2}{9}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) First find $|C|=-2(-9)-6(3)=18-18=0$. Because $|C|=0, C$ has no inverse.\\n',\n",
       " '\\n2.19. Let $A=\\\\left[\\\\begin{array}{lll}1 & 1 & 1 \\\\\\\\ 0 & 1 & 2 \\\\\\\\ 1 & 2 & 4\\\\end{array}\\\\right]$. Find $A^{-1}=\\\\left[\\\\begin{array}{lll}x_{1} & x_{2} & x_{3} \\\\\\\\ y_{1} & y_{2} & y_{3} \\\\\\\\ z_{1} & z_{2} & z_{3}\\\\end{array}\\\\right]$.\\n\\nMultiplying $A$ by $A^{-1}$ and setting the nine entries equal to the nine entries of the identity matrix $I$ yields the following three systems of three equations in three of the unknowns:\\n\\n$$\\n\\\\begin{aligned}\\n& x_{1}+y_{1}+z_{1}=1 \\\\quad x_{2}+y_{2}+z_{2}=0 \\\\quad x_{3}+y_{3}+z_{3}=0 \\\\\\\\\\n& y_{1}+2 z_{1}=0 \\\\quad y_{2}+2 z_{2}=1 \\\\quad y_{3}+2 z_{3}=0 \\\\\\\\\\n& x_{1}+2 y_{1}+4 z_{1}=0 \\\\quad x_{2}+2 y_{2}+4 z_{2}=0 \\\\quad x_{3}+2 y_{3}+4 z_{3}=1\\n\\\\end{aligned}\\n$$\\n\\n[Note that $A$ is the coefficient matrix for all three systems.]\\n\\nSolving the three systems for the nine unknowns yields\\n\\n$$\\nx_{1}=0, \\\\quad y_{1}=2, \\\\quad z_{1}=-1 ; \\\\quad x_{2}=-2, \\\\quad y_{2}=3, \\\\quad z_{2}=-1 ; \\\\quad x_{3}=1, \\\\quad y_{3}=-2, \\\\quad z_{3}=1\\n$$\\n\\nThus,\\n\\n$$\\nA^{-1}=\\\\left[\\\\begin{array}{rrr}\\n0 & -2 & 1 \\\\\\\\\\n2 & 3 & -2 \\\\\\\\\\n-1 & -1 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(Remark: Chapter 3 gives an efficient way to solve the three systems.)\\n',\n",
       " '\\n2.20. Let $A$ and $B$ be invertible matrices (with the same size). Show that $A B$ is also invertible and $(A B)^{-1}=B^{-1} A^{-1}$. [Thus, by induction, $\\\\left(A_{1} A_{2} \\\\ldots A_{m}\\\\right)^{-1}=A_{m}^{-1} \\\\ldots A_{2}^{-1} A_{1}^{-1}$.]\\n\\nUsing the associativity of matrix multiplication, we get\\n\\n$$\\n\\\\begin{aligned}\\n& (A B)\\\\left(B^{-1} A^{-1}\\\\right)=A\\\\left(B B^{-1}\\\\right) A^{-1}=A I A^{-1}=A A^{-1}=I \\\\\\\\\\n& \\\\left(B^{-1} A^{-1}\\\\right)(A B)=B^{-1}\\\\left(A^{-1} A\\\\right) B=A^{-1} I B=B^{-1} B=I\\n\\\\end{aligned}\\n$$\\n\\nThus, $(A B)^{-1}=B^{-1} A^{-1}$.\\n\\n\\n\\\\section*{Diagonal and Triangular Matrices}\\n',\n",
       " \"2.21. Write out the diagonal matrices $A=\\\\operatorname{diag}(4,-3,7), B=\\\\operatorname{diag}(2,-6), C=\\\\operatorname{diag}(3,-8,0,5)$.\\n\\nPut the given scalars on the diagonal and 0 's elsewhere:\\n\\n$$\\n\\\\mathrm{A}=\\\\left[\\\\begin{array}{rrr}\\n4 & 0 & 0 \\\\\\\\\\n0 & -3 & 0 \\\\\\\\\\n0 & 0 & 7\\n\\\\end{array}\\\\right], \\\\quad \\\\mathrm{B}=\\\\left[\\\\begin{array}{rr}\\n2 & 0 \\\\\\\\\\n0 & -6\\n\\\\end{array}\\\\right], \\\\quad \\\\mathrm{C}=\\\\left[\\\\begin{array}{llll}\\n3 & & & \\\\\\\\\\n& -8 & & \\\\\\\\\\n& & 0 & \\\\\\\\\\n& & & 5\\n\\\\end{array}\\\\right]\\n$$\\n\",\n",
       " '\\n2.22. Let $A=\\\\operatorname{diag}(2,3,5)$ and $B=\\\\operatorname{diag}(7,0,-4)$. Find\\\\\\\\\\n(a) $A B, A^{2}, B^{2}$;\\\\\\\\\\n(b) $f(A)$, where $f(x)=x^{2}+3 x-2$;\\\\\\\\\\n(c) $A^{-1}$ and $B^{-1}$.\\n\\n(a) The product matrix $A B$ is a diagonal matrix obtained by multiplying corresponding diagonal entries; hence,\\n\\n$$\\nA B=\\\\operatorname{diag}(2(7), 3(0), 5(-4))=\\\\operatorname{diag}(14,0,-20)\\n$$\\n\\nThus, the squares $A^{2}$ and $B^{2}$ are obtained by squaring each diagonal entry; hence,\\n\\n$$\\nA^{2}=\\\\operatorname{diag}\\\\left(2^{2}, 3^{2}, 5^{2}\\\\right)=\\\\operatorname{diag}(4,9,25) \\\\quad \\\\text { and } \\\\quad B^{2}=\\\\operatorname{diag}(49,0,16)\\n$$\\n\\n(b) $f(A)$ is a diagonal matrix obtained by evaluating $f(x)$ at each diagonal entry. We have\\n\\n$$\\nf(2)=4+6-2=8, \\\\quad f(3)=9+9-2=16, \\\\quad f(5)=25+15-2=38\\n$$\\n\\nThus, $f(A)=\\\\operatorname{diag}(8,16,38)$.\\n\\n(c) The inverse of a diagonal matrix is a diagonal matrix obtained by taking the inverse (reciprocal) of each diagonal entry. Thus, $A^{-1}=\\\\operatorname{diag}\\\\left(\\\\frac{1}{2}, \\\\frac{1}{3}, \\\\frac{1}{5}\\\\right)$, but $B$ has no inverse because there is a 0 on the diagonal.\\n',\n",
       " '\\n2.23. Find a $2 \\\\times 2$ matrix $A$ such that $A^{2}$ is diagonal but not $A$.\\n\\nLet $A=\\\\left[\\\\begin{array}{rr}1 & 2 \\\\\\\\ 3 & -1\\\\end{array}\\\\right]$. Then $A^{2}=\\\\left[\\\\begin{array}{ll}7 & 0 \\\\\\\\ 0 & 7\\\\end{array}\\\\right]$, which is diagonal.\\n',\n",
       " '\\n2.24. Find an upper triangular matrix $A$ such that $A^{3}=\\\\left[\\\\begin{array}{rr}8 & -57 \\\\\\\\ 0 & 27\\\\end{array}\\\\right]$.\\n\\nSet $A=\\\\left[\\\\begin{array}{ll}x & y \\\\\\\\ 0 & z\\\\end{array}\\\\right]$. Then $x^{3}=8$, so $x=2$; and $z^{3}=27$, so $z=3$. Next calculate $A^{3}$ using $x=2$ and $y=3$ :\\n\\n$$\\nA^{2}=\\\\left[\\\\begin{array}{ll}\\n2 & y \\\\\\\\\\n0 & 3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n2 & y \\\\\\\\\\n0 & 3\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n4 & 5 y \\\\\\\\\\n0 & 9\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad A^{3}=\\\\left[\\\\begin{array}{ll}\\n2 & y \\\\\\\\\\n0 & 3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc}\\n4 & 5 y \\\\\\\\\\n0 & 9\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n8 & 19 y \\\\\\\\\\n0 & 27\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $19 y=-57$, or $y=-3$. Accordingly, $A=\\\\left[\\\\begin{array}{rr}2 & -3 \\\\\\\\ 0 & 3\\\\end{array}\\\\right]$.\\n',\n",
       " '\\n2.25. Let $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$ be upper triangular matrices. Prove that $A B$ is upper triangular with diagonal $a_{11} b_{11}, a_{22} b_{22}, \\\\ldots, a_{n n} b_{n n}$.\\n\\nLet $A B=\\\\left[c_{i j}\\\\right]$. Then $c_{i j}=\\\\sum_{k=1}^{n} a_{i k} b_{k j}$ and $c_{i i}=\\\\sum_{k=1}^{n} a_{i k} b_{k i}$. Suppose $i>j$. Then, for any $k$, either $i>k$ or $k>j$, so that either $a_{i k}=0$ or $b_{k j}=0$. Thus, $c_{i j}=0$, and $A B$ is upper triangular. Suppose $i=j$. Then, for $k<i$, we have $a_{i k}=0$; and, for $k>i$, we have $b_{k i}=0$. Hence, $c_{i i}=a_{i i} b_{i i}$, as claimed. [This proves one part of Theorem 2.5(i); the statements for $A+B$ and $k A$ are left as exercises.]\\n\\n\\n\\\\section*{Special Real Matrices: Symmetric and Orthogonal}\\n',\n",
       " '2.26. Determine whether or not each of the following matrices is symmetric-that is, $A^{T}=A$-or skew-symmetric-that is, $A^{T}=-A$ :\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rrr}5 & -7 & 1 \\\\\\\\ -7 & 8 & 2 \\\\\\\\ 1 & 2 & -4\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrr}0 & 4 & -3 \\\\\\\\ -4 & 0 & 5 \\\\\\\\ 3 & -5 & 0\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{lll}0 & 0 & 0 \\\\\\\\ 0 & 0 & 0\\\\end{array}\\\\right]$\\n\\n(a) By inspection, the symmetric elements (mirror images in the diagonal) are -7 and $-7,1$ and 1,2 and 2 . Thus, $A$ is symmetric, because symmetric elements are equal.\\n\\n(b) By inspection, the diagonal elements are all 0 , and the symmetric elements, 4 and $-4,-3$ and 3 , and 5 and -5 , are negatives of each other. Hence, $B$ is skew-symmetric.\\n\\n(c) Because $C$ is not square, $C$ is neither symmetric nor skew-symmetric.\\n',\n",
       " '\\n2.27. Suppose $B=\\\\left[\\\\begin{array}{cc}4 & x+2 \\\\\\\\ 2 x-3 & x+1\\\\end{array}\\\\right]$ is symmetric. Find $x$ and $B$.\\n\\nSet the symmetric elements $x+2$ and $2 x-3$ equal to each other, obtaining $2 x-3=x+2$ or $x=5$. Hence, $B=\\\\left[\\\\begin{array}{ll}4 & 7 \\\\\\\\ 7 & 6\\\\end{array}\\\\right]$.\\n',\n",
       " '\\n2.28. Let $A$ be an arbitrary $2 \\\\times 2$ (real) orthogonal matrix.\\n\\n(a) Prove: If $(a, b)$ is the first row of $A$, then $a^{2}+b^{2}=1$ and\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\na & b \\\\\\\\\\n-b & a\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\na & b \\\\\\\\\\nb & -a\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Prove Theorem 2.7: For some real number $\\\\theta$,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n\\\\cos \\\\theta & \\\\sin \\\\theta \\\\\\\\\\n-\\\\sin \\\\theta & \\\\cos \\\\theta\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\n\\\\cos \\\\theta & \\\\sin \\\\theta \\\\\\\\\\n\\\\sin \\\\theta & -\\\\cos \\\\theta\\n\\\\end{array}\\\\right]\\n$$\\n\\n(a) Suppose $(x, y)$ is the second row of $A$. Because the rows of $A$ form an orthonormal set, we get\\n\\n$$\\na^{2}+b^{2}=1, \\\\quad x^{2}+y^{2}=1, \\\\quad a x+b y=0\\n$$\\n\\nSimilarly, the columns form an orthogonal set, so\\n\\n$$\\na^{2}+x^{2}=1, \\\\quad b^{2}+y^{2}=1, \\\\quad a b+x y=0\\n$$\\n\\nTherefore, $x^{2}=1-a^{2}=b^{2}$, whence $x= \\\\pm b$.\\n\\nCase (i): $x=b$. Then $b(a+y)=0$, so $y=-a$.\\n\\nCase (ii): $x=-b$. Then $b(y-a)=0$, so $y=a$.\\n\\nThis means, as claimed,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\na & b \\\\\\\\\\n-b & a\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\na & b \\\\\\\\\\nb & -a\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Because $a^{2}+b^{2}=1$, we have $-1 \\\\leq a \\\\leq 1$. Let $a=\\\\cos \\\\theta$. Then $b^{2}=1-\\\\cos ^{2} \\\\theta$, so $b=\\\\sin \\\\theta$. This proves the theorem.\\n',\n",
       " '\\n2.29. Find a $2 \\\\times 2$ orthogonal matrix $A$ whose first row is a (positive) multiple of $(3,4)$.\\n\\nNormalize $(3,4)$ to get $\\\\left(\\\\frac{3}{5}, \\\\frac{4}{5}\\\\right)$. Then, by Problem 2.28 ,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n\\\\frac{3}{5} & \\\\frac{4}{5} \\\\\\\\\\n-\\\\frac{4}{5} & \\\\frac{3}{5}\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\n\\\\frac{3}{5} & \\\\frac{4}{5} \\\\\\\\\\n\\\\frac{4}{5} & -\\\\frac{3}{5}\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n2.30. Find a $3 \\\\times 3$ orthogonal matrix $P$ whose first two rows are multiples of $u_{1}=(1,1,1)$ and $u_{2}=(0,-1,1)$, respectively. (Note that, as required, $u_{1}$ and $u_{2}$ are orthogonal.)\\n\\nFirst find a nonzero vector $u_{3}$ orthogonal to $u_{1}$ and $u_{2}$; say (cross product) $u_{3}=u_{1} \\\\times u_{2}=(2,-1,-1)$. Let $A$ be the matrix whose rows are $u_{1}, u_{2}, u_{3}$; and let $P$ be the matrix obtained from $A$ by normalizing the rows of $A$. Thus,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n0 & -1 & 1 \\\\\\\\\\n2 & -1 & -1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad P=\\\\left[\\\\begin{array}{ccc}\\n1 / \\\\sqrt{3} & 1 / \\\\sqrt{3} & 1 / \\\\sqrt{3} \\\\\\\\\\n0 & -1 / \\\\sqrt{2} & 1 / \\\\sqrt{2} \\\\\\\\\\n2 / \\\\sqrt{6} & -1 / \\\\sqrt{6} & -1 / \\\\sqrt{6}\\n\\\\end{array}\\\\right]\\n$$\\n\\n\\n\\\\section*{Complex Matrices: Hermitian and Unitary Matrices}\\n',\n",
       " '2.31. Find $A^{H}$ where (a) $A=\\\\left[\\\\begin{array}{ll}3-5 i & 2+4 i \\\\\\\\ 6+7 i & 1+8 i\\\\end{array}\\\\right], \\\\quad$ (b) $\\\\quad A=\\\\left[\\\\begin{array}{cc}2-3 i & 5+8 i \\\\\\\\ -4 & 3-7 i \\\\\\\\ -6-i & 5 i\\\\end{array}\\\\right]$\\n\\nRecall that $A^{H}=\\\\bar{A}^{T}$, the conjugate tranpose of $A$. Thus,\\\\\\\\\\n(a) $A^{H}=\\\\left[\\\\begin{array}{ll}3+5 i & 6-7 i \\\\\\\\ 2-4 i & 1-8 i\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $A^{H}=\\\\left[\\\\begin{array}{ccc}2+3 i & -4 & -6+i \\\\\\\\ 5-8 i & 3+7 i & -5 i\\\\end{array}\\\\right]$\\n',\n",
       " '\\n2.32. Show that $A=\\\\left[\\\\begin{array}{cc}\\\\frac{1}{3}-\\\\frac{2}{3} i & \\\\frac{2}{3} i \\\\\\\\ -\\\\frac{2}{3} i & -\\\\frac{1}{3}-\\\\frac{2}{3} i\\\\end{array}\\\\right]$ is unitary.\\n\\nThe rows of $A$ form an orthonormal set:\\n\\n$$\\n\\\\begin{gathered}\\n\\\\left(\\\\frac{1}{3}-\\\\frac{2}{3} i, \\\\frac{2}{3} i\\\\right) \\\\cdot\\\\left(\\\\frac{1}{3}-\\\\frac{2}{3} i, \\\\frac{2}{3} i\\\\right)=\\\\left(\\\\frac{1}{9}+\\\\frac{4}{9}\\\\right)+\\\\frac{4}{9}=1 \\\\\\\\\\n\\\\left(\\\\frac{1}{3}-\\\\frac{2}{3} i, \\\\frac{2}{3} i\\\\right) \\\\cdot\\\\left(-\\\\frac{2}{3} i,-\\\\frac{1}{3}-\\\\frac{2}{3} i\\\\right)=\\\\left(\\\\frac{2}{9} i+\\\\frac{4}{9}\\\\right)+\\\\left(-\\\\frac{2}{9} i-\\\\frac{4}{9}\\\\right)=0 \\\\\\\\\\n\\\\left(-\\\\frac{2}{3} i,-\\\\frac{1}{3}-\\\\frac{2}{3} i\\\\right) \\\\cdot\\\\left(-\\\\frac{2}{3} i,-\\\\frac{1}{3}-\\\\frac{2}{3} i\\\\right)=\\\\frac{4}{9}+\\\\left(\\\\frac{1}{9}+\\\\frac{4}{9}\\\\right)=1\\n\\\\end{gathered}\\n$$\\n\\nThus, $A$ is unitary.\\n',\n",
       " '\\n2.33. Prove the complex analogue of Theorem 2.6: Let $A$ be a complex matrix. Then the following are equivalent: (i) $A$ is unitary. (ii) The rows of $A$ form an orthonormal set. (iii) The columns of $A$ form an orthonormal set.\\n\\n(The proof is almost identical to the proof on page 37 for the case when $A$ is a $3 \\\\times 3$ real matrix.)\\n\\nFirst recall that the vectors $u_{1}, u_{2}, \\\\ldots, u_{n}$ in $\\\\mathbf{C}^{n}$ form an orthonormal set if they are unit vectors and are orthogonal to each other, where the dot product in $\\\\mathbf{C}^{n}$ is defined by\\n\\n$$\\n\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right) \\\\cdot\\\\left(b_{1}, b_{2}, \\\\ldots, b_{n}\\\\right)=a_{1} \\\\bar{b}_{1}+a_{2} \\\\bar{b}_{2}+\\\\cdots+a_{n} \\\\bar{b}_{n}\\n$$\\n\\nSuppose $A$ is unitary, and $R_{1}, R_{2}, \\\\ldots, R_{n}$ are its rows. Then $\\\\bar{R}_{1}^{T}, \\\\bar{R}_{2}^{T}, \\\\ldots, \\\\bar{R}_{n}^{T}$ are the columns of $A^{H}$. Let $A A^{H}=\\\\left[c_{i j}\\\\right]$. By matrix multiplication, $c_{i j}=R_{i} \\\\bar{R}_{j}^{T}=R_{i} \\\\cdot R_{j}$. Because $A$ is unitary, we have $A A^{H}=I$. Multiplying $A$ by $A^{H}$ and setting each entry $c_{i j}$ equal to the corresponding entry in $I$ yields the following $n^{2}$ equations:\\n\\n$$\\nR_{1} \\\\cdot R_{1}=1, \\\\quad R_{2} \\\\cdot R_{2}=1, \\\\quad \\\\ldots, \\\\quad R_{n} \\\\cdot R_{n}=1, \\\\quad \\\\text { and } \\\\quad R_{i} \\\\cdot R_{j}=0, \\\\quad \\\\text { for } i \\\\neq j\\n$$\\n\\nThus, the rows of $A$ are unit vectors and are orthogonal to each other; hence, they form an orthonormal set of vectors. The condition $A^{T} A=I$ similarly shows that the columns of $A$ also form an orthonormal set of vectors. Furthermore, because each step is reversible, the converse is true. This proves the theorem.\\n\\n\\n\\\\section*{Block Matrices}\\n',\n",
       " '2.34. Consider the following block matrices (which are partitions of the same matrix):\\\\\\\\\\n(a) $\\\\left[\\\\begin{array}{rr:rr:r}1 & -2 & 0 & 1 & 3 \\\\\\\\ 2 & -3 & 5 & 7 & -2 \\\\\\\\ 3 & 1 & 4 & 5 & 9\\\\end{array}\\\\right]$,\\\\\\\\\\n(b)\\\\\\\\\\n$\\\\left[\\\\begin{array}{ll:ll}1 & -2 & 0 & \\\\frac{1}{7}--\\\\frac{3}{2} \\\\\\\\ \\\\hdashline 2 & 3 & \\\\frac{5}{5}-\\\\frac{7}{2}-\\\\frac{2}{2} \\\\\\\\ \\\\hdashline 3 & 1 & 4 & 5\\\\end{array}\\\\right]$\\n\\nFind the size of each block matrix and also the size of each block.\\n\\n(a) The block matrix has two rows of matrices and three columns of matrices; hence, its size is $2 \\\\times 3$. The block sizes are $2 \\\\times 2,2 \\\\times 2$, and $2 \\\\times 1$ for the first row; and $1 \\\\times 2,1 \\\\times 2$, and $1 \\\\times 1$ for the second row.\\n\\n(b) The size of the block matrix is $3 \\\\times 2$; and the block sizes are $1 \\\\times 3$ and $1 \\\\times 2$ for each of the three rows.\\n',\n",
       " '\\n2.35. Compute $A B$ using block multiplication, where\\n\\n$$\\nA=\\\\left[\\\\begin{array}{cc:c}\\n1 & 2 & 1 \\\\\\\\\\n3 & 4 & 0 \\\\\\\\\\n\\\\hdashline 0 & 0 & 2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad B=\\\\left[\\\\begin{array}{ccc:c}\\n1 & 2 & 3 & 1 \\\\\\\\\\n4 & 5 & 6 & 1 \\\\\\\\\\n\\\\hdashline 0 & 0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nHere $A=\\\\left[\\\\begin{array}{cc}E & F \\\\\\\\ 0_{1 \\\\times 2} & G\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{cc}R & S \\\\\\\\ 0_{1 \\\\times 3} & T\\\\end{array}\\\\right]$, where $E, F, G, R, S, T$ are the given blocks, and $0_{1 \\\\times 2}$ and $0_{1 \\\\times 3}$ are zero matrices of the indicated sites. Hence,\\n\\n$$\\nA B=\\\\left[\\\\begin{array}{cc}\\nE R & E S+F T \\\\\\\\\\n0_{1 \\\\times 3} & G T\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\n{\\\\left[\\\\begin{array}{rrr}\\n9 & 12 & 15 \\\\\\\\\\n19 & 26 & 33\\n\\\\end{array}\\\\right]} & {\\\\left[\\\\begin{array}{l}\\n3 \\\\\\\\\\n7\\n\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n0\\n\\\\end{array}\\\\right]}\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrrr}\\n9 & 12 & 15 & 4 \\\\\\\\\\n19 & 26 & 33 & 7 \\\\\\\\\\n0 & 0 & 0 & 2\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n2.36. Let $M=\\\\operatorname{diag}(A, B, C)$, where $A=\\\\left[\\\\begin{array}{ll}1 & 2 \\\\\\\\ 3 & 4\\\\end{array}\\\\right], B=[5], C=\\\\left[\\\\begin{array}{ll}1 & 3 \\\\\\\\ 5 & 7\\\\end{array}\\\\right]$. Find $M^{2}$.\\n\\nBecause $M$ is block diagonal, square each block:\\n\\nso\\n\\n$$\\nA^{2}=\\\\left[\\\\begin{array}{rr}\\n7 & 10 \\\\\\\\\\n15 & 22\\n\\\\end{array}\\\\right], \\\\quad B^{2}=[25], \\\\quad C^{2}=\\\\left[\\\\begin{array}{ll}\\n16 & 24 \\\\\\\\\\n40 & 64\\n\\\\end{array}\\\\right]\\n$$\\n\\n$$\\nM^{2}=\\\\left[\\\\begin{array}{rr:rrr}\\n7 & 10 & & & \\\\\\\\\\n15 & 22 & & & \\\\\\\\\\n\\\\hdashline & & 25 & & \\\\\\\\\\n& & & 16 & 2 \\\\\\\\\\n& & & 40 & 64\\n\\\\end{array}\\\\right]\\n$$\\n\\n\\n\\\\section*{Miscellaneous Problem}\\n',\n",
       " '2.37. Let $f(x)$ and $g(x)$ be polynomials and let $A$ be a square matrix. Prove\\n\\n(a) $(f+g)(A)=f(A)+g(A)$,\\n\\n(b) $(f \\\\cdot g)(A)=f(A) g(A)$,\\n\\n(c) $f(A) g(A)=g(A) f(A)$.\\n\\nSuppose $f(x)=\\\\sum_{i=1}^{r} a_{i} x^{i}$ and $g(x)=\\\\sum_{j=1}^{s} b_{j} x^{j}$.\\n\\n(a) We can assume $r=s=n$ by adding powers of $x$ with 0 as their coefficients. Then\\n\\nHence,\\n\\n$$\\n\\\\begin{gathered}\\nf(x)+g(x)=\\\\sum_{i=1}^{n}\\\\left(a_{i}+b_{i}\\\\right) x^{i} \\\\\\\\\\n(f+g)(A)=\\\\sum_{i=1}^{n}\\\\left(a_{i}+b_{i}\\\\right) A^{i}=\\\\sum_{i=1}^{n} a_{i} A^{i}+\\\\sum_{i=1}^{n} b_{i} A^{i}=f(A)+g(A)\\n\\\\end{gathered}\\n$$\\n\\n(b) We have $f(x) g(x)=\\\\sum_{i, j} a_{i} b_{j} x^{i+j}$. Then\\n\\n$$\\nf(A) g(A)=\\\\left(\\\\sum_{i} a_{i} A^{i}\\\\right)\\\\left(\\\\sum_{j} b_{j} A^{j}\\\\right)=\\\\sum_{i, j} a_{i} b_{j} A^{i+j}=(f g)(A)\\n$$\\n\\n(c) Using $f(x) g(x)=g(x) f(x)$, we have\\n\\n$$\\nf(A) g(A)=(f g)(A)=(g f)(A)=g(A) f(A)\\n$$\\n\\n',\n",
       " '3.1. Determine whether each of the following equations is linear:\\\\\\\\\\n(a) $5 x+7 y-8 y z=16$\\\\\\\\\\n(b) $x+\\\\pi y+e z=\\\\log 5$,\\\\\\\\\\n(c) $3 x+k y-8 z=16$\\n\\n(a) No, because the product $y z$ of two unknowns is of second degree.\\n\\n(b) Yes, because $\\\\pi, e$, and $\\\\log 5$ are constants.\\n\\n(c) As it stands, there are four unknowns: $x, y, z, k$. Because of the term $k y$ it is not a linear equation. However, assuming $k$ is a constant, the equation is linear in the unknowns $x, y, z$.\\n',\n",
       " '\\n3.2. Determine whether the following vectors are solutions of $x_{1}+2 x_{2}-4 x_{3}+3 x_{4}=15$ :\\n\\n(a) $u=(3,2,1,4)$ and (b) $v=(1,2,4,5)$.\\n\\n(a) Substitute to obtain $3+2(2)-4(1)+3(4)=15$, or $15=15$; yes, it is a solution.\\n\\n(b) Substitute to obtain $1+2(2)-4(4)+3(5)=15$, or $4=15$; no, it is not a solution.\\\\\\\\\\n',\n",
       " '3.3. Solve\\\\\\\\\\n(a) $e x=\\\\pi$\\\\\\\\\\n(b) $3 x-4-x=2 x+3$,\\\\\\\\\\n(c) $7+2 x-4=3 x+3-x$\\n\\n(a) Because $e \\\\neq 0$, multiply by $1 / e$ to obtain $x=\\\\pi / e$.\\n\\n(b) Rewrite in standard form, obtaining $0 x=7$. The equation has no solution.\\n\\n(c) Rewrite in standard form, obtaining $0 x=0$. Every scalar $k$ is a solution.\\n',\n",
       " '\\n3.4. Prove Theorem 3.4: Consider the equation $a x=b$.\\n\\n(i) If $a \\\\neq 0$, then $x=b / a$ is a unique solution of $a x=b$.\\n\\n(ii) If $a=0$ but $b \\\\neq 0$, then $a x=b$ has no solution.\\n\\n(iii) If $a=0$ and $b=0$, then every scalar $k$ is a solution of $a x=b$.\\n\\nSuppose $a \\\\neq 0$. Then the scalar $b / a$ exists. Substituting $b / a$ in $a x=b$ yields $a(b / a)=b$, or $b=b$; hence, $b / a$ is a solution. On the other hand, suppose $x_{0}$ is a solution to $a x=b$, so that $a x_{0}=b$. Multiplying both sides by $1 / a$ yields $x_{0}=b / a$. Hence, $b / a$ is the unique solution of $a x=b$. Thus, (i) is proved.\\n\\nOn the other hand, suppose $a=0$. Then, for any scalar $k$, we have $a k=0 k=0$. If $b \\\\neq 0$, then $a k \\\\neq b$. Accordingly, $k$ is not a solution of $a x=b$, and so (ii) is proved. If $b=0$, then $a k=b$. That is, any scalar $k$ is a solution of $a x=b$, and so (iii) is proved.\\n',\n",
       " '\\n3.5. Solve each of the following systems:\\\\\\\\\\n(a) $\\\\quad \\\\begin{aligned} & 2 x-5 y=11 \\\\\\\\ & 3 x+4 y=5\\\\end{aligned}$\\\\\\\\\\n(b)\\\\\\\\\\n$\\\\begin{aligned} 2 x-3 y & =8 \\\\\\\\ -6 x+9 y & =6\\\\end{aligned}$\\\\\\\\\\n(c)\\\\\\\\\\n$\\\\begin{aligned} 2 x-3 y & =8 \\\\\\\\ -4 x+6 y & =-16\\\\end{aligned}$\\n\\n(a) Eliminate $x$ from the equations by forming the new equation $L=-3 L_{1}+2 L_{2}$. This yields the equation\\n\\n$$\\n23 y=-23, \\\\quad \\\\text { and so } \\\\quad y=-1\\n$$\\n\\nSubstitute $y=-1$ in one of the original equations, say $L_{1}$, to get\\n\\n$$\\n2 x-5(-1)=11 \\\\quad \\\\text { or } \\\\quad 2 x+5=11 \\\\quad \\\\text { or } \\\\quad 2 x=6 \\\\quad \\\\text { or } \\\\quad x=3\\n$$\\n\\nThus, $x=3, y=-1$ or the pair $u=(3,-1)$ is the unique solution of the system.\\n\\n(b) Eliminate $x$ from the equations by forming the new equation $L=3 L_{1}+L_{2}$. This yields the equation\\n\\n$$\\n0 x+0 y=30\\n$$\\n\\nThis is a degenerate equation with a nonzero constant; hence, this equation and the system have no solution. (Geometrically, the lines corresponding to the equations are parallel.)\\n\\n(c) Eliminate $x$ from the equations by forming the new equation $L=2 L_{1}+L_{2}$. This yields the equation\\n\\n$$\\n0 x+0 y=0\\n$$\\n\\nThis is a degenerate equation where the constant term is also zero. Thus, the system has an infinite number of solutions, which correspond to the solution of either equation. (Geometrically, the lines corresponding to the equations coincide.)\\n\\nTo find the general solution, set $y=a$ and substitute in $L_{1}$ to obtain\\n\\n$$\\n2 x-3 a=8 \\\\quad \\\\text { or } \\\\quad 2 x=3 a+8 \\\\quad \\\\text { or } \\\\quad x=\\\\frac{3}{2} a+4\\n$$\\n\\nThus, the general solution is\\n\\n$$\\nx=\\\\frac{3}{2} a+4, \\\\quad y=a \\\\quad \\\\text { or } \\\\quad u=\\\\left(\\\\frac{3}{2} a+4, a\\\\right)\\n$$\\n\\nwhere $a$ is any scalar.\\n',\n",
       " '\\n3.6. Consider the system\\n\\n$$\\n\\\\begin{array}{r}\\nx+a y=4 \\\\\\\\\\na x+9 y=b\\n\\\\end{array}\\n$$\\n\\n(a) For which values of $a$ does the system have a unique solution?\\n\\n(b) Find those pairs of values $(a, b)$ for which the system has more than one solution.\\n\\n(a) Eliminate $x$ from the equations by forming the new equation $L=-a L_{1}+L_{2}$. This yields the equation\\n\\n\\n\\\\begin{equation*}\\n\\\\left(9-a^{2}\\\\right) y=b-4 a \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nThe system has a unique solution if and only if the coefficient of $y$ in (1) is not zero-that is, if $9-a^{2} \\\\neq 0$ or if $a \\\\neq \\\\pm 3$.\\n\\n(b) The system has more than one solution if both sides of (1) are zero. The left-hand side is zero when $a= \\\\pm 3$. When $a=3$, the right-hand side is zero when $b-12=0$ or $b=12$. When $a=-3$, the righthand side is zero when $b+12-0$ or $b=-12$. Thus, $(3,12)$ and $(-3,-12)$ are the pairs for which the system has more than one solution.\\n\\n\\n\\\\section*{Systems in Triangular and Echelon Form}\\n',\n",
       " '3.7. Determine the pivot and free variables in each of the following systems:\\n\\n$$\\n\\\\begin{array}{r}\\n2 x_{1}-3 x_{2}-6 x_{3}-5 x_{4}+2 x_{5}=7 \\\\\\\\\\nx_{3}+3 x_{4}-7 x_{5}=6 \\\\\\\\\\nx_{4}-2 x_{5}=1\\n\\\\end{array}\\n$$\\n\\n(a)\\n\\n$$\\n\\\\begin{array}{r}\\n2 x-6 y+7 z=1 \\\\\\\\\\n4 y+3 z=8 \\\\\\\\\\n2 z=4\\n\\\\end{array}\\n$$\\n\\n(b)\\n\\n$$\\n\\\\begin{array}{r}\\nx+2 y-3 z=2 \\\\\\\\\\n2 x+3 y+z=4 \\\\\\\\\\n3 x+4 y+5 z=8\\n\\\\end{array}\\n$$\\n\\n(c)\\n\\n(a) In echelon form, the leading unknowns are the pivot variables, and the others are the free variables. Here $x_{1}, x_{3}, x_{4}$ are the pivot variables, and $x_{2}$ and $x_{5}$ are the free variables.\\\\\\\\\\n(b) The leading unknowns are $x, y, z$, so they are the pivot variables. There are no free variables (as in any triangular system).\\n\\n(c) The notion of pivot and free variables applies only to a system in echelon form.\\n',\n",
       " '\\n3.8. Solve the triangular system in Problem 3.7(b).\\n\\nBecause it is a triangular system, solve by back-substitution.\\n\\n(i) The last equation gives $z=2$.\\n\\n(ii) Substitute $z=2$ in the second equation to get $4 y+6=8$ or $y=\\\\frac{1}{2}$.\\n\\n(iii) Substitute $z=2$ and $y=\\\\frac{1}{2}$ in the first equation to get\\n\\n$$\\n2 x-6\\\\left(\\\\frac{1}{2}\\\\right)+7(2)=1 \\\\quad \\\\text { or } \\\\quad 2 x+11=1 \\\\quad \\\\text { or } \\\\quad x=-5\\n$$\\n\\nThus, $\\\\quad x=-5, \\\\quad y=\\\\frac{1}{2}, \\\\quad z=2 \\\\quad$ or $u=\\\\left(-5, \\\\frac{1}{2}, 2\\\\right) \\\\quad$ is the unique solution to the system.\\n',\n",
       " '\\n3.9. Solve the echelon system in Problem 3.7(a).\\n\\nAssign parameters to the free variables, say $x_{2}=a$ and $x_{5}=b$, and solve for the pivot variables by backsubstitution.\\n\\n(i) Substitute $x_{5}=b$ in the last equation to get $x_{4}-2 b=1$ or $x_{4}=2 b+1$.\\n\\n(ii) Substitute $x_{5}=b$ and $x_{4}=2 b+1$ in the second equation to get\\n\\n$$\\nx_{3}+3(2 b+1)-7 b=6 \\\\quad \\\\text { or } \\\\quad x_{3}-b+3=6 \\\\quad \\\\text { or } \\\\quad x_{3}=b+3\\n$$\\n\\n(iii) Substitute $x_{5}=b, x_{4}=2 b+1, x_{3}=b+3, x_{2}=a$ in the first equation to get\\n\\n$$\\n\\\\begin{gathered}\\n2 x_{1}-3 a-6(b+3)-5(2 b+1)+2 b=7 \\\\quad \\\\text { or } \\\\quad 2 x_{1}-3 a-14 b-23=7 \\\\\\\\\\n\\\\text { or } \\\\quad x_{1}=\\\\frac{3}{2} a+7 b+15\\n\\\\end{gathered}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\begin{gathered}\\nx_{1}=\\\\frac{3}{2} a+7 b+15, \\\\quad x_{2}=a, \\\\quad x_{3}=b+3, \\\\quad x_{4}=2 b+1, \\\\quad x_{5}=b \\\\\\\\\\n\\\\text { or } \\\\quad u=\\\\left(\\\\frac{3}{2} a+7 b+15, \\\\quad a, \\\\quad b+3, \\\\quad 2 b+1, \\\\quad b\\\\right)\\n\\\\end{gathered}\\n$$\\n\\nis the parametric form of the general solution.\\n\\nAlternatively, solving for the pivot variable $x_{1}, x_{3}, x_{4}$ in terms of the free variables $x_{2}$ and $x_{5}$ yields the following free-variable form of the general solution:\\n\\n$$\\nx_{1}=\\\\frac{3}{2} x_{2}+7 x_{5}+15, \\\\quad x_{3}=x_{5}+3, \\\\quad x_{4}=2 x_{5}+1\\n$$\\n',\n",
       " '\\n3.10. Prove Theorem 3.6. Consider the system (3.4) of linear equations in echelon form with $r$ equations and $n$ unknowns.\\n\\n(i) If $r=n$, then the system has a unique solution.\\n\\n(ii) If $r<n$, then we can arbitrarily assign values to the $n-r$ free variable and solve uniquely for the $r$ pivot variables, obtaining a solution of the system.\\n\\n(i) Suppose $r=n$. Then we have a square system $A X=B$ where the matrix $A$ of coefficients is (upper) triangular with nonzero diagonal elements. Thus, $A$ is invertible. By Theorem 3.10, the system has a unique solution.\\n\\n(ii) Assigning values to the $n-r$ free variables yields a triangular system in the pivot variables, which, by (i), has a unique solution.\\n\\n\\n\\\\section*{Gaussian Elimination}\\n',\n",
       " '3.11. Solve each of the following systems:\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y-4 z & =-4 \\\\\\\\\\n2 x+5 y-9 z & =-10 \\\\\\\\\\n3 x-2 y+3 z & =11\\n\\\\end{aligned}\\n$$\\n\\n(a)\\n\\n$$\\n\\\\begin{array}{rr}\\nx+2 y-3 z= & -1 \\\\\\\\\\n-3 x+y-2 z= & -7 \\\\\\\\\\n5 x+3 y-4 z= & 2\\n\\\\end{array}\\n$$\\n\\n(b)\\n\\n$$\\n\\\\begin{array}{r}\\nx+2 y-3 z=1 \\\\\\\\\\n2 x+5 y-8 z=4 \\\\\\\\\\n3 x+8 y-13 z=7\\n\\\\end{array}\\n$$\\n\\n(c)\\n\\nReduce each system to triangular or echelon form using Gaussian elimination:\\n\\n(a) Apply \"Replace $L_{2}$ by $-2 L_{1}+L_{2}$ \" and \"Replace $L_{3}$ by $-3 L_{1}+L_{3}$ \" to eliminate $x$ from the second and third equations, and then apply \"Replace $L_{3}$ by $8 L_{2}+L_{3}$ \" to eliminate $y$ from the third equation. These operations yield\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y-4 z=-4 \\\\quad x+2 y-4 z=-4 \\\\\\\\\\n& y-z=-2 \\\\quad \\\\text { and then } \\\\quad y-z=-2 \\\\\\\\\\n& -8 y+15 z=23 \\\\quad 7 z=7\\n\\\\end{aligned}\\n$$\\n\\nThe system is in triangular form. Solve by back-substitution to obtain the unique solution $u=(2,-1,1)$.\\n\\n(b) Eliminate $x$ from the second and third equations by the operations \"Replace $L_{2}$ by $3 L_{1}+L_{2}$ \" and \"Replace $L_{3}$ by $-5 L_{1}+L_{3}$. \" This gives the equivalent system\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y-3 z & =-1 \\\\\\\\\\n7 y-11 z & =-10 \\\\\\\\\\n-7 y+11 z & =7\\n\\\\end{aligned}\\n$$\\n\\nThe operation \"Replace $L_{3}$ by $L_{2}+L_{3}$ \" yields the following degenerate equation with a nonzero constant:\\n\\n$$\\n0 x+0 y+0 z=-3\\n$$\\n\\nThis equation and hence the system have no solution.\\n\\n(c) Eliminate $x$ from the second and third equations by the operations \"Replace $L_{2}$ by $-2 L_{1}+L_{2}$ \" and \"Replace $L_{3}$ by $-3 L_{1}+L_{3}$.\" This yields the new system\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y-3 z=1 \\\\\\\\\\n& y-2 z=2 \\\\quad \\\\text { or } \\\\quad x+2 y-3 z=1 \\\\\\\\\\n& 2 y-4 z=4 \\\\\\\\\\n& y-2 z=2\\n\\\\end{aligned}\\n$$\\n\\n(The third equation is deleted, because it is a multiple of the second equation.) The system is in echelon form with pivot variables $x$ and $y$ and free variable $z$.\\n\\nTo find the parametric form of the general solution, set $z=a$ and solve for $x$ and $y$ by backsubstitution. Substitute $z=a$ in the second equation to get $y=2+2 a$. Then substitute $z=a$ and $y=2+2 a$ in the first equation to get\\n\\n$$\\nx+2(2+2 a)-3 a=1 \\\\quad \\\\text { or } \\\\quad x+4+a=1 \\\\quad \\\\text { or } \\\\quad x=-3-a\\n$$\\n\\nThus, the general solution is\\n\\n$$\\nx=-3-a, \\\\quad y=2+2 a, \\\\quad z=a \\\\quad \\\\text { or } \\\\quad u=(-3-a, \\\\quad 2+2 a, \\\\quad a)\\n$$\\n\\nwhere $a$ is a parameter.\\n',\n",
       " '\\n3.12. Solve each of the following systems:\\n\\n\\\\[\\n\\\\begin{array}{rlrl}\\nx_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5} & =2 & x_{1}+2 x_{2}-3 x_{3}+4 x_{4} & =2 \\\\\\\\\\n3 x_{1}-9 x_{2}+7 x_{3}-x_{4}+3 x_{5} & =7 & 2 x_{1}+5 x_{2}-2 x_{3}+x_{4} & =1 \\\\\\\\\\n2 x_{1}-6 x_{2}+7 x_{3}+4 x_{4}-5 x_{5} & =7 & 5 x_{1}+12 x_{2}-7 x_{3}+6 x_{4} & =3 \\\\tag{a}\\n\\\\end{array}\\n\\\\]\\n\\nReduce each system to echelon form using Gaussian elimination:\\\\\\\\\\n(a) Apply \"Replace $L_{2}$ by $-3 L_{1}+L_{2}$ \"\\' and \"Replace $L_{3}$ by $-2 L_{1}+L_{3}$ \" to eliminate $x$ from the second and third equations. This yields\\n\\n$$\\n\\\\begin{aligned}\\n& x_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5}=2 \\\\\\\\\\n& x_{3}+2 x_{4}-3 x_{5}=1 \\\\\\\\\\n& 3 x_{3}+6 x_{4}-9 x_{5}=3 \\\\\\\\\\n& x_{1}-3 x_{2}+2 x_{3}-x_{4}+2 x_{5}=2 \\\\\\\\\\n& x_{3}+2 x_{4}-3 x_{5}=1\\n\\\\end{aligned}\\n$$\\n\\n(We delete $L_{3}$, because it is a multiple of $L_{2}$.) The system is in echelon form with pivot variables $x_{1}$ and $x_{3}$ and free variables $x_{2}, x_{4}, x_{5}$.\\n\\nTo find the parametric form of the general solution, set $x_{2}=a, x_{4}=b, x_{5}=c$, where $a, b, c$ are parameters. Back-substitution yields $x_{3}=1-2 b+3 c$ and $x_{1}=3 a+5 b-8 c$. The general solution is\\n\\n$$\\nx_{1}=3 a+5 b-8 c, x_{2}=a, x_{3}=1-2 b+3 c, x_{4}=b, x_{5}=c\\n$$\\n\\nor, equivalently, $u=(3 a+5 b-8 c, a, 1-2 b+3 c, b, c)$.\\n\\n(b) Eliminate $x_{1}$ from the second and third equations by the operations \"Replace $L_{2}$ by $-2 L_{1}+L_{2}$ \" and \"Replace $L_{3}$ by $-5 L_{1}+L_{3}$.\" This yields the system\\n\\n$$\\n\\\\begin{aligned}\\nx_{1}+2 x_{2}-3 x_{3}+4 x_{4} & =2 \\\\\\\\\\nx_{2}+4 x_{3}-7 x_{4} & =-3 \\\\\\\\\\n2 x_{2}+8 x_{3}-14 x_{4} & =-7\\n\\\\end{aligned}\\n$$\\n\\nThe operation \"Replace $L_{3}$ by $-2 L_{2}+L_{3}$ \", yields the degenerate equation $0=-1$. Thus, the system has no solution (even though the system has more unknowns than equations).\\n',\n",
       " '\\n3.13. Solve using the condensed format:\\n\\n$$\\n\\\\begin{aligned}\\n2 y+3 z= & 3 \\\\\\\\\\nx+y+z= & 4 \\\\\\\\\\n4 x+8 y-3 z= & 35\\n\\\\end{aligned}\\n$$\\n\\nThe condensed format follows:\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\text { Number } \\\\\\\\\\n& \\\\text { Equation } \\\\\\\\\\n& \\\\text { Operation } \\\\\\\\\\n& -13 z=13\\n\\\\end{aligned}\\n$$\\n\\nHere (1), (2), and ( $3^{\\\\prime \\\\prime}$ ) form a triangular system. (We emphasize that the interchange of $L_{1}$ and $L_{2}$ is accomplished by simply renumbering $L_{1}$ and $L_{2}$ as above.)\\n\\nUsing back-substitution with the triangular system yields $z=-1$ from $L_{3}, y=3$ from $L_{2}$, and $x=2$ from $L_{1}$. Thus, the unique solution of the system is $x=2, y=3, z=-1$ or the triple $u=(2,3,-1)$.\\n',\n",
       " '\\n3.14. Consider the system\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y+z & =3 \\\\\\\\\\na y+5 z= & 10 \\\\\\\\\\n2 x+7 y+a z= & b\\n\\\\end{aligned}\\n$$\\n\\n(a) Find those values of $a$ for which the system has a unique solution.\\n\\n(b) Find those pairs of values $(a, b)$ for which the system has more than one solution.\\n\\nReduce the system to echelon form. That is, eliminate $x$ from the third equation by the operation \"Replace $L_{3}$ by $-2 L_{1}+L_{3}$ \" and then eliminate $y$ from the third equation by the operation\\n\\n\"\\'Replace $L_{3}$ by $-3 L_{2}+a L_{3}$.\" This yields\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y \\\\quad+z=3 \\\\quad x+2 y+z=3 \\\\\\\\\\n& \\\\text { ay } \\\\quad+5 z=10 \\\\\\\\\\n& 3 y+(a-2) z=b-6 \\\\\\\\\\n& a y+5 z=10 \\\\\\\\\\n& \\\\left(a^{2}-2 a-15\\\\right) z=a b-6 a-30\\n\\\\end{aligned}\\n$$\\n\\nExamine the last equation $\\\\left(a^{2}-2 a-15\\\\right) z=a b-6 a-30$.\\n\\n(a) The system has a unique solution if and only if the coefficient of $z$ is not zero; that is, if\\n\\n$$\\na^{2}-2 a-15=(a-5)(a+3) \\\\neq 0 \\\\quad \\\\text { or } \\\\quad a \\\\neq 5 \\\\quad \\\\text { and } \\\\quad a \\\\neq-3\\n$$\\n\\n(b) The system has more than one solution if both sides are zero. The left-hand side is zero when $a=5$ or $a=-3$. When $a=5$, the right-hand side is zero when $5 b-60=0$, or $b=12$. When $a=-3$, the righthand side is zero when $-3 b-12=0$, or $b=-4$. Thus, $(5,12)$ and $(-3,-4)$ are the pairs for which the system has more than one solution.\\n\\n\\n\\\\section*{Echelon Matrices, Row Equivalence, Row Canonical Form}\\n',\n",
       " '3.15. Row reduce each of the following matrices to echelon form:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{llll}1 & 2 & -3 & 0 \\\\\\\\ 2 & 4 & -2 & 2 \\\\\\\\ 3 & 6 & -4 & 3\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrr}-4 & 1 & -6 \\\\\\\\ 1 & 2 & -5 \\\\\\\\ 6 & 3 & -4\\\\end{array}\\\\right]$\\n\\n(a) Use $a_{11}=1$ as a pivot to obtain 0 \\'s below $a_{11}$; that is, apply the row operations \"Replace $R_{2}$ by $-2 R_{1}+R_{2}$ \"\\' and \\'Replace $R_{3}$ by $-3 R_{1}+R_{3}$.\" Then use $a_{23}=4$ as a pivot to obtain a 0 below $a_{23}$; that is, apply the row operation \"Replace $R_{3}$ by $-5 R_{2}+4 R_{3}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -3 & 0 \\\\\\\\\\n0 & 0 & 4 & 2 \\\\\\\\\\n0 & 0 & 5 & 3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -3 & 0 \\\\\\\\\\n0 & 0 & 4 & 2 \\\\\\\\\\n0 & 0 & 0 & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe matrix is now in echelon form.\\n\\n(b) Hand calculations are usually simpler if the pivot element equals 1 . Therefore, first interchange $R_{1}$ and $R_{2}$. Next apply the operations \\'Replace $R_{2}$ by $4 R_{1}+R_{2}$ \"\\' and \\'Replace $R_{3}$ by $-6 R_{1}+R_{3}$ \"; and then apply the operation \"Replace $R_{3}$ by $R_{2}+R_{3}$.\" These operations yield\\n\\n$$\\nB \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & -5 \\\\\\\\\\n-4 & 1 & -6 \\\\\\\\\\n6 & 3 & -4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & -5 \\\\\\\\\\n0 & 9 & -26 \\\\\\\\\\n0 & -9 & 26\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & -5 \\\\\\\\\\n0 & 9 & -26 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe matrix is now in echelon form.\\n',\n",
       " '\\n3.16. Describe the pivoting row-reduction algorithm. Also describe the advantages, if any, of using this pivoting algorithm.\\n\\nThe row-reduction algorithm becomes a pivoting algorithm if the entry in column $j$ of greatest absolute value is chosen as the pivot $a_{1 j_{1}}$ and if one uses the row operation\\n\\n$$\\n\\\\left(-a_{i j_{1}} / a_{1 j_{1}}\\\\right) R_{1}+R_{i} \\\\rightarrow R_{i}\\n$$\\n\\nThe main advantage of the pivoting algorithm is that the above row operation involves division by the (current) pivot $a_{1 j_{1}}$, and, on the computer, roundoff errors may be substantially reduced when one divides by a number as large in absolute value as possible.\\n',\n",
       " '\\n3.17. Let $A=\\\\left[\\\\begin{array}{rrrr}2 & -2 & 2 & 1 \\\\\\\\ -3 & 6 & 0 & -1 \\\\\\\\ 1 & -7 & 10 & 2\\\\end{array}\\\\right]$. Reduce $A$ to echelon form using the pivoting algorithm.\\n\\nFirst interchange $R_{1}$ and $R_{2}$ so that -3 can be used as the pivot, and then apply the operations \"Replace $R_{2}$ by $\\\\frac{2}{3} R_{1}+R_{2}$ \" and \"Replace $R_{3}$ by $\\\\frac{1}{3} R_{1}+R_{3}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n-3 & 6 & 0 & -1 \\\\\\\\\\n2 & -2 & 2 & 1 \\\\\\\\\\n1 & -7 & 10 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n-3 & 6 & 0 & -1 \\\\\\\\\\n0 & 2 & 2 & \\\\frac{1}{3} \\\\\\\\\\n0 & -5 & 10 & \\\\frac{5}{3}\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow interchange $R_{2}$ and $R_{3}$ so that -5 can be used as the pivot, and then apply the operation \"Replace $R_{3}$ by $\\\\frac{2}{5} R_{2}+R_{3}$.\" We obtain\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n-3 & 6 & 0 & -1 \\\\\\\\\\n0 & -5 & 10 & \\\\frac{5}{3} \\\\\\\\\\n0 & 2 & 2 & \\\\frac{1}{3}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n-3 & 6 & 0 & -1 \\\\\\\\\\n0 & -5 & 10 & \\\\frac{5}{3} \\\\\\\\\\n0 & 0 & 6 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe matrix has been brought to echelon form using partial pivoting.\\n',\n",
       " '\\n3.18. Reduce each of the following matrices to row canonical form:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rrrrr}2 & 2 & -1 & 6 & 4 \\\\\\\\ 4 & 4 & 1 & 10 & 13 \\\\\\\\ 8 & 8 & -1 & 26 & 23\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrr}5 & -9 & 6 \\\\\\\\ 0 & 2 & 3 \\\\\\\\ 0 & 0 & 7\\\\end{array}\\\\right]$\\n\\n(a) First reduce $A$ to echelon form by applying the operations \"Replace $R_{2}$ by $-2 R_{1}+R_{2}$ \" and \"Replace $R_{3}$ by $-4 R_{1}+R_{3}$,\" and then applying the operation \"Replace $R_{3}$ by $-R_{2}+R_{3}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n2 & 2 & -1 & 6 & 4 \\\\\\\\\\n0 & 0 & 3 & -2 & 5 \\\\\\\\\\n0 & 0 & 3 & 2 & 7\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n2 & 2 & -1 & 6 & 4 \\\\\\\\\\n0 & 0 & 3 & -2 & 5 \\\\\\\\\\n0 & 0 & 0 & 4 & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow use back-substitution on the echelon matrix to obtain the row canonical form of $A$. Specifically, first multiply $R_{3}$ by $\\\\frac{1}{4}$ to obtain the pivot $a_{34}=1$, and then apply the operations \"Replace $R_{2}$ by $2 R_{3}+R_{2}$ \" and \"Replace $R_{1}$ by $-6 R_{3}+R_{1}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n2 & 2 & -1 & 6 & 4 \\\\\\\\\\n0 & 0 & 3 & -2 & 5 \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n2 & 2 & -1 & 0 & 1 \\\\\\\\\\n0 & 0 & 3 & 0 & 6 \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow multiply $R_{2}$ by $\\\\frac{1}{3}$, making the pivot $a_{23}=1$, and then apply \"Replace $R_{1}$ by $R_{2}+R_{1}$,\" yielding\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n2 & 2 & -1 & 0 & 1 \\\\\\\\\\n0 & 0 & 1 & 0 & 2 \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lllll}\\n2 & 2 & 0 & 0 & 3 \\\\\\\\\\n0 & 0 & 1 & 0 & 2 \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nFinally, multiply $R_{1}$ by $\\\\frac{1}{2}$, so the pivot $a_{11}=1$. Thus, we obtain the following row canonical form of $A$ :\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{lllll}\\n1 & 1 & 0 & 0 & \\\\frac{3}{2} \\\\\\\\\\n0 & 0 & 1 & 0 & 2 \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Because $B$ is in echelon form, use back-substitution to obtain\\n\\n$$\\nB \\\\sim\\\\left[\\\\begin{array}{rrr}\\n5 & -9 & 6 \\\\\\\\\\n0 & 2 & 3 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n5 & -9 & 0 \\\\\\\\\\n0 & 2 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n5 & -9 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n5 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe last matrix, which is the identity matrix $I$, is the row canonical form of $B$. (This is expected, because $B$ is invertible, and so its row canonical form must be $I$.)\\n',\n",
       " \"\\n3.19. Describe the Gauss-Jordan elimination algorithm, which also row reduces an arbitrary matrix $A$ to its row canonical form.\\n\\nThe Gauss-Jordan algorithm is similar in some ways to the Gaussian elimination algorithm, except that here each pivot is used to place 0 's both below and above the pivot, not just below the pivot, before working with the next pivot. Also, one variation of the algorithm first normalizes each row-that is, obtains a unit pivot-before it is used to produce 0's in the other rows, rather than normalizing the rows at the end of the algorithm.\\n\",\n",
       " '\\n3.20. Let $A=\\\\left[\\\\begin{array}{rrrrr}1 & -2 & 3 & 1 & 2 \\\\\\\\ 1 & 1 & 4 & -1 & 3 \\\\\\\\ 2 & 5 & 9 & -2 & 8\\\\end{array}\\\\right]$. Use Gauss-Jordan to find the row canonical form of $A$.\\n\\nUse $a_{11}=1$ as a pivot to obtain 0 \\'s below $a_{11}$ by applying the operations \"Replace $R_{2}$ by $-R_{1}+R_{2}$ \" and \\'Replace $R_{3}$ by $-2 R_{1}+R_{3}$.\" This yields\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & -2 & 3 & 1 & 2 \\\\\\\\\\n0 & 3 & 1 & -2 & 1 \\\\\\\\\\n0 & 9 & 3 & -4 & 4\\n\\\\end{array}\\\\right]\\n$$\\n\\nMultiply $R_{2}$ by $\\\\frac{1}{3}$ to make the pivot $a_{22}=1$, and then produce 0 \\'s below and above $a_{22}$ by applying the operations \"Replace $R_{3}$ by $-9 R_{2}+R_{3}$ \" and \"Replace $R_{1}$ by $2 R_{2}+R_{1}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & -2 & 3 & 1 & 2 \\\\\\\\\\n0 & 1 & \\\\frac{1}{3} & -\\\\frac{2}{3} & \\\\frac{1}{3} \\\\\\\\\\n0 & 9 & 3 & -4 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 0 & \\\\frac{11}{3} & -\\\\frac{1}{3} & \\\\frac{8}{3} \\\\\\\\\\n0 & 1 & \\\\frac{1}{3} & -\\\\frac{2}{3} & \\\\frac{1}{3} \\\\\\\\\\n0 & 0 & 0 & 2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nFinally, multiply $R_{3}$ by $\\\\frac{1}{2}$ to make the pivot $a_{34}=1$, and then produce 0 \\'s above $a_{34}$ by applying the operations \\'Replace $R_{2}$ by $\\\\frac{2}{3} R_{3}+R_{2}$ \"\\' and \\'Replace $R_{1}$ by $\\\\frac{1}{3} R_{3}+R_{1}$.\" These operations yield\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 0 & \\\\frac{11}{3} & -\\\\frac{1}{3} & \\\\frac{8}{3} \\\\\\\\\\n0 & 1 & \\\\frac{1}{3} & -\\\\frac{2}{3} & \\\\frac{1}{3} \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{ccccc}\\n1 & 0 & \\\\frac{11}{3} & 0 & \\\\frac{17}{6} \\\\\\\\\\n0 & 1 & \\\\frac{1}{3} & 0 & \\\\frac{2}{3} \\\\\\\\\\n0 & 0 & 0 & 1 & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhich is the row canonical form of $A$.\\n\\n\\n\\\\section*{Systems of Linear Equations in Matrix Form}\\n',\n",
       " '3.21. Find the augmented matrix $M$ and the coefficient matrix $A$ of the following system:\\n\\n$$\\n\\\\begin{array}{r}\\nx+2 y-3 z=4 \\\\\\\\\\n3 y-4 z+7 x=5 \\\\\\\\\\n6 z+8 x-9 y=1\\n\\\\end{array}\\n$$\\n\\nFirst align the unknowns in the system, and then use the aligned system to obtain $M$ and $A$. We have\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y-3 z & =4 \\\\\\\\\\n7 x+3 y-4 z & =5 ; \\\\\\\\\\n8 x-9 y+6 z & =1\\n\\\\end{aligned} \\\\quad \\\\text { then } \\\\quad M=\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -3 & 4 \\\\\\\\\\n7 & 3 & -4 & 5 \\\\\\\\\\n8 & -9 & 6 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad A=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & -3 \\\\\\\\\\n7 & 3 & -4 \\\\\\\\\\n8 & -9 & 6\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n3.22. Solve each of the following systems using its augmented matrix $M$ :\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y-z=3 \\\\quad x-2 y+4 z=2 \\\\quad x+y+3 z=1 \\\\\\\\\\n& x+3 y+z=5 \\\\quad 2 x-3 y+5 z=3 \\\\quad 2 x+3 y-z=3 \\\\\\\\\\n& 3 x+8 y+4 z=17 \\\\quad 3 x-4 y+6 z=7 \\\\quad 5 x+7 y+z=7\\n\\\\end{aligned}\\n$$\\n\\n(a) Reduce the augmented matrix $M$ to echelon form as follows:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -1 & 3 \\\\\\\\\\n1 & 3 & 1 & 5 \\\\\\\\\\n3 & 8 & 4 & 17\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -1 & 3 \\\\\\\\\\n0 & 1 & 2 & 2 \\\\\\\\\\n0 & 2 & 7 & 8\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -1 & 3 \\\\\\\\\\n0 & 1 & 2 & 2 \\\\\\\\\\n0 & 0 & 3 & 4\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow write down the corresponding triangular system\\n\\n$$\\n\\\\begin{array}{r}\\nx+2 y-z=3 \\\\\\\\\\ny+2 z=2 \\\\\\\\\\n3 z=4\\n\\\\end{array}\\n$$\\n\\nand solve by back-substitution to obtain the unique solution\\n\\n$$\\nx=\\\\frac{17}{3}, y=-\\\\frac{2}{3}, z=\\\\frac{4}{3} \\\\quad \\\\text { or } \\\\quad u=\\\\left(\\\\frac{17}{3},-\\\\frac{2}{3}, \\\\frac{4}{3}\\\\right)\\n$$\\n\\nAlternately, reduce the echelon form of $M$ to row canonical form, obtaining\\n\\n$$\\nM \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & -1 & 3 \\\\\\\\\\n0 & 1 & 2 & 2 \\\\\\\\\\n0 & 0 & 1 & \\\\frac{4}{3}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 0 & \\\\frac{13}{3} \\\\\\\\\\n0 & 1 & 0 & -\\\\frac{2}{3} \\\\\\\\\\n0 & 0 & 1 & \\\\frac{4}{3}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 0 & 0 & \\\\frac{17}{3} \\\\\\\\\\n0 & 1 & 0 & -\\\\frac{2}{3} \\\\\\\\\\n0 & 0 & 1 & \\\\frac{4}{3}\\n\\\\end{array}\\\\right]\\n$$\\n\\nThis also corresponds to the above solution.\\n\\n(b) First reduce the augmented matrix $M$ to echelon form as follows:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{llll}\\n1 & -2 & 4 & 2 \\\\\\\\\\n2 & -3 & 5 & 3 \\\\\\\\\\n3 & -4 & 6 & 7\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & -2 & 4 & 2 \\\\\\\\\\n0 & 1 & -3 & -1 \\\\\\\\\\n0 & 2 & -6 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & -2 & 4 & 2 \\\\\\\\\\n0 & 1 & -3 & -1 \\\\\\\\\\n0 & 0 & 0 & 3\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe third row corresponds to the degenerate equation $0 x+0 y+0 z=3$, which has no solution. Thus, \"DO NOT CONTINUE.\" The original system also has no solution. (Note that the echelon form indicates whether or not the system has a solution.)\\n\\n(c) Reduce the augmented matrix $M$ to echelon form and then to row canonical form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 3 & 1 \\\\\\\\\\n2 & 3 & -1 & 3 \\\\\\\\\\n5 & 7 & 1 & 7\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 3 & 1 \\\\\\\\\\n0 & 1 & -7 & 1 \\\\\\\\\\n0 & 2 & -14 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 0 & 10 & 0 \\\\\\\\\\n0 & 1 & -7 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(The third row of the second matrix is deleted, because it is a multiple of the second row and will result in a zero row.) Write down the system corresponding to the row canonical form of $M$ and then transfer the free variables to the other side to obtain the free-variable form of the solution:\\n\\n$$\\n\\\\begin{aligned}\\n& x+10 z=0 \\\\\\\\\\n& y-7 z=1\\n\\\\end{aligned} \\\\quad \\\\text { and } \\\\quad \\\\begin{gathered}\\nx=-10 z \\\\\\\\\\ny=1+7 z\\n\\\\end{gathered}\\n$$\\n\\nHere $z$ is the only free variable. The parametric solution, using $z=a$, is as follows:\\n\\n$$\\nx=-10 a, y=1+7 a, z=a \\\\quad \\\\text { or } \\\\quad u=(-10 a, 1+7 a, a)\\n$$\\n',\n",
       " '\\n3.23. Solve the following system using its augmented matrix $M$ :\\n\\n$$\\n\\\\begin{array}{r}\\nx_{1}+2 x_{2}-3 x_{3}-2 x_{4}+4 x_{5}=1 \\\\\\\\\\n2 x_{1}+5 x_{2}-8 x_{3}-x_{4}+6 x_{5}=4 \\\\\\\\\\nx_{1}+4 x_{2}-7 x_{3}+5 x_{4}+2 x_{5}=8\\n\\\\end{array}\\n$$\\n\\nReduce the augmented matrix $M$ to echelon form and then to row canonical form:\\n\\n$$\\n\\\\begin{aligned}\\n& M=\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & -3 & -2 & 4 & 1 \\\\\\\\\\n2 & 5 & -8 & -1 & 6 & 4 \\\\\\\\\\n1 & 4 & -7 & 5 & 2 & 8\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & -3 & -2 & 4 & 1 \\\\\\\\\\n0 & 1 & -2 & 3 & -2 & 2 \\\\\\\\\\n0 & 2 & -4 & 7 & -2 & 7\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & -3 & -2 & 4 & 1 \\\\\\\\\\n0 & 1 & -2 & 3 & -2 & 2 \\\\\\\\\\n0 & 0 & 0 & 1 & 2 & 3\\n\\\\end{array}\\\\right] \\\\\\\\\\n& \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & -3 & 0 & 8 & 7 \\\\\\\\\\n0 & 1 & -2 & 0 & -8 & -7 \\\\\\\\\\n0 & 0 & 0 & 1 & 2 & 3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 0 & 1 & 0 & 24 & 21 \\\\\\\\\\n0 & 1 & -2 & 0 & -8 & -7 \\\\\\\\\\n0 & 0 & 0 & 1 & 2 & 3\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nWrite down the system corresponding to the row canonical form of $M$ and then transfer the free variables to the other side to obtain the free-variable form of the solution:\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-104}\\n\\\\end{center}\\n\\n$$\\n\\\\begin{aligned}\\n& x_{4}+2 x_{5}=3 \\\\quad x_{4}=3-2 x_{5}\\n\\\\end{aligned}\\n$$\\n\\nHere $x_{1}, x_{2}, x_{4}$ are the pivot variables and $x_{3}$ and $x_{5}$ are the free variables. Recall that the parametric form of the solution can be obtained from the free-variable form of the solution by simply setting the free variables equal to parameters, say $x_{3}=a, x_{5}=b$. This process yields\\n\\nor\\n\\n$$\\n\\\\begin{gathered}\\nx_{1}=21-a-24 b, x_{2}=-7+2 a+8 b, x_{3}=a, x_{4}=3-2 b, x_{5}=b \\\\\\\\\\nu=(21-a-24 b,-7+2 a+8 b, a, 3-2 b, b)\\n\\\\end{gathered}\\n$$\\n\\nwhich is another form of the solution.\\n\\n\\n\\\\section*{Linear Combinations, Homogeneous Systems}\\n',\n",
       " '3.24. Write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where\\n\\n(a) $v=(3,10,7)$ and $u_{1}=(1,3,-2), u_{2}=(1,4,2), u_{3}=(2,8,1)$;\\n\\n(b) $v=(2,7,10)$ and $u_{1}=(1,2,3), u_{2}=(1,3,5), u_{3}=(1,5,9)$;\\n\\n(c) $v=(1,5,4)$ and $u_{1}=(1,3,-2), u_{2}=(2,7,-1), u_{3}=(1,6,7)$.\\n\\nFind the equivalent system of linear equations by writing $v=x u_{1}+y u_{2}+z u_{3}$. Alternatively, use the augmented matrix $M$ of the equivalent system, where $M=\\\\left[u_{1}, u_{2}, u_{3}, v\\\\right]$. (Here $u_{1}, u_{2}, u_{3}, v$ are the columns of $M$.)\\n\\n(a) The vector equation $v=x u_{1}+y u_{2}+z u_{3}$ for the given vectors is as follows:\\n\\n$$\\n\\\\left[\\\\begin{array}{r}\\n3 \\\\\\\\\\n10 \\\\\\\\\\n7\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n3 \\\\\\\\\\n-2\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n4 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{l}\\n2 \\\\\\\\\\n8 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\nx+y+2 z \\\\\\\\\\n3 x+4 y+8 z \\\\\\\\\\n-2 x+2 y+z\\n\\\\end{array}\\\\right]\\n$$\\n\\nForm the equivalent system of linear equations by setting corresponding entries equal to each other, and then reduce the system to echelon form:\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-105}\\n\\\\end{center}\\n\\nThe system is in triangular form. Back-substitution yields the unique solution $x=2, y=7, z=-3$. Thus, $v=2 u_{1}+7 u_{2}-3 u_{3}$.\\n\\nAlternatively, form the augmented matrix $M=\\\\left[u_{1}, u_{2}, u_{3}, v\\\\right]$ of the equivalent system, and reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 3 \\\\\\\\\\n3 & 4 & 8 & 10 \\\\\\\\\\n-2 & 2 & 1 & 7\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 3 \\\\\\\\\\n0 & 1 & 2 & 1 \\\\\\\\\\n0 & 4 & 5 & 13\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 3 \\\\\\\\\\n0 & 1 & 2 & 1 \\\\\\\\\\n0 & 0 & -3 & 9\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe last matrix corresponds to a triangular system that has a unique solution. Back-substitution yields the solution $x=2, y=7, z=-3$. Thus, $v=2 u_{1}+7 u_{2}-3 u_{3}$.\\n\\n(b) Form the augmented matrix $M=\\\\left[u_{1}, u_{2}, u_{3}, v\\\\right]$ of the equivalent system, and reduce $M$ to the echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 1 & 2 \\\\\\\\\\n2 & 3 & 5 & 7 \\\\\\\\\\n3 & 5 & 9 & 10\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 2 \\\\\\\\\\n0 & 1 & 3 & 3 \\\\\\\\\\n0 & 2 & 6 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{cccc}\\n1 & 1 & 1 & 2 \\\\\\\\\\n0 & 1 & 3 & 3 \\\\\\\\\\n0 & 0 & 0 & -2\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe third row corresponds to the degenerate equation $0 x+0 y+0 z=-2$, which has no solution. Thus, the system also has no solution, and $v$ cannot be written as a linear combination of $u_{1}, u_{2}, u_{3}$.\\n\\n(c) Form the augmented matrix $M=\\\\left[u_{1}, u_{2}, u_{3}, v\\\\right]$ of the equivalent system, and reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 1 & 1 \\\\\\\\\\n3 & 7 & 6 & 5 \\\\\\\\\\n-2 & -1 & 7 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 2 & 1 & 1 \\\\\\\\\\n0 & 1 & 3 & 2 \\\\\\\\\\n0 & 3 & 9 & 6\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 2 & 1 & 1 \\\\\\\\\\n0 & 1 & 3 & 2 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe last matrix corresponds to the following system with free variable $z$ :\\n\\n$$\\n\\\\begin{array}{r}\\nx+2 y+z=1 \\\\\\\\\\ny+3 z=2\\n\\\\end{array}\\n$$\\n\\nThus, $v$ can be written as a linear combination of $u_{1}, u_{2}, u_{3}$ in many ways. For example, let the free variable $z=1$, and, by back-substitution, we get $y=-2$ and $x=2$. Thus, $v=2 u_{1}-2 u_{2}+u_{3}$.\\n',\n",
       " '\\n3.25. Let $u_{1}=(1,2,4), u_{2}=(2,-3,1), u_{3}=(2,1,-1)$ in $\\\\mathbf{R}^{3}$. Show that $u_{1}, u_{2}, u_{3}$ are orthogonal, and write $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$, where (a) $v=(7,16,6)$, (b) $v=(3,5,2)$.\\n\\nTake the dot product of pairs of vectors to get\\n\\n$$\\nu_{1} \\\\cdot u_{2}=2-6+4=0, \\\\quad u_{1} \\\\cdot u_{3}=2+2-4=0, \\\\quad u_{2} \\\\cdot u_{3}=4-3-1=0\\n$$\\n\\nThus, the three vectors in $\\\\mathbf{R}^{3}$ are orthogonal, and hence Fourier coefficients can be used. That is, $v=x u_{1}+y u_{2}+z u_{3}$, where\\n\\n(a) We have\\n\\n$$\\nx=\\\\frac{v \\\\cdot u_{1}}{u_{1} \\\\cdot u_{1}}, \\\\quad y=\\\\frac{v \\\\cdot u_{2}}{u_{2} \\\\cdot u_{2}}, \\\\quad z=\\\\frac{v \\\\cdot u_{3}}{u_{3} \\\\cdot u_{3}}\\n$$\\n\\n$x=\\\\frac{7+32+24}{1+4+16}=\\\\frac{63}{21}=3$,\\n\\n$y=\\\\frac{14-48+6}{4+9+1}=\\\\frac{-28}{14}=-2$,\\n\\n$$\\nz=\\\\frac{14+16-6}{4+1+1}=\\\\frac{24}{6}=4\\n$$\\n\\nThus, $v=3 u_{1}-2 u_{2}+4 u_{3}$.\\n\\n(b) We have\\n\\n$$\\nx=\\\\frac{3+10+8}{1+4+16}=\\\\frac{21}{21}=1, \\\\quad y=\\\\frac{6-15+2}{4+9+1}=\\\\frac{-7}{14}=-\\\\frac{1}{2}, \\\\quad z=\\\\frac{6+5-2}{4+1+1}=\\\\frac{9}{6}=\\\\frac{3}{2}\\n$$\\n\\nThus, $v=u_{1}-\\\\frac{1}{2} u_{2}+\\\\frac{3}{2} u_{3}$.\\n',\n",
       " '\\n3.26. Find the dimension and a basis for the general solution $W$ of each of the following homogeneous systems:\\n\\n\\\\[\\n\\\\begin{array}{rr}\\n2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4}=0 & x-2 y-3 z=0 \\\\\\\\\\n3 x_{1}+6 x_{2}-7 x_{3}+4 x_{4}=0 & 2 x+y+3 z=0 \\\\\\\\\\n5 x_{1}+10 x_{2}-11 x_{3}+6 x_{4}=0 & 3 x-4 y-2 z=0 \\\\tag{a}\\n\\\\end{array}\\n\\\\]\\n\\n(a) Reduce the system to echelon form using the operations \"Replace $L_{2}$ by $-3 L_{1}+2 L_{2}$,\" \"Replace $L_{3}$ by $-5 L_{1}+2 L_{3}$,\" and then \"Replace $L_{3}$ by $-2 L_{2}+L_{3}$.\" These operations yield\\n\\n$$\\n\\\\begin{aligned}\\n2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4} & =0 \\\\\\\\\\nx_{3}-x_{4} & =0 \\\\\\\\\\n3 x_{3}-3 x_{4} & =0\\n\\\\end{aligned} \\\\quad \\\\text { and } \\\\quad \\\\begin{aligned}\\n2 x_{1}+4 x_{2}-5 x_{3}+3 x_{4} & =0 \\\\\\\\\\nx_{3}-x_{4} & =0\\n\\\\end{aligned}\\n$$\\n\\nThe system in echelon form has two free variables, $x_{2}$ and $x_{4}$, so $\\\\operatorname{dim} W=2$. A basis $\\\\left[u_{1}, u_{2}\\\\right]$ for $W$ may be obtained as follows:\\n\\n(1) Set $x_{2}=1, x_{4}=0$. Back-substitution yields $x_{3}=0$, and then $x_{1}=-2$. Thus, $u_{1}=(-2,1,0,0)$.\\n\\n(2) Set $x_{2}=0, x_{4}=1$. Back-substitution yields $x_{3}=1$, and then $x_{1}=1$. Thus, $u_{2}=(1,0,1,1)$.\\n\\n(b) Reduce the system to echelon form, obtaining\\n\\n$$\\n\\\\begin{aligned}\\nx-2 y-3 z & =0 & & x-2 y-3 z & =0 \\\\\\\\\\n5 y+9 z & =0 & \\\\text { and } & 5 y+9 z & =0 \\\\\\\\\\n2 y+7 z & =0 & & 17 z & =0\\n\\\\end{aligned}\\n$$\\n\\nThere are no free variables (the system is in triangular form). Hence, $\\\\operatorname{dim} W=0$, and $W$ has no basis. Specifically, $W$ consists only of the zero solution; that is, $W=\\\\{0\\\\}$.\\n',\n",
       " '\\n3.27. Find the dimension and a basis for the general solution $W$ of the following homogeneous system using matrix notation:\\n\\n$$\\n\\\\begin{array}{r}\\nx_{1}+2 x_{2}+3 x_{3}-2 x_{4}+4 x_{5}=0 \\\\\\\\\\n2 x_{1}+4 x_{2}+8 x_{3}+x_{4}+9 x_{5}=0 \\\\\\\\\\n3 x_{1}+6 x_{2}+13 x_{3}+4 x_{4}+14 x_{5}=0\\n\\\\end{array}\\n$$\\n\\nShow how the basis gives the parametric form of the general solution of the system.\\n\\nWhen a system is homogeneous, we represent the system by its coefficient matrix $A$ rather than by its\\\\\\\\\\naugmented matrix $M$, because the last column of the augmented matrix $M$ is a zero column, and it will remain a zero column during any row-reduction process.\\n\\nReduce the coefficient matrix $A$ to echelon form, obtaining\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 3 & -2 & 4 \\\\\\\\\\n2 & 4 & 8 & 1 & 9 \\\\\\\\\\n3 & 6 & 13 & 4 & 14\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 3 & -2 & 4 \\\\\\\\\\n0 & 0 & 2 & 5 & 1 \\\\\\\\\\n0 & 0 & 4 & 10 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 3 & -2 & 4 \\\\\\\\\\n0 & 0 & 2 & 5 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(The third row of the second matrix is deleted, because it is a multiple of the second row and will result in a zero row.) We can now proceed in one of two ways.\\n\\n(a) Write down the corresponding homogeneous system in echelon form:\\n\\n$$\\n\\\\begin{aligned}\\nx_{1}+2 x_{2}+3 x_{3}-2 x_{4}+4 x_{5} & =0 \\\\\\\\\\n2 x_{3}+5 x_{4}+x_{5} & =0\\n\\\\end{aligned}\\n$$\\n\\nThe system in echelon form has three free variables, $x_{2}, x_{4}, x_{5}$, so $\\\\operatorname{dim} W=3$. A basis $\\\\left[u_{1}, u_{2}, u_{3}\\\\right]$ for $W$ may be obtained as follows:\\n\\n(1) Set $x_{2}=1, x_{4}=0, x_{5}=0$. Back-substitution yields $x_{3}=0$, and then $x_{1}=-2$. Thus,\\n\\n$$\\nu_{1}=(-2,1,0,0,0)\\n$$\\n\\n(2) Set $x_{2}=0, x_{4}=1, x_{5}=0$. Back-substitution yields $x_{3}=-\\\\frac{5}{2}$, and then $x_{1}=\\\\frac{19}{2}$. Thus,\\n\\n$$\\nu_{2}=\\\\left(\\\\frac{19}{2}, 0,-\\\\frac{5}{2}, 1,0\\\\right) \\\\text {. }\\n$$\\n\\n(3) Set $x_{2}=0, x_{4}=0, x_{5}=1$. Back-substitution yields $x_{3}=-\\\\frac{1}{2}$, and then $x_{1}=-\\\\frac{5}{2}$. Thus,\\n\\n$$\\nu_{3}=\\\\left(-\\\\frac{5}{2}, 0,-\\\\frac{1}{2}, 0,1\\\\right) \\\\text {. }\\n$$\\n\\n[One could avoid fractions in the basis by choosing $x_{4}=2$ in (2) and $x_{5}=2$ in (3), which yields multiples of $u_{2}$ and $u_{3}$.] The parametric form of the general solution is obtained from the following linear combination of the basis vectors using parameters $a, b, c$ :\\n\\n$$\\na u_{1}+b u_{2}+c u_{3}=\\\\left(-2 a+\\\\frac{19}{2} b-\\\\frac{5}{2} c, a,-\\\\frac{5}{2} b-\\\\frac{1}{2} c, b, c\\\\right)\\n$$\\n\\n(b) Reduce the echelon form of $A$ to row canonical form:\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{ccccc}\\n1 & 2 & 3 & -2 & 4 \\\\\\\\\\n0 & 0 & 1 & \\\\frac{5}{2} & \\\\frac{1}{2}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 3 & -\\\\frac{19}{2} & \\\\frac{5}{2} \\\\\\\\\\n0 & 0 & 1 & \\\\frac{5}{2} & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nWrite down the corresponding free-variable solution:\\n\\n$$\\n\\\\begin{aligned}\\n& x_{1}=-2 x_{2}+\\\\frac{19}{2} x_{4}-\\\\frac{5}{2} x_{5} \\\\\\\\\\n& x_{3}=-\\\\frac{5}{2} x_{4}-\\\\frac{1}{2} x_{5}\\n\\\\end{aligned}\\n$$\\n\\nUsing these equations for the pivot variables $x_{1}$ and $x_{3}$, repeat the above process to obtain a basis $\\\\left[u_{1}, u_{2}, u_{3}\\\\right]$ for $W$. That is, set $x_{2}=1, x_{4}=0, x_{5}=0$ to get $u_{1}$; set $x_{2}=0, x_{4}=1, x_{5}=0$ to get $u_{2}$; and set $x_{2}=0$, $x_{4}=0, x_{5}=1$ to get $u_{3}$.\\n',\n",
       " '\\n3.28. Prove Theorem 3.15. Let $v_{0}$ be a particular solution of $A X=B$, and let $W$ be the general solution of $A X=0$. Then $U=v_{0}+W=\\\\left\\\\{v_{0}+w: w \\\\in W\\\\right\\\\}$ is the general solution of $A X=B$. Let $w$ be a solution of $A X=0$. Then\\n\\n$$\\nA\\\\left(v_{0}+w\\\\right)=A v_{0}+A w=B+0=B\\n$$\\n\\nThus, the sum $v_{0}+w$ is a solution of $A X=B$. On the other hand, suppose $v$ is also a solution of $A X=B$. Then\\n\\n$$\\nA\\\\left(v-v_{0}\\\\right)=A v-A v_{0}=B-B=0\\n$$\\n\\nTherefore, $v-v_{0}$ belongs to $W$. Because $v=v_{0}+\\\\left(v-v_{0}\\\\right)$, we find that any solution of $A X=B$ can be obtained by adding a solution of $A X=0$ to a solution of $A X=B$. Thus, the theorem is proved.\\n\\n\\n\\\\section*{Elementary Matrices, Applications}\\n',\n",
       " '3.29. Let $e_{1}, e_{2}, e_{3}$ denote, respectively, the elementary row operations\\n\\n\"Interchange rows $R_{1}$ and $R_{2}$,\" \"Replace $R_{3}$ by $7 R_{3}$,\" \"Replace $R_{2}$ by $-3 R_{1}+R_{2}$ \"\\n\\nFind the corresponding three-square elementary matrices $E_{1}, E_{2}, E_{3}$. Apply each operation to the $3 \\\\times 3$ identity matrix $I_{3}$ to obtain\\n\\n$$\\nE_{1}=\\\\left[\\\\begin{array}{lll}\\n0 & 1 & 0 \\\\\\\\\\n1 & 0 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right], \\\\quad E_{2}=\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 7\\n\\\\end{array}\\\\right], \\\\quad E_{3}=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n-3 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n3.30. Consider the elementary row operations in Problem 3.29.\\n\\n(a) Describe the inverse operations $e_{1}^{-1}, e_{2}^{-1}, e_{3}^{-1}$.\\n\\n(b) Find the corresponding three-square elementary matrices $E_{1}^{\\\\prime}, E_{2}^{\\\\prime}, E_{3}^{\\\\prime}$.\\n\\n(c) What is the relationship between the matrices $E_{1}^{\\\\prime}, E_{2}^{\\\\prime}, E_{3}^{\\\\prime}$ and the matrices $E_{1}, E_{2}, E_{3}$ ?\\n\\n(a) The inverses of $e_{1}, e_{2}, e_{3}$ are, respectively,\\n\\n$$\\n\\\\text { \"Interchange rows } R_{1} \\\\text { and } R_{2}, \" \\\\quad \\\\text { \"Replace } R_{3} \\\\text { by } \\\\frac{1}{7} R_{3}, \" \\\\quad \\\\text { \"Replace } R_{2} \\\\text { by } 3 R_{1}+R_{2} \\\\text {.\" }\\n$$\\n\\n(b) Apply each inverse operation to the $3 \\\\times 3$ identity matrix $I_{3}$ to obtain\\n\\n$$\\nE_{1}^{\\\\prime}=\\\\left[\\\\begin{array}{ccc}\\n0 & 1 & 0 \\\\\\\\\\n1 & 0 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right], \\\\quad E_{2}^{\\\\prime}=\\\\left[\\\\begin{array}{ccc}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & \\\\frac{1}{7}\\n\\\\end{array}\\\\right], \\\\quad E_{3}^{\\\\prime}=\\\\left[\\\\begin{array}{ccc}\\n1 & 0 & 0 \\\\\\\\\\n3 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) The matrices $E_{1}^{\\\\prime}, E_{2}^{\\\\prime}, E_{3}^{\\\\prime}$ are, respectively, the inverses of the matrices $E_{1}, E_{2}, E_{3}$.\\n',\n",
       " '\\n3.31. Write each of the following matrices as a product of elementary matrices:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rr}1 & -3 \\\\\\\\ -2 & 4\\\\end{array}\\\\right]$\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 0 & 1 & 4 \\\\\\\\ 0 & 0 & 1\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rrr}1 & 1 & 2 \\\\\\\\ 2 & 3 & 8 \\\\\\\\ -3 & -1 & 2\\\\end{array}\\\\right]$\\n\\nThe following three steps write a matrix $M$ as a product of elementary matrices:\\n\\nStep 1. Row reduce $M$ to the identity matrix $I$, keeping track of the elementary row operations.\\n\\nStep 2. Write down the inverse row operations.\\n\\nStep 3. Write $M$ as the product of the elementary matrices corresponding to the inverse operations. This gives the desired result.\\n\\nIf a zero row appears in Step 1, then $M$ is not row equivalent to the identity matrix $I$, and $M$ cannot be written as a product of elementary matrices.\\n\\n(a) (1) We have\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n1 & -3 \\\\\\\\\\n-2 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{ll}\\n1 & -3 \\\\\\\\\\n0 & -2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rr}\\n1 & -3 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]=I\\n$$\\n\\nwhere the row operations are, respectively,\\n\\n\"Replace $R_{2}$ by $2 R_{1}+R_{2}$,\" \"Replace $R_{2}$ by $-\\\\frac{1}{2} R_{2}, \" \\\\quad$ \"Replace $R_{1}$ by $3 R_{2}+R_{1}$ ”\\n\\n(2) Inverse operations:\\n\\n\"Replace $R_{2}$ by $-2 R_{1}+R_{2}$,\" $\\\\quad$ \"Replace $R_{2}$ by $-2 R_{2}$,\" $\\\\quad$ \"Replace $R_{1}$ by $-3 R_{2}+R_{1}$ \"\\n\\n(3) $A=\\\\left[\\\\begin{array}{rr}1 & 0 \\\\\\\\ -2 & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & 0 \\\\\\\\ 0 & -2\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & -3 \\\\\\\\ 0 & 1\\\\end{array}\\\\right]$\\\\\\\\\\n(b) (1) We have\\n\\n$$\\nB=\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 3 \\\\\\\\\\n0 & 1 & 4 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]=I\\n$$\\n\\nwhere the row operations are, respectively,\\n\\n\"Replace $R_{2}$ by $-4 R_{3}+R_{2}$,\" \"Replace $R_{1}$ by $-3 R_{3}+R_{1}$,\" \"Replace $R_{1}$ by $-2 R_{2}+R_{1}$ \"\\n\\n(2) Inverse operations:\\n\\n\"Replace $R_{2}$ by $4 R_{3}+R_{2}$,\" \"Replace $R_{1}$ by $3 R_{3}+R_{1}$,\" $\\\\quad$ Replace $R_{1}$ by $2 R_{2}+R_{1}$ \"\\n\\n(3) $B=\\\\left[\\\\begin{array}{lll}1 & 0 & 0 \\\\\\\\ 0 & 1 & 4 \\\\\\\\ 0 & 0 & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{lll}1 & 0 & 3 \\\\\\\\ 0 & 1 & 0 \\\\\\\\ 0 & 0 & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{lll}1 & 2 & 0 \\\\\\\\ 0 & 1 & 0 \\\\\\\\ 0 & 0 & 1\\\\end{array}\\\\right]$\\n\\n(c) (1) First row reduce $C$ to echelon form. We have\\n\\n$$\\nC=\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 2 \\\\\\\\\\n2 & 3 & 8 \\\\\\\\\\n-3 & -1 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 4 \\\\\\\\\\n0 & 2 & 8\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 4 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nIn echelon form, $C$ has a zero row. \"STOP.\" The matrix $C$ cannot be row reduced to the identity matrix $I$, and $C$ cannot be written as a product of elementary matrices. (We note, in particular, that $C$ has no inverse.)\\n',\n",
       " '\\n3.32. Find the inverse of (a) $A=\\\\left[\\\\begin{array}{rrr}1 & 2 & -4 \\\\\\\\ -1 & -1 & 5 \\\\\\\\ 2 & 7 & -3\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{rrr}1 & 3 & -4 \\\\\\\\ 1 & 5 & -1 \\\\\\\\ 3 & 13 & -6\\\\end{array}\\\\right]$.\\n\\n(a) Form the matrix $M=[A, I]$ and row reduce $M$ to echelon form:\\n\\n$$\\n\\\\begin{aligned}\\nM & =\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 2 & -4 & 1 & 0 & 0 \\\\\\\\\\n-1 & -1 & 5 & 0 & 1 & 0 \\\\\\\\\\n2 & 7 & -3 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 2 & -4 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & 1 & 1 & 0 \\\\\\\\\\n0 & 3 & 5 & -2 & 0 & 1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& \\\\sim\\\\left[\\\\begin{array}{rrrrrrr}\\n1 & 2 & -4 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & 1 & 1 & 0 \\\\\\\\\\n0 & 0 & 2 & -5 & -3 & 1\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nIn echelon form, the left half of $M$ is in triangular form; hence, $A$ has an inverse. Further reduce $M$ to row canonical form:\\n\\n$$\\nM \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 2 & 0 & -9 & -6 & 2 \\\\\\\\\\n0 & 1 & 0 & \\\\frac{7}{2} & \\\\frac{5}{2} & -\\\\frac{1}{2} \\\\\\\\\\n0 & 0 & 1 & -\\\\frac{5}{2} & -\\\\frac{3}{2} & \\\\frac{1}{2}\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 0 & 0 & -16 & -11 & 3 \\\\\\\\\\n0 & 1 & 0 & \\\\frac{7}{2} & \\\\frac{5}{2} & -\\\\frac{1}{2} \\\\\\\\\\n0 & 0 & 1 & -\\\\frac{5}{2} & -\\\\frac{3}{2} & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe final matrix has the form $\\\\left[I, A^{-1}\\\\right]$; that is, $A^{-1}$ is the right half of the last matrix. Thus,\\n\\n$$\\nA^{-1}=\\\\left[\\\\begin{array}{rrr}\\n-16 & -11 & 3 \\\\\\\\\\n\\\\frac{7}{2} & \\\\frac{5}{2} & -\\\\frac{1}{2} \\\\\\\\\\n-\\\\frac{5}{2} & -\\\\frac{3}{2} & \\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Form the matrix $M=[B, I]$ and row reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 3 & -4 & 1 & 0 & 0 \\\\\\\\\\n1 & 5 & -1 & 0 & 1 & 0 \\\\\\\\\\n3 & 13 & -6 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 3 & -4 & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & 3 & -1 & 1 & 0 \\\\\\\\\\n0 & 4 & 6 & -3 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 3 & -4 & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & 3 & -1 & 1 & 0 \\\\\\\\\\n0 & 0 & 0 & -1 & -2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nIn echelon form, $M$ has a zero row in its left half; that is, $B$ is not row reducible to triangular form. Accordingly, $B$ has no inverse.\\n',\n",
       " '\\n3.33. Show that every elementary matrix $E$ is invertible, and its inverse is an elementary matrix.\\n\\nLet $E$ be the elementary matrix corresponding to the elementary operation $e$; that is, $e(I)=E$. Let $e^{\\\\prime}$ be the inverse operation of $e$ and let $E^{\\\\prime}$ be the corresponding elementary matrix; that is, $e^{\\\\prime}(I)=E^{\\\\prime}$. Then\\n\\n$$\\nI=e^{\\\\prime}(e(I))=e^{\\\\prime}(E)=E^{\\\\prime} E \\\\quad \\\\text { and } \\\\quad I=e\\\\left(e^{\\\\prime}(I)\\\\right)=e\\\\left(E^{\\\\prime}\\\\right)=E E^{\\\\prime}\\n$$\\n\\nTherefore, $E^{\\\\prime}$ is the inverse of $E$.\\n',\n",
       " '\\n3.34. Prove Theorem 3.16: Let $e$ be an elementary row operation and let $E$ be the corresponding $m$-square elementary matrix; that is, $E=e(I)$. Then $e(A)=E A$, where $A$ is any $m \\\\times n$ matrix.\\n\\nLet $R_{i}$ be the row $i$ of $A$; we denote this by writing $A=\\\\left[R_{1}, \\\\ldots, R_{m}\\\\right]$. If $B$ is a matrix for which $A B$ is defined then $A B=\\\\left[R_{1} B, \\\\ldots, R_{m} B\\\\right]$. We also let\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-110(1)}\\n\\\\end{center}\\n\\nHere ${ }^{\\\\wedge}=i$ means 1 is the $i$ th entry. One can show (Problem 2.45) that $e_{i} A=R_{i}$. We also note that $I=\\\\left[e_{1}, e_{2}, \\\\ldots, e_{m}\\\\right]$ is the identity matrix.\\n\\n(i) Let $e$ be the elementary row operation \"Interchange rows $R_{i}$ and $R_{j}$.\" Then, for ${ }^{\\\\wedge}=i$ and ${ }^{\\\\hat{}}=j$,\\n\\n$$\\nE=e(I)=\\\\left[e_{1}, \\\\ldots, \\\\widehat{e}_{j}, \\\\ldots, \\\\widehat{\\\\widehat{e}}_{i}, \\\\ldots, e_{m}\\\\right]\\n$$\\n\\nand\\n\\n$$\\ne(A)=\\\\left[R_{1}, \\\\ldots, \\\\widehat{R}_{j}, \\\\ldots, \\\\widehat{\\\\hat{R}}_{i}, \\\\ldots, R_{m}\\\\right]\\n$$\\n\\nThus,\\n\\n$$\\nE A=\\\\left[e_{1} A, \\\\ldots, \\\\widehat{e_{j} A}, \\\\ldots, \\\\widehat{e_{i} A}, \\\\ldots, e_{m} A\\\\right]=\\\\left[R_{1}, \\\\ldots, \\\\widehat{R}_{j}, \\\\ldots, \\\\widehat{\\\\widehat{R}}_{i}, \\\\ldots, R_{m}\\\\right]=e(A)\\n$$\\n\\n(ii) Let $e$ be the elementary row operation \"Replace $R_{i}$ by $k R_{i}(k \\\\neq 0)$.\" Then, for ${ }^{\\\\wedge}=i$,\\n\\n$$\\nE=e(I)=\\\\left[e_{1}, \\\\ldots, \\\\widehat{k e}_{i}, \\\\ldots, e_{m}\\\\right]\\n$$\\n\\nand\\n\\n$$\\ne(A)=\\\\left[R_{1}, \\\\ldots, \\\\widehat{k R}_{i}, \\\\ldots, R_{m}\\\\right]\\n$$\\n\\nThus,\\n\\n$$\\nE A=\\\\left[e_{1} A, \\\\ldots, \\\\widehat{k e_{i} A}, \\\\ldots, e_{m} A\\\\right]=\\\\left[R_{1}, \\\\ldots, \\\\widehat{k R_{i}}, \\\\ldots, R_{m}\\\\right]=e(A)\\n$$\\n\\n(iii) Let $e$ be the elementary row operation \"Replace $R_{i}$ by $k R_{j}+R_{i}$.\" Then, for ${ }^{\\\\wedge}=i$,\\n\\n$$\\nE=e(I)=\\\\left[e_{1}, \\\\ldots, k \\\\widehat{e}_{j} \\\\widehat{+e}_{i}, \\\\ldots, e_{m}\\\\right]\\n$$\\n\\nand\\n\\n$$\\ne(A)=\\\\left[R_{1}, \\\\ldots, k R_{j} \\\\widehat{+R} R_{i}, \\\\ldots, R_{m}\\\\right]\\n$$\\n\\nUsing $\\\\left(k e_{j}+e_{i}\\\\right) A=k\\\\left(e_{j} A\\\\right)+e_{i} A=k R_{j}+R_{i}$, we have\\n\\n$$\\n\\\\begin{aligned}\\n& E A=\\\\left[\\\\begin{array}{lllll}\\ne_{1} A, & \\\\ldots, & \\\\left(k e_{j}+e_{i}\\\\right) A, & \\\\ldots, & e_{m} A\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-110}\\n\\\\end{center}\\n',\n",
       " '\\n3.35. Prove Theorem 3.17: Let $A$ be a square matrix. Then the following are equivalent:\\n\\n(a) $A$ is invertible (nonsingular).\\n\\n(b) $A$ is row equivalent to the identity matrix $I$.\\n\\n(c) $A$ is a product of elementary matrices.\\n\\nSuppose $A$ is invertible and suppose $A$ is row equivalent to matrix $B$ in row canonical form. Then there exist elementary matrices $E_{1}, E_{2}, \\\\ldots, E_{s}$ such that $E_{s} \\\\ldots E_{2} E_{1} A=B$. Because $A$ is invertible and each elementary matrix is invertible, $B$ is also invertible. But if $B \\\\neq I$, then $B$ has a zero row; whence $B$ is not invertible. Thus, $B=I$, and (a) implies (b).\\n\\nIf (b) holds, then there exist elementary matrices $E_{1}, E_{2}, \\\\ldots, E_{s}$ such that $E_{s} \\\\ldots E_{2} E_{1} A=I$. Hence, $A=\\\\left(E_{s} \\\\ldots E_{2} E_{1}\\\\right)^{-1}=E_{1}^{-1} E_{2}^{-1} \\\\ldots, E_{s}^{-1}$. But the $E_{i}^{-1}$ are also elementary matrices. Thus (b) implies (c).\\n\\nIf (c) holds, then $A=E_{1} E_{2} \\\\ldots E_{s}$. The $E_{i}$ are invertible matrices; hence, their product $A$ is also invertible. Thus, (c) implies (a). Accordingly, the theorem is proved.\\n',\n",
       " '\\n3.36. Prove Theorem 3.18: If $A B=I$, then $B A=I$, and hence $B=A^{-1}$.\\n\\nSuppose $A$ is not invertible. Then $A$ is not row equivalent to the identity matrix $I$, and so $A$ is row equivalent to a matrix with a zero row. In other words, there exist elementary matrices $E_{1}, \\\\ldots, E_{s}$ such that $E_{s} \\\\ldots E_{2} E_{1} A$ has a zero row. Hence, $E_{s} \\\\ldots E_{2} E_{1} A B=E_{s} \\\\ldots E_{2} E_{1}$, an invertible matrix, also has a zero row. But invertible matrices cannot have zero rows; hence $A$ is invertible, with inverse $A^{-1}$. Then also,\\n\\n$$\\nB=I B=\\\\left(A^{-1} A\\\\right) B=A^{-1}(A B)=A^{-1} I=A^{-1}\\n$$\\n',\n",
       " '\\n3.37. Prove Theorem 3.19: $B$ is row equivalent to $A$ (written $B \\\\sim A$ ) if and only if there exists a nonsingular matrix $P$ such that $B=P A$.\\n\\nIf $B \\\\sim A$, then $B=e_{s}\\\\left(\\\\ldots\\\\left(e_{2}\\\\left(e_{1}(A)\\\\right)\\\\right) \\\\ldots\\\\right)=E_{s} \\\\ldots E_{2} E_{1} A=P A$ where $P=E_{s} \\\\ldots E_{2} E_{1}$ is nonsingular. Conversely, suppose $B=P A$, where $P$ is nonsingular. By Theorem 3.17, $P$ is a product of elementary matrices, and so $B$ can be obtained from $A$ by a sequence of elementary row operations; that is, $B \\\\sim A$. Thus, the theorem is proved.\\n',\n",
       " '\\n3.38. Prove Theorem 3.21: Every $m \\\\times n$ matrix $A$ is equivalent to a unique block matrix of the form $\\\\left[\\\\begin{array}{cc}I_{r} & 0 \\\\\\\\ 0 & 0\\\\end{array}\\\\right]$, where $I_{r}$ is the $r \\\\times r$ identity matrix.\\n\\nThe proof is constructive, in the form of an algorithm.\\n\\nStep 1. Row reduce $A$ to row canonical form, with leading nonzero entries $a_{1 j_{1}}, a_{2 j_{2}}, \\\\ldots, a_{r j_{r}}$.\\n\\nStep 2. Interchange $C_{1}$ and $C_{1 j_{1}}$, interchange $C_{2}$ and $C_{2 j_{2}}, \\\\ldots$, and interchange $C_{r}$ and $C_{j r}$. This gives a matrix in the form $\\\\left[\\\\begin{array}{ccc}I_{r} & B \\\\\\\\ \\\\hdashline 0 & -1 & 0\\\\end{array}\\\\right]$, with leading nonzero entries $a_{11}, a_{22}, \\\\ldots, a_{r r}$.\\n\\nStep 3. Use column operations, with the $a_{i i}$ as pivots, to replace each entry in $B$ with a zero; that is, for $i=1,2, \\\\ldots, r$ and $j=r+1, r+2, \\\\ldots, n$, apply the operation $-b_{i j} C_{i}+C_{j} \\\\rightarrow C_{j}$.\\n\\nThe final matrix has the desired form $\\\\left[\\\\begin{array}{c:c}I_{r} & 0 \\\\\\\\ \\\\hdashline 0 & 0\\\\end{array}\\\\right]$.\\n\\nLu Factorization\\\\\\\\\\n',\n",
       " '3.39. Find the $\\\\mathrm{LU}$ factorization of (a) $A=\\\\left[\\\\begin{array}{rrr}1 & -3 & 5 \\\\\\\\ 2 & -4 & 7 \\\\\\\\ -1 & -2 & 1\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{rrr}1 & 4 & -3 \\\\\\\\ 2 & 8 & 1 \\\\\\\\ -5 & -9 & 7\\\\end{array}\\\\right]$.\\n\\n(a) Reduce $A$ to triangular form by the following operations:\\n\\n$$\\n\\\\begin{gathered}\\n\\\\text { \"Replace } R_{2} \\\\text { by }-2 R_{1}+R_{2}, \" \\\\quad \\\\text { \"Replace } R_{3} \\\\text { by } R_{1}+R_{3}, \" \\\\quad \\\\text { and then } \\\\\\\\\\n\\\\text { \"Replace } R_{3} \\\\text { by } \\\\frac{5}{2} R_{2}+R_{3} \"\\n\\\\end{gathered}\\n$$\\n\\nThese operations yield the following, where the triangular form is $U$ :\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & -3 & 5 \\\\\\\\\\n0 & 2 & -3 \\\\\\\\\\n0 & -5 & 6\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & -3 & 5 \\\\\\\\\\n0 & 2 & -3 \\\\\\\\\\n0 & 0 & -\\\\frac{3}{2}\\n\\\\end{array}\\\\right]=U \\\\quad \\\\text { and } \\\\quad L=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n2 & 1 & 0 \\\\\\\\\\n-1 & -\\\\frac{5}{2} & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe entries 2, $-1,-\\\\frac{5}{2}$ in $L$ are the negatives of the multipliers $-2,1, \\\\frac{5}{2}$ in the above row operations. (As a check, multiply $L$ and $U$ to verify $A=L U$.)\\\\\\\\\\n(b) Reduce $B$ to triangular form by first applying the operations \"Replace $R_{2}$ by $-2 R_{1}+R_{2}$ \" and \"Replace $R_{3}$ by $5 R_{1}+R_{3}$.\" These operations yield\\n\\n$$\\nB \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 4 & -3 \\\\\\\\\\n0 & 0 & 7 \\\\\\\\\\n0 & 11 & -8\\n\\\\end{array}\\\\right]\\n$$\\n\\nObserve that the second diagonal entry is 0 . Thus, $B$ cannot be brought into triangular form without row interchange operations. Accordingly, $B$ is not $L U$-factorable. (There does exist a $P L U$ factorization of such a matrix $B$, where $P$ is a permutation matrix, but such a factorization lies beyond the scope of this text.)\\n',\n",
       " \"\\n3.40. Find the $L D U$ factorization of the matrix $A$ in Problem 3.39.\\n\\nThe $A=L D U$ factorization refers to the situation where $L$ is a lower triangular matrix with 1 's on the diagonal (as in the $L U$ factorization of $A$ ), $D$ is a diagonal matrix, and $U$ is an upper triangular matrix with 1 's on the diagonal. Thus, simply factor out the diagonal entries in the matrix $U$ in the above $L U$ factorization of $A$ to obtain $D$ and $L$. That is,\\n\\n$$\\nL=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n2 & 1 & 0 \\\\\\\\\\n-1 & -\\\\frac{5}{2} & 1\\n\\\\end{array}\\\\right], \\\\quad D=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n0 & 2 & 0 \\\\\\\\\\n0 & 0 & -\\\\frac{3}{2}\\n\\\\end{array}\\\\right], \\\\quad U=\\\\left[\\\\begin{array}{rrr}\\n1 & -3 & 5 \\\\\\\\\\n0 & 1 & -3 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\",\n",
       " '\\n3.41. Find the $L U$ factorization of the matrix $A=\\\\left[\\\\begin{array}{rrr}1 & 2 & 1 \\\\\\\\ 2 & 3 & 3 \\\\\\\\ -3 & -10 & 2\\\\end{array}\\\\right]$.\\n\\nReduce $A$ to triangular form by the following operations:\\n\\n(1) \"Replace $R_{2}$ by $-2 R_{1}+R_{2}$,\" (2) \"Replace $R_{3}$ by $3 R_{1}+R_{3}$,\" (3) \"Replace $R_{3}$ by $-4 R_{2}+R_{3}$ \"\\n\\nThese operations yield the following, where the triangular form is $U$ :\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 1 \\\\\\\\\\n0 & -1 & 1 \\\\\\\\\\n0 & -4 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 1 \\\\\\\\\\n0 & -1 & 1 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]=U \\\\quad \\\\text { and } \\\\quad L=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n2 & 1 & 0 \\\\\\\\\\n-3 & 4 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe entries 2,-3,4 in $L$ are the negatives of the multipliers $-2,3,-4$ in the above row operations. (As a check, multiply $L$ and $U$ to verify $A=L U$.)\\n',\n",
       " '\\n3.42. Let $A$ be the matrix in Problem 3.41. Find $X_{1}, X_{2}, X_{3}$, where $X_{i}$ is the solution of $A X=B_{i}$ for (a) $B_{1}=(1,1,1)$, (b) $B_{2}=B_{1}+X_{1}$, (c) $B_{3}=B_{2}+X_{2}$.\\n\\n(a) Find $L^{-1} B_{1}$ by applying the row operations (1), (2), and then (3) in Problem 3.41 to $B_{1}$ :\\n\\n$$\\nB_{1}=\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right] \\\\xrightarrow{(1) \\\\text { and }(2)}\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-1 \\\\\\\\\\n4\\n\\\\end{array}\\\\right] \\\\xrightarrow{(3)}\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-1 \\\\\\\\\\n8\\n\\\\end{array}\\\\right]\\n$$\\n\\nSolve $U X=B$ for $B=(1,-1,8)$ by back-substitution to obtain $X_{1}=(-25,9,8)$.\\n\\n(b) First find $B_{2}=B_{1}+X_{1}=(1,1,1)+(-25,9,8)=(-24,10,9)$. Then as above\\n\\n$$\\nB_{2}=[-24,10,9]^{T} \\\\xrightarrow{(1) \\\\text { and }(2)}[-24,58,-63]^{T} \\\\xrightarrow{(3)}[-24,58,-295]^{T}\\n$$\\n\\nSolve $U X=B$ for $B=(-24,58,-295)$ by back-substitution to obtain $X_{2}=(943,-353,-295)$.\\n\\n(c) First find $B_{3}=B_{2}+X_{2}=(-24,10,9)+(943,-353,-295)=(919,-343,-286)$. Then, as above\\n\\n$$\\nB_{3}=[943,-353,-295]^{T} \\\\xrightarrow{(1) \\\\text { and }(2)}[919,-2181,2671]^{T} \\\\xrightarrow{(3)}[919,-2181,11395]^{T}\\n$$\\n\\nSolve $U X=B$ for $B=(919,-2181,11395)$ by back-substitution to obtain\\n\\n$$\\nX_{3}=(-37628,13576,11395)\\n$$\\n\\n\\n\\\\section*{Miscellaneous Problems}\\n',\n",
       " '3.43. Let $L$ be a linear combination of the $m$ equations in $n$ unknowns in the system (3.2). Say $L$ is the equation\\n\\n\\n\\\\begin{equation*}\\n\\\\left(c_{1} a_{11}+\\\\cdots+c_{m} a_{m 1}\\\\right) x_{1}+\\\\cdots+\\\\left(c_{1} a_{1 n}+\\\\cdots+c_{m} a_{m n}\\\\right) x_{n}=c_{1} b_{1}+\\\\cdots+c_{m} b_{m} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nShow that any solution of the system (3.2) is also a solution of $L$.\\n\\nLet $u=\\\\left(k_{1}, \\\\ldots, k_{n}\\\\right)$ be a solution of (3.2). Then\\n\\n\\n\\\\begin{equation*}\\na_{i 1} k_{1}+a_{i 2} k_{2}+\\\\cdots+a_{i n} k_{n}=b_{i} \\\\quad(i=1,2, \\\\ldots, m) \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nSubstituting $u$ in the left-hand side of (1) and using (2), we get\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(c_{1} a_{11}\\\\right. & \\\\left.+\\\\cdots+c_{m} a_{m 1}\\\\right) k_{1}+\\\\cdots+\\\\left(c_{1} a_{1 n}+\\\\cdots+c_{m} a_{m n}\\\\right) k_{n} \\\\\\\\\\n& =c_{1}\\\\left(a_{11} k_{1}+\\\\cdots+a_{1 n} k_{n}\\\\right)+\\\\cdots+c_{m}\\\\left(a_{m 1} k_{1}+\\\\cdots+a_{m n} k_{n}\\\\right) \\\\\\\\\\n& =c_{1} b_{1}+\\\\cdots+c_{m} b_{m}\\n\\\\end{aligned}\\n$$\\n\\nThis is the right-hand side of (1); hence, $u$ is a solution of (1).\\n',\n",
       " '\\n3.44. Suppose a system $\\\\mathscr{M}$ of linear equations is obtained from a system $\\\\mathscr{L}$ by applying an elementary operation (page 64). Show that $\\\\mathscr{M}$ and $\\\\mathscr{L}$ have the same solutions.\\n\\nEach equation $L$ in $\\\\mathscr{M}$ is a linear combination of equations in $\\\\mathscr{L}$. Hence, by Problem 3.43, any solution of $\\\\mathscr{L}$ will also be a solution of $\\\\mathscr{M}$. On the other hand, each elementary operation has an inverse elementary operation, so $\\\\mathscr{L}$ can be obtained from $\\\\mathscr{M}$ by an elementary operation. This means that any solution of $\\\\mathscr{M}$ is a solution of $\\\\mathscr{L}$. Thus, $\\\\mathscr{L}$ and $\\\\mathscr{M}$ have the same solutions.\\n',\n",
       " '\\n3.45. Prove Theorem 3.4: Suppose a system $\\\\mathscr{M}$ of linear equations is obtained from a system $\\\\mathscr{L}$ by a sequence of elementary operations. Then $\\\\mathscr{M}$ and $\\\\mathscr{L}$ have the same solutions.\\n\\nEach step of the sequence does not change the solution set (Problem 3.44). Thus, the original system $\\\\mathscr{L}$ and the final system $\\\\mathscr{M}$ (and any system in between) have the same solutions.\\n',\n",
       " '\\n3.46. A system $\\\\mathscr{L}$ of linear equations is said to be consistent if no linear combination of its equations is a degenerate equation $L$ with a nonzero constant. Show that $\\\\mathscr{L}$ is consistent if and only if $\\\\mathscr{L}$ is reducible to echelon form.\\n\\nSuppose $\\\\mathscr{L}$ is reducible to echelon form. Then $\\\\mathscr{L}$ has a solution, which must also be a solution of every linear combination of its equations. Thus, $L$, which has no solution, cannot be a linear combination of the equations in $\\\\mathscr{L}$. Thus, $\\\\mathscr{L}$ is consistent.\\n\\nOn the other hand, suppose $\\\\mathscr{L}$ is not reducible to echelon form. Then, in the reduction process, it must yield a degenerate equation $L$ with a nonzero constant, which is a linear combination of the equations in $\\\\mathscr{L}$. Therefore, $\\\\mathscr{L}$ is not consistent; that is, $\\\\mathscr{L}$ is inconsistent.\\n',\n",
       " '\\n3.47. Suppose $u$ and $v$ are distinct vectors. Show that, for distinct scalars $k$, the vectors $u+k(u-v)$ are distinct.\\n\\nSuppose $u+k_{1}(u-v)=u+k_{2}(u-v)$. We need only show that $k_{1}=k_{2}$. We have\\n\\n$$\\nk_{1}(u-v)=k_{2}(u-v), \\\\quad \\\\text { and so } \\\\quad\\\\left(k_{1}-k_{2}\\\\right)(u-v)=0\\n$$\\n\\nBecause $u$ and $v$ are distinct, $u-v \\\\neq 0$. Hence, $k_{1}-k_{2}=0$, and so $k_{1}=k_{2}$.\\n',\n",
       " '\\n3.48. Suppose $A B$ is defined. Prove\\n\\n(a) Suppose $A$ has a zero row. Then $A B$ has a zero row.\\n\\n(b) Suppose $B$ has a zero column. Then $A B$ has a zero column.\\\\\\\\\\n(a) Let $R_{i}$ be the zero row of $A$, and $C_{1}, \\\\ldots, C_{n}$ the columns of $B$. Then the $i$ th row of $A B$ is\\n\\n$$\\n\\\\left(R_{i} C_{1}, R_{i} C_{2}, \\\\ldots, R_{i} C_{n}\\\\right)=(0,0,0, \\\\ldots, 0)\\n$$\\n\\n(b) $B^{T}$ has a zero row, and so $B^{T} A^{T}=(A B)^{T}$ has a zero row. Hence, $A B$ has a zero column.\\n\\n',\n",
       " '4.1. Suppose $u$ and $v$ belong to a vector space $V$. Simplify each of the following expressions:\\\\\\\\\\n(a) $E_{1}=3(2 u-4 v)+5 u+7 v$,\\\\\\\\\\n(c) $E_{3}=2 u v+3(2 u+4 v)$\\\\\\\\\\n(b) $E_{2}=3 u-6(3 u-5 v)+7 u$,\\\\\\\\\\n(d) $E_{4}=5 u-\\\\frac{3}{v}+5 u$\\n\\nMultiply out and collect terms:\\n\\n(a) $E_{1}=6 u-12 v+5 u+7 v=11 u-5 v$\\n\\n(b) $E_{2}=3 u-18 u+30 v+7 u=-8 u+30 v$\\n\\n(c) $E_{3}$ is not defined because the product $u v$ of vectors is not defined.\\n\\n(d) $E_{4}$ is not defined because division by a vector is not defined.\\n',\n",
       " '\\n4.2. Prove Theorem 4.1: Let $V$ be a vector space over a field $K$.\\n\\n(i) $k 0=0$. (ii) $0 u=0$. (iii) If $k u=0$, then $k=0$ or $u=0$. (iv) $(-k) u=k(-u)=-k u$.\\n\\n(i) By Axiom $\\\\left[\\\\mathrm{A}_{2}\\\\right]$ with $u=0$, we have $0+0=0$. Hence, by Axiom $\\\\left[\\\\mathrm{M}_{1}\\\\right]$, we have\\n\\n$$\\nk 0=k(0+0)=k 0+k 0\\n$$\\n\\nAdding $-k 0$ to both sides gives the desired result.\\n\\n(ii) For scalars, $0+0=0$. Hence, by Axiom $\\\\left[\\\\mathrm{M}_{2}\\\\right]$, we have\\n\\n$$\\n0 u=(0+0) u=0 u+0 u\\n$$\\n\\nAdding $-0 u$ to both sides gives the desired result.\\n\\n(iii) Suppose $k u=0$ and $k \\\\neq 0$. Then there exists a scalar $k^{-1}$ such that $k^{-1} k=1$. Thus,\\n\\n$$\\nu=1 u=\\\\left(k^{-1} k\\\\right) u=k^{-1}(k u)=k^{-1} 0=0\\n$$\\n\\n(iv) Using $u+(-u)=0$ and $k+(-k)=0$ yields\\n\\n$$\\n0=k 0=k[u+(-u)]=k u+k(-u) \\\\quad \\\\text { and } \\\\quad 0=0 u=[k+(-k)] u=k u+(-k) u\\n$$\\n\\nAdding $-k u$ to both sides of the first equation gives $-k u=k(-u)$, and adding $-k u$ to both sides of the second equation gives $-k u=(-k) u$. Thus, $(-k) u=k(-u)=-k u$.\\n',\n",
       " '\\n4.3. Show that (a) $k(u-v)=k u-k v$, (b) $u+u=2 u$.\\n\\n(a) Using the definition of subtraction, that $u-v=u+(-v)$, and Theorem 4.1(iv), that $k(-v)=-k v$, we have\\n\\n$$\\nk(u-v)=k[u+(-v)]=k u+k(-v)=k u+(-k v)=k u-k v\\n$$\\n\\n(b) Using Axiom $\\\\left[\\\\mathrm{M}_{4}\\\\right]$ and then Axiom $\\\\left[\\\\mathrm{M}_{2}\\\\right]$, we have\\n\\n$$\\nu+u=1 u+1 u=(1+1) u=2 u\\n$$\\n',\n",
       " '\\n4.4. Express $v=(1,-2,5)$ in $\\\\mathbf{R}^{3}$ as a linear combination of the vectors\\n\\n$$\\nu_{1}=(1,1,1), \\\\quad u_{2}=(1,2,3), \\\\quad u_{3}=(2,-1,1)\\n$$\\n\\nWe seek scalars $x, y, z$, as yet unknown, such that $v=x u_{1}+y u_{2}+z u_{3}$. Thus, we require\\n\\n$$\\n\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-2 \\\\\\\\\\n5\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n& x+y+2 z=1 \\\\\\\\\\n& x+2 y-z=-2 \\\\\\\\\\n& x+3 y+z=5\\n\\\\end{aligned}\\n$$\\n\\n(For notational convenience, we write the vectors in $\\\\mathbf{R}^{3}$ as columns, because it is then easier to find the equivalent system of linear equations.) Reducing the system to echelon form yields the triangular system\\n\\n$$\\nx+y+2 z=1, \\\\quad y-3 z=-3, \\\\quad 5 z=10\\n$$\\n\\nThe system is consistent and has a solution. Solving by back-substitution yields the solution $x=-6, y=3$, $z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.\\n\\nAlternatively, write down the augmented matrix $M$ of the equivalent system of linear equations, where $u_{1}, u_{2}, u_{3}$ are the first three columns of $M$ and $v$ is the last column, and then reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 1 \\\\\\\\\\n1 & 2 & -1 & -2 \\\\\\\\\\n1 & 3 & 1 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 1 \\\\\\\\\\n0 & 1 & -3 & -3 \\\\\\\\\\n0 & 2 & -1 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & 1 \\\\\\\\\\n0 & 1 & -3 & -3 \\\\\\\\\\n0 & 0 & 5 & 10\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe last matrix corresponds to a triangular system, which has a solution. Solving the triangular system by back-substitution yields the solution $x=-6, y=3, z=2$. Thus, $v=-6 u_{1}+3 u_{2}+2 u_{3}$.\\n',\n",
       " '\\n4.5. Express $v=(2,-5,3)$ in $\\\\mathbf{R}^{3}$ as a linear combination of the vectors\\n\\n$$\\nu_{1}=(1,-3,2), u_{2}=(2,-4,-1), u_{3}=(1,-5,7)\\n$$\\n\\nWe seek scalars $x, y, z$, as yet unknown, such that $v=x u_{1}+y u_{2}+z u_{3}$. Thus, we require\\n\\n$$\\n\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-5 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-3 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{r}\\n2 \\\\\\\\\\n-4 \\\\\\\\\\n-1\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-5 \\\\\\\\\\n7\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{array}{r}\\nx+2 y+z=2 \\\\\\\\\\n-3 x-4 y-5 z=-5 \\\\\\\\\\n2 x-y+7 z=3\\n\\\\end{array}\\n$$\\n\\nReducing the system to echelon form yields the system\\n\\n$$\\nx+2 y+z=2, \\\\quad 2 y-2 z=1, \\\\quad 0=3\\n$$\\n\\nThe system is inconsistent and so has no solution. Thus, $v$ cannot be written as a linear combination of $u_{1}, u_{2}, u_{3}$.\\n',\n",
       " '\\n4.6. Express the polynomial $v=t^{2}+4 t-3$ in $\\\\mathbf{P}(t)$ as a linear combination of the polynomials\\n\\n$$\\np_{1}=t^{2}-2 t+5, \\\\quad p_{2}=2 t^{2}-3 t, \\\\quad p_{3}=t+1\\n$$\\n\\nSet $v$ as a linear combination of $p_{1}, p_{2}, p_{3}$ using unknowns $x, y, z$ to obtain\\n\\n\\n\\\\begin{equation*}\\nt^{2}+4 t-3=x\\\\left(t^{2}-2 t+5\\\\right)+y\\\\left(2 t^{2}-3 t\\\\right)+z(t+1) \\\\tag{*}\\n\\\\end{equation*}\\n\\n\\nWe can proceed in two ways.\\n\\nMethod 1. Expand the right side of $(*)$ and express it in terms of powers of $t$ as follows:\\n\\n$$\\n\\\\begin{aligned}\\nt^{2}+4 t-3 & =x t^{2}-2 x t+5 x+2 y t^{2}-3 y t+z t+z \\\\\\\\\\n& =(x+2 y) t^{2}+(-2 x-3 y+z) t+(5 x+3 z)\\n\\\\end{aligned}\\n$$\\n\\nSet coefficients of the same powers of $t$ equal to each other, and reduce the system to echelon form. This yields\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-142}\\n\\\\end{center}\\n\\nThe system is consistent and has a solution. Solving by back-substitution yields the solution $x=-3, y=2$, $z=4$. Thus, $v=-3 p_{1}+2 p_{2}+4 p_{2}$.\\n\\nMethod 2. The equation $\\\\left({ }^{*}\\\\right)$ is an identity in $t$; that is, the equation holds for any value of $t$. Thus, we can set $t$ equal to any numbers to obtain equations in the unknowns.\\n\\n(a) Set $t=0$ in $\\\\left(^{*}\\\\right)$ to obtain the equation $-3=5 x+z$.\\n\\n(b) Set $t=1$ in $(*)$ to obtain the equation $2=4 x-y+2 z$.\\n\\n(c) Set $t=-1$ in (*) to obtain the equation $-6=8 x+5 y$.\\n\\nSolve the system of the three equations to again obtain the solution $x=-3, y=2, z=4$. Thus, $v=-3 p_{1}+2 p_{2}+4 p_{3}$.\\n',\n",
       " '\\n4.7. Express $M$ as a linear combination of the matrices $A, B, C$, where\\n\\n$M=\\\\left[\\\\begin{array}{ll}4 & 7 \\\\\\\\ 7 & 9\\\\end{array}\\\\right], \\\\quad$ and $\\\\quad A=\\\\left[\\\\begin{array}{ll}1 & 1 \\\\\\\\ 1 & 1\\\\end{array}\\\\right], \\\\quad B=\\\\left[\\\\begin{array}{ll}1 & 2 \\\\\\\\ 3 & 4\\\\end{array}\\\\right], \\\\quad C=\\\\left[\\\\begin{array}{ll}1 & 1 \\\\\\\\ 4 & 5\\\\end{array}\\\\right]$\\n\\nSet $M$ as a linear combination of $A, B, C$ using unknown scalars $x, y, z$; that is, set $M=x A+y B+z C$. This yields\\n\\n$$\\n\\\\left[\\\\begin{array}{ll}\\n4 & 7 \\\\\\\\\\n7 & 9\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n1 & 1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n4 & 5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\nx+y+z & x+2 y+z \\\\\\\\\\nx+3 y+4 z & x+4 y+5 z\\n\\\\end{array}\\\\right]\\n$$\\n\\nForm the equivalent system of equations by setting corresponding entries equal to each other:\\n\\n$$\\nx+y+z=4, \\\\quad x+2 y+z=7, \\\\quad x+3 y+4 z=7, \\\\quad x+4 y+5 z=9\\n$$\\n\\nReducing the system to echelon form yields\\n\\n$$\\nx+y+z=4, \\\\quad y=3, \\\\quad 3 z=-3, \\\\quad 4 z=-4\\n$$\\n\\nThe last equation drops out. Solving the system by back-substitution yields $z=-1, y=3, x=2$. Thus, $M=2 A+3 B-C$.\\n\\n\\n\\\\section*{Subspaces}\\n',\n",
       " '4.8. Prove Theorem 4.2: $W$ is a subspace of $V$ if the following two conditions hold:\\n\\n(a) $0 \\\\in W$. (b) If $u, v \\\\in W$, then $u+v$, $k u \\\\in W$.\\n\\nBy (a), $W$ is nonempty, and, by (b), the operations of vector addition and scalar multiplication are well defined for $W$. Axioms $\\\\left[\\\\mathrm{A}_{1}\\\\right],\\\\left[\\\\mathrm{A}_{4}\\\\right],\\\\left[\\\\mathrm{M}_{1}\\\\right],\\\\left[\\\\mathrm{M}_{2}\\\\right],\\\\left[\\\\mathrm{M}_{3}\\\\right],\\\\left[\\\\mathrm{M}_{4}\\\\right]$ hold in $W$ because the vectors in $W$ belong to $V$. Thus, we need only show that $\\\\left[\\\\mathrm{A}_{2}\\\\right]$ and $\\\\left[\\\\mathrm{A}_{3}\\\\right]$ also hold in $W$. Now $\\\\left[\\\\mathrm{A}_{2}\\\\right]$ holds because the zero vector in $V$ belongs to $W$ by (a). Finally, if $v \\\\in W$, then $(-1) v=-v \\\\in W$, and $v+(-v)=0$. Thus $\\\\left[\\\\mathrm{A}_{3}\\\\right]$ holds.\\n',\n",
       " '\\n4.9. Let $V=\\\\mathbf{R}^{3}$. Show that $W$ is not a subspace of $V$, where\\n\\n(a) $W=\\\\{(a, b, c): a \\\\geq 0\\\\}$, (b) $W=\\\\left\\\\{(a, b, c): a^{2}+b^{2}+c^{2} \\\\leq 1\\\\right\\\\}$.\\n\\nIn each case, show that Theorem 4.2 does not hold.\\\\\\\\\\n(a) $W$ consists of those vectors whose first entry is nonnegative. Thus, $v=(1,2,3)$ belongs to $W$. Let $k=-3$. Then $k v=(-3,-6,-9)$ does not belong to $W$, because -3 is negative. Thus, $W$ is not a subspace of $V$.\\n\\n(b) $W$ consists of vectors whose length does not exceed 1. Hence, $u=(1,0,0)$ and $v=(0,1,0)$ belong to $W$, but $u+v=(1,1,0)$ does not belong to $W$, because $1^{2}+1^{2}+0^{2}=2>1$. Thus, $W$ is not a subspace of $V$.\\n',\n",
       " '\\n4.10. Let $V=\\\\mathbf{P}(t)$, the vector space of real polynomials. Determine whether or not $W$ is a subspace of $V$, where\\n\\n(a) $W$ consists of all polynomials with integral coefficients.\\n\\n(b) $W$ consists of all polynomials with degree $\\\\geq 6$ and the zero polynomial.\\n\\n(c) $W$ consists of all polynomials with only even powers of $t$.\\n\\n(a) No, because scalar multiples of polynomials in $W$ do not always belong to $W$. For example,\\n\\n$$\\nf(t)=3+6 t+7 t^{2} \\\\in W \\\\quad \\\\text { but } \\\\quad \\\\frac{1}{2} f(t)=\\\\frac{3}{2}+3 t+\\\\frac{7}{2} t^{2} \\\\notin W\\n$$\\n\\n(b and c) Yes. In each case, $W$ contains the zero polynomial, and sums and scalar multiples of polynomials in $W$ belong to $W$.\\n',\n",
       " '\\n4.11. Let $V$ be the vector space of functions $f: \\\\mathbf{R} \\\\rightarrow \\\\mathbf{R}$. Show that $W$ is a subspace of $V$, where\\n\\n(a) $W=\\\\{f(x): f(1)=0\\\\}$, all functions whose value at 1 is 0 .\\n\\n(b) $W=\\\\{f(x): f(3)=f(1)\\\\}$, all functions assigning the same value to 3 and 1 .\\n\\n(c) $W=\\\\{f(t): f(-x)=-f(x)\\\\}$, all odd functions.\\n\\nLet $\\\\hat{0}$ denote the zero function, so $\\\\hat{0}(x)=0$ for every value of $x$.\\n\\n(a) $\\\\hat{0} \\\\in W$, because $\\\\hat{0}(1)=0$. Suppose $f, g \\\\in W$. Then $f(1)=0$ and $g(1)=0$. Also, for scalars $a$ and $b$, we have\\n\\n$$\\n(a f+b g)(1)=a f(1)+b g(1)=a 0+b 0=0\\n$$\\n\\nThus, $a f+b g \\\\in W$, and hence $W$ is a subspace.\\n\\n(b) $\\\\hat{0} \\\\in W$, because $\\\\hat{0}(3)=0=\\\\hat{0}(1)$. Suppose $f, g \\\\in W$. Then $f(3)=f(1)$ and $g(3)=g(1)$. Thus, for any scalars $a$ and $b$, we have\\n\\n$$\\n(a f+b g)(3)=a f(3)+b g(3)=a f(1)+b g(1)=(a f+b g)(1)\\n$$\\n\\nThus, $a f+b g \\\\in W$, and hence $W$ is a subspace.\\n\\n(c) $\\\\hat{0} \\\\in W$, because $\\\\hat{0}(-x)=0=-0=-\\\\hat{0}(x)$. Suppose $f, g \\\\in W$. Then $f(-x)=-f(x)$ and $g(-x)=-g(x)$. Also, for scalars $a$ and $b$,\\n\\n$$\\n(a f+b g)(-x)=a f(-x)+b g(-x)=-a f(x)-b g(x)=-(a f+b g)(x)\\n$$\\n\\nThus, $a b+g f \\\\in W$, and hence $W$ is a subspace of $V$.\\n',\n",
       " '\\n4.12. Prove Theorem 4.3: The intersection of any number of subspaces of $V$ is a subspace of $V$.\\n\\nLet $\\\\left\\\\{W_{i}: i \\\\in I\\\\right\\\\}$ be a collection of subspaces of $V$ and let $W=\\\\cap\\\\left(W_{i}: i \\\\in I\\\\right)$. Because each $W_{i}$ is a subspace of $V$, we have $0 \\\\in W_{i}$, for every $i \\\\in I$. Hence, $0 \\\\in W$. Suppose $u, v \\\\in W$. Then $u, v \\\\in W_{i}$, for every $i \\\\in I$. Because each $W_{i}$ is a subspace, $a u+b v \\\\in W_{i}$, for every $i \\\\in I$. Hence, $a u+b v \\\\in W$. Thus, $W$ is a subspace of $V$.\\n\\n\\n\\\\section*{Linear Spans}\\n',\n",
       " '4.13. Show that the vectors $u_{1}=(1,1,1), u_{2}=(1,2,3), u_{3}=(1,5,8)$ span $\\\\mathbf{R}^{3}$.\\n\\nWe need to show that an arbitrary vector $v=(a, b, c)$ in $\\\\mathbf{R}^{3}$ is a linear combination of $u_{1}, u_{2}, u_{3}$. Set $v=x u_{1}+y u_{2}+z u_{3}$; that is, set\\n\\n$$\\n(a, b, c)=x(1,1,1)+y(1,2,3)+z(1,5,8)=(x+y+z, \\\\quad x+2 y+5 z, \\\\quad x+3 y+8 z)\\n$$\\n\\nForm the equivalent system and reduce it to echelon form:\\n\\n$$\\n\\\\begin{aligned}\\n& x+y+z=a \\\\quad x+y+z=a \\\\quad x+y+z=a \\\\\\\\\\n& x+2 y+5 z=b \\\\quad \\\\text { or } \\\\quad y+4 z=b-a \\\\quad \\\\text { or } \\\\\\\\\\n& x+3 y+8 z=c \\\\quad 2 y+7 c=c-a \\\\quad-z=c-2 b+a\\n\\\\end{aligned}\\n$$\\n\\nThe above system is in echelon form and is consistent; in fact,\\n\\n$$\\nx=-a+5 b-3 c, y=3 a-7 b+4 c, z=a+2 b-c\\n$$\\n\\nis a solution. Thus, $u_{1}, u_{2}, u_{3}$ span $\\\\mathbf{R}^{3}$.\\n',\n",
       " '\\n4.14. Find conditions on $a, b, c$ so that $v=(a, b, c)$ in $\\\\mathbf{R}^{3}$ belongs to $W=\\\\operatorname{span}\\\\left(u_{1}, u_{2}, u_{3}\\\\right)$, where\\n\\n$$\\nu_{1}=(1,2,0), u_{2}=(-1,1,2), u_{3}=(3,0,-4)\\n$$\\n\\nyields\\n\\nSet $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknowns $x, y, z$; that is, set $v=x u_{1}+y u_{2}+z u_{3}$. This\\n\\n$$\\n(a, b, c)=x(1,2,0)+y(-1,1,2)+z(3,0,-4)=(x-y+3 z, \\\\quad 2 x+y, \\\\quad 2 y-4 z)\\n$$\\n\\nForm the equivalent system of linear equations and reduce it to echelon form:\\n\\n$$\\n\\\\begin{aligned}\\n& x-y+3 z=a \\\\\\\\\\n& x-y+3 z=a \\\\quad x-y+3 z=a \\\\\\\\\\n& 2 x+y \\\\quad=b \\\\quad \\\\text { or } \\\\quad 3 y-6 z=b-2 a \\\\quad \\\\text { or } \\\\quad 3 y-6 z=b-2 a \\\\\\\\\\n& 2 y-4 z=c \\\\quad 2 y-4 z=c \\\\quad 0=4 a-2 b+3 c\\n\\\\end{aligned}\\n$$\\n\\nThe vector $v=(a, b, c)$ belongs to $W$ if and only if the system is consistent, and it is consistent if and only if $4 a-2 b+3 c=0$. Note, in particular, that $u_{1}, u_{2}, u_{3}$ do not span the whole space $\\\\mathbf{R}^{3}$.\\n',\n",
       " '\\n4.15. Show that the vector space $V=\\\\mathbf{P}(t)$ of real polynomials cannot be spanned by a finite number of polynomials.\\n\\nAny finite set $S$ of polynomials contains a polynomial of maximum degree, say $m$. Then the linear span $\\\\operatorname{span}(\\\\mathrm{S})$ of $S$ cannot contain a polynomial of degree greater than $m$. Thus, $\\\\operatorname{span}(S) \\\\neq V$, for any finite set $S$.\\n',\n",
       " '\\n4.16. Prove Theorem 4.5: Let $S$ be a subset of $V$. (i) Then $\\\\operatorname{span}(\\\\mathrm{S})$ is a subspace of $V$ containing $S$. (ii) If $W$ is a subspace of $V$ containing $S$, then $\\\\operatorname{span}(S) \\\\subseteq W$.\\n\\n(i) Suppose $S$ is empty. By definition, $\\\\operatorname{span}(S)=\\\\{0\\\\}$. Hence $\\\\operatorname{span}(S)=\\\\{0\\\\}$ is a subspace of $V$ and $S \\\\subseteq \\\\operatorname{span}(S)$. Suppose $S$ is not empty and $v \\\\in S$. Then $v=1 v \\\\in \\\\operatorname{span}(S)$; hence, $S \\\\subseteq \\\\operatorname{span}(S)$. Also $0=0 v \\\\in \\\\operatorname{span}(S)$. Now suppose $u, w \\\\in \\\\operatorname{span}(S)$, say\\n\\n$$\\nu=a_{1} u_{1}+\\\\cdots+a_{r} u_{r}=\\\\sum_{i} a_{i} u_{i} \\\\quad \\\\text { and } \\\\quad w=b_{1} w_{1}+\\\\cdots+b_{s} w_{s}=\\\\sum_{j} b_{j} w_{j}\\n$$\\n\\nwhere $u_{i}, w_{j} \\\\in S$ and $a_{i}, b_{j} \\\\in K$. Then\\n\\n$$\\nu+v=\\\\sum_{i} a_{i} u_{i}+\\\\sum_{j} b_{j} w_{j} \\\\quad \\\\text { and } \\\\quad k u=k\\\\left(\\\\sum_{i} a_{i} u_{i}\\\\right)=\\\\sum_{i} k a_{i} u_{i}\\n$$\\n\\nbelong to $\\\\operatorname{span}(\\\\mathrm{S})$ because each is a linear combination of vectors in $S$. Thus, $\\\\operatorname{span}(\\\\mathrm{S})$ is a subspace of $V$.\\n\\n(ii) Suppose $u_{1}, u_{2}, \\\\ldots, u_{r} \\\\in S$. Then all the $u_{i}$ belong to $W$. Thus, all multiples $a_{1} u_{1}, a_{2} u_{2}, \\\\ldots, a_{r} u_{r} \\\\in W$, and so the sum $a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{r} u_{r} \\\\in W$. That is, $W$ contains all linear combinations of elements in $S$, or, in other words, $\\\\operatorname{span}(S) \\\\subseteq W$, as claimed.\\n\\n\\n\\\\section*{Linear Dependence}\\n',\n",
       " '4.17. Determine whether or not $u$ and $v$ are linearly dependent, where\\\\\\\\\\n(a) $u=(1,2), v=(3,-5)$\\\\\\\\\\n(c) $u=(1,2,-3), v=(4,5,-6)$\\\\\\\\\\n(b) $u=(1,-3), v=(-2,6)$,\\\\\\\\\\n(d) $u=(2,4,-8), v=(3,6,-12)$\\n\\nTwo vectors $u$ and $v$ are linearly dependent if and only if one is a multiple of the other.\\\\\\\\\\n(a) No. (b) Yes; for $v=-2 u$.\\\\\\\\\\n(c) No\\\\\\\\\\n(d) Yes, for $v=\\\\frac{3}{2} u$.\\n',\n",
       " '\\n4.18. Determine whether or not $u$ and $v$ are linearly dependent, where\\n\\n(a) $u=2 t^{2}+4 t-3, v=4 t^{2}+8 t-6$,\\n\\n(c) $u=\\\\left[\\\\begin{array}{rrr}1 & 3 & -4 \\\\\\\\ 5 & 0 & -1\\\\end{array}\\\\right], v=\\\\left[\\\\begin{array}{rrr}-4 & -12 & 16 \\\\\\\\ -20 & 0 & 4\\\\end{array}\\\\right]$, (b) $u=2 t^{2}-3 t+4, v=4 t^{2}-3 t+2$,\\n\\n(d) $u=\\\\left[\\\\begin{array}{lll}1 & 1 & 1 \\\\\\\\ 2 & 2 & 2\\\\end{array}\\\\right], v=\\\\left[\\\\begin{array}{lll}2 & 2 & 2 \\\\\\\\ 3 & 3 & 3\\\\end{array}\\\\right]$\\n\\nTwo vectors $u$ and $v$ are linearly dependent if and only if one is a multiple of the other.\\n\\n(a) Yes; for $v=2 u$. (b) No. (c) Yes, for $v=-4 u$. (d) No.\\n',\n",
       " '\\n4.19. Determine whether or not the vectors $u=(1,1,2), v=(2,3,1), w=(4,5,5)$ in $\\\\mathbf{R}^{3}$ are linearly dependent.\\n\\nMethod 1. Set a linear combination of $u, v, w$ equal to the zero vector using unknowns $x, y, z$ to obtain the equivalent homogeneous system of linear equations and then reduce the system to echelon form. This yields\\n\\n$$\\nx\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n2 \\\\\\\\\\n3 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{l}\\n4 \\\\\\\\\\n5 \\\\\\\\\\n5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n0 \\\\\\\\\\n0 \\\\\\\\\\n0\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+2 y+4 z & =0 \\\\\\\\\\nx+3 y+5 z & =0 \\\\\\\\\\n2 x+y+5 z & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+2 y+4 z & =0 \\\\\\\\\\ny+z & =0\\n\\\\end{aligned}\\n$$\\n\\nThe echelon system has only two nonzero equations in three unknowns; hence, it has a free variable and a nonzero solution. Thus, $u, v, w$ are linearly dependent.\\n\\nMethod 2. Form the matrix $A$ whose columns are $u, v, w$ and reduce to echelon form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 4 \\\\\\\\\\n1 & 3 & 5 \\\\\\\\\\n2 & 1 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 4 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & -3 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 4 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe third column does not have a pivot; hence, the third vector $w$ is a linear combination of the first two vectors $u$ and $v$. Thus, the vectors are linearly dependent. (Observe that the matrix $A$ is also the coefficient matrix in Method 1. In other words, this method is essentially the same as the first method.)\\n\\nMethod 3. Form the matrix $B$ whose rows are $u, v, w$, and reduce to echelon form:\\n\\n$$\\nB=\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n2 & 3 & 1 \\\\\\\\\\n4 & 5 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n0 & 1 & 2 \\\\\\\\\\n0 & 1 & -3 \\\\\\\\\\n0 & 1 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & -3 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nBecause the echelon matrix has only two nonzero rows, the three vectors are linearly dependent. (The three given vectors span a space of dimension 2.)\\n',\n",
       " '\\n4.20. Determine whether or not each of the following lists of vectors in $\\\\mathbf{R}^{3}$ is linearly dependent:\\n\\n(a) $u_{1}=(1,2,5), u_{2}=(1,3,1), u_{3}=(2,5,7), u_{4}=(3,1,4)$,\\n\\n(b) $u=(1,2,5), v=(2,5,1), w=(1,5,2)$,\\n\\n(c) $u=(1,2,3), v=(0,0,0), w=(1,5,6)$.\\n\\n(a) Yes, because any four vectors in $\\\\mathbf{R}^{3}$ are linearly dependent.\\n\\n(b) Use Method 2 above; that is, form the matrix $A$ whose columns are the given vectors, and reduce the matrix to echelon form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 1 \\\\\\\\\\n2 & 5 & 5 \\\\\\\\\\n5 & 1 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 1 \\\\\\\\\\n0 & 1 & 3 \\\\\\\\\\n0 & -9 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 1 \\\\\\\\\\n0 & 1 & 3 \\\\\\\\\\n0 & 0 & 24\\n\\\\end{array}\\\\right]\\n$$\\n\\nEvery column has a pivot entry; hence, no vector is a linear combination of the previous vectors. Thus, the vectors are linearly independent.\\n\\n(c) Because $0=(0,0,0)$ is one of the vectors, the vectors are linearly dependent.\\n',\n",
       " '\\n4.21. Show that the functions $f(t)=\\\\sin t, g(t) \\\\cos t, h(t)=t$ from $\\\\mathbf{R}$ into $\\\\mathbf{R}$ are linearly independent.\\n\\nSet a linear combination of the functions equal to the zero function $\\\\mathbf{0}$ using unknown scalars $x, y, z$; that is, set $x f+y g+z h=\\\\mathbf{0}$. Then show $x=0, y=0, z=0$. We emphasize that $x f+y g+z h=\\\\mathbf{0}$ means that, for every value of $t$, we have $x f(t)+y g(t)+z h(t)=0$.\\n\\nThus, in the equation $x \\\\sin t+y \\\\cos t+z t=0$ :\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-146}\\n\\\\end{center}\\n\\nThe three equations have only the zero solution; that is, $x=0, y=0, z=0$. Thus, $f, g, h$ are linearly independent.\\n',\n",
       " '\\n4.22. Suppose the vectors $u, v, w$ are linearly independent. Show that the vectors $u+v, u-v$, $u-2 v+w$ are also linearly independent.\\n\\nSuppose $x(u+v)+y(u-v)+z(u-2 v+w)=0$. Then\\n\\n$$\\nx u+x v+y u-y v+z u-2 z v+z w=0\\n$$\\n\\nor\\n\\n$$\\n(x+y+z) u+(x-y-2 z) v+z w=0\\n$$\\n\\nBecause $u, v, w$ are linearly independent, the coefficients in the above equation are each 0 ; hence,\\n\\n$$\\nx+y+z=0, \\\\quad x-y-2 z=0, \\\\quad z=0\\n$$\\n\\nThe only solution to the above homogeneous system is $x=0, y=0, z=0$. Thus, $u+v, u-v, u-2 v+w$ are linearly independent.\\n',\n",
       " '\\n4.23. Show that the vectors $u=(1+i, 2 i)$ and $w=(1,1+i)$ in $\\\\mathbf{C}^{2}$ are linearly dependent over the complex field $\\\\mathbf{C}$ but linearly independent over the real field $\\\\mathbf{R}$.\\n\\nRecall that two vectors are linearly dependent (over a field $K$ ) if and only if one of them is a multiple of the other (by an element in $K$ ). Because\\n\\n$$\\n(1+i) w=(1+i)(1,1+i)=(1+i, 2 i)=u\\n$$\\n\\n$u$ and $w$ are linearly dependent over $\\\\mathbf{C}$. On the other hand, $u$ and $w$ are linearly independent over $\\\\mathbf{R}$, as no real multiple of $w$ can equal $u$. Specifically, when $k$ is real, the first component of $k w=(k, k+k i)$ must be real, and it can never equal the first component $1+i$ of $u$, which is complex.\\n\\n\\n\\\\section*{Basis and Dimension}\\n',\n",
       " '4.24. Determine whether or not each of the following form a basis of $\\\\mathbf{R}^{3}$ :\\\\\\\\\\n(a) $(1,1,1),(1,0,1)$;\\\\\\\\\\n(c) $(1,1,1),(1,2,3),(2,-1,1)$;\\\\\\\\\\n(b) $(1,2,3),(1,3,5),(1,0,1),(2,3,0)$;\\\\\\\\\\n(d) $(1,1,2),(1,2,5),(5,3,4)$.\\n\\n(a and b) No, because a basis of $\\\\mathbf{R}^{3}$ must contain exactly three elements because $\\\\operatorname{dim} \\\\mathbf{R}^{3}=3$.\\n\\n(c) The three vectors form a basis if and only if they are linearly independent. Thus, form the matrix whose rows are the given vectors, and row reduce the matrix to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n1 & 2 & 3 \\\\\\\\\\n2 & -1 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & -3 & -1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 0 & 5\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe echelon matrix has no zero rows; hence, the three vectors are linearly independent, and so they do form a basis of $\\\\mathbf{R}^{3}$.\\\\\\\\\\n(d) Form the matrix whose rows are the given vectors, and row reduce the matrix to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n1 & 2 & 5 \\\\\\\\\\n5 & 3 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 3 \\\\\\\\\\n0 & -2 & -6\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 3 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe echelon matrix has a zero row; hence, the three vectors are linearly dependent, and so they do not form a basis of $\\\\mathbf{R}^{3}$.\\n',\n",
       " '\\n4.25. Determine whether $(1,1,1,1),(1,2,3,2),(2,5,6,4),(2,6,8,5)$ form a basis of $\\\\mathbf{R}^{4}$. If not, find the dimension of the subspace they span.\\n\\nForm the matrix whose rows are the given vectors, and row reduce to echelon form:\\n\\n$$\\nB=\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 1 \\\\\\\\\\n1 & 2 & 3 & 2 \\\\\\\\\\n2 & 5 & 6 & 4 \\\\\\\\\\n2 & 6 & 8 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 & 1 \\\\\\\\\\n0 & 3 & 4 & 2 \\\\\\\\\\n0 & 4 & 6 & 3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 & 1 \\\\\\\\\\n0 & 0 & -2 & -1 \\\\\\\\\\n0 & 0 & -2 & -1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 & 1 \\\\\\\\\\n0 & 0 & 2 & 1 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe echelon matrix has a zero row. Hence, the four vectors are linearly dependent and do not form a basis of $\\\\mathbf{R}^{4}$. Because the echelon matrix has three nonzero rows, the four vectors span a subspace of dimension 3.\\n',\n",
       " '\\n4.26. Extend $\\\\left\\\\{u_{1}=(1,1,1,1), u_{2}=(2,2,3,4)\\\\right\\\\}$ to a basis of $\\\\mathbf{R}^{4}$.\\n\\nFirst form the matrix with rows $u_{1}$ and $u_{2}$, and reduce to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 1 \\\\\\\\\\n2 & 2 & 3 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llll}\\n1 & 1 & 1 & 1 \\\\\\\\\\n0 & 0 & 1 & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\nThen $w_{1}=(1,1,1,1)$ and $w_{2}=(0,0,1,2)$ span the same set of vectors as spanned by $u_{1}$ and $u_{2}$. Let $u_{3}=(0,1,0,0)$ and $u_{4}=(0,0,0,1)$. Then $w_{1}, u_{3}, w_{2}, u_{4}$ form a matrix in echelon form. Thus, they are linearly independent, and they form a basis of $\\\\mathbf{R}^{4}$. Hence, $u_{1}, u_{2}, u_{3}, u_{4}$ also form a basis of $\\\\mathbf{R}^{4}$.\\n',\n",
       " '\\n4.27. Consider the complex field $\\\\mathbf{C}$, which contains the real field $\\\\mathbf{R}$, which contains the rational field $\\\\mathbf{Q}$. (Thus, $\\\\mathbf{C}$ is a vector space over $\\\\mathbf{R}$, and $\\\\mathbf{R}$ is a vector space over $\\\\mathbf{Q}$.)\\n\\n(a) Show that $\\\\{1, i\\\\}$ is a basis of $\\\\mathbf{C}$ over $\\\\mathbf{R}$; hence, $\\\\mathbf{C}$ is a vector space of dimension 2 over $\\\\mathbf{R}$.\\n\\n(b) Show that $\\\\mathbf{R}$ is a vector space of infinite dimension over $\\\\mathbf{Q}$.\\n\\n(a) For any $v \\\\in \\\\mathbf{C}$, we have $v=a+b i=a(1)+b(i)$, where $a, b \\\\in \\\\mathbf{R}$. Hence, $\\\\{1, i\\\\}$ spans $\\\\mathbf{C}$ over $\\\\mathbf{R}$. Furthermore, if $x(1)+y(i)=0$ or $x+y i=0$, where $x, y \\\\in \\\\mathbf{R}$, then $x=0$ and $y=0$. Hence, $\\\\{1, i\\\\}$ is linearly independent over $\\\\mathbf{R}$. Thus, $\\\\{1, i\\\\}$ is a basis for $\\\\mathbf{C}$ over $\\\\mathbf{R}$.\\n\\n(b) It can be shown that $\\\\pi$ is a transcendental number; that is, $\\\\pi$ is not a root of any polynomial over $\\\\mathbf{Q}$. Thus, for any $n$, the $n+1$ real numbers $1, \\\\pi, \\\\pi^{2}, \\\\ldots, \\\\pi^{n}$ are linearly independent over $\\\\mathbf{Q}$. $\\\\mathbf{R}$ cannot be of dimension $n$ over $\\\\mathbf{Q}$. Accordingly, $\\\\mathbf{R}$ is of infinite dimension over $\\\\mathbf{Q}$.\\n',\n",
       " '\\n4.28. Suppose $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is a subset of $V$. Show that the following Definitions A and B of a basis of $V$ are equivalent:\\n\\n(A) $S$ is linearly independent and spans $V$.\\n\\n(B) Every $v \\\\in V$ is a unique linear combination of vectors in $S$.\\n\\nSuppose (A) holds. Because $S$ spans $V$, the vector $v$ is a linear combination of the $u_{i}$, say\\n\\n$$\\nu=a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} u_{n} \\\\quad \\\\text { and } \\\\quad u=b_{1} u_{1}+b_{2} u_{2}+\\\\cdots+b_{n} u_{n}\\n$$\\n\\nSubtracting, we get\\n\\n$$\\n0=v-v=\\\\left(a_{1}-b_{1}\\\\right) u_{1}+\\\\left(a_{2}-b_{2}\\\\right) u_{2}+\\\\cdots+\\\\left(a_{n}-b_{n}\\\\right) u_{n}\\n$$\\n\\nBut the $u_{i}$ are linearly independent. Hence, the coefficients in the above relation are each 0 :\\n\\n$$\\na_{1}-b_{1}=0, \\\\quad a_{2}-b_{2}=0, \\\\quad \\\\ldots, \\\\quad a_{n}-b_{n}=0\\n$$\\n\\nTherefore, $a_{1}=b_{1}, a_{2}=b_{2}, \\\\ldots, a_{n}=b_{n}$. Hence, the representation of $v$ as a linear combination of the $u_{i}$ is unique. Thus, (A) implies (B).\\n\\nSuppose (B) holds. Then $S$ spans $V$. Suppose\\n\\n$$\\n0=c_{1} u_{1}+c_{2} u_{2}+\\\\cdots+c_{n} u_{n}\\n$$\\n\\nHowever, we do have\\n\\n$$\\n0=0 u_{1}+0 u_{2}+\\\\cdots+0 u_{n}\\n$$\\n\\nBy hypothesis, the representation of 0 as a linear combination of the $u_{i}$ is unique. Hence, each $c_{i}=0$ and the $u_{i}$ are linearly independent. Thus, (B) implies (A).\\n\\n\\n\\\\section*{Dimension and Subspaces}\\n',\n",
       " '4.29. Find a basis and dimension of the subspace $W$ of $\\\\mathbf{R}^{3}$ where\\\\\\\\\\n(a) $W=\\\\{(a, b, c): a+b+c=0\\\\}$,\\\\\\\\\\n(b) $W=\\\\{(a, b, c):(a=b=c)\\\\}$\\n\\n(a) Note that $W \\\\neq \\\\mathbf{R}^{3}$, because, for example, $(1,2,3) \\\\notin W$. Thus, $\\\\operatorname{dim} W<3$. Note that $u_{1}=(1,0,-1)$ and $u_{2}=(0,1,-1)$ are two independent vectors in $W$. Thus, $\\\\operatorname{dim} W=2$, and so $u_{1}$ and $u_{2}$ form a basis of $W$.\\n\\n(b) The vector $u=(1,1,1) \\\\in W$. Any vector $w \\\\in W$ has the form $w=(k, k, k)$. Hence, $w=k u$. Thus, $u$ spans $W$ and $\\\\operatorname{dim} W=1$.\\n',\n",
       " '\\n4.30. Let $W$ be the subspace of $\\\\mathbf{R}^{4}$ spanned by the vectors\\n\\n$$\\nu_{1}=(1,-2,5,-3), \\\\quad u_{2}=(2,3,1,-4), \\\\quad u_{3}=(3,8,-3,-5)\\n$$\\n\\n(a) Find a basis and dimension of $W$. (b) Extend the basis of $W$ to a basis of $\\\\mathbf{R}^{4}$.\\n\\n(a) Apply Algorithm 4.1, the row space algorithm. Form the matrix whose rows are the given vectors, and reduce it to echelon form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrrr}\\n1 & -2 & 5 & -3 \\\\\\\\\\n2 & 3 & 1 & -4 \\\\\\\\\\n3 & 8 & -3 & -5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & -2 & 5 & -3 \\\\\\\\\\n0 & 7 & -9 & 2 \\\\\\\\\\n0 & 14 & -18 & 4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & -2 & 5 & -3 \\\\\\\\\\n0 & 7 & -9 & 2 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe nonzero rows $(1,-2,5,-3)$ and $(0,7,-9,2)$ of the echelon matrix form a basis of the row space of $A$ and hence of $W$. Thus, in particular, $\\\\operatorname{dim} W=2$.\\n\\n(b) We seek four linearly independent vectors, which include the above two vectors. The four vectors $(1,-2,5,-3),(0,7,-9,2),(0,0,1,0)$, and $(0,0,0,1)$ are linearly independent (because they form an echelon matrix), and so they form a basis of $\\\\mathbf{R}^{4}$, which is an extension of the basis of $W$.\\n',\n",
       " '\\n4.31. Let $W$ be the subspace of $\\\\mathbf{R}^{5}$ spanned by $u_{1}=(1,2,-1,3,4), \\\\quad u_{2}=(2,4,-2,6,8)$, $u_{3}=(1,3,2,2,6), \\\\quad u_{4}=(1,4,5,1,8), \\\\quad u_{5}=(2,7,3,3,9)$. Find a subset of the vectors that form a basis of $W$.\\n\\nHere we use Algorithm 4.2, the casting-out algorithm. Form the matrix $M$ whose columns (not rows) are the given vectors, and reduce it to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 1 & 1 & 2 \\\\\\\\\\n2 & 4 & 3 & 4 & 7 \\\\\\\\\\n-1 & -2 & 2 & 5 & 3 \\\\\\\\\\n3 & 6 & 2 & 1 & 3 \\\\\\\\\\n4 & 8 & 6 & 8 & 9\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 1 & 1 & 2 \\\\\\\\\\n0 & 0 & 1 & 2 & 3 \\\\\\\\\\n0 & 0 & 3 & 6 & 5 \\\\\\\\\\n0 & 0 & -1 & -2 & -3 \\\\\\\\\\n0 & 0 & 2 & 4 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 2 & 1 & 1 & 2 \\\\\\\\\\n0 & 0 & 1 & 2 & 3 \\\\\\\\\\n0 & 0 & 0 & 0 & -4 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe pivot positions are in columns $C_{1}, C_{3}, C_{5}$. Hence, the corresponding vectors $u_{1}, u_{3}, u_{5}$ form a basis of $W$, and $\\\\operatorname{dim} W=3$.\\n',\n",
       " '\\n4.32. Let $V$ be the vector space of $2 \\\\times 2$ matrices over $K$. Let $W$ be the subspace of symmetric matrices. Show that $\\\\operatorname{dim} W=3$, by finding a basis of $W$.\\n\\nRecall that a matrix $A=\\\\left[a_{i j}\\\\right]$ is symmetric if $A^{T}=A$, or, equivalently, each $a_{i j}=a_{j i}$. Thus, $A=\\\\left[\\\\begin{array}{ll}a & b \\\\\\\\ b & d\\\\end{array}\\\\right]$ denotes an arbitrary $2 \\\\times 2$ symmetric matrix. Setting (i) $a=1, b=0, d=0$; (ii) $a=0, b=1, d=0$; (iii) $a=0, b=0, d=1$, we obtain the respective matrices:\\n\\n$$\\nE_{1}=\\\\left[\\\\begin{array}{cc}\\n1 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right], \\\\quad E_{2}=\\\\left[\\\\begin{array}{ll}\\n0 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right], \\\\quad E_{3}=\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nWe claim that $S=\\\\left\\\\{E_{1}, E_{2}, E_{3}\\\\right\\\\}$ is a basis of $W$; that is, (a) $S$ spans $W$ and (b) $S$ is linearly independent.\\n\\n(a) The above matrix $A=\\\\left[\\\\begin{array}{ll}a & b \\\\\\\\ b & d\\\\end{array}\\\\right]=a E_{1}+b E_{2}+d E_{3}$. Thus, $S$ spans $W$.\\n\\n(b) Suppose $x E_{1}+y E_{2}+z E_{3}=0$, where $x, y, z$ are unknown scalars. That is, suppose\\n\\n$$\\nx\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{ll}\\n0 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad\\\\left[\\\\begin{array}{ll}\\nx & y \\\\\\\\\\ny & z\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nSetting corresponding entries equal to each other yields $x=0, y=0, z=0$. Thus, $S$ is linearly independent. Therefore, $S$ is a basis of $W$, as claimed.\\n\\n\\n\\\\section*{Theorems on Linear Dependence, Basis, and Dimension}\\n',\n",
       " '4.33. Prove Lemma 4.10: Suppose two or more nonzero vectors $v_{1}, v_{2}, \\\\ldots, v_{m}$ are linearly dependent. Then one of them is a linear combination of the preceding vectors.\\n\\nBecause the $v_{i}$ are linearly dependent, there exist scalars $a_{1}, \\\\ldots, a_{m}$, not all 0 , such that $a_{1} v_{1}+\\\\cdots+a_{m} v_{m}=0$. Let $k$ be the largest integer such that $a_{k} \\\\neq 0$. Then\\n\\n$$\\na_{1} v_{1}+\\\\cdots+a_{k} v_{k}+0 v_{k+1}+\\\\cdots+0 v_{m}=0 \\\\quad \\\\text { or } \\\\quad a_{1} v_{1}+\\\\cdots+a_{k} v_{k}=0\\n$$\\n\\nSuppose $k=1$; then $a_{1} v_{1}=0, a_{1} \\\\neq 0$, and so $v_{1}=0$. But the $v_{i}$ are nonzero vectors. Hence, $k>1$ and\\n\\n$$\\nv_{k}=-a_{k}^{-1} a_{1} v_{1}-\\\\cdots-a_{k}^{-1} a_{k-1} v_{k-1}\\n$$\\n\\nThat is, $v_{k}$ is a linear combination of the preceding vectors.\\n',\n",
       " \"\\n4.34. Suppose $S=\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{m}\\\\right\\\\}$ spans a vector space $V$.\\n\\n(a) If $w \\\\in V$, then $\\\\left\\\\{w, v_{1}, \\\\ldots, v_{m}\\\\right\\\\}$ is linearly dependent and spans $V$.\\n\\n(b) If $v_{i}$ is a linear combination of $v_{1}, \\\\ldots, v_{i-1}$, then $S$ without $v_{i}$ spans $V$.\\n\\n(a) The vector $w$ is a linear combination of the $v_{i}$, because $\\\\left\\\\{v_{i}\\\\right\\\\}$ spans $V$. Accordingly, $\\\\left\\\\{w, v_{1}, \\\\ldots, v_{m}\\\\right\\\\}$ is linearly dependent. Clearly, $w$ with the $v_{i}$ span $V$, as the $v_{i}$ by themselves span $V$; that is, $\\\\left\\\\{w, v_{1}, \\\\ldots, v_{m}\\\\right\\\\}$ spans $V$.\\n\\n(b) Suppose $v_{i}=k_{1} v_{1}+\\\\cdots+k_{i-1} v_{i-1}$. Let $u \\\\in V$. Because $\\\\left\\\\{v_{i}\\\\right\\\\}$ spans $V, u$ is a linear combination of the $v_{j}$ 's, say $u=a_{1} v_{1}+\\\\cdots+a_{m} v_{m}$. Substituting for $v_{i}$, we obtain\\n\\n$$\\n\\\\begin{aligned}\\nu & =a_{1} v_{1}+\\\\cdots+a_{i-1} v_{i-1}+a_{i}\\\\left(k_{1} v_{1}+\\\\cdots+k_{i-1} v_{i-1}\\\\right)+a_{i+1} v_{i+1}+\\\\cdots+a_{m} v_{m} \\\\\\\\\\n& =\\\\left(a_{1}+a_{i} k_{1}\\\\right) v_{1}+\\\\cdots+\\\\left(a_{i-1}+a_{i} k_{i-1}\\\\right) v_{i-1}+a_{i+1} v_{i+1}+\\\\cdots+a_{m} v_{m}\\n\\\\end{aligned}\\n$$\\n\\nThus, $\\\\left\\\\{v_{1}, \\\\ldots, v_{i-1}, v_{i+1}, \\\\ldots, v_{m}\\\\right\\\\}$ spans $V$. In other words, we can delete $v_{i}$ from the spanning set and still retain a spanning set.\\n\",\n",
       " \"\\n4.35. Prove Lemma 4.13: Suppose $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ spans $V$, and suppose $\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{m}\\\\right\\\\}$ is linearly independent. Then $m \\\\leq n$, and $V$ is spanned by a set of the form\\n\\n$\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{m}, v_{i_{1}}, v_{i_{2}}, \\\\ldots, v_{i_{n-m}}\\\\right\\\\}$\\n\\nThus, any $n+1$ or more vectors in $V$ are linearly dependent.\\n\\nIt suffices to prove the lemma in the case that the $v_{i}$ are all not 0 . (Prove!) Because $\\\\left\\\\{v_{i}\\\\right\\\\}$ spans $V$, we have by Problem 4.34 that\\n\\n\\n\\\\begin{equation*}\\n\\\\left\\\\{w_{1}, v_{1}, \\\\ldots, v_{n}\\\\right\\\\} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nis linearly dependent and also spans $V$. By Lemma 4.10, one of the vectors in (1) is a linear combination of the preceding vectors. This vector cannot be $w_{1}$, so it must be one of the $v$ 's, say $v_{j}$. Thus by Problem 4.34, we can delete $v_{j}$ from the spanning set (1) and obtain the spanning set\\n\\n\\n\\\\begin{equation*}\\n\\\\left\\\\{w_{1}, v_{1}, \\\\ldots, v_{j-1}, v_{j+1}, \\\\ldots, v_{n}\\\\right\\\\} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nNow we repeat the argument with the vector $w_{2}$. That is, because (2) spans $V$, the set\\n\\n\\n\\\\begin{equation*}\\n\\\\left\\\\{w_{1}, w_{2}, v_{1}, \\\\ldots, v_{j-1}, v_{j+1}, \\\\ldots, v_{n}\\\\right\\\\} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nis linearly dependent and also spans $V$. Again by Lemma 4.10, one of the vectors in (3) is a linear combination of the preceding vectors. We emphasize that this vector cannot be $w_{1}$ or $w_{2}$, because $\\\\left\\\\{w_{1}, \\\\ldots, w_{m}\\\\right\\\\}$ is independent; hence, it must be one of the $v$ 's, say $v_{k}$. Thus, by Problem 4.34, we can delete $v_{k}$ from the spanning set (3) and obtain the spanning set\\n\\n$$\\n\\\\left\\\\{w_{1}, w_{2}, v_{1}, \\\\ldots, v_{j-1}, v_{j+1}, \\\\ldots, v_{k-1}, v_{k+1}, \\\\ldots, v_{n}\\\\right\\\\}\\n$$\\n\\nWe repeat the argument with $w_{3}$, and so forth. At each step, we are able to add one of the $w$ 's and delete one of the $v$ 's in the spanning set. If $m \\\\leq n$, then we finally obtain a spanning set of the required form:\\n\\n$$\\n\\\\left\\\\{w_{1}, \\\\ldots, w_{m}, v_{i_{1}}, \\\\ldots, v_{i_{n-m}}\\\\right\\\\}\\n$$\\n\\nFinally, we show that $m>n$ is not possible. Otherwise, after $n$ of the above steps, we obtain the spanning set $\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$. This implies that $w_{n+1}$ is a linear combination of $w_{1}, \\\\ldots, w_{n}$, which contradicts the hypothesis that $\\\\left\\\\{w_{i}\\\\right\\\\}$ is linearly independent.\\n\",\n",
       " '\\n4.36. Prove Theorem 4.12: Every basis of a vector space $V$ has the same number of elements.\\n\\nSuppose $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is a basis of $V$, and suppose $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots\\\\right\\\\}$ is another basis of $V$. Because $\\\\left\\\\{u_{i}\\\\right\\\\}$ spans $V$, the basis $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots\\\\right\\\\}$ must contain $n$ or less vectors, or else it is linearly dependent by Problem 4.35-Lemma 4.13. On the other hand, if the basis $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots\\\\right\\\\}$ contains less than $n$ elements, then $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is linearly dependent by Problem 4.35. Thus, the basis $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots\\\\right\\\\}$ contains exactly $n$ vectors, and so the theorem is true.\\n',\n",
       " '\\n4.37. Prove Theorem 4.14: Let $V$ be a vector space of finite dimension $n$. Then\\n\\n(i) Any $n+1$ or more vectors must be linearly dependent.\\n\\n(ii) Any linearly independent set $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots u_{n}\\\\right\\\\}$ with $n$ elements is a basis of $V$.\\n\\n(iii) Any spanning set $T=\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ of $V$ with $n$ elements is a basis of $V$.\\n\\nSuppose $B=\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}$ is a basis of $V$.\\n\\n(i) Because $B$ spans $V$, any $n+1$ or more vectors are linearly dependent by Lemma 4.13.\\n\\n(ii) By Lemma 4.13, elements from $B$ can be adjoined to $S$ to form a spanning set of $V$ with $n$ elements. Because $S$ already has $n$ elements, $S$ itself is a spanning set of $V$. Thus, $S$ is a basis of $V$.\\n\\n(iii) Suppose $T$ is linearly dependent. Then some $v_{i}$ is a linear combination of the preceding vectors. By Problem 4.34, $V$ is spanned by the vectors in $T$ without $v_{i}$ and there are $n-1$ of them. By Lemma 4.13, the independent set $B$ cannot have more than $n-1$ elements. This contradicts the fact that $B$ has $n$ elements. Thus, $T$ is linearly independent, and hence $T$ is a basis of $V$.\\n\\n\\\\subsection*{4.38. Prove Theorem 4.15: Suppose $S$ spans a vector space $V$. Then}\\n(i) Any maximum number of linearly independent vectors in $S$ form a basis of $V$.\\n\\n(ii) Suppose one deletes from $S$ every vector that is a linear combination of preceding vectors in $S$. Then the remaining vectors form a basis of $V$.\\n\\n(i) Suppose $\\\\left\\\\{v_{1}, \\\\ldots, v_{m}\\\\right\\\\}$ is a maximum linearly independent subset of $S$, and suppose $w \\\\in S$. Accordingly, $\\\\left\\\\{v_{1}, \\\\ldots, v_{m}, w\\\\right\\\\}$ is linearly dependent. No $v_{k}$ can be a linear combination of preceding vectors.\\n\\nHence, $w$ is a linear combination of the $v_{i}$. Thus, $w \\\\in \\\\operatorname{span}\\\\left(v_{i}\\\\right)$, and hence $S \\\\subseteq \\\\operatorname{span}\\\\left(v_{i}\\\\right)$. This leads to\\n\\n$$\\nV=\\\\operatorname{span}(S) \\\\subseteq \\\\operatorname{span}\\\\left(v_{i}\\\\right) \\\\subseteq V\\n$$\\n\\nThus, $\\\\left\\\\{v_{i}\\\\right\\\\}$ spans $V$, and, as it is linearly independent, it is a basis of $V$.\\n\\n(ii) The remaining vectors form a maximum linearly independent subset of $S$; hence, by (i), it is a basis of $V$.\\n',\n",
       " '\\n4.39. Prove Theorem 4.16: Let $V$ be a vector space of finite dimension and let $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{r}\\\\right\\\\}$ be a set of linearly independent vectors in $V$. Then $S$ is part of a basis of $V$; that is, $S$ may be extended to a basis of $V$.\\n\\nSuppose $B=\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}$ is a basis of $V$. Then $B$ spans $V$, and hence $V$ is spanned by\\n\\n$$\\nS \\\\cup B=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{r}, w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}\\n$$\\n\\nBy Theorem 4.15, we can delete from $S \\\\cup B$ each vector that is a linear combination of preceding vectors to obtain a basis $B^{\\\\prime}$ for $V$. Because $S$ is linearly independent, no $u_{k}$ is a linear combination of preceding vectors. Thus, $B^{\\\\prime}$ contains every vector in $S$, and $S$ is part of the basis $B^{\\\\prime}$ for $V$.\\n',\n",
       " '\\n4.40. Prove Theorem 4.17: Let $W$ be a subspace of an $n$-dimensional vector space $V$. Then $\\\\operatorname{dim} W \\\\leq n$. In particular, if $\\\\operatorname{dim} W=n$, then $W=V$.\\n\\nBecause $V$ is of dimension $n$, any $n+1$ or more vectors are linearly dependent. Furthermore, because a basis of $W$ consists of linearly independent vectors, it cannot contain more than $n$ elements. Accordingly, $\\\\operatorname{dim} W \\\\leq n$.\\n\\nIn particular, if $\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$ is a basis of $W$, then, because it is an independent set with $n$ elements, it is also a basis of $V$. Thus, $W=V$ when $\\\\operatorname{dim} W=n$.\\n\\n\\n\\\\section*{Rank of a Matrix, Row and Column Spaces}\\n',\n",
       " '4.41. Find the rank and basis of the row space of each of the following matrices:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rrrr}1 & 2 & 0 & -1 \\\\\\\\ 2 & 6 & -3 & -3 \\\\\\\\ 3 & 10 & -6 & -5\\\\end{array}\\\\right]$\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrrrr}1 & 3 & 1 & -2 & -3 \\\\\\\\ 1 & 4 & 3 & -1 & -4 \\\\\\\\ 2 & 3 & -4 & -7 & -3 \\\\\\\\ 3 & 8 & 1 & -7 & -8\\\\end{array}\\\\right]$.\\n\\n(a) Row reduce $A$ to echelon form:\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 0 & -1 \\\\\\\\\\n0 & 2 & -3 & -1 \\\\\\\\\\n0 & 4 & -6 & -2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 0 & -1 \\\\\\\\\\n0 & 2 & -3 & -1 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe two nonzero rows $(1,2,0,-1)$ and $(0,2,-3,-1)$ of the echelon form of $A$ form a basis for rowsp(A). In particular, $\\\\operatorname{rank}(A)=2$.\\n\\n(b) Row reduce $B$ to echelon form:\\n\\n$$\\nB \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 3 & 1 & -2 & -3 \\\\\\\\\\n0 & 1 & 2 & 1 & -1 \\\\\\\\\\n0 & -3 & -6 & -3 & 3 \\\\\\\\\\n0 & -1 & -2 & -1 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 3 & 1 & -2 & -3 \\\\\\\\\\n0 & 1 & 2 & 1 & -1 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe two nonzero rows $(1,3,1,-2,-3)$ and $(0,1,2,1,-1)$ of the echelon form of $B$ form a basis for rowsp(B). In particular, $\\\\operatorname{rank}(B)=2$.\\n',\n",
       " '\\n4.42. Show that $U=W$, where $U$ and $W$ are the following subspaces of $\\\\mathbf{R}^{3}$ :\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\left.U=\\\\operatorname{span}\\\\left(u_{1}, u_{2}, u_{3}\\\\right)=\\\\operatorname{span}(1,1,-1),(2,3,-1),(3,1,-5)\\\\right\\\\} \\\\\\\\\\n& \\\\left.W=\\\\operatorname{span}\\\\left(w_{1}, w_{2}, w_{3}\\\\right)=\\\\operatorname{span}(1,-1,-3),(3,-2,-8),(2,1,-3)\\\\right\\\\}\\n\\\\end{aligned}\\n$$\\n\\nForm the matrix $A$ whose rows are the $u_{i}$, and row reduce $A$ to row canonical form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{lll}\\n1 & 1 & -1 \\\\\\\\\\n2 & 3 & -1 \\\\\\\\\\n3 & 1 & -5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & -1 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & -2 & -2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & -2 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nNext form the matrix $B$ whose rows are the $w_{j}$, and row reduce $B$ to row canonical form:\\n\\n$$\\nB=\\\\left[\\\\begin{array}{rrr}\\n1 & -1 & -3 \\\\\\\\\\n3 & -2 & -8 \\\\\\\\\\n2 & 1 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & -1 & -3 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & 3 & 3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & -2 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nBecause $A$ and $B$ have the same row canonical form, the row spaces of $A$ and $B$ are equal, and so $U=W$.\\n',\n",
       " '\\n4.43. Let $A=\\\\left[\\\\begin{array}{rrrrrr}1 & 2 & 1 & 2 & 3 & 1 \\\\\\\\ 2 & 4 & 3 & 7 & 7 & 4 \\\\\\\\ 1 & 2 & 2 & 5 & 5 & 6 \\\\\\\\ 3 & 6 & 6 & 15 & 14 & 15\\\\end{array}\\\\right]$.\\n\\n(a) Find $\\\\operatorname{rank}\\\\left(M_{k}\\\\right)$, for $k=1,2, \\\\ldots, 6$, where $M_{k}$ is the submatrix of $A$ consisting of the first $k$ columns $C_{1}, C_{2}, \\\\ldots, C_{k}$ of $A$.\\n\\n(b) Which columns $C_{k+1}$ are linear combinations of preceding columns $C_{1}, \\\\ldots, C_{k}$ ?\\n\\n(c) Find columns of $A$ that form a basis for the column space of $A$.\\n\\n(d) Express column $C_{4}$ as a linear combination of the columns in part (c).\\n\\n(a) Row reduce $A$ to echelon form:\\n\\n$$\\nA \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & 1 & 2 & 3 & 1 \\\\\\\\\\n0 & 0 & 1 & 3 & 1 & 2 \\\\\\\\\\n0 & 0 & 1 & 3 & 2 & 5 \\\\\\\\\\n0 & 0 & 3 & 9 & 5 & 12\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & 1 & 2 & 3 & 1 \\\\\\\\\\n0 & 0 & 1 & 3 & 1 & 2 \\\\\\\\\\n0 & 0 & 0 & 0 & 1 & 3 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nObserve that this simultaneously reduces all the matrices $M_{k}$ to echelon form; for example, the first four columns of the echelon form of $A$ are an echelon form of $M_{4}$. We know that $\\\\operatorname{rank}\\\\left(M_{k}\\\\right)$ is equal to the number of pivots or, equivalently, the number of nonzero rows in an echelon form of $M_{k}$. Thus,\\n\\n$$\\n\\\\begin{gathered}\\n\\\\operatorname{rank}\\\\left(M_{1}\\\\right)=\\\\operatorname{rank}\\\\left(M_{2}\\\\right)=1, \\\\quad \\\\operatorname{rank}\\\\left(M_{3}\\\\right)=\\\\operatorname{rank}\\\\left(M_{4}\\\\right)=2 \\\\\\\\\\n\\\\operatorname{rank}\\\\left(M_{5}\\\\right)=\\\\operatorname{rank}\\\\left(M_{6}\\\\right)=3\\n\\\\end{gathered}\\n$$\\n\\n(b) The vector equation $x_{1} C_{1}+x_{2} C_{2}+\\\\cdots+x_{k} C_{k}=C_{k+1}$ yields the system with coefficient matrix $M_{k}$ and augmented $M_{k+1}$. Thus, $C_{k+1}$ is a linear combination of $C_{1}, \\\\ldots, C_{k}$ if and only if $\\\\operatorname{rank}\\\\left(M_{k}\\\\right)=\\\\operatorname{rank}\\\\left(M_{k+1}\\\\right)$ or, equivalently, if $C_{k+1}$ does not contain a pivot. Thus, each of $C_{2}, C_{4}, C_{6}$ is a linear combination of preceding columns.\\n\\n(c) In the echelon form of $A$, the pivots are in the first, third, and fifth columns. Thus, columns $C_{1}, C_{3}, C_{5}$ of $A$ form a basis for the columns space of $A$. Alternatively, deleting columns $C_{2}, C_{4}, C_{6}$ from the spanning set of columns (they are linear combinations of other columns), we obtain, again, $C_{1}, C_{3}, C_{5}$.\\n\\n(d) The echelon matrix tells us that $C_{4}$ is a linear combination of columns $C_{1}$ and $C_{3}$. The augmented matrix $M$ of the vector equation $C_{4}=x C_{1}+y C_{2}$ consists of the columns $C_{1}, C_{3}, C_{4}$ of $A$ which, when reduced to echelon form, yields the matrix (omitting zero rows)\\n\\n$$\\n\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 3\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+y & =2 \\\\\\\\\\ny & =3\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x=-1, \\\\quad y=3\\n$$\\n\\nThus, $C_{4}=-C_{1}+3 C_{3}=-C_{1}+3 C_{3}+0 C_{5}$.\\n',\n",
       " '\\n4.44. Suppose $u=\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right)$ is a linear combination of the rows $R_{1}, R_{2}, \\\\ldots, R_{m}$ of a matrix $B=\\\\left[b_{i j}\\\\right]$, say $u=k_{1} R_{1}+k_{2} R_{2}+\\\\cdots+k_{m} R_{m}$. Prove that\\n\\n$$\\na_{i}=k_{1} b_{1 i}+k_{2} b_{2 i}+\\\\cdots+k_{m} b_{m i}, \\\\quad i=1,2, \\\\ldots, n\\n$$\\n\\nwhere $b_{1 i}, b_{2 i}, \\\\ldots, b_{m i}$ are the entries in the $i$ th column of $B$.\\n\\nWe are given that $u=k_{1} R_{1}+k_{2} R_{2}+\\\\cdots+k_{m} R_{m}$. Hence,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right) & =k_{1}\\\\left(b_{11}, \\\\ldots, b_{1 n}\\\\right)+\\\\cdots+k_{m}\\\\left(b_{m 1}, \\\\ldots, b_{m n}\\\\right) \\\\\\\\\\n& =\\\\left(k_{1} b_{11}+\\\\cdots+k_{m} b_{m 1}, \\\\ldots, k_{1} b_{1 n}+\\\\cdots+k_{m} b_{m n}\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nSetting corresponding components equal to each other, we obtain the desired result.\\n',\n",
       " '\\n4.45. Prove Theorem 4.7: Suppose $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$ are row equivalent echelon matrices with respective pivot entries\\n\\n$$\\na_{1 j_{1}}, a_{2 j_{2}}, \\\\ldots, a_{r j_{r}} \\\\quad \\\\text { and } \\\\quad b_{1 k_{1}}, b_{2 k_{2}}, \\\\ldots, b_{s k_{s}}\\n$$\\n\\n(pictured in Fig. 4-5). Then $A$ and $B$ have the same number of nonzero rows - that is, $r=s-$ and their pivot entries are in the same positions; that is, $j_{1}=k_{1}, j_{2}=k_{2}, \\\\ldots, j_{r}=k_{r}$.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-153}\\n\\\\end{center}\\n\\nFigure 4-5\\n\\nClearly $A=0$ if and only if $B=0$, and so we need only prove the theorem when $r \\\\geq 1$ and $s \\\\geq 1$. We first show that $j_{1}=k_{1}$. Suppose $j_{1}<k_{1}$. Then the $j_{1}$ th column of $B$ is zero. Because the first row $R^{*}$ of $A$ is in the row space of $B$, we have $R^{*}=c_{1} R_{1}+c_{1} R_{2}+\\\\cdots+c_{m} R_{m}$, where the $R_{i}$ are the rows of $B$. Because the $j_{1}$ th column of $B$ is zero, we have\\n\\n$$\\na_{1 j_{1}}=c_{1} 0+c_{2} 0+\\\\cdots+c_{m} 0=0\\n$$\\n\\nBut this contradicts the fact that the pivot entry $a_{1 j_{1}} \\\\neq 0$. Hence, $j_{1} \\\\geq k_{1}$ and, similarly, $k_{1} \\\\geq j_{1}$. Thus $j_{1}=k_{1}$.\\n\\nNow let $A^{\\\\prime}$ be the submatrix of $A$ obtained by deleting the first row of $A$, and let $B^{\\\\prime}$ be the submatrix of $B$ obtained by deleting the first row of $B$. We prove that $A^{\\\\prime}$ and $B^{\\\\prime}$ have the same row space. The theorem will then follow by induction, because $A^{\\\\prime}$ and $B^{\\\\prime}$ are also echelon matrices.\\n\\nLet $R=\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right)$ be any row of $A^{\\\\prime}$ and let $R_{1}, \\\\ldots, R_{m}$ be the rows of $B$. Because $R$ is in the row space of $B$, there exist scalars $d_{1}, \\\\ldots, d_{m}$ such that $R=d_{1} R_{1}+d_{2} R_{2}+\\\\cdots+d_{m} R_{m}$. Because $A$ is in echelon form and $R$ is not the first row of $A$, the $j_{1}$ th entry of $R$ is zero: $a_{i}=0$ for $i=j_{1}=k_{1}$. Furthermore, because $B$ is in echelon form, all the entries in the $k_{1}$ th column of $B$ are 0 except the first: $b_{1 k_{1}} \\\\neq 0$, but $b_{2 k_{1}}=0, \\\\ldots, b_{m k_{1}}=0$. Thus,\\n\\n$$\\n0=a_{k_{1}}=d_{1} b_{1 k_{1}}+d_{2} 0+\\\\cdots+d_{m} 0=d_{1} b_{1 k_{1}}\\n$$\\n\\nNow $b_{1 k_{1}} \\\\neq 0$ and so $d_{1}=0$. Thus, $R$ is a linear combination of $R_{2}, \\\\ldots, R_{m}$ and so is in the row space of $B^{\\\\prime}$. Because $R$ was any row of $A^{\\\\prime}$, the row space of $A^{\\\\prime}$ is contained in the row space of $B^{\\\\prime}$. Similarly, the row space of $B^{\\\\prime}$ is contained in the row space of $A^{\\\\prime}$. Thus, $A^{\\\\prime}$ and $B^{\\\\prime}$ have the same row space, and so the theorem is proved.\\n',\n",
       " '\\n4.46. Prove Theorem 4.8: Suppose $A$ and $B$ are row canonical matrices. Then $A$ and $B$ have the same row space if and only if they have the same nonzero rows.\\n\\nObviously, if $A$ and $B$ have the same nonzero rows, then they have the same row space. Thus we only have to prove the converse.\\n\\nSuppose $A$ and $B$ have the same row space, and suppose $R \\\\neq 0$ is the $i$ th row of $A$. Then there exist scalars $c_{1}, \\\\ldots, c_{s}$ such that\\n\\n\\n\\\\begin{equation*}\\nR=c_{1} R_{1}+c_{2} R_{2}+\\\\cdots+c_{s} R_{s} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nwhere the $R_{i}$ are the nonzero rows of $B$. The theorem is proved if we show that $R=R_{i}$; that is, that $c_{i}=1$ but $c_{k}=0$ for $k \\\\neq i$.\\n\\nLet $a_{i j}$, be the pivot entry in $R$ - that is, the first nonzero entry of $R$. By (1) and Problem 4.44,\\n\\n\\n\\\\begin{equation*}\\na_{i j_{i}}=c_{1} b_{1 j_{i}}+c_{2} b_{2 j_{i}}+\\\\cdots+c_{s} b_{s j_{i}} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nBut, by Problem 4.45, $b_{i j_{i}}$ is a pivot entry of $B$, and, as $B$ is row reduced, it is the only nonzero entry in the $j$ th column of $B$. Thus, from (2), we obtain $a_{i j_{i}}=c_{i} b_{i j}$. However, $a_{i j_{i}}=1$ and $b_{i j_{i}}=1$, because $A$ and $B$ are row reduced; hence, $c_{i}=1$.\\n\\nNow suppose $k \\\\neq i$, and $b_{k j_{k}}$ is the pivot entry in $R_{k}$. By (1) and Problem 4.44,\\n\\n\\n\\\\begin{equation*}\\na_{i j_{k}}=c_{1} b_{1 j_{k}}+c_{2} b_{2 j_{k}}+\\\\cdots+c_{s} b_{s j_{k}} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nBecause $B$ is row reduced, $b_{k_{k}}$ is the only nonzero entry in the $j$ th column of $B$. Hence, by (3), $a_{i j_{k}}=c_{k} b_{k j_{k}}$. Furthermore, by Problem 4.45, $a_{k_{k}}$ is a pivot entry of $A$, and because $A$ is row reduced, $a_{i j_{k}}=0$. Thus, $c_{k} b_{k j_{k}}=0$, and as $b_{k j_{k}}=1, c_{k}=0$. Accordingly $R=R_{i}$, and the theorem is proved.\\n',\n",
       " '\\n4.47. Prove Corollary 4.9: Every matrix $A$ is row equivalent to a unique matrix in row canonical form.\\n\\nSuppose $A$ is row equivalent to matrices $A_{1}$ and $A_{2}$, where $A_{1}$ and $A_{2}$ are in row canonical form. Then $\\\\operatorname{rowsp}(A)=\\\\operatorname{rowsp}\\\\left(A_{1}\\\\right)$ and $\\\\operatorname{rowsp}(A)=\\\\operatorname{rowsp}\\\\left(A_{2}\\\\right)$. Hence, $\\\\operatorname{rowsp}\\\\left(A_{1}\\\\right)=\\\\operatorname{rowsp}\\\\left(A_{2}\\\\right)$. Because $A_{1}$ and $A_{2}$ are in row canonical form, $A_{1}=A_{2}$ by Theorem 4.8. Thus, the corollary is proved.\\n',\n",
       " '\\n4.48. Suppose $R B$ and $A B$ are defined, where $R$ is a row vector and $A$ and $B$ are matrices. Prove\\n\\n(a) $R B$ is a linear combination of the rows of $B$.\\n\\n(b) The row space of $A B$ is contained in the row space of $B$.\\n\\n(c) The column space of $A B$ is contained in the column space of $A$.\\n\\n(d) If $C$ is a column vector and $A C$ is defined, then $A C$ is a linear combination of the columns of $A$.\\n\\n(e) $\\\\operatorname{rank}(A B) \\\\leq \\\\operatorname{rank}(B)$ and $\\\\operatorname{rank}(A B) \\\\leq \\\\operatorname{rank}(A)$.\\n\\n(a) Suppose $R=\\\\left(a_{1}, a_{2}, \\\\ldots, a_{m}\\\\right)$ and $B=\\\\left[b_{i j}\\\\right]$. Let $B_{1}, \\\\ldots, B_{m}$ denote the rows of $B$ and $B^{1}, \\\\ldots, B^{n}$ its columns. Then\\n\\n$$\\n\\\\left.\\\\begin{array}{rl}\\nR B & =\\\\left(R B^{1}, R B^{2}, \\\\ldots, R B^{n}\\\\right) \\\\\\\\\\n& =\\\\left(a_{1} b_{11}+a_{2} b_{21}+\\\\cdots+a_{m} b_{m 1}, \\\\quad \\\\ldots, \\\\quad a_{1} b_{1 n}+a_{2} b_{2 n}+\\\\cdots+a_{m} b_{m n}\\\\right.\\n\\\\end{array}\\\\right)\\n$$\\n\\nThus, $R B$ is a linear combination of the rows of $B$, as claimed.\\n\\n(b) The rows of $A B$ are $R_{i} B$, where $R_{i}$ is the $i$ th row of $A$. Thus, by part (a), each row of $A B$ is in the row space of $B$. Thus, $\\\\operatorname{rowsp}(A B) \\\\subseteq \\\\operatorname{rowsp}(B)$, as claimed.\\n\\n(c) Using part (b), we have $\\\\operatorname{colsp}(A B)=\\\\operatorname{rowsp}(A B)^{T}=\\\\operatorname{rowsp}\\\\left(B^{T} A^{T}\\\\right) \\\\subseteq \\\\operatorname{rowsp}\\\\left(A^{T}\\\\right)=\\\\operatorname{colsp}(A)$.\\n\\n(d) Follows from $(c)$ where $C$ replaces $B$.\\n\\n(e) The row space of $A B$ is contained in the row space of $B$; hence, $\\\\operatorname{rank}(A B) \\\\leq \\\\operatorname{rank}(B)$. Furthermore, the column space of $A B$ is contained in the column space of $A$; hence, $\\\\operatorname{rank}(A B) \\\\leq \\\\operatorname{rank}(A)$.\\n',\n",
       " '\\n4.49. Let $A$ be an $n$-square matrix. Show that $A$ is invertible if and only if $\\\\operatorname{rank}(A)=n$.\\n\\nNote that the rows of the $n$-square identity matrix $I_{n}$ are linearly independent, because $I_{n}$ is in echelon form; hence, $\\\\operatorname{rank}\\\\left(I_{n}\\\\right)=n$. Now if $A$ is invertible, then $A$ is row equivalent to $I_{n}$; hence, $\\\\operatorname{rank}(A)=n$. But if $A$ is not invertible, then $A$ is row equivalent to a matrix with a zero row; hence, $\\\\operatorname{rank}(A)<n$; that is, $A$ is invertible if and only if $\\\\operatorname{rank}(A)=n$.\\n\\n\\n\\\\section*{Applications to Linear Equations}\\n',\n",
       " '4.50. Find the dimension and a basis of the solution space $W$ of each homogeneous system:\\n\\n$$\\n\\\\begin{aligned}\\n& x+2 y+2 z-s+3 t=0 \\\\quad x+2 y+z-2 t=0 \\\\quad x+y+2 z=0 \\\\\\\\\\n& x+2 y+3 z+s+t=0 \\\\quad 2 x+4 y+4 z-3 t=0 \\\\quad 2 x+3 y+3 z=0 \\\\\\\\\\n& 3 x+6 y+8 z+s+5 t=0 \\\\quad 3 x+6 y+7 z-4 t=0 \\\\quad x+3 y+5 z=0 \\\\\\\\\\n& \\\\begin{aligned}\\nx+2 y+2 z-s+3 t & =0 & & x+2 y+2 z-s+3 t & =0 \\\\\\\\\\nz+2 s-2 t & =0 & \\\\text { or } & & =0\\n\\\\end{aligned} \\\\\\\\\\n& 2 z+4 s-4 t=0 \\\\quad 2+2 s-2 t=0\\n\\\\end{aligned}\\n$$\\n\\nThe system in echelon form has two (nonzero) equations in five unknowns. Hence, the system has $5-2=3$ free variables, which are $y, s, t$. Thus, $\\\\operatorname{dim} W=3$. We obtain a basis for $W$ :\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\text { (1) Set } y=1, s=0, t=0 \\\\quad \\\\text { to obtain the solution } \\\\\\\\\\n& v_{1}=(-2,1,0,0,0) \\\\text {. } \\\\\\\\\\n& \\\\text { (2) Set } y=0, s=1, t=0 \\\\quad \\\\text { to obtain the solution } \\\\\\\\\\n& v_{2}=(5,0,-2,1,0) \\\\text {. } \\\\\\\\\\n& \\\\text { (3) Set } y=0, s=0, t=1 \\\\\\\\\\n& \\\\text { to obtain the solution } \\\\quad v_{3}=(-7,0,2,0,1) \\\\text {. }\\n\\\\end{aligned}\\n$$\\n\\nThe set $\\\\left\\\\{v_{1}, v_{2}, v_{3}\\\\right\\\\}$ is a basis of the solution space $W$.\\n\\n(b) (Here we use the matrix format of our homogeneous system.) Reduce the coefficient matrix $A$ to echelon form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{llll}\\n1 & 2 & 1 & -2 \\\\\\\\\\n2 & 4 & 4 & -3 \\\\\\\\\\n3 & 6 & 7 & -4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 1 & -2 \\\\\\\\\\n0 & 0 & 2 & 1 \\\\\\\\\\n0 & 0 & 4 & 2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 1 & -2 \\\\\\\\\\n0 & 0 & 2 & 1 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThis corresponds to the system\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y+2 z-2 t & =0 \\\\\\\\\\n2 z+t & =0\\n\\\\end{aligned}\\n$$\\n\\nThe free variables are $y$ and $t$, and $\\\\operatorname{dim} W=2$.\\n\\n(i) Set $y=1, z=0$ to obtain the solution $u_{1}=(-2,1,0,0)$.\\n\\n(ii) Set $y=0, z=2$ to obtain the solution $u_{2}=(6,0,-1,2)$.\\n\\nThen $\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}$ is a basis of $W$.\\n\\n(c) Reduce the coefficient matrix $A$ to echelon form:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 2 \\\\\\\\\\n2 & 3 & 3 \\\\\\\\\\n1 & 3 & 5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & -1 \\\\\\\\\\n0 & 2 & 3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & -1 \\\\\\\\\\n0 & 0 & 5\\n\\\\end{array}\\\\right]\\n$$\\n\\nThis corresponds to a triangular system with no free variables. Thus, 0 is the only solution; that is, $W=\\\\{0\\\\}$. Hence, $\\\\operatorname{dim} W=0$.\\n',\n",
       " '\\n4.51. Find a homogeneous system whose solution set $W$ is spanned by\\n\\n$$\\n\\\\left\\\\{u_{1}, u_{2}, u_{3}\\\\right\\\\}=\\\\{(1,-2,0,3), \\\\quad(1,-1,-1,4), \\\\quad(1,0,-2,5)\\\\}\\n$$\\n\\nLet $v=(x, y, z, t)$. Then $v \\\\in W$ if and only if $v$ is a linear combination of the vectors $u_{1}, u_{2}, u_{3}$ that span $W$. Thus, form the matrix $M$ whose first columns are $u_{1}, u_{2}, u_{3}$ and whose last column is $v$, and then row reduce $M$ to echelon form. This yields\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 1 & x \\\\\\\\\\n-2 & -1 & 0 & y \\\\\\\\\\n0 & -1 & -2 & z \\\\\\\\\\n3 & 4 & 5 & t\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrc}\\n1 & 1 & 1 & x \\\\\\\\\\n0 & 1 & 2 & 2 x+y \\\\\\\\\\n0 & -1 & -2 & z \\\\\\\\\\n0 & 1 & 2 & -3 x+t\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrcc}\\n1 & 1 & 1 & x \\\\\\\\\\n0 & 1 & 2 & 2 x+y \\\\\\\\\\n0 & 0 & 0 & 2 x+y+z \\\\\\\\\\n0 & 0 & 0 & -5 x-y+t\\n\\\\end{array}\\\\right]\\n$$\\n\\nThen $v$ is a linear combination of $u_{1}, u_{2}, u_{3}$ if $\\\\operatorname{rank}(M)=\\\\operatorname{rank}(A)$, where $A$ is the submatrix without column $v$. Thus, set the last two entries in the fourth column on the right equal to zero to obtain the required homogeneous system:\\n\\n$$\\n\\\\begin{aligned}\\n& 2 x+y+z=0 \\\\\\\\\\n& 5 x+y \\\\quad-t=0\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n4.52. Let $x_{i_{1}}, x_{i_{2}}, \\\\ldots, x_{i_{k}}$ be the free variables of a homogeneous system of linear equations with $n$ unknowns. Let $v_{j}$ be the solution for which $x_{i_{j}}=1$, and all other free variables equal 0 . Show that the solutions $v_{1}, v_{2}, \\\\ldots, v_{k}$ are linearly independent.\\n\\nLet $A$ be the matrix whose rows are the $v_{i}$. We interchange column 1 and column $i_{1}$, then column 2 and column $i_{2}, \\\\ldots$, then column $k$ and column $i_{k}$, and we obtain the $k \\\\times n$ matrix\\n\\n$$\\nB=[I, C]=\\\\left[\\\\begin{array}{ccccccccc}\\n1 & 0 & 0 & \\\\ldots & 0 & 0 & c_{1, k+1} & \\\\ldots & c_{1 n} \\\\\\\\\\n0 & 1 & 0 & \\\\ldots & 0 & 0 & c_{2, k+1} & \\\\ldots & c_{2 n} \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\n0 & 0 & 0 & \\\\ldots & 0 & 1 & c_{k, k+1} & \\\\ldots & c_{k n}\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe above matrix $B$ is in echelon form, and so its rows are independent; hence, $\\\\operatorname{rank}(B)=k$. Because $A$ and $B$ are column equivalent, they have the same $\\\\operatorname{rank}-\\\\operatorname{rank}(A)=k$. But $A$ has $k$ rows; hence, these rows (i.e., the $v_{i}$ ) are linearly independent, as claimed.\\n\\n\\n\\\\section*{Sums, Direct Sums, Intersections}\\n',\n",
       " '4.53. Let $U$ and $W$ be subspaces of a vector space $V$. Show that\\n\\n(a) $U+V$ is a subspace of $V$.\\n\\n(b) $U$ and $W$ are contained in $U+W$.\\n\\n(c) $U+W$ is the smallest subspace containing $U$ and $W$; that is, $U+W=\\\\operatorname{span}(U, W)$.\\n\\n(d) $W+W=W$.\\n\\n(a) Because $U$ and $W$ are subspaces, $0 \\\\in U$ and $0 \\\\in W$. Hence, $0=0+0$ belongs to $U+W$. Now suppose $v, v^{\\\\prime} \\\\in U+W$. Then $v=u+w$ and $v^{\\\\prime}=u^{\\\\prime}+v^{\\\\prime}$, where $u, u^{\\\\prime} \\\\in U$ and $w, w^{\\\\prime} \\\\in W$. Then\\n\\n$$\\na v+b v^{\\\\prime}=\\\\left(a u+b u^{\\\\prime}\\\\right)+\\\\left(a w+b w^{\\\\prime}\\\\right) \\\\in U+W\\n$$\\n\\nThus, $U+W$ is a subspace of $V$.\\n\\n(b) Let $u \\\\in U$. Because $W$ is a subspace, $0 \\\\in W$. Hence, $u=u+0$ belongs to $U+W$. Thus, $U \\\\subseteq U+W$. Similarly, $W \\\\subseteq U+W$.\\n\\n(c) Because $U+W$ is a subspace of $V$ containing $U$ and $W$, it must also contain the linear span of $U$ and $W$. That is, $\\\\operatorname{span}(U, W) \\\\subseteq U+W$.\\n\\nOn the other hand, if $v \\\\in U+W$, then $v=u+w=1 u+1 w$, where $u \\\\in U$ and $w \\\\in W$. Thus, $v$ is a linear combination of elements in $U \\\\cup W$, and so $v \\\\in \\\\operatorname{span}(U, W)$. Hence, $U+W \\\\subseteq \\\\operatorname{span}(U, W)$.\\n\\nThe two inclusion relations give the desired result.\\n\\n(d) Because $W$ is a subspace of $V$, we have that $W$ is closed under vector addition; hence, $W+W \\\\subseteq W$. By part (a), $W \\\\subseteq W+W$. Hence, $W+W=W$.\\n',\n",
       " '\\n4.54. Consider the following subspaces of $\\\\mathbf{R}^{5}$ :\\n\\n$$\\n\\\\begin{aligned}\\n& U=\\\\operatorname{span}\\\\left(u_{1}, u_{2}, u_{3}\\\\right)=\\\\operatorname{span}\\\\{(1,3,-2,2,3), \\\\quad(1,4,-3,4,2), \\\\quad(2,3,-1,-2,9)\\\\} \\\\\\\\\\n& W=\\\\operatorname{span}\\\\left(w_{1}, w_{2}, w_{3}\\\\right)=\\\\operatorname{span}\\\\{(1,3,0,2,1), \\\\quad(1,5,-6,6,3), \\\\quad(2,5,3,2,1)\\\\}\\n\\\\end{aligned}\\n$$\\n\\nFind a basis and the dimension of (a) $U+W$, (b) $U \\\\cap W$.\\\\\\\\\\n(a) $U+W$ is the space spanned by all six vectors. Hence, form the matrix whose rows are the given six vectors, and then row reduce to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{rrrrr}\\n1 & 3 & -2 & 2 & 3 \\\\\\\\\\n1 & 4 & -3 & 4 & 2 \\\\\\\\\\n2 & 3 & -1 & -2 & 9 \\\\\\\\\\n1 & 3 & 0 & 2 & 1 \\\\\\\\\\n1 & 5 & -6 & 6 & 3 \\\\\\\\\\n2 & 5 & 3 & 2 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 3 & -2 & 2 & 3 \\\\\\\\\\n0 & 1 & -1 & 2 & -1 \\\\\\\\\\n0 & -3 & 3 & -6 & 3 \\\\\\\\\\n0 & 0 & 2 & 0 & -2 \\\\\\\\\\n0 & 2 & -4 & 4 & 0 \\\\\\\\\\n0 & -1 & 7 & -2 & -5\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrr}\\n1 & 3 & -2 & 2 & 3 \\\\\\\\\\n0 & 1 & -1 & 2 & -1 \\\\\\\\\\n0 & 0 & 1 & 0 & -1 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe following three nonzero rows of the echelon matrix form a basis of $U \\\\cap W$ :\\n\\n$$\\n(1,3,-2,2,2,3), \\\\quad(0,1,-1,2,-1), \\\\quad(0,0,1,0,-1)\\n$$\\n\\nThus, $\\\\operatorname{dim}(U+W)=3$.\\n\\n(b) Let $v=(x, y, z, s, t)$ denote an arbitrary element in $\\\\mathbf{R}^{5}$. First find, say as in Problem 4.49, homogeneous systems whose solution sets are $U$ and $W$, respectively.\\n\\nLet $M$ be the matrix whose columns are the $u_{i}$ and $v$, and reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & x \\\\\\\\\\n3 & 4 & 3 & y \\\\\\\\\\n-2 & -3 & -1 & z \\\\\\\\\\n2 & 4 & -2 & s \\\\\\\\\\n3 & 2 & 9 & t\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{cccc}\\n1 & 1 & 2 & x \\\\\\\\\\n0 & 1 & -3 & -3 x+y \\\\\\\\\\n0 & 0 & 0 & -x+y+z \\\\\\\\\\n0 & 0 & 0 & 4 x-2 y+s \\\\\\\\\\n0 & 0 & 0 & -6 x+y+t\\n\\\\end{array}\\\\right]\\n$$\\n\\nSet the last three entries in the last column equal to zero to obtain the following homogeneous system whose solution set is $U$ :\\n\\n$$\\n-x+y+z=0, \\\\quad 4 x-2 y+s=0, \\\\quad-6 x+y+t=0\\n$$\\n\\nNow let $M^{\\\\prime}$ be the matrix whose columns are the $w_{i}$ and $v$, and reduce $M^{\\\\prime}$ to echelon form:\\n\\n$$\\nM^{\\\\prime}=\\\\left[\\\\begin{array}{rrrr}\\n1 & 1 & 2 & x \\\\\\\\\\n3 & 5 & 5 & y \\\\\\\\\\n0 & -6 & 3 & z \\\\\\\\\\n2 & 6 & 2 & s \\\\\\\\\\n1 & 3 & 1 & t\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrc}\\n1 & 1 & 2 & x \\\\\\\\\\n0 & 2 & -1 & -3 x+y \\\\\\\\\\n0 & 0 & 0 & -9 x+3 y+z \\\\\\\\\\n0 & 0 & 0 & 4 x-2 y+s \\\\\\\\\\n0 & 0 & 0 & 2 x-y+t\\n\\\\end{array}\\\\right]\\n$$\\n\\nAgain set the last three entries in the last column equal to zero to obtain the following homogeneous system whose solution set is $W$ :\\n\\n$$\\n-9+3+z=0, \\\\quad 4 x-2 y+s=0, \\\\quad 2 x-y+t=0\\n$$\\n\\nCombine both of the above systems to obtain a homogeneous system, whose solution space is $U \\\\cap W$, and reduce the system to echelon form, yielding\\n\\n$$\\n\\\\begin{aligned}\\n-x+y+z & =0 \\\\\\\\\\n2 y+4 z+ & =0 \\\\\\\\\\n8 z+5 s+2 t & =0 \\\\\\\\\\ns-2 t & =0\\n\\\\end{aligned}\\n$$\\n\\nThere is one free variable, which is $t$; hence, $\\\\operatorname{dim}(U \\\\cap W)=1$. Setting $t=2$, we obtain the solution $u=(1,4,-3,4,2)$, which forms our required basis of $U \\\\cap W$.\\n',\n",
       " '\\n4.55. Suppose $U$ and $W$ are distinct four-dimensional subspaces of a vector space $V$, where $\\\\operatorname{dim} V=6$. Find the possible dimensions of $U \\\\cap W$.\\n\\nBecause $U$ and $W$ are distinct, $U+W$ properly contains $U$ and $W$; consequently, $\\\\operatorname{dim}(U+W)>4$. But $\\\\operatorname{dim}(U+W)$ cannot be greater than 6 , as $\\\\operatorname{dim} V=6$. Hence, we have two possibilities: (a) $\\\\operatorname{dim}(U+W)=5$ or (b) $\\\\operatorname{dim}(U+W)=6$. By Theorem 4.20,\\n\\n$$\\n\\\\operatorname{dim}(U \\\\cap W)=\\\\operatorname{dim} U+\\\\operatorname{dim} W-\\\\operatorname{dim}(U+W)=8-\\\\operatorname{dim}(U+W)\\n$$\\n\\nThus (a) $\\\\operatorname{dim}(U \\\\cap W)=3$ or (b) $\\\\operatorname{dim}(U \\\\cap W)=2$.\\n',\n",
       " '\\n4.56. Let $U$ and $W$ be the following subspaces of $\\\\mathbf{R}^{3}$ :\\n\\n$U=\\\\{(a, b, c): a=b=c\\\\} \\\\quad$ and $\\\\quad W=\\\\{(0, b, c)\\\\}$\\n\\n(Note that $W$ is the $y z$-plane.) Show that $\\\\mathbf{R}^{3}=U \\\\oplus W$.\\n\\nFirst we show that $U \\\\cap W=\\\\{0\\\\}$. Suppose $v=(a, b, c) \\\\in U \\\\cap W$. Then $a=b=c$ and $a=0$. Hence, $a=0, b=0, c=0$. Thus, $v=0=(0,0,0)$.\\n\\nNext we show that $\\\\mathbf{R}^{3}=U+W$. For, if $v=(a, b, c) \\\\in \\\\mathbf{R}^{3}$, then\\n\\n$$\\nv=(a, a, a)+(0, b-a, c-a) \\\\quad \\\\text { where } \\\\quad(a, a, a) \\\\in U \\\\quad \\\\text { and } \\\\quad(0, b-a, c-a) \\\\in W\\n$$\\n\\nBoth conditions $U \\\\cap W=\\\\{0\\\\}$ and $U+W=\\\\mathbf{R}^{3}$ imply that $\\\\mathbf{R}^{3}=U \\\\oplus W$.\\n',\n",
       " '\\n4.57. Suppose that $U$ and $W$ are subspaces of a vector space $V$ and that $S=\\\\left\\\\{u_{i}\\\\right\\\\}$ spans $U$ and $S^{\\\\prime}=\\\\left\\\\{w_{j}\\\\right\\\\}$ spans $W$. Show that $S \\\\cup S^{\\\\prime}$ spans $U+W$. (Accordingly, by induction, if $S_{i}$ spans $W_{i}$, for $i=1,2, \\\\ldots, n$, then $S_{1} \\\\cup \\\\ldots \\\\cup S_{n}$ spans $W_{1}+\\\\cdots+W_{n}$.)\\n\\nLet $v \\\\in U+W$. Then $v=u+w$, where $u \\\\in U$ and $w \\\\in W$. Because $S$ spans $U, u$ is a linear combination of $u_{i}$, and as $S^{\\\\prime}$ spans $W, w$ is a linear combination of $w_{j}$; say\\n\\n$$\\nu=a_{1} u_{i_{1}}+a_{2} u_{i_{2}}+\\\\cdots+a_{r} u_{i_{r}} \\\\quad \\\\text { and } \\\\quad v=b_{1} w_{j_{1}}+b_{2} w_{j_{2}}+\\\\cdots+b_{s} w_{j_{s}}\\n$$\\n\\nwhere $a_{i}, b_{j} \\\\in K$. Then\\n\\n$$\\nv=u+w=a_{1} u_{i_{1}}+a_{2} u_{i_{2}}+\\\\cdots+a_{r} u_{i_{r}}+b_{1} w_{j_{1}}+b_{2} w_{j_{2}}+\\\\cdots+b_{s} w_{j_{s}}\\n$$\\n\\nAccordingly, $S \\\\cup S^{\\\\prime}=\\\\left\\\\{u_{i}, w_{j}\\\\right\\\\}$ spans $U+W$.\\n',\n",
       " '\\n4.58. Prove Theorem 4.20: Suppose $U$ and $V$ are finite-dimensional subspaces of a vector space $V$. Then $U+W$ has finite dimension and\\n\\n$$\\n\\\\operatorname{dim}(U+W)=\\\\operatorname{dim} U+\\\\operatorname{dim} W-\\\\operatorname{dim}(U \\\\cap W)\\n$$\\n\\nObserve that $U \\\\cap W$ is a subspace of both $U$ and $W$. Suppose $\\\\operatorname{dim} U=m, \\\\operatorname{dim} W=n$, $\\\\operatorname{dim}(U \\\\cap W)=r$. Suppose $\\\\left\\\\{v_{1}, \\\\ldots, v_{r}\\\\right\\\\}$ is a basis of $U \\\\cap W$. By Theorem 4.16, we can extend $\\\\left\\\\{v_{i}\\\\right\\\\}$ to a basis of $U$ and to a basis of $W$; say\\n\\n$$\\n\\\\left\\\\{v_{1}, \\\\ldots, v_{r}, u_{1}, \\\\ldots, u_{m-r}\\\\right\\\\} \\\\quad \\\\text { and } \\\\quad\\\\left\\\\{v_{1}, \\\\ldots, v_{r}, w_{1}, \\\\ldots, w_{n-r}\\\\right\\\\}\\n$$\\n\\nare bases of $U$ and $W$, respectively. Let\\n\\n$$\\nB=\\\\left\\\\{v_{1}, \\\\ldots, v_{r}, u_{1}, \\\\ldots, u_{m-r}, w_{1}, \\\\ldots, w_{n-r}\\\\right\\\\}\\n$$\\n\\nNote that $B$ has exactly $m+n-r$ elements. Thus, the theorem is proved if we can show that $B$ is a basis of $U+W$. Because $\\\\left\\\\{v_{i}, u_{j}\\\\right\\\\}$ spans $U$ and $\\\\left\\\\{v_{i}, w_{k}\\\\right\\\\}$ spans $W$, the union $B=\\\\left\\\\{v_{i}, u_{j}, w_{k}\\\\right\\\\}$ spans $U+W$. Thus, it suffices to show that $B$ is independent.\\n\\nSuppose\\n\\n\\n\\\\begin{equation*}\\na_{1} v_{1}+\\\\cdots+a_{r} v_{r}+b_{1} u_{1}+\\\\cdots+b_{m-r} u_{m-r}+c_{1} w_{1}+\\\\cdots+c_{n-r} w_{n-r}=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nwhere $a_{i}, b_{j}, c_{k}$ are scalars. Let\\n\\n\\n\\\\begin{equation*}\\nv=a_{1} v_{1}+\\\\cdots+a_{r} v_{r}+b_{1} u_{1}+\\\\cdots+b_{m-r} u_{m-r} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nBy (1), we also have\\n\\n\\n\\\\begin{equation*}\\nv=-c_{1} w_{1}-\\\\cdots-c_{n-r} w_{n-r} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nBecause $\\\\left\\\\{v_{i}, u_{j}\\\\right\\\\} \\\\subseteq U, v \\\\in U$ by (2); and as $\\\\left\\\\{w_{k}\\\\right\\\\} \\\\subseteq W, v \\\\in W$ by (3). Accordingly, $v \\\\in U \\\\cap W$. Now $\\\\left\\\\{v_{i}\\\\right\\\\}$ is a basis of $U \\\\cap W$, and so there exist scalars $d_{1}, \\\\ldots, d_{r}$ for which $v=d_{1} v_{1}+\\\\cdots+d_{r} v_{r}$. Thus, by (3), we have\\n\\n$$\\nd_{1} v_{1}+\\\\cdots+d_{r} v_{r}+c_{1} w_{1}+\\\\cdots+c_{n-r} w_{n-r}=0\\n$$\\n\\nBut $\\\\left\\\\{v_{i}, w_{k}\\\\right\\\\}$ is a basis of $W$, and so is independent. Hence, the above equation forces $c_{1}=0, \\\\ldots, c_{n-r}=0$. Substituting this into (1), we obtain\\n\\n$$\\na_{1} v_{1}+\\\\cdots+a_{r} v_{r}+b_{1} u_{1}+\\\\cdots+b_{m-r} u_{m-r}=0\\n$$\\n\\nBut $\\\\left\\\\{v_{i}, u_{j}\\\\right\\\\}$ is a basis of $U$, and so is independent. Hence, the above equation forces $a_{1}=$ $0, \\\\ldots, a_{r}=0, b_{1}=0, \\\\ldots, b_{m-r}=0$. proved.\\n\\nBecause (1) implies that the $a_{i}, b_{j}, c_{k}$ are all $0, B=\\\\left\\\\{v_{i}, u_{j}, w_{k}\\\\right\\\\}$ is independent, and the theorem is\\n',\n",
       " '\\n4.59. Prove Theorem 4.21: $V=U \\\\oplus W$ if and only if (i) $V=U+W$, (ii) $U \\\\cap W=\\\\{0\\\\}$.\\n\\nSuppose $V=U \\\\oplus W$. Then any $v \\\\in V$ can be uniquely written in the form $v=u+w$, where $u \\\\in U$ and $w \\\\in W$. Thus, in particular, $V=U+W$. Now suppose $v \\\\in U \\\\cap W$. Then\\n\\n(1) $v=v+0$, where $v \\\\in U, 0 \\\\in W$, (2) $v=0+v$, where $0 \\\\in U, v \\\\in W$.\\n\\nThus, $v=0+0=0$ and $U \\\\cap W=\\\\{0\\\\}$.\\n\\nOn the other hand, suppose $V=U+W$ and $U \\\\cap W=\\\\{0\\\\}$. Let $v \\\\in V$. Because $V=U+W$, there exist $u \\\\in U$ and $w \\\\in W$ such that $v=u+w$. We need to show that such a sum is unique. Suppose also that $v=u^{\\\\prime}+w^{\\\\prime}$, where $u^{\\\\prime} \\\\in U$ and $w^{\\\\prime} \\\\in W$. Then\\n\\n$$\\nu+w=u^{\\\\prime}+w^{\\\\prime}, \\\\quad \\\\text { and so } \\\\quad u-u^{\\\\prime}=w^{\\\\prime}-w\\n$$\\n\\nBut $u-u^{\\\\prime} \\\\in U$ and $w^{\\\\prime}-w \\\\in W$; hence, by $U \\\\cap W=\\\\{0\\\\}$,\\n\\n$$\\nu-u^{\\\\prime}=0, \\\\quad w^{\\\\prime}-w=0, \\\\quad \\\\text { and so } \\\\quad u=u^{\\\\prime}, \\\\quad w=w^{\\\\prime}\\n$$\\n\\nThus, such a sum for $v \\\\in V$ is unique, and $V=U \\\\oplus W$.\\n',\n",
       " '\\n4.60. Prove Theorem 4.22 (for two factors): Suppose $V=U \\\\oplus W$. Also, suppose $S=\\\\left\\\\{u_{1}, \\\\ldots, u_{m}\\\\right\\\\}$ and $S^{\\\\prime}=\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$ are linearly independent subsets of $U$ and $W$, respectively. Then\\n\\n(a) The union $S \\\\cup S^{\\\\prime}$ is linearly independent in $V$.\\n\\n(b) If $S$ and $S^{\\\\prime}$ are bases of $U$ and $W$, respectively, then $S \\\\cup S^{\\\\prime}$ is a basis of $V$.\\n\\n(c) $\\\\operatorname{dim} V=\\\\operatorname{dim} U+\\\\operatorname{dim} W$.\\n\\n(a) Suppose $a_{1} u_{1}+\\\\cdots+a_{m} u_{m}+b_{1} w_{1}+\\\\cdots+b_{n} w_{n}=0$, where $a_{i}$, $b_{j}$ are scalars. Then\\n\\n$$\\n\\\\left(a_{1} u_{1}+\\\\cdots+a_{m} u_{m}\\\\right)+\\\\left(b_{1} w_{1}+\\\\cdots+b_{n} w_{n}\\\\right)=0=0+0\\n$$\\n\\nwhere $0, a_{1} u_{1}+\\\\cdots+a_{m} u_{m} \\\\in U$ and $0, b_{1} w_{1}+\\\\cdots+b_{n} w_{n} \\\\in W$. Because such a sum for 0 is unique, this leads to\\n\\n$$\\na_{1} u_{1}+\\\\cdots+a_{m} u_{m}=0 \\\\quad \\\\text { and } \\\\quad b_{1} w_{1}+\\\\cdots+b_{n} w_{n}=0\\n$$\\n\\nBecause $S_{1}$ is linearly independent, each $a_{i}=0$, and because $S_{2}$ is linearly independent, each $b_{j}=0$. Thus, $S=S_{1} \\\\cup S_{2}$ is linearly independent.\\n\\n(b) By part (a), $S=S_{1} \\\\cup S_{2}$ is linearly independent, and, by Problem 4.55, $S=S_{1} \\\\cup S_{2}$ spans $V=U+W$. Thus, $S=S_{1} \\\\cup S_{2}$ is a basis of $V$.\\n\\n(c) This follows directly from part (b).\\n\\n\\n\\\\section*{Coordinates}\\n',\n",
       " '4.61. Relative to the basis $S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,1),(2,3)\\\\}$ of $\\\\mathbf{R}^{2}$, find the coordinate vector of $v$, where (a) $v=(4,-3)$, (b) $v=(a, b)$.\\n\\nIn each case, set\\n\\n$$\\nv=x u_{1}+y u_{2}=x(1,1)+y(2,3)=(x+2 y, x+3 y)\\n$$\\n\\nand then solve for $x$ and $y$.\\n\\n(a) We have\\n\\n$$\\n(4,-3)=(x+2 y, x+3 y) \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+2 y & =4 \\\\\\\\\\nx+3 y & =-3\\n\\\\end{aligned}\\n$$\\n\\nThe solution is $x=18, y=-7$. Hence, $[v]=[18,-7]$.\\n\\n(b) We have\\n\\n$$\\n(a, b)=(x+2 y, x+3 y) \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n& x+2 y=a \\\\\\\\\\n& x+3 y=b\\n\\\\end{aligned}\\n$$\\n\\nThe solution is $x=3 a-2 b, y=-a+b$. Hence, $[v]=\\\\left[\\\\begin{array}{ll}3 a-2 b, & a+b\\\\end{array}\\\\right]$.\\n',\n",
       " '\\n4.62. Find the coordinate vector of $v=(a, b, c)$ in $\\\\mathbf{R}^{3}$ relative to\\n\\n(a) the usual basis $E=\\\\{(1,0,0),(0,1,0),(0,0,1)\\\\}$,\\n\\n(b) the basis $S=\\\\left\\\\{u_{1}, u_{2}, u_{3}\\\\right\\\\}=\\\\{(1,1,1),(1,1,0),(1,0,0)\\\\}$.\\n\\n(a) Relative to the usual basis $E$, the coordinates of $[v]_{E}$ are the same as $v$. That is, $[v]_{E}=[a, b, c]$.\\n\\n(b) Set $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknown scalars $x, y$, $z$. This yields\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb \\\\\\\\\\nc\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n0\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n0 \\\\\\\\\\n0\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n& x+y+z=a \\\\\\\\\\n& x+y=b \\\\\\\\\\n& x\\n\\\\end{aligned}\\n$$\\n\\nSolving the system yields $x=c, y=b-c, z=a-b$. Thus, $[v]_{S}=[c, b-c, a-b]$.\\n',\n",
       " '\\n4.63. Consider the vector space $\\\\mathbf{P}_{3}(t)$ of polynomials of degree $\\\\leq 3$.\\n\\n(a) Show that $S=\\\\left\\\\{(t-1)^{3},(t-1)^{2}, t-1,1\\\\right\\\\}$ is a basis of $\\\\mathbf{P}_{3}(t)$.\\n\\n(b) Find the coordinate vector $[v]$ of $v=3 t^{3}-4 t^{2}+2 t-5$ relative to $S$.\\n\\n(a) The degree of $(t-1)^{k}$ is $k$; writing the polynomials of $S$ in reverse order, we see that no polynomial is a linear combination of preceding polynomials. Thus, the polynomials are linearly independent, and, because $\\\\operatorname{dim} \\\\mathbf{P}_{3}(t)=4$, they form a basis of $\\\\mathbf{P}_{3}(t)$.\\n\\n(b) Set $v$ as a linear combination of the basis vectors using unknown scalars $x, y, z, s$. We have\\n\\n$$\\n\\\\begin{aligned}\\nv & =3 t^{3}+4 t^{2}+2 t-5=x(t-1)^{3}+y(t-1)^{2}+z(t-1)+s(1) \\\\\\\\\\n& =x\\\\left(t^{3}-3 t^{2}+3 t-1\\\\right)+y\\\\left(t^{2}-2 t+1\\\\right)+z(t-1)+s(1) \\\\\\\\\\n& =x t^{3}-3 x t^{2}+3 x t-x+y t^{2}-2 y t+y+z t-z+s \\\\\\\\\\n& =x t^{3}+(-3 x+y) t^{2}+(3 x-2 y+z) t+(-x+y-z+s)\\n\\\\end{aligned}\\n$$\\n\\nThen set coefficients of the same powers of $t$ equal to each other to obtain\\n\\n$$\\nx=3, \\\\quad-3 x+y=4, \\\\quad 3 x-2 y+z=2, \\\\quad-x+y-z+s=-5\\n$$\\n\\nSolving the system yields $x=3, y=13, z=19, s=4$. Thus, $[v]=[3,13,19,4]$.\\n',\n",
       " '\\n4.64. Find the coordinate vector of $A=\\\\left[\\\\begin{array}{rr}2 & 3 \\\\\\\\ 4 & -7\\\\end{array}\\\\right]$ in the real vector space $\\\\mathbf{M}=\\\\mathbf{M}_{2,2}$ relative to\\n\\n(a) the basis $S=\\\\left\\\\{\\\\left[\\\\begin{array}{ll}1 & 1 \\\\\\\\ 1 & 1\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rr}1 & -1 \\\\\\\\ 1 & 0\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rr}1 & -1 \\\\\\\\ 0 & 0\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}1 & 0 \\\\\\\\ 0 & 0\\\\end{array}\\\\right]\\\\right\\\\}$,\\n\\n(b) the usual basis $E=\\\\left\\\\{\\\\left[\\\\begin{array}{ll}1 & 0 \\\\\\\\ 0 & 0\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}0 & 1 \\\\\\\\ 0 & 0\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}0 & 0 \\\\\\\\ 1 & 0\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}0 & 0 \\\\\\\\ 0 & 1\\\\end{array}\\\\right]\\\\right\\\\}$\\n\\n(a) Set $A$ as a linear combination of the basis vectors using unknown scalars $x, y, z, t$ as follows:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n2 & 3 \\\\\\\\\\n4 & -7\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n1 & 1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{rr}\\n1 & -1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{rr}\\n1 & -1 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]+t\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}\\nx+z+t & x-y-z \\\\\\\\\\nx+y & x\\n\\\\end{array}\\\\right]\\n$$\\n\\nSet corresponding entries equal to each other to obtain the system\\n\\n$$\\nx+z+t=2, \\\\quad x-y-z=3, \\\\quad x+y=4, \\\\quad x=-7\\n$$\\n\\nSolving the system yields $x=-7, y=11, z=-21, t=30$. Thus, $[A]_{S}=[-7,11,-21,30]$. (Note that the coordinate vector of $A$ is a vector in $\\\\mathbf{R}^{4}$, because $\\\\operatorname{dim} \\\\mathbf{M}=4$.)\\n\\n(b) Expressing $A$ as a linear combination of the basis matrices yields\\n\\n$$\\n\\\\left[\\\\begin{array}{rr}\\n2 & 3 \\\\\\\\\\n4 & -7\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{ll}\\n0 & 1 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]+t\\\\left[\\\\begin{array}{ll}\\n0 & 0 \\\\\\\\\\n0 & 1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\nx & y \\\\\\\\\\nz & t\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $x=2, y=3, z=4, t=-7$. Hence, $[A]=[2,3,4,-7]$, whose components are the elements of $A$ written row by row.\\n\\nRemark: This result is true in general; that is, if $A$ is any $m \\\\times n$ matrix in $\\\\mathbf{M}=\\\\mathbf{M}_{m, n}$, then the coordinates of $A$ relative to the usual basis of $\\\\mathbf{M}$ are the elements of $A$ written row by row.\\n',\n",
       " '\\n4.65. In the space $\\\\mathbf{M}=\\\\mathbf{M}_{2,3}$, determine whether or not the following matrices are linearly dependent:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 3 \\\\\\\\\\n4 & 0 & 5\\n\\\\end{array}\\\\right], \\\\quad B=\\\\left[\\\\begin{array}{rrr}\\n2 & 4 & 7 \\\\\\\\\\n10 & 1 & 13\\n\\\\end{array}\\\\right], \\\\quad C=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 5 \\\\\\\\\\n8 & 2 & 11\\n\\\\end{array}\\\\right]\\n$$\\n\\nIf the matrices are linearly dependent, find the dimension and a basis of the subspace $W$ of $\\\\mathbf{M}$ spanned by the matrices.\\n\\nThe coordinate vectors of the above matrices relative to the usual basis of $\\\\mathbf{M}$ are as follows:\\n\\n$$\\n[A]=[1,2,3,4,0,5], \\\\quad[B]=[2,4,7,10,1,13], \\\\quad[C]=[1,2,5,8,2,11]\\n$$\\n\\nForm the matrix $M$ whose rows are the above coordinate vectors, and reduce $M$ to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 2 & 3 & 4 & 0 & 5 \\\\\\\\\\n2 & 4 & 7 & 10 & 1 & 13 \\\\\\\\\\n1 & 2 & 5 & 8 & 2 & 11\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{llllll}\\n1 & 2 & 3 & 4 & 0 & 5 \\\\\\\\\\n0 & 0 & 1 & 2 & 1 & 3 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nBecause the echelon matrix has only two nonzero rows, the coordinate vectors $[A],[B],[C]$ span a space of dimension two, and so they are linearly dependent. Thus, $A, B, C$ are linearly dependent. Furthermore, $\\\\operatorname{dim} W=2$, and the matrices\\n\\n$$\\nw_{1}=\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 3 \\\\\\\\\\n4 & 0 & 5\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad w_{2}=\\\\left[\\\\begin{array}{lll}\\n0 & 0 & 1 \\\\\\\\\\n2 & 1 & 3\\n\\\\end{array}\\\\right]\\n$$\\n\\ncorresponding to the nonzero rows of the echelon matrix form a basis of $W$.\\n\\n\\n\\\\section*{Miscellaneous Problems}\\n',\n",
       " '4.66. Consider a finite sequence of vectors $S=\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$. Let $T$ be the sequence of vectors obtained from $S$ by one of the following \"elementary operations\": (i) interchange two vectors, (ii) multiply a vector by a nonzero scalar, (iii) add a multiple of one vector to another. Show that $S$ and $T$ span the same space $W$. Also show that $T$ is independent if and only if $S$ is independent.\\n\\nObserve that, for each operation, the vectors in $T$ are linear combinations of vectors in $S$. On the other hand, each operation has an inverse of the same type (Prove!); hence, the vectors in $S$ are linear combinations of vectors in $T$. Thus $S$ and $T$ span the same space $W$. Also, $T$ is independent if and only if $\\\\operatorname{dim} W=n$, and this is true if and only if $S$ is also independent.\\n',\n",
       " '\\n4.67. Let $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$ be row equivalent $m \\\\times n$ matrices over a field $K$, and let $v_{1}, \\\\ldots, v_{n}$ be any vectors in a vector space $V$ over $K$. Let\\n\\n$$\\n\\\\begin{aligned}\\n& u_{1}=a_{11} v_{1}+a_{12} v_{2}+\\\\cdots+a_{1 n} v_{n} \\\\quad w_{1}=b_{11} v_{1}+b_{12} v_{2}+\\\\cdots+b_{1 n} v_{n} \\\\\\\\\\n& u_{2}=a_{21} v_{1}+a_{22} v_{2}+\\\\cdots+a_{2 n} v_{n} \\\\quad w_{2}=b_{21} v_{1}+b_{22} v_{2}+\\\\cdots+b_{2 n} v_{n} \\\\\\\\\\n& u_{m}=a_{m 1} v_{1}+a_{m 2} v_{2}+\\\\cdots+a_{m n} v_{n} \\\\quad w_{m}=b_{m 1} v_{1}+b_{m 2} v_{2}+\\\\cdots+b_{m n} v_{n}\\n\\\\end{aligned}\\n$$\\n\\nShow that $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{w_{i}\\\\right\\\\}$ span the same space.\\n\\nApplying an \"elementary operation\" of Problem 4.66 to $\\\\left\\\\{u_{i}\\\\right\\\\}$ is equivalent to applying an elementary row operation to the matrix $A$. Because $A$ and $B$ are row equivalent, $B$ can be obtained from $A$ by a sequence of elementary row operations; hence, $\\\\left\\\\{w_{i}\\\\right\\\\}$ can be obtained from $\\\\left\\\\{u_{i}\\\\right\\\\}$ by the corresponding sequence of operations. Accordingly, $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{w_{i}\\\\right\\\\}$ span the same space.\\n',\n",
       " '\\n4.68. Let $v_{1}, \\\\ldots, v_{n}$ belong to a vector space $V$ over $K$, and let $P=\\\\left[a_{i j}\\\\right]$ be an $n$-square matrix over $K$. Let\\n\\n$$\\nw_{1}=a_{11} v_{1}+a_{12} v_{2}+\\\\cdots+a_{1 n} v_{n}, \\\\quad \\\\cdots, \\\\quad w_{n}=a_{n 1} v_{1}+a_{n 2} v_{2}+\\\\cdots+a_{n n} v_{n}\\n$$\\n\\n(a) Suppose $P$ is invertible. Show that $\\\\left\\\\{w_{i}\\\\right\\\\}$ and $\\\\left\\\\{v_{i}\\\\right\\\\}$ span the same space; hence, $\\\\left\\\\{w_{i}\\\\right\\\\}$ is independent if and only if $\\\\left\\\\{v_{i}\\\\right\\\\}$ is independent.\\n\\n(b) Suppose $P$ is not invertible. Show that $\\\\left\\\\{w_{i}\\\\right\\\\}$ is dependent.\\n\\n(c) Suppose $\\\\left\\\\{w_{i}\\\\right\\\\}$ is independent. Show that $P$ is invertible.\\\\\\\\\\n(a) Because $P$ is invertible, it is row equivalent to the identity matrix $I$. Hence, by Problem 4.67, $\\\\left\\\\{w_{i}\\\\right\\\\}$ and $\\\\left\\\\{v_{i}\\\\right\\\\}$ span the same space. Thus, one is independent if and only if the other is.\\n\\n(b) Because $P$ is not invertible, it is row equivalent to a matrix with a zero row. This means that $\\\\left\\\\{w_{i}\\\\right\\\\}$ spans a space that has a spanning set of less than $n$ elements. Thus, $\\\\left\\\\{w_{i}\\\\right\\\\}$ is dependent.\\n\\n(c) This is the contrapositive of the statement of (b), and so it follows from (b).\\n',\n",
       " '\\n4.69. Suppose that $A_{1}, A_{2}, \\\\ldots$ are linearly independent sets of vectors, and that $A_{1} \\\\subseteq A_{2} \\\\subseteq \\\\ldots$ Show that the union $A=A_{1} \\\\cup A_{2} \\\\cup \\\\ldots$ is also linearly independent.\\n\\nSuppose $A$ is linearly dependent. Then there exist vectors $v_{1}, \\\\ldots, v_{n} \\\\in A$ and scalars $a_{1}, \\\\ldots, a_{n} \\\\in K$, not all of them 0 , such that\\n\\n\\n\\\\begin{equation*}\\na_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause $A=\\\\cup A_{i}$ and the $v_{i} \\\\in A$, there exist sets $A_{i_{1}}, \\\\ldots, A_{i_{n}}$ such that\\n\\n$$\\nv_{1} \\\\in A_{i_{1}}, \\\\quad v_{2} \\\\in A_{i_{2}}, \\\\quad \\\\ldots, \\\\quad v_{n} \\\\in A_{i_{n}}\\n$$\\n\\nLet $k$ be the maximum index of the sets $A_{i_{j}}: k=\\\\max \\\\left(i_{1}, \\\\ldots, i_{n}\\\\right)$. It follows then, as $A_{1} \\\\subseteq A_{2} \\\\subseteq \\\\ldots$, that each $A_{i_{j}}$ is contained in $A_{k}$. Hence, $v_{1}, v_{2}, \\\\ldots, v_{n} \\\\in A_{k}$, and so, by (1), $A_{k}$ is linearly dependent, which contradicts our hypothesis. Thus, $A$ is linearly independent.\\n',\n",
       " '\\n4.70. Let $K$ be a subfield of a field $L$, and let $L$ be a subfield of a field $E$. (Thus, $K \\\\subseteq L \\\\subseteq E$, and $K$ is a subfield of $E$.) Suppose $E$ is of dimension $n$ over $L$, and $L$ is of dimension $m$ over $K$. Show that $E$ is of dimension $m n$ over $K$.\\n\\nSuppose $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ is a basis of $E$ over $L$ and $\\\\left\\\\{a_{1}, \\\\ldots, a_{m}\\\\right\\\\}$ is a basis of $L$ over $K$. We claim that $\\\\left\\\\{a_{i} v_{j}: i=1, \\\\ldots, m, j=1, \\\\ldots, n\\\\right\\\\}$ is a basis of $E$ over $K$. Note that $\\\\left\\\\{a_{i} v_{j}\\\\right\\\\}$ contains $m n$ elements.\\n\\nLet $w$ be any arbitrary element in $E$. Because $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ spans $E$ over $L, w$ is a linear combination of the $v_{i}$ with coefficients in $L$ :\\n\\n\\n\\\\begin{equation*}\\nw=b_{1} v_{1}+b_{2} v_{2}+\\\\cdots+b_{n} v_{n}, \\\\quad b_{i} \\\\in L \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause $\\\\left\\\\{a_{1}, \\\\ldots, a_{m}\\\\right\\\\}$ spans $L$ over $K$, each $b_{i} \\\\in L$ is a linear combination of the $a_{j}$ with coefficients in $K$ :\\n\\n$$\\n\\\\begin{aligned}\\n& b_{1}=k_{11} a_{1}+k_{12} a_{2}+\\\\cdots+k_{1 m} a_{m} \\\\\\\\\\n& b_{2}=k_{21} a_{1}+k_{22} a_{2}+\\\\cdots+k_{2 m} a_{m} \\\\\\\\\\n& b_{n}=k_{n 1} a_{1}+k_{n 2} a_{2}+\\\\cdots+k_{m n} a_{m}\\n\\\\end{aligned}\\n$$\\n\\nwhere $k_{i j} \\\\in K$. Substituting in (1), we obtain\\n\\n$$\\n\\\\begin{aligned}\\nw & =\\\\left(k_{11} a_{1}+\\\\cdots+k_{1 m} a_{m}\\\\right) v_{1}+\\\\left(k_{21} a_{1}+\\\\cdots+k_{2 m} a_{m}\\\\right) v_{2}+\\\\cdots+\\\\left(k_{n 1} a_{1}+\\\\cdots+k_{n m} a_{m}\\\\right) v_{n} \\\\\\\\\\n& =k_{11} a_{1} v_{1}+\\\\cdots+k_{1 m} a_{m} v_{1}+k_{21} a_{1} v_{2}+\\\\cdots+k_{2 m} a_{m} v_{2}+\\\\cdots+k_{n 1} a_{1} v_{n}+\\\\cdots+k_{n m} a_{m} v_{n} \\\\\\\\\\n& =\\\\sum_{i, j} k_{j i}\\\\left(a_{i} v_{j}\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nwhere $k_{j i} \\\\in K$. Thus, $w$ is a linear combination of the $a_{i} v_{j}$ with coefficients in $K$; hence, $\\\\left\\\\{a_{i} v_{j}\\\\right\\\\}$ spans $E$ over K.\\n\\nThe proof is complete if we show that $\\\\left\\\\{a_{i} v_{j}\\\\right\\\\}$ is linearly independent over $K$. Suppose, for scalars $x_{j i} \\\\in K$, we have $\\\\sum_{i, j} x_{j i}\\\\left(a_{i} v_{j}\\\\right)=0$; that is,\\n\\n$$\\n\\\\left(x_{11} a_{1} v_{1}+x_{12} a_{2} v_{1}+\\\\cdots+x_{1 m} a_{m} v_{1}\\\\right)+\\\\cdots+\\\\left(x_{n 1} a_{1} v_{n}+x_{n 2} a_{2} v_{n}+\\\\cdots+x_{n m} a_{m} v_{m}\\\\right)=0\\n$$\\n\\nor\\n\\n$$\\n\\\\left(x_{11} a_{1}+x_{12} a_{2}+\\\\cdots+x_{1 m} a_{m}\\\\right) v_{1}+\\\\cdots+\\\\left(x_{n 1} a_{1}+x_{n 2} a_{2}+\\\\cdots+x_{n m} a_{m}\\\\right) v_{n}=0\\n$$\\n\\nBecause $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ is linearly independent over $L$ and the above coefficients of the $v_{i}$ belong to $L$, each coefficient must be 0 :\\n\\n$$\\nx_{11} a_{1}+x_{12} a_{2}+\\\\cdots+x_{1 m} a_{m}=0, \\\\quad \\\\ldots, \\\\quad x_{n 1} a_{1}+x_{n 2} a_{2}+\\\\cdots+x_{n m} a_{m}=0\\n$$\\n\\nBut $\\\\left\\\\{a_{1}, \\\\ldots, a_{m}\\\\right\\\\}$ is linearly independent over $K$; hence, because the $x_{j i} \\\\in K$,\\n\\n$$\\nx_{11}=0, x_{12}=0, \\\\ldots, x_{1 m}=0, \\\\ldots, x_{n 1}=0, x_{n 2}=0, \\\\ldots, x_{n m}=0\\n$$\\n\\nAccordingly, $\\\\left\\\\{a_{i} v_{j}\\\\right\\\\}$ is linearly independent over $K$, and the theorem is proved.\\n\\n',\n",
       " '5.1. State whether each diagram in Fig. 5-3 defines a mapping from $A=\\\\{a, b, c\\\\}$ into $B=\\\\{x, y, z\\\\}$.\\n\\n(a) No. There is nothing assigned to the element $b \\\\in A$.\\n\\n(b) No. Two elements, $x$ and $z$, are assigned to $c \\\\in A$.\\n\\n(c) Yes.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(4)}\\n\\\\end{center}\\n\\n(a)\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(1)}\\n\\\\end{center}\\n\\n(b)\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(3)}\\n\\\\end{center}\\n\\n(c)\\n\\nFigure 5-3\\n',\n",
       " '\\n5.2. Let $f: A \\\\rightarrow B$ and $g: B \\\\rightarrow C$ be defined by Fig. 5-4.\\n\\n(a) Find the composition mapping $(g \\\\circ f): A \\\\rightarrow C$.\\n\\n(b) Find the images of the mappings $f, g, g \\\\circ f$.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-183(2)}\\n\\\\end{center}\\n\\nFigure 5-4\\n\\n(a) Use the definition of the composition mapping to compute\\n\\n$$\\n\\\\begin{gathered}\\n(g \\\\circ f)(a)=g(f(a))=g(y)=t, \\\\quad(g \\\\circ f)(b)=g(f(b))=g(x)=s \\\\\\\\\\n(g \\\\circ f)(c)=g(f(c))=g(y)=t\\n\\\\end{gathered}\\n$$\\n\\nObserve that we arrive at the same answer if we \"follow the arrows\" in Fig. 5-4:\\n\\n$$\\na \\\\rightarrow y \\\\rightarrow t, \\\\quad b \\\\rightarrow x \\\\rightarrow s, \\\\quad c \\\\rightarrow y \\\\rightarrow t\\n$$\\n\\n(b) By Fig. 5-4, the image values under the mapping $f$ are $x$ and $y$, and the image values under $g$ are $r, s, t$.\\n\\nHence,\\n\\n$$\\n\\\\operatorname{Im} f=\\\\{x, y\\\\} \\\\quad \\\\text { and } \\\\quad \\\\operatorname{Im} g=\\\\{r, s, t\\\\}\\n$$\\n\\nAlso, by part (a), the image values under the composition mapping $g \\\\circ f$ are $t$ and $s$; accordingly, $\\\\operatorname{Im} g \\\\circ f=\\\\{s, t\\\\}$. Note that the images of $g$ and $g \\\\circ f$ are different.\\n',\n",
       " '\\n5.3. Consider the mapping $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y, z)=\\\\left(y z, x^{2}\\\\right)$. Find\\\\\\\\\\n(a) $F(2,3,4)$;\\\\\\\\\\n(b) $F(5,-2,7)$;\\\\\\\\\\n(c) $F^{-1}(0,0)$, that is, all $v \\\\in \\\\mathbf{R}^{3}$ such that $F(v)=0$.\\n\\n(a) Substitute in the formula for $F$ to get $F(2,3,4)=\\\\left(3 \\\\cdot 4,2^{2}\\\\right)=(12,4)$.\\n\\n(b) $F(5,-2,7)=\\\\left(-2 \\\\cdot 7,5^{2}\\\\right)=(-14,25)$.\\n\\n(c) Set $F(v)=0$, where $v=(x, y, z)$, and then solve for $x, y, z$ :\\n\\n$$\\nF(x, y, z)=\\\\left(y z, x^{2}\\\\right)=(0,0) \\\\quad \\\\text { or } \\\\quad y z=0, x^{2}=0\\n$$\\n\\nThus, $x=0$ and either $y=0$ or $z=0$. In other words, $x=0, y=0$ or $x=0, z=0$ - that is, the $z$-axis and the $y$-axis.\\n',\n",
       " '\\n5.4. Consider the mapping $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(3 y, 2 x)$. Let $S$ be the unit circle in $\\\\mathbf{R}^{2}$, that is, the solution set of $x^{2}+y^{2}=1$. (a) Describe $F(S)$. (b) Find $F^{-1}(S)$.\\n\\n(a) Let $(a, b)$ be an element of $F(S)$. Then there exists $(x, y) \\\\in S$ such that $F(x, y)=(a, b)$. Hence,\\n\\n$$\\n(3 y, 2 x)=(a, b) \\\\quad \\\\text { or } \\\\quad 3 y=a, 2 x=b \\\\quad \\\\text { or } \\\\quad y=\\\\frac{a}{3}, x=\\\\frac{b}{2}\\n$$\\n\\nBecause $(x, y) \\\\in S$-that is, $x^{2}+y^{2}=1$ - we have\\n\\n$$\\n\\\\left(\\\\frac{b}{2}\\\\right)^{2}+\\\\left(\\\\frac{a}{3}\\\\right)^{2}=1 \\\\quad \\\\text { or } \\\\quad \\\\frac{a^{2}}{9}+\\\\frac{b^{2}}{4}=1\\n$$\\n\\nThus, $F(S)$ is an ellipse.\\n\\n(b) Let $F(x, y)=(a, b)$, where $(a, b) \\\\in S$. Then $(3 y, 2 x)=(a, b)$ or $3 y=a, 2 x=b$. Because $(a, b) \\\\in S$, we have $a^{2}+b^{2}=1$. Thus, $(3 y)^{2}+(2 x)^{2}=1$. Accordingly, $F^{-1}(S)$ is the ellipse $4 x^{2}+9 y^{2}=1$.\\n',\n",
       " '\\n5.5. Let the mappings $f: A \\\\rightarrow B, g: B \\\\rightarrow C, h: C \\\\rightarrow D$ be defined by Fig. 5-5. Determine whether or not each function is (a) one-to-one; (b) onto; (c) invertible (i.e., has an inverse).\\n\\n(a) The mapping $f: A \\\\rightarrow B$ is one-to-one, as each element of $A$ has a different image. The mapping $g: B \\\\rightarrow C$ is not one-to one, because $x$ and $z$ both have the same image 4. The mapping $h: C \\\\rightarrow D$ is one-to-one.\\n\\n(b) The mapping $f: A \\\\rightarrow B$ is not onto, because $z \\\\in B$ is not the image of any element of $A$. The mapping $g: B \\\\rightarrow C$ is onto, as each element of $C$ is the image of some element of $B$. The mapping $h: C \\\\rightarrow D$ is also onto.\\n\\n(c) A mapping has an inverse if and only if it is one-to-one and onto. Hence, only $h$ has an inverse.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-184}\\n\\\\end{center}\\n\\nFigure 5-5\\n',\n",
       " '\\n5.6. Suppose $f: A \\\\rightarrow B$ and $g: B \\\\rightarrow C$. Hence, $(g \\\\circ f): A \\\\rightarrow C$ exists. Prove\\n\\n(a) If $f$ and $g$ are one-to-one, then $g \\\\circ f$ is one-to-one.\\n\\n(b) If $f$ and $g$ are onto mappings, then $g \\\\circ f$ is an onto mapping.\\n\\n(c) If $g \\\\circ f$ is one-to-one, then $f$ is one-to-one.\\n\\n(d) If $g \\\\circ f$ is an onto mapping, then $g$ is an onto mapping.\\n\\n(a) Suppose $(g \\\\circ f)(x)=(g \\\\circ f)(y)$. Then $g(f(x))=g(f(y))$. Because $g$ is one-to-one, $f(x)=f(y)$. Because $f$ is one-to-one, $x=y$. We have proven that $(g \\\\circ f)(x)=(g \\\\circ f)(y)$ implies $x=y$; hence $g \\\\circ f$ is one-to-one.\\n\\n(b) Suppose $c \\\\in C$. Because $g$ is onto, there exists $b \\\\in B$ for which $g(b)=c$. Because $f$ is onto, there exists $a \\\\in A$ for which $f(a)=b$. Thus, $(g \\\\circ f)(a)=g(f(a))=g(b)=c$. Hence, $g \\\\circ f$ is onto.\\n\\n(c) Suppose $f$ is not one-to-one. Then there exist distinct elements $x, y \\\\in A$ for which $f(x)=f(y)$. Thus, $(g \\\\circ f)(x)=g(f(x))=g(f(y))=(g \\\\circ f)(y)$. Hence, $g \\\\circ f$ is not one-to-one. Therefore, if $g \\\\circ f$ is one-toone, then $f$ must be one-to-one.\\n\\n(d) If $a \\\\in A$, then $(g \\\\circ f)(a)=g(f(a)) \\\\in g(B)$. Hence, $(g \\\\circ f)(A) \\\\subseteq g(B)$. Suppose $g$ is not onto. Then $g(B)$ is properly contained in $C$ and so $(g \\\\circ f)(A)$ is properly contained in $C$; thus, $g \\\\circ f$ is not onto. Accordingly, if $g \\\\circ f$ is onto, then $g$ must be onto.\\n',\n",
       " '\\n5.7. Prove that $f: A \\\\rightarrow B$ has an inverse if and only if $f$ is one-to-one and onto.\\n\\nSuppose $f$ has an inverse-that is, there exists a function $f^{-1}: B \\\\rightarrow A$ for which $f^{-1} \\\\circ f=\\\\mathbf{1}_{A}$ and $f \\\\circ f^{-1}=\\\\mathbf{1}_{B}$. Because $\\\\mathbf{1}_{A}$ is one-to-one, $f$ is one-to-one by Problem 5.6(c), and because $\\\\mathbf{1}_{B}$ is onto, $f$ is onto by Problem 5.6(d); that is, $f$ is both one-to-one and onto.\\n\\nNow suppose $f$ is both one-to-one and onto. Then each $b \\\\in B$ is the image of a unique element in $A$, say $b^{*}$. Thus, if $f(a)=b$, then $a=b^{*}$; hence, $f\\\\left(b^{*}\\\\right)=b$. Now let $g$ denote the mapping from $B$ to $A$ defined by $b \\\\mapsto b^{*}$. We have\\n\\n(i) $(g \\\\circ f)(a)=g(f(a))=g(b)=b^{*}=a$ for every $a \\\\in A$; hence, $g \\\\circ f=\\\\mathbf{1}_{A}$.\\n\\n(ii) $(f \\\\circ g)(b)=f(g(b))=f\\\\left(b^{*}\\\\right)=b$ for every $b \\\\in B$; hence, $f \\\\circ g=\\\\mathbf{1}_{B}$.\\n\\nAccordingly, $f$ has an inverse. Its inverse is the mapping $g$.\\n',\n",
       " '\\n5.8. Let $f: \\\\mathbf{R} \\\\rightarrow \\\\mathbf{R}$ be defined by $f(x)=2 x-3$. Now $f$ is one-to-one and onto; hence, $f$ has an inverse mapping $f^{-1}$. Find a formula for $f^{-1}$.\\n\\nLet $y$ be the image of $x$ under the mapping $f$; that is, $y=f(x)=2 x-3$. Hence, $x$ will be the image of $y$ under the inverse mapping $f^{-1}$. Thus, solve for $x$ in terms of $y$ in the above equation to obtain $x=\\\\frac{1}{2}(y+3)$. Then the formula defining the inverse function is $f^{-1}(y)=\\\\frac{1}{2}(y+3)$, or, using $x$ instead of $y, f^{-1}(x)=\\\\frac{1}{2}(x+3)$.\\n\\n\\n\\\\section*{Linear Mappings}\\n',\n",
       " '5.9. Suppose the mapping $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ is defined by $F(x, y)=(x+y, x)$. Show that $F$ is linear.\\n\\nWe need to show that $F(v+w)=F(v)+F(w)$ and $F(k v)=k F(v)$, where $u$ and $v$ are any elements of $\\\\mathbf{R}^{2}$ and $k$ is any scalar. Let $v=(a, b)$ and $w=\\\\left(a^{\\\\prime}, b^{\\\\prime}\\\\right)$. Then\\n\\n$$\\nv+w=\\\\left(a+a^{\\\\prime}, b+b^{\\\\prime}\\\\right) \\\\quad \\\\text { and } \\\\quad k v=(k a, k b)\\n$$\\n\\nWe have $F(v)=(a+b, a)$ and $F(w)=\\\\left(a^{\\\\prime}+b^{\\\\prime}, a^{\\\\prime}\\\\right)$. Thus,\\n\\n$$\\n\\\\begin{aligned}\\nF(v+w) & =F\\\\left(a+a^{\\\\prime}, b+b^{\\\\prime}\\\\right)=\\\\left(a+a^{\\\\prime}+b+b^{\\\\prime}, a+a^{\\\\prime}\\\\right) \\\\\\\\\\n& =(a+b, a)+\\\\left(a^{\\\\prime}+b^{\\\\prime}, a^{\\\\prime}\\\\right)=F(v)+F(w)\\n\\\\end{aligned}\\n$$\\n\\nand\\n\\n$$\\nF(k v)=F(k a, k b)=(k a+k b, k a)=k(a+b, a)=k F(v)\\n$$\\n\\nBecause $v, w, k$ were arbitrary, $F$ is linear.\\n',\n",
       " '\\n5.10. Suppose $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ is defined by $F(x, y, z)=(x+y+z, 2 x-3 y+4 z)$. Show that $F$ is linear.\\n\\nWe argue via matrices. Writing vectors as columns, the mapping $F$ may be written in the form $F(v)=A v$, where $v=[x, y, z]^{T}$ and\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n2 & -3 & 4\\n\\\\end{array}\\\\right]\\n$$\\n\\nThen, using properties of matrices, we have\\n\\n$$\\nF(v+w)=A(v+w)=A v+A w=F(v)+F(w)\\n$$\\n\\nand\\n\\n$$\\nF(k v)=A(k v)=k(A v)=k F(v)\\n$$\\n\\nThus, $F$ is linear.\\n',\n",
       " '\\n5.11. Show that the following mappings are not linear:\\n\\n(a) $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(x y, x)$\\n\\n(b) $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{3}$ defined by $F(x, y)=(x+3,2 y, x+y)$\\n\\n(c) $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y, z)=(|x|, y+z)$\\n\\n(a) Let $v=(1,2)$ and $w=(3,4)$; then $v+w=(4,6)$. Also,\\n\\n$$\\nF(v)=(1(2), 1)=(2,1) \\\\quad \\\\text { and } \\\\quad F(w)=(3(4), 3)=(12,3)\\n$$\\n\\nHence,\\n\\n$$\\nF(v+w)=(4(6), 4)=(24,6) \\\\neq F(v)+F(w)\\n$$\\n\\n(b) Because $F(0,0)=(3,0,0) \\\\neq(0,0,0), F$ cannot be linear.\\n\\n(c) Let $v=(1,2,3)$ and $k=-3$. Then $k v=(-3,-6,-9)$. We have\\n\\n$$\\nF(v)=(1,5) \\\\text { and } k F(v)=-3(1,5)=(-3,-15) .\\n$$\\n\\nThus,\\n\\n$$\\nF(k v)=F(-3,-6,-9)=(3,-15) \\\\neq k F(v)\\n$$\\n\\nAccordingly, $F$ is not linear.\\n',\n",
       " '\\n5.12. Let $V$ be the vector space of $n$-square real matrices. Let $M$ be an arbitrary but fixed matrix in $V$. Let $F: V \\\\rightarrow V$ be defined by $F(A)=A M+M A$, where $A$ is any matrix in $V$. Show that $F$ is linear.\\n\\nFor any matrices $A$ and $B$ in $V$ and any scalar $k$, we have\\n\\n$$\\n\\\\begin{aligned}\\nF(A+B) & =(A+B) M+M(A+B)=A M+B M+M A+M B \\\\\\\\\\n& =(A M+M A)=(B M+M B)=F(A)+F(B)\\n\\\\end{aligned}\\n$$\\n\\nand\\n\\n$$\\nF(k A)=(k A) M+M(k A)=k(A M)+k(M A)=k(A M+M A)=k F(A)\\n$$\\n\\nThus, $F$ is linear.\\n',\n",
       " '\\n5.13. Prove Theorem 5.2: Let $V$ and $U$ be vector spaces over a field $K$. Let $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ be a basis of $V$ and let $u_{1}, u_{2}, \\\\ldots, u_{n}$ be any vectors in $U$. Then there exists a unique linear mapping $F: V \\\\rightarrow U$ such that $F\\\\left(v_{1}\\\\right)=u_{1}, F\\\\left(v_{2}\\\\right)=u_{2}, \\\\ldots, F\\\\left(v_{n}\\\\right)=u_{n}$.\\n\\nThere are three steps to the proof of the theorem: (1) Define the mapping $F: V \\\\rightarrow U$ such that $F\\\\left(v_{i}\\\\right)=u_{i}, i=1, \\\\ldots, n$. (2) Show that $F$ is linear. (3) Show that $F$ is unique.\\n\\nStep 1. Let $v \\\\in V$. Because $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ is a basis of $V$, there exist unique scalars $a_{1}, \\\\ldots, a_{n} \\\\in K$ for which $v=a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}$. We define $F: V \\\\rightarrow U$ by\\n\\n$$\\nF(v)=a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} u_{n}\\n$$\\n\\n(Because the $a_{i}$ are unique, the mapping $F$ is well defined.) Now, for $i=1, \\\\ldots, n$,\\n\\n$$\\nv_{i}=0 v_{1}+\\\\cdots+1 v_{i}+\\\\cdots+0 v_{n}\\n$$\\n\\nHence,\\n\\n$$\\nF\\\\left(v_{i}\\\\right)=0 u_{1}+\\\\cdots+1 u_{i}+\\\\cdots+0 u_{n}=u_{i}\\n$$\\n\\nThus, the first step of the proof is complete.\\n\\nStep 2. Suppose $v=a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}$ and $w=b_{1} v_{1}+b_{2} v_{2}+\\\\cdots+b_{n} v_{n}$. Then\\n\\n$$\\nv+w=\\\\left(a_{1}+b_{1}\\\\right) v_{1}+\\\\left(a_{2}+b_{2}\\\\right) v_{2}+\\\\cdots+\\\\left(a_{n}+b_{n}\\\\right) v_{n}\\n$$\\n\\nand, for any $k \\\\in K, k v=k a_{1} v_{1}+k a_{2} v_{2}+\\\\cdots+k a_{n} v_{n}$. By definition of the mapping $F$,\\n\\n$$\\nF(v)=a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} v_{n} \\\\quad \\\\text { and } \\\\quad F(w)=b_{1} u_{1}+b_{2} u_{2}+\\\\cdots+b_{n} u_{n}\\n$$\\n\\nHence,\\n\\n$$\\n\\\\begin{aligned}\\nF(v+w) & =\\\\left(a_{1}+b_{1}\\\\right) u_{1}+\\\\left(a_{2}+b_{2}\\\\right) u_{2}+\\\\cdots+\\\\left(a_{n}+b_{n}\\\\right) u_{n} \\\\\\\\\\n& =\\\\left(a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} u_{n}\\\\right)+\\\\left(b_{1} u_{1}+b_{2} u_{2}+\\\\cdots+b_{n} u_{n}\\\\right) \\\\\\\\\\n& =F(v)+F(w)\\n\\\\end{aligned}\\n$$\\n\\nand\\n\\n$$\\nF(k v)=k\\\\left(a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} u_{n}\\\\right)=k F(v)\\n$$\\n\\nThus, $F$ is linear.\\n\\nStep 3. Suppose $G: V \\\\rightarrow U$ is linear and $G\\\\left(v_{1}\\\\right)=u_{i}, i=1, \\\\ldots, n$. Let\\n\\n$$\\nv=a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}\\n$$\\n\\nThen\\n\\n$$\\n\\\\begin{aligned}\\nG(v) & =G\\\\left(a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}\\\\right)=a_{1} G\\\\left(v_{1}\\\\right)+a_{2} G\\\\left(v_{2}\\\\right)+\\\\cdots+a_{n} G\\\\left(v_{n}\\\\right) \\\\\\\\\\n& =a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{n} u_{n}=F(v)\\n\\\\end{aligned}\\n$$\\n\\nBecause $G(v)=F(v)$ for every $v \\\\in V, G=F$. Thus, $F$ is unique and the theorem is proved.\\n',\n",
       " '\\n5.14. Let $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ be the linear mapping for which $F(1,2)=(2,3)$ and $F(0,1)=(1,4)$. [Note that $\\\\{(1,2),(0,1)\\\\}$ is a basis of $\\\\mathbf{R}^{2}$, so such a linear map $F$ exists and is unique by Theorem 5.2.] Find a formula for $F$; that is, find $F(a, b)$.\\n\\nWrite $(a, b)$ as a linear combination of $(1,2)$ and $(0,1)$ using unknowns $x$ and $y$,\\n\\n$$\\n(a, b)=x(1,2)+y(0,1)=(x, 2 x+y), \\\\quad \\\\text { so } \\\\quad a=x, b=2 x+y\\n$$\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=a, \\\\quad y=-2 a+b$. Then\\n\\n$$\\nF(a, b)=x F(1,2)+y F(0,1)=a(2,3)+(-2 a+b)(1,4)=(b,-5 a+4 b)\\n$$\\n',\n",
       " '\\n5.15. Suppose a linear mapping $F: V \\\\rightarrow U$ is one-to-one and onto. Show that the inverse mapping $F^{-1}: U \\\\rightarrow V$ is also linear.\\n\\nSuppose $u, u^{\\\\prime} \\\\in U$. Because $F$ is one-to-one and onto, there exist unique vectors $v, v^{\\\\prime} \\\\in V$ for which $F(v)=u$ and $F\\\\left(v^{\\\\prime}\\\\right)=u^{\\\\prime}$. Because $F$ is linear, we also have\\n\\n$$\\nF\\\\left(v+v^{\\\\prime}\\\\right)=F(v)+F\\\\left(v^{\\\\prime}\\\\right)=u+u^{\\\\prime} \\\\quad \\\\text { and } \\\\quad F(k v)=k F(v)=k u\\n$$\\n\\nBy definition of the inverse mapping,\\n\\n$$\\nF^{-1}(u)=v, F^{-1}\\\\left(u^{\\\\prime}\\\\right)=v^{\\\\prime}, F^{-1}\\\\left(u+u^{\\\\prime}\\\\right)=v+v^{\\\\prime}, F^{-1}(k u)=k v .\\n$$\\n\\nThen\\n\\n$$\\nF^{-1}\\\\left(u+u^{\\\\prime}\\\\right)=v+v^{\\\\prime}=F^{-1}(u)+F^{-1}\\\\left(u^{\\\\prime}\\\\right) \\\\quad \\\\text { and } \\\\quad F^{-1}(k u)=k v=k F^{-1}(u)\\n$$\\n\\nThus, $F^{-1}$ is linear.\\n\\n\\n\\\\section*{Kernel and Image of Linear Mappings}\\n',\n",
       " '5.16. Let $F: \\\\mathbf{R}^{4} \\\\rightarrow \\\\mathbf{R}^{3}$ be the linear mapping defined by\\n\\n$$\\nF(x, y, z, t)=(x-y+z+t, \\\\quad x+2 z-t, \\\\quad x+y+3 z-3 t)\\n$$\\n\\nFind a basis and the dimension of (a) the image of $F$, (b) the kernel of $F$.\\n\\n(a) Find the images of the usual basis of $\\\\mathbf{R}^{4}$ :\\n\\n$$\\n\\\\begin{array}{ll}\\nF(1,0,0,0)=(1,1,1), & F(0,0,1,0)=(1,2,3) \\\\\\\\\\nF(0,1,0,0)=(-1,0,1), & F(0,0,0,1)=(1,-1,-3)\\n\\\\end{array}\\n$$\\n\\nBy Proposition 5.4, the image vectors span $\\\\operatorname{Im} F$. Hence, form the matrix whose rows are these image vectors, and row reduce to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n-1 & 0 & 1 \\\\\\\\\\n1 & 2 & 3 \\\\\\\\\\n1 & -1 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & -2 & -4\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 1 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $(1,1,1)$ and $(0,1,2)$ form a basis for $\\\\operatorname{Im} F$; hence, $\\\\operatorname{dim}(\\\\operatorname{Im} F)=2$.\\n\\n(b) Set $F(v)=0$, where $v=(x, y, z, t)$; that is, set\\n\\n$$\\nF(x, y, z, t)=(x-y+z+t, x+2 z-t, x+y+3 z-3 t)=(0,0,0)\\n$$\\n\\nSet corresponding entries equal to each other to form the following homogeneous system whose solution space is $\\\\operatorname{Ker} F$ :\\n\\n$$\\n\\\\begin{aligned}\\n& x-y+z+t=0 \\\\quad x-y+z+t=0 \\\\\\\\\\n& x+2 z-t=0 \\\\quad \\\\text { or } \\\\quad y+z-2 t=0 \\\\\\\\\\n& x+y+3 z-3 t=0 \\\\quad 2 y+2 z-4 t=0 \\\\\\\\\\n& x-y+z+t=0 \\\\\\\\\\n& y+z-2 t=0\\n\\\\end{aligned}\\n$$\\n\\nThe free variables are $z$ and $t$. Hence, $\\\\operatorname{dim}(\\\\operatorname{Ker} F)=2$.\\n\\n(i) Set $z=-1, t=0$ to obtain the solution $(2,1,-1,0)$.\\n\\n(ii) Set $z=0, t=1$ to obtain the solution $(1,2,0,1)$.\\n\\nThus, $(2,1,-1,0)$ and $(1,2,0,1)$ form a basis of $\\\\operatorname{Ker} F$.\\n\\n[As expected, $\\\\operatorname{dim}(\\\\operatorname{Im} F)+\\\\operatorname{dim}(\\\\operatorname{Ker} F)=2+2=4=\\\\operatorname{dim} \\\\mathbf{R}^{4}$, the domain of $F$.]\\n',\n",
       " '\\n5.17. Let $G: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{3}$ be the linear mapping defined by\\n\\n$$\\nG(x, y, z)=(x+2 y-z, \\\\quad y+z, \\\\quad x+y-2 z)\\n$$\\n\\nFind a basis and the dimension of (a) the image of $G$, (b) the kernel of $G$.\\n\\n(a) Find the images of the usual basis of $\\\\mathbf{R}^{3}$ :\\n\\n$$\\nG(1,0,0)=(1,0,1), \\\\quad G(0,1,0)=(2,1,1), \\\\quad G(0,0,1)=(-1,1,-2)\\n$$\\n\\nBy Proposition 5.4, the image vectors span $\\\\operatorname{Im} G$. Hence, form the matrix $M$ whose rows are these image vectors, and row reduce to echelon form:\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 1 \\\\\\\\\\n2 & 1 & 1 \\\\\\\\\\n-1 & 1 & -2\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 1 \\\\\\\\\\n0 & 1 & -1 \\\\\\\\\\n0 & 1 & -1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 1 \\\\\\\\\\n0 & 1 & -1 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $(1,0,1)$ and $(0,1,-1)$ form a basis for $\\\\operatorname{Im} G$; hence, $\\\\operatorname{dim}(\\\\operatorname{Im} G)=2$.\\n\\n(b) Set $G(v)=0$, where $v=(x, y, z)$; that is,\\n\\n$$\\nG(x, y, z)=(x+2 y-z, \\\\quad y+z, \\\\quad x+y-2 z)=(0,0,0)\\n$$\\n\\nSet corresponding entries equal to each other to form the following homogeneous system whose solution space is $\\\\operatorname{Ker} G$ :\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-189}\\n\\\\end{center}\\n\\nThe only free variable is $z$; hence, $\\\\operatorname{dim}(\\\\operatorname{Ker} G)=1$. Set $z=1$; then $y=-1$ and $x=3$. Thus, $(3,-1,1)$ forms a basis of $\\\\operatorname{Ker} G$. [As expected, $\\\\operatorname{dim}(\\\\operatorname{Im} G)+\\\\operatorname{dim}(\\\\operatorname{Ker} G)=2+1=3=\\\\operatorname{dim} \\\\mathbf{R}^{3}$, the domain of $G$.]\\n',\n",
       " '\\n5.18. Consider the matrix mapping $A: \\\\mathbf{R}^{4} \\\\rightarrow \\\\mathbf{R}^{3}$, where $A=\\\\left[\\\\begin{array}{rrrr}1 & 2 & 3 & 1 \\\\\\\\ 1 & 3 & 5 & -2 \\\\\\\\ 3 & 8 & 13 & -3\\\\end{array}\\\\right]$. Find a basis and the dimension of (a) the image of $A$, (b) the kernel of $A$.\\n\\n(a) The column space of $A$ is equal to $\\\\operatorname{Im} A$. Now reduce $A^{T}$ to echelon form:\\n\\n$$\\nA^{T}=\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 3 \\\\\\\\\\n2 & 3 & 8 \\\\\\\\\\n3 & 5 & 13 \\\\\\\\\\n1 & -2 & -3\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 3 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 2 & 4 \\\\\\\\\\n0 & -3 & -6\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll}\\n1 & 1 & 3 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $\\\\{(1,1,3),(0,1,2)\\\\}$ is a basis of $\\\\operatorname{Im} A$, and $\\\\operatorname{dim}(\\\\operatorname{Im} A)=2$.\\n\\n(b) Here $\\\\operatorname{Ker} A$ is the solution space of the homogeneous system $A X=0$, where $X=\\\\{x, y, z, t)^{T}$. Thus, reduce the matrix $A$ of coefficients to echelon form:\\n\\n$$\\n\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 3 & 1 \\\\\\\\\\n0 & 1 & 2 & -3 \\\\\\\\\\n0 & 2 & 4 & -6\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrr}\\n1 & 2 & 3 & 1 \\\\\\\\\\n0 & 1 & 2 & -3 \\\\\\\\\\n0 & 0 & 0 & 0\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{array}{r}\\nx+2 y+3 z+t=0 \\\\\\\\\\ny+2 z-3 t=0\\n\\\\end{array}\\n$$\\n\\nThe free variables are $z$ and $t$. Thus, $\\\\operatorname{dim}(\\\\operatorname{Ker} A)=2$.\\n\\n(i) Set $z=1, t=0$ to get the solution $(1,-2,1,0)$.\\n\\n(ii) Set $z=0, t=1$ to get the solution $(-7,3,0,1)$.\\n\\nThus, $(1,-2,1,0)$ and $(-7,3,0,1)$ form a basis for $\\\\operatorname{Ker} A$.\\n',\n",
       " '\\n5.19. Find a linear map $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{4}$ whose image is spanned by $(1,2,0,-4)$ and $(2,0,-1,-3)$.\\n\\nForm a $4 \\\\times 3$ matrix whose columns consist only of the given vectors, say\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & 2 \\\\\\\\\\n2 & 0 & 0 \\\\\\\\\\n0 & -1 & -1 \\\\\\\\\\n-4 & -3 & -3\\n\\\\end{array}\\\\right]\\n$$\\n\\nRecall that $A$ determines a linear map $A: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{4}$ whose image is spanned by the columns of $A$. Thus, $A$ satisfies the required condition.\\n',\n",
       " '\\n5.20. Suppose $f: V \\\\rightarrow U$ is linear with kernel $W$, and that $f(v)=u$. Show that the \"coset\" $v+W=\\\\{v+w: w \\\\in W\\\\}$ is the preimage of $u$; that is, $f^{-1}(u)=v+W$.\\n\\nWe must prove that (i) $f^{-1}(u) \\\\subseteq v+W$ and (ii) $v+W \\\\subseteq f^{-1}(u)$.\\n\\nWe first prove (i). Suppose $v^{\\\\prime} \\\\in f^{-1}(u)$. Then $f\\\\left(v^{\\\\prime}\\\\right)=u$, and so\\n\\n$$\\nf\\\\left(v^{\\\\prime}-v\\\\right)=f\\\\left(v^{\\\\prime}\\\\right)-f(v)=u-u=0\\n$$\\n\\nthat is, $v^{\\\\prime}-v \\\\in W$. Thus, $v^{\\\\prime}=v+\\\\left(v^{\\\\prime}-v\\\\right) \\\\in v+W$, and hence $f^{-1}(u) \\\\subseteq v+W$.\\n\\nNow we prove (ii). Suppose $v^{\\\\prime} \\\\in v+W$. Then $v^{\\\\prime}=v+w$, where $w \\\\in W$. Because $W$ is the kernel of $f$, we have $f(w)=0$. Accordingly,\\n\\n$$\\nf\\\\left(v^{\\\\prime}\\\\right)=f(v+w)+f(v)+f(w)=f(v)+0=f(v)=u\\n$$\\n\\nThus, $v^{\\\\prime} \\\\in f^{-1}(u)$, and so $v+W \\\\subseteq f^{-1}(u)$.\\n\\nBoth inclusions imply $f^{-1}(u)=v+W$.\\n',\n",
       " '\\n5.21. Suppose $F: V \\\\rightarrow U$ and $G: U \\\\rightarrow W$ are linear. Prove\\\\\\\\\\n(a) $\\\\operatorname{rank}(G \\\\circ F) \\\\leq \\\\operatorname{rank}(G)$,\\\\\\\\\\n(b) $\\\\operatorname{rank}(G \\\\circ F) \\\\leq \\\\operatorname{rank}(F)$.\\n\\n(a) Because $F(V) \\\\subseteq U$, we also have $G(F(V)) \\\\subseteq G(U)$, and so $\\\\operatorname{dim}[G(F(V))] \\\\leq \\\\operatorname{dim}[G(U)]$. Then $\\\\operatorname{rank}(G \\\\circ F)=\\\\operatorname{dim}[(G \\\\circ F)(V)]=\\\\operatorname{dim}[G(F(V))] \\\\leq \\\\operatorname{dim}[G(U)]=\\\\operatorname{rank}(G)$.\\n\\n(b) We have $\\\\operatorname{dim}[G(F(V))] \\\\leq \\\\operatorname{dim}[F(V)]$. Hence,\\n\\n$$\\n\\\\operatorname{rank}(G \\\\circ F)=\\\\operatorname{dim}[(G \\\\circ F)(V)]=\\\\operatorname{dim}[G(F(V))] \\\\leq \\\\operatorname{dim}[F(V)]=\\\\operatorname{rank}(F)\\n$$\\n',\n",
       " '\\n5.22. Prove Theorem 5.3: Let $F: V \\\\rightarrow U$ be linear. Then,\\n\\n(a) $\\\\operatorname{Im} F$ is a subspace of $U$, (b) $\\\\operatorname{Ker} F$ is a subspace of $V$.\\n\\n(a) Because $F(0)=0$, we have $0 \\\\in \\\\operatorname{Im} F$. Now suppose $u, u^{\\\\prime} \\\\in \\\\operatorname{Im} F$ and $a, b \\\\in K$. Because $u$ and $u^{\\\\prime}$ belong to the image of $F$, there exist vectors $v, v^{\\\\prime} \\\\in V$ such that $F(v)=u$ and $F\\\\left(v^{\\\\prime}\\\\right)=u^{\\\\prime}$. Then\\n\\n$$\\nF\\\\left(a v+b v^{\\\\prime}\\\\right)=a F(v)+b F\\\\left(v^{\\\\prime}\\\\right)=a u+b u^{\\\\prime} \\\\in \\\\operatorname{Im} F\\n$$\\n\\nThus, the image of $F$ is a subspace of $U$.\\n\\n(b) Because $F(0)=0$, we have $0 \\\\in \\\\operatorname{Ker} F$. Now suppose $v, w \\\\in \\\\operatorname{Ker} F$ and $a, b \\\\in K$. Because $v$ and $w$ belong to the kernel of $F, F(v)=0$ and $F(w)=0$. Thus,\\n\\n$$\\nF(a v+b w)=a F(v)+b F(w)=a 0+b 0=0+0=0, \\\\quad \\\\text { and so } \\\\quad a v+b w \\\\in \\\\operatorname{Ker} F\\n$$\\n\\nThus, the kernel of $F$ is a subspace of $V$.\\n',\n",
       " '\\n5.23. Prove Theorem 5.6: Suppose $V$ has finite dimension and $F: V \\\\rightarrow U$ is linear. Then\\n\\n$$\\n\\\\operatorname{dim} V=\\\\operatorname{dim}(\\\\operatorname{Ker} F)+\\\\operatorname{dim}(\\\\operatorname{Im} F)=\\\\operatorname{nullity}(F)+\\\\operatorname{rank}(F)\\n$$\\n\\nSuppose $\\\\operatorname{dim}(\\\\operatorname{Ker} F)=r$ and $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}\\\\right\\\\}$ is a basis of $\\\\operatorname{Ker} F$, and suppose $\\\\operatorname{dim}(\\\\operatorname{Im} F)=s$ and $\\\\left\\\\{u_{1}, \\\\ldots, u_{s}\\\\right\\\\}$ is a basis of $\\\\operatorname{Im} F$. (By Proposition 5.4, $\\\\operatorname{Im} F$ has finite dimension.) Because every $u_{j} \\\\in \\\\operatorname{Im} F$, there exist vectors $v_{1}, \\\\ldots, v_{s}$ in $V$ such that $F\\\\left(v_{1}\\\\right)=u_{1}, \\\\ldots, F\\\\left(v_{s}\\\\right)=u_{s}$. We claim that the set\\n\\n$$\\nB=\\\\left\\\\{w_{1}, \\\\ldots, w_{r}, v_{1}, \\\\ldots, v_{s}\\\\right\\\\}\\n$$\\n\\nis a basis of $V$; that is, (i) $B$ spans $V$, and (ii) $B$ is linearly independent. Once we prove (i) and (ii), then $\\\\operatorname{dim} V=r+s=\\\\operatorname{dim}(\\\\operatorname{Ker} F)+\\\\operatorname{dim}(\\\\operatorname{Im} F)$.\\n\\n(i) $B$ spans $V$. Let $v \\\\in V$. Then $F(v) \\\\in \\\\operatorname{Im} F$. Because the $u_{j} \\\\operatorname{span} \\\\operatorname{Im} F$, there exist scalars $a_{1}, \\\\ldots, a_{s}$ such that $F(v)=a_{1} u_{1}+\\\\cdots+a_{s} u_{s}$. Set $\\\\hat{v}=a_{1} v_{1}+\\\\cdots+a_{s} v_{s}-v$. Then\\n\\n$$\\n\\\\begin{aligned}\\nF(\\\\hat{v}) & =F\\\\left(a_{1} v_{1}+\\\\cdots+a_{s} v_{s}-v\\\\right)=a_{1} F\\\\left(v_{1}\\\\right)+\\\\cdots+a_{s} F\\\\left(v_{s}\\\\right)-F(v) \\\\\\\\\\n& =a_{1} u_{1}+\\\\cdots+a_{s} u_{s}-F(v)=0\\n\\\\end{aligned}\\n$$\\n\\nThus, $\\\\hat{v} \\\\in \\\\operatorname{Ker} F$. Because the $w_{i}$ span $\\\\operatorname{Ker} F$, there exist scalars $b_{1}, \\\\ldots, b_{r}$, such that\\n\\n$$\\n\\\\hat{v}=b_{1} w_{1}+\\\\cdots+b_{r} w_{r}=a_{1} v_{1}+\\\\cdots+a_{s} v_{s}-v\\n$$\\n\\nAccordingly,\\n\\n$$\\nv=a_{1} v_{1}+\\\\cdots+a_{s} v_{s}-b_{1} w_{1}-\\\\cdots-b_{r} w_{r}\\n$$\\n\\nThus, $B$ spans $V$.\\\\\\\\\\n(ii) B is linearly independent. Suppose\\n\\n\\n\\\\begin{equation*}\\nx_{1} w_{1}+\\\\cdots+x_{r} w_{r}+y_{1} v_{1}+\\\\cdots+y_{s} v_{s}=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nwhere $x_{i}, y_{j} \\\\in K$. Then\\n\\n\\n\\\\begin{align*}\\n0 & =F(0)=F\\\\left(x_{1} w_{1}+\\\\cdots+x_{r} w_{r}+y_{1} v_{1}+\\\\cdots+y_{s} v_{s}\\\\right) \\\\\\\\\\n& =x_{1} F\\\\left(w_{1}\\\\right)+\\\\cdots+x_{r} F\\\\left(w_{r}\\\\right)+y_{1} F\\\\left(v_{1}\\\\right)+\\\\cdots+y_{s} F\\\\left(v_{s}\\\\right) \\\\tag{2}\\n\\\\end{align*}\\n\\n\\nBut $F\\\\left(w_{i}\\\\right)=0$, since $w_{i} \\\\in \\\\operatorname{Ker} F$, and $F\\\\left(v_{j}\\\\right)=u_{j}$. Substituting into (2), we will obtain $y_{1} u_{1}+\\\\cdots+y_{s} u_{s}=0$. Since the $u_{j}$ are linearly independent, each $y_{j}=0$. Substitution into (1) gives $x_{1} w_{1}+\\\\cdots+x_{r} w_{r}=0$. Since the $w_{i}$ are linearly independent, each $x_{i}=0$. Thus $B$ is linearly independent.\\n\\n\\n\\\\section*{Singular and Nonsingular Linear Maps, Isomorphisms}\\n',\n",
       " '5.24. Determine whether or not each of the following linear maps is nonsingular. If not, find a nonzero vector $v$ whose image is 0 .\\n\\n(a) $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(x-y, x-2 y)$.\\n\\n(b) $G: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $G(x, y)=(2 x-4 y, 3 x-6 y)$.\\n\\n(a) Find Ker $F$ by setting $F(v)=0$, where $v=(x, y)$,\\n\\n$$\\n\\\\begin{array}{rlrl}\\n(x-y, x-2 y)=(0,0) \\\\quad \\\\text { or } & \\\\begin{aligned}\\nx-y & =0 \\\\\\\\\\nx-2 y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } & x-y & =0 \\\\\\\\\\n-y & =0\\n\\\\end{array}\\n$$\\n\\nThe only solution is $x=0, y=0$. Hence, $F$ is nonsingular.\\n\\n(b) Set $G(x, y)=(0,0)$ to find $\\\\operatorname{Ker} G$ :\\n\\n$$\\n(2 x-4 y, 3 x-6 y)=(0,0) \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n& 2 x-4 y=0 \\\\\\\\\\n& 3 x-6 y=0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x-2 y=0\\n$$\\n\\nThe system has nonzero solutions, because $y$ is a free variable. Hence, $G$ is singular. Let $y=1$ to obtain the solution $v=(2,1)$, which is a nonzero vector, such that $G(v)=0$.\\n',\n",
       " '\\n5.25. The linear map $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(x-y, x-2 y)$ is nonsingular by the previous Problem 5.24. Find a formula for $F^{-1}$.\\n\\nSet $F(x, y)=(a, b)$, so that $F^{-1}(a, b)=(x, y)$. We have\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-191}\\n\\\\end{center}\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=2 a-b, y=a-b$. Thus,\\n\\n$$\\nF^{-1}(a, b)=(2 a-b, a-b) \\\\quad \\\\text { or } \\\\quad F^{-1}(x, y)=(2 x-y, x-y)\\n$$\\n\\n(The second equation is obtained by replacing $a$ and $b$ by $x$ and $y$, respectively.)\\n',\n",
       " '\\n5.26. Let $G: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{3}$ be defined by $G(x, y)=(x+y, x-2 y, 3 x+y)$.\\n\\n(a) Show that $G$ is nonsingular. (b) Find a formula for $G^{-1}$.\\n\\n(a) Set $G(x, y)=(0,0,0)$ to find $\\\\operatorname{Ker} G$. We have\\n\\n$$\\n(x+y, x-2 y, 3 x+y)=(0,0,0) \\\\quad \\\\text { or } \\\\quad x+y=0, x-2 y=0,3 x+y=0\\n$$\\n\\nThe only solution is $x=0, y=0$; hence, $G$ is nonsingular.\\n\\n(b) Although $G$ is nonsingular, it is not invertible, because $\\\\mathbf{R}^{2}$ and $\\\\mathbf{R}^{3}$ have different dimensions. (Thus, Theorem 5.9 does not apply.) Accordingly, $G^{-1}$ does not exist.\\n',\n",
       " '\\n5.27. Suppose that $F: V \\\\rightarrow U$ is linear and that $V$ is of finite dimension. Show that $V$ and the image of $F$ have the same dimension if and only if $F$ is nonsingular. Determine all nonsingular linear mappings $T: \\\\mathbf{R}^{4} \\\\rightarrow \\\\mathbf{R}^{3}$.\\n\\nBy Theorem 5.6, $\\\\operatorname{dim} V=\\\\operatorname{dim}(\\\\operatorname{Im} F)+\\\\operatorname{dim}(\\\\operatorname{Ker} F)$. Hence, $V$ and $\\\\operatorname{Im} F$ have the same dimension if and only if $\\\\operatorname{dim}(\\\\operatorname{Ker} F)=0$ or $\\\\operatorname{Ker} F=\\\\{0\\\\}$ (i.e., if and only if $F$ is nonsingular).\\n\\nBecause $\\\\operatorname{dim} \\\\mathbf{R}^{3}$ is less than $\\\\operatorname{dim} \\\\mathbf{R}^{4}$, we have that $\\\\operatorname{dim}(\\\\operatorname{Im} T)$ is less than the dimension of the domain $\\\\mathbf{R}^{4}$ of $T$. Accordingly no linear mapping $T: \\\\mathbf{R}^{4} \\\\rightarrow \\\\mathbf{R}^{3}$ can be nonsingular.\\n',\n",
       " '\\n5.28. Prove Theorem 5.7: Let $F: V \\\\rightarrow U$ be a nonsingular linear mapping. Then the image of any linearly independent set is linearly independent.\\n\\nSuppose $v_{1}, v_{2}, \\\\ldots, v_{n}$ are linearly independent vectors in $V$. We claim that $F\\\\left(v_{1}\\\\right), F\\\\left(v_{2}\\\\right), \\\\ldots, F\\\\left(v_{n}\\\\right)$ are also linearly independent. Suppose $a_{1} F\\\\left(v_{1}\\\\right)+a_{2} F\\\\left(v_{2}\\\\right)+\\\\cdots+a_{n} F\\\\left(v_{n}\\\\right)=0$, where $a_{i} \\\\in K$. Because $F$ is linear, $F\\\\left(a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}\\\\right)=0$. Hence,\\n\\n$$\\na_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n} \\\\in \\\\operatorname{Ker} F\\n$$\\n\\nBut $F$ is nonsingular-that is, $\\\\operatorname{Ker} F=\\\\{0\\\\}$. Hence, $a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n}=0$. Because the $v_{i}$ are linearly independent, all the $a_{i}$ are 0 . Accordingly, the $F\\\\left(v_{i}\\\\right)$ are linearly independent. Thus, the theorem is proved.\\n',\n",
       " '\\n5.29. Prove Theorem 5.9: Suppose $V$ has finite dimension and $\\\\operatorname{dim} V=\\\\operatorname{dim} U$. Suppose $F: V \\\\rightarrow U$ is linear. Then $F$ is an isomorphism if and only if $F$ is nonsingular.\\n\\nIf $F$ is an isomorphism, then only 0 maps to 0 ; hence, $F$ is nonsingular. Conversely, suppose $F$ is nonsingular. Then $\\\\operatorname{dim}(\\\\operatorname{Ker} F)=0$. By Theorem 5.6, $\\\\operatorname{dim} V=\\\\operatorname{dim}(\\\\operatorname{Ker} F)+\\\\operatorname{dim}(\\\\operatorname{Im} F)$. Thus,\\n\\n$$\\n\\\\operatorname{dim} U=\\\\operatorname{dim} V=\\\\operatorname{dim}(\\\\operatorname{Im} F)\\n$$\\n\\nBecause $U$ has finite dimension, $\\\\operatorname{Im} F=U$. This means $F$ maps $V$ onto $U$. Thus, $F$ is one-to-one and onto; that is, $F$ is an isomorphism.\\n\\n\\n\\\\section*{Operations with Linear Maps}\\n',\n",
       " '5.30. Define $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ and $G: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ by $F(x, y, z)=(2 x, y+z)$ and $G(x, y, z)=(x-z, y)$. Find formulas defining the maps: (a) $F+G$, (b) $3 F$, (c) $2 F-5 G$.\\n\\n(a) $(F+G)(x, y, z)=F(x, y, z)+G(x, y, z)=(2 x, y+z)+(x-z, y)=(3 x-z, 2 y+z)$\\n\\n(b) $(3 F)(x, y, z)=3 F(x, y, z)=3(2 x, y+z)=(6 x, 3 y+3 z)$\\n\\n(c) $(2 F-5 G)(x, y, z)=2 F(x, y, z)-5 G(x, y, z)=2(2 x, y+z)-5(x-z, y)$\\n\\n$$\\n=(4 x, 2 y+2 z)+(-5 x+5 z, \\\\quad-5 y)=(-x+5 z,-3 y+2 z)\\n$$\\n',\n",
       " '\\n5.31. Let $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ and $G: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ be defined by $F(x, y, z)=(2 x, y+z)$ and $G(x, y)=(y, x)$. Derive formulas defining the mappings: (a) $G \\\\circ F$, (b) $F \\\\circ G$.\\n\\n(a) $(G \\\\circ F)(x, y, z)=G(F(x, y, z))=G(2 x, y+z)=(y+z, 2 x)$\\n\\n(b) The mapping $F \\\\circ G$ is not defined, because the image of $G$ is not contained in the domain of $F$.\\n',\n",
       " '\\n5.32. Prove: (a) The zero mapping $\\\\mathbf{0}$, defined by $\\\\mathbf{0}(v)=0 \\\\in U$ for every $v \\\\in V$, is the zero element of $\\\\operatorname{Hom}(V, U)$. (b) The negative of $F \\\\in \\\\operatorname{Hom}(V, U)$ is the mapping $(-1) F$, that is, $-F=(-1) F$.\\n\\nLet $F \\\\in \\\\operatorname{Hom}(V, U)$. Then, for every $v \\\\in V$ :\\n\\n\\n\\\\begin{equation*}\\n(F+\\\\mathbf{0})(v)=F(v)+\\\\mathbf{0}(v)=F(v)+0=F(v) \\\\tag{a}\\n\\\\end{equation*}\\n\\n\\nBecause $(F+\\\\mathbf{0})(v)=F(v)$ for every $v \\\\in V$, we have $F+\\\\mathbf{0}=F$. Similarly, $\\\\mathbf{0}+F=F$.\\n\\n\\n\\\\begin{equation*}\\n(F+(-1) F)(v)=F(v)+(-1) F(v)=F(v)-F(v)=0=\\\\mathbf{0}(v) \\\\tag{b}\\n\\\\end{equation*}\\n\\n\\nThus, $F+(-1) F=\\\\mathbf{0}$. Similarly $(-1) F+F=\\\\mathbf{0}$. Hence, $-F=(-1) F$.\\n',\n",
       " '\\n5.33. Suppose $F_{1}, F_{2}, \\\\ldots, F_{n}$ are linear maps from $V$ into $U$. Show that, for any scalars $a_{1}, a_{2}, \\\\ldots, a_{n}$, and for any $v \\\\in V$,\\n\\n$$\\n\\\\left(a_{1} F_{1}+a_{2} F_{2}+\\\\cdots+a_{n} F_{n}\\\\right)(v)=a_{1} F_{1}(v)+a_{2} F_{2}(v)+\\\\cdots+a_{n} F_{n}(v)\\n$$\\n\\nThe mapping $a_{1} F_{1}$ is defined by $\\\\left(a_{1} F_{1}\\\\right)(v)=a_{1} F(v)$. Hence, the theorem holds for $n=1$. Accordingly, by induction,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(a_{1} F_{1}+a_{2} F_{2}+\\\\cdots+a_{n} F_{n}\\\\right)(v) & =\\\\left(a_{1} F_{1}\\\\right)(v)+\\\\left(a_{2} F_{2}+\\\\cdots+a_{n} F_{n}\\\\right)(v) \\\\\\\\\\n& =a_{1} F_{1}(v)+a_{2} F_{2}(v)+\\\\cdots+a_{n} F_{n}(v)\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n5.34. Consider linear mappings $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}, G: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}, H: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y, z)=(x+y+z, x+y), \\\\quad G(x, y, z)=(2 x+z, x+y), \\\\quad H(x, y, z)=(2 y, x)$\\n\\nShow that $F, G, H$ are linearly independent [as elements of $\\\\left.\\\\operatorname{Hom}\\\\left(\\\\mathbf{R}^{3}, \\\\mathbf{R}^{2}\\\\right)\\\\right]$.\\n\\nSuppose, for scalars $a, b, c \\\\in K$,\\n\\n\\n\\\\begin{equation*}\\na F+b G+c H=\\\\mathbf{0} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\n(Here $\\\\mathbf{0}$ is the zero mapping.) For $e_{1}=(1,0,0) \\\\in \\\\mathbf{R}^{3}$, we have $\\\\mathbf{0}\\\\left(e_{1}\\\\right)=(0,0)$ and\\n\\n$$\\n\\\\begin{aligned}\\n(a F+b G+c H)\\\\left(e_{1}\\\\right) & =a F(1,0,0)+b G(1,0,0)+c H(1,0,0) \\\\\\\\\\n& =a(1,1)+b(2,1)+c(0,1)=(a+2 b, \\\\quad a+b+c)\\n\\\\end{aligned}\\n$$\\n\\nThus by $(1),(a+2 b, \\\\quad a+b+c)=(0,0)$ and so\\n\\n\\n\\\\begin{equation*}\\na+2 b=0 \\\\quad \\\\text { and } \\\\quad a+b+c=0 \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nSimilarly for $e_{2}=(0,1,0) \\\\in \\\\mathbf{R}^{3}$, we have $\\\\mathbf{0}\\\\left(e_{2}\\\\right)=(0,0)$ and\\n\\n$$\\n\\\\begin{aligned}\\n(a F+b G+c H)\\\\left(e_{2}\\\\right) & =a F(0,1,0)+b G(0,1,0)+c H(0,1,0) \\\\\\\\\\n& =a(1,1)+b(0,1)+c(2,0)=(a+2 c, \\\\quad a+b)\\n\\\\end{aligned}\\n$$\\n\\nThus,\\n\\n\\n\\\\begin{equation*}\\na+2 c=0 \\\\quad \\\\text { and } \\\\quad a+b=0 \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nUsing (2) and (3), we obtain\\n\\n\\n\\\\begin{equation*}\\na=0, \\\\quad b=0, \\\\quad c=0 \\\\tag{4}\\n\\\\end{equation*}\\n\\n\\nBecause (1) implies (4), the mappings $F, G, H$ are linearly independent.\\n',\n",
       " '\\n5.35. Let $k$ be a nonzero scalar. Show that a linear map $T$ is singular if and only if $k T$ is singular. Hence, $T$ is singular if and only if $-T$ is singular.\\n\\nSuppose $T$ is singular. Then $T(v)=0$ for some vector $v \\\\neq 0$. Hence,\\n\\n$$\\n(k T)(v)=k T(v)=k 0=0\\n$$\\n\\nand so $k T$ is singular.\\n\\nNow suppose $k T$ is singular. Then $(k T)(w)=0$ for some vector $w \\\\neq 0$. Hence,\\n\\n$$\\nT(k w)=k T(w)=(k T)(w)=0\\n$$\\n\\nBut $k \\\\neq 0$ and $w \\\\neq 0$ implies $k w \\\\neq 0$. Thus, $T$ is also singular.\\n',\n",
       " '\\n5.36. Find the dimension $d$ of:\\\\\\\\\\n(a) $\\\\operatorname{Hom}\\\\left(\\\\mathbf{R}^{3}, \\\\mathbf{R}^{4}\\\\right)$,\\\\\\\\\\n(b) $\\\\operatorname{Hom}\\\\left(\\\\mathbf{R}^{5}, \\\\mathbf{R}^{3}\\\\right)$\\\\\\\\\\n(c) $\\\\operatorname{Hom}\\\\left(\\\\mathbf{P}_{3}(t), \\\\mathbf{R}^{2}\\\\right)$,\\\\\\\\\\n(d) $\\\\operatorname{Hom}\\\\left(\\\\mathbf{M}_{2,3}, \\\\mathbf{R}^{4}\\\\right)$.\\n\\nUse $\\\\operatorname{dim}[\\\\operatorname{Hom}(V, U)]=m n$, where $\\\\operatorname{dim} V=m$ and $\\\\operatorname{dim} U=n$.\\\\\\\\\\n(a) $d=3(4)=12$.\\\\\\\\\\n(c) Because $\\\\operatorname{dim} \\\\mathbf{P}_{3}(t)=4, d=4(2)=8$.\\\\\\\\\\n(b) $d=5(3)=15$.\\\\\\\\\\n(d) Because $\\\\operatorname{dim} \\\\mathbf{M}_{2,3}=6, d=6(4)=24$.\\n',\n",
       " \"\\n5.37. Prove Theorem 5.11. Suppose $\\\\operatorname{dim} V=m$ and $\\\\operatorname{dim} U=n$. Then $\\\\operatorname{dim}[\\\\operatorname{Hom}(V, U)]=m n$.\\n\\nSuppose $\\\\left\\\\{v_{1}, \\\\ldots, v_{m}\\\\right\\\\}$ is a basis of $V$ and $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ is a basis of $U$. By Theorem 5.2, a linear mapping in $\\\\operatorname{Hom}(V, U)$ is uniquely determined by arbitrarily assigning elements of $U$ to the basis elements $v_{i}$ of $V$. We define\\n\\n$$\\nF_{i j} \\\\in \\\\operatorname{Hom}(V, U), \\\\quad i=1, \\\\ldots, m, \\\\quad j=1, \\\\ldots, n\\n$$\\n\\nto be the linear mapping for which $F_{i j}\\\\left(v_{i}\\\\right)=u_{j}$, and $F_{i j}\\\\left(v_{k}\\\\right)=0$ for $k \\\\neq i$. That is, $F_{i j}$ maps $v_{i}$ into $u_{j}$ and the other $v$ 's into 0 . Observe that $\\\\left\\\\{F_{i j}\\\\right\\\\}$ contains exactly $m n$ elements; hence, the theorem is proved if we show that it is a basis of $\\\\operatorname{Hom}(V, U)$.\\n\\nProof that $\\\\left\\\\{F_{i j}\\\\right\\\\}$ generates $\\\\operatorname{Hom}(V, U)$. Consider an arbitrary function $F \\\\in \\\\operatorname{Hom}(V, U)$. Suppose $F\\\\left(v_{1}\\\\right)=w_{1}, F\\\\left(v_{2}\\\\right)=w_{2}, \\\\ldots, F\\\\left(v_{m}\\\\right)=w_{m}$. Because $w_{k} \\\\in U$, it is a linear combination of the $u$ 's; say,\\n\\n\\n\\\\begin{equation*}\\nw_{k}=a_{k 1} u_{1}+a_{k 2} u_{2}+\\\\cdots+a_{k n} u_{n}, \\\\quad k=1, \\\\ldots, m, \\\\quad a_{i j} \\\\in K \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nConsider the linear mapping $G=\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} a_{i j} F_{i j}$. Because $G$ is a linear combination of the $F_{i j}$, the proof that $\\\\left\\\\{F_{i j}\\\\right\\\\}$ generates $\\\\operatorname{Hom}(V, U)$ is complete if we show that $F=G$.\\n\\nWe now compute $G\\\\left(v_{k}\\\\right), k=1, \\\\ldots, m$. Because $F_{i j}\\\\left(v_{k}\\\\right)=0$ for $k \\\\neq i$ and $F_{k i}\\\\left(v_{k}\\\\right)=u_{i}$,\\n\\n$$\\n\\\\begin{aligned}\\nG\\\\left(v_{k}\\\\right) & =\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} a_{i j} F_{i j}\\\\left(v_{k}\\\\right)=\\\\sum_{j=1}^{n} a_{k j} F_{k j}\\\\left(v_{k}\\\\right)=\\\\sum_{j=1}^{n} a_{k j} u_{j} \\\\\\\\\\n& =a_{k 1} u_{1}+a_{k 2} u_{2}+\\\\cdots+a_{k n} u_{n}\\n\\\\end{aligned}\\n$$\\n\\nThus, by (1), $G\\\\left(v_{k}\\\\right)=w_{k}$ for each $k$. But $F\\\\left(v_{k}\\\\right)=w_{k}$ for each $k$. Accordingly, by Theorem 5.2, $F=G$; hence, $\\\\left\\\\{F_{i j}\\\\right\\\\}$ generates $\\\\operatorname{Hom}(V, U)$.\\n\\nProof that $\\\\left\\\\{F_{i j}\\\\right\\\\}$ is linearly independent. Suppose, for scalars $c_{i j} \\\\in K$,\\n\\n$$\\n\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} c_{i j} F_{i j}=\\\\mathbf{0}\\n$$\\n\\nFor $v_{k}, k=1, \\\\ldots, m$,\\n\\n$$\\n\\\\begin{aligned}\\n0=\\\\mathbf{0}\\\\left(v_{k}\\\\right) & =\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} c_{i j} F_{i j}\\\\left(v_{k}\\\\right)=\\\\sum_{j=1}^{n} c_{k j} F_{k j}\\\\left(v_{k}\\\\right)=\\\\sum_{j=1}^{n} c_{k j} u_{j} \\\\\\\\\\n& =c_{k 1} u_{1}+c_{k 2} u_{2}+\\\\cdots+c_{k n} u_{n}\\n\\\\end{aligned}\\n$$\\n\\nBut the $u_{i}$ are linearly independent; hence, for $k=1, \\\\ldots, m$, we have $c_{k 1}=0, c_{k 2}=0, \\\\ldots, c_{k n}=0$. In other words, all the $c_{i j}=0$, and so $\\\\left\\\\{F_{i j}\\\\right\\\\}$ is linearly independent.\\n\",\n",
       " '\\n5.38. Prove Theorem 5.12: (i) $G \\\\circ\\\\left(F+F^{\\\\prime}\\\\right)=G \\\\circ F+G \\\\circ F^{\\\\prime}$. (ii) $\\\\left(G+G^{\\\\prime}\\\\right) \\\\circ F=G \\\\circ F+G^{\\\\prime} \\\\circ F$. (iii) $k(G \\\\circ F)=(k G) \\\\circ F=G \\\\circ(k F)$.\\n\\n(i) For every $v \\\\in V$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(G \\\\circ\\\\left(F+F^{\\\\prime}\\\\right)\\\\right)(v) & =G\\\\left(\\\\left(F+F^{\\\\prime}\\\\right)(v)\\\\right)=G\\\\left(F(v)+F^{\\\\prime}(v)\\\\right) \\\\\\\\\\n& =G(F(v))+G\\\\left(F^{\\\\prime}(v)\\\\right)=(G \\\\circ F)(v)+\\\\left(G \\\\circ F^{\\\\prime}\\\\right)(v)=\\\\left(G \\\\circ F+G \\\\circ F^{\\\\prime}\\\\right)(v)\\n\\\\end{aligned}\\n$$\\n\\nThus, $G \\\\circ\\\\left(F+F^{\\\\prime}\\\\right)=G \\\\circ F+G \\\\circ F^{\\\\prime}$.\\n\\n(ii) For every $v \\\\in V$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(\\\\left(G+G^{\\\\prime}\\\\right) \\\\circ F\\\\right)(v) & =\\\\left(G+G^{\\\\prime}\\\\right)(F(v))=G(F(v))+G^{\\\\prime}(F(v)) \\\\\\\\\\n& =(G \\\\circ F)(v)+\\\\left(G^{\\\\prime} \\\\circ F\\\\right)(v)=\\\\left(G \\\\circ F+G^{\\\\prime} \\\\circ F\\\\right)(v)\\n\\\\end{aligned}\\n$$\\n\\nThus, $\\\\left(G+G^{\\\\prime}\\\\right) \\\\circ F=G \\\\circ F+G^{\\\\prime} \\\\circ F$.\\\\\\\\\\n(iii) For every $v \\\\in V$,\\n\\n$$\\n(k(G \\\\circ F))(v)=k(G \\\\circ F)(v)=k(G(F(v)))=(k G)(F(v))=(k G \\\\circ F)(v)\\n$$\\n\\nand\\n\\n$$\\n(k(G \\\\circ F))(v)=k(G \\\\circ F)(v)=k(G(F(v)))=G(k F(v))=G((k F)(v))=(G \\\\circ k F)(v)\\n$$\\n\\nAccordingly, $k(G \\\\circ F)=(k G) \\\\circ F=G \\\\circ(k F)$. (We emphasize that two mappings are shown to be equal by showing that each of them assigns the same image to each point in the domain.)\\n\\n\\n\\\\section*{Algebra of Linear Maps}\\n',\n",
       " '5.39. Let $F$ and $G$ be the linear operators on $\\\\mathbf{R}^{2}$ defined by $F(x, y)=(y, x)$ and $G(x, y)=(0, x)$. Find formulas defining the following operators:\\\\\\\\\\n(a) $F+G$,\\\\\\\\\\n(b) $2 F-3 G$,\\\\\\\\\\n(c) $F G$,\\\\\\\\\\n(d) $G F$,\\\\\\\\\\n(e) $F^{2}$,\\\\\\\\\\n(f) $G^{2}$.\\n\\n(a) $(F+G)(x, y)=F(x, y)+G(x, y)=(y, x)+(0, x)=(y, 2 x)$.\\n\\n(b) $(2 F-3 G)(x, y)=2 F(x, y)-3 G(x, y)=2(y, x)-3(0, x)=(2 y,-x)$.\\n\\n(c) $(F G)(x, y)=F(G(x, y))=F(0, x)=(x, 0)$.\\n\\n(d) $(G F)(x, y)=G(F(x, y))=G(y, x)=(0, y)$.\\n\\n(e) $F^{2}(x, y)=F(F(x, y))=F(y, x)=(x, y)$. (Note that $F^{2}=I$, the identity mapping.)\\n\\n(f) $G^{2}(x, y)=G(G(x, y))=G(0, x)=(0,0)$. (Note that $G^{2}=\\\\mathbf{0}$, the zero mapping.)\\n',\n",
       " '\\n5.40. Consider the linear operator $T$ on $\\\\mathbf{R}^{3}$ defined by $T(x, y, z)=(2 x, 4 x-y, 2 x+3 y-z)$.\\n\\n(a) Show that $T$ is invertible. Find formulas for (b) $T^{-1}$, (c) $T^{2},(d) T^{-2}$.\\n\\n(a) Let $W=\\\\operatorname{Ker} T$. We need only show that $T$ is nonsingular (i.e., that $W=\\\\{0\\\\}$ ). $\\\\operatorname{Set} T(x, y, z)=(0,0,0)$, which yields\\n\\n$$\\nT(x, y, z)=(2 x, 4 x-y, 2 x+3 y-z)=(0,0,0)\\n$$\\n\\nThus, $W$ is the solution space of the homogeneous system\\n\\n$$\\n2 x=0, \\\\quad 4 x-y=0, \\\\quad 2 x+3 y-z=0\\n$$\\n\\nwhich has only the trivial solution $(0,0,0)$. Thus, $W=\\\\{0\\\\}$. Hence, $T$ is nonsingular, and so $T$ is invertible.\\n\\n(b) Set $T(x, y, z)=(r, s, t)$ [and so $\\\\left.T^{-1}(r, s, t)=(x, y, z)\\\\right]$. We have\\n\\n$$\\n(2 x, 4 x-y, 2 x+3 y-z)=(r, s, t) \\\\quad \\\\text { or } \\\\quad 2 x=r, \\\\quad 4 x-y=s, \\\\quad 2 x+3 y-z=t\\n$$\\n\\nSolve for $x, y, z$ in terms of $r, s, t$ to get $x=\\\\frac{1}{2} r, y=2 r-s, z=7 r-3 s-t$. Thus,\\n\\n$$\\nT^{-1}(r, s, t)=\\\\left(\\\\frac{1}{2} r, 2 r-s, 7 r-3 s-t\\\\right) \\\\quad \\\\text { or } \\\\quad T^{-1}(x, y, z)=\\\\left(\\\\frac{1}{2} x, 2 x-y, 7 x-3 y-z\\\\right)\\n$$\\n\\n(c) Apply $T$ twice to get\\n\\n$$\\n\\\\begin{aligned}\\nT^{2}(x, y, z) & =T(2 x, \\\\quad 4 x-y, \\\\quad 2 x+3 y-z) \\\\\\\\\\n& =\\\\left[\\\\begin{array}{ll}\\n4 x, & 4(2 x)-(4 x-y), \\\\quad 2(2 x)+3(4 x-y)-(2 x+3 y-z)\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =(4 x, \\\\quad 4 x+y, \\\\quad 14 x-6 y+z)\\n\\\\end{aligned}\\n$$\\n\\n(d) Apply $T^{-1}$ twice to get\\n\\n$$\\n\\\\begin{aligned}\\nT^{-2}(x, y, z) & =T^{-2}\\\\left(\\\\frac{1}{2} x, \\\\quad 2 x-y, \\\\quad 7 x-3 y-z\\\\right) \\\\\\\\\\n& =\\\\left[\\\\begin{array}{ll}\\n\\\\frac{1}{4} x, \\\\quad 2\\\\left(\\\\frac{1}{2} x\\\\right)-(2 x-y), \\\\quad 7\\\\left(\\\\frac{1}{2} x\\\\right)-3(2 x-y)-(7 x-3 y-z)\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =\\\\left(\\\\begin{array}{ll}\\n\\\\frac{1}{4} x, & -x+y, \\\\quad-\\\\frac{19}{2} x+6 y+z\\n\\\\end{array}\\\\right)\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n5.41. Let $V$ be of finite dimension and let $T$ be a linear operator on $V$ for which $T R=I$, for some operator $R$ on $V$. (We call $R$ a right inverse of $T$.)\\n\\n(a) Show that $T$ is invertible. (b) Show that $R=T^{-1}$.\\n\\n(c) Give an example showing that the above need not hold if $V$ is of infinite dimension.\\n\\n(a) Let $\\\\operatorname{dim} V=n$. By Theorem 5.14, $T$ is invertible if and only if $T$ is onto; hence, $T$ is invertible if and only if $\\\\operatorname{rank}(T)=n$. We have $n=\\\\operatorname{rank}(I)=\\\\operatorname{rank}(T R) \\\\leq \\\\operatorname{rank}(T) \\\\leq n$. Hence, $\\\\operatorname{rank}(T)=n$ and $T$ is invertible.\\n\\n(b) $T T^{-1}=T^{-1} T=I$. Then $R=I R=\\\\left(T^{-1} T\\\\right) R=T^{-1}(T R)=T^{-1} I=T^{-1}$.\\n\\n(c) Let $V$ be the space of polynomials in $t$ over $K$; say, $p(t)=a_{0}+a_{1} t+a_{2} t^{2}+\\\\cdots+a_{s} t^{s}$. Let $T$ and $R$ be the operators on $V$ defined by\\n\\n$$\\nT(p(t))=0+a_{1}+a_{2} t+\\\\cdots+a_{s} t^{s-1} \\\\quad \\\\text { and } \\\\quad R(p(t))=a_{0} t+a_{1} t^{2}+\\\\cdots+a_{s} t^{s+1}\\n$$\\n\\nWe have\\n\\n$$\\n(T R)(p(t))=T(R(p(t)))=T\\\\left(a_{0} t+a_{1} t^{2}+\\\\cdots+a_{s} t^{s+1}\\\\right)=a_{0}+a_{1} t+\\\\cdots+a_{s} t^{s}=p(t)\\n$$\\n\\nand so $T R=I$, the identity mapping. On the other hand, if $k \\\\in K$ and $k \\\\neq 0$, then\\n\\n$$\\n(R T)(k)=R(T(k))=R(0)=0 \\\\neq k\\n$$\\n\\nAccordingly, $R T \\\\neq I$.\\n',\n",
       " '\\n5.42. Let $F$ and $G$ be linear operators on $\\\\mathbf{R}^{2}$ defined by $F(x, y)=(0, x)$ and $G(x, y)=(x, 0)$. Show that\\n\\n(a) $G F=\\\\mathbf{0}$, the zero mapping, but $F G \\\\neq \\\\mathbf{0}$. (b) $G^{2}=G$.\\n\\n(a) $(G F)(x, y)=G(F(x, y))=G(0, x)=(0,0)$. Because $G F$ assigns $0=(0,0)$ to every vector $(x, y)$ in $\\\\mathbf{R}^{2}$, it is the zero mapping; that is, $G F=\\\\mathbf{0}$.\\n\\nOn the other hand, $(F G)(x, y)=F(G(x, y))=F(x, 0)=(0, x)$. For example, $(F G)(2,3)=(0,2)$. Thus, $F G \\\\neq \\\\mathbf{0}$, as it does not assign $0=(0,0)$ to every vector in $\\\\mathbf{R}^{2}$.\\n\\n(b) For any vector $(x, y)$ in $\\\\mathbf{R}^{2}$, we have $G^{2}(x, y)=G(G(x, y))=G(x, 0)=(x, 0)=G(x, y)$. Hence, $G^{2}=G$.\\n',\n",
       " '\\n5.43. Find the dimension of (a) $A\\\\left(\\\\mathbf{R}^{4}\\\\right)$, (b) $A\\\\left(\\\\mathbf{P}_{2}(t)\\\\right.$ ), (c) $A\\\\left(\\\\mathbf{M}_{2,3}\\\\right)$.\\n\\nUse $\\\\operatorname{dim}[A(V)]=n^{2}$ where $\\\\operatorname{dim} V=n$. Hence, (a) $\\\\operatorname{dim}\\\\left[A\\\\left(\\\\mathbf{R}^{4}\\\\right)\\\\right]=4^{2}=16$, (b) $\\\\operatorname{dim}\\\\left[A\\\\left(\\\\mathbf{P}_{2}(t)\\\\right)\\\\right]=3^{2}=9$, (c) $\\\\operatorname{dim}\\\\left[A\\\\left(\\\\mathbf{M}_{2,3}\\\\right)\\\\right]=6^{2}=36$.\\n',\n",
       " '\\n5.44. Let $E$ be a linear operator on $V$ for which $E^{2}=E$. (Such an operator is called a projection.) Let $U$ be the image of $E$, and let $W$ be the kernel. Prove\\n\\n(a) If $u \\\\in U$, then $E(u)=u$ (i.e., $E$ is the identity mapping on $U$ ).\\n\\n(b) If $E \\\\neq I$, then $E$ is singular - that is, $E(v)=0$ for some $v \\\\neq 0$.\\n\\n(c) $V=U \\\\oplus W$.\\n\\n(a) If $u \\\\in U$, the image of $E$, then $E(v)=u$ for some $v \\\\in V$. Hence, using $E^{2}=E$, we have\\n\\n$$\\nu=E(v)=E^{2}(v)=E(E(v))=E(u)\\n$$\\n\\n(b) If $E \\\\neq I$, then for some $v \\\\in V, E(v)=u$, where $v \\\\neq u$. By (i), $E(u)=u$. Thus,\\n\\n$$\\nE(v-u)=E(v)-E(u)=u-u=0, \\\\quad \\\\text { where } \\\\quad v-u \\\\neq 0\\n$$\\n\\n(c) We first show that $V=U+W$. Let $v \\\\in V$. Set $u=E(v)$ and $w=v-E(v)$. Then\\n\\n$$\\nv=E(v)+v-E(v)=u+w\\n$$\\n\\nBy deflnition, $u=E(v) \\\\in U$, the image of $E$. We now show that $w \\\\in W$, the kernel of $E$,\\n\\n$$\\nE(w)=E(v-E(v))=E(v)-E^{2}(v)=E(v)-E(v)=0\\n$$\\n\\nand thus $w \\\\in W$. Hence, $V=U+W$.\\n\\nWe next show that $U \\\\cap W=\\\\{0\\\\}$. Let $v \\\\in U \\\\cap W$. Because $v \\\\in U, E(v)=v$ by part (a). Because $v \\\\in W, E(v)=0$. Thus, $v=E(v)=0$ and so $U \\\\cap W=\\\\{0\\\\}$.\\n\\nThe above two properties imply that $V=U \\\\oplus W$.\\n\\n',\n",
       " \"6.1. Consider the linear mapping $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(3 x+4 y, \\\\quad 2 x-5 y)$ and the following bases of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nE=\\\\left\\\\{e_{1}, e_{2}\\\\right\\\\}=\\\\{(1,0),(0,1)\\\\} \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,2),(2,3)\\\\}\\n$$\\n\\n(a) Find the matrix $A$ representing $F$ relative to the basis $E$.\\n\\n(b) Find the matrix $B$ representing $F$ relative to the basis $S$.\\n\\n(a) Because $E$ is the usual basis, the rows of $A$ are simply the coefficients in the components of $F(x, y)$; that is, using $(a, b)=a e_{1}+b e_{2}$, we have\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(e_{1}\\\\right)=F(1,0)=(3,2)=3 e_{1}+2 e_{2} \\\\\\\\\\n& F\\\\left(e_{2}\\\\right)=F(0,1)=(4,-5)=4 e_{1}-5 e_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\n3 & 4 \\\\\\\\\\n2 & -5\\n\\\\end{array}\\\\right]\\n$$\\n\\nNote that the coefficients of the basis vectors are written as columns in the matrix representation.\\\\\\\\\\n(b) First find $F\\\\left(u_{1}\\\\right)$ and write it as a linear combination of the basis vectors $u_{1}$ and $u_{2}$. We have\\n\\n$$\\nF\\\\left(u_{1}\\\\right)=F(1,2)=(11,-8)=x(1,2)+y(2,3), \\\\quad \\\\text { and so } \\\\quad \\\\begin{aligned}\\nx+2 y & =11 \\\\\\\\\\n2 x+3 y & =-8\\n\\\\end{aligned}\\n$$\\n\\nSolve the system to obtain $x=-49, y=30$. Therefore,\\n\\n$$\\nF\\\\left(u_{1}\\\\right)=-49 u_{1}+30 u_{2}\\n$$\\n\\nNext find $F\\\\left(u_{2}\\\\right)$ and write it as a linear combination of the basis vectors $u_{1}$ and $u_{2}$. We have\\n\\n$$\\nF\\\\left(u_{2}\\\\right)=F(2,3)=(18,-11)=x(1,2)+y(2,3), \\\\quad \\\\text { and so } \\\\quad \\\\begin{aligned}\\nx+2 y & =18 \\\\\\\\\\n2 x+3 y & =-11\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x$ and $y$ to obtain $x=-76, y=47$. Hence,\\n\\n$$\\nF\\\\left(u_{2}\\\\right)=-76 u_{1}+47 u_{2}\\n$$\\n\\nWrite the coefficients of $u_{1}$ and $u_{2}$ as columns to obtain $B=\\\\left[\\\\begin{array}{rr}-49 & -76 \\\\\\\\ 30 & 47\\\\end{array}\\\\right]$\\n\\n(b') Alternatively, one can first find the coordinates of an arbitrary vector $(a, b)$ in $\\\\mathbf{R}^{2}$ relative to the basis $S$. We have\\n\\n$$\\n(a, b)=x(1,2)+y(2,3)=(x+2 y, 2 x+3 y), \\\\quad \\\\text { and so } \\\\quad \\\\begin{aligned}\\nx+2 y & =a \\\\\\\\\\n2 x+3 y & =b\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-3 a+2 b, y=2 a-b$. Thus,\\n\\n$$\\n(a, b)=(-3 a+2 b) u_{1}+(2 a-b) u_{2}\\n$$\\n\\nThen use the formula for $(a, b)$ to find the coordinates of $F\\\\left(u_{1}\\\\right)$ and $F\\\\left(u_{2}\\\\right)$ relative to $S$ :\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(u_{1}\\\\right)=F(1,2)=(11,-8)=-49 u_{1}+30 u_{2} \\\\\\\\\\n& F\\\\left(u_{2}\\\\right)=F(2,3)=(18,-11)=-76 u_{1}+47 u_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad B=\\\\left[\\\\begin{array}{rr}\\n-49 & -76 \\\\\\\\\\n30 & 47\\n\\\\end{array}\\\\right]\\n$$\\n\",\n",
       " '\\n6.2. Consider the following linear operator $G$ on $\\\\mathbf{R}^{2}$ and basis $S$ :\\n\\n$$\\nG(x, y)=(2 x-7 y, 4 x+3 y) \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,3),(2,5)\\\\}\\n$$\\n\\n(a) Find the matrix representation $[G]_{S}$ of $G$ relative to $S$.\\n\\n(b) Verify $[G]_{S}[v]_{S}=[G(v)]_{S}$ for the vector $v=(4,-3)$ in $\\\\mathbf{R}^{2}$. have\\n\\nFirst find the coordinates of an arbitrary vector $v=(a, b)$ in $\\\\mathbf{R}^{2}$ relative to the basis $S$. We\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n2 \\\\\\\\\\n5\\n\\\\end{array}\\\\right], \\\\quad \\\\text { and so } \\\\quad \\\\begin{array}{r}\\nx+2 y=a \\\\\\\\\\n3 x+5 y=b\\n\\\\end{array}\\n$$\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-5 a+2 b, y=3 a-b$. Thus,\\n\\n$$\\n(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}, \\\\quad \\\\text { and so } \\\\quad[v]=\\\\left[\\\\begin{array}{ll}\\n-5 a+2 b, & 3 a-b\\n\\\\end{array}\\\\right]^{T}\\n$$\\n\\n(a) Using the formula for $(a, b)$ and $G(x, y)=(2 x-7 y, 4 x+3 y)$, we have\\n\\n$$\\n\\\\begin{aligned}\\n& G\\\\left(u_{1}\\\\right)=G(1,3)=(-19,13)=121 u_{1}-70 u_{2} \\\\\\\\\\n& G\\\\left(u_{2}\\\\right)=G(2,5)=(-31,23)=201 u_{1}-116 u_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad[G]_{S}=\\\\left[\\\\begin{array}{rr}\\n121 & 201 \\\\\\\\\\n-70 & -116\\n\\\\end{array}\\\\right]\\n$$\\n\\n(We emphasize that the coefficients of $u_{1}$ and $u_{2}$ are written as columns, not rows, in the matrix representation.)\\n\\n(b) Use the formula $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$ to get\\n\\n$$\\n\\\\begin{aligned}\\nv & =(4,-3)=-26 u_{1}+15 u_{2} \\\\\\\\\\nG(v) & =G(4,-3)=(20,7)=-131 u_{1}+80 u_{2}\\n\\\\end{aligned}\\n$$\\n\\nThen\\n\\n$$\\n[v]_{S}=[-26,15]^{T} \\\\quad \\\\text { and } \\\\quad[G(v)]_{S}=[-131,80]^{T}\\n$$\\n\\nAccordingly,\\n\\n$$\\n[G]_{S}[v]_{S}=\\\\left[\\\\begin{array}{rr}\\n121 & 201 \\\\\\\\\\n-70 & -116\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n-26 \\\\\\\\\\n15\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\n-131 \\\\\\\\\\n80\\n\\\\end{array}\\\\right]=[G(v)]_{S}\\n$$\\n\\n(This is expected from Theorem 6.1.)\\n',\n",
       " '\\n6.3. Consider the following $2 \\\\times 2$ matrix $A$ and basis $S$ of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nA=\\\\left[\\\\begin{array}{ll}\\n2 & 4 \\\\\\\\\\n5 & 6\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\left\\\\{\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-2\\n\\\\end{array}\\\\right], \\\\quad\\\\left[\\\\begin{array}{r}\\n3 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right]\\\\right\\\\}\\n$$\\n\\nThe matrix $A$ defines a linear operator on $\\\\mathbf{R}^{2}$. Find the matrix $B$ that represents the mapping $A$ relative to the basis $S$.\\n\\nFirst find the coordinates of an arbitrary vector $(a, b)^{T}$ with respect to the basis $S$. We have\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-2\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{r}\\n3 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+3 y & =a \\\\\\\\\\n-2 x-7 y & =b\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to obtain $x=7 a+3 b, y=-2 a-b$. Thus,\\n\\n$$\\n(a, b)^{T}=(7 a+3 b) u_{1}+(-2 a-b) u_{2}\\n$$\\n\\nThen use the formula for $(a, b)^{T}$ to find the coordinates of $A u_{1}$ and $A u_{2}$ relative to the basis $S$ :\\n\\n$$\\n\\\\begin{aligned}\\n& A u_{1}=\\\\left[\\\\begin{array}{ll}\\n2 & 4 \\\\\\\\\\n5 & 6\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n1 \\\\\\\\\\n-2\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n-6 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right]=-63 u_{1}+19 u_{2} \\\\\\\\\\n& A u_{2}=\\\\left[\\\\begin{array}{ll}\\n2 & 4 \\\\\\\\\\n5 & 6\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n3 \\\\\\\\\\n-7\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n-22 \\\\\\\\\\n-27\\n\\\\end{array}\\\\right]=-235 u_{1}+71 u_{2}\\n\\\\end{aligned}\\n$$\\n\\nWriting the coordinates as columns yields\\n\\n$$\\nB=\\\\left[\\\\begin{array}{rr}\\n-63 & -235 \\\\\\\\\\n19 & 71\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.4. Find the matrix representation of each of the following linear operators $F$ on $\\\\mathbf{R}^{3}$ relative to the usual basis $E=\\\\left\\\\{e_{1}, e_{2}, e_{3}\\\\right\\\\}$ of $\\\\mathbf{R}^{3}$; that is, find $[F]=[F]_{E}$ :\\n\\n(a) $F$ defined by $F(x, y, z)=(x+2 y-3 z, 4 x-5 y-6 z, 7 x+8 y+9 z)$.\\n\\n(b) $F$ defined by the $3 \\\\times 3$ matrix $A=\\\\left[\\\\begin{array}{lll}1 & 1 & 1 \\\\\\\\ 2 & 3 & 4 \\\\\\\\ 5 & 5 & 5\\\\end{array}\\\\right]$.\\n\\n(c) $F$ defined by $F\\\\left(e_{1}\\\\right)=(1,3,5), F\\\\left(e_{2}\\\\right)=(2,4,6), F\\\\left(e_{3}\\\\right)=(7,7,7)$. (Theorem 5.2 states that a linear map is completely defined by its action on the vectors in a basis.)\\n\\n(a) Because $E$ is the usual basis, simply write the coefficients of the components of $F(x, y, z)$ as rows:\\n\\n$$\\n[F]=\\\\left[\\\\begin{array}{rrr}\\n1 & 2 & -3 \\\\\\\\\\n4 & -5 & -6 \\\\\\\\\\n7 & 8 & 9\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Because $E$ is the usual basis, $[F]=A$, the matrix $A$ itself.\\n\\n(c) Here\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(e_{1}\\\\right)=(1,3,5)=e_{1}+3 e_{2}+5 e_{3} \\\\\\\\\\n& F\\\\left(e_{2}\\\\right)=(2,4,6)=2 e_{1}+4 e_{2}+6 e_{3} \\\\\\\\\\n& F\\\\left(e_{3}\\\\right)=(7,7,7)=7 e_{1}+7 e_{2}+7 e_{3}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad[F]=\\\\left[\\\\begin{array}{lll}\\n1 & 2 & 7 \\\\\\\\\\n3 & 4 & 7 \\\\\\\\\\n5 & 6 & 7\\n\\\\end{array}\\\\right]\\n$$\\n\\nThat is, the columns of $[F]$ are the images of the usual basis vectors.\\n',\n",
       " '\\n6.5. Let $G$ be the linear operator on $\\\\mathbf{R}^{3}$ defined by $G(x, y, z)=(2 y+z, x-4 y, 3 x)$.\\n\\n(a) Find the matrix representation of $G$ relative to the basis\\n\\n$$\\nS=\\\\left\\\\{w_{1}, w_{2}, w_{3}\\\\right\\\\}=\\\\{(1,1,1), \\\\quad(1,1,0), \\\\quad(1,0,0)\\\\}\\n$$\\n\\n(b) Verify that $[G][v]=[G(v)]$ for any vector $v$ in $\\\\mathbf{R}^{3}$.\\n\\nFirst find the coordinates of an arbitrary vector $(a, b, c) \\\\in \\\\mathbf{R}^{3}$ with respect to the basis $S$. Write $(a, b, c)$ as a linear combination of $w_{1}, w_{2}, w_{3}$ using unknown scalars $x, y$, and $z$ :\\n\\n$$\\n(a, b, c)=x(1,1,1)+y(1,1,0)+z(1,0,0)=(x+y+z, x+y, x)\\n$$\\n\\nSet corresponding components equal to each other to obtain the system of equations\\n\\n$$\\nx+y+z=a, \\\\quad x+y=b, \\\\quad x=c\\n$$\\n\\nSolve the system for $x, y, z$ in terms of $a, b, c$ to find $x=c, y=b-c, z=a-b$. Thus,\\n\\n$(a, b, c)=c w_{1}+(b-c) w_{2}+(a-b) w_{3}, \\\\quad$ or equivalently, $\\\\quad[(a, b, c)]=[c, b-c, a-b]^{T}$\\n\\n(a) Because $G(x, y, z)=(2 y+z, x-4 y, 3 x)$,\\n\\n$$\\n\\\\begin{aligned}\\n& G\\\\left(w_{1}\\\\right)=G(1,1,1)=(3,-3,3)=3 w_{1}-6 x_{2}+6 x_{3} \\\\\\\\\\n& G\\\\left(w_{2}\\\\right)=G(1,1,0)=(2,-3,3)=3 w_{1}-6 w_{2}+5 w_{3} \\\\\\\\\\n& G\\\\left(w_{3}\\\\right)=G(1,0,0)=(0,1,3)=3 w_{1}-2 w_{2}-w_{3}\\n\\\\end{aligned}\\n$$\\n\\nWrite the coordinates $G\\\\left(w_{1}\\\\right), G\\\\left(w_{2}\\\\right), G\\\\left(w_{3}\\\\right)$ as columns to get\\n\\n$$\\n[G]=\\\\left[\\\\begin{array}{rrr}\\n3 & 3 & 3 \\\\\\\\\\n-6 & -6 & -2 \\\\\\\\\\n6 & 5 & -1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Write $G(v)$ as a linear combination of $w_{1}, w_{2}, w_{3}$, where $v=(a, b, c)$ is an arbitrary vector in $\\\\mathbf{R}^{3}$,\\n\\n$$\\nG(v)=G(a, b, c)=(2 b+c, a-4 b, 3 a)=3 a w_{1}+(-2 a-4 b) w_{2}+(-a+6 b+c) w_{3}\\n$$\\n\\nor equivalently,\\n\\nAccordingly,\\n\\n$$\\n[G(v)]=[3 a,-2 a-4 b,-a+6 b+c]^{T}\\n$$\\n\\n$$\\n[G][v]=\\\\left[\\\\begin{array}{rrr}\\n3 & 3 & 3 \\\\\\\\\\n-6 & -6 & -2 \\\\\\\\\\n6 & 5 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}\\nc \\\\\\\\\\nb-c \\\\\\\\\\na-b\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{c}\\n3 a \\\\\\\\\\n-2 a-4 b \\\\\\\\\\n-a+6 b+c\\n\\\\end{array}\\\\right]=[G(v)]\\n$$\\n',\n",
       " '\\n6.6. Consider the following $3 \\\\times 3$ matrix $A$ and basis $S$ of $\\\\mathbf{R}^{3}$ :\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & -2 & 1 \\\\\\\\\\n3 & -1 & 0 \\\\\\\\\\n1 & 4 & -2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}, u_{3}\\\\right\\\\}=\\\\left\\\\{\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right], \\\\quad\\\\left[\\\\begin{array}{l}\\n0 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right], \\\\quad\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]\\\\right\\\\}\\n$$\\n\\nThe matrix $A$ defines a linear operator on $\\\\mathbf{R}^{3}$. Find the matrix $B$ that represents the mapping $A$ relative to the basis $S$. (Recall that $A$ represents itself relative to the usual basis of $\\\\mathbf{R}^{3}$.)\\n\\nFirst find the coordinates of an arbitrary vector $(a, b, c)$ in $\\\\mathbf{R}^{3}$ with respect to the basis $S$. We have\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb \\\\\\\\\\nc\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n0 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\n& x+z=a \\\\\\\\\\n& x+y+2 z=b \\\\\\\\\\n& x+y+3 z=c\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x, y, z$ in terms of $a, b, c$ to get\\n\\n$$\\n\\\\begin{aligned}\\nx & =a+b-c, \\\\quad y=-a+2 b-c, \\\\quad z=c-b \\\\\\\\\\n\\\\text { thus, } \\\\quad(a, b, c)^{T} & =(a+b-c) u_{1}+(-a+2 b-c) u_{2}+(c-b) u_{3}\\n\\\\end{aligned}\\n$$\\n\\nThen use the formula for $(a, b, c)^{T}$ to find the coordinates of $A u_{1}, A u_{2}, A u_{3}$ relative to the basis $S$ :\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-216(1)}\\n\\\\end{center}\\n',\n",
       " '\\n6.7. For each of the following linear transformations (operators) $L$ on $\\\\mathbf{R}^{2}$, find the matrix $A$ that represents $L$ (relative to the usual basis of $\\\\mathbf{R}^{2}$ ):\\n\\n(a) $L$ is defined by $L(1,0)=(2,4)$ and $L(0,1)=(5,8)$.\\n\\n(b) $L$ is the rotation in $\\\\mathbf{R}^{2}$ counterclockwise by $90^{\\\\circ}$.\\n\\n(c) $L$ is the reflection in $\\\\mathbf{R}^{2}$ about the line $y=-x$.\\n\\n(a) Because $\\\\{(1,0),(0,1)\\\\}$ is the usual basis of $\\\\mathbf{R}^{2}$, write their images under $L$ as columns to get\\n\\n$$\\nA=\\\\left[\\\\begin{array}{ll}\\n2 & 5 \\\\\\\\\\n4 & 8\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Under the rotation $L$, we have $L(1,0)=(0,1)$ and $L(0,1)=(-1,0)$. Thus,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n0 & -1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Under the reflection $L$, we have $L(1,0)=(0,-1)$ and $L(0,1)=(-1,0)$. Thus,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n0 & -1 \\\\\\\\\\n-1 & 0\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.8. The set $S=\\\\left\\\\{e^{3 t}, t e^{3 t}, t^{2} e^{3 t}\\\\right\\\\}$ is a basis of a vector space $V$ of functions $f: \\\\mathbf{R} \\\\rightarrow \\\\mathbf{R}$. Let $\\\\mathbf{D}$ be the differential operator on $V$; that is, $\\\\mathbf{D}(f)=d f / d t$. Find the matrix representation of $\\\\mathbf{D}$ relative to the basis $S$.\\n\\nFind the image of each basis function:\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-216}\\n\\\\end{center}\\n',\n",
       " '\\n6.9. Prove Theorem 6.1: Let $T: V \\\\rightarrow V$ be a linear operator, and let $S$ be a (finite) basis of $V$. Then, for any vector $v$ in $V,[T]_{S}[v]_{S}=[T(v)]_{S}$.\\n\\nSuppose $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$, and suppose, for $i=1, \\\\ldots, n$,\\n\\n$$\\nT\\\\left(u_{i}\\\\right)=a_{i 1} u_{1}+a_{i 2} u_{2}+\\\\cdots+a_{i n} u_{n}=\\\\sum_{j=1}^{n} a_{i j} u_{j}\\n$$\\n\\nThen $[T]_{S}$ is the $n$-square matrix whose $j$ th row is\\n\\n\\n\\\\begin{equation*}\\n\\\\left(a_{1 j}, a_{2 j}, \\\\ldots, a_{n j}\\\\right) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nNow suppose\\n\\n$$\\nv=k_{1} u_{1}+k_{2} u_{2}+\\\\cdots+k_{n} u_{n}=\\\\sum_{i=1}^{n} k_{i} u_{i}\\n$$\\n\\nWriting a column vector as the transpose of a row vector, we have\\n\\n\\n\\\\begin{equation*}\\n[v]_{S}=\\\\left[k_{1}, k_{2}, \\\\ldots, k_{n}\\\\right]^{T} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nFurthermore, using the linearity of $T$,\\n\\n$$\\n\\\\begin{aligned}\\nT(v) & =T\\\\left(\\\\sum_{i=1}^{n} k_{i} u_{i}\\\\right)=\\\\sum_{i=1}^{n} k_{i} T\\\\left(u_{i}\\\\right)=\\\\sum_{i=1}^{n} k_{i}\\\\left(\\\\sum_{j=1}^{n} a_{i j} u_{j}\\\\right) \\\\\\\\\\n& =\\\\sum_{j=1}^{n}\\\\left(\\\\sum_{i=1}^{n} a_{i j} k_{i}\\\\right) u_{j}=\\\\sum_{j=1}^{n}\\\\left(a_{1 j} k_{1}+a_{2 j} k_{2}+\\\\cdots+a_{n j} k_{n}\\\\right) u_{j}\\n\\\\end{aligned}\\n$$\\n\\nThus, $[T(v)]_{S}$ is the column vector whose $j$ th entry is\\n\\n\\n\\\\begin{equation*}\\na_{1 j} k_{1}+a_{2 j} k_{2}+\\\\cdots+a_{n j} k_{n} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nOn the other hand, the $j$ th entry of $[T]_{S}[v]_{S}$ is obtained by multiplying the $j$ th row of $[T]_{S}$ by $[v]_{S}$ - that is (1) by (2). But the product of (1) and (2) is (3). Hence, $[T]_{S}[v]_{S}$ and $[T(v)]_{S}$ have the same entries. Thus, $[T]_{S}[v]_{S}=[T(v)]_{S}$.\\n',\n",
       " '\\n6.10. Prove Theorem 6.2: Let $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ be a basis for $V$ over $K$, and let $\\\\mathbf{M}$ be the algebra of $n$-square matrices over $K$. Then the mapping $m: A(V) \\\\rightarrow \\\\mathbf{M}$ defined by $m(T)=[T]_{S}$ is a vector space isomorphism. That is, for any $F, G \\\\in A(V)$ and any $k \\\\in K$, we have\\\\\\\\\\n(i) $[F+G]=[F]+[G]$,\\\\\\\\\\n(ii) $[k F]=k[F]$,\\\\\\\\\\n(iii) $m$ is one-to-one and onto.\\n\\n(i) Suppose, for $i=1, \\\\ldots, n$,\\n\\n$$\\nF\\\\left(u_{i}\\\\right)=\\\\sum_{j=1}^{n} a_{i j} u_{j} \\\\quad \\\\text { and } \\\\quad G\\\\left(u_{i}\\\\right)=\\\\sum_{j=1}^{n} b_{i j} u_{j}\\n$$\\n\\nConsider the matrices $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$. Then $[F]=A^{T}$ and $[G]=B^{T}$. We have, for $i=1, \\\\ldots, n$,\\n\\n$$\\n(F+G)\\\\left(u_{i}\\\\right)=F\\\\left(u_{i}\\\\right)+G\\\\left(u_{i}\\\\right)=\\\\sum_{j=1}^{n}\\\\left(a_{i j}+b_{i j}\\\\right) u_{j}\\n$$\\n\\nBecause $A+B$ is the matrix $\\\\left(a_{i j}+b_{i j}\\\\right)$, we have\\n\\n(ii) Also, for $i=1, \\\\ldots, n$,\\n\\n$$\\n[F+G]=(A+B)^{T}=A^{T}+B^{T}=[F]+[G]\\n$$\\n\\n$$\\n(k F)\\\\left(u_{i}\\\\right)=k F\\\\left(u_{i}\\\\right)=k \\\\sum_{j=1}^{n} a_{i j} u_{j}=\\\\sum_{j=1}^{n}\\\\left(k a_{i j}\\\\right) u_{j}\\n$$\\n\\nBecause $k A$ is the matrix $\\\\left(k a_{i j}\\\\right)$, we have\\n\\n$$\\n[k F]=(k A)^{T}=k A^{T}=k[F]\\n$$\\n\\n(iii) Finally, $m$ is one-to-one, because a linear mapping is completely determined by its values on a basis. Also, $m$ is onto, because matrix $A=\\\\left[a_{i j}\\\\right]$ in $\\\\mathbf{M}$ is the image of the linear operator,\\n\\n$$\\nF\\\\left(u_{i}\\\\right)=\\\\sum_{j=1}^{n} a_{i j} u_{j}, \\\\quad i=1, \\\\ldots, n\\n$$\\n\\nThus, the theorem is proved.\\n',\n",
       " '\\n6.11. Prove Theorem 6.3: For any linear operators $G, F \\\\in A(V),[G \\\\circ F]=[G][F]$.\\n\\nUsing the notation in Problem 6.10, we have\\n\\n$$\\n\\\\begin{aligned}\\n(G \\\\circ F)\\\\left(u_{i}\\\\right) & =G\\\\left(F\\\\left(u_{i}\\\\right)\\\\right)=G\\\\left(\\\\sum_{j=1}^{n} a_{i j} u_{j}\\\\right)=\\\\sum_{j=1}^{n} a_{i j} G\\\\left(u_{j}\\\\right) \\\\\\\\\\n& =\\\\sum_{j=1}^{n} a_{i j}\\\\left(\\\\sum_{k=1}^{n} b_{j k} u_{k}\\\\right)=\\\\sum_{k=1}^{n}\\\\left(\\\\sum_{j=1}^{n} a_{i j} b_{j k}\\\\right) u_{k}\\n\\\\end{aligned}\\n$$\\n\\nRecall that $A B$ is the matrix $A B=\\\\left[c_{i k}\\\\right]$, where $c_{i k}=\\\\sum_{j=1}^{n} a_{i j} b_{j k}$. Accordingly,\\n\\n$$\\n[G \\\\circ F]=(A B)^{T}=B^{T} A^{T}=[G][F]\\n$$\\n\\nThe theorem is proved.\\n',\n",
       " '\\n6.12. Let $A$ be the matrix representation of a linear operator $T$. Prove that, for any polynomial $f(t)$, we have that $f(A)$ is the matrix representation of $f(T)$. [Thus, $f(T)=0$ if and only if $f(A)=0$.]\\n\\nLet $\\\\phi$ be the mapping that sends an operator $T$ into its matrix representation $A$. We need to prove that $\\\\phi(f(T))=f(A)$. Suppose $f(t)=a_{n} t^{n}+\\\\cdots+a_{1} t+a_{0}$. The proof is by induction on $n$, the degree of $f(t)$.\\n\\nSuppose $n=0$. Recall that $\\\\phi\\\\left(I^{\\\\prime}\\\\right)=I$, where $I^{\\\\prime}$ is the identity mapping and $I$ is the identity matrix. Thus,\\n\\n$$\\n\\\\phi(f(T))=\\\\phi\\\\left(a_{0} I^{\\\\prime}\\\\right)=a_{0} \\\\phi\\\\left(I^{\\\\prime}\\\\right)=a_{0} I=f(A)\\n$$\\n\\nand so the theorem holds for $n=0$.\\n\\nNow assume the theorem holds for polynomials of degree less than $n$. Then, because $\\\\phi$ is an algebra isomorphism,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\phi(f(T)) & =\\\\phi\\\\left(a_{n} T^{n}+a_{n-1} T^{n-1}+\\\\cdots+a_{1} T+a_{0} I^{\\\\prime}\\\\right) \\\\\\\\\\n& =a_{n} \\\\phi(T) \\\\phi\\\\left(T^{n-1}\\\\right)+\\\\phi\\\\left(a_{n-1} T^{n-1}+\\\\cdots+a_{1} T+a_{0} I^{\\\\prime}\\\\right) \\\\\\\\\\n& =a_{n} A A^{n-1}+\\\\left(a_{n-1} A^{n-1}+\\\\cdots+a_{1} A+a_{0} I\\\\right)=f(A)\\n\\\\end{aligned}\\n$$\\n\\nand the theorem is proved.\\n\\n\\n\\\\section*{Change of Basis}\\nThe coordinate vector $[v]_{S}$ in this section will always denote a column vector; that is,\\n\\n$$\\n[v]_{S}=\\\\left[a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right]^{T}\\n$$\\n',\n",
       " '\\n6.13. Consider the following bases of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nE=\\\\left\\\\{e_{1}, e_{2}\\\\right\\\\}=\\\\{(1,0),(0,1)\\\\} \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,3),(1,4)\\\\}\\n$$\\n\\n(a) Find the change-of-basis matrix $P$ from the usual basis $E$ to $S$.\\n\\n(b) Find the change-of-basis matrix $Q$ from $S$ back to $E$.\\n\\n(c) Find the coordinate vector $[v]$ of $v=(5,-3)$ relative to $S$.\\n\\n(a) Because $E$ is the usual basis, simply write the basis vectors in $S$ as columns: $P=\\\\left[\\\\begin{array}{ll}1 & 1 \\\\\\\\ 3 & 4\\\\end{array}\\\\right]$\\n\\n(b) Method 1. Use the definition of the change-of-basis matrix. That is, express each vector in $E$ as a linear combination of the vectors in $S$. We do this by first finding the coordinates of an arbitrary vector $v=(a, b)$ relative to $S$. We have\\n\\n$$\\n(a, b)=x(1,3)+y(1,4)=(x+y, 3 x+4 y) \\\\quad \\\\text { or } \\\\quad \\\\begin{aligned}\\nx+y & =a \\\\\\\\\\n3 x+4 y & =b\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x$ and $y$ to obtain $x=4 a-b, y=-3 a+b$. Thus,\\n\\n$$\\nv=(4 a-b) u_{1}+(-3 a+b) u_{2} \\\\quad \\\\text { and } \\\\quad[v]_{S}=[(a, b)]_{S}=[4 a-b,-3 a+b]^{T}\\n$$\\n\\nUsing the above formula for $[v]_{S}$ and writing the coordinates of the $e_{i}$ as columns yields\\n\\n$$\\n\\\\begin{aligned}\\n& e_{1}=(1,0)=4 u_{1}-3 u_{2} \\\\\\\\\\n& e_{2}=(0,1)=-u_{1}+u_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and } \\\\quad Q=\\\\left[\\\\begin{array}{rr}\\n4 & -1 \\\\\\\\\\n-3 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nMethod 2. Because $Q=P^{-1}$, find $P^{-1}$, say by using the formula for the inverse of a $2 \\\\times 2$ matrix. Thus,\\n\\n$$\\nP^{-1}=\\\\left[\\\\begin{array}{rr}\\n4 & -1 \\\\\\\\\\n-3 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Method 1. Write $v$ as a linear combination of the vectors in $S$, say by using the above formula for $v=(a, b)$. We have $v=(5,-3)=23 u_{1}-18 u_{2}$, and so $[v]_{S}=[23,-18]^{T}$.\\n\\nMethod 2. Use, from Theorem 6.6, the fact that $[v]_{S}=P^{-1}[v]_{E}$ and the fact that $[v]_{E}=[5,-3]^{T}$ :\\n\\n$$\\n[v]_{S}=P^{-1}[v]_{E}=\\\\left[\\\\begin{array}{rr}\\n4 & -1 \\\\\\\\\\n-3 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n5 \\\\\\\\\\n-3\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\n23 \\\\\\\\\\n-18\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.14. The vectors $u_{1}=(1,2,0), u_{2}=(1,3,2), u_{3}=(0,1,3)$ form a basis $S$ of $\\\\mathbf{R}^{3}$. Find\\n\\n(a) The change-of-basis matrix $P$ from the usual basis $E=\\\\left\\\\{e_{1}, e_{2}, e_{3}\\\\right\\\\}$ to $S$.\\n\\n(b) The change-of-basis matrix $Q$ from $S$ back to $E$.\\n\\n(a) Because $E$ is the usual basis, simply write the basis vectors of $S$ as columns: $P=\\\\left[\\\\begin{array}{lll}1 & 1 & 0 \\\\\\\\ 2 & 3 & 1 \\\\\\\\ 0 & 2 & 3\\\\end{array}\\\\right]$\\n\\n(b) Method 1. Express each basis vector of $E$ as a linear combination of the basis vectors of $S$ by first finding the coordinates of an arbitrary vector $v=(a, b, c)$ relative to the basis $S$. We have\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb \\\\\\\\\\nc\\n\\\\end{array}\\\\right]=x\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n0\\n\\\\end{array}\\\\right]+y\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n3 \\\\\\\\\\n2\\n\\\\end{array}\\\\right]+z\\\\left[\\\\begin{array}{l}\\n0 \\\\\\\\\\n1 \\\\\\\\\\n3\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\begin{array}{rr}\\nx+y & =a \\\\\\\\\\n2 x+3 y+z & =b \\\\\\\\\\n2 y+3 z & =c\\n\\\\end{array}\\n$$\\n\\nSolve for $x, y, z$ to get $x=7 a-3 b+c, y=-6 a+3 b-c, z=4 a-2 b+c$. Thus,\\n\\nor\\n\\n$$\\n\\\\begin{aligned}\\nv=(a, b, c) & =(7 a-3 b+c) u_{1}+(-6 a+3 b-c) u_{2}+(4 a-2 b+c) u_{3} \\\\\\\\\\n{[v]_{S}=[(a, b, c)]_{S} } & =[7 a-3 b+c,-6 a+3 b-c, 4 a-2 b+c]^{T}\\n\\\\end{aligned}\\n$$\\n\\nUsing the above formula for $[v]_{S}$ and then writing the coordinates of the $e_{i}$ as columns yields\\n\\n$$\\n\\\\begin{aligned}\\n& e_{1}=(1,0,0)=7 u_{1}-6 u_{2}+4 u_{3} \\\\\\\\\\n& e_{2}=(0,1,0)=-3 u_{1}+3 u_{2}-2 u_{3} \\\\\\\\\\n& e_{3}=(0,0,1)=u_{1}-u_{2}+u_{3}\\n\\\\end{aligned} \\\\quad \\\\text { and } \\\\quad Q=\\\\left[\\\\begin{array}{rrr}\\n7 & -3 & 1 \\\\\\\\\\n-6 & 3 & -1 \\\\\\\\\\n4 & -2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nMethod 2. Find $P^{-1}$ by row reducing $M=[P, I]$ to the form $\\\\left[I, P^{-1}\\\\right]$ :\\n\\n$$\\n\\\\begin{aligned}\\nM & =\\\\left[\\\\begin{array}{lll:lll}\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\\\\\\n2 & 3 & 1 & 0 & 1 & 0 \\\\\\\\\\n0 & 2 & 3 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll:rrr}\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\\\\\\n0 & 2 & 3 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\\\\\\n0 & 0 & 1 & 4 & -2 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 0 & 0 & 7 & -3 & 1 \\\\\\\\\\n0 & 1 & 0 & -6 & 3 & -1 \\\\\\\\\\n0 & 0 & 1 & 4 & -2 & 1\\n\\\\end{array}\\\\right]=\\\\left[I, P^{-1}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nThus, $Q=P^{-1}=\\\\left[\\\\begin{array}{rrr}7 & -3 & 1 \\\\\\\\ -6 & 3 & -1 \\\\\\\\ 4 & -2 & 1\\\\end{array}\\\\right]$.\\n',\n",
       " '\\n6.15. Suppose the $x$-axis and $y$-axis in the plane $\\\\mathbf{R}^{2}$ are rotated counterclockwise $45^{\\\\circ}$ so that the new $x^{\\\\prime}$-axis and $y^{\\\\prime}$-axis are along the line $y=x$ and the line $y=-x$, respectively.\\n\\n(a) Find the change-of-basis matrix $P$.\\n\\n(b) Find the coordinates of the point $A(5,6)$ under the given rotation.\\n\\n(a) The unit vectors in the direction of the new $x^{\\\\prime}$ - and $y^{\\\\prime}$-axes are\\n\\n$$\\nu_{1}=\\\\left(\\\\frac{1}{2} \\\\sqrt{2}, \\\\frac{1}{2} \\\\sqrt{2}\\\\right) \\\\quad \\\\text { and } \\\\quad u_{2}=\\\\left(-\\\\frac{1}{2} \\\\sqrt{2}, \\\\frac{1}{2} \\\\sqrt{2}\\\\right)\\n$$\\n\\n(The unit vectors in the direction of the original $x$ and $y$ axes are the usual basis of $\\\\mathbf{R}^{2}$.) Thus, write the coordinates of $u_{1}$ and $u_{2}$ as columns to obtain\\n\\n$$\\nP=\\\\left[\\\\begin{array}{cc}\\n\\\\frac{1}{2} \\\\sqrt{2} & -\\\\frac{1}{2} \\\\sqrt{2} \\\\\\\\\\n\\\\frac{1}{2} \\\\sqrt{2} & \\\\frac{1}{2} \\\\sqrt{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Multiply the coordinates of the point by $P^{-1}$ :\\n\\n$$\\n\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{2} \\\\sqrt{2} & \\\\frac{1}{2} \\\\sqrt{2} \\\\\\\\\\n-\\\\frac{1}{2} \\\\sqrt{2} & \\\\frac{1}{2} \\\\sqrt{2}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n5 \\\\\\\\\\n6\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n\\\\frac{11}{2} \\\\sqrt{2} \\\\\\\\\\n\\\\frac{1}{2} \\\\sqrt{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(Because $P$ is orthogonal, $P^{-1}$ is simply the transpose of $P$.)\\n',\n",
       " '\\n6.16. The vectors $u_{1}=(1,1,0), u_{2}=(0,1,1), u_{3}=(1,2,2)$ form a basis $S$ of $\\\\mathbf{R}^{3}$. Find the coordinates of an arbitrary vector $v=(a, b, c)$ relative to the basis $S$.\\n\\nMethod 1. Express $v$ as a linear combination of $u_{1}, u_{2}, u_{3}$ using unknowns $x, y, z$. We have\\n\\n$$\\n(a, b, c)=x(1,1,0)+y(0,1,1)+z(1,2,2)=(x+z, x+y+2 z, y+2 z)\\n$$\\n\\nthis yields the system\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-220(1)}\\n\\\\end{center}\\n\\n$$\\n\\\\begin{aligned}\\n& y+2 z=c \\\\quad y+2 z=c \\\\quad z=a-b+c\\n\\\\end{aligned}\\n$$\\n\\nSolving by back-substitution yields $x=b-c, y=-2 a+2 b-c, z=a-b+c$. Thus,\\n\\n$$\\n[v]_{S}=[b-c,-2 a+2 b-c, a-b+c]^{T}\\n$$\\n\\nMethod 2. Find $P^{-1}$ by row reducing $M=[P, I]$ to the form $\\\\left[I, P^{-1}\\\\right]$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$ or, in other words, the matrix whose columns are the basis vectors of $S$.\\n\\nWe have\\n\\n$$\\n\\\\begin{aligned}\\nM & =\\\\left[\\\\begin{array}{lll:lll}\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\\\\\\n1 & 1 & 2 & 0 & 1 & 0 \\\\\\\\\\n0 & 1 & 2 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & -1 & 1 & 0 \\\\\\\\\\n0 & 1 & 2 & 0 & 0 & 1\\n\\\\end{array}\\\\right] \\\\\\\\\\n& \\\\sim\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\\\\\\n0 & 1 & 1 & -1 & 1 & 0 \\\\\\\\\\n0 & 0 & 1 & 1 & -1 & 1\\n\\\\end{array}\\\\right] \\\\sim\\\\left[\\\\begin{array}{lll:rrr}\\n1 & 0 & 0 & 0 & 1 & -1 \\\\\\\\\\n0 & 1 & 0 & -2 & 2 & -1 \\\\\\\\\\n0 & 0 & 1 & 1 & -1 & 1\\n\\\\end{array}\\\\right]=\\\\left[I, P^{-1}\\\\right] \\\\\\\\\\n\\\\text { Thus, } \\\\quad P^{-1} & =\\\\left[\\\\begin{array}{rrrr}\\n0 & 1 & -1 \\\\\\\\\\n-2 & 2 & -1 \\\\\\\\\\n1 & -1 & 1\\n\\\\end{array}\\\\right] \\\\text { and }[v]_{S}=P^{-1}[v]_{E}=\\\\left[\\\\begin{array}{rrr}\\n0 & 1 & -1 \\\\\\\\\\n-2 & 2 & -1 \\\\\\\\\\n1 & -1 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\na \\\\\\\\\\nb \\\\\\\\\\nc\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{c}\\nb-c \\\\\\\\\\n-2 a+2 b-c \\\\\\\\\\na-b+c\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n6.17. Consider the following bases of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nS=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,-2),(3,-4)\\\\} \\\\quad \\\\text { and } \\\\quad S^{\\\\prime}=\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}=\\\\{(1,3),(3,8)\\\\}\\n$$\\n\\n(a) Find the coordinates of $v=(a, b)$ relative to the basis $S$.\\n\\n(b) Find the change-of-basis matrix $P$ from $S$ to $S^{\\\\prime}$.\\n\\n(c) Find the coordinates of $v=(a, b)$ relative to the basis $S^{\\\\prime}$.\\n\\n(d) Find the change-of-basis matrix $Q$ from $S^{\\\\prime}$ back to $S$.\\n\\n(e) Verify $Q=P^{-1}$.\\n\\n(f) Show that, for any vector $v=(a, b)$ in $\\\\mathbf{R}^{2}, P^{-1}[v]_{S}=[v]_{S^{\\\\prime}}$. (See Theorem 6.6.)\\n\\n(a) Let $v=x u_{1}+y u_{2}$ for unknowns $x$ and $y$; that is,\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-220}\\n\\\\end{center}\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-2 a-\\\\frac{3}{2} b$ and $y=a+\\\\frac{1}{2} b$. Thus,\\n\\n$$\\n(a, b)=\\\\left(-2 a-\\\\frac{3}{2}\\\\right) u_{1}+\\\\left(a+\\\\frac{1}{2} b\\\\right) u_{2} \\\\quad \\\\text { or } \\\\quad[(a, b)]_{S}=\\\\left[-2 a-\\\\frac{3}{2} b, a+\\\\frac{1}{2} b\\\\right]^{T}\\n$$\\n\\n(b) Use part (a) to write each of the basis vectors $v_{1}$ and $v_{2}$ of $S^{\\\\prime}$ as a linear combination of the basis vectors $u_{1}$ and $u_{2}$ of $S$; that is,\\n\\n$$\\n\\\\begin{aligned}\\n& v_{1}=(1,3)=\\\\left(-2-\\\\frac{9}{2}\\\\right) u_{1}+\\\\left(1+\\\\frac{3}{2}\\\\right) u_{2}=-\\\\frac{13}{2} u_{1}+\\\\frac{5}{2} u_{2} \\\\\\\\\\n& v_{2}=(3,8)=(-6-12) u_{1}+(3+4) u_{2}=-18 u_{1}+7 u_{2}\\n\\\\end{aligned}\\n$$\\n\\nThen $P$ is the matrix whose columns are the coordinates of $v_{1}$ and $v_{2}$ relative to the basis $S$; that is,\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rr}\\n-\\\\frac{13}{2} & -18 \\\\\\\\\\n\\\\frac{5}{2} & 7\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Let $v=x v_{1}+y v_{2}$ for unknown scalars $x$ and $y$ :\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-221}\\n\\\\end{center}\\n\\nSolve for $x$ and $y$ to get $x=-8 a+3 b$ and $y=3 a-b$. Thus,\\n\\n$$\\n(a, b)=(-8 a+3 b) v_{1}+(3 a-b) v_{2} \\\\quad \\\\text { or } \\\\quad[(a, b)]_{S^{\\\\prime}}=[-8 a+3 b, \\\\quad 3 a-b]^{T}\\n$$\\n\\n(d) Use part (c) to express each of the basis vectors $u_{1}$ and $u_{2}$ of $S$ as a linear combination of the basis vectors $v_{1}$ and $v_{2}$ of $S^{\\\\prime}$ :\\n\\n$$\\n\\\\begin{aligned}\\n& u_{1}=(1,-2)=(-8-6) v_{1}+(3+2) v_{2}=-14 v_{1}+5 v_{2} \\\\\\\\\\n& u_{2}=(3,-4)=(-24-12) v_{1}+(9+4) v_{2}=-36 v_{1}+13 v_{2}\\n\\\\end{aligned}\\n$$\\n\\nWrite the coordinates of $u_{1}$ and $u_{2}$ relative to $S^{\\\\prime}$ as columns to obtain $Q=\\\\left[\\\\begin{array}{rr}-14 & -36 \\\\\\\\ 5 & 13\\\\end{array}\\\\right]$.\\n\\n(e) $Q P=\\\\left[\\\\begin{array}{rr}-14 & -36 \\\\\\\\ 5 & 13\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}-\\\\frac{13}{2} & -18 \\\\\\\\ \\\\frac{5}{2} & 7\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}1 & 0 \\\\\\\\ 0 & 1\\\\end{array}\\\\right]=I$\\n\\n(f) Use parts (a), (c), and (d) to obtain\\n\\n$$\\nP^{-1}[v]_{S}=Q[v]_{S}=\\\\left[\\\\begin{array}{rr}\\n-14 & -36 \\\\\\\\\\n5 & 13\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}\\n-2 a-\\\\frac{3}{2} b \\\\\\\\\\na+\\\\frac{1}{2} b\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{c}\\n-8 a+3 b \\\\\\\\\\n3 a-b\\n\\\\end{array}\\\\right]=[v]_{S^{\\\\prime}}\\n$$\\n',\n",
       " '\\n6.18. Suppose $P$ is the change-of-basis matrix from a basis $\\\\left\\\\{u_{i}\\\\right\\\\}$ to a basis $\\\\left\\\\{w_{i}\\\\right\\\\}$, and suppose $Q$ is the change-of-basis matrix from the basis $\\\\left\\\\{w_{i}\\\\right\\\\}$ back to $\\\\left\\\\{u_{i}\\\\right\\\\}$. Prove that $P$ is invertible and that $Q=P^{-1}$.\\n\\nSuppose, for $i=1,2, \\\\ldots, n$, that\\n\\n\\n\\\\begin{equation*}\\nw_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\\\\ldots+a_{i n} u_{n}=\\\\sum_{j=1}^{n} a_{i j} u_{j} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nand, for $j=1,2, \\\\ldots, n$,\\n\\n\\n\\\\begin{equation*}\\nu_{j}=b_{j 1} w_{1}+b_{j 2} w_{2}+\\\\cdots+b_{j n} w_{n}=\\\\sum_{k=1}^{n} b_{j k} w_{k} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nLet $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{j k}\\\\right]$. Then $P=A^{T}$ and $Q=B^{T}$. Substituting (2) into (1) yields\\n\\n$$\\nw_{i}=\\\\sum_{j=1}^{n} a_{i j}\\\\left(\\\\sum_{k=1}^{n} b_{j k} w_{k}\\\\right)=\\\\sum_{k=1}^{n}\\\\left(\\\\sum_{j=1}^{n} a_{i j} b_{j k}\\\\right) w_{k}\\n$$\\n\\nBecause $\\\\left\\\\{w_{i}\\\\right\\\\}$ is a basis, $\\\\sum a_{i j} b_{j k}=\\\\delta_{i k}$, where $\\\\delta_{i k}$ is the Kronecker delta; that is, $\\\\delta_{i k}=1$ if $i=k$ but $\\\\delta_{i k}=0$ if $i \\\\neq k$. Suppose $A B=\\\\left[c_{i k}\\\\right]$. Then $c_{i k}=\\\\delta_{i k}$. Accordingly, $A B=I$, and so\\n\\n$$\\nQ P=B^{T} A^{T}=(A B)^{T}=I^{T}=I\\n$$\\n\\nThus, $Q=P^{-1}$.\\n',\n",
       " '\\n6.19. Consider a finite sequence of vectors $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$. Let $S^{\\\\prime}$ be the sequence of vectors obtained from $S$ by one of the following \"elementary operations\":\\n\\n(1) Interchange two vectors.\\n\\n(2) Multiply a vector by a nonzero scalar.\\n\\n(3) Add a multiple of one vector to another vector.\\n\\nShow that $S$ and $S^{\\\\prime}$ span the same subspace $W$. Also, show that $S^{\\\\prime}$ is linearly independent if and only if $S$ is linearly independent.\\n\\nObserve that, for each operation, the vectors $S^{\\\\prime}$ are linear combinations of vectors in $S$. Also, because each operation has an inverse of the same type, each vector in $S$ is a linear combination of vectors in $S^{\\\\prime}$. Thus, $S$ and $S^{\\\\prime}$ span the same subspace $W$. Moreover, $S^{\\\\prime}$ is linearly independent if and only if $\\\\operatorname{dim} W=n$, and this is true if and only if $S$ is linearly independent.\\n',\n",
       " '\\n6.20. Let $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$ be row equivalent $m \\\\times n$ matrices over a field $K$, and let $v_{1}, v_{2}, \\\\ldots, v_{n}$ be any vectors in a vector space $V$ over $K$. For $i=1,2, \\\\ldots, m$, let $u_{i}$ and $w_{i}$ be defined by\\n\\n$$\\nu_{i}=a_{i 1} v_{1}+a_{i 2} v_{2}+\\\\cdots+a_{i n} v_{n} \\\\quad \\\\text { and } \\\\quad w_{i}=b_{i 1} v_{1}+b_{i 2} v_{2}+\\\\cdots+b_{i n} v_{n}\\n$$\\n\\nShow that $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{w_{i}\\\\right\\\\}$ span the same subspace of $V$.\\n\\nApplying an \"elementary operation\" of Problem 6.19 to $\\\\left\\\\{u_{i}\\\\right\\\\}$ is equivalent to applying an elementary row operation to the matrix $A$. Because $A$ and $B$ are row equivalent, $B$ can be obtained from $A$ by a sequence of elementary row operations. Hence, $\\\\left\\\\{w_{i}\\\\right\\\\}$ can be obtained from $\\\\left\\\\{u_{i}\\\\right\\\\}$ by the corresponding sequence of operations. Accordingly, $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{w_{i}\\\\right\\\\}$ span the same space.\\n',\n",
       " '\\n6.21. Suppose $u_{1}, u_{2}, \\\\ldots, u_{n}$ belong to a vector space $V$ over a field $K$, and suppose $P=\\\\left[a_{i j}\\\\right]$ is an $n$-square matrix over $K$. For $i=1,2, \\\\ldots, n$, let $v_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\\\\cdots+a_{i n} u_{n}$.\\n\\n(a) Suppose $P$ is invertible. Show that $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{v_{i}\\\\right\\\\}$ span the same subspace of $V$. Hence, $\\\\left\\\\{u_{i}\\\\right\\\\}$ is linearly independent if and only if $\\\\left\\\\{v_{i}\\\\right\\\\}$ is linearly independent.\\n\\n(b) Suppose $P$ is singular (not invertible). Show that $\\\\left\\\\{v_{i}\\\\right\\\\}$ is linearly dependent.\\n\\n(c) Suppose $\\\\left\\\\{v_{i}\\\\right\\\\}$ is linearly independent. Show that $P$ is invertible.\\n\\n(a) Because $P$ is invertible, it is row equivalent to the identity matrix $I$. Hence, by Problem 6.19, $\\\\left\\\\{v_{i}\\\\right\\\\}$ and $\\\\left\\\\{u_{i}\\\\right\\\\}$ span the same subspace of $V$. Thus, one is linearly independent if and only if the other is linearly independent.\\n\\n(b) Because $P$ is not invertible, it is row equivalent to a matrix with a zero row. This means $\\\\left\\\\{v_{i}\\\\right\\\\}$ spans a substance that has a spanning set with less than $n$ elements. Thus, $\\\\left\\\\{v_{i}\\\\right\\\\}$ is linearly dependent.\\n\\n(c) This is the contrapositive of the statement of part (b), and so it follows from part (b).\\n',\n",
       " '\\n6.22. Prove Theorem 6.6: Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\\\\prime}$ in a vector space $V$. Then, for any vector $v \\\\in V$, we have $P[v]_{S^{\\\\prime}}=[v]_{S}$, and hence, $P^{-1}[v]_{S}=[v]_{S^{\\\\prime}}$.\\n\\nSuppose $S=\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ and $S^{\\\\prime}=\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$, and suppose, for $i=1, \\\\ldots, n$,\\n\\n$$\\nw_{i}=a_{i 1} u_{1}+a_{i 2} u_{2}+\\\\cdots+a_{i n} u_{n}=\\\\sum_{j=1}^{n} a_{i j} u_{j}\\n$$\\n\\nThen $P$ is the $n$-square matrix whose $j$ th row is\\n\\n\\n\\\\begin{equation*}\\n\\\\left(a_{1 j}, a_{2 j}, \\\\ldots, a_{n j}\\\\right) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nAlso suppose $v=k_{1} w_{1}+k_{2} w_{2}+\\\\cdots+k_{n} w_{n}=\\\\sum_{i=1}^{n} k_{i} w_{i}$. Then\\n\\n\\n\\\\begin{equation*}\\n[v]_{S^{\\\\prime}}=\\\\left[k_{1}, k_{2}, \\\\ldots, k_{n}\\\\right]^{T} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nSubstituting for $w_{i}$ in the equation for $v$, we obtain\\n\\n$$\\n\\\\begin{aligned}\\nv & =\\\\sum_{i=1}^{n} k_{i} w_{i}=\\\\sum_{i=1}^{n} k_{i}\\\\left(\\\\sum_{j=1}^{n} a_{i j} u_{j}\\\\right)=\\\\sum_{j=1}^{n}\\\\left(\\\\sum_{i=1}^{n} a_{i j} k_{i}\\\\right) u_{j} \\\\\\\\\\n& =\\\\sum_{j=1}^{n}\\\\left(a_{1 j} k_{1}+a_{2 j} k_{2}+\\\\cdots+a_{n j} k_{n}\\\\right) u_{j}\\n\\\\end{aligned}\\n$$\\n\\nAccordingly, $[v]_{S}$ is the column vector whose $j$ th entry is\\n\\n\\n\\\\begin{equation*}\\na_{1 j} k_{1}+a_{2 j} k_{2}+\\\\cdots+a_{n j} k_{n} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nOn the other hand, the $j$ th entry of $P[v]_{S^{\\\\prime}}$ is obtained by multiplying the $j$ th row of $P$ by $[v]_{S^{\\\\prime}}$-that is, (1) by (2). However, the product of (1) and (2) is (3). Hence, $P[v]_{S^{\\\\prime}}$ and $[v]_{S}$ have the same entries. Thus, $P[v]_{S^{\\\\prime}}=[v]_{S^{\\\\prime}}$, as claimed.\\n\\nFurthermore, multiplying the above by $P^{-1}$ gives $P^{-1}[v]_{S}=P^{-1} P[v]_{S^{\\\\prime}}=[v]_{S^{\\\\prime}}$.\\n\\n\\n\\\\section*{Linear Operators and Change of Basis}\\n',\n",
       " '6.23. Consider the linear transformation $F$ on $\\\\mathbf{R}^{2}$ defined by $F(x, y)=(5 x-y, 2 x+y)$ and the following bases of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nE=\\\\left\\\\{e_{1}, e_{2}\\\\right\\\\}=\\\\{(1,0),(0,1)\\\\} \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,4),(2,7)\\\\}\\n$$\\n\\n(a) Find the change-of-basis matrix $P$ from $E$ to $S$ and the change-of-basis matrix $Q$ from $S$ back to $E$.\\n\\n(b) Find the matrix $A$ that represents $F$ in the basis $E$.\\n\\n(c) Find the matrix $B$ that represents $F$ in the basis $S$.\\n\\n(a) Because $E$ is the usual basis, simply write the vectors in $S$ as columns to obtain the change-of-basis matrix $P$. Recall, also, that $Q=P^{-1}$. Thus,\\n\\n$$\\nP=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n4 & 7\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad Q=P^{-1}=\\\\left[\\\\begin{array}{rr}\\n-7 & 2 \\\\\\\\\\n4 & -1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) Write the coefficients of $x$ and $y$ in $F(x, y)=(5 x-y, 2 x+y)$ as rows to get\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n5 & -1 \\\\\\\\\\n2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Method 1. Find the coordinates of $F\\\\left(u_{1}\\\\right)$ and $F\\\\left(u_{2}\\\\right)$ relative to the basis $S$. This may be done by first finding the coordinates of an arbitrary vector $(a, b)$ in $\\\\mathbf{R}^{2}$ relative to the basis $S$. We have\\n\\n$$\\n(a, b)=x(1,4)+y(2,7)=(x+2 y, 4 x+7 y), \\\\quad \\\\text { and so } \\\\quad \\\\begin{aligned}\\nx+2 y & =a \\\\\\\\\\n4 x+7 y & =b\\n\\\\end{aligned}\\n$$\\n\\nSolve for $x$ and $y$ in terms of $a$ and $b$ to get $x=-7 a+2 b, y=4 a-b$. Then\\n\\n$$\\n(a, b)=(-7 a+2 b) u_{1}+(4 a-b) u_{2}\\n$$\\n\\nNow use the formula for $(a, b)$ to obtain\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(u_{1}\\\\right)=F(1,4)=(1,6)=5 u_{1}-2 u_{2} \\\\\\\\\\n& F\\\\left(u_{2}\\\\right)=F(2,7)=(3,11)=u_{1}+u_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad B=\\\\left[\\\\begin{array}{rr}\\n5 & 1 \\\\\\\\\\n-2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nMethod 2. By Theorem 6.7, $B=P^{-1} A P$. Thus,\\n\\n$$\\nB=P^{-1} A P=\\\\left[\\\\begin{array}{rr}\\n-7 & 2 \\\\\\\\\\n4 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n5 & -1 \\\\\\\\\\n2 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n4 & 7\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n5 & 1 \\\\\\\\\\n-2 & 1\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.24. Let $A=\\\\left[\\\\begin{array}{rr}2 & 3 \\\\\\\\ 4 & -1\\\\end{array}\\\\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to the basis $S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\left\\\\{[1,3]^{T},[2,5]^{T}\\\\right\\\\} .\\\\left[\\\\right.$ Recall $A$ defines a linear operator $A: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ relative to the usual basis $E$ of $\\\\mathbf{R}^{2}$ ].\\n\\nMethod 1. Find the coordinates of $A\\\\left(u_{1}\\\\right)$ and $A\\\\left(u_{2}\\\\right)$ relative to the basis $S$ by first finding the coordinates of an arbitrary vector $[a, b]^{T}$ in $\\\\mathbf{R}^{2}$ relative to the basis $S$. By Problem 6.2,\\n\\n$$\\n[a, b]^{T}=(-5 a+2 b) u_{1}+(3 a-b) u_{2}\\n$$\\n\\nUsing the formula for $[a, b]^{T}$, we obtain\\n\\nand\\n\\n$$\\n\\\\begin{gathered}\\nA\\\\left(u_{1}\\\\right)=\\\\left[\\\\begin{array}{rr}\\n2 & 3 \\\\\\\\\\n4 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\n11 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=-53 u_{1}+32 u_{2} \\\\\\\\\\nA\\\\left(u_{2}\\\\right)=\\\\left[\\\\begin{array}{rr}\\n2 & 3 \\\\\\\\\\n4 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n2 \\\\\\\\\\n5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\n19 \\\\\\\\\\n3\\n\\\\end{array}\\\\right]=-89 u_{1}+54 u_{2} \\\\\\\\\\nB=\\\\left[\\\\begin{array}{rr}\\n-53 & -89 \\\\\\\\\\n32 & 54\\n\\\\end{array}\\\\right]\\n\\\\end{gathered}\\n$$\\n\\nThus,\\n\\nMethod 2. Use $B=P^{-1} A P$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$. Thus, simply write the vectors in $S$ (as columns) to obtain the change-of-basis matrix $P$ and then use the formula\\\\\\\\\\nfor $P^{-1}$. This gives\\n\\n$$\\n\\\\begin{gathered}\\nP=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right] \\\\text { and } P^{-1}=\\\\left[\\\\begin{array}{rr}\\n-5 & 2 \\\\\\\\\\n3 & -1\\n\\\\end{array}\\\\right] \\\\\\\\\\n\\\\text { Then } \\\\quad B=P^{-1} A P=\\\\left[\\\\begin{array}{lr}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n2 & 3 \\\\\\\\\\n4 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n-5 & 2 \\\\\\\\\\n3 & -1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n-53 & -89 \\\\\\\\\\n32 & 54\\n\\\\end{array}\\\\right]\\n\\\\end{gathered}\\n$$\\n',\n",
       " '\\n6.25. Let $A=\\\\left[\\\\begin{array}{rrr}1 & 3 & 1 \\\\\\\\ 2 & 5 & -4 \\\\\\\\ 1 & -2 & 2\\\\end{array}\\\\right]$. Find the matrix $B$ that represents the linear operator $A$ relative to the basis\\n\\n$$\\nS=\\\\left\\\\{u_{1}, u_{2}, u_{3}\\\\right\\\\}=\\\\left\\\\{[1,1,0]^{T}, \\\\quad[0,1,1]^{T}, \\\\quad[1,2,2]^{T}\\\\right\\\\}\\n$$\\n\\n[Recall $A$ that defines a linear operator $A: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{3}$ relative to the usual basis $E$ of $\\\\mathbf{R}^{3}$.]\\n\\nMethod 1. Find the coordinates of $A\\\\left(u_{1}\\\\right), A\\\\left(u_{2}\\\\right), A\\\\left(u_{3}\\\\right)$ relative to the basis $S$ by first finding the coordinates of an arbitrary vector $v=(a, b, c)$ in $\\\\mathbf{R}^{3}$ relative to the basis $S$. By Problem 6.16,\\n\\n$$\\n[v]_{S}=(b-c) u_{1}+(-2 a+2 b-c) u_{2}+(a-b+c) u_{3}\\n$$\\n\\nUsing this formula for $[a, b, c]^{T}$, we obtain\\n\\n$$\\n\\\\begin{gathered}\\nA\\\\left(u_{1}\\\\right)=[4,7,-1]^{T}=8 u_{1}+7 u_{2}-5 u_{3}, \\\\quad A\\\\left(u_{2}\\\\right)=[4,1,0]^{T}=u_{1}-6 u_{2}+3 u_{3} \\\\\\\\\\nA\\\\left(u_{3}\\\\right)=[9,4,1]^{T}=3 u_{1}-11 u_{2}+6 u_{3}\\n\\\\end{gathered}\\n$$\\n\\nWriting the coefficients of $u_{1}, u_{2}, u_{3}$ as columns yields\\n\\n$$\\nB=\\\\left[\\\\begin{array}{rrr}\\n8 & 1 & 3 \\\\\\\\\\n7 & -6 & -11 \\\\\\\\\\n-5 & 3 & 6\\n\\\\end{array}\\\\right]\\n$$\\n\\nMethod 2. Use $B=P^{-1} A P$, where $P$ is the change-of-basis matrix from the usual basis $E$ to $S$. The matrix $P$ (whose columns are simply the vectors in $S$ ) and $P^{-1}$ appear in Problem 6.16. Thus,\\n\\n$$\\nB=P^{-1} A P=\\\\left[\\\\begin{array}{rrr}\\n0 & 1 & -1 \\\\\\\\\\n-2 & 2 & -1 \\\\\\\\\\n1 & -1 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rrr}\\n1 & 3 & 1 \\\\\\\\\\n2 & 5 & -4 \\\\\\\\\\n1 & -2 & 2\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 1 \\\\\\\\\\n1 & 1 & 2 \\\\\\\\\\n0 & 1 & 2\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrr}\\n8 & 1 & 3 \\\\\\\\\\n7 & -6 & -11 \\\\\\\\\\n-5 & 3 & 6\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.26. Prove Theorem 6.7: Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\\\\prime}$ in a vector space $V$. Then, for any linear operator $T$ on $V,[T]_{S^{\\\\prime}}=P^{-1}[T]_{S} P$.\\n\\nLet $v$ be a vector in $V$. Then, by Theorem 6.6, $P[v]_{S^{\\\\prime}}=[v]_{S}$. Therefore,\\n\\n$$\\nP^{-1}[T]_{S} P[v]_{S^{\\\\prime}}=P^{-1}[T]_{S}[v]_{S}=P^{-1}[T(v)]_{S}=[T(v)]_{S^{\\\\prime}}\\n$$\\n\\nBut $[T]_{S^{\\\\prime}}[v]_{S^{\\\\prime}}=[T(v)]_{S^{\\\\prime}}$. Hence,\\n\\n$$\\nP^{-1}[T]_{S} P[v]_{S^{\\\\prime}}=[T]_{S^{\\\\prime}}[v]_{S^{\\\\prime}}\\n$$\\n\\nBecause the mapping $v \\\\mapsto[v]_{S^{\\\\prime}}$ is onto $K^{n}$, we have $P^{-1}[T]_{S} P X=[T]_{S^{\\\\prime}} X$ for every $X \\\\in K^{n}$. Thus, $P^{-1}[T]_{S} P=[T]_{S^{\\\\prime}}$, as claimed.\\n\\n\\n\\\\section*{Similarity of Matrices}\\n',\n",
       " '6.27. Let $A=\\\\left[\\\\begin{array}{rr}4 & -2 \\\\\\\\ 3 & 6\\\\end{array}\\\\right]$ and $P=\\\\left[\\\\begin{array}{ll}1 & 2 \\\\\\\\ 3 & 4\\\\end{array}\\\\right]$.\\\\\\\\\\n(a) Find $B=P^{-1} A P$.\\\\\\\\\\n(b) Verify $\\\\operatorname{tr}(B)=\\\\operatorname{tr}(A)$.\\\\\\\\\\n(c) Verify $\\\\operatorname{det}(B)=\\\\operatorname{det}(A)$.\\n\\n(a) First find $P^{-1}$ using the formula for the inverse of a $2 \\\\times 2$ matrix. We have\\n\\n$$\\nP^{-1}=\\\\left[\\\\begin{array}{rr}\\n-2 & 1 \\\\\\\\\\n\\\\frac{3}{2} & -\\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nThen\\n\\n$$\\nB=P^{-1} A P=\\\\left[\\\\begin{array}{rr}\\n-2 & 1 \\\\\\\\\\n\\\\frac{3}{2} & -\\\\frac{1}{2}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n4 & -2 \\\\\\\\\\n3 & 6\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n25 & 30 \\\\\\\\\\n-\\\\frac{27}{2} & -15\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) $\\\\operatorname{tr}(A)=4+6=10$ and $\\\\operatorname{tr}(B)=25-15=10$. Hence, $\\\\operatorname{tr}(B)=\\\\operatorname{tr}(A)$.\\n\\n(c) $\\\\operatorname{det}(A)=24+6=30$ and $\\\\operatorname{det}(B)=-375+405=30$. Hence, $\\\\operatorname{det}(B)=\\\\operatorname{det}(A)$.\\n',\n",
       " '\\n6.28. Find the trace of each of the linear transformations $F$ on $\\\\mathbf{R}^{3}$ in Problem 6.4.\\n\\nFind the trace (sum of the diagonal elements) of any matrix representation of $F$ such as the matrix representation $[F]=[F]_{E}$ of $F$ relative to the usual basis $E$ given in Problem 6.4.\\n\\n(a) $\\\\operatorname{tr}(F)=\\\\operatorname{tr}([F])=1-5+9=5$.\\n\\n(b) $\\\\operatorname{tr}(F)=\\\\operatorname{tr}([F])=1+3+5=9$.\\n\\n(c) $\\\\operatorname{tr}(F)=\\\\operatorname{tr}([F])=1+4+7=12$.\\n',\n",
       " '\\n6.29. Write $A \\\\approx B$ if $A$ is similar to $B$-that is, if there exists an invertible matrix $P$ such that $A=P^{-1} B P$. Prove that $\\\\approx$ is an equivalence relation (on square matrices); that is,\\n\\n(a) $A \\\\approx A$, for every $A . \\\\quad$ (b) If $A \\\\approx B$, then $B \\\\approx A$.\\n\\n(c) If $A \\\\approx B$ and $B \\\\approx C$, then $A \\\\approx C$.\\n\\n(a) The identity matrix $I$ is invertible, and $I^{-1}=I$. Because $A=I^{-1} A I$, we have $A \\\\approx A$.\\n\\n(b) Because $A \\\\approx B$, there exists an invertible matrix $P$ such that $A=P^{-1} B P$. Hence, $B=P A P^{-1}=\\\\left(P^{-1}\\\\right)^{-1} A P$ and $P^{-1}$ is also invertible. Thus, $B \\\\approx A$.\\n\\n(c) Because $A \\\\approx B$, there exists an invertible matrix $P$ such that $A=P^{-1} B P$, and as $B \\\\approx C$, there exists an invertible matrix $Q$ such that $B=Q^{-1} C Q$. Thus,\\n\\n$$\\nA=P^{-1} B P=P^{-1}\\\\left(Q^{-1} C Q\\\\right) P=\\\\left(P^{-1} Q^{-1}\\\\right) C(Q P)=(Q P)^{-1} C(Q P)\\n$$\\n\\nand $Q P$ is also invertible. Thus, $A \\\\approx C$.\\n',\n",
       " '\\n6.30. Suppose $B$ is similar to $A$, say $B=P^{-1} A P$. Prove\\n\\n(a) $B^{n}=P^{-1} A^{n} P$, and so $B^{n}$ is similar to $A^{n}$.\\n\\n(b) $f(B)=P^{-1} f(A) P$, for any polynomial $f(x)$, and so $f(B)$ is similar to $f(A)$.\\n\\n(c) $B$ is a root of a polynomial $g(x)$ if and only if $A$ is a root of $g(x)$.\\n\\n(a) The proof is by induction on $n$. The result holds for $n=1$ by hypothesis. Suppose $n>1$ and the result holds for $n-1$. Then\\n\\n$$\\nB^{n}=B B^{n-1}=\\\\left(P^{-1} A P\\\\right)\\\\left(P^{-1} A^{n-1} P\\\\right)=P^{-1} A^{n} P\\n$$\\n\\n(b) Suppose $f(x)=a_{n} x^{n}+\\\\cdots+a_{1} x+a_{0}$. Using the left and right distributive laws and part (a), we have\\n\\n$$\\n\\\\begin{aligned}\\nP^{-1} f(A) P & =P^{-1}\\\\left(a_{n} A^{n}+\\\\cdots+a_{1} A+a_{0} I\\\\right) P \\\\\\\\\\n& =P^{-1}\\\\left(a_{n} A^{n}\\\\right) P+\\\\cdots+P^{-1}\\\\left(a_{1} A\\\\right) P+P^{-1}\\\\left(a_{0} I\\\\right) P \\\\\\\\\\n& =a_{n}\\\\left(P^{-1} A^{n} P\\\\right)+\\\\cdots+a_{1}\\\\left(P^{-1} A P\\\\right)+a_{0}\\\\left(P^{-1} I P\\\\right) \\\\\\\\\\n& =a_{n} B^{n}+\\\\cdots+a_{1} B+a_{0} I=f(B)\\n\\\\end{aligned}\\n$$\\n\\n(c) By part (b), $g(B)=0$ if and only if $P^{-1} g(A) P=0$ if and only if $g(A)=P 0 P^{-1}=0$.\\n\\n\\n\\\\section*{Matrix Representations of General Linear Mappings}\\n',\n",
       " '6.31. Let $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ be the linear map defined by $F(x, y, z)=(3 x+2 y-4 z, x-5 y+3 z)$.\\n\\n(a) Find the matrix of $F$ in the following bases of $\\\\mathbf{R}^{3}$ and $\\\\mathbf{R}^{2}$ :\\n\\n\\n\\\\begin{equation*}\\nS=\\\\left\\\\{w_{1}, w_{2}, w_{3}\\\\right\\\\}=\\\\{(1,1,1),(1,1,0),(1,0,0)\\\\} \\\\quad \\\\text { and } \\\\quad S^{\\\\prime}=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,3) \\\\tag{2,5}\\n\\\\end{equation*}\\n\\n\\n(b) Verify Theorem 6.10: The action of $F$ is preserved by its matrix representation; that is, for any $v$ in $\\\\mathbf{R}^{3}$, we have $[F]_{S, S^{\\\\prime}}[v]_{S}=[F(v)]_{S^{\\\\prime}}$.\\n\\n(a) From Problem 6.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Thus,\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(w_{1}\\\\right)=F(1,1,1)=(1,-1)=-7 u_{1}+4 u_{2} \\\\\\\\\\n& F\\\\left(w_{2}\\\\right)=F(1,1,0)=(5,-4)=-33 u_{1}+19 u_{2} \\\\\\\\\\n& F\\\\left(w_{3}\\\\right)=F(1,0,0)=(3,1)=-13 u_{1}+8 u_{2}\\n\\\\end{aligned}\\n$$\\n\\nWrite the coordinates of $F\\\\left(w_{1}\\\\right), F\\\\left(w_{2}\\\\right), F\\\\left(w_{3}\\\\right)$ as columns to get\\n\\n$$\\n[F]_{S, S^{\\\\prime}}=\\\\left[\\\\begin{array}{rrr}\\n-7 & -33 & 13 \\\\\\\\\\n4 & 19 & 8\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) If $v=(x, y, z)$, then, by Problem 6.5, $v=z w_{1}+(y-z) w_{2}+(x-y) w_{3}$. Also,\\n\\n$F(v)=(3 x+2 y-4 z, x-5 y+3 z)=(-13 x-20 y+26 z) u_{1}+(8 x+11 y-15 z) u_{2}$\\n\\nHence, $\\\\quad[v]_{S}=(z, y-z, x-y)^{T} \\\\quad$ and $\\\\quad[F(v)]_{S^{\\\\prime}}=\\\\left[\\\\begin{array}{c}-13 x-20 y+26 z \\\\\\\\ 8 x+11 y-15 z\\\\end{array}\\\\right]$\\n\\nThus, $\\\\quad[F]_{S, S^{\\\\prime}}[v]_{S}=\\\\left[\\\\begin{array}{rrr}-7 & -33 & -13 \\\\\\\\ 4 & 19 & 8\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}z \\\\\\\\ y-x \\\\\\\\ x-y\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{c}-13 x-20 y+26 z \\\\\\\\ 8 x+11 y-15 z\\\\end{array}\\\\right]=[F(v)]_{S^{\\\\prime}}$\\n',\n",
       " '\\n6.32. Let $F: \\\\mathbf{R}^{n} \\\\rightarrow \\\\mathbf{R}^{m}$ be the linear mapping defined as follows:\\n\\n$$\\nF\\\\left(x_{1}, x_{2}, \\\\ldots, x_{n}\\\\right)=\\\\left(a_{11} x_{1}+\\\\cdots+a_{1 n} x_{n}, a_{21} x_{1}+\\\\cdots+a_{2 n} x_{n}, \\\\ldots, a_{m 1} x_{1}+\\\\cdots+a_{m n} x_{n}\\\\right)\\n$$\\n\\n(a) Show that the rows of the matrix $[F]$ representing $F$ relative to the usual bases of $\\\\mathbf{R}^{n}$ and $\\\\mathbf{R}^{m}$ are the coefficients of the $x_{i}$ in the components of $F\\\\left(x_{1}, \\\\ldots, x_{n}\\\\right)$.\\n\\n(b) Find the matrix representation of each of the following linear mappings relative to the usual basis of $\\\\mathbf{R}^{n}$ :\\n\\n(i) $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{3}$ defined by $F(x, y)=(3 x-y, \\\\quad 2 x+4 y, \\\\quad 5 x-6 y)$.\\n\\n(ii) $F: \\\\mathbf{R}^{4} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y, s, t)=(3 x-4 y+2 s-5 t, \\\\quad 5 x+7 y-s-2 t)$.\\n\\n(iii) $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{4}$ defined by $F(x, y, z)=(2 x+3 y-8 z, \\\\quad x+y+z, \\\\quad 4 x-5 z, \\\\quad 6 y)$.\\n\\n(a) We have\\n\\n$$\\n\\\\begin{aligned}\\n& F(1,0, \\\\ldots, 0)=\\\\left(a_{11}, a_{21}, \\\\ldots, a_{m 1}\\\\right) \\\\\\\\\\n& F(0,1, \\\\ldots, 0)=\\\\left(a_{12}, a_{22}, \\\\ldots, a_{m 2}\\\\right) \\\\\\\\\\n& \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\n& F(0,0, \\\\ldots, 1)=\\\\left(a_{1 n}, a_{2 n}, \\\\ldots, a_{m n}\\\\right)\\n\\\\end{aligned} \\\\quad \\\\text { and thus, } \\\\quad[F]=\\\\left[\\\\begin{array}{cccc}\\na_{11} & a_{12} & \\\\ldots & a_{1 n} \\\\\\\\\\na_{21} & a_{22} & \\\\ldots & a_{2 n} \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\na_{m 1} & a_{m 2} & \\\\ldots & a_{m n}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) By part (a), we need only look at the coefficients of the unknown $x, y, \\\\ldots$ in $F(x, y, \\\\ldots)$. Thus,\\\\\\\\\\n(i) $[F]=\\\\left[\\\\begin{array}{rr}3 & -1 \\\\\\\\ 2 & 4 \\\\\\\\ 5 & -6\\\\end{array}\\\\right]$,\\\\\\\\\\n(ii) $[F]=\\\\left[\\\\begin{array}{rrrr}3 & -4 & 2 & -5 \\\\\\\\ 5 & 7 & -1 & -2\\\\end{array}\\\\right]$,\\\\\\\\\\n(iii) $[F]=\\\\left[\\\\begin{array}{rrr}2 & 3 & -8 \\\\\\\\ 1 & 1 & 1 \\\\\\\\ 4 & 0 & -5 \\\\\\\\ 0 & 6 & 0\\\\end{array}\\\\right]$\\n',\n",
       " '\\n6.33. Let $A=\\\\left[\\\\begin{array}{rrr}2 & 5 & -3 \\\\\\\\ 1 & -4 & 7\\\\end{array}\\\\right]$. Recall that $A$ determines a mapping $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(v)=A v$, where vectors are written as columns. Find the matrix $[F]$ that represents the mapping relative to the following bases of $\\\\mathbf{R}^{3}$ and $\\\\mathbf{R}^{2}$ :\\n\\n(a) The usual bases of $\\\\mathbf{R}^{3}$ and of $\\\\mathbf{R}^{2}$.\\n\\n(b) $S=\\\\left\\\\{w_{1}, w_{2}, w_{3}\\\\right\\\\}=\\\\{(1,1,1),(1,1,0),(1,0,0)\\\\}$ and $S^{\\\\prime}=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,3),(2,5)\\\\}$.\\n\\n(a) Relative to the usual bases, $[F]$ is the matrix $A$.\\\\\\\\\\n(b) From Problem 9.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Thus,\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\qquad F\\\\left(w_{1}\\\\right)=\\\\left[\\\\begin{array}{rrr}\\n2 & 5 & -3 \\\\\\\\\\n1 & -4 & 7\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n4 \\\\\\\\\\n4\\n\\\\end{array}\\\\right]=-12 u_{1}+8 u_{2} \\\\\\\\\\n& F\\\\left(w_{2}\\\\right)=\\\\left[\\\\begin{array}{rrr}\\n2 & 5 & -3 \\\\\\\\\\n1 & -4 & 7\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n1 \\\\\\\\\\n0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{r}\\n7 \\\\\\\\\\n-3\\n\\\\end{array}\\\\right]=-41 u_{1}+24 u_{2} \\\\\\\\\\n& F\\\\left(w_{3}\\\\right)=\\\\left[\\\\begin{array}{rrr}\\n2 & 5 & -3 \\\\\\\\\\n1 & -4 & 7\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n1 \\\\\\\\\\n0 \\\\\\\\\\n0\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{l}\\n2 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=-8 u_{1}+5 u_{2} \\\\\\\\\\n& \\\\text { Writing the coefficients of } F\\\\left(w_{1}\\\\right), F\\\\left(w_{2}\\\\right), F\\\\left(w_{3}\\\\right) \\\\text { as columns yields }[F]=\\\\left[\\\\begin{array}{rrr}\\n-12 & -41 & -8 \\\\\\\\\\n8 & 24 & 5\\n\\\\end{array}\\\\right] .\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n6.34. Consider the linear transformation $T$ on $\\\\mathbf{R}^{2}$ defined by $T(x, y)=(2 x-3 y, \\\\quad x+4 y)$ and the following bases of $\\\\mathbf{R}^{2}$ :\\n\\n$$\\nE=\\\\left\\\\{e_{1}, e_{2}\\\\right\\\\}=\\\\{(1,0),(0,1)\\\\} \\\\quad \\\\text { and } \\\\quad S=\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}=\\\\{(1,3),(2,5)\\\\}\\n$$\\n\\n(a) Find the matrix $A$ representing $T$ relative to the bases $E$ and $S$.\\n\\n(b) Find the matrix $B$ representing $T$ relative to the bases $S$ and $E$.\\n\\n(We can view $T$ as a linear mapping from one space into another, each having its own basis.)\\n\\n(a) From Problem 6.2, $(a, b)=(-5 a+2 b) u_{1}+(3 a-b) u_{2}$. Hence,\\n\\n$$\\n\\\\begin{aligned}\\n& T\\\\left(e_{1}\\\\right)=T(1,0)=(2,1)=-8 u_{1}+5 u_{2} \\\\\\\\\\n& T\\\\left(e_{2}\\\\right)=T(0,1)=(-3,4)=23 u_{1}-13 u_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\n-8 & 23 \\\\\\\\\\n5 & -13\\n\\\\end{array}\\\\right]\\n$$\\n\\n(b) We have\\n\\n$$\\n\\\\begin{aligned}\\n& T\\\\left(u_{1}\\\\right)=T(1,3)=(-7,13)=-7 e_{1}+13 e_{2} \\\\\\\\\\n& T\\\\left(u_{2}\\\\right)=T(2,5)=(-11,22)=-11 e_{1}+22 e_{2}\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad B=\\\\left[\\\\begin{array}{rr}\\n-7 & -11 \\\\\\\\\\n13 & 22\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n6.35. How are the matrices $A$ and $B$ in Problem 6.34 related?\\n\\nBy Theorem 6.12, the matrices $A$ and $B$ are equivalent to each other; that is, there exist nonsingular matrices $P$ and $Q$ such that $B=Q^{-1} A P$, where $P$ is the change-of-basis matrix from $S$ to $E$, and $Q$ is the change-of-basis matrix from $E$ to $S$. Thus,\\n\\nand\\n\\n$$\\n\\\\begin{gathered}\\nP=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right], \\\\quad Q=\\\\left[\\\\begin{array}{rr}\\n-5 & 2 \\\\\\\\\\n3 & -1\\n\\\\end{array}\\\\right], \\\\quad Q^{-1}=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right] \\\\\\\\\\nQ^{-1} A P=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n-8 & -23 \\\\\\\\\\n5 & -13\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 5\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n-7 & -11 \\\\\\\\\\n13 & 22\\n\\\\end{array}\\\\right]=B\\n\\\\end{gathered}\\n$$\\n',\n",
       " '\\n6.36. Prove Theorem 6.14: Let $F: V \\\\rightarrow U$ be linear and, say, $\\\\operatorname{rank}(F)=r$. Then there exist bases $V$ and of $U$ such that the matrix representation of $F$ has the following form, where $I_{r}$ is the $r$-square identity matrix:\\n\\n$$\\nA=\\\\left[\\\\begin{array}{cc}\\nI_{r} & 0 \\\\\\\\\\n0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nSuppose $\\\\operatorname{dim} V=m$ and $\\\\operatorname{dim} U=n$. Let $W$ be the kernel of $F$ and $U^{\\\\prime}$ the image of $F$. We are given that $\\\\operatorname{rank}(F)=r$. Hence, the dimension of the kernel of $F$ is $m-r$. Let $\\\\left\\\\{w_{1}, \\\\ldots, w_{m-r}\\\\right\\\\}$ be a basis of the kernel of $F$ and extend this to a basis of $V$ :\\n\\n$$\\n\\\\begin{gathered}\\n\\\\left\\\\{v_{1}, \\\\ldots, v_{r}, w_{1}, \\\\ldots, w_{m-r}\\\\right\\\\} \\\\\\\\\\nu_{1}=F\\\\left(v_{1}\\\\right), u_{2}=F\\\\left(v_{2}\\\\right), \\\\ldots, u_{r}=F\\\\left(v_{r}\\\\right)\\n\\\\end{gathered}\\n$$\\n\\nThen $\\\\left\\\\{u_{1}, \\\\ldots, u_{r}\\\\right\\\\}$ is a basis of $U^{\\\\prime}$, the image of $F$. Extend this to a basis of $U$, say\\n\\n$$\\n\\\\left\\\\{u_{1}, \\\\ldots, u_{r}, u_{r+1}, \\\\ldots, u_{n}\\\\right\\\\}\\n$$\\n\\nObserve that\\n\\n$$\\n\\\\begin{aligned}\\n& F\\\\left(v_{1}\\\\right) \\\\quad=u_{1}=1 u_{1}+0 u_{2}+\\\\cdots+0 u_{r}+0 u_{r+1}+\\\\cdots+0 u_{n} \\\\\\\\\\n& F\\\\left(v_{2}\\\\right) \\\\quad=u_{2}=0 u_{1}+1 u_{2}+\\\\cdots+0 u_{r}+0 u_{r+1}+\\\\cdots+0 u_{n} \\\\\\\\\\n& F\\\\left(v_{r}\\\\right) \\\\quad=u_{r}=0 u_{1}+0 u_{2}+\\\\cdots+1 u_{r}+0 u_{r+1}+\\\\cdots+0 u_{n} \\\\\\\\\\n& F\\\\left(w_{1}\\\\right)=0=0 u_{1}+0 u_{2}+\\\\cdots+0 u_{r}+0 u_{r+1}+\\\\cdots+0 u_{n} \\\\\\\\\\n& F\\\\left(w_{m-r}\\\\right)=0=0 u_{1}+0 u_{2}+\\\\cdots+0 u_{r}+0 u_{r+1}+\\\\cdots+0 u_{n}\\n\\\\end{aligned}\\n$$\\n\\nThus, the matrix of $F$ in the above bases has the required form.\\n\\n',\n",
       " '7.1. Expand:\\n\\n(a) $\\\\left\\\\langle 5 u_{1}+8 u_{2}, 6 v_{1}-7 v_{2}\\\\right\\\\rangle$\\n\\n(b) $\\\\langle 3 u+5 v, 4 u-6 v\\\\rangle$,\\n\\n(c) $\\\\|2 u-3 v\\\\|^{2}$\\n\\nUse linearity in both positions and, when possible, symmetry, $\\\\langle u, v\\\\rangle=\\\\langle v, u\\\\rangle$.\\\\\\\\\\n(a) Take the inner product of each term on the left with each term on the right:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle 5 u_{1}+8 u_{2}, \\\\quad 6 v_{1}-7 v_{2}\\\\right\\\\rangle & =\\\\left\\\\langle 5 u_{1}, 6 v_{1}\\\\right\\\\rangle+\\\\left\\\\langle 5 u_{1},-7 v_{2}\\\\right\\\\rangle+\\\\left\\\\langle 8 u_{2}, 6 v_{1}\\\\right\\\\rangle+\\\\left\\\\langle 8 u_{2},-7 v_{2}\\\\right\\\\rangle \\\\\\\\\\n& =30\\\\left\\\\langle u_{1}, v_{1}\\\\right\\\\rangle-35\\\\left\\\\langle u_{1}, v_{2}\\\\right\\\\rangle+48\\\\left\\\\langle u_{2}, v_{1}\\\\right\\\\rangle-56\\\\left\\\\langle u_{2}, v_{2}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\n[Remark: Observe the similarity between the above expansion and the expansion $(5 a-8 b)(6 c-7 d)$ in ordinary algebra.]\\n\\n(b) $\\\\langle 3 u+5 v, 4 u-6 v\\\\rangle=12\\\\langle u, u\\\\rangle-18\\\\langle u, v\\\\rangle+20\\\\langle v, u\\\\rangle-30\\\\langle v, v\\\\rangle$\\n\\n$$\\n=12\\\\langle u, u\\\\rangle+2\\\\langle u, v\\\\rangle-30\\\\langle v, v\\\\rangle\\n$$\\n\\n(c) $\\\\begin{aligned}\\\\|2 u-3 v\\\\|^{2} & =\\\\langle 2 u-3 v, 2 u-3 v\\\\rangle=4\\\\langle u, u\\\\rangle-6\\\\langle u, v\\\\rangle-6\\\\langle v, u\\\\rangle+9\\\\langle v, v\\\\rangle \\\\\\\\ & =4\\\\|u\\\\|^{2}-12(u, v)+9\\\\|v\\\\|^{2}\\\\end{aligned}$\\n',\n",
       " '\\n7.2. Consider vectors $u=(1,2,4), v=(2,-3,5), w=(4,2,-3)$ in $\\\\mathbf{R}^{3}$. Find\\\\\\\\\\n(a) $u \\\\cdot v$,\\\\\\\\\\n(b) $u \\\\cdot w$,\\\\\\\\\\n(c) $v \\\\cdot w$,\\\\\\\\\\n(d) $\\\\quad(u+v) \\\\cdot w$\\\\\\\\\\n(e) $\\\\|u\\\\|$, (f)\\\\\\\\\\n$\\\\|v\\\\|$.\\n\\n(a) Multiply corresponding components and add to get $u \\\\cdot v=2-6+20=16$.\\n\\n(b) $u \\\\cdot w=4+4-12=-4$.\\n\\n(c) $v \\\\cdot w=8-6-15=-13$.\\n\\n(d) First find $u+v=(3,-1,9)$. Then $(u+v) \\\\cdot w=12-2-27=-17$. Alternatively, using $\\\\left[\\\\mathrm{I}_{1}\\\\right]$, $(u+v) \\\\cdot w=u \\\\cdot w+v \\\\cdot w=-4-13=-17$.\\n\\n(e) First find $\\\\|u\\\\|^{2}$ by squaring the components of $u$ and adding:\\n\\n$$\\n\\\\|u\\\\|^{2}=1^{2}+2^{2}+4^{2}=1+4+16=21, \\\\quad \\\\text { and so } \\\\quad\\\\|u\\\\|=\\\\sqrt{21}\\n$$\\n\\n(f) $\\\\|v\\\\|^{2}=4+9+25=38$, and so $\\\\|v\\\\|=\\\\sqrt{38}$.\\n',\n",
       " '\\n7.3. Verify that the following defines an inner product in $\\\\mathbf{R}^{2}$ :\\n\\n$$\\n\\\\langle u, v\\\\rangle=x_{1} y_{1}-x_{1} y_{2}-x_{2} y_{1}+3 x_{2} y_{2}, \\\\quad \\\\text { where } \\\\quad u=\\\\left(x_{1}, x_{2}\\\\right), \\\\quad v=\\\\left(y_{1}, y_{2}\\\\right)\\n$$\\n\\nWe argue via matrices. We can write $\\\\langle u, v\\\\rangle$ in matrix notation as follows:\\n\\n$$\\n\\\\langle u, v\\\\rangle=u^{T} A v=\\\\left[x_{1}, x_{2}\\\\right]\\\\left[\\\\begin{array}{rr}\\n1 & -1 \\\\\\\\\\n-1 & 3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\ny_{1} \\\\\\\\\\ny_{2}\\n\\\\end{array}\\\\right]\\n$$\\n\\nBecause $A$ is real and symmetric, we need only show that $A$ is positive definite. The diagonal elements 1 and 3 are positive, and the determinant $\\\\|A\\\\|=3-1=2$ is positive. Thus, by Theorem 7.14, $A$ is positive definite. Accordingly, by Theorem 7.15, $\\\\langle u, v\\\\rangle$ is an inner product.\\n',\n",
       " '\\n7.4. Consider the vectors $u=(1,5)$ and $v=(3,4)$ in $\\\\mathbf{R}^{2}$. Find\\n\\n(a) $\\\\langle u, v\\\\rangle$ with respect to the usual inner product in $\\\\mathbf{R}^{2}$.\\n\\n(b) $\\\\langle u, v\\\\rangle$ with respect to the inner product in $\\\\mathbf{R}^{2}$ in Problem 7.3.\\n\\n(c) $\\\\|v\\\\|$ using the usual inner product in $\\\\mathbf{R}^{2}$.\\n\\n(d) $\\\\|v\\\\|$ using the inner product in $\\\\mathbf{R}^{2}$ in Problem 7.3.\\n\\n(a) $\\\\langle u, v\\\\rangle=3+20=23$.\\n\\n(b) $\\\\langle u, v\\\\rangle=1 \\\\cdot 3-1 \\\\cdot 4-5 \\\\cdot 3+3 \\\\cdot 5 \\\\cdot 4=3-4-15+60=44$.\\n\\n(c) $\\\\|v\\\\|^{2}=\\\\langle v, v\\\\rangle=\\\\langle(3,4),(3,4)\\\\rangle=9+16=25$; hence, $\\\\mid v \\\\|=5$.\\n\\n(d) $\\\\|v\\\\|^{2}=\\\\langle v, v\\\\rangle=\\\\langle(3,4),(3,4)\\\\rangle=9-12-12+48=33$; hence, $\\\\|v\\\\|=\\\\sqrt{33}$.\\n',\n",
       " '\\n7.5. Consider the following polynomials in $\\\\mathbf{P}(t)$ with the inner product $\\\\langle f, g\\\\rangle=\\\\int_{0}^{1} f(t) g(t) d t$ :\\n\\n$$\\nf(t)=t+2, \\\\quad g(t)=3 t-2, \\\\quad h(t)=t^{2}-2 t-3\\n$$\\n\\n(a) Find $\\\\langle f, g\\\\rangle$ and $\\\\langle f, h\\\\rangle$.\\n\\n(b) Find $\\\\|f\\\\|$ and $\\\\|g\\\\|$.\\n\\n(c) Normalize $f$ and $g$.\\\\\\\\\\n(a) Integrate as follows:\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\langle f, g\\\\rangle=\\\\int_{0}^{1}(t+2)(3 t-2) d t=\\\\int_{0}^{1}\\\\left(3 t^{2}+4 t-4\\\\right) d t=\\\\left.\\\\left(t^{3}+2 t^{2}-4 t\\\\right)\\\\right|_{0} ^{1}=-1 \\\\\\\\\\n& \\\\langle f, h\\\\rangle=\\\\int_{0}^{1}(t+2)\\\\left(t^{2}-2 t-3\\\\right) d t=\\\\left.\\\\left(\\\\frac{t^{4}}{4}-\\\\frac{7 t^{2}}{2}-6 t\\\\right)\\\\right|_{0} ^{1}=-\\\\frac{37}{4}\\n\\\\end{aligned}\\n$$\\n\\n(b)\\n\\n$$\\n\\\\begin{array}{r}\\n\\\\langle f, f\\\\rangle=\\\\int_{0}^{1}(t+2)(t+2) d t=\\\\frac{19}{3} ; \\\\quad \\\\text { hence, } \\\\quad\\\\|f\\\\|=\\\\sqrt{\\\\frac{19}{3}}=\\\\frac{1}{3} \\\\sqrt{57} \\\\\\\\\\n\\\\langle g, g\\\\rangle=\\\\int_{0}^{1}(3 t-2)(3 t-2)=1 ; \\\\quad \\\\text { hence, } \\\\quad\\\\|g\\\\|=\\\\sqrt{1}=1\\n\\\\end{array}\\n$$\\n\\n(c) Because $\\\\|f\\\\|=\\\\frac{1}{3} \\\\sqrt{57}$ and $g$ is already a unit vector, we have\\n\\n$$\\n\\\\hat{f}=\\\\frac{1}{\\\\|f\\\\|} f=\\\\frac{3}{\\\\sqrt{57}}(t+2) \\\\quad \\\\text { and } \\\\quad \\\\hat{g}=g=3 t-2\\n$$\\n',\n",
       " '\\n7.6. Find $\\\\cos \\\\theta$ where $\\\\theta$ is the angle between:\\n\\n(a) $u=(1,3,-5,4)$ and $v=(2,-3,4,1)$ in $\\\\mathbf{R}^{4}$,\\n\\n(b) $A=\\\\left[\\\\begin{array}{lll}9 & 8 & 7 \\\\\\\\ 6 & 5 & 4\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 4 & 5 & 6\\\\end{array}\\\\right]$, where $\\\\langle A, B\\\\rangle=\\\\operatorname{tr}\\\\left(B^{T} A\\\\right)$.\\n\\nUse $\\\\cos \\\\theta=\\\\frac{\\\\langle u, v\\\\rangle}{\\\\|u\\\\|\\\\|v\\\\|}$\\n\\n(a) Compute:\\n\\n$\\\\langle u, v\\\\rangle=2-9-20+4=-23, \\\\quad\\\\|u\\\\|^{2}=1+9+25+16=51, \\\\quad\\\\|v\\\\|^{2}=4+9+16+1=30$\\n\\nThus,\\n\\n$$\\n\\\\cos \\\\theta=\\\\frac{-23}{\\\\sqrt{51} \\\\sqrt{30}}=\\\\frac{-23}{3 \\\\sqrt{170}}\\n$$\\n\\n(b) Use $\\\\langle A, B\\\\rangle=\\\\operatorname{tr}\\\\left(B^{T} A\\\\right)=\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} a_{i j} b_{i j}$, the sum of the products of corresponding entries.\\n\\n$$\\n\\\\langle A, B\\\\rangle=9+16+21+24+25+24=119\\n$$\\n\\nUse $\\\\|A\\\\|^{2}=\\\\langle A, A\\\\rangle=\\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} a_{i j}^{2}$, the sum of the squares of all the elements of $A$.\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\|A\\\\|^{2}=\\\\langle A, A\\\\rangle=9^{2}+8^{2}+7^{2}+6^{2}+5^{2}+4^{2}=271, \\\\quad \\\\text { and so } \\\\quad\\\\|A\\\\|=\\\\sqrt{271} \\\\\\\\\\n& \\\\|B\\\\|^{2}=\\\\langle B, B\\\\rangle=1^{2}+2^{2}+3^{2}+4^{2}+5^{2}+6^{2}=91, \\\\quad \\\\text { and so } \\\\quad\\\\|B\\\\|=\\\\sqrt{91}\\n\\\\end{aligned}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\cos \\\\theta=\\\\frac{119}{\\\\sqrt{271} \\\\sqrt{91}}\\n$$\\n',\n",
       " '\\n7.7. Verify each of the following:\\n\\n(a) Parallelogram Law (Fig. 7-7): $\\\\|u+v\\\\|^{2}+\\\\|u-v\\\\|^{2}=2\\\\|u\\\\|^{2}+2\\\\|v\\\\|^{2}$.\\n\\n(b) Polar form for $\\\\langle u, v\\\\rangle$ (which shows the inner product can be obtained from the norm function):\\n\\n$$\\n\\\\langle u, v\\\\rangle=\\\\frac{1}{4}\\\\left(\\\\|u+v\\\\|^{2}-\\\\|u-v\\\\|^{2}\\\\right) .\\n$$\\n\\nExpand as follows to obtain\\n\\n\\n\\\\begin{align*}\\n\\\\|u+v\\\\|^{2} & =\\\\langle u+v, u+v\\\\rangle=\\\\|u\\\\|^{2}+2\\\\langle u, v\\\\rangle+\\\\|v\\\\|^{2}  \\\\tag{1}\\\\\\\\\\n\\\\|u-v\\\\|^{2} & =\\\\langle u-v, u-v\\\\rangle=\\\\|u\\\\|^{2}-2\\\\langle u, v\\\\rangle+\\\\|v\\\\|^{2} \\\\tag{2}\\n\\\\end{align*}\\n\\n\\nAdd (1) and (2) to get the Parallelogram Law (a). Subtract (2) from (1) to obtain\\n\\n$$\\n\\\\|u+v\\\\|^{2}-\\\\|u-v\\\\|^{2}=4\\\\langle u, v\\\\rangle\\n$$\\n\\nDivide by 4 to obtain the (real) polar form (b).\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-253}\\n\\\\end{center}\\n\\nFigure 7-7\\n',\n",
       " '\\n7.8. Prove Theorem 7.1 (Cauchy-Schwarz): For $u$ and $v$ in a real inner product space $V$, $\\\\langle u, u\\\\rangle^{2} \\\\leq\\\\langle u, u\\\\rangle\\\\langle v, v\\\\rangle \\\\quad$ or $\\\\quad|\\\\langle u, v\\\\rangle| \\\\leq\\\\|u\\\\|\\\\|v\\\\|$.\\n\\nFor any real number $t$,\\n\\n$$\\n\\\\langle t u+v, \\\\quad t u+v\\\\rangle=t^{2}\\\\langle u, u\\\\rangle+2 t\\\\langle u, v\\\\rangle+\\\\langle v, v\\\\rangle=t^{2}\\\\|u\\\\|^{2}+2 t\\\\langle u, v\\\\rangle+\\\\|v\\\\|^{2}\\n$$\\n\\nLet $a=\\\\|u\\\\|^{2}, b=2\\\\langle u, v), c=\\\\|v\\\\|^{2}$. Because $\\\\|t u+v\\\\|^{2} \\\\geq 0$, we have\\n\\n$$\\na t^{2}+b t+c \\\\geq 0\\n$$\\n\\nfor every value of $t$. This means that the quadratic polynomial cannot have two real roots, which implies that $b^{2}-4 a c \\\\leq 0$ or $b^{2} \\\\leq 4 a c$. Thus,\\n\\nDividing by 4 gives our result.\\n\\n$$\\n4\\\\langle u, v\\\\rangle^{2} \\\\leq 4\\\\|u\\\\|^{2}\\\\|v\\\\|^{2}\\n$$\\n',\n",
       " '\\n7.9. Prove Theorem 7.2: The norm in an inner product space $V$ satisfies\\n\\n(a) $\\\\left[\\\\mathrm{N}_{1}\\\\right]\\\\|v\\\\| \\\\geq 0$; and $\\\\|v\\\\|=0$ if and only if $v=0$.\\n\\n(b) $\\\\left[\\\\mathrm{N}_{2}\\\\right]\\\\|k v\\\\|=|k|\\\\|v\\\\|$.\\n\\n(c) $\\\\left[\\\\mathrm{N}_{3}\\\\right]\\\\|u+v\\\\| \\\\leq\\\\|u\\\\|+\\\\|v\\\\|$.\\n\\n(a) If $v \\\\neq 0$, then $\\\\langle v, v\\\\rangle>0$, and hence, $\\\\|v\\\\|=\\\\sqrt{\\\\langle v, v\\\\rangle}>0$. If $v=0$, then $\\\\langle 0,0\\\\rangle=0$. Consequently, $\\\\|0\\\\|=\\\\sqrt{0}=0$. Thus, $\\\\left[\\\\mathrm{N}_{1}\\\\right]$ is true.\\n\\n(b) We have $\\\\|k v\\\\|^{2}=\\\\langle k v, k v\\\\rangle=k^{2}\\\\langle v, v\\\\rangle=k^{2}\\\\|v\\\\|^{2}$. Taking the square root of both sides gives $\\\\left[\\\\mathrm{N}_{2}\\\\right]$.\\n\\n(c) Using the Cauchy-Schwarz inequality, we obtain\\n\\n$$\\n\\\\begin{gathered}\\n\\\\|u+v\\\\|^{2}=\\\\langle u+v, \\\\quad u+v\\\\rangle=\\\\langle u, u\\\\rangle+\\\\langle u, v\\\\rangle+\\\\langle u, v\\\\rangle+\\\\langle v, v\\\\rangle \\\\\\\\\\n\\\\leq\\\\|u\\\\|^{2}+2\\\\|u\\\\|\\\\|v\\\\|+\\\\|v\\\\|^{2}=(\\\\|u\\\\|+\\\\|v\\\\|)^{2}\\n\\\\end{gathered}\\n$$\\n\\nTaking the square root of both sides yields $\\\\left[\\\\mathrm{N}_{3}\\\\right]$.\\n\\n\\n\\\\section*{Orthogonality, Orthonormal Complements, Orthogonal Sets}\\n',\n",
       " '7.10. Find $k$ so that $u=(1,2, k, 3)$ and $v=(3, k, 7,-5)$ in $\\\\mathbf{R}^{4}$ are orthogonal.\\n\\nFirst find\\n\\n$$\\n\\\\langle u, v\\\\rangle=(1,2, k, 3) \\\\cdot(3, k, 7,-5)=3+2 k+7 k-15=9 k-12\\n$$\\n\\nThen set $\\\\langle u, v\\\\rangle=9 k-12=0$ to obtain $k=\\\\frac{4}{3}$.\\n',\n",
       " '\\n7.11. Let $W$ be the subspace of $\\\\mathbf{R}^{5}$ spanned by $u=(1,2,3,-1,2)$ and $v=(2,4,7,2,-1)$. Find a basis of the orthogonal complement $W^{\\\\perp}$ of $W$.\\n\\nWe seek all vectors $w=(x, y, z, s, t)$ such that\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\langle w, u\\\\rangle=x+2 y+3 z-s+2 t=0 \\\\\\\\\\n& \\\\langle w, v\\\\rangle=2 x+4 y+7 z+2 s-t=0\\n\\\\end{aligned}\\n$$\\n\\nEliminating $x$ from the second equation, we find the equivalent system\\n\\n$$\\n\\\\begin{aligned}\\nx+2 y+3 z-s+2 t & =0 \\\\\\\\\\nz+4 s-5 t & =0\\n\\\\end{aligned}\\n$$\\n\\nThe free variables are $y, s$, and $t$. Therefore,\\n\\n(1) Set $y=-1, s=0, t=0$ to obtain the solution $w_{1}=(2,-1,0,0,0)$.\\n\\n(2) Set $y=0, s=1, t=0$ to find the solution $w_{2}=(13,0,-4,1,0)$.\\n\\n(3) Set $y=0, s=0, t=1$ to obtain the solution $w_{3}=(-17,0,5,0,1)$.\\n\\nThe set $\\\\left\\\\{w_{1}, w_{2}, w_{3}\\\\right\\\\}$ is a basis of $W^{\\\\perp}$.\\n',\n",
       " '\\n7.12. Let $w=(1,2,3,1)$ be a vector in $\\\\mathbf{R}^{4}$. Find an orthogonal basis for $w^{\\\\perp}$.\\n\\nFind a nonzero solution of $x+2 y+3 z+t=0$, say $v_{1}=(0,0,1,-3)$. Now find a nonzero solution of the system\\n\\n$$\\nx+2 y+3 z+t=0, \\\\quad z-3 t=0\\n$$\\n\\nsay $v_{2}=(0,-5,3,1)$. Last, find a nonzero solution of the system\\n\\n$$\\nx+2 y+3 z+t=0, \\\\quad-5 y+3 z+t=0, \\\\quad z-3 t=0\\n$$\\n\\nsay $v_{3}=(-14,2,3,1)$. Thus, $v_{1}, v_{2}, v_{3}$ form an orthogonal basis for $w^{\\\\perp}$.\\n',\n",
       " '\\n7.13. Let $\\\\mathrm{S}$ consist of the following vectors in $\\\\mathbf{R}^{4}$ :\\n\\n$$\\nu_{1}=(1,1,0,-1), u_{2}=(1,2,1,3), u_{3}=(1,1,-9,2), u_{4}=(16,-13,1,3)\\n$$\\n\\n(a) Show that $S$ is orthogonal and a basis of $\\\\mathbf{R}^{4}$.\\n\\n(b) Find the coordinates of an arbitrary vector $v=(a, b, c, d)$ in $\\\\mathbf{R}^{4}$ relative to the basis $S$.\\n\\n(a) Compute\\n\\n$u_{1} \\\\cdot u_{2}=1+2+0-3=0, \\\\quad u_{1} \\\\cdot u_{3}=1+1+0-2=0, \\\\quad u_{1} \\\\cdot u_{4}=16-13+0-3=0$\\n\\n$u_{2} \\\\cdot u_{3}=1+2-9+6=0, \\\\quad u_{2} \\\\cdot u_{4}=16-26+1+9=0, \\\\quad u_{3} \\\\cdot u_{4}=16-13-9+6=0$\\n\\nThus, $S$ is orthogonal, and $S$ is linearly independent. Accordingly, $S$ is a basis for $\\\\mathbf{R}^{4}$ because any four linearly independent vectors form a basis of $\\\\mathbf{R}^{4}$.\\n\\n(b) Because $S$ is orthogonal, we need only find the Fourier coefficients of $v$ with respect to the basis vectors, as in Theorem 7.7. Thus,\\n\\n$$\\n\\\\begin{array}{ll}\\nk_{1}=\\\\frac{\\\\left\\\\langle v, u_{1}\\\\right\\\\rangle}{\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle}=\\\\frac{a+b-d}{3}, & k_{3}=\\\\frac{\\\\left\\\\langle v, u_{3}\\\\right\\\\rangle}{\\\\left\\\\langle u_{3}, u_{3}\\\\right\\\\rangle}=\\\\frac{a+b-9 c+2 d}{87} \\\\\\\\\\nk_{2}=\\\\frac{\\\\left\\\\langle v, u_{2}\\\\right\\\\rangle}{\\\\left\\\\langle u_{2}, u_{2}\\\\right\\\\rangle}=\\\\frac{a+2 b+c+3 d}{15}, & k_{4}=\\\\frac{\\\\left\\\\langle v, u_{4}\\\\right\\\\rangle}{\\\\left\\\\langle u_{4}, u_{4}\\\\right\\\\rangle}=\\\\frac{16 a-13 b+c+3 d}{435}\\n\\\\end{array}\\n$$\\n\\nare the coordinates of $v$ with respect to the basis $S$.\\n',\n",
       " '\\n7.14. Suppose $S, S_{1}, S_{2}$ are the subsets of $V$. Prove the following:\\n\\n(a) $S \\\\subseteq S^{\\\\perp \\\\perp}$.\\n\\n(b) If $S_{1} \\\\subseteq S_{2}$, then $S_{2}^{\\\\perp} \\\\subseteq S_{1}^{\\\\perp}$.\\n\\n(c) $S^{\\\\perp}=\\\\operatorname{span}(S)^{\\\\perp}$.\\n\\n(a) Let $w \\\\in S$. Then $\\\\langle w, v\\\\rangle=0$ for every $v \\\\in S^{\\\\perp}$; hence, $w \\\\in S^{\\\\perp \\\\perp}$. Accordingly, $S \\\\subseteq S^{\\\\perp \\\\perp}$.\\n\\n(b) Let $w \\\\in S_{2}^{\\\\perp}$. Then $\\\\langle w, v\\\\rangle=0$ for every $v \\\\in S_{2}$. Because $S_{1} \\\\subseteq S_{2},\\\\langle w, v\\\\rangle=0$ for every $v=S_{1}$. Thus, $w \\\\in S_{1}^{\\\\perp}$, and hence, $S_{2}^{\\\\perp} \\\\subseteq S_{1}^{\\\\perp}$.\\n\\n(c) Because $S \\\\subseteq \\\\operatorname{span}(S)$, part (b) gives us $\\\\operatorname{span}(S)^{\\\\perp} \\\\subseteq S^{\\\\perp}$. Suppose $u \\\\in S^{\\\\perp}$ and $v \\\\in \\\\operatorname{span}(S)$. Then there exist $w_{1}, w_{2}, \\\\ldots, w_{k}$ in $S$ such that $v=a_{1} w_{1}+a_{2} w_{2}+\\\\cdots+a_{k} w_{k}$. Then, using $u \\\\in S^{\\\\perp}$, we have\\n\\n$$\\n\\\\begin{aligned}\\n\\\\langle u, v\\\\rangle & =\\\\left\\\\langle u, a_{1} w_{1}+a_{2} w_{2}+\\\\cdots+a_{k} w_{k}\\\\right\\\\rangle=a_{1}\\\\left\\\\langle u, w_{1}\\\\right\\\\rangle+a_{2}\\\\left\\\\langle u, w_{2}\\\\right\\\\rangle+\\\\cdots+a_{k}\\\\left\\\\langle u, w_{k}\\\\right\\\\rangle \\\\\\\\\\n& =a_{1}(0)+a_{2}(0)+\\\\cdots+a_{k}(0)=0\\n\\\\end{aligned}\\n$$\\n\\nThus, $u \\\\in \\\\operatorname{span}(S)^{\\\\perp}$. Accordingly, $S^{\\\\perp} \\\\subseteq \\\\operatorname{span}(S)^{\\\\perp}$. Both inclusions give $S^{\\\\perp}=\\\\operatorname{span}(S)^{\\\\perp}$.\\n',\n",
       " '\\n7.15. Prove Theorem 7.5: Suppose $S$ is an orthogonal set of nonzero vectors. Then $S$ is linearly independent.\\n\\nSuppose $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{r}\\\\right\\\\}$ and suppose\\n\\n\\n\\\\begin{equation*}\\na_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{r} u_{r}=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nTaking the inner product of (1) with $u_{1}$, we get\\n\\n$$\\n\\\\begin{aligned}\\n0 & =\\\\left\\\\langle 0, u_{1}\\\\right\\\\rangle=\\\\left\\\\langle a_{1} u_{1}+a_{2} u_{2}+\\\\cdots+a_{r} u_{r}, u_{1}\\\\right\\\\rangle \\\\\\\\\\n& =a_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle+a_{2}\\\\left\\\\langle u_{2}, u_{1}\\\\right\\\\rangle+\\\\cdots+a_{r}\\\\left\\\\langle u_{r}, u_{1}\\\\right\\\\rangle \\\\\\\\\\n& =a_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle+a_{2} \\\\cdot 0+\\\\cdots+a_{r} \\\\cdot 0=a_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nBecause $u_{1} \\\\neq 0$, we have $\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle \\\\neq 0$. Thus, $a_{1}=0$. Similarly, for $i=2, \\\\ldots, r$, taking the inner product of (1) with $u_{i}$,\\n\\n$$\\n\\\\begin{aligned}\\n0 & =\\\\left\\\\langle 0, u_{i}\\\\right\\\\rangle=\\\\left\\\\langle a_{1} u_{1}+\\\\cdots+a_{r} u_{r}, u_{i}\\\\right\\\\rangle \\\\\\\\\\n& =a_{1}\\\\left\\\\langle u_{1}, u_{i}\\\\right\\\\rangle+\\\\cdots+a_{i}\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle+\\\\cdots+a_{r}\\\\left\\\\langle u_{r}, u_{i}\\\\right\\\\rangle=a_{i}\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nBut $\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle \\\\neq 0$, and hence, every $a_{i}=0$. Thus, $S$ is linearly independent.\\n',\n",
       " '\\n7.16. Prove Theorem 7.6 (Pythagoras): Suppose $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{r}\\\\right\\\\}$ is an orthogonal set of vectors. Then\\n\\n$$\\n\\\\left\\\\|u_{1}+u_{2}+\\\\cdots+u_{r}\\\\right\\\\|^{2}=\\\\left\\\\|u_{1}\\\\right\\\\|^{2}+\\\\left\\\\|u_{2}\\\\right\\\\|^{2}+\\\\cdots+\\\\left\\\\|u_{r}\\\\right\\\\|^{2}\\n$$\\n\\nExpanding the inner product, we have\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\|u_{1}+u_{2}+\\\\cdots+u_{r}\\\\right\\\\|^{2} & =\\\\left\\\\langle u_{1}+u_{2}+\\\\cdots+u_{r}, u_{1}+u_{2}+\\\\cdots+u_{r}\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle+\\\\left\\\\langle u_{2}, u_{2}\\\\right\\\\rangle+\\\\cdots+\\\\left\\\\langle u_{r}, u_{r}\\\\right\\\\rangle+\\\\sum_{i \\\\neq j}\\\\left\\\\langle u_{i}, u_{j}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThe theorem follows from the fact that $\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle=\\\\left\\\\|u_{i}\\\\right\\\\|^{2}$ and $\\\\left\\\\langle u_{i}, u_{j}\\\\right\\\\rangle=0$ for $i \\\\neq j$.\\n',\n",
       " '\\n7.17. Prove Theorem 7.7: Let $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ be an orthogonal basis of $V$. Then for any $v \\\\in V$,\\n\\n$$\\nv=\\\\frac{\\\\left\\\\langle v, u_{1}\\\\right\\\\rangle}{\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle} u_{1}+\\\\frac{\\\\left\\\\langle v, u_{2}\\\\right\\\\rangle}{\\\\left\\\\langle u_{2}, u_{2}\\\\right\\\\rangle} u_{2}+\\\\cdots+\\\\frac{\\\\left\\\\langle v, u_{n}\\\\right\\\\rangle}{\\\\left\\\\langle u_{n}, u_{n}\\\\right\\\\rangle} u_{n}\\n$$\\n\\nSuppose $v=k_{1} u_{1}+k_{2} u_{2}+\\\\cdots+k_{n} u_{n}$. Taking the inner product of both sides with $u_{1}$ yields\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle v, u_{1}\\\\right\\\\rangle & =\\\\left\\\\langle k_{1} u_{2}+k_{2} u_{2}+\\\\cdots+k_{n} u_{n}, u_{1}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle+k_{2}\\\\left\\\\langle u_{2}, u_{1}\\\\right\\\\rangle+\\\\cdots+k_{n}\\\\left\\\\langle u_{n}, u_{1}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle+k_{2} \\\\cdot 0+\\\\cdots+k_{n} \\\\cdot 0=k_{1}\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThus, $k_{1}=\\\\frac{\\\\left\\\\langle v, u_{1}\\\\right\\\\rangle}{\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle}$. Similarly, for $i=2, \\\\ldots, n$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle v, u_{i}\\\\right\\\\rangle & =\\\\left\\\\langle k_{1} u_{i}+k_{2} u_{2}+\\\\cdots+k_{n} u_{n}, u_{i}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}\\\\left\\\\langle u_{1}, u_{i}\\\\right\\\\rangle+k_{2}\\\\left\\\\langle u_{2}, u_{i}\\\\right\\\\rangle+\\\\cdots+k_{n}\\\\left\\\\langle u_{n}, u_{i}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1} \\\\cdot 0+\\\\cdots+k_{i}\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle+\\\\cdots+k_{n} \\\\cdot 0=k_{i}\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThus, $k_{i}=\\\\frac{\\\\left\\\\langle v, u_{i}\\\\right\\\\rangle}{\\\\left\\\\langle u_{1}, u_{i}\\\\right\\\\rangle}$. Substituting for $k_{i}$ in the equation $v=k_{1} u_{1}+\\\\cdots+k_{n} u_{n}$, we obtain the desired result.\\n',\n",
       " '\\n7.18. Suppose $E=\\\\left\\\\{e_{1}, e_{2}, \\\\ldots, e_{n}\\\\right\\\\}$ is an orthonormal basis of $V$. Prove\\n\\n(a) For any $u \\\\in V$, we have $u=\\\\left\\\\langle u, e_{1}\\\\right\\\\rangle e_{1}+\\\\left\\\\langle u, e_{2}\\\\right\\\\rangle e_{2}+\\\\cdots+\\\\left\\\\langle u, e_{n}\\\\right\\\\rangle e_{n}$.\\n\\n(b) $\\\\left\\\\langle a_{1} e_{1}+\\\\cdots+a_{n} e_{n}, \\\\quad b_{1} e_{1}+\\\\cdots+b_{n} e_{n}\\\\right\\\\rangle=a_{1} b_{1}+a_{2} b_{2}+\\\\cdots+a_{n} b_{n}$.\\n\\n(c) For any $u, v \\\\in V$, we have $\\\\langle u, v\\\\rangle=\\\\left\\\\langle u, e_{1}\\\\right\\\\rangle\\\\left\\\\langle v, e_{1}\\\\right\\\\rangle+\\\\cdots+\\\\left\\\\langle u, e_{n}\\\\right\\\\rangle\\\\left\\\\langle v, e_{n}\\\\right\\\\rangle$.\\n\\n(a) Suppose $u=k_{1} e_{1}+k_{2} e_{2}+\\\\cdots+k_{n} e_{n}$. Taking the inner product of $u$ with $e_{1}$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle u, e_{1}\\\\right\\\\rangle & =\\\\left\\\\langle k_{1} e_{1}+k_{2} e_{2}+\\\\cdots+k_{n} e_{n}, \\\\quad e_{1}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}\\\\left\\\\langle e_{1}, e_{1}\\\\right\\\\rangle+k_{2}\\\\left\\\\langle e_{2}, e_{1}\\\\right\\\\rangle+\\\\cdots+k_{n}\\\\left\\\\langle e_{n}, e_{1}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}(1)+k_{2}(0)+\\\\cdots+k_{n}(0)=k_{1}\\n\\\\end{aligned}\\n$$\\n\\nSimilarly, for $i=2, \\\\ldots, n$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle u, e_{i}\\\\right\\\\rangle & =\\\\left\\\\langle k_{1} e_{1}+\\\\cdots+k_{i} e_{i}+\\\\cdots+k_{n} e_{n}, \\\\quad e_{i}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}\\\\left\\\\langle e_{1}, e_{i}\\\\right\\\\rangle+\\\\cdots+k_{i}\\\\left\\\\langle e_{i}, e_{i}\\\\right\\\\rangle+\\\\cdots+k_{n}\\\\left\\\\langle e_{n}, e_{i}\\\\right\\\\rangle \\\\\\\\\\n& =k_{1}(0)+\\\\cdots+k_{i}(1)+\\\\cdots+k_{n}(0)=k_{i}\\n\\\\end{aligned}\\n$$\\n\\nSubstituting $\\\\left\\\\langle u, e_{i}\\\\right\\\\rangle$ for $k_{i}$ in the equation $u=k_{1} e_{1}+\\\\cdots+k_{n} e_{n}$, we obtain the desired result.\\n\\n(b) We have\\n\\n$$\\n\\\\left\\\\langle\\\\sum_{i=1}^{n} a_{i} e_{i}, \\\\quad \\\\sum_{j=1}^{n} b_{j} e_{j}\\\\right\\\\rangle=\\\\sum_{i, j=1}^{n} a_{i} b_{j}\\\\left\\\\langle e_{i}, e_{j}\\\\right\\\\rangle=\\\\sum_{i=1}^{n} a_{i} b_{i}\\\\left\\\\langle e_{i}, e_{i}\\\\right\\\\rangle+\\\\sum_{i \\\\neq j} a_{i} b_{j}\\\\left\\\\langle e_{i}, e_{j}\\\\right\\\\rangle\\n$$\\n\\nBut $\\\\left\\\\langle e_{i}, e_{j}\\\\right\\\\rangle=0$ for $i \\\\neq j$, and $\\\\left\\\\langle e_{i}, e_{j}\\\\right\\\\rangle=1$ for $i=j$. Hence, as required,\\n\\n$$\\n\\\\left\\\\langle\\\\sum_{i=1}^{n} a_{i} e_{i}, \\\\quad \\\\sum_{j=1}^{n} b_{j} e_{j}\\\\right\\\\rangle=\\\\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\\\\cdots+a_{n} b_{n}\\n$$\\n\\n(c) By part (a), we have\\n\\n$$\\nu=\\\\left\\\\langle u, e_{1}\\\\right\\\\rangle e_{1}+\\\\cdots+\\\\left\\\\langle u, e_{n}\\\\right\\\\rangle e_{n} \\\\quad \\\\text { and } \\\\quad v=\\\\left\\\\langle v, e_{1}\\\\right\\\\rangle e_{1}+\\\\cdots+\\\\left\\\\langle v, e_{n}\\\\right\\\\rangle e_{n}\\n$$\\n\\nThus, by part (b),\\n\\n$$\\n\\\\langle u, v\\\\rangle=\\\\left\\\\langle u, e_{1}\\\\right\\\\rangle\\\\left\\\\langle v, e_{1}\\\\right\\\\rangle+\\\\left\\\\langle u, e_{2}\\\\right\\\\rangle\\\\left\\\\langle v, e_{2}\\\\right\\\\rangle+\\\\cdots+\\\\left\\\\langle u, e_{n}\\\\right\\\\rangle\\\\left\\\\langle v, e_{n}\\\\right\\\\rangle\\n$$\\n\\n\\n\\\\section*{Projections, Gram-Schmidt Algorithm, Applications}\\n',\n",
       " '7.19. Suppose $w \\\\neq 0$. Let $v$ be any vector in $V$. Show that\\n\\n$$\\nc=\\\\frac{\\\\langle v, w\\\\rangle}{\\\\langle w, w\\\\rangle}=\\\\frac{\\\\langle v, w\\\\rangle}{\\\\|w\\\\|^{2}}\\n$$\\n\\nis the unique scalar such that $v^{\\\\prime}=v-c w$ is orthogonal to $w$.\\n\\nIn order for $v^{\\\\prime}$ to be orthogonal to $w$ we must have\\n\\n$$\\n\\\\langle v-c w, \\\\quad w\\\\rangle=0 \\\\quad \\\\text { or } \\\\quad\\\\langle v, w\\\\rangle-c\\\\langle w, w\\\\rangle=0 \\\\quad \\\\text { or } \\\\quad\\\\langle v, w\\\\rangle=c\\\\langle w, w\\\\rangle\\n$$\\n\\nThus, $c \\\\frac{\\\\langle v, w\\\\rangle}{\\\\langle w, w\\\\rangle}$. Conversely, suppose $c=\\\\frac{\\\\langle v, w\\\\rangle}{\\\\langle w, w\\\\rangle}$. Then\\n\\n$$\\n\\\\langle v-c w, \\\\quad w\\\\rangle=\\\\langle v, w\\\\rangle-c\\\\langle w, w\\\\rangle=\\\\langle v, w\\\\rangle-\\\\frac{\\\\langle v, w\\\\rangle}{\\\\langle w, w\\\\rangle}\\\\langle w, w\\\\rangle=0\\n$$\\n',\n",
       " '\\n7.20. Find the Fourier coefficient $c$ and the projection of $v=(1,-2,3,-4)$ along $w=(1,2,1,2)$ in $\\\\mathbf{R}^{4}$.\\n\\nCompute $\\\\langle v, w\\\\rangle=1-4+3-8=-8$ and $\\\\|w\\\\|^{2}=1+4+1+4=10$. Then\\n\\n$$\\nc=-\\\\frac{8}{10}=-\\\\frac{4}{5} \\\\quad \\\\text { and } \\\\quad \\\\operatorname{proj}(v, w)=c w=\\\\left(-\\\\frac{4}{5},-\\\\frac{8}{5},-\\\\frac{4}{5},-\\\\frac{8}{5}\\\\right)\\n$$\\n',\n",
       " '\\n7.21. Consider the subspace $U$ of $\\\\mathbf{R}^{4}$ spanned by the vectors:\\n\\n$$\\nv_{1}=(1,1,1,1), \\\\quad v_{2}=(1,1,2,4), \\\\quad v_{3}=(1,2,-4,-3)\\n$$\\n\\nFind (a) an orthogonal basis of $U$; (b) an orthonormal basis of $U$.\\n\\n(a) Use the Gram-Schmidt algorithm. Begin by setting $w_{1}=u=(1,1,1,1)$. Next find\\n\\n$$\\nv_{2}-\\\\frac{\\\\left\\\\langle v_{2}, w_{1}\\\\right\\\\rangle}{\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle} w_{1}=(1,1,2,4)-\\\\frac{8}{4}(1,1,1,1)=(-1,-1,0,2)\\n$$\\n\\nSet $w_{2}=(-1,-1,0,2)$. Then find\\n\\n$$\\n\\\\begin{aligned}\\nv_{3}-\\\\frac{\\\\left\\\\langle v_{3}, w_{1}\\\\right\\\\rangle}{\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle} w_{1}-\\\\frac{\\\\left\\\\langle v_{3}, w_{2}\\\\right\\\\rangle}{\\\\left\\\\langle w_{2}, w_{2}\\\\right\\\\rangle} w_{2} & =(1,2,-4,-3)-\\\\frac{(-4)}{4}(1,1,1,1)-\\\\frac{(-9)}{6}(-1,-1,0,2) \\\\\\\\\\n& =\\\\left(\\\\frac{1}{2}, \\\\frac{3}{2},-3,1\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nClear fractions to obtain $w_{3}=(1,3,-6,2)$. Then $w_{1}, w_{2}, w_{3}$ form an orthogonal basis of $U$.\\\\\\\\\\n(b) Normalize the orthogonal basis consisting of $w_{1}, w_{2}, w_{3}$. Because $\\\\left\\\\|w_{1}\\\\right\\\\|^{2}=4,\\\\left\\\\|w_{2}\\\\right\\\\|^{2}=6$, and $\\\\left\\\\|w_{3}\\\\right\\\\|^{2}=50$, the following vectors form an orthonormal basis of $U$ :\\n\\n$$\\nu_{1}=\\\\frac{1}{2}(1,1,1,1), \\\\quad u_{2}=\\\\frac{1}{\\\\sqrt{6}}(-1,-1,0,2), \\\\quad u_{3}=\\\\frac{1}{5 \\\\sqrt{2}}(1,3,-6,2)\\n$$\\n',\n",
       " '\\n7.22. Consider the vector space $\\\\mathbf{P}(t)$ with inner product $\\\\langle f, g\\\\rangle=\\\\int_{0}^{1} f(t) g(t) d t$. Apply the GramSchmidt algorithm to the set $\\\\left\\\\{1, t, t^{2}\\\\right\\\\}$ to obtain an orthogonal set $\\\\left\\\\{f_{0}, f_{1}, f_{2}\\\\right\\\\}$ with integer coefficients.\\n\\nFirst set $f_{0}=1$. Then find\\n\\n$$\\nt-\\\\frac{\\\\langle t, 1\\\\rangle}{\\\\langle 1,1\\\\rangle} \\\\cdot 1=t-\\\\frac{\\\\frac{1}{2}}{1} \\\\cdot 1=t-\\\\frac{1}{2}\\n$$\\n\\nClear fractions to obtain $f_{1}=2 t-1$. Then find\\n\\n$$\\nt^{2}-\\\\frac{\\\\left\\\\langle t^{2}, 1\\\\right\\\\rangle}{\\\\langle 1,1\\\\rangle}(1)-\\\\frac{\\\\left\\\\langle t^{2}, 2 t-1\\\\right\\\\rangle}{\\\\langle 2 t-1,2 t-1\\\\rangle}(2 t-1)=t^{2}-\\\\frac{\\\\frac{1}{3}}{1}(1)-\\\\frac{\\\\frac{1}{6}}{\\\\frac{1}{3}}(2 t-1)=t^{2}-t+\\\\frac{1}{6}\\n$$\\n\\nClear fractions to obtain $f_{2}=6 t^{2}-6 t+1$. Thus, $\\\\left\\\\{1,2 t-1,6 t^{2}-6 t+1\\\\right\\\\}$ is the required orthogonal set.\\n',\n",
       " '\\n7.23. Suppose $v=(1,3,5,7)$. Find the projection of $v$ onto $W$ or, in other words, find $w \\\\in W$ that minimizes $\\\\|v-w\\\\|$, where $W$ is the subspance of $\\\\mathbf{R}^{4}$ spanned by\\n\\n(a) $u_{1}=(1,1,1,1)$ and $u_{2}=(1,-3,4,-2)$,\\n\\n(b) $v_{1}=(1,1,1,1)$ and $v_{2}=(1,2,3,2)$.\\n\\n(a) Because $u_{1}$ and $u_{2}$ are orthogonal, we need only compute the Fourier coefficients:\\n\\n$$\\n\\\\begin{aligned}\\n& c_{1}=\\\\frac{\\\\left\\\\langle v, u_{1}\\\\right\\\\rangle}{\\\\left\\\\langle u_{1}, u_{1}\\\\right\\\\rangle}=\\\\frac{1+3+5+7}{1+1+1+1}=\\\\frac{16}{4}=4 \\\\\\\\\\n& c_{2}=\\\\frac{\\\\left\\\\langle v, u_{2}\\\\right\\\\rangle}{\\\\left\\\\langle u_{2}, u_{2}\\\\right\\\\rangle}=\\\\frac{1-9+20-14}{1+9+16+4}=\\\\frac{-2}{30}=-\\\\frac{1}{15}\\n\\\\end{aligned}\\n$$\\n\\nThen $w=\\\\operatorname{proj}(v, W)=c_{1} u_{1}+c_{2} u_{2}=4(1,1,1,1)-\\\\frac{1}{15}(1,-3,4,-2)=\\\\left(\\\\frac{59}{15}, \\\\frac{63}{5}, \\\\frac{56}{15}, \\\\frac{62}{15}\\\\right)$.\\n\\n(b) Because $v_{1}$ and $v_{2}$ are not orthogonal, first apply the Gram-Schmidt algorithm to find an orthogonal basis for $W$. Set $w_{1}=v_{1}=(1,1,1,1)$. Then find\\n\\n$$\\nv_{2}-\\\\frac{\\\\left\\\\langle v_{2}, w_{1}\\\\right\\\\rangle}{\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle} w_{1}=(1,2,3,2)-\\\\frac{8}{4}(1,1,1,1)=(-1,0,1,0)\\n$$\\n\\nSet $w_{2}=(-1,0,1,0)$. Now compute\\n\\n$$\\n\\\\begin{aligned}\\n& c_{1}=\\\\frac{\\\\left\\\\langle v, w_{1}\\\\right\\\\rangle}{\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle}=\\\\frac{1+3+5+7}{1+1+1+1}=\\\\frac{16}{4}=4 \\\\\\\\\\n& c_{2}=\\\\frac{\\\\left\\\\langle v, w_{2}\\\\right\\\\rangle}{\\\\left\\\\langle w_{2}, w_{2}\\\\right\\\\rangle}-\\\\frac{-1+0+5+0}{1+0+1+0}=\\\\frac{-6}{2}=-3\\n\\\\end{aligned}\\n$$\\n\\nThen $w=\\\\operatorname{proj}(v, W)=c_{1} w_{1}+c_{2} w_{2}=4(1,1,1,1)-3(-1,0,1,0)=(7,4,1,4)$.\\n',\n",
       " '\\n7.24. Suppose $w_{1}$ and $w_{2}$ are nonzero orthogonal vectors. Let $v$ be any vector in $V$. Find $c_{1}$ and $c_{2}$ so that $v^{\\\\prime}$ is orthogonal to $w_{1}$ and $w_{2}$, where $v^{\\\\prime}=v-c_{1} w_{1}-c_{2} w_{2}$.\\n\\nIf $v^{\\\\prime}$ is orthogonal to $w_{1}$, then\\n\\n$$\\n\\\\begin{aligned}\\n0 & =\\\\left\\\\langle v-c_{1} w_{1}-c_{2} w_{2}, \\\\quad w_{1}\\\\right\\\\rangle=\\\\left\\\\langle v, w_{1}\\\\right\\\\rangle-c_{1}\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle-c_{2}\\\\left\\\\langle w_{2}, w_{1}\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle v, w_{1}\\\\right\\\\rangle-c_{1}\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle-c_{2} 0=\\\\left\\\\langle v, w_{1}\\\\right\\\\rangle-c_{1}\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThus, $c_{1}=\\\\left\\\\langle v, w_{1}\\\\right\\\\rangle /\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle$. (That is, $c_{1}$ is the component of $v$ along $w_{1}$.) Similarly, if $v^{\\\\prime}$ is orthogonal to $w_{2}$, then\\n\\n$$\\n0=\\\\left\\\\langle v-c_{1} w_{1}-c_{2} w_{2}, \\\\quad w_{2}\\\\right\\\\rangle=\\\\left\\\\langle v, w_{2}\\\\right\\\\rangle-c_{2}\\\\left\\\\langle w_{2}, w_{2}\\\\right\\\\rangle\\n$$\\n\\nThus, $c_{2}=\\\\left\\\\langle v, w_{2}\\\\right\\\\rangle /\\\\left\\\\langle w_{2}, w_{2}\\\\right\\\\rangle$. (That is, $c_{2}$ is the component of $v$ along $w_{2}$.)\\n',\n",
       " '\\n7.25. Prove Theorem 7.8: Suppose $w_{1}, w_{2}, \\\\ldots, w_{r}$ form an orthogonal set of nonzero vectors in $V$. Let $v \\\\in V$. Define\\n\\n$$\\nv^{\\\\prime}=v-\\\\left(c_{1} w_{1}+c_{2} w_{2}+\\\\cdots+c_{r} w_{r}\\\\right), \\\\quad \\\\text { where } \\\\quad c_{i}=\\\\frac{\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle}{\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle}\\n$$\\n\\nThen $v^{\\\\prime}$ is orthogonal to $w_{1}, w_{2}, \\\\ldots, w_{r}$.\\n\\nFor $i=1,2, \\\\ldots, r$ and using $\\\\left\\\\langle w_{i}, w_{j}\\\\right\\\\rangle=0$ for $i \\\\neq j$, we have\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle v-c_{1} w_{1}-c_{2} x_{2}-\\\\cdots-c_{r} w_{r}, w_{i}\\\\right\\\\rangle & =\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle-c_{1}\\\\left\\\\langle w_{1}, w_{i}\\\\right\\\\rangle-\\\\cdots-c_{i}\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle-\\\\cdots-c_{r}\\\\left\\\\langle w_{r}, w_{i}\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle-c_{1} \\\\cdot 0-\\\\cdots-c_{i}\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle-\\\\cdots-c_{r} \\\\cdot 0 \\\\\\\\\\n& =\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle-c_{i}\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle=\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle-\\\\frac{\\\\left\\\\langle v, w_{i}\\\\right\\\\rangle}{\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle}\\\\left\\\\langle w_{i}, w_{i}\\\\right\\\\rangle=0\\n\\\\end{aligned}\\n$$\\n\\nThe theorem is proved.\\n',\n",
       " '\\n7.26. Prove Theorem 7.9: Let $\\\\left\\\\{v_{1}, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ be any basis of an inner product space $V$. Then there exists an orthonormal basis $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ of $V$ such that the change-of-basis matrix from $\\\\left\\\\{v_{i}\\\\right\\\\}$ to $\\\\left\\\\{u_{i}\\\\right\\\\}$ is triangular; that is, for $k=1,2, \\\\ldots, n$,\\n\\n$$\\nu_{k}=a_{k 1} v_{1}+a_{k 2} v_{2}+\\\\cdots+a_{k k} v_{k}\\n$$\\n\\nThe proof uses the Gram-Schmidt algorithm and Remarks 1 and 3 of Section 7.7. That is, apply the algorithm to $\\\\left\\\\{v_{i}\\\\right\\\\}$ to obtain an orthogonal basis $\\\\left\\\\{w_{i}, \\\\ldots, w_{n}\\\\right\\\\}$, and then normalize $\\\\left\\\\{w_{i}\\\\right\\\\}$ to obtain an orthonormal basis $\\\\left\\\\{u_{i}\\\\right\\\\}$ of $V$. The specific algorithm guarantees that each $w_{k}$ is a linear combination of $v_{1}, \\\\ldots, v_{k}$, and hence, each $u_{k}$ is a linear combination of $v_{1}, \\\\ldots, v_{k}$.\\n',\n",
       " '\\n7.27. Prove Theorem 7.10: Suppose $S=\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{r}\\\\right\\\\}$, is an orthogonal basis for a subspace $W$ of $V$. Then one may extend $S$ to an orthogonal basis for $V$; that is, one may find vectors $w_{r+1}, \\\\ldots, w_{r}$ such that $\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}$ is an orthogonal basis for $V$.\\n\\nExtend $S$ to a basis $S^{\\\\prime}=\\\\left\\\\{w_{1}, \\\\ldots, w_{r}, v_{r+1}, \\\\ldots, v_{n}\\\\right\\\\}$ for $V$. Applying the Gram-Schmidt algorithm to $S^{\\\\prime}$, we first obtain $w_{1}, w_{2}, \\\\ldots, w_{r}$ because $S$ is orthogonal, and then we obtain vectors $w_{r+1}, \\\\ldots, w_{n}$, where $\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}$ is an orthogonal basis for $V$. Thus, the theorem is proved.\\n',\n",
       " '\\n7.28. Prove Theorem 7.4: Let $W$ be a subspace of $V$. Then $V=W \\\\oplus W^{\\\\perp}$.\\n\\nBy Theorem 7.9, there exists an orthogonal basis $\\\\left\\\\{u_{1}, \\\\ldots, u_{r}\\\\right\\\\}$ of $W$, and by Theorem 7.10 we can extend it to an orthogonal basis $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ of $V$. Hence, $u_{r+1}, \\\\ldots, u_{n} \\\\in W^{\\\\perp}$. If $v \\\\in V$, then\\n\\n$$\\nv=a_{1} u_{1}+\\\\cdots+a_{n} u_{n}, \\\\text { where } a_{1} u_{1}+\\\\cdots+a_{r} u_{r} \\\\in W \\\\text { and } a_{r+1} u_{r+1}+\\\\cdots+a_{n} u_{n} \\\\in W^{\\\\perp}\\n$$\\n\\nAccordingly, $V=W+W^{\\\\perp}$.\\n\\nOn the other hand, if $w \\\\in W \\\\cap W^{\\\\perp}$, then $\\\\langle w, w\\\\rangle=0$. This yields $w=0$. Hence, $W \\\\cap W^{\\\\perp}=\\\\{0\\\\}$.\\n\\nThe two conditions $V=W+W^{\\\\perp}$ and $W \\\\cap W^{\\\\perp}=\\\\{0\\\\}$ give the desired result $V=W \\\\oplus W^{\\\\perp}$.\\n\\nRemark: Note that we have proved the theorem for the case that $V$ has finite dimension. We remark that the theorem also holds for spaces of arbitrary dimension.\\n',\n",
       " '\\n7.29. Suppose $W$ is a subspace of a finite-dimensional space $V$. Prove that $W=W^{\\\\perp \\\\perp}$.\\n\\nBy Theorem 7.4, $V=W \\\\oplus W^{\\\\perp}$, and also $V=W^{\\\\perp} \\\\oplus W^{\\\\perp \\\\perp}$. Hence,\\n\\n$$\\n\\\\operatorname{dim} W=\\\\operatorname{dim} V-\\\\operatorname{dim} W^{\\\\perp} \\\\quad \\\\text { and } \\\\quad \\\\operatorname{dim} W^{\\\\perp \\\\perp}=\\\\operatorname{dim} V-\\\\operatorname{dim} W^{\\\\perp}\\n$$\\n\\nThis yields $\\\\operatorname{dim} W=\\\\operatorname{dim} W^{\\\\perp \\\\perp}$. But $W \\\\subseteq W^{\\\\perp \\\\perp}$ (see Problem 7.14). Hence, $W=W^{\\\\perp \\\\perp}$, as required.\\n',\n",
       " '\\n7.30. Prove the following: Suppose $w_{1}, w_{2}, \\\\ldots, w_{r}$ form an orthogonal set of nonzero vectors in $V$. Let $v$ be any vector in $V$ and let $c_{i}$ be the component of $v$ along $w_{i}$. Then, for any scalars $a_{1}, \\\\ldots, a_{r}$, we have\\n\\n$$\\n\\\\left\\\\|v-\\\\sum_{k=1}^{r} c_{k} w_{k}\\\\right\\\\| \\\\leq\\\\left\\\\|v-\\\\sum_{k=1}^{r} a_{k} w_{k}\\\\right\\\\|\\n$$\\n\\nThat is, $\\\\sum c_{i} w_{i}$ is the closest approximation to $v$ as a linear combination of $w_{1}, \\\\ldots, w_{r}$.\\n\\nBy Theorem 7.8, $v-\\\\sum c_{k} w_{k}$ is orthogonal to every $w_{i}$ and hence orthogonal to any linear combination of $w_{1}, w_{2}, \\\\ldots, w_{r}$. Therefore, using the Pythagorean theorem and summing from $k=1$ to $r$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\|v-\\\\sum a_{k} w_{k}\\\\right\\\\|^{2} & =\\\\left\\\\|v-\\\\sum c_{k} w_{k}+\\\\sum\\\\left(c_{k}-a_{k}\\\\right) w_{k}\\\\right\\\\|^{2}=\\\\left\\\\|v-\\\\sum c_{k} w_{k}\\\\right\\\\|^{2}+\\\\left\\\\|\\\\sum\\\\left(c_{k}-a_{k}\\\\right) w_{k}\\\\right\\\\|^{2} \\\\\\\\\\n& \\\\geq\\\\left\\\\|v-\\\\sum c_{k} w_{k}\\\\right\\\\|^{2}\\n\\\\end{aligned}\\n$$\\n\\nThe square root of both sides gives our theorem.\\n',\n",
       " \"\\n7.31. Suppose $\\\\left\\\\{e_{1}, e_{2}, \\\\ldots, e_{r}\\\\right\\\\}$ is an orthonormal set of vectors in $V$. Let $v$ be any vector in $V$ and let $c_{i}$ be the Fourier coefficient of $v$ with respect to $u_{i}$. Prove Bessel's inequality:\\n\\n$$\\n\\\\sum_{k=1}^{r} c_{k}^{2} \\\\leq\\\\|v\\\\|^{2}\\n$$\\n\\nNote that $c_{i}=\\\\left\\\\langle v, e_{i}\\\\right\\\\rangle$, because $\\\\left\\\\|e_{i}\\\\right\\\\|=1$. Then, using $\\\\left\\\\langle e_{i}, e_{j}\\\\right\\\\rangle=0$ for $i \\\\neq j$ and summing from $k=1$ to $r$, we get\\n\\n$$\\n\\\\begin{aligned}\\n0 & \\\\leq\\\\left\\\\langle v-\\\\sum c_{k} e_{k}, \\\\quad v-\\\\sum c_{k}, e_{k}\\\\right\\\\rangle=\\\\langle v, v\\\\rangle-2\\\\left\\\\langle v, \\\\quad \\\\sum c_{k} e_{k}\\\\right\\\\rangle+\\\\sum c_{k}^{2}=\\\\langle v, v\\\\rangle-\\\\sum 2 c_{k}\\\\left\\\\langle v, e_{k}\\\\right\\\\rangle+\\\\sum c_{k}^{2} \\\\\\\\\\n& =\\\\langle v, v\\\\rangle-\\\\sum 2 c_{k}^{2}+\\\\sum c_{k}^{2}=\\\\langle v, v\\\\rangle-\\\\sum c_{k}^{2}\\n\\\\end{aligned}\\n$$\\n\\nThis gives us our inequality.\\n\\n\\n\\\\section*{Orthogonal Matrices}\\n\",\n",
       " '7.32. Find an orthogonal matrix $P$ whose first row is $u_{1}=\\\\left(\\\\frac{1}{3}, \\\\frac{2}{3}, \\\\frac{2}{3}\\\\right)$.\\n\\nFirst find a nonzero vector $w_{2}=(x, y, z)$ that is orthogonal to $u_{1}$-that is, for which\\n\\n$$\\n0=\\\\left\\\\langle u_{1}, w_{2}\\\\right\\\\rangle=\\\\frac{x}{3}+\\\\frac{2 y}{3}+\\\\frac{2 z}{3}=0 \\\\quad \\\\text { or } \\\\quad x+2 y+2 z=0\\n$$\\n\\nOne such solution is $w_{2}=(0,1,-1)$. Normalize $w_{2}$ to obtain the second row of $P$ :\\n\\n$$\\nu_{2}=(0,1 / \\\\sqrt{2},-1 / \\\\sqrt{2})\\n$$\\n\\nNext find a nonzero vector $w_{3}=(x, y, z)$ that is orthogonal to both $u_{1}$ and $u_{2}$-that is, for which\\n\\n$$\\n\\\\begin{array}{lll}\\n0=\\\\left\\\\langle u_{1}, w_{3}\\\\right\\\\rangle=\\\\frac{x}{3}+\\\\frac{2 y}{3}+\\\\frac{2 z}{3}=0 & \\\\text { or } & x+2 y+2 z=0 \\\\\\\\\\n0=\\\\left\\\\langle u_{2}, w_{3}\\\\right\\\\rangle=\\\\frac{y}{\\\\sqrt{2}}-\\\\frac{y}{\\\\sqrt{2}}=0 & \\\\text { or } & y-z=0\\n\\\\end{array}\\n$$\\n\\nSet $z=-1$ and find the solution $w_{3}=(4,-1,-1)$. Normalize $w_{3}$ and obtain the third row of $P$; that is,\\n\\nThus,\\n\\n$$\\n\\\\begin{gathered}\\nu_{3}=(4 / \\\\sqrt{18},-1 / \\\\sqrt{18},-1 / \\\\sqrt{18}) . \\\\\\\\\\nP=\\\\left[\\\\begin{array}{ccc}\\n\\\\frac{1}{3} & \\\\frac{2}{3} & \\\\frac{2}{3} \\\\\\\\\\n0 & 1 / \\\\sqrt{2} & -1 / \\\\sqrt{2} \\\\\\\\\\n4 / 3 \\\\sqrt{2} & -1 / 3 \\\\sqrt{2} & -1 / 3 \\\\sqrt{2}\\n\\\\end{array}\\\\right]\\n\\\\end{gathered}\\n$$\\n\\nWe emphasize that the above matrix $P$ is not unique.\\n',\n",
       " '\\n7.33. Let $A=\\\\left[\\\\begin{array}{rrr}1 & 1 & -1 \\\\\\\\ 1 & 3 & 4 \\\\\\\\ 7 & -5 & 2\\\\end{array}\\\\right]$. Determine whether or not: (a) the rows of $A$ are orthogonal;\\n\\n(b) $A$ is an orthogonal matrix; (c) the columns of $A$ are orthogonal.\\n\\n(a) Yes, because $(1,1,-1) \\\\cdot(1,3,4)=1+3-4=0, \\\\quad(1,1-1) \\\\cdot(7,-5,2)=7-5-2=0$, and $(1,3,4) \\\\cdot(7,-5,2)=7-15+8=0$.\\n\\n(b) No, because the rows of $A$ are not unit vectors, for example, $(1,1,-1)^{2}=1+1+1=3$.\\n\\n(c) No; for example, $(1,1,7) \\\\cdot(1,3,-5)=1+3-35=-31 \\\\neq 0$.\\n',\n",
       " '\\n7.34. Let $B$ be the matrix obtained by normalizing each row $A$ in Problem 7.33 .\\n\\n(a) Find $B$.\\n\\n(b) Is $B$ an orthogonal matrix?\\n\\n(c) Are the columns of $B$ orthogonal?\\\\\\\\\\n(a) We have\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\|(1,1,-1)\\\\|^{2}=1+1+1=3, \\\\quad\\\\|(1,3,4)\\\\|^{2}=1+9+16=26 \\\\\\\\\\n& \\\\|(7,-5,2)\\\\|^{2}=49+25+4=78 \\\\\\\\\\n& B=\\\\left[\\\\begin{array}{ccc}\\n1 / \\\\sqrt{3} & 1 / \\\\sqrt{3} & -1 / \\\\sqrt{3} \\\\\\\\\\n1 / \\\\sqrt{26} & 3 / \\\\sqrt{26} & 4 / \\\\sqrt{26} \\\\\\\\\\n7 / \\\\sqrt{78} & -5 / \\\\sqrt{78} & 2 / \\\\sqrt{78}\\n\\\\end{array}\\\\right]\\n\\\\end{aligned}\\n$$\\n\\nThus,\\n\\n(b) Yes, because the rows of $B$ are still orthogonal and are now unit vectors.\\n\\n(c) Yes, because the rows of $B$ form an orthonormal set of vectors. Then, by Theorem 7.11, the columns of $B$ must automatically form an orthonormal set.\\n',\n",
       " '\\n7.35. Prove each of the following:\\n\\n(a) $P$ is orthogonal if and only if $P^{T}$ is orthogonal.\\n\\n(b) If $P$ is orthogonal, then $P^{-1}$ is orthogonal.\\n\\n(c) If $P$ and $Q$ are orthogonal, then $P Q$ is orthogonal.\\n\\n(a) We have $\\\\left(P^{T}\\\\right)^{T}=P$. Thus, $P$ is orthogonal if and only if $P P^{T}=I$ if and only if $P^{T T} P^{T}=I$ if and only if $P^{T}$ is orthogonal.\\n\\n(b) We have $P^{T}=P^{-1}$, because $P$ is orthogonal. Thus, by part (a), $P^{-1}$ is orthogonal.\\n\\n(c) We have $P^{T}=P^{-1}$ and $Q^{T}=Q^{-1}$. Thus, $(P Q)(P Q)^{T}=P Q Q^{T} P^{T}=P Q Q^{-1} P^{-1}=I$. Therefore, $(P Q)^{T}=(P Q)^{-1}$, and so $P Q$ is orthogonal.\\n',\n",
       " '\\n7.36. Suppose $P$ is an orthogonal matrix. Show that\\n\\n(a) $\\\\langle P u, P v\\\\rangle=\\\\langle u, v\\\\rangle$ for any $u, v \\\\in V$;\\n\\n(b) $\\\\|P u\\\\|=\\\\|u\\\\|$ for every $u \\\\in V$.\\n\\nUse $P^{T} P=I$ and $\\\\langle u, v\\\\rangle=u^{T} v$.\\n\\n(a) $\\\\langle P u, P v\\\\rangle=(P u)^{T}(P v)=u^{T} P^{T} P v=u^{T} v=\\\\langle u, v\\\\rangle$.\\n\\n(b) We have\\n\\n$$\\n\\\\|P u\\\\|^{2}=\\\\langle P u, P u\\\\rangle=u^{T} P^{T} P u=u^{T} u=\\\\langle u, u\\\\rangle=\\\\|u\\\\|^{2}\\n$$\\n\\nTaking the square root of both sides gives our result.\\n',\n",
       " '\\n7.37. Prove Theorem 7.12: Suppose $E=\\\\left\\\\{e_{i}\\\\right\\\\}$ and $E^{\\\\prime}=\\\\left\\\\{e_{i}^{\\\\prime}\\\\right\\\\}$ are orthonormal bases of $V$. Let $P$ be the change-of-basis matrix from $E$ to $E^{\\\\prime}$. Then $P$ is orthogonal.\\n\\nSuppose\\n\\n\\n\\\\begin{equation*}\\ne_{i}^{\\\\prime}=b_{i 1} e_{1}+b_{i 2} e_{2}+\\\\cdots+b_{i n} e_{n}, \\\\quad i=1, \\\\ldots, n \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nUsing Problem 7.18(b) and the fact that $E^{\\\\prime}$ is orthonormal, we get\\n\\n\\n\\\\begin{equation*}\\n\\\\delta_{i j}=\\\\left\\\\langle e_{i}^{\\\\prime}, e_{j}^{\\\\prime}\\\\right\\\\rangle=b_{i 1} b_{j 1}+b_{i 2} b_{j 2}+\\\\cdots+b_{i n} b_{j n} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nLet $B=\\\\left[b_{i j}\\\\right]$ be the matrix of the coefficients in (1). (Then $P=B^{T}$.) Suppose $B B^{T}=\\\\left[c_{i j}\\\\right]$. Then\\n\\n\\n\\\\begin{equation*}\\nc_{i j}=b_{i 1} b_{j 1}+b_{i 2} b_{j 2}+\\\\cdots+b_{i n} b_{j n} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nBy (2) and (3), we have $c_{i j}=\\\\delta_{i j}$. Thus, $B B^{T}=I$. Accordingly, $B$ is orthogonal, and hence, $P=B^{T}$ is orthogonal.\\n',\n",
       " '\\n7.38. Prove Theorem 7.13: Let $\\\\left\\\\{e_{1}, \\\\ldots, e_{n}\\\\right\\\\}$ be an orthonormal basis of an inner product space $V$. Let $P=\\\\left[a_{i j}\\\\right]$ be an orthogonal matrix. Then the following $n$ vectors form an orthonormal basis for $V$ :\\n\\n$$\\ne_{i}^{\\\\prime}=a_{1 i} e_{1}+a_{2 i} e_{2}+\\\\cdots+a_{n i} e_{n}, \\\\quad i=1,2, \\\\ldots, n\\n$$\\n\\nBecause $\\\\left\\\\{e_{i}\\\\right\\\\}$ is orthonormal, we get, by Problem 7.18(b),\\n\\n$$\\n\\\\left\\\\langle e_{i}^{\\\\prime}, e_{j}^{\\\\prime}\\\\right\\\\rangle=a_{1 i} a_{1 j}+a_{2 i} a_{2 j}+\\\\cdots+a_{n i} a_{n j}=\\\\left\\\\langle C_{i}, C_{j}\\\\right\\\\rangle\\n$$\\n\\nwhere $C_{i}$ denotes the $i$ th column of the orthogonal matrix $P=\\\\left[a_{i j}\\\\right]$. Because $P$ is orthogonal, its columns form an orthonormal set. This implies $\\\\left\\\\langle e_{i}^{\\\\prime}, e_{j}^{\\\\prime}\\\\right\\\\rangle=\\\\left\\\\langle C_{i}, C_{j}\\\\right\\\\rangle=\\\\delta_{i j}$. Thus, $\\\\left\\\\{e_{i}^{\\\\prime}\\\\right\\\\}$ is an orthonormal basis.\\n\\n\\n\\\\section*{Inner Products And Positive Definite Matrices}\\n',\n",
       " '7.39. Which of the following symmetric matrices are positive definite?\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{ll}3 & 4 \\\\\\\\ 4 & 5\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rr}8 & -3 \\\\\\\\ -3 & 2\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rr}2 & 1 \\\\\\\\ 1 & -3\\\\end{array}\\\\right]$,\\\\\\\\\\n(d) $D=\\\\left[\\\\begin{array}{ll}3 & 5 \\\\\\\\ 5 & 9\\\\end{array}\\\\right]$\\n\\nUse Theorem 7.14 that a $2 \\\\times 2$ real symmetric matrix is positive definite if and only if its diagonal entries are positive and if its determinant is positive.\\n\\n(a) No, because $|A|=15-16=-1$ is negative.\\n\\n(b) Yes.\\n\\n(c) No, because the diagonal entry -3 is negative.\\n\\n(d) Yes.\\n',\n",
       " '\\n7.40. Find the values of $k$ that make each of the following matrices positive definite:\\n\\n(a) $A=\\\\left[\\\\begin{array}{rr}2 & -4 \\\\\\\\ -4 & k\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{ll}4 & k \\\\\\\\ k & 9\\\\end{array}\\\\right]$, (c) $C=\\\\left[\\\\begin{array}{rr}k & 5 \\\\\\\\ 5 & -2\\\\end{array}\\\\right]$\\n\\n(a) First, $k$ must be positive. Also, $|A|=2 k-16$ must be positive; that is, $2 k-16>0$. Hence, $k>8$.\\n\\n(b) We need $|B|=36-k^{2}$ positive; that is, $36-k^{2}>0$. Hence, $k^{2}<36$ or $-6<k<6$.\\n\\n(c) $C$ can never be positive definite, because $C$ has a negative diagonal entry -2 .\\n',\n",
       " '\\n7.41. Find the matrix $A$ that represents the usual inner product on $\\\\mathbf{R}^{2}$ relative to each of the following bases of $\\\\mathbf{R}^{2}$ : (a) $\\\\left\\\\{v_{1}=(1,3), \\\\quad v_{2}=(2,5)\\\\right\\\\}$; (b) $\\\\left\\\\{w_{1}=(1,2), \\\\quad w_{2}=(4,-2)\\\\right\\\\}$.\\n\\n(a) Compute $\\\\left\\\\langle v_{1}, v_{1}\\\\right\\\\rangle=1+9=10,\\\\left\\\\langle v_{1}, v_{2}\\\\right\\\\rangle=2+15=17,\\\\left\\\\langle v_{2}, v_{2}\\\\right\\\\rangle=4+25=29$. Thus, $A=\\\\left[\\\\begin{array}{ll}10 & 17 \\\\\\\\ 17 & 29\\\\end{array}\\\\right]$\\n\\n(b) Compute $\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle=1+4=5,\\\\left\\\\langle w_{1}, w_{2}\\\\right\\\\rangle=4-4=0,\\\\left\\\\langle w_{2}, w_{2}\\\\right\\\\rangle=16+4=20$. Thus, $A=\\\\left[\\\\begin{array}{rr}5 & 0 \\\\\\\\ 0 & 20\\\\end{array}\\\\right]$.\\\\\\\\\\n(Because the basis vectors are orthogonal, the matrix $A$ is diagonal.)\\n',\n",
       " '\\n7.42. Consider the vector space $\\\\mathbf{P}_{2}(t)$ with inner product $\\\\langle f, g\\\\rangle=\\\\int_{-1}^{1} f(t) g(t) d t$.\\n\\n(a) Find $\\\\langle f, g\\\\rangle$, where $f(t)=t+2$ and $g(t)=t^{2}-3 t+4$.\\n\\n(b) Find the matrix $A$ of the inner product with respect to the basis $\\\\left\\\\{1, t, t^{2}\\\\right\\\\}$ of $V$.\\n\\n(c) Verify Theorem 7.16 by showing that $\\\\langle f, g\\\\rangle=[f]^{T} A[g]$ with respect to the basis $\\\\left\\\\{1, t, t^{2}\\\\right\\\\}$.\\n\\n(a) $\\\\langle f, g\\\\rangle=\\\\int_{-1}^{1}(t+2)\\\\left(t^{2}-3 t+4\\\\right) d t=\\\\int_{-1}^{1}\\\\left(t^{3}-t^{2}-2 t+8\\\\right) d t=\\\\left.\\\\left(\\\\frac{t^{4}}{4}-\\\\frac{t^{3}}{3}-t^{2}+8 t\\\\right)\\\\right|_{-1} ^{1}=\\\\frac{46}{3}$\\n\\n(b) Here we use the fact that if $r+s=n$,\\n\\n$$\\n\\\\left\\\\langle t^{r}, t^{r}\\\\right\\\\rangle=\\\\int_{-1}^{1} t^{n} d t=\\\\left.\\\\frac{t^{n+1}}{n+1}\\\\right|_{-1} ^{1}= \\\\begin{cases}2 /(n+1) & \\\\text { if } n \\\\text { is even, } \\\\\\\\ 0 & \\\\text { if } n \\\\text { is odd. }\\\\end{cases}\\n$$\\n\\nThen $\\\\langle 1,1\\\\rangle=2,\\\\langle 1, t\\\\rangle=0,\\\\left\\\\langle 1, t^{2}\\\\right\\\\rangle=\\\\frac{2}{3},\\\\langle t, t\\\\rangle=\\\\frac{2}{3},\\\\left\\\\langle t, t^{2}\\\\right\\\\rangle=0,\\\\left\\\\langle t^{2}, t^{2}\\\\right\\\\rangle=\\\\frac{2}{5}$. Thus,\\n\\n$$\\nA=\\\\left[\\\\begin{array}{ccc}\\n2 & 0 & \\\\frac{2}{3} \\\\\\\\\\n0 & \\\\frac{2}{3} & 0 \\\\\\\\\\n\\\\frac{2}{3} & 0 & \\\\frac{2}{5}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) We have $[f]^{T}=(2,1,0)$ and $[g]^{T}=(4,-3,1)$ relative to the given basis. Then\\n\\n$$\\n[f]^{T} A[g]=(2,1,0)\\\\left[\\\\begin{array}{lll}\\n2 & 0 & \\\\frac{2}{3} \\\\\\\\\\n0 & \\\\frac{2}{3} & 0 \\\\\\\\\\n\\\\frac{2}{3} & 0 & \\\\frac{2}{5}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{r}\\n4 \\\\\\\\\\n-3 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=\\\\left(4, \\\\frac{2}{3}, \\\\frac{4}{3}\\\\right)\\\\left[\\\\begin{array}{r}\\n4 \\\\\\\\\\n-3 \\\\\\\\\\n1\\n\\\\end{array}\\\\right]=\\\\frac{46}{3}=\\\\langle f, g\\\\rangle\\n$$\\n',\n",
       " '\\n7.43. Prove Theorem 7.14: $A=\\\\left[\\\\begin{array}{ll}a & b \\\\\\\\ b & c\\\\end{array}\\\\right]$ is positive definite if and only if $a$ and $d$ are positive and\\\\\\\\\\n$|A|=a d-b^{2}$ is positive.\\n\\nLet $u=[x, y]^{T}$. Then\\n\\n$$\\nf(u)=u^{T} A u=[x, y]\\\\left[\\\\begin{array}{ll}\\na & b \\\\\\\\\\nb & d\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right]=a x^{2}+2 b x y+d y^{2}\\n$$\\n\\nSuppose $f(u)>0$ for every $u \\\\neq 0$. Then $f(1,0)=a>0$ and $f(0,1)=d>0$. Also, we have $f(b,-a)=a\\\\left(a d-b^{2}\\\\right)>0$. Because $a>0$, we get $a d-b^{2}>0$.\\n\\nConversely, suppose $a>0, b=0, a d-b^{2}>0$. Completing the square gives us\\n\\n$$\\nf(u)=a\\\\left(x^{2}+\\\\frac{2 b}{a} x y+\\\\frac{b^{2}}{a_{2}} y^{2}\\\\right)+d y^{2}-\\\\frac{b^{2}}{a} y^{2}=a\\\\left(x+\\\\frac{b y}{a}\\\\right)^{2}+\\\\frac{a d-b^{2}}{a} y^{2}\\n$$\\n\\nAccordingly, $f(u)>0$ for every $u \\\\neq 0$.\\n',\n",
       " '\\n7.44. Prove Theorem 7.15: Let $A$ be a real positive definite matrix. Then the function $\\\\langle u, v\\\\rangle=u^{T} A v$ is an inner product on $\\\\mathbf{R}^{n}$.\\n\\nFor any vectors $u_{1}, u_{2}$, and $v$,\\n\\n$$\\n\\\\left\\\\langle u_{1}+u_{2}, \\\\quad v\\\\right\\\\rangle=\\\\left(u_{1}+u_{2}\\\\right)^{T} A v=\\\\left(u_{1}^{T}+u_{2}^{T}\\\\right) A v=u_{1}^{T} A v+u_{2}^{T} A v=\\\\left\\\\langle u_{1}, v\\\\right\\\\rangle+\\\\left\\\\langle u_{2}, v\\\\right\\\\rangle\\n$$\\n\\nand, for any scalar $k$ and vectors $u, v$,\\n\\n$$\\n\\\\langle k u, v\\\\rangle=(k u)^{T} A v=k u^{T} A v=k\\\\langle u, v\\\\rangle\\n$$\\n\\nThus $\\\\left[\\\\mathrm{I}_{1}\\\\right]$ is satisfied.\\n\\nBecause $u^{T} A v$ is a scalar, $\\\\left(u^{T} A v\\\\right)^{T}=u^{T} A v$. Also, $A^{T}=A$ because $A$ is symmetric. Therefore,\\n\\n$$\\n\\\\langle u, v\\\\rangle=u^{T} A v=\\\\left(u^{T} A v\\\\right)^{T}=v^{T} A^{T} u^{T T}=v^{T} A u=\\\\langle v, u\\\\rangle\\n$$\\n\\nThus, $\\\\left[\\\\mathrm{I}_{2}\\\\right]$ is satisfied.\\n\\nLast, because $A$ is positive definite, $X^{T} A X>0$ for any nonzero $X \\\\in \\\\mathbf{R}^{n}$. Thus, for any nonzero vector $v,\\\\langle v, v\\\\rangle=v^{T} A v>0$. Also, $\\\\langle 0,0\\\\rangle=0^{T} A 0=0$. Thus, $\\\\left[\\\\mathrm{I}_{3}\\\\right]$ is satisfied. Accordingly, the function $\\\\langle u, v\\\\rangle=A v$ is an inner product.\\n',\n",
       " '\\n7.45. Prove Theorem 7.16: Let $A$ be the matrix representation of an inner product relative to a basis $S$ of $V$. Then, for any vectors $u, v \\\\in V$, we have\\n\\n$$\\n\\\\langle u, v\\\\rangle=[u]^{T} A[v]\\n$$\\n\\nSuppose $S=\\\\left\\\\{w_{1}, w_{2}, \\\\ldots, w_{n}\\\\right\\\\}$ and $A=\\\\left[k_{i j}\\\\right]$. Hence, $k_{i j}=\\\\left\\\\langle w_{i}, w_{j}\\\\right\\\\rangle$. Suppose\\n\\n$$\\nu=a_{1} w_{1}+a_{2} w_{2}+\\\\cdots+a_{n} w_{n} \\\\quad \\\\text { and } \\\\quad v=b_{1} w_{1}+b_{2} w_{2}+\\\\cdots+b_{n} w_{n}\\n$$\\n\\nThen\\n\\n\\n\\\\begin{equation*}\\n\\\\langle u, v\\\\rangle=\\\\sum_{i=1}^{n} \\\\sum_{j=1}^{n} a_{i} b_{j}\\\\left\\\\langle w_{i}, w_{j}\\\\right\\\\rangle \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nOn the other hand,\\n\\n\\n\\\\begin{align*}\\n{[u]^{T} A[v] } & =\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right)\\\\left[\\\\begin{array}{cccc}\\nk_{11} & k_{12} & \\\\ldots & k_{1 n} \\\\\\\\\\nk_{21} & k_{22} & \\\\ldots & k_{2 n} \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\nk_{n 1} & k_{n 2} & \\\\ldots & k_{n n}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}\\nb_{1} \\\\\\\\\\nb_{2} \\\\\\\\\\n\\\\vdots \\\\\\\\\\nb_{n}\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =\\\\left(\\\\sum_{i=1}^{n} a_{i} k_{i 1}, \\\\sum_{i=1}^{n} a_{i} k_{i 2}, \\\\ldots, \\\\sum_{i=1}^{n} a_{i} k_{i n}\\\\right)\\\\left[\\\\begin{array}{c}\\nb_{1} \\\\\\\\\\nb_{2} \\\\\\\\\\n\\\\vdots \\\\\\\\\\nb_{n}\\n\\\\end{array}\\\\right]=\\\\sum_{j=1}^{n} \\\\sum_{i=1}^{n} a_{i} b_{j} k_{i j} \\\\tag{2}\\n\\\\end{align*}\\n\\n\\nEquations (1) and (2) give us our result.\\n',\n",
       " '\\n7.46. Prove Theorem 7.17: Let $A$ be the matrix representation of any inner product on $V$. Then $A$ is a positive definite matrix.\\n\\nBecause $\\\\left\\\\langle w_{i}, w_{j}\\\\right\\\\rangle=\\\\left\\\\langle w_{j}, w_{i}\\\\right\\\\rangle$ for any basis vectors $w_{i}$ and $w_{j}$, the matrix $A$ is symmetric. Let $X$ be any nonzero vector in $\\\\mathbf{R}^{n}$. Then $[u]=X$ for some nonzero vector $u \\\\in V$. Theorem 7.16 tells us that $X^{T} A X=[u]^{T} A[u]=\\\\langle u, u\\\\rangle>0$. Thus, $A$ is positive definite.\\n\\n\\n\\\\section*{Complex Inner Product Spaces}\\n',\n",
       " '7.47. Let $V$ be a complex inner product space. Verify the relation\\n\\n$$\\n\\\\left\\\\langle u, a v_{1}+b v_{2}\\\\right\\\\rangle=\\\\bar{a}\\\\left\\\\langle u, v_{1}\\\\right\\\\rangle+\\\\bar{b}\\\\left\\\\langle u, v_{2}\\\\right\\\\rangle\\n$$\\n\\nUsing $\\\\left[I_{2}^{*}\\\\right],\\\\left[I_{1}^{*}\\\\right]$, and then $\\\\left[I_{2}^{*}\\\\right]$, we find\\n\\n$$\\n\\\\left\\\\langle u, a v_{1}+b v_{2}\\\\right\\\\rangle=\\\\overline{\\\\left\\\\langle a v_{1}+b v_{2}, u\\\\right\\\\rangle}=\\\\overline{a\\\\left\\\\langle v_{1}, u\\\\right\\\\rangle+b\\\\left\\\\langle v_{2}, u\\\\right\\\\rangle}=\\\\bar{a} \\\\overline{\\\\left\\\\langle v_{1}, u\\\\right\\\\rangle}+\\\\bar{b}\\\\left\\\\langle v_{2}, u\\\\right\\\\rangle=\\\\bar{a}\\\\left\\\\langle u, v_{1}\\\\right\\\\rangle+\\\\bar{b}\\\\left\\\\langle u, v_{2}\\\\right\\\\rangle\\n$$\\n',\n",
       " '\\n7.48. Suppose $\\\\langle u, v\\\\rangle=3+2 i$ in a complex inner product space $V$. Find\\n\\n(a) $\\\\langle(2-4 i) u, v\\\\rangle$; (b) $\\\\langle u,(4+3 i) v\\\\rangle$; (c) $\\\\langle(3-6 i) u$, $(5-2 i) v\\\\rangle$.\\n\\n(a) $\\\\langle(2-4 i) u, v\\\\rangle=(2-4 i)\\\\langle u, v\\\\rangle=(2-4 i)(3+2 i)=14-8 i$\\n\\n(b) $\\\\langle u, \\\\quad(4+3 i) v\\\\rangle=\\\\overline{(4+3 i)}\\\\langle u, v\\\\rangle=(4-3 i)(3+2 i)=18-i$\\n\\n(c) $\\\\langle(3-6 i) u, \\\\quad(5-2 i) v\\\\rangle=(3-6 i) \\\\overline{(5-2 i)}\\\\langle u, v\\\\rangle=(3-6 i)(5+2 i)(3+2 i)=129-18 i$\\n',\n",
       " '\\n7.49. Find the Fourier coefficient (component) $c$ and the projection $c w$ of $v=(3+4 i, 2-3 i)$ along $w=(5+i, 2 i)$ in $\\\\mathbf{C}^{2}$.\\n\\nRecall that $c=\\\\langle v, w\\\\rangle /\\\\langle w, w\\\\rangle$. Compute\\n\\n$$\\n\\\\begin{aligned}\\n\\\\langle v, w\\\\rangle & =(3+4 i)(\\\\overline{5+i})+(2-3 i)(\\\\overline{2 i})=(3+4 i)(5-i)+(2-3 i)(-2 i) \\\\\\\\\\n& =19+17 i-6-4 i=13+13 i \\\\\\\\\\n\\\\langle w, w\\\\rangle & =25+1+4=30\\n\\\\end{aligned}\\n$$\\n\\nThus, $c=(13+13 i) / 30=\\\\frac{13}{30}+\\\\frac{13}{30} i$. Accordingly, $\\\\operatorname{proj}(v, w)=c w=\\\\left(\\\\frac{26}{15}+\\\\frac{39}{15} i,-\\\\frac{13}{15}+\\\\frac{1}{15} i\\\\right)$\\n',\n",
       " '\\n7.50. Prove Theorem 7.18 (Cauchy-Schwarz): Let $V$ be a complex inner product space. Then $|\\\\langle u, v\\\\rangle| \\\\leq\\\\|u\\\\|\\\\|v\\\\|$.\\n\\nIf $v=0$, the inequality reduces to $0 \\\\leq 0$ and hence is valid. Now suppose $v \\\\neq 0$. Using $z \\\\bar{z}=|z|^{2}$ (for any complex number $z$ ) and $\\\\langle v, u\\\\rangle=\\\\overline{\\\\langle u, v\\\\rangle}$, we expand $\\\\|u-\\\\langle u, v\\\\rangle t v\\\\|^{2} \\\\geq 0$, where $t$ is any real value:\\n\\n$$\\n\\\\begin{aligned}\\n0 & \\\\leq\\\\|u-\\\\langle u, v\\\\rangle t v\\\\|^{2}=\\\\langle u-\\\\langle u, v\\\\rangle t v, u-\\\\langle u, v\\\\rangle t v\\\\rangle \\\\\\\\\\n& =\\\\langle u, u\\\\rangle-\\\\overline{\\\\langle u, v\\\\rangle} t\\\\langle u, v\\\\rangle-\\\\langle u, v) t\\\\langle v, u\\\\rangle+\\\\langle u, v\\\\rangle \\\\overline{\\\\langle u, v\\\\rangle} t^{2}\\\\langle v, v\\\\rangle \\\\\\\\\\n& =\\\\|u\\\\|^{2}-2 t|\\\\langle u, v\\\\rangle|^{2}+|\\\\langle u, v\\\\rangle|^{2} t^{2}\\\\|v\\\\|^{2}\\n\\\\end{aligned}\\n$$\\n\\nSet $t=1 /\\\\|v\\\\|^{2}$ to find $0 \\\\leq\\\\|u\\\\|^{2}-\\\\frac{|\\\\langle u, v\\\\rangle|^{2}}{\\\\|v\\\\|^{2}}$, from which $|\\\\langle u, v\\\\rangle|^{2} \\\\leq\\\\|v\\\\|^{2}\\\\|v\\\\|^{2}$. Taking the square root of both sides, we obtain the required inequality.\\n',\n",
       " '\\n7.51. Find an orthogonal basis for $u^{\\\\perp}$ in $C^{3}$ where $u=(1, i, 1+i)$.\\n\\nHere $u^{\\\\perp}$ consists of all vectors $s=(x, y, z)$ such that\\n\\n$$\\n\\\\langle w, u\\\\rangle=x-i y+(1-i) z=0\\n$$\\n\\nFind one solution, say $w_{1}=(0,1-i, i)$. Then find a solution of the system\\n\\n$$\\nx-i y+(1-i) z=0, \\\\quad(1+i) y-i z=0\\n$$\\n\\nHere $z$ is a free variable. Set $z=1$ to obtain $y=i /(1+i)=(1+i) / 2$ and $x=(3 i-3) 2$. Multiplying by 2 yields the solution $w_{2}=(3 i-3,1+i, 2)$. The vectors $w_{1}$ and $w_{2}$ form an orthogonal basis for $u^{\\\\perp}$.\\n',\n",
       " '\\n7.52. Find an orthonormal basis of the subspace $W$ of $\\\\mathbf{C}^{3}$ spanned by\\n\\n$$\\nv_{1}=(1, i, 0) \\\\quad \\\\text { and } \\\\quad v_{2}=(1,2,1-i) .\\n$$\\n\\nApply the Gram-Schmidt algorithm. Set $w_{1}=v_{1}=(1, i, 0)$. Compute\\n\\n$$\\nv_{2}-\\\\frac{\\\\left\\\\langle v_{2}, w_{1}\\\\right\\\\rangle}{\\\\left\\\\langle w_{1}, w_{1}\\\\right\\\\rangle} w_{1}=(1, \\\\quad 2, \\\\quad 1-i)-\\\\frac{1-2 i}{2}(1, i, 0)=\\\\left(\\\\frac{1}{2}+i, \\\\quad 1-\\\\frac{1}{2} i, \\\\quad 1-i\\\\right)\\n$$\\n\\nMultiply by 2 to clear fractions, obtaining $w_{2}=(1+2 i, 2-i, 2-2 i)$. Next find $\\\\left\\\\|w_{1}\\\\right\\\\|=\\\\sqrt{2}$ and then $\\\\left\\\\|w_{2}\\\\right\\\\|=\\\\sqrt{18}$. Normalizing $\\\\left\\\\{w_{1}, w_{2}\\\\right\\\\}$, we obtain the following orthonormal basis of $W$ :\\n\\n$$\\n\\\\left\\\\{u_{1}=\\\\left(\\\\frac{1}{\\\\sqrt{2}}, \\\\frac{i}{\\\\sqrt{2}}, 0\\\\right), u_{2}=\\\\left(\\\\frac{1+2 i}{\\\\sqrt{18}}, \\\\frac{2-i}{\\\\sqrt{18}}, \\\\frac{2-2 i}{\\\\sqrt{18}}\\\\right)\\\\right\\\\}\\n$$\\n',\n",
       " '\\n7.53. Find the matrix $P$ that represents the usual inner product on $\\\\mathbf{C}^{3}$ relative to the basis $\\\\{1, i, 1-i\\\\}$.\\n\\nCompute the following six inner products:\\n\\n$$\\n\\\\begin{array}{lll}\\n\\\\langle 1,1\\\\rangle=1, & \\\\langle 1, i\\\\rangle=\\\\bar{i}=-i, & \\\\langle 1,1-i\\\\rangle=\\\\overline{1-i}=1+i \\\\\\\\\\n\\\\langle i, i\\\\rangle=\\\\bar{i} i=1, & \\\\langle i, 1-i\\\\rangle=i(\\\\overline{1-i})=-1+i, & \\\\langle 1-i, 1-i\\\\rangle=2\\n\\\\end{array}\\n$$\\n\\nThen, using $(u, v)=\\\\overline{\\\\langle v, u\\\\rangle}$, we obtain\\n\\n$$\\nP=\\\\left[\\\\begin{array}{ccc}\\n1 & -i & 1+i \\\\\\\\\\ni & 1 & -1+i \\\\\\\\\\n1-i & -1-i & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\n(As expected, $P$ is Hermitian; that is, $P^{H}=P$.)\\n\\n\\n\\\\section*{Normed Vector Spaces}\\n',\n",
       " '7.54. Consider vectors $u=(1,3,-6,4)$ and $v=(3,-5,1,-2)$ in $\\\\mathbf{R}^{4}$. Find\\n\\n(a) $\\\\|u\\\\|_{\\\\infty}$ and $\\\\|\\\\left. v\\\\right|_{\\\\infty}$, (b) $\\\\|u\\\\|_{1}$ and $\\\\|v\\\\|_{1}$, (c) $\\\\|u\\\\|_{2}$ and $\\\\|v\\\\|_{2}$,\\n\\n(d) $d_{\\\\infty}(u, v), d_{1}(u, v), d_{2}(u, v)$.\\n\\n(a) The infinity norm chooses the maximum of the absolute values of the components. Hence,\\n\\n$$\\n\\\\|u\\\\|_{\\\\infty}=6 \\\\quad \\\\text { and } \\\\quad\\\\|v\\\\|_{\\\\infty}=5\\n$$\\n\\n(b) The one-norm adds the absolute values of the components. Thus,\\n\\n$$\\n\\\\|u\\\\|_{1}=1+3+6+4=14 \\\\quad \\\\text { and } \\\\quad\\\\|v\\\\|_{1}=3+5+1+2=11\\n$$\\n\\n(c) The two-norm is equal to the square root of the sum of the squares of the components (i.e., the norm induced by the usual inner product on $\\\\mathbf{R}^{3}$ ). Thus,\\n\\n$$\\n\\\\|u\\\\|_{2}=\\\\sqrt{1+9+36+16}=\\\\sqrt{62} \\\\quad \\\\text { and } \\\\quad\\\\|v\\\\|_{2}=\\\\sqrt{9+25+1+4}=\\\\sqrt{39}\\n$$\\n\\n(d) First find $u-v=(-2,8,-7,6)$. Then\\n\\n$$\\n\\\\begin{aligned}\\nd_{\\\\infty}(u, v) & =\\\\|u-v\\\\|_{\\\\infty}=8 \\\\\\\\\\nd_{1}(u, v) & =\\\\|u-v\\\\|_{1}=2+8+7+6=23 \\\\\\\\\\nd_{2}(u, v) & =\\\\|u-v\\\\|_{2}=\\\\sqrt{4+64+49+36}=\\\\sqrt{153}\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n7.55. Consider the function $f(t)=t^{2}-4 t$ in $C[0,3]$.\\n\\n(a) Find $\\\\|f\\\\|_{\\\\infty}$, (b) Plot $f(t)$ in the plane $\\\\mathbf{R}^{2}$, (c) Find $\\\\|f\\\\|_{1}$, (d) Find $\\\\|f\\\\|_{2}$.\\n\\n(a) We seek $\\\\|f\\\\|_{\\\\infty}=\\\\max (|f(t)|)$. Because $f(t)$ is differentiable on $[0,3],|f(t)|$ has a maximum at a critical point of $f(t)$ (i.e., when the derivative $f^{\\\\prime}(t)=0$ ), or at an endpoint of $[0,3]$. Because $f^{\\\\prime}(t)=2 t-4$, we set $2 t-4=0$ and obtain $t=2$ as a critical point. Compute\\n\\n$$\\nf(2)=4-8=-4, \\\\quad f(0)=0-0=0, \\\\quad f(3)=9-12=-3\\n$$\\n\\nThus, $\\\\|f\\\\|_{\\\\infty}=|f(2)|=|-4|=4$.\\\\\\\\\\n(b) Compute $f(t)$ for various values of $t$ in $[0,3]$, for example,\\n\\n$$\\n\\\\begin{array}{c|rrrr}\\nt & 0 & 1 & 2 & 3 \\\\\\\\\\n\\\\hline f(t) & 0 & -3 & -4 & -3\\n\\\\end{array}\\n$$\\n\\nPlot the points in $\\\\mathbf{R}^{2}$ and then draw a continuous curve through the points, as shown in Fig. 7-8.\\n\\n(c) We seek $\\\\|f\\\\|_{1}=\\\\int_{0}^{3}|f(t)| d t$. As indicated in Fig. 7-3, $f(t)$ is negative in $[0,3]$; hence,\\n\\n$$\\n|f(t)|=-\\\\left(t^{2}-4 t\\\\right)=4 t-t^{2}\\n$$\\n\\nThus, $\\\\|f\\\\|_{1}=\\\\int_{0}^{3}\\\\left(4 t-t^{2}\\\\right) d t=\\\\left.\\\\left(2 t^{2}-\\\\frac{t^{3}}{3}\\\\right)\\\\right|_{0} ^{3}=18-9=9$\\n\\n(d)\\n\\n$\\\\|f\\\\|_{2}^{2}=\\\\int_{0}^{3} f(t)^{2} d t=\\\\int_{0}^{3}\\\\left(t^{4}-8 t^{3}+16 t^{2}\\\\right) d t=\\\\left.\\\\left(\\\\frac{t^{5}}{5}-2 t^{4}+\\\\frac{16 t^{3}}{3}\\\\right)\\\\right|_{0} ^{3}=\\\\frac{153}{5}$.\\n\\nThus, $\\\\|f\\\\|_{2}=\\\\sqrt{\\\\frac{153}{5}}$.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-265}\\n\\\\end{center}\\n\\nFigure 7-8\\n',\n",
       " '\\n7.56. Prove Theorem 7.24: Let $V$ be a normed vector space. Then the function $d(u, v)=\\\\|u-v\\\\|$ satisfies the following three axioms of a metric space:\\n\\n$\\\\left[\\\\mathrm{M}_{1}\\\\right] \\\\quad d(u, v) \\\\geq 0$; and $d(u, v)=0$ iff $u=v$.\\n\\n$\\\\left[\\\\mathrm{M}_{2}\\\\right] \\\\quad d(u, v)=d(v, u)$.\\n\\n$\\\\left[\\\\mathrm{M}_{3}\\\\right] \\\\quad d(u, v) \\\\leq d(u, w)+d(w, v)$.\\n\\nIf $u \\\\neq v$, then $u-v \\\\neq 0$, and hence, $d(u, v)=\\\\|u-v\\\\|>0$. Also, $d(u, u)=\\\\|u-u\\\\|=\\\\|0\\\\|=0$. Thus, $\\\\left[\\\\mathrm{M}_{1}\\\\right]$ is satisfied. We also have\\n\\n$$\\nd(u, v)=\\\\|u-v\\\\|=\\\\|-1(v-u)\\\\|=|-1|\\\\|v-u\\\\|=\\\\|v-u\\\\|=d(v, u)\\n$$\\n\\nand $\\\\quad d(u, v)=\\\\|u-v\\\\|=\\\\|(u-w)+(w-v)\\\\| \\\\leq\\\\|u-w\\\\|+\\\\|w-v\\\\|=d(u, w)+d(w, v)$\\n\\nThus, $\\\\left[\\\\mathrm{M}_{2}\\\\right]$ and $\\\\left[\\\\mathrm{M}_{3}\\\\right]$ are satisfied.\\n\\n',\n",
       " '8.1. Evaluate the determinant of each of the following matrices:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{ll}6 & 5 \\\\\\\\ 2 & 3\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rr}2 & -3 \\\\\\\\ 4 & 7\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rr}4 & -5 \\\\\\\\ -1 & -2\\\\end{array}\\\\right]$\\\\\\\\\\n(d) $D=\\\\left[\\\\begin{array}{cc}t-5 & 6 \\\\\\\\ 3 & t+2\\\\end{array}\\\\right]$\\n\\nUse the formula $\\\\left|\\\\begin{array}{ll}a & b \\\\\\\\ c & d\\\\end{array}\\\\right|=a d-b c$ :\\n\\n(a) $|A|=6(3)-5(2)=18-10=8$\\n\\n(b) $|B|=14+12=26$\\n\\n(c) $|C|=-8-5=-13$\\n\\n(d) $|D|=(t-5)(t+2)-18=t^{2}-3 t-10-18=t^{2}-10 t-28$\\n',\n",
       " '\\n8.2. Evaluate the determinant of each of the following matrices:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{lll}2 & 3 & 4 \\\\\\\\ 5 & 4 & 3 \\\\\\\\ 1 & 2 & 1\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{rrr}1 & -2 & 3 \\\\\\\\ 2 & 4 & -1 \\\\\\\\ 1 & 5 & -2\\\\end{array}\\\\right]$,\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rrr}1 & 3 & -5 \\\\\\\\ 3 & -1 & 2 \\\\\\\\ 1 & -2 & 1\\\\end{array}\\\\right]$\\n\\nUse the diagram in Fig. 8-1 to obtain the six products:\\n\\n(a) $|A|=2(4)(1)+3(3)(1)+4(2)(5)-1(4)(4)-2(3)(2)-1(3)(5)=8+9+40-16-12-15=14$\\n\\n(b) $|B|=-8+2+30-12+5-8=9$\\n\\n(c) $|C|=-1+6+30-5+4-9=25$\\n',\n",
       " '\\n8.3. Compute the determinant of each of the following matrices:\\n\\n(a) $A=\\\\left[\\\\begin{array}{rrr}2 & 3 & 4 \\\\\\\\ 5 & 6 & 7 \\\\\\\\ 8 & 9 & 1\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{rrrr}4 & -6 & 8 & 9 \\\\\\\\ 0 & -2 & 7 & -3 \\\\\\\\ 0 & 0 & 5 & 6 \\\\\\\\ 0 & 0 & 0 & 3\\\\end{array}\\\\right]$, (c) $C=\\\\left[\\\\begin{array}{rrr}\\\\frac{1}{2} & -1 & -\\\\frac{1}{3} \\\\\\\\ \\\\frac{3}{4} & \\\\frac{1}{2} & -1 \\\\\\\\ 1 & -4 & 1\\\\end{array}\\\\right]$.\\n\\n(a) One can simplify the entries by first subtracting twice the first row from the second row-that is, by applying the row operation \" Replace $R_{2}$ by $-2_{1}+R_{2}$.\" Then\\n\\n$$\\n|A|=\\\\left|\\\\begin{array}{lll}\\n2 & 3 & 4 \\\\\\\\\\n5 & 6 & 7 \\\\\\\\\\n8 & 9 & 1\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rrr}\\n2 & 3 & 4 \\\\\\\\\\n1 & 0 & -1 \\\\\\\\\\n8 & 9 & 1\\n\\\\end{array}\\\\right|=0-24+36-0+18-3=27\\n$$\\n\\n(b) $B$ is triangular, so $|B|=$ product of the diagonal entries $=-120$.\\n\\n(c) The arithmetic is simpler if fractions are first eliminated. Hence, multiply the first row $R_{1}$ by 6 and the second row $R_{2}$ by 4 . Then\\n\\n$$\\n|24 C|=\\\\left|\\\\begin{array}{rrr}\\n3 & -6 & -2 \\\\\\\\\\n3 & 2 & -4 \\\\\\\\\\n1 & -4 & 1\\n\\\\end{array}\\\\right|=6+24+24+4-48+18=28, \\\\quad \\\\text { so }|C|=\\\\frac{28}{24}=\\\\frac{7}{6}\\n$$\\n',\n",
       " '\\n8.4. Compute the determinant of each of the following matrices:\\n\\n(a) $A=\\\\left[\\\\begin{array}{rrrr}2 & 5 & -3 & -2 \\\\\\\\ -2 & -3 & 2 & -5 \\\\\\\\ 1 & 3 & -2 & 2 \\\\\\\\ -1 & -6 & 4 & 3\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{rrrrr}6 & 2 & 1 & 0 & 5 \\\\\\\\ 2 & 1 & 1 & -2 & 1 \\\\\\\\ 1 & 1 & 2 & -2 & 3 \\\\\\\\ 3 & 0 & 2 & 3 & -1 \\\\\\\\ -1 & -1 & -3 & 4 & 2\\\\end{array}\\\\right]$\\n\\n(a) Use $a_{31}=1$ as a pivot to put 0 \\'s in the first column, by applying the row operations \"Replace $R_{1}$ by $-2 R_{3}+R_{1}$,\" \\'Replace $R_{2}$ by $2 R_{3}+R_{2}$,\" and \"Replace $R_{4}$ by $R_{3}+R_{4}$.\" Then\\n\\n$$\\n\\\\begin{aligned}\\n|A| & =\\\\left|\\\\begin{array}{rrrr}\\n2 & 5 & -3 & -2 \\\\\\\\\\n-2 & -3 & 2 & -5 \\\\\\\\\\n1 & 3 & -2 & 2 \\\\\\\\\\n-1 & -6 & 4 & 3\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rrrr}\\n0 & -1 & 1 & -6 \\\\\\\\\\n0 & 3 & -2 & -1 \\\\\\\\\\n1 & 3 & -2 & 2 \\\\\\\\\\n0 & -3 & 2 & 5\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rrr}\\n-1 & 1 & -6 \\\\\\\\\\n3 & -2 & -1 \\\\\\\\\\n-3 & 2 & 5\\n\\\\end{array}\\\\right| \\\\\\\\\\n& =10+3-36+36-2-15=-4\\n\\\\end{aligned}\\n$$\\n\\n(b) First reduce $|B|$ to a determinant of order 4, and then to a determinant of order 3, for which we can use Fig. 8-1. First use $c_{22}=1$ as a pivot to put 0 \\'s in the second column, by applying the row operations \"Replace $R_{1}$ by $-2 R_{2}+R_{1}$,\" \"Replace $R_{3}$ by $-R_{2}+R_{3}$,\" and \"Replace $R_{5}$ by $R_{2}+R_{5}$.\" Then\\n\\n$$\\n\\\\begin{aligned}\\n|B| & =\\\\left|\\\\begin{array}{rrrrr}\\n2 & 0 & -1 & 4 & 3 \\\\\\\\\\n2 & 1 & 1 & -2 & 1 \\\\\\\\\\n-1 & 0 & 1 & 0 & 2 \\\\\\\\\\n3 & 0 & 2 & 3 & -1 \\\\\\\\\\n1 & 0 & -2 & 2 & 3\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rrrr}\\n2 & -1 & 4 & 3 \\\\\\\\\\n-1 & 1 & 0 & 2 \\\\\\\\\\n3 & 2 & 3 & -1 \\\\\\\\\\n1 & -2 & 2 & 3\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rrrr}\\n1 & 1 & 4 & 5 \\\\\\\\\\n0 & 1 & 0 & 0 \\\\\\\\\\n5 & 2 & 3 & -5 \\\\\\\\\\n-1 & -2 & 2 & 7\\n\\\\end{array}\\\\right| \\\\\\\\\\n& =\\\\left|\\\\begin{array}{rrr}\\n1 & 4 & 5 \\\\\\\\\\n5 & 3 & -5 \\\\\\\\\\n-1 & 2 & 7\\n\\\\end{array}\\\\right|=21+20+50+15+10-140=-34\\n\\\\end{aligned}\\n$$\\n\\n\\n\\\\section*{Cofactors, Classical Adjoints, Minors, Principal Minors}\\n',\n",
       " '8.5. Let $A=\\\\left[\\\\begin{array}{rrrr}2 & 1 & -3 & 4 \\\\\\\\ 5 & -4 & 7 & -2 \\\\\\\\ 4 & 0 & 6 & -3 \\\\\\\\ 3 & -2 & 5 & 2\\\\end{array}\\\\right]$.\\n\\n(a) Find $A_{23}$, the cofactor (signed minor) of 7 in $A$.\\n\\n(b) Find the minor and the signed minor of the submatrix $M=A(2,4 ; 2,3)$.\\n\\n(c) Find the principal minor determined by the first and third diagonal entries - that is, by $M=A(1,3 ; \\\\quad 1,3)$.\\n\\n(a) Take the determinant of the submatrix of $A$ obtained by deleting row 2 and column 3 (those which contain the 7 ), and multiply the determinant by $(-1)^{2+3}$ :\\n\\n$$\\nA_{23}=-\\\\left|\\\\begin{array}{rrr}\\n2 & 1 & 4 \\\\\\\\\\n4 & 0 & -3 \\\\\\\\\\n3 & -2 & 2\\n\\\\end{array}\\\\right|=-(-61)=61\\n$$\\n\\nThe exponent $2+3$ comes from the subscripts of $A_{23}$-that is, from the fact that 7 appears in row 2 and column 3.\\n\\n(b) The row subscripts are 2 and 4 and the column subscripts are 2 and 3. Hence, the minor is the determinant\\n\\n$$\\n|M|=\\\\left|\\\\begin{array}{ll}\\na_{22} & a_{23} \\\\\\\\\\na_{42} & a_{43}\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{ll}\\n-4 & 7 \\\\\\\\\\n-2 & 5\\n\\\\end{array}\\\\right|=-20+14=-6\\n$$\\n\\nand the signed minor is $(-1)^{2+4+2+3}|M|=-|M|=-(-6)=6$.\\n\\n(c) The principal minor is the determinant\\n\\n$$\\n|M|=\\\\left|\\\\begin{array}{ll}\\na_{11} & a_{13} \\\\\\\\\\na_{31} & a_{33}\\n\\\\end{array}\\\\right|=\\\\left|\\\\begin{array}{rr}\\n2 & -3 \\\\\\\\\\n4 & 6\\n\\\\end{array}\\\\right|=12+12=24\\n$$\\n\\nNote that now the diagonal entries of the submatrix are diagonal entries of the original matrix. Also, the sign of the principal minor is positive.\\n',\n",
       " '\\n8.6. Let $B=\\\\left[\\\\begin{array}{lll}1 & 1 & 1 \\\\\\\\ 2 & 3 & 4 \\\\\\\\ 5 & 8 & 9\\\\end{array}\\\\right]$. Find: (a) $|B|, \\\\quad$ (b) $\\\\operatorname{adj} B$, (c) $B^{-1}$ using adj $B$.\\n\\n(a) $|B|=27+20+16-15-32-18=-2$\\n\\n(b) Take the transpose of the matrix of cofactors:\\n\\n$$\\n\\\\operatorname{adj} B=\\\\left[\\\\begin{array}{rrr}\\n\\\\left|\\\\begin{array}{ll}\\n3 & 4 \\\\\\\\\\n8 & 9\\n\\\\end{array}\\\\right| & -\\\\left|\\\\begin{array}{ll}\\n2 & 4 \\\\\\\\\\n5 & 9\\n\\\\end{array}\\\\right| & \\\\left|\\\\begin{array}{ll}\\n2 & 3 \\\\\\\\\\n5 & 8\\n\\\\end{array}\\\\right| \\\\\\\\\\n-\\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n8 & 9\\n\\\\end{array}\\\\right| & \\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n5 & 9\\n\\\\end{array}\\\\right| & -\\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n5 & 8\\n\\\\end{array}\\\\right| \\\\\\\\\\n\\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right| & -\\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n2 & 4\\n\\\\end{array}\\\\right| & \\\\left|\\\\begin{array}{ll}\\n1 & 1 \\\\\\\\\\n2 & 3\\n\\\\end{array}\\\\right|\\n\\\\end{array}\\\\right]^{T}=\\\\left[\\\\begin{array}{rrr}\\n-5 & 2 & 1 \\\\\\\\\\n-1 & 4 & -3 \\\\\\\\\\n1 & -2 & 1\\n\\\\end{array}\\\\right]^{T}=\\\\left[\\\\begin{array}{rrr}\\n-5 & -1 & 1 \\\\\\\\\\n2 & 4 & -2 \\\\\\\\\\n1 & -3 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Because $|B| \\\\neq 0, B^{-1}=\\\\frac{1}{|B|}(\\\\operatorname{adj} B)=\\\\frac{1}{-2}\\\\left[\\\\begin{array}{rrr}-5 & -1 & 1 \\\\\\\\ 2 & 4 & -2 \\\\\\\\ 1 & -3 & 1\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rrr}\\\\frac{5}{2} & \\\\frac{1}{2} & -\\\\frac{1}{2} \\\\\\\\ -1 & -2 & 1 \\\\\\\\ -\\\\frac{1}{2} & \\\\frac{3}{2} & -\\\\frac{1}{2}\\\\end{array}\\\\right]$\\n',\n",
       " '\\n8.7. Let $A=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\\\\\ 0 & 7 & 8\\\\end{array}\\\\right]$, and let $S_{k}$ denote the sum of its principal minors of order $k$. Find $S_{k}$ for\\\\\\\\\\n(a) $k=1$,\\\\\\\\\\n(b) $k=2$,\\\\\\\\\\n(c) $k=3$.\\\\\\\\\\n(a) The principal minors of order 1 are the diagonal elements. Thus, $S_{1}$ is the trace of $A$; that is,\\n\\n$$\\nS_{1}=\\\\operatorname{tr}(A)=1+5+8=14\\n$$\\n\\n(b) The principal minors of order 2 are the cofactors of the diagonal elements. Thus,\\n\\n$$\\nS_{2}=A_{11}+A_{22}+A_{33}=\\\\left|\\\\begin{array}{ll}\\n5 & 6 \\\\\\\\\\n7 & 8\\n\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{ll}\\n1 & 3 \\\\\\\\\\n0 & 8\\n\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n4 & 5\\n\\\\end{array}\\\\right|=-2+8-3=3\\n$$\\n\\n(c) There is only one principal minor of order 3, the determinant of $A$. Then\\n\\n$$\\nS_{3}=|A|=40+0+84-0-42-64=18\\n$$\\n',\n",
       " '\\n8.8. Let $A=\\\\left[\\\\begin{array}{rrrr}1 & 3 & 0 & -1 \\\\\\\\ -4 & 2 & 5 & 1 \\\\\\\\ 1 & 0 & 3 & -2 \\\\\\\\ 3 & -2 & 1 & 4\\\\end{array}\\\\right]$. Find the number $N_{k}$ and sum $S_{k}$ of principal minors of order:\\\\\\\\\\n(a) $k=1$,\\\\\\\\\\n(b) $k=2$,\\\\\\\\\\n(c) $k=3$,\\\\\\\\\\n(d) $k=4$.\\n\\nEach (nonempty) subset of the diagonal (or equivalently, each nonempty subset of $\\\\{1,2,3,4\\\\}$ ) determines a principal minor of $A$, and $N_{k}=\\\\left(\\\\begin{array}{l}n \\\\\\\\ k\\\\end{array}\\\\right)=\\\\frac{n !}{k !(n-k) !}$ of them are of order $k$.\\n\\nThus, $N_{1}=\\\\left(\\\\begin{array}{l}4 \\\\\\\\ 1\\\\end{array}\\\\right)=4, \\\\quad N_{2}=\\\\left(\\\\begin{array}{l}4 \\\\\\\\ 2\\\\end{array}\\\\right)=6, \\\\quad N_{3}=\\\\left(\\\\begin{array}{l}4 \\\\\\\\ 3\\\\end{array}\\\\right)=4, \\\\quad N_{4}=\\\\left(\\\\begin{array}{l}4 \\\\\\\\ 4\\\\end{array}\\\\right)=1$\\n\\n(a) $S_{1}=|1|+|2|+|3|+|4|=1+2+3+4=10$\\n\\n(b) $S_{2}=\\\\left|\\\\begin{array}{rr}1 & 3 \\\\\\\\ -4 & 2\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{ll}1 & 0 \\\\\\\\ 1 & 3\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rr}1 & -1 \\\\\\\\ 3 & 4\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{ll}2 & 5 \\\\\\\\ 0 & 3\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rr}2 & 1 \\\\\\\\ -2 & 4\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rr}3 & -2 \\\\\\\\ 1 & 4\\\\end{array}\\\\right|$\\n\\n$$\\n=14+3+7+6+10+14=54\\n$$\\n\\n(c) $S_{3}=\\\\left|\\\\begin{array}{rrr}1 & 3 & 0 \\\\\\\\ -4 & 2 & 5 \\\\\\\\ 1 & 0 & 3\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rrr}1 & 3 & -1 \\\\\\\\ -4 & 2 & 1 \\\\\\\\ 3 & -2 & 4\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rrr}1 & 0 & -1 \\\\\\\\ 1 & 3 & -2 \\\\\\\\ 3 & 1 & 4\\\\end{array}\\\\right|+\\\\left|\\\\begin{array}{rrr}2 & 5 & 1 \\\\\\\\ 0 & 3 & -2 \\\\\\\\ -2 & 1 & 4\\\\end{array}\\\\right|$\\n\\n$$\\n=57+65+22+54=198\\n$$\\n\\n(d) $S_{4}=\\\\operatorname{det}(A)=378$\\n\\n\\n\\\\section*{Determinants and Systems of Linear Equations}\\n',\n",
       " '8.9. Use determinants to solve the system $\\\\left\\\\{\\\\begin{array}{l}3 y+2 x=z+1 \\\\\\\\ 3 x+2 z=8-5 y \\\\\\\\ 3 z-1=x-2 y\\\\end{array}\\\\right.$.\\n\\nFirst arrange the equation in standard form, then compute the determinant $D$ of the matrix of coefficients:\\n\\n$$\\n\\\\begin{aligned}\\n2 x+3 y-z & =1 \\\\\\\\\\n3 x+5 y+2 z & =8 \\\\\\\\\\nx-2 y-3 z & =-1\\n\\\\end{aligned} \\\\quad \\\\text { and } \\\\quad D=\\\\left|\\\\begin{array}{rrr}\\n2 & 3 & -1 \\\\\\\\\\n3 & 5 & 2 \\\\\\\\\\n1 & -2 & -3\\n\\\\end{array}\\\\right|=-30+6+6+5+8+27=22\\n$$\\n\\nBecause $D \\\\neq 0$, the system has a unique solution. To compute $N_{x}, N_{y}, N_{z}$, we replace, respectively, the coefficients of $x, y, z$ in the matrix of coefficients by the constant terms. Then\\n\\n$$\\nN_{x}=\\\\left|\\\\begin{array}{rrr}\\n1 & 3 & -1 \\\\\\\\\\n8 & 5 & 2 \\\\\\\\\\n-1 & -2 & -1\\n\\\\end{array}\\\\right|=66, \\\\quad N_{y}=\\\\left|\\\\begin{array}{rrr}\\n2 & 1 & -1 \\\\\\\\\\n3 & 8 & 2 \\\\\\\\\\n1 & -1 & -3\\n\\\\end{array}\\\\right|=-22, \\\\quad N_{z}=\\\\left|\\\\begin{array}{rrr}\\n2 & 3 & 1 \\\\\\\\\\n3 & 5 & 8 \\\\\\\\\\n1 & -2 & -1\\n\\\\end{array}\\\\right|=44\\n$$\\n\\nThus,\\n\\n$$\\nx=\\\\frac{N_{x}}{D}=\\\\frac{66}{22}=3, \\\\quad y=\\\\frac{N_{y}}{D}=\\\\frac{-22}{22}=-1, \\\\quad z=\\\\frac{N_{z}}{D}=\\\\frac{44}{22}=2\\n$$\\n',\n",
       " '\\n8.10. Consider the system $\\\\left\\\\{\\\\begin{array}{l}k x+y+z=1 \\\\\\\\ x+k y+z=1 \\\\\\\\ x+y+k z=1\\\\end{array}\\\\right.$\\n\\nUse determinants to find those values of $k$ for which the system has\\n\\n(a) a unique solution, (b) more than one solution, (c) no solution.\\n\\n(a) The system has a unique solution when $D \\\\neq 0$, where $D$ is the determinant of the matrix of coefficients. Compute\\n\\n$$\\nD=\\\\left|\\\\begin{array}{lll}\\nk & 1 & 1 \\\\\\\\\\n1 & k & 1 \\\\\\\\\\n1 & 1 & k\\n\\\\end{array}\\\\right|=k^{3}+1+1-k-k-k=k^{3}-3 k+2=(k-1)^{2}(k+2)\\n$$\\n\\nThus, the system has a unique solution when\\n\\n$$\\n(k-1)^{2}(k+2) \\\\neq 0, \\\\quad \\\\text { when } k \\\\neq 1 \\\\text { and } k \\\\neq 2\\n$$\\n\\n(b and c) Gaussian elimination shows that the system has more than one solution when $k=1$, and the system has no solution when $k=-2$.\\n\\n\\n\\\\section*{Miscellaneous Problems}\\n',\n",
       " '8.11. Find the volume $V(S)$ of the parallelepiped $S$ in $\\\\mathbf{R}^{3}$ determined by the vectors:\\n\\n(a) $u_{1}=(1,1,1), u_{2}=(1,3,-4), u_{3}=(1,2,-5)$.\\n\\n(b) $u_{1}=(1,2,4), u_{2}=(2,1,-3), u_{3}=(5,7,9)$.\\n\\n$V(S)$ is the absolute value of the determinant of the matrix $M$ whose rows are the given vectors. Thus,\\n\\n(a) $|M|=\\\\left|\\\\begin{array}{rrr}1 & 1 & 1 \\\\\\\\ 1 & 3 & -4 \\\\\\\\ 1 & 2 & -5\\\\end{array}\\\\right|=-15-4+2-3+8+5=-7$. Hence, $V(S)=|-7|=7$.\\n\\n(b) $|M|=\\\\left|\\\\begin{array}{rrr}1 & 2 & 4 \\\\\\\\ 2 & 1 & -3 \\\\\\\\ 5 & 7 & 9\\\\end{array}\\\\right|=9-30+56-20+21-36=0$. Thus, $V(S)=0$, or, in other words, $u_{1}, u_{2}, u_{3}$ lie in a plane and are linearly dependent.\\n',\n",
       " '\\n8.12. Find $\\\\operatorname{det}(M)$ where $M=\\\\left[\\\\begin{array}{lllll}3 & 4 & 0 & 0 & 0 \\\\\\\\ 2 & 5 & 0 & 0 & 0 \\\\\\\\ 0 & 9 & 2 & 0 & 0 \\\\\\\\ 0 & 5 & 0 & 6 & 7 \\\\\\\\ 0 & 0 & 4 & 3 & 4\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc:c:ccc}3 & 4 & 0 & 0 & 0 \\\\\\\\ 2 & 5 & 0 & 0 & 0 \\\\\\\\ \\\\hdashline 0 & 9 & 2 & 0 & 0 \\\\\\\\ \\\\hdashline 0 & 5 & 0 & 6 & 7 \\\\\\\\ 0 & 0 & 4 & 3 & 4\\\\end{array}\\\\right]$\\n\\n$M$ is a (lower) triangular block matrix; hence, evaluate the determinant of each diagonal block:\\n\\n$$\\n\\\\left|\\\\begin{array}{ll}\\n3 & 4 \\\\\\\\\\n2 & 5\\n\\\\end{array}\\\\right|=15-8=7, \\\\quad|2|=2, \\\\quad\\\\left|\\\\begin{array}{ll}\\n6 & 7 \\\\\\\\\\n3 & 4\\n\\\\end{array}\\\\right|=24-21=3\\n$$\\n\\nThus, $|M|=7(2)(3)=42$.\\n',\n",
       " '\\n8.13. Find the determinant of $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{3}$ defined by\\n\\n$$\\nF(x, y, z)=(x+3 y-4 z, 2 y+7 z, x+5 y-3 z)\\n$$\\n\\nThe determinant of a linear operator $F$ is equal to the determinant of any matrix that represents $F$. Thus first find the matrix $A$ representing $F$ in the usual basis (whose rows, respectively, consist of the coefficients of $x, y, z$ ). Then\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 3 & -4 \\\\\\\\\\n0 & 2 & 7 \\\\\\\\\\n1 & 5 & -3\\n\\\\end{array}\\\\right], \\\\quad \\\\text { and so } \\\\quad \\\\operatorname{det}(F)=|A|=-6+21+0+8-35-0=-8\\n$$\\n',\n",
       " '\\n8.14. Write out $g=g\\\\left(x_{1}, x_{2}, x_{3}, x_{4}\\\\right)$ explicitly where\\n\\n$$\\ng\\\\left(x_{1}, x_{2}, \\\\ldots, x_{n}\\\\right)=\\\\prod_{i<j}\\\\left(x_{i}-x_{j}\\\\right)\\n$$\\n\\nThe symbol $\\\\prod$ is used for a product of terms in the same way that the symbol $\\\\sum$ is used for a sum of terms. That is, $\\\\prod_{i<j}\\\\left(x_{i}-x_{j}\\\\right)$ means the product of all terms $\\\\left(x_{i}-x_{j}\\\\right)$ for which $i<j$. Hence,\\n\\n$$\\ng=g\\\\left(x_{1}, \\\\ldots, x_{4}\\\\right)=\\\\left(x_{1}-x_{2}\\\\right)\\\\left(x_{1}-x_{3}\\\\right)\\\\left(x_{1}-x_{4}\\\\right)\\\\left(x_{2}-x_{3}\\\\right)\\\\left(x_{2}-x_{4}\\\\right)\\\\left(x_{3}-x_{4}\\\\right)\\n$$\\n',\n",
       " '\\n8.15. Let $D$ be a 2-linear, alternating function. Show that $D(A, B)=-D(B, A)$.\\n\\nBecause $D$ is alternating, $D(A, A)=0, D(B, B)=0$. Hence,\\n\\n$$\\nD(A+B, A+B)=D(A, A)+D(A, B)+D(B, A)+D(B, B)=D(A, B)+D(B, A)\\n$$\\n\\nHowever, $D(A+B, A+B)=0$. Hence, $D(A, B)=-D(B, A)$, as required.\\n\\n\\n\\\\section*{Permutations}\\n',\n",
       " '8.16. Determine the parity (sign) of the permutation $\\\\sigma=364152$.\\n\\nCount the number of inversions. That is, for each element $k$, count the number of elements $i$ in $\\\\sigma$ such that $i>k$ and $i$ precedes $k$ in $\\\\sigma$. Namely,\\n\\n$$\\n\\\\begin{array}{llll}\\nk=1: & 3 \\\\text { numbers }(3,6,4) & k=4: & 1 \\\\text { number }(6) \\\\\\\\\\nk=2: & 4 \\\\text { numbers }(3,6,4,5) & k=5: & 1 \\\\text { number }(6) \\\\\\\\\\nk=3: & 0 \\\\text { numbers } & k=6: & 0 \\\\text { numbers }\\n\\\\end{array}\\n$$\\n\\nBecause $3+4+0+1+1+0=9$ is odd, $\\\\sigma$ is an odd permutation, and $\\\\operatorname{sgn} \\\\sigma=-1$.\\n',\n",
       " '\\n8.17. Let $\\\\sigma=24513$ and $\\\\tau=41352$ be permutations in $S_{5}$. Find (a) $\\\\tau \\\\circ \\\\sigma$, (b) $\\\\sigma^{-1}$.\\n\\nRecall that $\\\\sigma=24513$ and $\\\\tau=41352$ are short ways of writing\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\sigma=\\\\left(\\\\begin{array}{lllll}\\n1 & 2 & 3 & 4 & 5 \\\\\\\\\\n2 & 4 & 5 & 1 & 3\\n\\\\end{array}\\\\right) \\\\quad \\\\text { or } \\\\quad \\\\sigma(1)=2, \\\\quad \\\\sigma(2)=4, \\\\quad \\\\sigma(3)=5, \\\\quad \\\\sigma(4)=1, \\\\quad \\\\sigma(5)=3 \\\\\\\\\\n& \\\\tau=\\\\left(\\\\begin{array}{ccccc}\\n1 & 2 & 3 & 4 & 5 \\\\\\\\\\n4 & 1 & 3 & 5 & 2 c\\n\\\\end{array}\\\\right) \\\\quad \\\\text { or } \\\\quad \\\\tau(1)=4, \\\\quad \\\\tau(2)=1, \\\\quad \\\\tau(3)=3, \\\\quad \\\\tau(4)=5, \\\\quad \\\\tau(5)=2\\n\\\\end{aligned}\\n$$\\n\\n(a) The effects of $\\\\sigma$ and then $\\\\tau$ on $1,2,3,4,5$ are as follows:\\n\\n$$\\n1 \\\\rightarrow 2 \\\\rightarrow 1, \\\\quad 2 \\\\rightarrow 4 \\\\rightarrow 5, \\\\quad 3 \\\\rightarrow 5 \\\\rightarrow 2, \\\\quad 4 \\\\rightarrow 1 \\\\rightarrow 4, \\\\quad 5 \\\\rightarrow 3 \\\\rightarrow 3\\n$$\\n\\n[That is, for example, $(\\\\tau \\\\circ \\\\sigma)(1)=\\\\tau(\\\\sigma(1))=\\\\tau(2)=1$.] Thus, $\\\\tau \\\\circ \\\\sigma=15243$.\\n\\n(b) By definition, $\\\\sigma^{-1}(j)=k$ if and only if $\\\\sigma(k)=j$. Hence,\\n\\n$$\\n\\\\sigma^{-1}=\\\\left(\\\\begin{array}{lllll}\\n2 & 4 & 5 & 1 & 3 \\\\\\\\\\n1 & 2 & 3 & 4 & 5\\n\\\\end{array}\\\\right)=\\\\left(\\\\begin{array}{lllll}\\n1 & 2 & 3 & 4 & 5 \\\\\\\\\\n4 & 1 & 5 & 2 & 3\\n\\\\end{array}\\\\right) \\\\quad \\\\text { or } \\\\quad \\\\sigma^{-1}=41523\\n$$\\n',\n",
       " '\\n8.18. Let $\\\\sigma=j_{1} j_{2} \\\\ldots j_{n}$ be any permutation in $S_{n}$. Show that, for each inversion $(i, k)$ where $i>k$ but $i$ precedes $k$ in $\\\\sigma$, there is a pair $\\\\left(i^{*}, j^{*}\\\\right)$ such that\\n\\n\\n\\\\begin{equation*}\\ni^{*}<k^{*} \\\\quad \\\\text { and } \\\\quad \\\\sigma\\\\left(i^{*}\\\\right)>\\\\sigma\\\\left(j^{*}\\\\right) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nand vice versa. Thus, $\\\\sigma$ is even or odd according to whether there is an even or an odd number of pairs satisfying (1).\\n\\nChoose $i^{*}$ and $k^{*}$ so that $\\\\sigma\\\\left(i^{*}\\\\right)=i$ and $\\\\sigma\\\\left(k^{*}\\\\right)=k$. Then $i>k$ if and only if $\\\\sigma\\\\left(i^{*}\\\\right)>\\\\sigma\\\\left(k^{*}\\\\right)$, and $i$ precedes $k$ in $\\\\sigma$ if and only if $i^{*}<k^{*}$.\\n',\n",
       " '\\n8.19. Consider the polynomials $g=g\\\\left(x_{1}, \\\\ldots, x_{n}\\\\right)$ and $\\\\sigma(g)$, defined by\\n\\n$$\\ng=g\\\\left(x_{1}, \\\\ldots, x_{n}\\\\right)=\\\\prod_{i<j}\\\\left(x_{i}-x_{j}\\\\right) \\\\quad \\\\text { and } \\\\quad \\\\sigma(g)=\\\\prod_{i<j}\\\\left(x_{\\\\sigma(i)}-x_{\\\\sigma(j)}\\\\right)\\n$$\\n\\n(See Problem 8.14.) Show that $\\\\sigma(g)=g$ when $\\\\sigma$ is an even permutation, and $\\\\sigma(g)=-g$ when $\\\\sigma$ is an odd permutation. That is, $\\\\sigma(g)=(\\\\operatorname{sgn} \\\\sigma) g$.\\n\\nBecause $\\\\sigma$ is one-to-one and onto,\\n\\n$$\\n\\\\sigma(g)=\\\\prod_{i<j}\\\\left(x_{\\\\sigma(i)}-x_{\\\\sigma(j)}\\\\right)=\\\\prod_{i<j \\\\text { or } i>j}\\\\left(x_{i}-x_{j}\\\\right)\\n$$\\n\\nThus, $\\\\sigma(g)$ or $\\\\sigma(g)=-g$ according to whether there is an even or an odd number of terms of the form $x_{i}-x_{j}$, where $i>j$. Note that for each pair $(i, j)$ for which\\n\\n$$\\ni<j \\\\quad \\\\text { and } \\\\quad \\\\sigma(i)>\\\\sigma(j)\\n$$\\n\\nthere is a term $\\\\left(x_{\\\\sigma(i)}-x_{\\\\sigma(j)}\\\\right)$ in $\\\\sigma(g)$ for which $\\\\sigma(i)>\\\\sigma(j)$. Because $\\\\sigma$ is even if and only if there is an even number of pairs satisfying (1), we have $\\\\sigma(g)=g$ if and only if $\\\\sigma$ is even. Hence, $\\\\sigma(g)=-g$ if and only if $\\\\sigma$ is odd.\\n',\n",
       " '\\n8.20. Let $\\\\sigma, \\\\tau \\\\in S_{n}$. Show that $\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma)=(\\\\operatorname{sgn} \\\\tau)(\\\\operatorname{sgn} \\\\sigma)$. Thus, the product of two even or two odd permutations is even, and the product of an odd and an even permutation is odd.\\n\\nUsing Problem 8.19, we have\\n\\n$$\\n\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma) g=(\\\\tau \\\\circ \\\\sigma)(g)=\\\\tau(\\\\sigma(g))=\\\\tau((\\\\operatorname{sgn} \\\\sigma) g)=(\\\\operatorname{sgn} \\\\tau)(\\\\operatorname{sgn} \\\\sigma) g\\n$$\\n\\nAccordingly, $\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma)=(\\\\operatorname{sgn} \\\\tau)(\\\\operatorname{sgn} \\\\sigma)$.\\n',\n",
       " '\\n8.21. Consider the permutation $\\\\sigma=j_{1} j_{2} \\\\cdots j_{n}$. Show that $\\\\operatorname{sgn} \\\\sigma^{-1}=\\\\operatorname{sgn} \\\\sigma$ and, for scalars $a_{i j}$, show that\\n\\n$$\\na_{j_{1} 1} a_{j_{2} 2} \\\\cdots a_{j_{n} n}=a_{1 k_{1}} a_{2 k_{2}} \\\\cdots a_{n k_{n}}\\n$$\\n\\nwhere $\\\\sigma^{-1}=k_{1} k_{2} \\\\cdots k_{n}$.\\n\\nWe have $\\\\sigma^{-1} \\\\circ \\\\sigma=\\\\varepsilon$, the identity permutation. Because $\\\\varepsilon$ is even, $\\\\sigma^{-1}$ and $\\\\sigma$ are both even or both odd. Hence $\\\\operatorname{sgn} \\\\sigma^{-1}=\\\\operatorname{sgn} \\\\sigma$.\\n\\nBecause $\\\\sigma=j_{1} j_{2} \\\\cdots j_{n}$ is a permutation, $a_{j_{1} 1} a_{j_{2} 2} \\\\cdots a_{j_{n} n}=a_{1 k_{1}} a_{2 k_{2}} \\\\cdots a_{n k_{n}}$. Then $k_{1}, k_{2}, \\\\ldots, k_{n}$ have the property that\\n\\n$$\\n\\\\sigma\\\\left(k_{1}\\\\right)=1, \\\\quad \\\\sigma\\\\left(k_{2}\\\\right)=2, \\\\quad \\\\ldots, \\\\quad \\\\sigma\\\\left(k_{n}\\\\right)=n\\n$$\\n\\nLet $\\\\tau=k_{1} k_{2} \\\\cdots k_{n}$. Then, for $i=1, \\\\ldots, n$,\\n\\n$$\\n(\\\\sigma \\\\circ \\\\tau)(i)=\\\\sigma(\\\\tau(i))=\\\\sigma\\\\left(k_{i}\\\\right)=i\\n$$\\n\\nThus, $\\\\sigma \\\\circ \\\\tau=\\\\varepsilon$, the identity permutation. Hence, $\\\\tau=\\\\sigma^{-1}$.\\n\\n\\n\\\\section*{Proofs of Theorems}\\n',\n",
       " '8.22. Prove Theorem 8.1: $\\\\left|A^{T}\\\\right|=|A|$.\\n\\nIf $A=\\\\left[a_{i j}\\\\right]$, then $A^{T}=\\\\left[b_{i j}\\\\right]$, with $b_{i j}=a_{j i}$. Hence,\\n\\n$$\\n\\\\left|A^{T}\\\\right|=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\sigma) b_{1 \\\\sigma(1)} b_{2 \\\\sigma(2)} \\\\cdots b_{n \\\\sigma(n)}=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\sigma) a_{\\\\sigma(1), 1} a_{\\\\sigma(2), 2} \\\\cdots a_{\\\\sigma(n), n}\\n$$\\n\\nLet $\\\\tau=\\\\sigma^{-1}$. By Problem $8.21 \\\\operatorname{sgn} \\\\tau=\\\\operatorname{sgn} \\\\sigma$, and $a_{\\\\sigma(1), 1} a_{\\\\sigma(2), 2} \\\\cdots a_{\\\\sigma(n), n}=a_{1 \\\\tau(1)} a_{2 \\\\tau(2)} \\\\cdots a_{n \\\\tau(n)}$. Hence,\\n\\n$$\\n\\\\left|A^{T}\\\\right|=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\tau) a_{1 t(1)} a_{2 \\\\tau(2)} \\\\cdots a_{n \\\\tau(n)}\\n$$\\n\\nHowever, as $\\\\sigma$ runs through all the elements of $S_{n}, \\\\tau=\\\\sigma^{-1}$ also runs through all the elements of $S_{n}$. Thus, $\\\\left|A^{T}\\\\right|=|A|$.\\n',\n",
       " '\\n8.23. Prove Theorem 8.3(i): If two rows (columns) of $A$ are interchanged, then $|B|=-|A|$.\\n\\nWe prove the theorem for the case that two columns are interchanged. Let $\\\\tau$ be the transposition that interchanges the two numbers corresponding to the two columns of $A$ that are interchanged. If $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$, then $b_{i j}=a_{i \\\\tau(j)}$. Hence, for any permutation $\\\\sigma$,\\n\\n$$\\nb_{1 \\\\sigma(1)} b_{2 \\\\sigma(2)} \\\\cdots b_{n \\\\sigma(n)}=a_{1(\\\\tau \\\\circ \\\\sigma)(1)} a_{2(\\\\tau \\\\circ \\\\sigma)(2)} \\\\cdots a_{n(\\\\tau \\\\circ \\\\sigma)(n)}\\n$$\\n\\nThus,\\n\\n$$\\n|B|=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\sigma) b_{1 \\\\sigma(1)} b_{2 \\\\sigma(2)} \\\\cdots b_{n \\\\sigma(n)}=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\sigma) a_{1(\\\\tau \\\\circ \\\\sigma)(1)} a_{2(\\\\tau \\\\circ \\\\sigma)(2)} \\\\cdots a_{n(\\\\tau \\\\circ \\\\sigma)(n)}\\n$$\\n\\nBecause the transposition $\\\\tau$ is an odd permutation, $\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma)=(\\\\operatorname{sgn} \\\\tau)(\\\\operatorname{sgn} \\\\sigma)=-\\\\operatorname{sgn} \\\\sigma$. Accordingly, $\\\\operatorname{sgn} \\\\sigma=-\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma)$, and so\\n\\n$$\\n|B|=-\\\\sum_{\\\\sigma \\\\in S_{n}}[\\\\operatorname{sgn}(\\\\tau \\\\circ \\\\sigma)] a_{1(\\\\tau \\\\circ \\\\sigma)(1)} a_{2(\\\\tau \\\\circ \\\\sigma)(2)} \\\\cdots a_{n(\\\\tau \\\\circ \\\\sigma)(n)}\\n$$\\n\\nBut as $\\\\sigma$ runs through all the elements of $S_{n}, \\\\tau \\\\circ \\\\sigma$ also runs through all the elements of $S_{n}$. Hence, $|B|=-|A|$.\\n\\n\\\\subsection*{8.24. Prove Theorem 8.2 .}\\n(i) If $A$ has a row (column) of zeros, then $|A|=0$.\\n\\n(ii) If $A$ has two identical rows (columns), then $|A|=0$.\\n\\n(iii) If $A$ is triangular, then $|A|=$ product of diagonal elements. Thus, $|I|=1$.\\n\\n(i) Each term in $|A|$ contains a factor from every row, and so from the row of zeros. Thus, each term of $|A|$ is zero, and so $|A|=0$.\\n\\n(ii) Suppose $1+1 \\\\neq 0$ in $K$. If we interchange the two identical rows of $A$, we still obtain the matrix $A$. Hence, by Problem 8.23, $|A|=-|A|$, and so $|A|=0$.\\n\\nNow suppose $1+1=0$ in $K$. Then $\\\\operatorname{sgn} \\\\sigma=1$ for every $\\\\sigma \\\\in S_{n}$. Because $A$ has two identical rows, we can arrange the terms of $A$ into pairs of equal terms. Because each pair is 0 , the determinant of $A$ is zero.\\n\\n(iii) Suppose $A=\\\\left[a_{i j}\\\\right]$ is lower triangular; that is, the entries above the diagonal are all zero: $a_{i j}=0$ whenever $i<j$. Consider a term $t$ of the determinant of $A$ :\\n\\n$$\\nt=(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{n i_{n}}, \\\\quad \\\\text { where } \\\\quad \\\\sigma=i_{1} i_{2} \\\\cdots i_{n}\\n$$\\n\\nSuppose $i_{1} \\\\neq 1$. Then $1<i_{1}$ and so $a_{1 i_{1}}=0$; hence, $t=0$. That is, each term for which $i_{1} \\\\neq 1$ is zero.\\n\\nNow suppose $i_{1}=1$ but $i_{2} \\\\neq 2$. Then $2<i_{2}$, and so $a_{2 i_{2}}=0$; hence, $t=0$. Thus, each term for which $i_{1} \\\\neq 1$ or $i_{2} \\\\neq 2$ is zero.\\n\\nSimilarly, we obtain that each term for which $i_{1} \\\\neq 1$ or $i_{2} \\\\neq 2$ or $\\\\ldots$ or $i_{n} \\\\neq n$ is zero. Accordingly, $|A|=a_{11} a_{22} \\\\cdots a_{n n}=$ product of diagonal elements.\\n',\n",
       " '\\n8.25. Prove Theorem 8.3: $B$ is obtained from $A$ by an elementary operation.\\n\\n(i) If two rows (columns) of $A$ were interchanged, then $|B|=-|A|$.\\n\\n(ii) If a row (column) of $A$ were multiplied by a scalar $k$, then $|B|=k|A|$.\\n\\n(iii) If a multiple of a row (column) of $A$ were added to another row (column) of $A$, then $|B|=|A|$.\\n\\n(i) This result was proved in Problem 8.23.\\n\\n(ii) If the $j$ th row of $A$ is multiplied by $k$, then every term in $|A|$ is multiplied by $k$, and so $|B|=k|A|$. That is,\\n\\n$$\\n|B|=\\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots\\\\left(k a_{j i_{j}}\\\\right) \\\\cdots a_{n i_{n}}=k \\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{n i_{n}}=k|A|\\n$$\\n\\n(iii) Suppose $c$ times the $k$ th row is added to the $j$ th row of $A$. Using the symbol ${ }^{\\\\wedge}$ to denote the $j$ th position in a determinant term, we have\\n\\n$$\\n\\\\begin{aligned}\\n|B| & =\\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots\\\\left(\\\\widehat{c a_{k i_{k}}+a_{j i_{j}}}\\\\right) \\\\ldots a_{n i_{n}} \\\\\\\\\\n& =c \\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots \\\\widehat{a_{k i_{k}}} \\\\cdots a_{n i_{n}}+\\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{j i_{j}} \\\\cdots a_{n i_{n}}\\n\\\\end{aligned}\\n$$\\n\\nThe first sum is the determinant of a matrix whose $k$ th and $j$ th rows are identical. Accordingly, by Theorem 8.2(ii), the sum is zero. The second sum is the determinant of $A$. Thus, $|B|=c \\\\cdot 0+|A|=|A|$.\\n',\n",
       " '\\n8.26. Prove Lemma 8.6: Let $E$ be an elementary matrix. Then $|E A|=|E||A|$.\\n\\nConsider the elementary row operations: (i) Multiply a row by a constant $k \\\\neq 0$,\\n\\n(ii) Interchange two rows, (iii) Add a multiple of one row to another.\\n\\nLet $E_{1}, E_{2}, E_{3}$ be the corresponding elementary matrices That is, $E_{1}, E_{2}, E_{3}$ are obtained by applying the above operations to the identity matrix $I$. By Problem 8.25,\\n\\n$$\\n\\\\left|E_{1}\\\\right|=k|I|=k, \\\\quad\\\\left|E_{2}\\\\right|=-|I|=-1, \\\\quad\\\\left|E_{3}\\\\right|=|I|=1\\n$$\\n\\nRecall (Theorem 3.11) that $E_{i} A$ is identical to the matrix obtained by applying the corresponding operation to $A$. Thus, by Theorem 8.3 , we obtain the following which proves our lemma:\\n\\n$$\\n\\\\left|E_{1} A\\\\right|=k|A|=\\\\left|E_{1}\\\\right||A|, \\\\quad\\\\left|E_{2} A\\\\right|=-|A|=\\\\left|E_{2}\\\\right||A|, \\\\quad\\\\left|E_{3} A\\\\right|=|A|=1|A|=\\\\left|E_{3}\\\\right||A|\\n$$\\n',\n",
       " '\\n8.27. Suppose $B$ is row equivalent to a square matrix $A$. Prove that $|B|=0$ if and only if $|A|=0$.\\n\\nBy Theorem 8.3, the effect of an elementary row operation is to change the sign of the determinant or to multiply the determinant by a nonzero scalar. Hence, $|B|=0$ if and only if $|A|=0$.\\n',\n",
       " '\\n8.28. Prove Theorem 8.5 : Let $A$ be an $n$-square matrix. Then the following are equivalent:\\\\\\\\\\n(i) $A$ is invertible,\\\\\\\\\\n(ii) $A X=0$ has only the zero solution,\\\\\\\\\\n(iii) $\\\\operatorname{det}(A) \\\\neq 0$.\\n\\nThe proof is by the Gaussian algorithm. If $A$ is invertible, it is row equivalent to $I$. But $|I| \\\\neq 0$. Hence, by Problem 8.27, $|A| \\\\neq 0$. If $A$ is not invertible, it is row equivalent to a matrix with a zero row. Hence, $\\\\operatorname{det}(A)=0$. Thus, (i) and (iii) are equivalent.\\n\\nIf $A X=0$ has only the solution $X=0$, then $A$ is row equivalent to $I$ and $A$ is invertible. Conversely, if $A$ is invertible with inverse $A^{-1}$, then\\n\\n$$\\nX=I X=\\\\left(A^{-1} A\\\\right) X=A^{-1}(A X)=A^{-1} 0=0\\n$$\\n\\nis the only solution of $A X=0$. Thus, (i) and (ii) are equivalent.\\n',\n",
       " '\\n8.29. Prove Theorem 8.4: $|A B|=|A||B|$.\\n\\nIf $A$ is singular, then $A B$ is also singular, and so $|A B|=0=|A||B|$. On the other hand, if $A$ is nonsingular, then $A=E_{n} \\\\cdots E_{2} E_{1}$, a product of elementary matrices. Then, Lemma 8.6 and induction yields\\n\\n$$\\n|A B|=\\\\left|E_{n} \\\\cdots E_{2} E_{1} B\\\\right|=\\\\left|E_{n}\\\\right| \\\\cdots\\\\left|E_{2}\\\\right|\\\\left|E_{1}\\\\right||B|=|A||B|\\n$$\\n',\n",
       " '\\n8.30. Suppose $P$ is invertible. Prove that $\\\\left|P^{-1}\\\\right|=|P|^{-1}$.\\n\\n$$\\nP^{-1} P=I \\\\text {. Hence, } 1=|I|=\\\\left|P^{-1} P\\\\right|=\\\\left|P^{-1}\\\\right||P| \\\\text {, and so }\\\\left|P^{-1}\\\\right|=|P|^{-1} \\\\text {. }\\n$$\\n',\n",
       " '\\n8.31. Prove Theorem 8.7: Suppose $A$ and $B$ are similar matrices. Then $|A|=|B|$.\\n\\nBecause $A$ and $B$ are similar, there exists an invertible matrix $P$ such that $B=P^{-1} A P$. Therefore, using Problem 8.30, we get $|B|=\\\\left|P^{-1} A P\\\\right|=\\\\left|P^{-1}\\\\right||A||P|=|A|\\\\left|P^{-1}\\\\right||P=| A \\\\mid$.\\n\\nWe remark that although the matrices $P^{-1}$ and $A$ may not commute, their determinants $\\\\left|P^{-1}\\\\right|$ and $|A|$ do commute, because they are scalars in the field $K$.\\n',\n",
       " '\\n8.32. Prove Theorem 8.8 (Laplace): Let $A=\\\\left[a_{i j}\\\\right]$, and let $A_{i j}$ denote the cofactor of $a_{i j}$. Then, for any $i$ or $j$\\n\\n$$\\n|A|=a_{i 1} A_{i 1}+\\\\cdots+a_{i n} A_{i n} \\\\quad \\\\text { and } \\\\quad|A|=a_{1 j} A_{1 j}+\\\\cdots+a_{n j} A_{n j}\\n$$\\n\\nBecause $|A|=\\\\left|A^{T}\\\\right|$, we need only prove one of the expansions, say, the first one in terms of rows of $A$. Each term in $|A|$ contains one and only one entry of the $i$ th row $\\\\left(a_{i 1}, a_{i 2}, \\\\ldots, a_{i n}\\\\right)$ of $A$. Hence, we can write $|A|$ in the form\\n\\n$$\\n|A|=a_{i 1} A_{i 1}^{*}+a_{i 2} A_{i 2}^{*}+\\\\cdots+a_{i n} A_{i n}^{*}\\n$$\\n\\n(Note that $A_{i j}^{*}$ is a sum of terms involving no entry of the $i$ th row of $A$.) Thus, the theorem is proved if we can show that\\n\\n$$\\nA_{i j}^{*}=A_{i j}=(-1)^{i+j}\\\\left|M_{i j}\\\\right|\\n$$\\n\\nwhere $M_{i j}$ is the matrix obtained by deleting the row and column containing the entry $a_{i j}$. (Historically, the expression $A_{i j}^{*}$ was defined as the cofactor of $a_{i j}$, and so the theorem reduces to showing that the two definitions of the cofactor are equivalent.)\\n\\nFirst we consider the case that $i=n, j=n$. Then the sum of terms in $|A|$ containing $a_{n n}$ is\\n\\n$$\\na_{n n} A_{n n}^{*}=a_{n n} \\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 \\\\sigma(1)} a_{2 \\\\sigma(2)} \\\\cdots a_{n-1, \\\\sigma(n-1)}\\n$$\\n\\nwhere we sum over all permutations $\\\\sigma \\\\in S_{n}$ for which $\\\\sigma(n)=n$. However, this is equivalent (Prove!) to summing over all permutations of $\\\\{1, \\\\ldots, n-1\\\\}$. Thus, $A_{n n}^{*}=\\\\left|M_{n n}\\\\right|=(-1)^{n+n}\\\\left|M_{n n}\\\\right|$.\\n\\nNow we consider any $i$ and $j$. We interchange the $i$ th row with each succeeding row until it is last, and we interchange the $j$ th column with each succeeding column until it is last. Note that the determinant $\\\\left|M_{i j}\\\\right|$ is not affected, because the relative positions of the other rows and columns are not affected by these interchanges. However, the \"sign\" of $|A|$ and of $A_{i j}^{*}$ is changed $n-1$ and then $n-j$ times. Accordingly,\\n\\n$$\\nA_{i j}^{*}=(-1)^{n-i+n-j}\\\\left|M_{i j}\\\\right|=(-1)^{i+j}\\\\left|M_{i j}\\\\right|\\n$$\\n',\n",
       " '\\n8.33. Let $A=\\\\left[a_{i j}\\\\right]$ and let $B$ be the matrix obtained from $A$ by replacing the $i$ th row of $A$ by the row vector $\\\\left(b_{i 1}, \\\\ldots, b_{i n}\\\\right)$. Show that\\n\\n$$\\n|B|=b_{i 1} A_{i 1}+b_{i 2} A_{i 2}+\\\\cdots+b_{i n} A_{i n}\\n$$\\n\\nFurthermore, show that, for $j \\\\neq i$,\\n\\n$$\\na_{j 1} A_{i 1}+a_{j 2} A_{i 2}+\\\\cdots+a_{j n} A_{i n}=0 \\\\quad \\\\text { and } \\\\quad a_{1 j} A_{1 i}+a_{2 j} A_{2 i}+\\\\cdots+a_{n j} A_{n i}=0\\n$$\\n\\nLet $B=\\\\left[b_{i j}\\\\right]$. By Theorem 8.8 ,\\n\\n$$\\n|B|=b_{i 1} B_{i 1}+b_{i 2} B_{i 2}+\\\\cdots+b_{i n} B_{i n}\\n$$\\n\\nBecause $B_{i j}$ does not depend on the $i$ th row of $B$, we get $B_{i j}=A_{i j}$ for $j=1, \\\\ldots, n$. Hence,\\n\\n$$\\n|B|=b_{i 1} A_{i 1}+b_{i 2} A_{i 2}+\\\\cdots+b_{i n} A_{i n}\\n$$\\n\\nNow let $A^{\\\\prime}$ be obtained from $A$ by replacing the $i$ th row of $A$ by the $j$ th row of $A$. Because $A^{\\\\prime}$ has two identical rows, $\\\\left|A^{\\\\prime}\\\\right|=0$. Thus, by the above result,\\n\\n$$\\n\\\\left|A^{\\\\prime}\\\\right|=a_{j 1} A_{i 1}+a_{j 2} A_{i 2}+\\\\cdots+a_{j n} A_{i n}=0\\n$$\\n\\nUsing $\\\\left|A^{T}\\\\right|=|A|$, we also obtain that $a_{1 j} A_{1 i}+a_{2 j} A_{2 i}+\\\\cdots+a_{n j} A_{n i}=0$.\\n',\n",
       " '\\n8.34. Prove Theorem 8.9: $A(\\\\operatorname{adj} A)=(\\\\operatorname{adj} A) A=|A| I$.\\n\\nLet $A=\\\\left[a_{i j}\\\\right]$ and let $A(\\\\operatorname{adj} A)=\\\\left[b_{i j}\\\\right]$. The $i$ th row of $A$ is\\n\\n\\n\\\\begin{equation*}\\n\\\\left(a_{i 1}, a_{i 2}, \\\\ldots, a_{i n}\\\\right) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause adj $A$ is the transpose of the matrix of cofactors, the $j$ th column of adj $A$ is the tranpose of the cofactors of the $j$ th row of $A$ :\\n\\n\\n\\\\begin{equation*}\\n\\\\left(A_{j}, A_{j 2}, \\\\ldots, A_{j n}\\\\right)^{T} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nNow $b_{i j}$, the $i j$ entry in $A(\\\\operatorname{adj} A)$, is obtained by multiplying expressions (1) and (2):\\n\\n$$\\nb_{i j}=a_{i 1} A_{j 1}+a_{i 2} A_{j 2}+\\\\cdots+a_{i n} A_{j n}\\n$$\\n\\nBy Theorem 8.8 and Problem 8.33 ,\\n\\n$$\\nb_{i j}=\\\\left\\\\{\\\\begin{array}{cc}\\n|A| & \\\\text { if } i=j \\\\\\\\\\n0 & \\\\text { if } i \\\\neq j\\n\\\\end{array}\\\\right.\\n$$\\n\\nAccordingly, $A(\\\\operatorname{adj} A)$ is the diagonal matrix with each diagonal element $|A|$. In other words, $A(\\\\operatorname{adj} A)=|A| I$. Similarly, $(\\\\operatorname{adj} A) A=|A| I$.\\n',\n",
       " \"\\n8.35. Prove Theorem 8.10 (Cramer's rule): The (square) system $A X=B$ has a unique solution if and only if $D \\\\neq 0$. In this case, $x_{i}=N_{i} / D$ for each $i$.\\n\\nBy previous results, $A X=B$ has a unique solution if and only if $A$ is invertible, and $A$ is invertible if and only if $D=|A| \\\\neq 0$.\\n\\nNow suppose $D \\\\neq 0$. By Theorem $8.9, A^{-1}=(1 / D)(\\\\operatorname{adj} A)$. Multiplying $A X=B$ by $A^{-1}$, we obtain\\n\\n\\n\\\\begin{equation*}\\nX=A^{-1} A X=(1 / D)(\\\\operatorname{adj} A) B \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nNote that the $i$ th row of $(1 / D)(\\\\operatorname{adj} A)$ is $(1 / D)\\\\left(A_{1 i}, A_{2 i}, \\\\ldots, A_{n i}\\\\right)$. If $B=\\\\left(b_{1}, b_{2}, \\\\ldots, b_{n}\\\\right)^{T}$, then, by (1),\\n\\n$$\\nx_{i}=(1 / D)\\\\left(b_{1} A_{1 i}+b_{2} A_{2 i}+\\\\cdots+b_{n} A_{n i}\\\\right)\\n$$\\n\\nHowever, as in Problem 8.33, $b_{1} A_{1 i}+b_{2} A_{2 i}+\\\\cdots+b_{n} A_{n i}=N_{i}$, the determinant of the matrix obtained by replacing the $i$ th column of $A$ by the column vector $B$. Thus, $x_{i}=(1 / D) N_{i}$, as required.\\n\",\n",
       " '\\n8.36. Prove Theorem 8.12: Suppose $M$ is an upper (lower) triangular block matrix with diagonal blocks $A_{1}, A_{2}, \\\\ldots, A_{n}$. Then\\n\\n$$\\n\\\\operatorname{det}(M)=\\\\operatorname{det}\\\\left(A_{1}\\\\right) \\\\operatorname{det}\\\\left(A_{2}\\\\right) \\\\cdots \\\\operatorname{det}\\\\left(A_{n}\\\\right)\\n$$\\n\\nWe need only prove the theorem for $n=2$ - that is, when $M$ is a square matrix of the form\\n\\n$M=\\\\left[\\\\begin{array}{cc}A & C \\\\\\\\ 0 & B\\\\end{array}\\\\right]$. The proof of the general theorem follows easily by induction.\\n\\nSuppose $A=\\\\left[a_{i j}\\\\right]$ is $r$-square, $B=\\\\left[b_{i j}\\\\right]$ is $s$-square, and $M=\\\\left[m_{i j}\\\\right]$ is $n$-square, where $n=r+s$. By definition,\\n\\n$$\\n\\\\operatorname{det}(M)=\\\\sum_{\\\\sigma \\\\in S_{n}}(\\\\operatorname{sgn} \\\\sigma) m_{1 \\\\sigma(1)} m_{2 \\\\sigma(2)} \\\\cdots m_{n \\\\sigma(n)}\\n$$\\n\\nIf $i>r$ and $j \\\\leq r$, then $m_{i j}=0$. Thus, we need only consider those permutations $\\\\sigma$ such that\\n\\n$$\\n\\\\sigma\\\\{r+1, r+2, \\\\ldots, r+s\\\\}=\\\\{r+1, r+2, \\\\ldots, r+s\\\\} \\\\quad \\\\text { and } \\\\quad \\\\sigma\\\\{1,2, \\\\ldots, r\\\\}=\\\\{1,2, \\\\ldots, r\\\\}\\n$$\\n\\nLet $\\\\sigma_{1}(k)=\\\\sigma(k)$ for $k \\\\leq r$, and let $\\\\sigma_{2}(k)=\\\\sigma(r+k)-r$ for $k \\\\leq s$. Then\\n\\n$$\\n(\\\\operatorname{sgn} \\\\sigma) m_{1 \\\\sigma(1)} m_{2 \\\\sigma(2)} \\\\cdots m_{n \\\\sigma(n)}=\\\\left(\\\\operatorname{sgn} \\\\sigma_{1}\\\\right) a_{1 \\\\sigma_{1}(1)} a_{2 \\\\sigma_{1}(2)} \\\\cdots a_{r \\\\sigma_{1}(r)}\\\\left(\\\\operatorname{sgn} \\\\sigma_{2}\\\\right) b_{1 \\\\sigma_{2}(1)} b_{2 \\\\sigma_{2}(2)} \\\\cdots b_{s \\\\sigma_{2}(s)}\\n$$\\n\\nwhich implies $\\\\operatorname{det}(M)=\\\\operatorname{det}(A) \\\\operatorname{det}(B)$.\\n',\n",
       " '\\n8.37. Prove Theorem 8.14: There exists a unique function $D: \\\\mathbf{M} \\\\rightarrow K$ such that\\n\\n(i) $D$ is multilinear, (ii) $D$ is alternating, (iii) $D(I)=1$.\\n\\nThis function $D$ is the determinant function; that is, $D(A)=|A|$.\\n\\nLet $D$ be the determinant function, $D(A)=|A|$. We must show that $D$ satisfies (i), (ii), and (iii), and that $D$ is the only function satisfying (i), (ii), and (iii).\\n\\nBy Theorem 8.2, $D$ satisfies (ii) and (iii). Hence, we show that it is multilinear. Suppose the $i$ th row of $A=\\\\left[a_{i j}\\\\right]$ has the form $\\\\left(b_{i 1}+c_{i 1}, b_{i 2}+c_{i 2}, \\\\ldots, b_{i n}+c_{i n}\\\\right)$. Then\\n\\n$$\\n\\\\begin{aligned}\\nD(A) & =D\\\\left(A_{1}, \\\\ldots, B_{i}+C_{i}, \\\\ldots, A_{n}\\\\right) \\\\\\\\\\n& =\\\\sum_{S_{n}}(\\\\operatorname{sgn} \\\\sigma) a_{1 \\\\sigma(1)} \\\\cdots a_{i-1, \\\\sigma(i-1)}\\\\left(b_{i \\\\sigma(i)}+c_{i \\\\sigma(i)}\\\\right) \\\\cdots a_{n \\\\sigma(n)} \\\\\\\\\\n& =\\\\sum_{S_{n}}(\\\\operatorname{sgn} \\\\sigma) a_{1 \\\\sigma(1)} \\\\cdots b_{i \\\\sigma(i)} \\\\cdots a_{n \\\\sigma(n)}+\\\\sum_{S_{n}}(\\\\operatorname{sgn} \\\\sigma) a_{1 \\\\sigma(1)} \\\\cdots c_{i \\\\sigma(i)} \\\\cdots a_{n \\\\sigma(n)} \\\\\\\\\\n& =D\\\\left(A_{1}, \\\\ldots, B_{i}, \\\\ldots, A_{n}\\\\right)+D\\\\left(A_{1}, \\\\ldots, C_{i}, \\\\ldots, A_{n}\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nAlso, by Theorem 8.3(ii),\\n\\n$$\\nD\\\\left(A_{1}, \\\\ldots, k A_{i}, \\\\ldots, A_{n}\\\\right)=k D\\\\left(A_{1}, \\\\ldots, A_{i}, \\\\ldots, A_{n}\\\\right)\\n$$\\n\\nThus, $D$ is multilinear $-D$ satisfies (i).\\n\\nWe next must prove the uniqueness of $D$. Suppose $D$ satisfies (i), (ii), and (iii). If $\\\\left\\\\{e_{1}, \\\\ldots, e_{n}\\\\right\\\\}$ is the usual basis of $K^{n}$, then, by (iii), $D\\\\left(e_{1}, e_{2}, \\\\ldots, e_{n}\\\\right)=D(I)=1$. Using (ii), we also have that\\n\\n\\n\\\\begin{equation*}\\nD\\\\left(e_{i_{1}}, e_{i_{2}}, \\\\ldots, e_{i_{n}}\\\\right)=\\\\operatorname{sgn} \\\\sigma, \\\\quad \\\\text { where } \\\\quad \\\\sigma=i_{1} i_{2} \\\\cdots i_{n} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nNow suppose $A=\\\\left[a_{i j}\\\\right]$. Observe that the $k$ th $\\\\operatorname{row} A_{k}$ of $A$ is\\n\\n$$\\nA_{k}=\\\\left(a_{k 1}, a_{k 2}, \\\\ldots, a_{k n}\\\\right)=a_{k 1} e_{1}+a_{k 2} e_{2}+\\\\cdots+a_{k n} e_{n}\\n$$\\n\\nThus,\\n\\n$$\\nD(A)=D\\\\left(a_{11} e_{1}+\\\\cdots+a_{1 n} e_{n}, a_{21} e_{1}+\\\\cdots+a_{2 n} e_{n}, \\\\ldots, a_{n 1} e_{1}+\\\\cdots+a_{n n} e_{n}\\\\right)\\n$$\\n\\nUsing the multilinearity of $D$, we can write $D(A)$ as a sum of terms of the form\\n\\n\\n\\\\begin{align*}\\nD(A) & =\\\\sum D\\\\left(a_{1 i_{1}} e_{i_{1}}, a_{2 i_{2}} e_{i_{2}}, \\\\ldots, a_{n i_{n}} e_{i_{n}}\\\\right) \\\\\\\\\\n& =\\\\sum\\\\left(a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{n i_{n}}\\\\right) D\\\\left(e_{i_{1}}, e_{i_{2}}, \\\\ldots, e_{i_{n}}\\\\right) \\\\tag{2}\\n\\\\end{align*}\\n\\n\\nwhere the sum is summed over all sequences $i_{1} i_{2} \\\\ldots i_{n}$, where $i_{k} \\\\in\\\\{1, \\\\ldots, n\\\\}$. If two of the indices are equal, say $i_{j}=i_{k}$ but $j \\\\neq k$, then, by (ii),\\n\\n$$\\nD\\\\left(e_{i_{1}}, e_{i_{2}}, \\\\ldots, e_{i_{n}}\\\\right)=0\\n$$\\n\\nAccordingly, the sum in (2) need only be summed over all permutations $\\\\sigma=i_{1} i_{2} \\\\cdots i_{n}$. Using (1), we finally have that\\n\\n$$\\n\\\\begin{aligned}\\nD(A) & =\\\\sum_{\\\\sigma}\\\\left(a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{n i_{n}}\\\\right) D\\\\left(e_{i_{1}}, e_{i_{2}}, \\\\ldots, e_{i_{n}}\\\\right) \\\\\\\\\\n& =\\\\sum_{\\\\sigma}(\\\\operatorname{sgn} \\\\sigma) a_{1 i_{1}} a_{2 i_{2}} \\\\cdots a_{n i_{n}}, \\\\quad \\\\text { where } \\\\quad \\\\sigma=i_{1} i_{2} \\\\cdots i_{n}\\n\\\\end{aligned}\\n$$\\n\\nHence, $D$ is the determinant function, and so the theorem is proved.\\n\\n',\n",
       " '9.1. Let $A=\\\\left[\\\\begin{array}{rr}1 & -2 \\\\\\\\ 4 & 5\\\\end{array}\\\\right]$. Find $f(A)$, where\\\\\\\\\\n(a) $f(t)=t^{2}-3 t+7$,\\\\\\\\\\n(b) $f(t)=t^{2}-6 t+13$\\n\\nFirst find $A^{2}=\\\\left[\\\\begin{array}{rr}1 & -2 \\\\\\\\ 4 & 5\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & -2 \\\\\\\\ 4 & 5\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}-7 & -12 \\\\\\\\ 24 & 17\\\\end{array}\\\\right]$. Then\\n\\n(a) $f(A)=A^{2}-3 A+7 I=\\\\left[\\\\begin{array}{rr}-7 & -12 \\\\\\\\ 24 & 17\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}-3 & 6 \\\\\\\\ -12 & -15\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{ll}7 & 0 \\\\\\\\ 0 & 7\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}-3 & -6 \\\\\\\\ 12 & 9\\\\end{array}\\\\right]$\\n\\n(b) $f(A)=A^{2}-6 A+13 I=\\\\left[\\\\begin{array}{rr}-7 & -12 \\\\\\\\ 24 & 17\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}-6 & 12 \\\\\\\\ -24 & -30\\\\end{array}\\\\right]+\\\\left[\\\\begin{array}{rr}13 & 0 \\\\\\\\ 0 & 13\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}0 & 0 \\\\\\\\ 0 & 0\\\\end{array}\\\\right]$\\n\\n[Thus, $A$ is a root of $f(t)$.]\\n',\n",
       " '\\n9.2. Find the characteristic polynomial $\\\\Delta(t)$ of each of the following matrices:\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{ll}2 & 5 \\\\\\\\ 4 & 1\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{ll}7 & -3 \\\\\\\\ 5 & -2\\\\end{array}\\\\right]$\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{ll}3 & -2 \\\\\\\\ 9 & -3\\\\end{array}\\\\right]$\\n\\nUse the formula $(t)=t^{2}-\\\\operatorname{tr}(M) t+|M|$ for a $2 \\\\times 2$ matrix $M$ :\\\\\\\\\\n(a) $\\\\operatorname{tr}(A)=2+1=3, \\\\quad|A|=2-20=-18$,\\\\\\\\\\nso $\\\\quad \\\\Delta(t)=t^{2}-3 t-18$\\\\\\\\\\n(b) $\\\\operatorname{tr}(B)=7-2=5, \\\\quad|B|=-14+15=1$,\\\\\\\\\\n$\\\\Delta(t)=t^{2}-5 t+1$\\\\\\\\\\n(c) $\\\\operatorname{tr}(C)=3-3=0$,\\\\\\\\\\n$|C|=-9+18=9, \\\\quad$ so $\\\\quad \\\\Delta(t)=t^{2}+9$\\n',\n",
       " '\\n9.3. Find the characteristic polynomial $\\\\Delta(t)$ of each of the following matrices:\\n\\n(a) $A=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 3 & 0 & 4 \\\\\\\\ 6 & 4 & 5\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{rrr}1 & 6 & -2 \\\\\\\\ -3 & 2 & 0 \\\\\\\\ 0 & 3 & -4\\\\end{array}\\\\right]$\\n\\nUse the formula $\\\\Delta(t)=t^{3}-\\\\operatorname{tr}(A) t^{2}+\\\\left(A_{11}+A_{22}+A_{33}\\\\right) t-|A|$, where $A_{i i}$ is the cofactor of $a_{i i}$ in the $3 \\\\times 3$ matrix $A=\\\\left[a_{i j}\\\\right]$.\\n\\n(a) $\\\\operatorname{tr}(A)=1+0+5=6$,\\n\\n$$\\n\\\\begin{gathered}\\nA_{11}=\\\\left|\\\\begin{array}{ll}\\n0 & 4 \\\\\\\\\\n4 & 5\\n\\\\end{array}\\\\right|=-16, \\\\quad A_{22}=\\\\left|\\\\begin{array}{ll}\\n1 & 3 \\\\\\\\\\n6 & 5\\n\\\\end{array}\\\\right|=-13, \\\\quad A_{33}=\\\\left|\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n3 & 0\\n\\\\end{array}\\\\right|=-6 \\\\\\\\\\nA_{11}+A_{22}+A_{33}=-35, \\\\quad \\\\text { and } \\\\quad|A|=48+36-16-30=38\\n\\\\end{gathered}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\Delta(t)=t^{3}-6 t^{2}-35 t-38\\n$$\\n\\n(b) $\\\\operatorname{tr}(B)=1+2-4=-1$\\n\\n$$\\n\\\\begin{gathered}\\nB_{11}=\\\\left|\\\\begin{array}{rr}\\n2 & 0 \\\\\\\\\\n3 & -4\\n\\\\end{array}\\\\right|=-8, \\\\quad B_{22}=\\\\left|\\\\begin{array}{rr}\\n1 & -2 \\\\\\\\\\n0 & -4\\n\\\\end{array}\\\\right|=-4, \\\\quad B_{33}=\\\\left|\\\\begin{array}{rr}\\n1 & 6 \\\\\\\\\\n-3 & 2\\n\\\\end{array}\\\\right|=20 \\\\\\\\\\nB_{11}+B_{22}+B_{33}=8, \\\\quad \\\\text { and } \\\\quad|B|=-8+18-72=-62\\n\\\\end{gathered}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\Delta(t)=t^{3}+t^{2}-8 t+62\\n$$\\n',\n",
       " '\\n9.4. Find the characteristic polynomial $\\\\Delta(t)$ of each of the following matrices:\\n\\n(a) $A=\\\\left[\\\\begin{array}{rrrr}2 & 5 & 1 & 1 \\\\\\\\ 1 & 4 & 2 & 2 \\\\\\\\ 0 & 0 & 6 & -5 \\\\\\\\ 0 & 0 & 2 & 3\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{llll}1 & 1 & 2 & 2 \\\\\\\\ 0 & 3 & 3 & 4 \\\\\\\\ 0 & 0 & 5 & 5 \\\\\\\\ 0 & 0 & 0 & 6\\\\end{array}\\\\right]$\\n\\n(a) $A$ is block triangular with diagonal blocks\\n\\n$$\\nA_{1}=\\\\left[\\\\begin{array}{ll}\\n2 & 5 \\\\\\\\\\n1 & 4\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad A_{2}=\\\\left[\\\\begin{array}{rr}\\n6 & -5 \\\\\\\\\\n2 & 3\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus,\\n\\n$$\\n\\\\Delta(t)=\\\\Delta_{A_{1}}(t) \\\\Delta_{A_{2}}(t)=\\\\left(t^{2}-6 t+3\\\\right)\\\\left(t^{2}-9 t+28\\\\right)\\n$$\\n\\n(b) Because $B$ is triangular, $\\\\Delta(t)=(t-1)(t-3)(t-5)(t-6)$.\\n',\n",
       " '\\n9.5. Find the characteristic polynomial $\\\\Delta(t)$ of each of the following linear operators:\\n\\n(a) $F: \\\\mathbf{R}^{2} \\\\rightarrow \\\\mathbf{R}^{2}$ defined by $F(x, y)=(3 x+5 y, \\\\quad 2 x-7 y)$.\\n\\n(b) $\\\\mathbf{D}: V \\\\rightarrow V$ defined by $\\\\mathbf{D}(f)=d f / d t$, where $V$ is the space of functions with basis $S=\\\\{\\\\sin t, \\\\cos t\\\\}$.\\n\\nThe characteristic polynomial $\\\\Delta(t)$ of a linear operator is equal to the characteristic polynomial of any matrix $A$ that represents the linear operator.\\n\\n(a) Find the matrix $A$ that represents $T$ relative to the usual basis of $\\\\mathbf{R}^{2}$. We have\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n3 & 5 \\\\\\\\\\n2 & -7\\n\\\\end{array}\\\\right], \\\\quad \\\\text { so } \\\\quad \\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}+4 t-31\\n$$\\n\\n(b) Find the matrix $A$ representing the differential operator $\\\\mathbf{D}$ relative to the basis $S$. We have\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{D}(\\\\sin t) & =\\\\cos t \\\\quad=0(\\\\sin t)+1(\\\\cos t) \\\\\\\\\\n\\\\mathbf{D}(\\\\cos t) & =-\\\\sin t=-1(\\\\sin t)+0(\\\\cos t)\\n\\\\end{aligned} \\\\quad \\\\text { and so } \\\\quad A=\\\\left[\\\\begin{array}{rr}\\n0 & -1 \\\\\\\\\\n1 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nTherefore,\\n\\n$$\\n\\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}+1\\n$$\\n',\n",
       " '\\n9.6. Show that a matrix $A$ and its transpose $A^{T}$ have the same characteristic polynomial.\\n\\nBy the transpose operation, $(t I-A)^{T}=t I^{T}-A^{T}=t I-A^{T}$. Because a matrix and its transpose have the same determinant,\\n\\n$$\\n\\\\Delta_{A}(t)=|t I-A|=\\\\left|(t I-A)^{T}\\\\right|=\\\\left|t I-A^{T}\\\\right|=\\\\Delta_{A^{T}}(t)\\n$$\\n',\n",
       " '\\n9.7. Prove Theorem 9.1: Let $f$ and $g$ be polynomials. For any square matrix $A$ and scalar $k$,\\n\\n(i) $(f+g)(A)=f(A)+g(A)$,\\n\\n(ii) $(f g)(A)=f(A) g(A)$,\\n\\n(iii) $(k f)(A)=k f(A)$,\\n\\n(iv) $f(A) g(A)=g(A) f(A)$.\\n\\nSuppose $f=a_{n} t^{n}+\\\\cdots+a_{1} t+a_{0}$ and $g=b_{m} t^{m}+\\\\cdots+b_{1} t+b_{0}$. Then, by definition,\\n\\n$$\\nf(A)=a_{n} A^{n}+\\\\cdots+a_{1} A+a_{0} I \\\\quad \\\\text { and } \\\\quad g(A)=b_{m} A^{m}+\\\\cdots+b_{1} A+b_{0} I\\n$$\\n\\n(i) Suppose $m \\\\leq n$ and let $b_{i}=0$ if $i>m$. Then\\n\\n$$\\nf+g=\\\\left(a_{n}+b_{n}\\\\right) t^{n}+\\\\cdots+\\\\left(a_{1}+b_{1}\\\\right) t+\\\\left(a_{0}+b_{0}\\\\right)\\n$$\\n\\nHence,\\n\\n$$\\n\\\\begin{aligned}\\n(f+g)(A) & =\\\\left(a_{n}+b_{n}\\\\right) A^{n}+\\\\cdots+\\\\left(a_{1}+b_{1}\\\\right) A+\\\\left(a_{0}+b_{0}\\\\right) I \\\\\\\\\\n& =a_{n} A^{n}+b_{n} A^{n}+\\\\cdots+a_{1} A+b_{1} A+a_{0} I+b_{0} I=f(A)+g(A)\\n\\\\end{aligned}\\n$$\\n\\n(ii) By definition, $f g=c_{n+m} t^{n+m}+\\\\cdots+c_{1} t+c_{0}=\\\\sum_{k=0}^{n+m} c_{k} t^{k}$, where\\n\\n$$\\nc_{k}=a_{0} b_{k}+a_{1} b_{k-1}+\\\\cdots+a_{k} b_{0}=\\\\sum_{i=0}^{k} a_{i} b_{k-i}\\n$$\\n\\nHence, $(f g)(A)=\\\\sum_{k=0}^{n+m} c_{k} A^{k}$ and\\n\\n$$\\nf(A) g(A)=\\\\left(\\\\sum_{i=0}^{n} a_{i} A^{i}\\\\right)\\\\left(\\\\sum_{j=0}^{m} b_{j} A^{j}\\\\right)=\\\\sum_{i=0}^{n} \\\\sum_{j=0}^{m} a_{i} b_{j} A^{i+j}=\\\\sum_{k=0}^{n+m} c_{k} A^{k}=(f g)(A)\\n$$\\n\\n(iii) By definition, $k f=k a_{n} t^{n}+\\\\cdots+k a_{1} t+k a_{0}$, and so\\n\\n$$\\n(k f)(A)=k a_{n} A^{n}+\\\\cdots+k a_{1} A+k a_{0} I=k\\\\left(a_{n} A^{n}+\\\\cdots+a_{1} A+a_{0} I\\\\right)=k f(A)\\n$$\\n\\n(iv) By (ii), $g(A) f(A)=(g f)(A)=(f g)(A)=f(A) g(A)$.\\n',\n",
       " '\\n9.8. Prove the Cayley-Hamilton Theorem 9.2: Every matrix $A$ is a root of its characterstic polynomial $\\\\Delta(t)$.\\n\\nLet $A$ be an arbitrary $n$-square matrix and let $\\\\Delta(t)$ be its characteristic polynomial, say,\\n\\n$$\\n\\\\Delta(t)=|t I-A|=t^{n}+a_{n-1} t^{n-1}+\\\\cdots+a_{1} t+a_{0}\\n$$\\n\\nNow let $B(t)$ denote the classical adjoint of the matrix $t I-A$. The elements of $B(t)$ are cofactors of the matrix $t I-A$ and hence are polynomials in $t$ of degree not exceeding $n-1$. Thus,\\n\\n$$\\nB(t)=B_{n-1} t^{n-1}+\\\\cdots+B_{1} t+B_{0}\\n$$\\n\\nwhere the $B_{i}$ are $n$-square matrices over $K$ which are independent of $t$. By the fundamental property of the classical adjoint (Theorem 8.9), $(t I-A) B(t)=|t I-A| I$, or\\n\\n$$\\n(t I-A)\\\\left(B_{n-1} t^{n-1}+\\\\cdots+B_{1} t+B_{0}\\\\right)=\\\\left(t^{n}+a_{n-1} t^{n-1}+\\\\cdots+a_{1} t+a_{0}\\\\right) I\\n$$\\n\\nRemoving the parentheses and equating corresponding powers of $t$ yields\\n\\n$$\\nB_{n-1}=I, \\\\quad B_{n-2}-A B_{n-1}=a_{n-1} I, \\\\quad \\\\ldots, \\\\quad B_{0}-A B_{1}=a_{1} I, \\\\quad-A B_{0}=a_{0} I\\n$$\\n\\nMultiplying the above equations by $A^{n}, A^{n-1}, \\\\ldots, A, I$, respectively, yields\\n\\n$$\\nA^{n} B_{n-1}=A_{n} I, \\\\quad A^{n-1} B_{n-2}-A^{n} B_{n-1}=a_{n-1} A^{n-1}, \\\\quad \\\\ldots, \\\\quad A B_{0}-A^{2} B_{1}=a_{1} A, \\\\quad-A B_{0}=a_{0} I\\n$$\\n\\nAdding the above matrix equations yields 0 on the left-hand side and $\\\\Delta(A)$ on the right-hand side; that is,\\n\\n$$\\n0=A^{n}+a_{n-1} A^{n-1}+\\\\cdots+a_{1} A+a_{0} I\\n$$\\n\\nTherefore, $\\\\Delta(A)=0$, which is the Cayley-Hamilton theorem.\\n\\n\\n\\\\section*{Eigenvalues and Eigenvectors of $\\\\mathbf{2} \\\\times \\\\mathbf{2}$ Matrices}\\n',\n",
       " '9.9. Let $A=\\\\left[\\\\begin{array}{ll}3 & -4 \\\\\\\\ 2 & -6\\\\end{array}\\\\right]$.\\n\\n(a) Find all eigenvalues and corresponding eigenvectors.\\n\\n(b) Find matrices $P$ and $D$ such that $P$ is nonsingular and $D=P^{-1} A P$ is diagonal.\\n\\n(a) First find the characteristic polynomial $\\\\Delta(t)$ of $A$ :\\n\\n$$\\n\\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}+3 t-10=(t-2)(t+5)\\n$$\\n\\nThe roots $\\\\lambda=2$ and $\\\\lambda=-5$ of $\\\\Delta(t)$ are the eigenvalues of $A$. We find corresponding eigenvectors.\\n\\n(i) Subtract $\\\\lambda=2$ down the diagonal of $A$ to obtain the matrix $M=A-2 I$, where the corresponding homogeneous system $M X=0$ yields the eigenvectors corresponding to $\\\\lambda=2$. We have\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n1 & -4 \\\\\\\\\\n2 & -8\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\nx-4 y & =0 \\\\\\\\\\n2 x-8 y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x-4 y=0\\n$$\\n\\nThe system has only one free variable, and $v_{1}=(4,1)$ is a nonzero solution. Thus, $v_{1}=(4,1)$ is an eigenvector belonging to (and spanning the eigenspace of) $\\\\lambda=2$.\\n\\n(ii) Subtract $\\\\lambda=-5$ (or, equivalently, add 5) down the diagonal of $A$ to obtain\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n8 & -4 \\\\\\\\\\n2 & -1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n& 8 x-4 y=0 \\\\\\\\\\n& 2 x-y=0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad 2 x-y=0\\n$$\\n\\nThe system has only one free variable, and $v_{2}=(1,2)$ is a nonzero solution. Thus, $v_{2}=(1,2)$ is an eigenvector belonging to $\\\\lambda=5$.\\n\\n(b) Let $P$ be the matrix whose columns are $v_{1}$ and $v_{2}$. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{ll}\\n4 & 1 \\\\\\\\\\n1 & 2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} A P=\\\\left[\\\\begin{array}{rr}\\n2 & 0 \\\\\\\\\\n0 & -5\\n\\\\end{array}\\\\right]\\n$$\\n\\nNote that $D$ is the diagonal matrix whose diagonal entries are the eigenvalues of $A$ corresponding to the eigenvectors appearing in $P$.\\n\\nRemark: Here $P$ is the change-of-basis matrix from the usual basis of $\\\\mathbf{R}^{2}$ to the basis $S=\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}$, and $D$ is the matrix that represents (the matrix function) $A$ relative to the new basis $S$.\\n',\n",
       " '\\n9.10. Let $A=\\\\left[\\\\begin{array}{ll}2 & 2 \\\\\\\\ 1 & 3\\\\end{array}\\\\right]$.\\n\\n(a) Find all eigenvalues and corresponding eigenvectors.\\n\\n(b) Find a nonsingular matrix $P$ such that $D=P^{-1} A P$ is diagonal, and $P^{-1}$.\\n\\n(c) Find $A^{6}$ and $f(A)$, where $t^{4}-3 t^{3}-6 t^{2}+7 t+3$.\\n\\n(d) Find a \"real cube root\" of $B$ - that is, a matrix $B$ such that $B^{3}=A$ and $B$ has real eigenvalues.\\n\\n(a) First find the characteristic polynomial $\\\\Delta(t)$ of $A$ :\\n\\n$$\\n\\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}-5 t+4=(t-1)(t-4)\\n$$\\n\\nThe roots $\\\\lambda=1$ and $\\\\lambda=4$ of $\\\\Delta(t)$ are the eigenvalues of $A$. We find corresponding eigenvectors.\\n\\n(i) Subtract $\\\\lambda=1$ down the diagonal of $A$ to obtain the matrix $M=A-\\\\lambda I$, where the corresponding homogeneous system $M X=0$ yields the eigenvectors belonging to $\\\\lambda=1$. We have\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n1 & 2 \\\\\\\\\\n1 & 2\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n& x+2 y=0 \\\\\\\\\\n& x+2 y=0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x+2 y=0\\n$$\\n\\nThe system has only one independent solution; for example, $x=2, y=-1$. Thus, $v_{1}=(2,-1)$ is an eigenvector belonging to (and spanning the eigenspace of) $\\\\lambda=1$.\\n\\n(ii) Subtract $\\\\lambda=4$ down the diagonal of $A$ to obtain\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n-2 & 2 \\\\\\\\\\n1 & -1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\begin{aligned}\\n-2 x+2 y & =0 \\\\\\\\\\nx-y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x-y=0\\n$$\\n\\nThe system has only one independent solution; for example, $x=1, y=1$. Thus, $v_{2}=(1,1)$ is an eigenvector belonging to $\\\\lambda=4$.\\n\\n(b) Let $P$ be the matrix whose columns are $v_{1}$ and $v_{2}$. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rr}\\n2 & 1 \\\\\\\\\\n-1 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} A P=\\\\left[\\\\begin{array}{ll}\\n1 & 0 \\\\\\\\\\n0 & 4\\n\\\\end{array}\\\\right], \\\\quad \\\\text { where } \\\\quad P^{-1}=\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{3} & -\\\\frac{1}{3} \\\\\\\\\\n\\\\frac{1}{3} & \\\\frac{2}{3}\\n\\\\end{array}\\\\right]\\n$$\\n\\n(c) Using the diagonal factorization $A=P D P^{-1}$, and $1^{6}=1$ and $4^{6}=4096$, we get\\n\\n$$\\nA^{6}=P D^{6} P^{-1}=\\\\left[\\\\begin{array}{rr}\\n2 & 1 \\\\\\\\\\n-1 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n1 & 0 \\\\\\\\\\n0 & 4096\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{3} & -\\\\frac{1}{3} \\\\\\\\\\n\\\\frac{1}{3} & \\\\frac{2}{3}\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n1366 & 2230 \\\\\\\\\\n1365 & 2731\\n\\\\end{array}\\\\right]\\n$$\\n\\nAlso, $f(1)=2$ and $f(4)=-1$. Hence,\\n\\n$$\\nf(A)=\\\\operatorname{Pf}(D) P^{-1}=\\\\left[\\\\begin{array}{rr}\\n2 & 1 \\\\\\\\\\n-1 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n2 & 0 \\\\\\\\\\n0 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{3} & -\\\\frac{1}{3} \\\\\\\\\\n\\\\frac{1}{3} & \\\\frac{2}{3}\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n-1 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\n(d) Here $\\\\left[\\\\begin{array}{cc}1 & 0 \\\\\\\\ 0 & \\\\sqrt[3]{4}\\\\end{array}\\\\right]$ is the real cube root of $D$. Hence the real cube root of $A$ is\\n\\n$$\\nB=P \\\\sqrt[3]{D} P^{-1}=\\\\left[\\\\begin{array}{rr}\\n2 & 1 \\\\\\\\\\n-1 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc}\\n1 & 0 \\\\\\\\\\n0 & \\\\sqrt[3]{4}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n\\\\frac{1}{3} & -\\\\frac{1}{3} \\\\\\\\\\n\\\\frac{1}{3} & \\\\frac{2}{3}\\n\\\\end{array}\\\\right]=\\\\frac{1}{3}\\\\left[\\\\begin{array}{rr}\\n2+\\\\sqrt[3]{4} & -2+2 \\\\sqrt[3]{4} \\\\\\\\\\n-1+\\\\sqrt[3]{4} & 1+2 \\\\sqrt[3]{4}\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n9.11. Each of the following real matrices defines a linear transformation on $\\\\mathbf{R}^{2}$ :\\\\\\\\\\n(a) $A=\\\\left[\\\\begin{array}{rr}5 & 6 \\\\\\\\ 3 & -2\\\\end{array}\\\\right]$,\\\\\\\\\\n(b) $B=\\\\left[\\\\begin{array}{ll}1 & -1 \\\\\\\\ 2 & -1\\\\end{array}\\\\right]$\\\\\\\\\\n(c) $C=\\\\left[\\\\begin{array}{rr}5 & -1 \\\\\\\\ 1 & 3\\\\end{array}\\\\right]$\\n\\nFind, for each matrix, all eigenvalues and a maximum set $S$ of linearly independent eigenvectors. Which of these linear operators are diagonalizable - that is, which can be represented by a diagonal matrix?\\n\\n(a) First find $\\\\Delta(t)=t^{2}-3 t-28=(t-7)(t+4)$. The roots $\\\\lambda=7$ and $\\\\lambda=-4$ are the eigenvalues of $A$. We find corresponding eigenvectors.\\n\\n(i) Subtract $\\\\lambda=7$ down the diagonal of $A$ to obtain\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n-2 & 6 \\\\\\\\\\n3 & -9\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n-2 x+6 y & =0 \\\\\\\\\\n3 x-9 y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x-3 y=0\\n$$\\n\\nHere $v_{1}=(3,1)$ is a nonzero solution.\\n\\n(ii) Subtract $\\\\lambda=-4$ (or add 4) down the diagonal of $A$ to obtain\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n9 & 6 \\\\\\\\\\n3 & 2\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n& 9 x+6 y=0 \\\\\\\\\\n& 3 x+2 y=0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad 3 x+2 y=0\\n$$\\n\\nHere $v_{2}=(2,-3)$ is a nonzero solution.\\n\\nThen $S=\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}=\\\\{(3,1),(2,-3)\\\\}$ is a maximal set of linearly independent eigenvectors. Because $S$ is a basis of $\\\\mathbf{R}^{2}, A$ is diagonalizable. Using the basis $S, A$ is represented by the diagonal matrix $D=\\\\operatorname{diag}(7,-4)$.\\n\\n(b) First find the characteristic polynomial $\\\\Delta(t)=t^{2}+1$. There are no real roots. Thus $B$, a real matrix representing a linear transformation on $\\\\mathbf{R}^{2}$, has no eigenvalues and no eigenvectors. Hence, in particular, $B$ is not diagonalizable.\\\\\\\\\\n(c) First find $\\\\Delta(t)=t^{2}-8 t+16=(t-4)^{2}$. Thus, $\\\\lambda=4$ is the only eigenvalue of $C$. Subtract $\\\\lambda=4$ down the diagonal of $C$ to obtain\\n\\n$$\\nM=\\\\left[\\\\begin{array}{cc}\\n1 & -1 \\\\\\\\\\n1 & -1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad x-y=0\\n$$\\n\\nThe homogeneous system has only one independent solution; for example, $x=1, y=1$. Thus, $v=(1,1)$ is an eigenvector of $C$. Furthermore, as there are no other eigenvalues, the singleton set $S=\\\\{v\\\\}=\\\\{(1,1)\\\\}$ is a maximal set of linearly independent eigenvectors of $C$. Furthermore, because $S$ is not a basis of $\\\\mathbf{R}^{2}, C$ is not diagonalizable.\\n',\n",
       " '\\n9.12. Suppose the matrix $B$ in Problem 9.11 represents a linear operator on complex space $\\\\mathbf{C}^{2}$. Show that, in this case, $B$ is diagonalizable by finding a basis $S$ of $\\\\mathbf{C}^{2}$ consisting of eigenvectors of $B$.\\n\\nThe characteristic polynomial of $B$ is still $\\\\Delta(t)=t^{2}+1$. As a polynomial over $\\\\mathbf{C}, \\\\Delta(t)$ does factor; specifically, $\\\\Delta(t)=(t-i)(t+i)$. Thus, $\\\\lambda=i$ and $\\\\lambda=-i$ are the eigenvalues of $B$.\\n\\n(i) Subtract $\\\\lambda=i$ down the diagonal of $B$ to obtain the homogeneous system\\n\\n$$\\n\\\\begin{aligned}\\n(1-i) x-y & =0 \\\\\\\\\\n2 x+(-1-i) y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad(1-i) x-y=0\\n$$\\n\\nThe system has only one independent solution; for example, $x=1, y=1-i$. Thus, $v_{1}=(1,1-i)$ is an eigenvector that spans the eigenspace of $\\\\lambda=i$.\\n\\n(ii) Subtract $\\\\lambda=-i$ (or add $i$ ) down the diagonal of $B$ to obtain the homogeneous system\\n\\n$$\\n\\\\begin{aligned}\\n(1+i) x-y & =0 \\\\\\\\\\n2 x+(-1+i) y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad(1+i) x-y=0\\n$$\\n\\nThe system has only one independent solution; for example, $x=1, y=1+i$. Thus, $v_{2}=(1,1+i)$ is an eigenvector that spans the eigenspace of $\\\\lambda=-i$.\\n\\nAs a complex matrix, $B$ is diagonalizable. Specifically, $S=\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}=\\\\{(1,1-i),(1,1+i)\\\\}$ is a basis of $C^{2}$ consisting of eigenvectors of $B$. Using this basis $S, B$ is represented by the diagonal matrix $D=\\\\operatorname{diag}(i,-i)$.\\n',\n",
       " '\\n9.13. Let $L$ be the linear transformation on $\\\\mathbf{R}^{2}$ that reflects each point $P$ across the line $y=k x$, where $k>0$. (See Fig. 9-1.)\\n\\n(a) Show that $v_{1}=(k, 1)$ and $v_{2}=(1,-k)$ are eigenvectors of $L$.\\n\\n(b) Show that $L$ is diagonalizable, and find a diagonal representation $D$.\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-318}\\n\\\\end{center}\\n\\nFigure 9-1\\n\\n(a) The vector $v_{1}=(k, 1)$ lies on the line $y=k x$, and hence is left fixed by $L$; that is, $L\\\\left(v_{1}\\\\right)=v_{1}$. Thus, $v_{1}$ is an eigenvector of $L$ belonging to the eigenvalue $\\\\lambda_{1}=1$.\\n\\nThe vector $v_{2}=(1,-k)$ is perpendicular to the line $y=k x$, and hence, $L$ reflects $v_{2}$ into its negative; that is, $L\\\\left(v_{2}\\\\right)=-v_{2}$. Thus, $v_{2}$ is an eigenvector of $L$ belonging to the eigenvalue $\\\\lambda_{2}=-1$.\\\\\\\\\\n(b) Here $S=\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}$ is a basis of $\\\\mathbf{R}^{2}$ consisting of eigenvectors of $L$. Thus, $L$ is diagonalizable, with the diagonal representation $D=\\\\left[\\\\begin{array}{rr}1 & 0 \\\\\\\\ 0 & -1\\\\end{array}\\\\right]$ (relative to the basis $S$ ).\\n\\n\\n\\\\section*{Eigenvalues and Eigenvectors}\\n',\n",
       " '9.14. Let $A=\\\\left[\\\\begin{array}{rrr}4 & 1 & -1 \\\\\\\\ 2 & 5 & -2 \\\\\\\\ 1 & 1 & 2\\\\end{array}\\\\right]$. (a) Find all eigenvalues of $A$.\\n\\n(b) Find a maximum set $S$ of linearly independent eigenvectors of $A$.\\n\\n(c) Is $A$ diagonalizable? If yes, find $P$ such that $D=P^{-1} A P$ is diagonal.\\n\\n(a) First find the characteristic polynomial $\\\\Delta(t)$ of $A$. We have\\n\\n$$\\n\\\\operatorname{tr}(A)=4+5+2=11 \\\\quad \\\\text { and } \\\\quad|A|=40-2-2+5+8-4=45\\n$$\\n\\nAlso, find each cofactor $A_{i i}$ of $a_{i i}$ in $A$ :\\n\\n$$\\nA_{11}=\\\\left|\\\\begin{array}{rr}\\n5 & -2 \\\\\\\\\\n1 & 2\\n\\\\end{array}\\\\right|=12, \\\\quad A_{22}=\\\\left|\\\\begin{array}{rr}\\n4 & -1 \\\\\\\\\\n1 & 2\\n\\\\end{array}\\\\right|=9, \\\\quad A_{33}=\\\\left|\\\\begin{array}{ll}\\n4 & 1 \\\\\\\\\\n2 & 5\\n\\\\end{array}\\\\right|=18\\n$$\\n\\nHence,\\n\\n$$\\n\\\\Delta(t)=t^{3}-\\\\operatorname{tr}(A) t^{2}+\\\\left(A_{11}+A_{22}+A_{33}\\\\right) t-|A|=t^{3}-11 t^{2}+39 t-45\\n$$\\n\\nAssuming $\\\\Delta t$ has a rational root, it must be among $\\\\pm 1, \\\\pm 3, \\\\pm 5, \\\\pm 9, \\\\pm 15, \\\\pm 45$. Testing, by synthetic division, we get\\n\\n$$\\n3 \\\\begin{array}{r}\\n1-11+39-45 \\\\\\\\\\n3-24+45 \\\\\\\\\\n1-8+15+0\\n\\\\end{array}\\n$$\\n\\nThus, $t=3$ is a root of $\\\\Delta(t)$. Also, $t-3$ is a factor and $t^{2}-8 t+15$ is a factor. Hence,\\n\\n$$\\n\\\\Delta(t)=(t-3)\\\\left(t^{2}-8 t+15\\\\right)=(t-3)(t-5)(t-3)=(t-3)^{2}(t-5)\\n$$\\n\\nAccordingly, $\\\\lambda=3$ and $\\\\lambda=5$ are eigenvalues of $A$.\\n\\n(b) Find linearly independent eigenvectors for each eigenvalue of $A$.\\n\\n(i) Subtract $\\\\lambda=3$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ccc}\\n1 & 1 & -1 \\\\\\\\\\n2 & 2 & -2 \\\\\\\\\\n1 & 1 & -1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad x+y-z=0\\n$$\\n\\nHere $u=(1,-1,0)$ and $v=(1,0,1)$ are linearly independent solutions.\\n\\n(ii) Subtract $\\\\lambda=5$ down the diagonal of $A$ to obtain the matrix\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-319}\\n\\\\end{center}\\n\\nOnly $z$ is a free variable. Here $w=(1,2,1)$ is a solution.\\n\\nThus, $S=\\\\{u, v, w\\\\}=\\\\{(1,-1,0),(1,0,1),(1,2,1)\\\\}$ is a maximal set of linearly independent eigenvectors of $A$.\\n\\nRemark: The vectors $u$ and $v$ were chosen so that they were independent solutions of the system $x+y-z=0$. On the other hand, $w$ is automatically independent of $u$ and $v$ because $w$ belongs to a different eigenvalue of $A$. Thus, the three vectors are linearly independent.\\\\\\\\\\n(c) $A$ is diagonalizable, because it has three linearly independent eigenvectors. Let $P$ be the matrix with columns $u, v, w$. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rrr}\\n1 & 1 & 1 \\\\\\\\\\n-1 & 0 & 2 \\\\\\\\\\n0 & 1 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} A P=\\\\left[\\\\begin{array}{lll}\\n3 & & \\\\\\\\\\n& 3 & \\\\\\\\\\n& & 5\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n9.15. Repeat Problem 9.14 for the matrix $B=\\\\left[\\\\begin{array}{lll}3 & -1 & 1 \\\\\\\\ 7 & -5 & 1 \\\\\\\\ 6 & -6 & 2\\\\end{array}\\\\right]$.\\n\\n(a) First find the characteristic polynomial $\\\\Delta(t)$ of $B$. We have\\n\\n$\\\\operatorname{tr}(B)=0, \\\\quad|B|=-16, \\\\quad B_{11}=-4, \\\\quad B_{22}=0, \\\\quad B_{33}=-8, \\\\quad$ so $\\\\quad \\\\sum_{i} B_{i i}=-12$\\n\\nTherefore, $\\\\Delta(t)=t^{3}-12 t+16=(t-2)^{2}(t+4)$. Thus, $\\\\lambda_{1}=2$ and $\\\\lambda_{2}=-4$ are the eigenvalues of $B$.\\n\\n(b) Find a basis for the eigenspace of each eigenvalue of $B$.\\n\\n(i) Subtract $\\\\lambda_{1}=2$ down the diagonal of $B$ to obtain\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-320}\\n\\\\end{center}\\n\\nThe system has only one independent solution; for example, $x=1, y=1, z=0$. Thus, $u=(1,1,0)$ forms a basis for the eigenspace of $\\\\lambda_{1}=2$.\\n\\n(ii) Subtract $\\\\lambda_{2}=-4$ (or add 4) down the diagonal of $B$ to obtain\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-320(1)}\\n\\\\end{center}\\n\\nThe system has only one independent solution; for example, $x=0, y=1, z=1$. Thus, $v=(0,1,1)$ forms a basis for the eigenspace of $\\\\lambda_{2}=-4$.\\n\\nThus $S=\\\\{u, v\\\\}$ is a maximal set of linearly independent eigenvectors of $B$.\\n\\n(c) Because $B$ has at most two linearly independent eigenvectors, $B$ is not similar to a diagonal matrix; that is, $B$ is not diagonalizable.\\n',\n",
       " '\\n9.16. Find the algebraic and geometric multiplicities of the eigenvalue $\\\\lambda_{1}=2$ of the matrix $B$ in Problem 9.15.\\n\\nThe algebraic multiplicity of $\\\\lambda_{1}=2$ is 2 , because $t-2$ appears with exponent 2 in $\\\\Delta(t)$. However, the geometric multiplicity of $\\\\lambda_{1}=2$ is 1 , because $\\\\operatorname{dim} E_{\\\\lambda_{1}}=1$ (where $E_{\\\\lambda_{1}}$ is the eigenspace of $\\\\lambda_{1}$ ).\\n',\n",
       " '\\n9.17. Let $T: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{3}$ be defined by $T(x, y, z)=(2 x+y-2 z, \\\\quad 2 x+3 y-4 z, \\\\quad x+y-z)$. Find all eigenvalues of $T$, and find a basis of each eigenspace. Is $T$ diagonalizable? If so, find the basis $S$ of $\\\\mathbf{R}^{3}$ that diagonalizes $T$, and find its diagonal representation $D$.\\n\\nFirst find the matrix $A$ that represents $T$ relative to the usual basis of $\\\\mathbf{R}^{3}$ by writing down the coefficients of $x, y, z$ as rows, and then find the characteristic polynomial of $A$ (and $T$ ). We have\\n\\n$$\\nA=[T]=\\\\left[\\\\begin{array}{ccc}\\n2 & 1 & -2 \\\\\\\\\\n2 & 3 & -4 \\\\\\\\\\n1 & 1 & -1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad \\\\begin{gathered}\\n\\\\operatorname{tr}(A)=4, \\\\quad|A|=2 \\\\\\\\\\nA_{11}=1, \\\\quad A_{22}=0, \\\\quad A_{33}=4 \\\\\\\\\\n\\\\sum_{i} A_{i i}=5\\n\\\\end{gathered}\\n$$\\n\\nTherefore, $\\\\Delta(t)=t^{3}-4 t^{2}+5 t-2=(t-1)^{2}(t-2)$, and so $\\\\lambda=1$ and $\\\\lambda=2$ are the eigenvalues of $A$ (and $T$ ). We next find linearly independent eigenvectors for each eigenvalue of $A$.\\\\\\\\\\n(i) Subtract $\\\\lambda=1$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ccc}\\n1 & 1 & -2 \\\\\\\\\\n2 & 2 & -4 \\\\\\\\\\n1 & 1 & -2\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad x+y-2 z=0\\n$$\\n\\nHere $y$ and $z$ are free variables, and so there are two linearly independent eigenvectors belonging to $\\\\lambda=1$. For example, $u=(1,-1,0)$ and $v=(2,0,1)$ are two such eigenvectors.\\n\\n(ii) Subtract $\\\\lambda=2$ down the diagonal of $A$ to obtain\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-321}\\n\\\\end{center}\\n\\nOnly $z$ is a free variable. Here $w=(1,2,1)$ is a solution.\\n\\nThus, $T$ is diagonalizable, because it has three independent eigenvectors. Specifically, choosing\\n\\n$$\\nS=\\\\{u, v, w\\\\}=\\\\{(1,-1,0), \\\\quad(2,0,1), \\\\quad(1,2,1)\\\\}\\n$$\\n\\nas a basis, $T$ is represented by the diagonal matrix $D=\\\\operatorname{diag}(1,1,2)$.\\n',\n",
       " '\\n9.18. Prove the following for a linear operator (matrix) $T$ :\\n\\n(a) The scalar 0 is an eigenvalue of $T$ if and only if $T$ is singular.\\n\\n(b) If $\\\\lambda$ is an eigenvalue of $T$, where $T$ is invertible, then $\\\\lambda^{-1}$ is an eigenvalue of $T^{-1}$.\\n\\n(a) We have that 0 is an eigenvalue of $T$ if and only if there is a vector $v \\\\neq 0$ such that $T(v)=0 v$ - that is, if and only if $T$ is singular.\\n\\n(b) Because $T$ is invertible, it is nonsingular; hence, by (a), $\\\\lambda \\\\neq 0$. By definition of an eigenvalue, there exists $v \\\\neq 0$ such that $T(v)=\\\\lambda v$. Applying $T^{-1}$ to both sides, we obtain\\n\\n$$\\nv=T^{-1}(\\\\lambda v)=\\\\lambda T^{-1}(v), \\\\quad \\\\text { and so } \\\\quad T^{-1}(v)=\\\\lambda^{-1} v\\n$$\\n\\nTherefore, $\\\\lambda^{-1}$ is an eigenvalue of $T^{-1}$.\\n',\n",
       " '\\n9.19. Let $\\\\lambda$ be an eigenvalue of a linear operator $T: V \\\\rightarrow V$, and let $E_{\\\\lambda}$ consists of all the eigenvectors belonging to $\\\\lambda$ (called the eigenspace of $\\\\lambda$ ). Prove that $E_{\\\\lambda}$ is a subspace of $V$. That is, prove\\n\\n(a) If $u \\\\in E_{\\\\lambda}$, then $k u \\\\in E_{\\\\lambda}$ for any scalar $k$. (b) If $u, v, \\\\in E_{\\\\lambda}$, then $u+v \\\\in E_{\\\\lambda}$.\\n\\n(a) Because $u \\\\in E_{\\\\lambda}$, we have $T(u)=\\\\lambda u$. Then $T(k u)=k T(u)=k(\\\\lambda u)=\\\\lambda(k u)$, and so $k u \\\\in E_{\\\\lambda}$.\\n\\n(We view the zero vector $0 \\\\in V$ as an \"eigenvector\" of $\\\\lambda$ in order for $E_{\\\\lambda}$ to be a subspace of $V$.)\\n\\n(b) As $u, v \\\\in E_{\\\\lambda}$, we have $T(u)=\\\\lambda u$ and $T(v)=\\\\lambda v$. Then\\n\\n$T(u+v)=T(u)+T(v)=\\\\lambda u+\\\\lambda v=\\\\lambda(u+v)$, and so $u+v \\\\in E_{\\\\lambda}$\\n',\n",
       " '\\n9.20. Prove Theorem 9.6: The following are equivalent: (i) The scalar $\\\\lambda$ is an eigenvalue of $A$.\\n\\n(ii) The matrix $\\\\lambda I-A$ is singular.\\n\\n(iii) The scalar $\\\\lambda$ is a root of the characteristic polynomial $\\\\Delta(t)$ of $A$.\\n\\nThe scalar $\\\\lambda$ is an eigenvalue of $A$ if and only if there exists a nonzero vector $v$ such that\\n\\n$$\\nA v=\\\\lambda v \\\\quad \\\\text { or } \\\\quad(\\\\lambda I) v-A v=0 \\\\quad \\\\text { or } \\\\quad(\\\\lambda I-A) v=0\\n$$\\n\\nor $\\\\lambda I-A$ is singular. In such a case, $\\\\lambda$ is a root of $\\\\Delta(t)=|t I-A|$. Also, $v$ is in the eigenspace $E_{\\\\lambda}$ of $\\\\lambda$ if and only if the above relations hold. Hence, $v$ is a solution of $(\\\\lambda I-A) X=0$.\\n',\n",
       " \"\\n9.21. Prove Theorem 9.8': Suppose $v_{1}, v_{2}, \\\\ldots, v_{n}$ are nonzero eigenvectors of $T$ belonging to distinct eigenvalues $\\\\lambda_{1}, \\\\lambda_{2}, \\\\ldots, \\\\lambda_{n}$. Then $v_{1}, v_{2}, \\\\ldots, v_{n}$ are linearly independent.\\n\\nSuppose the theorem is not true. Let $v_{1}, v_{2}, \\\\ldots, v_{s}$ be a minimal set of vectors for which the theorem is not true. We have $s>1$, because $v_{1} \\\\neq 0$. Also, by the minimality condition, $v_{2}, \\\\ldots, v_{s}$ are linearly independent. Thus, $v_{1}$ is a linear combination of $v_{2}, \\\\ldots, v_{s}$, say,\\n\\n\\n\\\\begin{equation*}\\nv_{1}=a_{2} v_{2}+a_{3} v_{3}+\\\\cdots+a_{s} v_{s} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\n(where some $a_{k} \\\\neq 0$ ). Applying $T$ to (1) and using the linearity of $T$ yields\\n\\n\\n\\\\begin{equation*}\\nT\\\\left(v_{1}\\\\right)=T\\\\left(a_{2} v_{2}+a_{3} v_{3}+\\\\cdots+a_{s} v_{s}\\\\right)=a_{2} T\\\\left(v_{2}\\\\right)+a_{3} T\\\\left(v_{3}\\\\right)+\\\\cdots+a_{s} T\\\\left(v_{s}\\\\right) \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nBecause $v_{j}$ is an eigenvector of $T$ belonging to $\\\\lambda_{j}$, we have $T\\\\left(v_{j}\\\\right)=\\\\lambda_{j} v_{j}$. Substituting in (2) yields\\n\\n\\n\\\\begin{equation*}\\n\\\\lambda_{1} v_{1}=a_{2} \\\\lambda_{2} v_{2}+a_{3} \\\\lambda_{3} v_{3}+\\\\cdots+a_{s} \\\\lambda_{s} v_{s} \\\\tag{3}\\n\\\\end{equation*}\\n\\n\\nMultiplying (1) by $\\\\lambda_{1}$ yields\\n\\n\\n\\\\begin{equation*}\\n\\\\lambda_{1} v_{1}=a_{2} \\\\lambda_{1} v_{2}+a_{3} \\\\lambda_{1} v_{3}+\\\\cdots+a_{s} \\\\lambda_{1} v_{s} \\\\tag{4}\\n\\\\end{equation*}\\n\\n\\nSetting the right-hand sides of (3) and (4) equal to each other, or subtracting (3) from (4) yields\\n\\n\\n\\\\begin{equation*}\\na_{2}\\\\left(\\\\lambda_{1}-\\\\lambda_{2}\\\\right) v_{2}+a_{3}\\\\left(\\\\lambda_{1}-\\\\lambda_{3}\\\\right) v_{3}+\\\\cdots+a_{s}\\\\left(\\\\lambda_{1}-\\\\lambda_{s}\\\\right) v_{s}=0 \\\\tag{5}\\n\\\\end{equation*}\\n\\n\\nBecause $v_{2}, v_{3}, \\\\ldots, v_{s}$ are linearly independent, the coefficients in (5) must all be zero. That is,\\n\\n$$\\na_{2}\\\\left(\\\\lambda_{1}-\\\\lambda_{2}\\\\right)=0, \\\\quad a_{3}\\\\left(\\\\lambda_{1}-\\\\lambda_{3}\\\\right)=0, \\\\quad \\\\ldots, \\\\quad a_{s}\\\\left(\\\\lambda_{1}-\\\\lambda_{s}\\\\right)=0\\n$$\\n\\nHowever, the $\\\\lambda_{i}$ are distinct. Hence $\\\\lambda_{1}-\\\\lambda_{j} \\\\neq 0$ for $j>1$. Hence, $a_{2}=0, a_{3}=0, \\\\ldots, a_{s}=0$. This contradicts the fact that some $a_{k} \\\\neq 0$. The theorem is proved.\\n\",\n",
       " '\\n9.22. Prove Theorem 9.9. Suppose $\\\\Delta(t)=\\\\left(t-a_{1}\\\\right)\\\\left(t-a_{2}\\\\right) \\\\ldots\\\\left(t-a_{n}\\\\right)$ is the characteristic polynomial of an $n$-square matrix $A$, and suppose the $n$ roots $a_{i}$ are distinct. Then $A$ is similar to the diagonal matrix $D=\\\\operatorname{diag}\\\\left(a_{1}, a_{2}, \\\\ldots, a_{n}\\\\right)$.\\n\\nLet $v_{1}, v_{2}, \\\\ldots, v_{n}$ be (nonzero) eigenvectors corresponding to the eigenvalues $a_{i}$. Then the $n$ eigenvectors $v_{i}$ are linearly independent (Theorem 9.8), and hence form a basis of $K^{n}$. Accordingly, $A$ is diagonalizable (i.e., $A$ is similar to a diagonal matrix $D$ ), and the diagonal elements of $D$ are the eigenvalues $a_{i}$.\\n',\n",
       " \"\\n9.23. Prove Theorem 9.10': The geometric multiplicity of an eigenvalue $\\\\lambda$ of $T$ does not exceed its algebraic multiplicity.\\n\\nSuppose the geometric multiplicity of $\\\\lambda$ is $r$. Then its eigenspace $E_{\\\\lambda}$ contains $r$ linearly independent eigenvectors $v_{1}, \\\\ldots, v_{r}$. Extend the set $\\\\left\\\\{v_{i}\\\\right\\\\}$ to a basis of $V$, say, $\\\\left\\\\{v_{i}, \\\\ldots, v_{r}, w_{1}, \\\\ldots, w_{s}\\\\right\\\\}$. We have\\n\\n$$\\n\\\\begin{aligned}\\n& T\\\\left(v_{1}\\\\right)=\\\\lambda v_{1}, \\\\quad T\\\\left(v_{2}\\\\right)=\\\\lambda v_{2}, \\\\quad \\\\ldots, \\\\quad T\\\\left(v_{r}\\\\right)=\\\\lambda v_{r} \\\\\\\\\\n& T\\\\left(w_{1}\\\\right)=a_{11} v_{1}+\\\\cdots+a_{1 r} v_{r}+b_{11} w_{1}+\\\\cdots+b_{1 s} w_{s} \\\\\\\\\\n& T\\\\left(w_{2}\\\\right)=a_{21} v_{1}+\\\\cdots+a_{2 r} v_{r}+b_{21} w_{1}+\\\\cdots+b_{2 s} w_{s} \\\\\\\\\\n& T\\\\left(w_{s}\\\\right)=a_{s 1} v_{1}+\\\\cdots+a_{s r} v_{r}+b_{s 1} w_{1}+\\\\cdots+b_{s s} w_{s}\\n\\\\end{aligned}\\n$$\\n\\nThen $M=\\\\left[\\\\begin{array}{cc}\\\\lambda I_{r} & A \\\\\\\\ 0 & B\\\\end{array}\\\\right]$ is the matrix of $T$ in the above basis, where $A=\\\\left[a_{i j}\\\\right]^{T}$ and $B=\\\\left[b_{i j}\\\\right]^{T}$.\\n\\nBecause $M$ is block diagonal, the characteristic polynomial $(t-\\\\lambda)^{r}$ of the block $\\\\lambda I_{r}$ must divide the characteristic polynomial of $M$ and hence of $T$. Thus, the algebraic multiplicity of $\\\\lambda$ for $T$ is at least $r$, as required.\\n\\n\\n\\\\section*{Diagonalizing Real Symmetric Matrices and Quadratic Forms}\\n\",\n",
       " '9.24. Let $A=\\\\left[\\\\begin{array}{rr}7 & 3 \\\\\\\\ 3 & -1\\\\end{array}\\\\right]$. Find an orthogonal matrix $P$ such that $D=P^{-1} A P$ is diagonal.\\n\\nFirst find the characteristic polynomial $\\\\Delta(t)$ of $A$. We have\\n\\n$$\\n\\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}-6 t-16=(t-8)(t+2)\\n$$\\n\\nThus, the eigenvalues of $A$ are $\\\\lambda=8$ and $\\\\lambda=-2$. We next find corresponding eigenvectors.\\n\\nSubtract $\\\\lambda=8$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n-1 & 3 \\\\\\\\\\n3 & -9\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n-x+3 y & =0 \\\\\\\\\\n3 x-9 y & =0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad x-3 y=0\\n$$\\n\\nA nonzero solution is $u_{1}=(3,1)$.\\n\\nSubtract $\\\\lambda=-2$ (or add 2) down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n9 & 3 \\\\\\\\\\n3 & 1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{aligned}\\n& 9 x+3 y=0 \\\\\\\\\\n& 3 x+y=0\\n\\\\end{aligned} \\\\quad \\\\text { or } \\\\quad 3 x+y=0\\n$$\\n\\nA nonzero solution is $u_{2}=(1,-3)$.\\n\\nAs expected, because $A$ is symmetric, the eigenvectors $u_{1}$ and $u_{2}$ are orthogonal. Normalize $u_{1}$ and $u_{2}$ to obtain, respectively, the unit vectors\\n\\n$$\\n\\\\hat{u}_{1}=(3 / \\\\sqrt{10}, 1 / \\\\sqrt{10}) \\\\quad \\\\text { and } \\\\quad \\\\hat{u}_{2}=(1 / \\\\sqrt{10},-3 / \\\\sqrt{10})\\n$$\\n\\nFinally, let $P$ be the matrix whose columns are the unit vectors $\\\\hat{u}_{1}$ and $\\\\hat{u}_{2}$, respectively. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rr}\\n3 / \\\\sqrt{10} & 1 / \\\\sqrt{10} \\\\\\\\\\n1 / \\\\sqrt{10} & -3 / \\\\sqrt{10}\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} A P=\\\\left[\\\\begin{array}{rr}\\n8 & 0 \\\\\\\\\\n0 & -2\\n\\\\end{array}\\\\right]\\n$$\\n\\nAs expected, the diagonal entries in $D$ are the eigenvalues of $A$.\\n',\n",
       " '\\n9.25. Let $B=\\\\left[\\\\begin{array}{rrr}11 & -8 & 4 \\\\\\\\ -8 & -1 & -2 \\\\\\\\ 4 & -2 & -4\\\\end{array}\\\\right]$. (a) Find all eigenvalues of $B$.\\n\\n(b) Find a maximal set $S$ of nonzero orthogonal eigenvectors of $B$.\\n\\n(c) Find an orthogonal matrix $P$ such that $D=P^{-1} B P$ is diagonal.\\n\\n(a) First find the characteristic polynomial of $B$. We have\\n\\n$$\\n\\\\operatorname{tr}(B)=6, \\\\quad|B|=400, \\\\quad B_{11}=0, \\\\quad B_{22}=-60, \\\\quad B_{33}=-75, \\\\quad \\\\text { so } \\\\quad \\\\sum_{i} B_{i i}=-135\\n$$\\n\\nHence, $\\\\Delta(t)=t^{3}-6 t^{2}-135 t-400$. If $\\\\Delta(t)$ has an integer root it must divide 400 . Testing $t=-5$, by synthetic division, yields\\n\\n$$\\n-5 \\\\left\\\\lvert\\\\, \\\\begin{array}{r}\\n1-6-135-400 \\\\\\\\\\n-5+55+400\\n\\\\end{array}\\\\right.\\n$$\\n\\nThus, $t+5$ is a factor of $\\\\Delta(t)$, and $t^{2}-11 t-80$ is a factor. Thus,\\n\\n$$\\n\\\\Delta(t)=(t+5)\\\\left(t^{2}-11 t-80\\\\right)=(t+5)^{2}(t-16)\\n$$\\n\\nThe eigenvalues of $B$ are $\\\\lambda=-5$ (multiplicity 2), and $\\\\lambda=16$ (multiplicity 1).\\n\\n(b) Find an orthogonal basis for each eigenspace. Subtract $\\\\lambda=-5$ (or, add 5) down the diagonal of $B$ to obtain the homogeneous system\\n\\n$$\\n16 x-8 y+4 z=0, \\\\quad-8 x+4 y-2 z=0, \\\\quad 4 x-2 y+z=0\\n$$\\n\\nThat is, $4 x-2 y+z=0$. The system has two independent solutions. One solution is $v_{1}=(0,1,2)$. We seek a second solution $v_{2}=(a, b, c)$, which is orthogonal to $v_{1}$, such that\\n\\n$$\\n4 a-2 b+c=0, \\\\quad \\\\text { and also } \\\\quad b-2 c=0\\n$$\\n\\nOne such solution is $v_{2}=(-5,-8,4)$.\\n\\nSubtract $\\\\lambda=16$ down the diagonal of $B$ to obtain the homogeneous system\\n\\n$$\\n-5 x-8 y+4 z=0, \\\\quad-8 x-17 y-2 z=0, \\\\quad 4 x-2 y-20 z=0\\n$$\\n\\nThis system yields a nonzero solution $v_{3}=(4,-2,1)$. (As expected from Theorem 9.13, the eigenvector $v_{3}$ is orthogonal to $v_{1}$ and $v_{2}$.)\\n\\nThen $v_{1}, v_{2}, v_{3}$ form a maximal set of nonzero orthogonal eigenvectors of $B$.\\n\\n(c) Normalize $v_{1}, v_{2}, v_{3}$ to obtain the orthonormal basis:\\n\\n$$\\n\\\\hat{v}_{1}=v_{1} / \\\\sqrt{5}, \\\\quad \\\\hat{v}_{2}=v_{2} / \\\\sqrt{105}, \\\\quad \\\\hat{v}_{3}=v_{3} / \\\\sqrt{21}\\n$$\\n\\nThen $P$ is the matrix whose columns are $\\\\hat{v}_{1}, \\\\hat{v}_{2}, \\\\hat{v}_{3}$. Thus,\\n\\n$$\\nP=\\\\left[\\\\begin{array}{crr}\\n0 & -5 / \\\\sqrt{105} & 4 / \\\\sqrt{21} \\\\\\\\\\n1 / \\\\sqrt{5} & -8 / \\\\sqrt{105} & -2 / \\\\sqrt{21} \\\\\\\\\\n2 / \\\\sqrt{5} & 4 / \\\\sqrt{105} & 1 / \\\\sqrt{21}\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} B P=\\\\left[\\\\begin{array}{lll}\\n-5 & & \\\\\\\\\\n& -5 & \\\\\\\\\\n& & 16\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n9.26. Let $q(x, y)=x^{2}+6 x y-7 y^{2}$. Find an orthogonal substitution that diagonalizes $q$.\\n\\nFind the symmetric matrix $A$ that represents $q$ and its characteristic polynomial $\\\\Delta(t)$. We have\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n1 & 3 \\\\\\\\\\n3 & -7\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad \\\\Delta(t)=t^{2}+6 t-16=(t-2)(t+8)\\n$$\\n\\nThe eigenvalues of $A$ are $\\\\lambda=2$ and $\\\\lambda=-8$. Thus, using $s$ and $t$ as new variables, a diagonal form of $q$ is\\n\\n$$\\nq(s, t)=2 s^{2}-8 t^{2}\\n$$\\n\\nThe corresponding orthogonal substitution is obtained by finding an orthogonal set of eigenvectors of $A$.\\n\\n(i) Subtract $\\\\lambda=2$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n-1 & 3 \\\\\\\\\\n3 & -9\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{array}{r}\\n-x+3 y=0 \\\\\\\\\\n3 x-9 y=0\\n\\\\end{array} \\\\quad \\\\text { or } \\\\quad-x+3 y=0\\n$$\\n\\nA nonzero solution is $u_{1}=(3,1)$.\\n\\n(ii) Subtract $\\\\lambda=-8$ (or add 8 ) down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n9 & 3 \\\\\\\\\\n3 & 1\\n\\\\end{array}\\\\right], \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{array}{r}\\n9 x+3 y=0 \\\\\\\\\\n3 x+y=0\\n\\\\end{array} \\\\quad \\\\text { or } \\\\quad 3 x+y=0\\n$$\\n\\nA nonzero solution is $u_{2}=(-1,3)$.\\n\\nAs expected, because $A$ is symmetric, the eigenvectors $u_{1}$ and $u_{2}$ are orthogonal.\\n\\nNow normalize $u_{1}$ and $u_{2}$ to obtain, respectively, the unit vectors\\n\\n$$\\n\\\\hat{u}_{1}=(3 / \\\\sqrt{10}, 1 / \\\\sqrt{10}) \\\\quad \\\\text { and } \\\\quad \\\\hat{u}_{2}=(-1 / \\\\sqrt{10}, 3 / \\\\sqrt{10}) \\\\text {. }\\n$$\\n\\nFinally, let $P$ be the matrix whose columns are the unit vectors $\\\\hat{u}_{1}$ and $\\\\hat{u}_{2}$, respectively, and then $[x, y]^{T}=P[s, t]^{T}$ is the required orthogonal change of coordinates. That is,\\n\\n$$\\n\\\\left.P=\\\\left\\\\lvert\\\\, \\\\begin{array}{rr}\\n3 / \\\\sqrt{10} & -1 / \\\\sqrt{10} \\\\\\\\\\n1 / \\\\sqrt{10} & 3 / \\\\sqrt{10}\\n\\\\end{array}\\\\right.\\\\right] \\\\quad \\\\text { and } \\\\quad x=\\\\frac{3 s-t}{\\\\sqrt{10}}, \\\\quad y=\\\\frac{s+3 t}{\\\\sqrt{10}}\\n$$\\n\\nOne can also express $s$ and $t$ in terms of $x$ and $y$ by using $P^{-1}=P^{T}$. That is,\\n\\n$$\\ns=\\\\frac{3 x+y}{\\\\sqrt{10}}, \\\\quad t=\\\\frac{-x+3 t}{\\\\sqrt{10}}\\n$$\\n\\n\\n\\\\section*{Minimal Polynomial}\\n',\n",
       " '9.27. Let $A=\\\\left[\\\\begin{array}{lll}4 & -2 & 2 \\\\\\\\ 6 & -3 & 4 \\\\\\\\ 3 & -2 & 3\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{lll}3 & -2 & 2 \\\\\\\\ 4 & -4 & 6 \\\\\\\\ 2 & -3 & 5\\\\end{array}\\\\right]$. The characteristic polynomial of both matrices is $\\\\Delta(t)=(t-2)(t-1)^{2}$. Find the minimal polynomial $m(t)$ of each matrix.\\n\\nThe minimal polynomial $m(t)$ must divide $\\\\Delta(t)$. Also, each factor of $\\\\Delta(t)$ (i.e., $t-2$ and $t-1$ ) must also be a factor of $m(t)$. Thus, $m(t)$ must be exactly one of the following:\\n\\n$$\\nf(t)=(t-2)(t-1) \\\\quad \\\\text { or } \\\\quad g(t)=(t-2)(t-1)^{2}\\n$$\\n\\n(a) By the Cayley-Hamilton theorem, $g(A)=\\\\Delta(A)=0$, so we need only test $f(t)$. We have\\n\\n$$\\nf(A)=(A-2 I)(A-I)=\\\\left[\\\\begin{array}{lll}\\n2 & -2 & 2 \\\\\\\\\\n6 & -5 & 4 \\\\\\\\\\n3 & -2 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{lll}\\n3 & -2 & 2 \\\\\\\\\\n6 & -4 & 4 \\\\\\\\\\n3 & -2 & 2\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll}\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, $m(t)=f(t)=(t-2)(t-1)=t^{2}-3 t+2$ is the minimal polynomial of $A$.\\n\\n(b) Again $g(B)=\\\\Delta(B)=0$, so we need only test $f(t)$. We get\\n\\n$$\\nf(B)=(B-2 I)(B-I)=\\\\left[\\\\begin{array}{lll}\\n1 & -2 & 2 \\\\\\\\\\n4 & -6 & 6 \\\\\\\\\\n2 & -3 & 3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{lll}\\n2 & -2 & 2 \\\\\\\\\\n4 & -5 & 6 \\\\\\\\\\n2 & -3 & 4\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{lll}\\n-2 & 2 & -2 \\\\\\\\\\n-4 & 4 & -4 \\\\\\\\\\n-2 & 2 & -2\\n\\\\end{array}\\\\right] \\\\neq 0\\n$$\\n\\nThus, $m(t) \\\\neq f(t)$. Accordingly, $m(t)=g(t)=(t-2)(t-1)^{2}$ is the minimal polynomial of $B$. [We emphasize that we do not need to compute $g(B)$; we know $g(B)=0$ from the Cayley-Hamilton theorem.]\\n',\n",
       " '\\n9.28. Find the minimal polynomial $m(t)$ of each of the following matrices:\\n\\n(a) $A=\\\\left[\\\\begin{array}{ll}5 & 1 \\\\\\\\ 3 & 7\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{lll}1 & 2 & 3 \\\\\\\\ 0 & 2 & 3 \\\\\\\\ 0 & 0 & 3\\\\end{array}\\\\right]$, (c) $C=\\\\left[\\\\begin{array}{rr}4 & -1 \\\\\\\\ 1 & 2\\\\end{array}\\\\right]$\\n\\n(a) The characteristic polynomial of $A$ is $\\\\Delta(t)=t^{2}-12 t+32=(t-4)(t-8)$. Because $\\\\Delta(t)$ has distinct factors, the minimal polynomial $m(t)=\\\\Delta(t)=t^{2}-12 t+32$.\\n\\n(b) Because $B$ is triangular, its eigenvalues are the diagonal elements $1,2,3$; and so its characteristic polynomial is $\\\\Delta(t)=(t-1)(t-2)(t-3)$. Because $\\\\Delta(t)$ has distinct factors, $m(t)=\\\\Delta(t)$.\\n\\n(c) The characteristic polynomial of $C$ is $\\\\Delta(t)=t^{2}-6 t+9=(t-3)^{2}$. Hence the minimal polynomial of $C$ is $f(t)=t-3$ or $g(t)=(t-3)^{2}$. However, $f(C) \\\\neq 0$; that is, $C-3 I \\\\neq 0$. Hence,\\n\\n$$\\nm(t)=g(t)=\\\\Delta(t)=(t-3)^{2}\\n$$\\n',\n",
       " \"\\n9.29. Suppose $S=\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is a basis of $V$, and suppose $F$ and $G$ are linear operators on $V$ such that $[F]$ has 0 's on and below the diagonal, and $[G]$ has $a \\\\neq 0$ on the superdiagonal and 0 's elsewhere. That is,\\n\\n$$\\n[F]=\\\\left[\\\\begin{array}{ccccc}\\n0 & a_{21} & a_{31} & \\\\ldots & a_{n 1} \\\\\\\\\\n0 & 0 & a_{32} & \\\\ldots & a_{n 2} \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\n0 & 0 & 0 & \\\\ldots & a_{n, n-1} \\\\\\\\\\n0 & 0 & 0 & \\\\ldots & 0\\n\\\\end{array}\\\\right], \\\\quad[G]=\\\\left[\\\\begin{array}{ccccc}\\n0 & a & 0 & \\\\ldots & 0 \\\\\\\\\\n0 & 0 & a & \\\\ldots & 0 \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\n0 & 0 & 0 & \\\\ldots & a \\\\\\\\\\n0 & 0 & 0 & \\\\ldots & 0\\n\\\\end{array}\\\\right]\\n$$\\n\\nShow that (a) $F^{n}=0$, (b) $G^{n-1} \\\\neq 0$, but $G^{n}=0$. (These conditions also hold for $[F]$ and $[G]$.)\\n\\n(a) We have $F\\\\left(u_{1}\\\\right)=0$ and, for $r>1, F\\\\left(u_{r}\\\\right)$ is a linear combination of vectors preceding $u_{r}$ in $S$. That is,\\n\\n$$\\nF\\\\left(u_{r}\\\\right)=a_{r 1} u_{1}+a_{r 2} u_{2}+\\\\cdots+a_{r, r-1} u_{r-1}\\n$$\\n\\nHence, $F^{2}\\\\left(u_{r}\\\\right)=F\\\\left(F\\\\left(u_{r}\\\\right)\\\\right)$ is a linear combination of vectors preceding $u_{r-1}$, and so on. Hence, $F^{r}\\\\left(u_{r}\\\\right)=0$ for each $r$. Thus, for each $r, F^{n}\\\\left(u_{r}\\\\right)=F^{n-r}(0)=0$, and so $F^{n}=0$, as claimed.\\n\\n(b) We have $G\\\\left(u_{1}\\\\right)=0$ and, for each $k>1, G\\\\left(u_{k}\\\\right)=a u_{k-1}$. Hence, $G^{r}\\\\left(u_{k}\\\\right)=a^{r} u_{k-r}$ for $r<k$. Because $a \\\\neq 0$, $a^{n-1} \\\\neq 0$. Therefore, $G^{n-1}\\\\left(u_{n}\\\\right)=a^{n-1} u_{1} \\\\neq 0$, and so $G^{n-1} \\\\neq 0$. On the other hand, by (a), $G^{n}=0$.\\n\",\n",
       " \"\\n9.30. Let $B$ be the matrix in Example 9.12(a) that has 1's on the diagonal, $a$ 's on the superdiagonal, where $a \\\\neq 0$, and 0 's elsewhere. Show that $f(t)=(t-\\\\lambda)^{n}$ is both the characteristic polynomial $\\\\Delta(t)$ and the minimum polynomial $m(t)$ of $A$.\\n\\nBecause $A$ is triangular with $\\\\lambda$ 's on the diagonal, $\\\\Delta(t)=f(t)=(t-\\\\lambda)^{n}$ is its characteristic polynomial. Thus, $m(t)$ is a power of $t-\\\\lambda$. By Problem 9.29, $(A-\\\\lambda I)^{r-1} \\\\neq 0$. Hence, $m(t)=\\\\Delta(t)=(t-\\\\lambda)^{n}$.\\n\",\n",
       " '\\n9.31. Find the characteristic polynomial $\\\\Delta(t)$ and minimal polynomial $m(t)$ of each matrix:\\n\\n(a) $M=\\\\left[\\\\begin{array}{lllll}4 & 1 & 0 & 0 & 0 \\\\\\\\ 0 & 4 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 4 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 4 & 1 \\\\\\\\ 0 & 0 & 0 & 0 & 4\\\\end{array}\\\\right]$, (b) $\\\\quad M^{\\\\prime}=\\\\left[\\\\begin{array}{rrrr}2 & 7 & 0 & 0 \\\\\\\\ 0 & 2 & 0 & 0 \\\\\\\\ 0 & 0 & 1 & 1 \\\\\\\\ 0 & 0 & -2 & 4\\\\end{array}\\\\right]$\\n\\n(a) $M$ is block diagonal with diagonal blocks\\n\\n$$\\nA=\\\\left[\\\\begin{array}{lll}\\n4 & 1 & 0 \\\\\\\\\\n0 & 4 & 1 \\\\\\\\\\n0 & 0 & 4\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad B=\\\\left[\\\\begin{array}{ll}\\n4 & 1 \\\\\\\\\\n0 & 4\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe characteristic and minimal polynomial of $A$ is $f(t)=(t-4)^{3}$ and the characteristic and minimal polynomial of $B$ is $g(t)=(t-4)^{2}$. Then\\n\\n$$\\n\\\\Delta(t)=f(t) g(t)=(t-4)^{5} \\\\quad \\\\text { but } \\\\quad m(t)=\\\\operatorname{LCM}[f(t), g(t)]=(t-4)^{3}\\n$$\\n\\n(where LCM means least common multiple). We emphasize that the exponent in $m(t)$ is the size of the largest block.\\n\\n(b) Here $M^{\\\\prime}$ is block diagonal with diagonal blocks $A^{\\\\prime}=\\\\left[\\\\begin{array}{ll}2 & 7 \\\\\\\\ 0 & 2\\\\end{array}\\\\right]$ and $\\\\quad B^{\\\\prime}=\\\\left[\\\\begin{array}{rr}1 & 1 \\\\\\\\ -2 & 4\\\\end{array}\\\\right]$ The characteristic and minimal polynomial of $A^{\\\\prime}$ is $f(t)=(t-2)^{2}$. The characteristic polynomial of $B^{\\\\prime}$ is $g(t)=t^{2}-5 t+6=(t-2)(t-3)$, which has distinct factors. Hence, $g(t)$ is also the minimal polynomial of $B$. Accordingly,\\n\\n$$\\n\\\\Delta(t)=f(t) g(t)=(t-2)^{3}(t-3) \\\\quad \\\\text { but } \\\\quad m(t)=\\\\operatorname{LCM}[f(t), g(t)]=(t-2)^{2}(t-3)\\n$$\\n',\n",
       " '\\n9.32. Find a matrix $A$ whose minimal polynomial is $f(t)=t^{3}-8 t^{2}+5 t+7$.\\n\\nSimply let $A=\\\\left[\\\\begin{array}{rrr}0 & 0 & -7 \\\\\\\\ 1 & 0 & -5 \\\\\\\\ 0 & 1 & 8\\\\end{array}\\\\right]$, the companion matrix of $f(t)$ [defined in Example 9.12(b)].\\n',\n",
       " '\\n9.33. Prove Theorem 9.15: The minimal polynomial $m(t)$ of a matrix (linear operator) $A$ divides every polynomial that has $A$ as a zero. In particular (by the Cayley-Hamilton theorem), $m(t)$ divides the characteristic polynomial $\\\\Delta(t)$ of $A$.\\n\\nSuppose $f(t)$ is a polynomial for which $f(A)=0$. By the division algorithm, there exist polynomials $q(t)$ and $r(t)$ for which $f(t)=m(t) q(t)+r(t)$ and $r(t)=0$ or $\\\\operatorname{deg} r(t)<\\\\operatorname{deg} m(t)$. Substituting $t=A$ in this equation, and using that $f(A)=0$ and $m(A)=0$, we obtain $r(A)=0$. If $r(t) \\\\neq 0$, then $r(t)$ is a polynomial of degree less than $m(t)$ that has $A$ as a zero. This contradicts the definition of the minimal polynomial. Thus, $r(t)=0$, and so $f(t)=m(t) q(t)$; that is, $m(t)$ divides $f(t)$.\\n',\n",
       " '\\n9.34. Let $m(t)$ be the minimal polynomial of an $n$-square matrix $A$. Prove that the characteristic polynomial $\\\\Delta(t)$ of $A$ divides $[m(t)]^{n}$.\\n\\nSuppose $m(t)=t^{r}+c_{1} t^{r-1}+\\\\cdots+c_{r-1} t+c_{r}$. Define matrices $B_{j}$ as follows:\\n\\n$$\\n\\\\begin{aligned}\\n& B_{0}=I \\\\quad \\\\text { so } \\\\quad I=B_{0} \\\\\\\\\\n& B_{1}=A+c_{1} I \\\\quad \\\\text { so } \\\\quad c_{1} I=B_{1}-A=B_{1}-A B_{0} \\\\\\\\\\n& B_{2}=A^{2}+c_{1} A+c_{2} I \\\\quad \\\\text { so } \\\\quad c_{2} I=B_{2}-A\\\\left(A+c_{1} I\\\\right)=B_{2}-A B_{1} \\\\\\\\\\n& B_{r-1}=A^{r-1}+c_{1} A^{r-2}+\\\\cdots+c_{r-1} I \\\\quad \\\\text { so } \\\\quad c_{r-1} I=B_{r-1}-A B_{r-2}\\n\\\\end{aligned}\\n$$\\n\\nThen\\n\\nSet\\n\\n$$\\n\\\\begin{gathered}\\n-A B_{r-1}=c_{r} I-\\\\left(A^{r}+c_{1} A^{r-1}+\\\\cdots+c_{r-1} A+c_{r} I\\\\right)=c_{r} I-m(A)=c_{r} I \\\\\\\\\\nB(t)=t^{r-1} B_{0}+t^{r-2} B_{1}+\\\\cdots+t B_{r-2}+B_{r-1}\\n\\\\end{gathered}\\n$$\\n\\nThen\\n\\n$$\\n\\\\begin{aligned}\\n(t I-A) B(t) & =\\\\left(t^{r} B_{0}+t^{r-1} B_{1}+\\\\cdots+t B_{r-1}\\\\right)-\\\\left(t^{r-1} A B_{0}+t^{r-2} A B_{1}+\\\\cdots+A B_{r-1}\\\\right) \\\\\\\\\\n& =t^{r} B_{0}+t^{r-1}\\\\left(B_{1}-A B_{0}\\\\right)+t^{r-2}\\\\left(B_{2}-A B_{1}\\\\right)+\\\\cdots+t\\\\left(B_{r-1}-A B_{r-2}\\\\right)-A B_{r-1} \\\\\\\\\\n& =t^{r} I+c_{1} t^{r-1} I+c_{2} t^{r-2} I+\\\\cdots+c_{r-1} t I+c_{r} I=m(t) I\\n\\\\end{aligned}\\n$$\\n\\nTaking the determinant of both sides gives $|t I-A||B(t)|=|m(t) I|=[m(t)]^{n}$. Because $|B(t)|$ is a polynomial, $|t I-A|$ divides $[m(t)]^{n}$; that is, the characteristic polynomial of $A$ divides $[m(t)]^{n}$.\\n',\n",
       " '\\n9.35. Prove Theorem 9.16: The characteristic polynomial $\\\\Delta(t)$ and the minimal polynomial $m(t)$ of $A$ have the same irreducible factors.\\n\\nSuppose $f(t)$ is an irreducible polynomial. If $f(t)$ divides $m(t)$, then $f(t)$ also divides $\\\\Delta(t)$ [because $m(t)$ divides $\\\\Delta(t)]$. On the other hand, if $f(t)$ divides $\\\\Delta(t)$, then by Problem 9.34, $f(t)$ also divides $[m(t)]^{n}$. But $f(t)$ is irreducible; hence, $f(t)$ also divides $m(t)$. Thus, $m(t)$ and $\\\\Delta(t)$ have the same irreducible factors.\\n',\n",
       " '\\n9.36. Prove Theorem 9.19: The minimal polynomial $m(t)$ of a block diagonal matrix $M$ with diagonal blocks $A_{i}$ is equal to the least common multiple (LCM) of the minimal polynomials of the diagonal blocks $A_{i}$.\\n\\nWe prove the theorem for the case $r=2$. The general theorem follows easily by induction. Suppose $M=\\\\left[\\\\begin{array}{cc}A & 0 \\\\\\\\ 0 & B\\\\end{array}\\\\right]$, where $A$ and $B$ are square matrices. We need to show that the minimal polynomial $m(t)$ of $M$ is the LCM of the minimal polynomials $g(t)$ and $h(t)$ of $A$ and $B$, respectively.\\n\\nBecause $m(t)$ is the minimal polynomial of $M, m(M)=\\\\left[\\\\begin{array}{cc}m(A) & 0 \\\\\\\\ 0 & m(B)\\\\end{array}\\\\right]=0$, and $m(A)=0$ and $m(B)=0$. Because $g(t)$ is the minimal polynomial of $A, g(t)$ divides $m(t)$. Similarly, $h(t)$ divides $m(t)$. Thus $m(t)$ is a multiple of $g(t)$ and $h(t)$.\\n\\nNow let $f(t)$ be another multiple of $g(t)$ and $h(t)$. Then $f(M)=\\\\left[\\\\begin{array}{cc}f(A) & 0 \\\\\\\\ 0 & f(B)\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}0 & 0 \\\\\\\\ 0 & 0\\\\end{array}\\\\right]=0$. But $m(t)$ is the minimal polynomial of $M$; hence, $m(t)$ divides $f(t)$. Thus, $m(t)$ is the LCM of $g(t)$ and $h(t)$.\\n',\n",
       " '\\n9.37. Suppose $m(t)=t^{r}+a_{r-1} t^{r-1}+\\\\cdots+a_{1} t+a_{0}$ is the minimal polynomial of an $n$-square matrix $A$. Prove the following:\\n\\n(a) $A$ is nonsingular if and only if the constant term $a_{0} \\\\neq 0$.\\n\\n(b) If $A$ is nonsingular, then $A^{-1}$ is a polynomial in $A$ of degree $r-1<n$.\\n\\n(a) The following are equivalent: (i) $A$ is nonsingular, (ii) 0 is not a root of $m(t)$, (iii) $a_{0} \\\\neq 0$. Thus, the statement is true.\\\\\\\\\\n(b) Because $A$ is nonsingular, $a_{0} \\\\neq 0$ by (a). We have\\n\\nThus,\\n\\n$$\\nm(A)=A^{r}+a_{r-1} A^{r-1}+\\\\cdots+a_{1} A+a_{0} I=0\\n$$\\n\\n$$\\n-\\\\frac{1}{a_{0}}\\\\left(A^{r-1}+a_{r-1} A^{r-2}+\\\\cdots+a_{1} I\\\\right) A=I\\n$$\\n\\nAccordingly,\\n\\n$$\\nA^{-1}=-\\\\frac{1}{a_{0}}\\\\left(A^{r-1}+a_{r-1} A^{r-2}+\\\\cdots+a_{1} I\\\\right)\\n$$\\n\\n',\n",
       " '10.1. Suppose $T: V \\\\rightarrow V$ is linear. Show that each of the following is invariant under $T$ :\\\\\\\\\\n(a) $\\\\{0\\\\}$,\\\\\\\\\\n(b) $V$,\\\\\\\\\\n(c) kernel of $T$,\\\\\\\\\\n(d) image of $T$.\\n\\n(a) We have $T(0)=0 \\\\in\\\\{0\\\\}$; hence, $\\\\{0\\\\}$ is invariant under $T$.\\n\\n(b) For every $v \\\\in V, T(v) \\\\in V$; hence, $V$ is invariant under $T$.\\n\\n(c) Let $u \\\\in \\\\operatorname{Ker} T$. Then $T(u)=0 \\\\in \\\\operatorname{Ker} T$ because the kernel of $T$ is a subspace of $V$. Thus, Ker $T$ is invariant under $T$.\\n\\n(d) Because $T(v) \\\\in \\\\operatorname{Im} T$ for every $v \\\\in V$, it is certainly true when $v \\\\in \\\\operatorname{Im} T$. Hence, the image of $T$ is invariant under $T$.\\n',\n",
       " '\\n10.2. Suppose $\\\\left\\\\{W_{i}\\\\right\\\\}$ is a collection of $T$-invariant subspaces of a vector space $V$. Show that the intersection $W=\\\\bigcap_{i} W_{i}$ is also $T$-invariant.\\n\\nSuppose $v \\\\in W$; then $v \\\\in W_{i}$ for every $i$. Because $W_{i}$ is $T$-invariant, $T(v) \\\\in W_{i}$ for every $i$. Thus, $T(v) \\\\in W$ and so $W$ is $T$-invariant.\\n',\n",
       " '\\n10.3. Prove Theorem 10.2: Let $T: V \\\\rightarrow V$ be linear. For any polynomial $f(t)$, the kernel of $f(T)$ is invariant under $T$.\\n\\nSuppose $v \\\\in \\\\operatorname{Ker} f(T)$ - that is, $f(T)(v)=0$. We need to show that $T(v)$ also belongs to the kernel of $f(T)$-that is, $f(T)(T(v))=(f(T) \\\\circ T)(v)=0$. Because $f(t) t=t f(t)$, we have $f(T) \\\\circ T=T \\\\circ f(T)$. Thus, as required,\\n\\n$$\\n(f(T) \\\\circ T)(v)=(T \\\\circ f(T))(v)=T(f(T)(v))=T(0)=0\\n$$\\n',\n",
       " '\\n10.4. Find all invariant subspaces of $A=\\\\left[\\\\begin{array}{ll}2 & -5 \\\\\\\\ 1 & -2\\\\end{array}\\\\right]$ viewed as an operator on $\\\\mathbf{R}^{2}$.\\n\\nBy Problem 10.1, $\\\\mathbf{R}^{2}$ and $\\\\{0\\\\}$ are invariant under $A$. Now if $A$ has any other invariant subspace, it must be one-dimensional. However, the characteristic polynomial of $A$ is\\n\\n$$\\n\\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}+1\\n$$\\n\\nHence, $A$ has no eigenvalues (in $\\\\mathbf{R}$ ) and so $A$ has no eigenvectors. But the one-dimensional invariant subspaces correspond to the eigenvectors; thus, $\\\\mathbf{R}^{2}$ and $\\\\{0\\\\}$ are the only subspaces invariant under $A$.\\n',\n",
       " '\\n10.5. Prove Theorem 10.3: Suppose $W$ is $T$-invariant. Then $T$ has a triangular block representation $\\\\left[\\\\begin{array}{ll}A & B \\\\\\\\ 0 & C\\\\end{array}\\\\right]$, where $A$ is the matrix representation of the restriction $\\\\hat{T}$ of $T$ to $W$.\\n\\nWe choose a basis $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}\\\\right\\\\}$ of $W$ and extend it to a basis $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}, v_{1}, \\\\ldots, v_{s}\\\\right\\\\}$ of $V$. We have\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\hat{T}\\\\left(w_{1}\\\\right)=T\\\\left(w_{1}\\\\right)=a_{11} w_{1}+\\\\cdots+a_{1 r} w_{r} \\\\\\\\\\n& \\\\hat{T}\\\\left(w_{2}\\\\right)=T\\\\left(w_{2}\\\\right)=a_{21} w_{1}+\\\\cdots+a_{2 r} w_{r} \\\\\\\\\\n& \\\\hat{T}\\\\left(w_{r}\\\\right)=T\\\\left(w_{r}\\\\right)=a_{r 1} w_{1}+\\\\cdots+a_{r r} w_{r} \\\\\\\\\\n& T\\\\left(v_{1}\\\\right)=b_{11} w_{1}+\\\\cdots+b_{1 r} w_{r}+c_{11} v_{1}+\\\\cdots+c_{1 s} v_{s} \\\\\\\\\\n& T\\\\left(v_{2}\\\\right)=b_{21} w_{1}+\\\\cdots+b_{2 r} w_{r}+c_{21} v_{1}+\\\\cdots+c_{2 s} v_{s} \\\\\\\\\\n& T\\\\left(v_{s}\\\\right)=b_{s 1} w_{1}+\\\\cdots+b_{s r} w_{r}+c_{s 1} v_{1}+\\\\cdots+c_{s s} v_{s}\\n\\\\end{aligned}\\n$$\\n\\nBut the matrix of $T$ in this basis is the transpose of the matrix of coefficients in the above system of equations (Section 6.2). Therefore, it has the form $\\\\left[\\\\begin{array}{cc}A & B \\\\\\\\ 0 & C\\\\end{array}\\\\right]$, where $A$ is the transpose of the matrix of coefficients for the obvious subsystem. By the same argument, $A$ is the matrix of $\\\\hat{T}$ relative to the basis $\\\\left\\\\{w_{i}\\\\right\\\\}$ of $W$.\\n',\n",
       " '\\n10.6. Let $\\\\hat{T}$ denote the restriction of an operator $T$ to an invariant subspace $W$. Prove\\n\\n(a) For any polynomial $f(t), f(\\\\hat{T})(w)=f(T)(w)$.\\n\\n(b) The minimal polynomial of $\\\\hat{T}$ divides the minimal polynomial of $T$.\\n\\n(a) If $f(t)=0$ or if $f(t)$ is a constant (i.e., of degree 1 ), then the result clearly holds.\\n\\nAssume $\\\\operatorname{deg} f=n>1$ and that the result holds for polynomials of degree less than $n$. Suppose that\\n\\n$$\\nf(t)=a_{n} t^{n}+a_{n-1} t^{n-1}+\\\\cdots+a_{1} t+a_{0}\\n$$\\n\\nThen\\n\\n$$\\n\\\\begin{aligned}\\nf(\\\\hat{T})(w) & =\\\\left(a_{n} \\\\hat{T}^{n}+a_{n-1} \\\\hat{T}^{n-1}+\\\\cdots+a_{0} I\\\\right)(w) \\\\\\\\\\n& =\\\\left(a_{n} \\\\hat{T}^{n-1}\\\\right)(\\\\hat{T}(w))+\\\\left(a_{n-1} \\\\hat{T}^{n-1}+\\\\cdots+a_{0} I\\\\right)(w) \\\\\\\\\\n& =\\\\left(a_{n} T^{n-1}\\\\right)(T(w))+\\\\left(a_{n-1} T^{n-1}+\\\\cdots+a_{0} I\\\\right)(w)=f(T)(w)\\n\\\\end{aligned}\\n$$\\n\\n(b) Let $m(t)$ denote the minimal polynomial of $T$. Then by (a), $m(\\\\hat{T})(w)=m(T)(w)=\\\\mathbf{0}(w)=0$ for every $w \\\\in W$; that is, $\\\\hat{T}$ is a zero of the polynomial $m(t)$. Hence, the minimal polynomial of $\\\\hat{T}$ divides $m(t)$.\\n\\n\\n\\\\section*{Invariant Direct-Sum Decompositions}\\n',\n",
       " \"10.7. Prove Theorem 10.4: Suppose $W_{1}, W_{2}, \\\\ldots, W_{r}$ are subspaces of $V$ with respective bases\\n\\n$$\\nB_{1}=\\\\left\\\\{w_{11}, w_{12}, \\\\ldots, w_{1 n_{1}}\\\\right\\\\}, \\\\quad \\\\ldots, \\\\quad B_{r}=\\\\left\\\\{w_{r 1}, w_{r 2}, \\\\ldots, w_{r n_{r}}\\\\right\\\\}\\n$$\\n\\nThen $V$ is the direct sum of the $W_{i}$ if and only if the union $B=\\\\bigcup_{i} B_{i}$ is a basis of $V$.\\n\\nSuppose $B$ is a basis of $V$. Then, for any $v \\\\in V$,\\n\\n$$\\nv=a_{11} w_{11}+\\\\cdots+a_{1 n_{1}} w_{1 n_{1}}+\\\\cdots+a_{r 1} w_{r 1}+\\\\cdots+a_{r n_{r}} w_{r n_{r}}=w_{1}+w_{2}+\\\\cdots+w_{r}\\n$$\\n\\nwhere $w_{i}=a_{i 1} w_{i 1}+\\\\cdots+a_{i n_{i}} w_{i n_{i}} \\\\in W_{i}$. We next show that such a sum is unique. Suppose\\n\\n$$\\nv=w_{1}^{\\\\prime}+w_{2}^{\\\\prime}+\\\\cdots+w_{r}^{\\\\prime}, \\\\quad \\\\text { where } \\\\quad w_{i}^{\\\\prime} \\\\in W_{i}\\n$$\\n\\nBecause $\\\\left\\\\{w_{i 1}, \\\\ldots, w_{i n_{i}}\\\\right\\\\}$ is a basis of $W_{i}, w_{i}^{\\\\prime}=b_{i 1} w_{i 1}+\\\\cdots+b_{i n_{i}} w_{i n_{i}}$, and so\\n\\n$$\\nv=b_{11} w_{11}+\\\\cdots+b_{1 n_{1}} w_{1 n_{1}}+\\\\cdots+b_{r 1} w_{r 1}+\\\\cdots+b_{r n_{r}} w_{r n_{r}}\\n$$\\n\\nBecause $B$ is a basis of $V, a_{i j}=b_{i j}$, for each $i$ and each $j$. Hence, $w_{i}=w_{i}^{\\\\prime}$, and so the sum for $v$ is unique. Accordingly, $V$ is the direct sum of the $W_{i}$.\\n\\nConversely, suppose $V$ is the direct sum of the $W_{i}$. Then for any $v \\\\in V, v=w_{1}+\\\\cdots+w_{r}$, where $w_{i} \\\\in W_{i}$. Because $\\\\left\\\\{w_{i j_{i}}\\\\right\\\\}$ is a basis of $W_{i}$, each $w_{i}$ is a linear combination of the $w_{i j}$, and so $v$ is a linear combination of the elements of $B$. Thus, $B$ spans $V$. We now show that $B$ is linearly independent. Suppose\\n\\n$$\\na_{11} w_{11}+\\\\cdots+a_{1 n_{1}} w_{1 n_{1}}+\\\\cdots+a_{r 1} w_{r 1}+\\\\cdots+a_{r n_{r}} w_{r n_{r}}=0\\n$$\\n\\nNote that $a_{i 1} w_{i 1}+\\\\cdots+a_{i n_{i}} w_{i n_{i}} \\\\in W_{i}$. We also have that $0=0+0 \\\\cdots 0 \\\\in W_{i}$. Because such a sum for 0 is unique,\\n\\n$$\\na_{i 1} w_{i 1}+\\\\cdots+a_{i n_{i}} w_{i n_{i}}=0 \\\\quad \\\\text { for } i=1, \\\\ldots, r\\n$$\\n\\nThe independence of the bases $\\\\left\\\\{w_{i j}\\\\right\\\\}$ implies that all the $a$ 's are 0 . Thus, $B$ is linearly independent and is a basis of $V$.\\n\",\n",
       " '\\n10.8. Suppose $T: V \\\\rightarrow V$ is linear and suppose $T=T_{1} \\\\oplus T_{2}$ with respect to a $T$-invariant direct-sum decomposition $V=U \\\\oplus W$. Show that\\n\\n(a) $m(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$, where $m(t), m_{1}(t), m_{2}(t)$ are the minimum polynomials of $T, T_{1}, T_{2}$, respectively.\\n\\n(b) $\\\\Delta(t)=\\\\Delta_{1}(t) \\\\Delta_{2}(t)$, where $\\\\Delta(t), \\\\Delta_{1}(t), \\\\Delta_{2}(t)$ are the characteristic polynomials of $T, T_{1}, T_{2}$, respectively.\\n\\n(a) By Problem 10.6, each of $m_{1}(t)$ and $m_{2}(t)$ divides $m(t)$. Now suppose $f(t)$ is a multiple of both $m_{1}(t)$ and $m_{2}(t)$, then $f\\\\left(T_{1}\\\\right)(U)=0$ and $f\\\\left(T_{2}\\\\right)(W)=0$. Let $v \\\\in V$, then $v=u+w$ with $u \\\\in U$ and $w \\\\in W$. Now\\n\\n$$\\nf(T) v=f(T) u+f(T) w=f\\\\left(T_{1}\\\\right) u+f\\\\left(T_{2}\\\\right) w=0+0=0\\n$$\\n\\nThat is, $T$ is a zero of $f(t)$. Hence, $m(t)$ divides $f(t)$, and so $m(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$. (b) By Theorem $10.5, T$ has a matrix representation $M=\\\\left[\\\\begin{array}{ll}A & 0 \\\\\\\\ 0 & B\\\\end{array}\\\\right]$, where $A$ and $B$ are matrix representations\\\\\\\\\\nof $T_{1}$ and $T_{2}$, respectively. Then, as required,\\n\\n$$\\n\\\\Delta(t)=|t I-M|=\\\\left|\\\\begin{array}{cc}\\nt I-A & 0 \\\\\\\\\\n0 & t I-B\\n\\\\end{array}\\\\right|=|t I-A||t I-B|=\\\\Delta_{1}(t) \\\\Delta_{2}(t)\\n$$\\n',\n",
       " '\\n10.9. Prove Theorem 10.7: Suppose $T: V \\\\rightarrow V$ is linear, and suppose $f(t)=g(t) h(t)$ are polynomials such that $f(T)=\\\\mathbf{0}$ and $g(t)$ and $h(t)$ are relatively prime. Then $V$ is the direct sum of the $T$-invariant subspaces $U$ and $W$ where $U=\\\\operatorname{Ker} g(T)$ and $W=\\\\operatorname{Ker} h(T)$.\\n\\nNote first that $U$ and $W$ are $T$-invariant by Theorem 10.2. Now, because $g(t)$ and $h(t)$ are relatively prime, there exist polynomials $r(t)$ and $s(t)$ such that\\n\\n\\n\\\\begin{equation*}\\nr(t) g(t)+s(t) h(t)=1 \\\\tag{*}\\n\\\\end{equation*}\\n\\n\\nHence, for the operator $T, \\\\quad r(T) g(T)+s(T) h(T)=I$\\n\\nLet $v \\\\in V$; then, by $\\\\left(^{*}\\\\right), \\\\quad v=r(T) g(T) v+s(T) h(T) v$\\n\\nBut the first term in this sum belongs to $W=\\\\operatorname{Ker} h(T)$, because\\n\\n$$\\nh(T) r(T) g(T) v=r(T) g(T) h(T) v=r(T) f(T) v=r(T) \\\\mathbf{0} v=0\\n$$\\n\\nSimilarly, the second term belongs to $U$. Hence, $V$ is the sum of $U$ and $W$.\\n\\nTo prove that $V=U \\\\oplus W$, we must show that a sum $v=u+w$ with $u \\\\in U, w \\\\in W$, is uniquely determined by $v$. Applying the operator $r(T) g(T)$ to $v=u+w$ and using $g(T) u=0$, we obtain\\n\\n$$\\nr(T) g(T) v=r(T) g(T) u+r(T) g(T) w=r(T) g(T) w\\n$$\\n\\nAlso, applying (*) to $w$ alone and using $h(T) w=0$, we obtain\\n\\n$$\\nw=r(T) g(T) w+s(T) h(T) w=r(T) g(T) w\\n$$\\n\\nBoth of the above formulas give us $w=r(T) g(T) v$, and so $w$ is uniquely determined by $v$. Similarly $u$ is uniquely determined by $v$. Hence, $V=U \\\\oplus W$, as required.\\n',\n",
       " '\\n10.10. Prove Theorem 10.8: In Theorem 10.7 (Problem 10.9), if $f(t)$ is the minimal polynomial of $T$ (and $g(t)$ and $h(t)$ are monic), then $g(t)$ is the minimal polynomial of the restriction $T_{1}$ of $T$ to $U$ and $h(t)$ is the minimal polynomial of the restriction $T_{2}$ of $T$ to $W$.\\n\\nLet $m_{1}(t)$ and $m_{2}(t)$ be the minimal polynomials of $T_{1}$ and $T_{2}$, respectively. Note that $g\\\\left(T_{1}\\\\right)=0$ and $h\\\\left(T_{2}\\\\right)=0$ because $U=\\\\operatorname{Ker} g(T)$ and $W=\\\\operatorname{Ker} h(T)$. Thus,\\n\\n\\n\\\\begin{equation*}\\nm_{1}(t) \\\\text { divides } g(t) \\\\quad \\\\text { and } \\\\quad m_{2}(t) \\\\text { divides } h(t) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBy Problem 10.9, $f(t)$ is the least common multiple of $m_{1}(t)$ and $m_{2}(t)$. But $m_{1}(t)$ and $m_{2}(t)$ are relatively prime because $g(t)$ and $h(t)$ are relatively prime. Accordingly, $f(t)=m_{1}(t) m_{2}(t)$. We also have that $f(t)=g(t) h(t)$. These two equations together with (1) and the fact that all the polynomials are monic imply that $g(t)=m_{1}(t)$ and $h(t)=m_{2}(t)$, as required.\\n',\n",
       " '\\n10.11. Prove the Primary Decomposition Theorem 10.6: Let $T: V \\\\rightarrow V$ be a linear operator with minimal polynomial\\n\\n$$\\nm(t)=f_{1}(t)^{n_{1}} f_{2}(t)^{n_{2}} \\\\ldots f_{r}(t)^{n_{r}}\\n$$\\n\\nwhere the $f_{i}(t)$ are distinct monic irreducible polynomials. Then $V$ is the direct sum of $T$ invariant subspaces $W_{1}, \\\\ldots, W_{r}$ where $W_{i}$ is the kernel of $f_{i}(T)^{n_{i}}$. Moreover, $f_{i}(t)^{n_{i}}$ is the minimal polynomial of the restriction of $T$ to $W_{i}$.\\n\\nThe proof is by induction on $r$. The case $r=1$ is trivial. Suppose that the theorem has been proved for $r-1$. By Theorem 10.7, we can write $V$ as the direct sum of $T$-invariant subspaces $W_{1}$ and $V_{1}$, where $W_{1}$ is the kernel of $f_{1}(T)^{n_{1}}$ and where $V_{1}$ is the kernel of $f_{2}(T)^{n_{2}} \\\\cdots f_{r}(T)^{n_{r}}$. By Theorem 10.8, the minimal polynomials of the restrictions of $T$ to $W_{1}$ and $V_{1}$ are $f_{1}(t)^{n_{1}}$ and $f_{2}(t)^{n_{2}} \\\\cdots f_{r}(t)^{n_{r}}$, respectively.\\n\\nDenote the restriction of $T$ to $V_{1}$ by $\\\\hat{T}_{1}$. By the inductive hypothesis, $V_{1}$ is the direct sum of subspaces $W_{2}, \\\\ldots, W_{r}$ such that $W_{i}$ is the kernel of $f_{i}\\\\left(T_{1}\\\\right)^{n_{i}}$ and such that $f_{i}(t)^{n_{i}}$ is the minimal polynomial for the restriction of $\\\\hat{T}_{1}$ to $W_{i}$. But the kernel of $f_{i}(T)^{n_{i}}$, for $i=2, \\\\ldots, r$ is necessarily contained in $V_{1}$, because $f_{i}(t)^{n_{i}}$ divides $f_{2}(t)^{n_{2}} \\\\cdots f_{r}(t)^{n_{r}}$. Thus, the kernel of $f_{i}(T)^{n_{i}}$ is the same as the kernel of $f_{i}\\\\left(T_{1}\\\\right)^{n_{i}}$, which is $W_{i}$. Also, the restriction of $T$ to $W_{i}$ is the same as the restriction of $\\\\hat{T}_{1}$ to $W_{i}$ (for $\\\\left.i=2, \\\\ldots, r\\\\right)$; hence, $f_{i}(t)^{n_{i}}$ is also the minimal polynomial for the restriction of $T$ to $W_{i}$. Thus, $V=W_{1} \\\\oplus W_{2} \\\\oplus \\\\cdots \\\\oplus W_{r}$ is the desired decomposition of $T$.\\n',\n",
       " '\\n10.12. Prove Theorem 10.9: A linear operator $T: V \\\\rightarrow V$ has a diagonal matrix representation if and only if its minimal polynomal $m(t)$ is a product of distinct linear polynomials.\\n\\nSuppose $m(t)$ is a product of distinct linear polynomials, say,\\n\\n$$\\nm(t)=\\\\left(t-\\\\lambda_{1}\\\\right)\\\\left(t-\\\\lambda_{2}\\\\right) \\\\cdots\\\\left(t-\\\\lambda_{r}\\\\right)\\n$$\\n\\nwhere the $\\\\lambda_{i}$ are distinct scalars. By the Primary Decomposition Theorem, $V$ is the direct sum of subspaces $W_{1}, \\\\ldots, W_{r}$, where $W_{i}=\\\\operatorname{Ker}\\\\left(T-\\\\lambda_{i} I\\\\right)$. Thus, if $v \\\\in W_{i}$, then $\\\\left(T-\\\\lambda_{i} I\\\\right)(v)=0$ or $T(v)=\\\\lambda_{i} v$. In other words, every vector in $W_{i}$ is an eigenvector belonging to the eigenvalue $\\\\lambda_{i}$. By Theorem 10.4, the union of bases for $W_{1}, \\\\ldots, W_{r}$ is a basis of $V$. This basis consists of eigenvectors, and so $T$ is diagonalizable.\\n\\nConversely, suppose $T$ is diagonalizable (i.e., $V$ has a basis consisting of eigenvectors of $T$ ). Let $\\\\lambda_{1}, \\\\ldots, \\\\lambda_{s}$ be the distinct eigenvalues of $T$. Then the operator\\n\\n$$\\nf(T)=\\\\left(T-\\\\lambda_{1} I\\\\right)\\\\left(T-\\\\lambda_{2} I\\\\right) \\\\cdots\\\\left(T-\\\\lambda_{s} I\\\\right)\\n$$\\n\\nmaps each basis vector into 0 . Thus, $f(T)=0$, and hence, the minimal polynomial $m(t)$ of $T$ divides the polynomial\\n\\n$$\\nf(t)=\\\\left(t-\\\\lambda_{1}\\\\right)\\\\left(t-\\\\lambda_{2}\\\\right) \\\\cdots\\\\left(t-\\\\lambda_{s} I\\\\right)\\n$$\\n\\nAccordingly, $m(t)$ is a product of distinct linear polynomials.\\n\\n\\n\\\\section*{Nilpotent Operators, Jordan Canonical Form}\\n',\n",
       " '10.13. Let $T: V$ be linear. Suppose, for $v \\\\in V, T^{k}(v)=0$ but $T^{k-1}(v) \\\\neq 0$. Prove\\n\\n(a) The set $S=\\\\left\\\\{v, T(v), \\\\ldots, T^{k-1}(v)\\\\right\\\\}$ is linearly independent.\\n\\n(b) The subspace $W$ generated by $S$ is $T$-invariant.\\n\\n(c) The restriction $\\\\hat{T}$ of $T$ to $W$ is nilpotent of index $k$.\\n\\n(d) Relative to the basis $\\\\left\\\\{T^{k-1}(v), \\\\ldots, T(v), v\\\\right\\\\}$ of $W$, the matrix of $T$ is the $k$-square Jordan nilpotent block $N_{k}$ of index $k$ (see Example 10.5).\\n\\n(a) Suppose\\n\\n\\n\\\\begin{equation*}\\na v+a_{1} T(v)+a_{2} T^{2}(v)+\\\\cdots+a_{k-1} T^{k-1}(v)=0 \\\\tag{*}\\n\\\\end{equation*}\\n\\n\\nApplying $T^{k-1}$ to $\\\\left(^{*}\\\\right)$ and using $T^{k}(v)=0$, we obtain $a T^{k-1}(v)=0$; because $T^{k-1}(v) \\\\neq 0, a=0$. Now applying $T^{k-2}$ to $\\\\left(^{*}\\\\right)$ and using $T^{k}(v)=0$ and $a=0$, we find $a_{1} T^{k-1}(v)=0$; hence, $a_{1}=0$. Next applying $T^{k-3}$ to $\\\\left(^{*}\\\\right)$ and using $T^{k}(v)=0$ and $a=a_{1}=0$, we obtain $a_{2} T^{k-1}(v)=0$; hence, $a_{2}=0$. Continuing this process, we find that all the $a^{\\\\prime}$ s are 0 ; hence, $S$ is independent.\\n\\n(b) Let $v \\\\in W$. Then\\n\\n$$\\nv=b v+b_{1} T(v)+b_{2} T^{2}(v)+\\\\cdots+b_{k-1} T^{k-1}(v)\\n$$\\n\\nUsing $T^{k}(v)=0$, we have\\n\\n$$\\nT(v)=b T(v)+b_{1} T^{2}(v)+\\\\cdots+b_{k-2} T^{k-1}(v) \\\\in W\\n$$\\n\\nThus, $W$ is $T$-invariant.\\n\\n(c) By hypothesis, $T^{k}(v)=0$. Hence, for $i=0, \\\\ldots, k-1$,\\n\\n$$\\n\\\\hat{T}^{k}\\\\left(T^{i}(v)\\\\right)=T^{k+i}(v)=0\\n$$\\n\\nThat is, applying $\\\\hat{T}^{k}$ to each generator of $W$, we obtain 0 ; hence, $\\\\hat{T}^{k}=\\\\mathbf{0}$ and so $\\\\hat{T}$ is nilpotent of index at most $k$. On the other hand, $\\\\hat{T}^{k-1}(v)=T^{k-1}(v) \\\\neq 0$; hence, $T$ is nilpotent of index exactly $k$.\\n\\n(d) For the basis $\\\\left\\\\{T^{k-1}(v), T^{k-2}(v), \\\\ldots, T(v), v\\\\right\\\\}$ of $W$,\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\hat{T}\\\\left(T^{k-1}(v)\\\\right)=T^{k}(v)=0 \\\\\\\\\\n& \\\\hat{T}\\\\left(T^{k-2}(v)\\\\right)=T^{k-1}(v) \\\\\\\\\\n& \\\\hat{T}\\\\left(T^{k-3}(v)\\\\right)=\\\\quad T^{k-2}(v) \\\\\\\\\\n& \\\\begin{array}{llll}\\n\\\\hat{T}(T(v)) & = & T^{2}(v) & \\\\\\\\\\n\\\\hat{T}(v) & = & T(v)\\n\\\\end{array}\\n\\\\end{aligned}\\n$$\\n\\nHence, as required, the matrix of $T$ in this basis is the $k$-square Jordan nilpotent block $N_{k}$.\\n',\n",
       " '\\n10.14. Let $T: V \\\\rightarrow V$ be linear. Let $U=\\\\operatorname{Ker} T^{i}$ and $W=\\\\operatorname{Ker} T^{i+1}$. Show that\\\\\\\\\\n(a) $U \\\\subseteq W$,\\\\\\\\\\n(b) $T(W) \\\\subseteq U$.\\n\\n(a) Suppose $u \\\\in U=\\\\operatorname{Ker} T^{i}$. Then $T^{i}(u)=0 \\\\quad$ and so $T^{i+1}(u)=T\\\\left(T^{i}(u)\\\\right)=T(0)=0$. Thus, $u \\\\in \\\\operatorname{Ker} T^{i+1}=W$. But this is true for every $u \\\\in U$; hence, $U \\\\subseteq W$.\\n\\n(b) Similarly, if $w \\\\in W=\\\\operatorname{Ker} T^{i+1}$, then $T^{i+1}(w)=0$. Thus, $T^{i+1}(w)=T^{i}(T(w))=T^{i}(0)=0$ and so $T(W) \\\\subseteq U$.\\n',\n",
       " '\\n10.15. Let $T: V$ be linear. Let $X=\\\\operatorname{Ker} T^{i-2}, Y=\\\\operatorname{Ker} T^{i-1}, Z=\\\\operatorname{Ker} T^{i}$. Therefore (Problem 10.14), $X \\\\subseteq Y \\\\subseteq Z$. Suppose\\n\\n$$\\n\\\\left\\\\{u_{1}, \\\\ldots, u_{r}\\\\right\\\\}, \\\\quad\\\\left\\\\{u_{1}, \\\\ldots, u_{r}, v_{1}, \\\\ldots, v_{s}\\\\right\\\\}, \\\\quad\\\\left\\\\{u_{1}, \\\\ldots, u_{r}, v_{1}, \\\\ldots, v_{s}, w_{1}, \\\\ldots, w_{t}\\\\right\\\\}\\n$$\\n\\nare bases of $X, Y, Z$, respectively. Show that\\n\\n$$\\nS=\\\\left\\\\{u_{1}, \\\\ldots, u_{r}, T\\\\left(w_{1}\\\\right), \\\\ldots, T\\\\left(w_{t}\\\\right)\\\\right\\\\}\\n$$\\n\\nis contained in $Y$ and is linearly independent.\\n\\nBy Problem 10.14, $T(Z) \\\\subseteq Y$, and hence $S \\\\subseteq Y$. Now suppose $S$ is linearly dependent. Then there exists a relation\\n\\n$$\\na_{1} u_{1}+\\\\cdots+a_{r} u_{r}+b_{1} T\\\\left(w_{1}\\\\right)+\\\\cdots+b_{t} T\\\\left(w_{t}\\\\right)=0\\n$$\\n\\nwhere at least one coefficient is not zero. Furthermore, because $\\\\left\\\\{u_{i}\\\\right\\\\}$ is independent, at least one of the $b_{k}$ must be nonzero. Transposing, we find\\n\\nHence,\\n\\n$$\\nb_{1} T\\\\left(w_{1}\\\\right)+\\\\cdots+b_{t} T\\\\left(w_{t}\\\\right)=-a_{1} u_{1}-\\\\cdots-a_{r} u_{r} \\\\in X=\\\\operatorname{Ker} T^{i-2}\\n$$\\n\\nThus,\\n\\n$$\\nT^{i-2}\\\\left(b_{1} T\\\\left(w_{1}\\\\right)+\\\\cdots+b_{t} T\\\\left(w_{t}\\\\right)\\\\right)=0\\n$$\\n\\nBecause $\\\\left\\\\{u_{i}, v_{j}\\\\right\\\\}$ generates $Y$, we obtain a relation among the $u_{i}, v_{i}, w_{k}$ where one of the coefficients (i.e., one of the $b_{k}$ ) is not zero. This contradicts the fact that $\\\\left\\\\{u_{i}, v_{j}, w_{k}\\\\right\\\\}$ is independent. Hence, $S$ must also be independent.\\n',\n",
       " '\\n10.16. Prove Theorem 10.10: Let $T: V \\\\rightarrow V$ be a nilpotent operator of index $k$. Then $T$ has a unique block diagonal matrix representation consisting of Jordan nilpotent blocks $N$. There is at least one $N$ of order $k$, and all other $N$ are of orders $\\\\leq k$. The total number of $N$ of all orders is equal to the nullity of $T$.\\n\\nSuppose $\\\\operatorname{dim} V=n$. Let $W_{1}=\\\\operatorname{Ker} T, W_{2}=\\\\operatorname{Ker} T^{2}, \\\\ldots, W_{k}=\\\\operatorname{Ker} T^{k}$. Let us set $m_{i}=\\\\operatorname{dim} W_{i}$, for $i=1, \\\\ldots, k$. Because $T$ is of index $k, W_{k}=V$ and $W_{k-1} \\\\neq V$ and so $m_{k-1}<m_{k}=n$. By Problem 10.14,\\n\\n$$\\nW_{1} \\\\subseteq W_{2} \\\\subseteq \\\\cdots \\\\subseteq W_{k}=V\\n$$\\n\\nThus, by induction, we can choose a basis $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ of $V$ such that $\\\\left\\\\{u_{1}, \\\\ldots, u_{m_{i}}\\\\right\\\\}$ is a basis of $W_{i}$.\\n\\nWe now choose a new basis for $V$ with respect to which $T$ has the desired form. It will be convenient to label the members of this new basis by pairs of indices. We begin by setting\\n\\n$$\\nv(1, k)=u_{m_{k-1}+1}, \\\\quad v(2, k)=u_{m_{k-1}+2}, \\\\quad \\\\ldots, \\\\quad v\\\\left(m_{k}-m_{k-1}, k\\\\right)=u_{m_{k}}\\n$$\\n\\nand setting\\n\\n$$\\nv(1, k-1)=T v(1, k), \\\\quad v(2, k-1)=T v(2, k), \\\\quad \\\\ldots, \\\\quad v\\\\left(m_{k}-m_{k-1}, k-1\\\\right)=T v\\\\left(m_{k}-m_{k-1}, k\\\\right)\\n$$\\n\\nBy the preceding problem,\\n\\n$$\\nS_{1}=\\\\left\\\\{u_{1} \\\\ldots, u_{m_{k-2}}, v(1, k-1), \\\\ldots, v\\\\left(m_{k}-m_{k-1}, k-1\\\\right)\\\\right\\\\}\\n$$\\n\\nis a linearly independent subset of $W_{k-1}$. We extend $S_{1}$ to a basis of $W_{k-1}$ by adjoining new elements (if necessary), which we denote by\\n\\n$$\\nv\\\\left(m_{k}-m_{k-1}+1, k-1\\\\right), \\\\quad v\\\\left(m_{k}-m_{k-1}+2, k-1\\\\right), \\\\quad \\\\ldots, \\\\quad v\\\\left(m_{k-1}-m_{k-2}, k-1\\\\right)\\n$$\\n\\nNext we set\\n\\n$$\\n\\\\begin{array}{rr}\\nv(1, k-2)=T v(1, k-1), & v(2, k-2)=T v(2, k-1), \\\\\\\\\\nv\\\\left(m_{k-1}-m_{k-2}, k-2\\\\right)=T v\\\\left(m_{k-1}-m_{k-2}, k-1\\\\right)\\n\\\\end{array}\\n$$\\n\\nAgain by the preceding problem,\\n\\n$$\\nS_{2}=\\\\left\\\\{u_{1}, \\\\ldots, u_{m_{k-s}}, v(1, k-2), \\\\ldots, v\\\\left(m_{k-1}-m_{k-2}, k-2\\\\right)\\\\right\\\\}\\n$$\\n\\nis a linearly independent subset of $W_{k-2}$, which we can extend to a basis of $W_{k-2}$ by adjoining elements\\n\\n$$\\nv\\\\left(m_{k-1}-m_{k-2}+1, k-2\\\\right), \\\\quad v\\\\left(m_{k-1}-m_{k-2}+2, k-2\\\\right), \\\\quad \\\\ldots, \\\\quad v\\\\left(m_{k-2}-m_{k-3}, k-2\\\\right)\\n$$\\n\\nContinuing in this manner, we get a new basis for $V$, which for convenient reference we arrange as follows:\\n\\n$$\\n\\\\begin{aligned}\\n& v(1, k) \\\\quad \\\\ldots, v\\\\left(m_{k}-m_{k-1}, k\\\\right) \\\\\\\\\\n& v(1, k-1), \\\\quad \\\\ldots, v\\\\left(m_{k}-m_{k-1}, k-1\\\\right) \\\\quad \\\\ldots, v\\\\left(m_{k-1}-m_{k-2}, k-1\\\\right) \\\\\\\\\\n& v(1,2), \\\\quad \\\\ldots, v\\\\left(m_{k}-m_{k-1}, 2\\\\right), \\\\quad \\\\ldots, v\\\\left(m_{k-1}-m_{k-2}, 2\\\\right), \\\\quad \\\\ldots, v\\\\left(m_{2}-m_{1}, 2\\\\right) \\\\\\\\\\n& v(1,1), \\\\quad \\\\ldots, v\\\\left(m_{k}-m_{k-1}, 1\\\\right), \\\\quad \\\\ldots, v\\\\left(m_{k-1}-m_{k-2}, 1\\\\right), \\\\quad \\\\ldots, v\\\\left(m_{2}-m_{1}, 1\\\\right), \\\\quad \\\\ldots, v\\\\left(m_{1}, 1\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nThe bottom row forms a basis of $W_{1}$, the bottom two rows form a basis of $W_{2}$, and so forth. But what is important for us is that $T$ maps each vector into the vector immediately below it in the table or into 0 if the vector is in the bottom row. That is,\\n\\n$$\\nT v(i, j)= \\\\begin{cases}v(i, j-1) & \\\\text { for } j>1 \\\\\\\\ 0 & \\\\text { for } j=1\\\\end{cases}\\n$$\\n\\nNow it is clear [see Problem 10.13(d)] that $T$ will have the desired form if the $v(i, j)$ are ordered lexicographically: beginning with $v(1,1)$ and moving up the first column to $v(1, k)$, then jumping to $v(2,1)$ and moving up the second column as far as possible.\\n\\nMoreover, there will be exactly $m_{k}-m_{k-1}$ diagonal entries of order $k$. Also, there will be\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\left(m_{k-1}-m_{k-2}\\\\right)-\\\\left(m_{k}-m_{k-1}\\\\right)=2 m_{k-1}-m_{k}-m_{k-2} \\\\text { diagonal entries of order } k-1 \\\\\\\\\\n& 2 m_{2}-m_{1}-m_{3} \\\\quad \\\\text { diagonal entries of order } 2 \\\\\\\\\\n& 2 m_{1}-m_{2} \\\\quad \\\\text { diagonal entries of order } 1\\n\\\\end{aligned}\\n$$\\n\\nas can be read off directly from the table. In particular, because the numbers $m_{1}, \\\\ldots, m_{k}$ are uniquely determined by $T$, the number of diagonal entries of each order is uniquely determined by $T$. Finally, the identity\\n\\n$$\\nm_{1}=\\\\left(m_{k}-m_{k-1}\\\\right)+\\\\left(2 m_{k-1}-m_{k}-m_{k-2}\\\\right)+\\\\cdots+\\\\left(2 m_{2}-m_{1}-m_{3}\\\\right)+\\\\left(2 m_{1}-m_{2}\\\\right)\\n$$\\n\\nshows that the nullity $m_{1}$ of $T$ is the total number of diagonal entries of $T$.\\n',\n",
       " '\\n10.17. Let $A=\\\\left[\\\\begin{array}{lllll}0 & 1 & 1 & 0 & 1 \\\\\\\\ 0 & 0 & 1 & 1 & 1 \\\\\\\\ 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0\\\\end{array}\\\\right]$ and $B=\\\\left[\\\\begin{array}{lllll}0 & 1 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1 & 1 & 1 \\\\\\\\ 0 & 0 & 0 & 1 & 1 \\\\\\\\ 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0\\\\end{array}\\\\right]$. The reader can verify that $A$ and $B$ are both nilpotent of index 3 ; that is, $A^{3}=0$ but $A^{2} \\\\neq 0$, and $B^{3}=0$ but $B^{2} \\\\neq 0$. Find the nilpotent matrices $M_{A}$ and $M_{B}$ in canonical form that are similar to $A$ and $B$, respectively.\\n\\nBecause $A$ and $B$ are nilpotent of index $3, M_{A}$ and $M_{B}$ must each contain a Jordan nilpotent block of order 3 , and none greater then 3 . Note that $\\\\operatorname{rank}(A)=2 \\\\operatorname{and} \\\\operatorname{rank}(B)=3$, so nullity $(A)=5-2=3$ and nullity $(B)=5-3=2$. Thus, $M_{A}$ must contain three diagonal blocks, which must be one of order 3 and two of order 1; and $M_{B}$ must contain two diagonal blocks, which must be one of order 3 and one of order 2 . Namely,\\n\\n$$\\nM_{A}=\\\\left[\\\\begin{array}{lllll}\\n0 & 1 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad M_{B}=\\\\left[\\\\begin{array}{ccccc}\\n0 & 1 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 0 & 1 \\\\\\\\\\n0 & 0 & 0 & 0 & 0\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n10.18. Prove Theorem 10.11 on the Jordan canonical form for an operator $T$.\\n\\nBy the primary decomposition theorem, $T$ is decomposable into operators $T_{1}, \\\\ldots, T_{r}$; that is, $T=T_{1} \\\\oplus \\\\cdots \\\\oplus T_{r}$, where $\\\\left(t-\\\\lambda_{i}\\\\right)^{m_{i}}$ is the minimal polynomial of $T_{i}$. Thus, in particular,\\n\\n$$\\n\\\\left(T_{1}-\\\\lambda_{1} I\\\\right)^{m_{1}}=\\\\mathbf{0}, \\\\ldots, \\\\quad\\\\left(T_{r}-\\\\lambda_{r} I\\\\right)^{m_{r}}=\\\\mathbf{0}\\n$$\\n\\nSet $N_{i}=T_{i}-\\\\lambda_{i} I$. Then, for $i=1, \\\\ldots, r$,\\n\\n$$\\nT_{i}=N_{i}+\\\\lambda_{i} I, \\\\quad \\\\text { where } \\\\quad N_{i}^{m^{i}}=\\\\mathbf{0}\\n$$\\n\\nThat is, $T_{i}$ is the sum of the scalar operator $\\\\lambda_{i} I$ and a nilpotent operator $N_{i}$, which is of index $m_{i}$ because $\\\\left(t-\\\\lambda_{i}\\\\right)_{i}^{m}$ is the minimal polynomial of $T_{i}$.\\n\\nNow, by Theorem 10.10 on nilpotent operators, we can choose a basis so that $N_{i}$ is in canonical form. In this basis, $T_{i}=N_{i}+\\\\lambda_{i} I$ is represented by a block diagonal matrix $M_{i}$ whose diagonal entries are the matrices $J_{i j}$. The direct sum $J$ of the matrices $M_{i}$ is in Jordan canonical form and, by Theorem 10.5, is a matrix representation of $T$.\\n\\nLast, we must show that the blocks $J_{i j}$ satisfy the required properties. Property (i) follows from the fact that $N_{i}$ is of index $m_{i}$. Property (ii) is true because $T$ and $J$ have the same characteristic polynomial. Property (iii) is true because the nullity of $N_{i}=T_{i}-\\\\lambda_{i} I$ is equal to the geometric multiplicity of the eigenvalue $\\\\lambda_{i}$. Property (iv) follows from the fact that the $T_{i}$ and hence the $N_{i}$ are uniquely determined by $T$.\\n',\n",
       " '\\n10.19. Determine all possible Jordan canonical forms $J$ for a linear operator $T: V \\\\rightarrow V$ whose characteristic polynomial $\\\\Delta(t)=(t-2)^{5}$ and whose minimal polynomial $m(t)=(t-2)^{2}$.\\n\\n$J$ must be a $5 \\\\times 5$ matrix, because $\\\\Delta(t)$ has degree 5 , and all diagonal elements must be 2 , because 2 is the only eigenvalue. Moreover, because the exponent of $t-2$ in $m(t)$ is $2, J$ must have one Jordan block of order 2, and the others must be of order 2 or 1 . Thus, there are only two possibilities:\\n\\n$$\\nJ=\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{ll}\\n2 & 1 \\\\\\\\\\n& 2\\n\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}\\n2 & 1 \\\\\\\\\\n& 2\\n\\\\end{array}\\\\right],[2]\\\\right) \\\\quad \\\\text { or } \\\\quad J=\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{ll}\\n2 & 1 \\\\\\\\\\n& 2\\n\\\\end{array}\\\\right],[2],[2],[2]\\\\right)\\n$$\\n',\n",
       " '\\n10.20. Determine all possible Jordan canonical forms for a linear operator $T: V \\\\rightarrow V$ whose characteristic polynomial $\\\\Delta(t)=(t-2)^{3}(t-5)^{2}$. In each case, find the minimal polynomial $m(t)$.\\n\\nBecause $t-2$ has exponent 3 in $\\\\Delta(t), 2$ must appear three times on the diagonal. Similarly, 5 must appear twice. Thus, there are six possibilities:\\\\\\\\\\n(a) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{lll}2 & 1 & \\\\\\\\ & 2 & 1 \\\\\\\\ & & 2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{ll}5 & 1 \\\\\\\\ & 5\\\\end{array}\\\\right]\\\\right)$,\\\\\\\\\\n(b) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{lll}2 & 1 & \\\\\\\\ & 2 & 1 \\\\\\\\ & & 2\\\\end{array}\\\\right],[5],[5]\\\\right)$,\\\\\\\\\\n(c) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{ll}2 & 1 \\\\\\\\ & 2\\\\end{array}\\\\right],[2],\\\\left[\\\\begin{array}{ll}5 & 1 \\\\\\\\ & 5\\\\end{array}\\\\right]\\\\right)$,\\\\\\\\\\n(d) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{ll}2 & 1 \\\\\\\\ & 2\\\\end{array}\\\\right],[2],[5],[5]\\\\right)$,\\\\\\\\\\n(e) $\\\\operatorname{diag}\\\\left([2],[2],[2],\\\\left[\\\\begin{array}{ll}5 & 1 \\\\\\\\ & 5\\\\end{array}\\\\right]\\\\right)$,\\\\\\\\\\n(f) $\\\\operatorname{diag}([2],[2],[2],[5], \\\\quad[5])$\\n\\nThe exponent in the minimal polynomial $m(t)$ is equal to the size of the largest block. Thus,\\\\\\\\\\n(a) $m(t)=(t-2)^{3}(t-5)^{2}$,\\\\\\\\\\n(b) $m(t)=(t-2)^{3}(t-5)$,\\\\\\\\\\n(c) $m(t)=(t-2)^{2}(t-5)^{2}$,\\\\\\\\\\n(d) $m(t)=(t-2)^{2}(t-5)$,\\\\\\\\\\n(e) $m(t)=(t-2)(t-5)^{2}$\\\\\\\\\\n(f) $m(t)=(t-2)(t-5)$\\n\\n\\n\\\\section*{Quotient Space and Triangular Form}\\n',\n",
       " '10.21. Let $W$ be a subspace of a vector space $V$. Show that the following are equivalent:\\\\\\\\\\n(i) $u \\\\in v+W$,\\\\\\\\\\n(ii) $u-v \\\\in W$,\\\\\\\\\\n(iii) $v \\\\in u+W$.\\n\\nSuppose $u \\\\in v+W$. Then there exists $w_{0} \\\\in W$ such that $u=v+w_{0}$. Hence, $u-v=w_{0} \\\\in W$. Conversely, suppose $u-v \\\\in W$. Then $u-v=w_{0}$ where $w_{0} \\\\in W$. Hence, $u=v+w_{0} \\\\in v+W$. Thus, (i) and (ii) are equivalent.\\n\\nWe also have $u-v \\\\in W$ iff $-(u-v)=v-u \\\\in W$ iff $v \\\\in u+W$. Thus, (ii) and (iii) are also equivalent.\\n',\n",
       " '\\n10.22. Prove the following: The cosets of $W$ in $V$ partition $V$ into mutually disjoint sets. That is,\\n\\n(a) Any two cosets $u+W$ and $v+W$ are either identical or disjoint.\\n\\n(b) Each $v \\\\in V$ belongs to a coset; in fact, $v \\\\in v+W$.\\n\\nFurthermore, $u+W=v+W$ if and only if $u-v \\\\in W$, and so $(v+w)+W=v+W$ for any $w \\\\in W$.\\n\\nLet $v \\\\in V$. Because $0 \\\\in W$, we have $v=v+0 \\\\in v+W$, which proves (b).\\n\\nNow suppose the cosets $u+W$ and $v+W$ are not disjoint; say, the vector $x$ belongs to both $u+W$ and $v+W$. Then $u-x \\\\in W$ and $x-v \\\\in W$. The proof of (a) is complete if we show that $u+W=v+W$. Let $u+w_{0}$ be any element in the coset $u+W$. Because $u-x, x-v$, $w_{0}$ belongs to $W$,\\n\\n$$\\n\\\\left(u+w_{0}\\\\right)-v=(u-x)+(x-v)+w_{0} \\\\in W\\n$$\\n\\nThus, $u+w_{0} \\\\in v+W$, and hence the cost $u+W$ is contained in the coset $v+W$. Similarly, $v+W$ is contained in $u+W$, and so $u+W=v+W$.\\n\\nThe last statement follows from the fact that $u+W=v+W$ if and only if $u \\\\in v+W$, and, by Problem 10.21, this is equivalent to $u-v \\\\in W$.\\n',\n",
       " '\\n10.23. Let $W$ be the solution space of the homogeneous equation $2 x+3 y+4 z=0$. Describe the cosets of $W$ in $\\\\mathbf{R}^{3}$.\\n\\n$W$ is a plane through the origin $O=(0,0,0)$, and the cosets of $W$ are the planes parallel to $W$. Equivalently, the cosets of $W$ are the solution sets of the family of equations\\n\\n$$\\n2 x+3 y+4 z=k, \\\\quad k \\\\in \\\\mathbf{R}\\n$$\\n\\nIn fact, the coset $v+W$, where $v=(a, b, c)$, is the solution set of the linear equation\\n\\n$$\\n2 x+3 y+4 z=2 a+3 b+4 c \\\\quad \\\\text { or } \\\\quad 2(x-a)+3(y-b)+4(z-c)=0\\n$$\\n',\n",
       " '\\n10.24. Suppose $W$ is a subspace of a vector space $V$. Show that the operations in Theorem 10.15 are well defined; namely, show that if $u+W=u^{\\\\prime}+W$ and $v+W=v^{\\\\prime}+W$, then\\n\\n(a) $(u+v)+W=\\\\left(u^{\\\\prime}+v^{\\\\prime}\\\\right)+W \\\\quad$ and $\\\\quad$ (b) $k u+W=k u^{\\\\prime}+W \\\\quad$ for any $k \\\\in K$\\n\\n(a) Because $u+W=u^{\\\\prime}+W$ and $v+W=v^{\\\\prime}+W$, both $u-u^{\\\\prime}$ and $v-v^{\\\\prime}$ belong to $W$. But then $(u+v)-\\\\left(u^{\\\\prime}+v^{\\\\prime}\\\\right)=\\\\left(u-u^{\\\\prime}\\\\right)+\\\\left(v-v^{\\\\prime}\\\\right) \\\\in W$. Hence, $(u+v)+W=\\\\left(u^{\\\\prime}+v^{\\\\prime}\\\\right)+W$.\\n\\n(b) Also, because $u-u^{\\\\prime} \\\\in W$ implies $k\\\\left(u-u^{\\\\prime}\\\\right) \\\\in W$, then $k u-k u^{\\\\prime}=k\\\\left(u-u^{\\\\prime}\\\\right) \\\\in W$; accordingly, $k u+W=k u^{\\\\prime}+W$.\\n',\n",
       " '\\n10.25. Let $V$ be a vector space and $W$ a subspace of $V$. Show that the natural map $\\\\eta: V \\\\rightarrow V / W$, defined by $\\\\eta(v)=v+W$, is linear.\\n\\nFor any $u, v \\\\in V$ and any $k \\\\in K$, we have\\n\\nand\\n\\n$$\\nn(u+v)=u+v+W=u+W+v+W=\\\\eta(u)+\\\\eta(v)\\n$$\\n\\nAccordingly, $\\\\eta$ is linear.\\n',\n",
       " \"\\n10.26. Let $W$ be a subspace of a vector space $V$. Suppose $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}\\\\right\\\\}$ is a basis of $W$ and the set of cosets $\\\\left\\\\{\\\\bar{v}_{1}, \\\\ldots, \\\\bar{v}_{s}\\\\right\\\\}$, where $\\\\bar{v}_{j}=v_{j}+W$, is a basis of the quotient space. Show that the set of vectors $B=\\\\left\\\\{v_{1}, \\\\ldots, v_{s}, w_{1}, \\\\ldots, w_{r}\\\\right\\\\}$ is a basis of $V$. Thus, $\\\\operatorname{dim} V=\\\\operatorname{dim} W+\\\\operatorname{dim}(V / W)$.\\n\\nSuppose $u \\\\in V$. Because $\\\\left\\\\{\\\\bar{v}_{j}\\\\right\\\\}$ is a basis of $V / W$,\\n\\n$$\\n\\\\bar{u}=u+W=a_{1} \\\\bar{v}_{1}+a_{2} \\\\bar{v}_{2}+\\\\cdots+a_{s} \\\\bar{v}_{s}\\n$$\\n\\nHence, $u=a_{1} v_{1}+\\\\cdots+a_{s} v_{s}+w$, where $w \\\\in W$. Since $\\\\left\\\\{w_{i}\\\\right\\\\}$ is a basis of $W$,\\n\\n$$\\nu=a_{1} v_{1}+\\\\cdots+a_{s} v_{s}+b_{1} w_{1}+\\\\cdots+b_{r} w_{r}\\n$$\\n\\nAccordingly, $B$ spans $V$.\\n\\nWe now show that $B$ is linearly independent. Suppose\\n\\n\\n\\\\begin{gather*}\\n \\\\tag{1}\\\\\\\\\\n\\\\text { Then }\\n\\\\end{gather*} \\\\begin{gather*}\\nc_{1} v_{1}+\\\\cdots+c_{s} v_{s}+d_{1} w_{1}+\\\\cdots+d_{r} w_{r}=0 \\\\\\\\\\nc_{1} \\\\bar{v}_{1}+\\\\cdots+c_{s} \\\\bar{v}_{s}=\\\\overline{0}=W\\n\\\\end{gather*}\\n\\n\\n$$\\nc_{1} \\\\bar{v}_{1}+\\\\cdots+c_{s} \\\\bar{v}_{s}=\\\\overline{0}=W\\n$$\\n\\nBecause $\\\\left\\\\{\\\\bar{v}_{j}\\\\right\\\\}$ is independent, the $c$ 's are all 0 . Substituting into (1), we find $d_{1} w_{1}+\\\\cdots+d_{r} w_{r}=0$. Because $\\\\left\\\\{w_{i}\\\\right\\\\}$ is independent, the $d$ 's are all 0 . Thus, $B$ is linearly independent and therefore a basis of $V$.\\n\",\n",
       " '\\n10.27. Prove Theorem 10.16: Suppose $W$ is a subspace invariant under a linear operator $T: V \\\\rightarrow V$. Then $T$ induces a linear operator $\\\\bar{T}$ on $V / W$ defined by $\\\\bar{T}(v+W)=T(v)+W$. Moreover, if $T$ is a zero of any polynomial, then so is $\\\\bar{T}$. Thus, the minimal polynomial of $\\\\bar{T}$ divides the minimal polynomial of $T$.\\n\\nWe first show that $\\\\bar{T}$ is well defined; that is, if $u+W=v+W$, then $\\\\bar{T}(u+W)=\\\\bar{T}(v+W)$. If $u+W=v+W$, then $u-v \\\\in W$, and, as $W$ is $T$-invariant, $T(u-v)=T(u)-T(v) \\\\in W$. Accordingly,\\n\\n$$\\n\\\\bar{T}(u+W)=T(u)+W=T(v)+W=\\\\bar{T}(v+W)\\n$$\\n\\nas required.\\n\\nWe next show that $\\\\bar{T}$ is linear. We have\\n\\n$$\\n\\\\begin{aligned}\\n\\\\bar{T}((u+W)+(v+W)) & =\\\\bar{T}(u+v+W)=T(u+v)+W=T(u)+T(v)+W \\\\\\\\\\n& =T(u)+W+T(v)+W=\\\\bar{T}(u+W)+\\\\bar{T}(v+W)\\n\\\\end{aligned}\\n$$\\n\\nFurthermore,\\n\\n$$\\n\\\\bar{T}(k(u+W))=\\\\bar{T}(k u+W)=T(k u)+W=k T(u)+W=k(T(u)+W)=k \\\\hat{T}(u+W)\\n$$\\n\\nThus, $\\\\bar{T}$ is linear.\\n\\nNow, for any coset $u+W$ in $V / W$,\\n\\n$$\\n\\\\overline{T^{2}}(u+W)=T^{2}(u)+W=T(T(u))+W=\\\\bar{T}(T(u)+W)=\\\\bar{T}(\\\\bar{T}(u+W))=\\\\bar{T}^{2}(u+W)\\n$$\\n\\nHence, $\\\\overline{T^{2}}=\\\\bar{T}^{2}$. Similarly, $\\\\overline{T^{n}}=\\\\bar{T}^{n}$ for any $n$. Thus, for any polynomial\\n\\n$$\\n\\\\begin{gathered}\\nf(t)=a_{n} t^{n}+\\\\cdots+a_{0}=\\\\sum a_{i} t^{i} \\\\\\\\\\n\\\\overline{f(T)}(u+W)=f(T)(u)+W=\\\\sum a_{i} T^{i}(u)+W=\\\\sum a_{i}\\\\left(T^{i}(u)+W\\\\right) \\\\\\\\\\n=\\\\sum a_{i} \\\\overline{T^{i}}(u+W)=\\\\sum a_{i} \\\\bar{T}^{i}(u+W)=\\\\left(\\\\sum a_{i} \\\\bar{T}^{i}\\\\right)(u+W)=f(\\\\bar{T})(u+W)\\n\\\\end{gathered}\\n$$\\n\\nand so $\\\\overline{f(T)}=f(\\\\bar{T})$. Accordingly, if $T$ is a root of $f(t)$ then $\\\\overline{f(T)}=\\\\overline{\\\\mathbf{0}}=W=f(\\\\bar{T})$; that is, $\\\\bar{T}$ is also a root of $f(t)$. The theorem is proved.\\n',\n",
       " '\\n10.28. Prove Theorem 10.1: Let $T: V \\\\rightarrow V$ be a linear operator whose characteristic polynomial factors into linear polynomials. Then $V$ has a basis in which $T$ is represented by a triangular matrix.\\n\\nThe proof is by induction on the dimension of $V$. If $\\\\operatorname{dim} V=1$, then every matrix representation of $T$ is a $1 \\\\times 1$ matrix, which is triangular.\\n\\nNow suppose $\\\\operatorname{dim} V=n>1$ and that the theorem holds for spaces of dimension less than $n$. Because the characteristic polynomial of $T$ factors into linear polynomials, $T$ has at least one eigenvalue and so at least one nonzero eigenvector $v$, say $T(v)=a_{11} v$. Let $W$ be the one-dimensional subspace spanned by $v$. Set $\\\\bar{V}=V / W$. Then (Problem 10.26) $\\\\operatorname{dim} \\\\bar{V}=\\\\operatorname{dim} V-\\\\operatorname{dim} W=n-1$. Note also that $W$ is invariant under $T$. By Theorem 10.16, $T$ induces a linear operator $\\\\bar{T}$ on $\\\\bar{V}$ whose minimal polynomial divides the minimal polynomial of $T$. Because the characteristic polynomial of $T$ is a product of linear polynomials, so is its minimal polynomial, and hence, so are the minimal and characteristic polynomials of $\\\\bar{T}$. Thus, $\\\\bar{V}$ and $\\\\bar{T}$ satisfy the hypothesis of the theorem. Hence, by induction, there exists a basis $\\\\left\\\\{\\\\bar{v}_{2}, \\\\ldots, \\\\bar{v}_{n}\\\\right\\\\}$ of $\\\\bar{V}$ such that\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\bar{T}\\\\left(\\\\bar{v}_{2}\\\\right)=a_{22} \\\\bar{v}_{2} \\\\\\\\\\n& \\\\bar{T}\\\\left(\\\\bar{v}_{3}\\\\right)=a_{32} \\\\bar{v}_{2}+a_{33} \\\\bar{v}_{3} \\\\\\\\\\n& \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\n& \\\\bar{T}\\\\left(\\\\bar{v}_{n}\\\\right)=a_{n 2} \\\\bar{v}_{n}+a_{n 3} \\\\bar{v}_{3}+\\\\cdots+a_{n n}\\n\\\\end{aligned}\\n$$\\n\\nNow let $v_{2}, \\\\ldots, v_{n}$ be elements of $V$ that belong to the cosets $v_{2}, \\\\ldots, v_{n}$, respectively. Then $\\\\left\\\\{v, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ is a basis of $V$ (Problem 10.26). Because $\\\\bar{T}\\\\left(v_{2}\\\\right)=a_{22} \\\\bar{v}_{2}$, we have\\n\\n$$\\n\\\\bar{T}\\\\left(\\\\bar{v}_{2}\\\\right)-a_{22} \\\\bar{v}_{22}=0, \\\\quad \\\\text { and so } \\\\quad T\\\\left(v_{2}\\\\right)-a_{22} v_{2} \\\\in W\\n$$\\n\\nBut $W$ is spanned by $v$; hence, $T\\\\left(v_{2}\\\\right)-a_{22} v_{2}$ is a multiple of $v$, say,\\n\\n$$\\nT\\\\left(v_{2}\\\\right)-a_{22} v_{2}=a_{21} v, \\\\quad \\\\text { and so } \\\\quad T\\\\left(v_{2}\\\\right)=a_{21} v+a_{22} v_{2}\\n$$\\n\\nSimilarly, for $i=3, \\\\ldots, n$\\n\\n$$\\nT\\\\left(v_{i}\\\\right)-a_{i 2} v_{2}-a_{i 3} v_{3}-\\\\cdots-a_{i i} v_{i} \\\\in W, \\\\quad \\\\text { and so } \\\\quad T\\\\left(v_{i}\\\\right)=a_{i 1} v+a_{i 2} v_{2}+\\\\cdots+a_{i i} v_{i}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\begin{aligned}\\n& T(v)=a_{11} v \\\\\\\\\\n& T\\\\left(v_{2}\\\\right)=a_{21} v+a_{22} v_{2} \\\\\\\\\\n& \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\cdots \\\\\\\\\\n& T\\\\left(v_{n}\\\\right)=a_{n 1} v+a_{n 2} v_{2}+\\\\cdots+a_{n n} v_{n}\\n\\\\end{aligned}\\n$$\\n\\nand hence the matrix of $T$ in this basis is triangular.\\n\\n\\n\\\\section*{Cyclic Subspaces, Rational Canonical Form}\\n',\n",
       " \"10.29. Prove Theorem 10.12: Let $Z(v, T)$ be a $T$-cyclic subspace, $T_{v}$ the restriction of $T$ to $Z(v, T)$, and $m_{v}(t)=t^{k}+a_{k-1} t^{k-1}+\\\\cdots+a_{0}$ the $T$-annihilator of $v$. Then,\\n\\n(i) The set $\\\\left\\\\{v, T(v), \\\\ldots, T^{k-1}(v)\\\\right\\\\}$ is a basis of $Z(v, T)$; hence, $\\\\operatorname{dim} Z(v, T)=k$.\\n\\n(ii) The minimal polynomial of $T_{v}$ is $m_{v}(t)$.\\n\\n(iii) The matrix of $T_{v}$ in the above basis is the companion matrix $C=C\\\\left(m_{v}\\\\right)$ of $m_{v}(t)$ [which has 1's below the diagonal, the negative of the coefficients $a_{0}, a_{1}, \\\\ldots, a_{k-1}$ of $m_{v}(t)$ in the last column, and 0's elsewhere].\\n\\n(i) By definition of $m_{v}(t), T^{k}(v)$ is the first vector in the sequence $v, T(v), T^{2}(v), \\\\ldots$ that, is a linear combination of those vectors that precede it in the sequence; hence, the set $B=\\\\left\\\\{v, T(v), \\\\ldots, T^{k-1}(v)\\\\right\\\\}$ is linearly independent. We now only have to show that $Z(v, T)=L(B)$, the linear span of $B$. By the above, $T^{k}(v) \\\\in L(B)$. We prove by induction that $T^{n}(v) \\\\in L(B)$ for every $n$. Suppose $n>k$ and $T^{n-1}(v) \\\\in L(B)$ - that is, $\\\\quad T^{n-1}(v) \\\\quad$ is $\\\\quad$ a linear combination of $\\\\quad v, \\\\ldots, T^{k-1}(v)$. Then $T^{n}(v)=T\\\\left(T^{n-1}(v)\\\\right)$ is a linear combination of $T(v), \\\\ldots, T^{k}(v)$. But $T^{k}(v) \\\\in L(B)$; hence, $T^{n}(v) \\\\in L(B)$ for every $n$. Consequently, $f(T)(v) \\\\in L(B)$ for any polynomial $f(t)$. Thus, $Z(v, T)=L(B)$, and so $B$ is a basis, as claimed.\\n\\n(ii) Suppose $m(t)=t^{s}+b_{s-1} t^{s-1}+\\\\cdots+b_{0}$ is the minimal polynomial of $T_{v}$. Then, because $v \\\\in Z(v, T)$,\\n\\n$$\\n0=m\\\\left(T_{v}\\\\right)(v)=m(T)(v)=T^{s}(v)+b_{s-1} T^{s-1}(v)+\\\\cdots+b_{0} v\\n$$\\n\\nThus, $T^{s}(v)$ is a linear combination of $v, T(v), \\\\ldots, T^{s-1}(v)$, and therefore $k \\\\leq s$. However, $m_{v}(T)=\\\\mathbf{0}$ and so $m_{v}\\\\left(T_{v}\\\\right)=\\\\mathbf{0}$. Then $m(t)$ divides $m_{v}(t)$, and so $s \\\\leq k$. Accordingly, $k=s$ and hence $m_{v}(t)=m(t)$.\\n\\n(iii)\\n\\n$$\\n\\\\begin{array}{llc}\\nT_{v}(v) & = & T(v) \\\\\\\\\\nT_{v}(T(v)) & = & T^{2}(v) \\\\\\\\\\n\\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\ldots \\\\\\\\\\nT_{v}\\\\left(T^{k-2}(v)\\\\right) & = & T^{k-1}(v) \\\\\\\\\\nT_{v}\\\\left(T^{k-1}(v)\\\\right) & = & T^{k}(v)=-a_{0} v-a_{1} T(v)-a_{2} T^{2}(v)-\\\\cdots-a_{k-1} T^{k-1}(v)\\n\\\\end{array}\\n$$\\n\\nBy definition, the matrix of $T_{v}$ in this basis is the tranpose of the matrix of coefficients of the above system of equations; hence, it is $C$, as required.\\n\",\n",
       " '\\n10.30. Let $T: V \\\\rightarrow V$ be linear. Let $W$ be a $T$-invariant subspace of $V$ and $\\\\bar{T}$ the induced operator on $V / W$. Prove\\n\\n(a) The T-annihilator of $v \\\\in V$ divides the minimal polynomial of $T$.\\n\\n(b) The $\\\\bar{T}$-annihilator of $\\\\bar{v} \\\\in V / W$ divides the minimal polynomial of $T$.\\\\\\\\\\n(a) The $T$-annihilator of $v \\\\in V$ is the minimal polynomial of the restriction of $T$ to $Z(v, T)$; therefore, by Problem 10.6, it divides the minimal polynomial of $T$.\\n\\n(b) The $\\\\bar{T}$-annihilator of $\\\\bar{v} \\\\in V / W$ divides the minimal polynomial of $\\\\bar{T}$, which divides the minimal polynomial of $T$ by Theorem 10.16 .\\n\\nRemark: In the case where the minimum polynomial of $T$ is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial, then the $T$-annihilator of $v \\\\in V$ and the $\\\\bar{T}$-annihilator of $\\\\bar{v} \\\\in V / W$ are of the form $f(t)^{m}$, where $m \\\\leq n$.\\n',\n",
       " '\\n10.31. Prove Lemma 10.13: Let $T: V \\\\rightarrow V$ be a linear operator whose minimal polynomial is $f(t)^{n}$, where $f(t)$ is a monic irreducible polynomial. Then $V$ is the direct sum of $T$-cyclic subspaces $Z_{i}=Z\\\\left(v_{i}, T\\\\right), i=1, \\\\ldots, r$, with corresponding $T$-annihilators\\n\\n$$\\nf(t)^{n_{1}}, f(t)^{n_{2}}, \\\\ldots, f(t)^{n_{r}}, \\\\quad n=n_{1} \\\\geq n_{2} \\\\geq \\\\cdots \\\\geq n_{r}\\n$$\\n\\nAny other decomposition of $V$ into the direct sum of $T$-cyclic subspaces has the same number of components and the same set of $T$-annihilators.\\n\\nThe proof is by induction on the dimension of $V$. If $\\\\operatorname{dim} V=1$, then $V$ is $T$-cyclic and the lemma holds. Now suppose $\\\\operatorname{dim} V>1$ and that the lemma holds for those vector spaces of dimension less than that of $V$.\\n\\nBecause the minimal polynomial of $T$ is $f(t)^{n}$, there exists $v_{1} \\\\in V$ such that $f(T)^{n-1}\\\\left(v_{1}\\\\right) \\\\neq 0$; hence, the $T$-annihilator of $v_{1}$ is $f(t)^{n}$. Let $Z_{1}=Z\\\\left(v_{1}, T\\\\right)$ and recall that $Z_{1}$ is $T$-invariant. Let $\\\\bar{V}=V / Z_{1}$ and let $\\\\bar{T}$ be the linear operator on $\\\\bar{V}$ induced by $T$. By Theorem 10.16, the minimal polynomial of $\\\\bar{T}$ divides $f(t)^{n}$; hence, the hypothesis holds for $\\\\bar{V}$ and $\\\\bar{T}$. Consequently, by induction, $\\\\bar{V}$ is the direct sum of $\\\\bar{T}$-cyclic subspaces; say,\\n\\n$$\\n\\\\bar{V}=Z\\\\left(\\\\bar{v}_{2}, \\\\bar{T}\\\\right) \\\\oplus \\\\cdots \\\\oplus Z\\\\left(\\\\bar{v}_{r}, \\\\bar{T}\\\\right)\\n$$\\n\\nwhere the corresponding $\\\\bar{T}$-annihilators are $f(t)^{n_{2}}, \\\\ldots, f(t)^{n_{r}}, n \\\\geq n_{2} \\\\geq \\\\cdots \\\\geq n_{r}$.\\n\\nWe claim that there is a vector $v_{2}$ in the coset $\\\\bar{v}_{2}$ whose $T$-annihilator is $f(t)^{n_{2}}$, the $\\\\bar{T}$-annihilator of $\\\\bar{v}_{2}$. Let $w$ be any vector in $\\\\bar{v}_{2}$. Then $f(T)^{n_{2}}(w) \\\\in Z_{1}$. Hence, there exists a polynomial $g(t)$ for which\\n\\n\\n\\\\begin{equation*}\\nf(T)^{n_{2}}(w)=g(T)\\\\left(v_{1}\\\\right) \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause $f(t)^{n}$ is the minimal polynomial of $T$, we have, by (1),\\n\\n$$\\n0=f(T)^{n}(w)=f(T)^{n-n_{2}} g(T)\\\\left(v_{1}\\\\right)\\n$$\\n\\nBut $f(t)^{n}$ is the $T$-annihilator of $v_{1}$; hence, $f(t)^{n}$ divides $f(t)^{n-n_{2}} g(t)$, and so $g(t)=f(t)^{n_{2}} h(t)$ for some polynomial $h(t)$. We set\\n\\n$$\\nv_{2}=w-h(T)\\\\left(v_{1}\\\\right)\\n$$\\n\\nBecause $w-v_{2}=h(T)\\\\left(v_{1}\\\\right) \\\\in Z_{1}, v_{2}$ also belongs to the coset $\\\\bar{v}_{2}$. Thus, the $T$-annihilator of $v_{2}$ is a multiple of the $\\\\bar{T}$-annihilator of $\\\\bar{v}_{2}$. On the other hand, by (1),\\n\\n$$\\nf(T)^{n_{2}}\\\\left(v_{2}\\\\right)=f(T)^{n_{s}}\\\\left(w-h(T)\\\\left(v_{1}\\\\right)\\\\right)=f(T)^{n_{2}}(w)-g(T)\\\\left(v_{1}\\\\right)=0\\n$$\\n\\nConsequently, the $T$-annihilator of $v_{2}$ is $f(t)^{n_{2}}$, as claimed.\\n\\nSimilarly, there exist vectors $v_{3}, \\\\ldots, v_{r} \\\\in V$ such that $v_{i} \\\\in \\\\overline{v_{i}}$ and that the $T$-annihilator of $v_{i}$ is $f(t)^{n_{i}}$, the $\\\\bar{T}$-annihilator of $\\\\overline{v_{i}}$. We set\\n\\n$$\\nZ_{2}=Z\\\\left(v_{2}, T\\\\right), \\\\quad \\\\ldots, \\\\quad Z_{r}=Z\\\\left(v_{r}, T\\\\right)\\n$$\\n\\nLet $d$ denote the degree of $f(t)$, so that $f(t)^{n_{i}}$ has degree $d n_{i}$. Then, because $f(t)^{n_{i}}$ is both the $T$-annihilator of $v_{i}$ and the $\\\\bar{T}$-annihilator of $\\\\bar{v}_{i}$, we know that\\n\\n$$\\n\\\\left\\\\{v_{i}, T\\\\left(v_{i}\\\\right), \\\\ldots, T^{d n_{i}-1}\\\\left(v_{i}\\\\right)\\\\right\\\\} \\\\quad \\\\text { and } \\\\quad\\\\left\\\\{\\\\bar{v}_{i} \\\\cdot \\\\bar{T}\\\\left(\\\\bar{v}_{i}\\\\right), \\\\ldots, \\\\bar{T}^{d n_{i}-1}\\\\left(\\\\bar{v}_{i}\\\\right)\\\\right\\\\}\\n$$\\n\\nare bases for $Z\\\\left(v_{i}, T\\\\right)$ and $Z\\\\left(\\\\overline{v_{i}}, \\\\bar{T}\\\\right)$, respectively, for $i=2, \\\\ldots, r$. But $\\\\bar{V}=Z\\\\left(\\\\overline{v_{2}}, \\\\bar{T}\\\\right) \\\\oplus \\\\cdots \\\\oplus Z\\\\left(\\\\overline{v_{r}}, \\\\bar{T}\\\\right)$; hence,\\n\\n$$\\n\\\\left\\\\{\\\\bar{v}_{2}, \\\\ldots, \\\\bar{T}^{d n_{2}-1}\\\\left(\\\\bar{v}_{2}\\\\right), \\\\ldots, \\\\bar{v}_{r}, \\\\ldots, \\\\bar{T}^{d n_{r}-1}\\\\left(\\\\bar{v}_{r}\\\\right)\\\\right\\\\}\\n$$\\n\\nis a basis for $\\\\bar{V}$. Therefore, by Problem 10.26 and the relation $\\\\bar{T}^{i}(\\\\bar{v})=\\\\overline{T^{i}(v)}$ (see Problem 10.27),\\n\\n$$\\n\\\\left\\\\{v_{1}, \\\\ldots, T^{d n_{1}-1}\\\\left(v_{1}\\\\right), v_{2}, \\\\ldots, T^{e n_{2}-1}\\\\left(v_{2}\\\\right), \\\\ldots, v_{r}, \\\\ldots, T^{d n_{r}-1}\\\\left(v_{r}\\\\right)\\\\right\\\\}\\n$$\\n\\nis a basis for $V$. Thus, by Theorem 10.4, $V=Z\\\\left(v_{1}, T\\\\right) \\\\oplus \\\\cdots \\\\oplus Z\\\\left(v_{r}, T\\\\right)$, as required.\\n\\nIt remains to show that the exponents $n_{1}, \\\\ldots, n_{r}$ are uniquely determined by $T$. Because $d=$ degree of $f(t)$,\\n\\n$$\\n\\\\operatorname{dim} V=d\\\\left(n_{1}+\\\\cdots+n_{r}\\\\right) \\\\quad \\\\text { and } \\\\quad \\\\operatorname{dim} Z_{i}=d n_{i}, \\\\quad i=1, \\\\ldots, r\\n$$\\n\\nAlso, if $s$ is any positive integer, then (Problem 10.59) $f(T)^{s}\\\\left(Z_{i}\\\\right)$ is a cyclic subspace generated by $f(T)^{s}\\\\left(v_{i}\\\\right)$, and it has dimension $d\\\\left(n_{i}-s\\\\right)$ if $n_{i}>s$ and dimension 0 if $n_{i} \\\\leq s$.\\n\\nNow any vector $v \\\\in V$ can be written uniquely in the form $v=w_{1}+\\\\cdots+w_{r}$, where $w_{i} \\\\in Z_{i}$. Hence, any vector in $f(T)^{s}(V)$ can be written uniquely in the form\\n\\n$$\\nf(T)^{s}(v)=f(T)^{s}\\\\left(w_{1}\\\\right)+\\\\cdots+f(T)^{s}\\\\left(w_{r}\\\\right)\\n$$\\n\\nwhere $f(T)^{s}\\\\left(w_{i}\\\\right) \\\\in f(T)^{s}\\\\left(Z_{i}\\\\right)$. Let $t$ be the integer, dependent on $s$, for which\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{lc}\\n & $n_{1}>s, \\\\quad \\\\cdots, \\\\quad n_{t}>s, \\\\quad n_{t+1} \\\\geq s$ \\\\\\\\\\nThen & $f(T)^{s}(V)=f(T)^{s}\\\\left(Z_{1}\\\\right) \\\\oplus \\\\cdots \\\\oplus f(T)^{s}\\\\left(Z_{t}\\\\right)$ \\\\\\\\\\nand so & $\\\\operatorname{dim}\\\\left[f(T)^{s}(V)\\\\right]=d\\\\left[\\\\left(n_{1}-s\\\\right)+\\\\cdots+\\\\left(n_{t}-s\\\\right)\\\\right]$ \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{center}\\n\\nThe numbers on the left of (2) are uniquely determined by $T$. Set $s=n-1$, and (2) determines the number of $n_{i}$ equal to $n$. Next set $s=n-2$, and (2) determines the number of $n_{i}$ (if any) equal to $n-1$. We repeat the process until we set $s=0$ and determine the number of $n_{i}$ equal to 1 . Thus, the $n_{i}$ are uniquely determined by $T$ and $V$, and the lemma is proved.\\n',\n",
       " '\\n10.32. Let $V$ be a seven-dimensional vector space over $\\\\mathbf{R}$, and let $T: V \\\\rightarrow V$ be a linear operator with minimal polynomial $m(t)=\\\\left(t^{2}-2 t+5\\\\right)(t-3)^{3}$. Find all possible rational canonical forms $M$ of $T$.\\n\\nBecause $\\\\operatorname{dim} V=7$, there are only two possible characteristic polynomials, $\\\\Delta_{1}(t)=\\\\left(t^{2}-2 t+5\\\\right)^{2}$ $(t-3)^{3}$ or $\\\\Delta_{1}(t)=\\\\left(t^{2}-2 t+5\\\\right)(t-3)^{5}$. Moreover, the sum of the orders of the companion matrices must add up to 7. Also, one companion matrix must be $C\\\\left(t^{2}-2 t+5\\\\right)$ and one must be $C\\\\left((t-3)^{3}\\\\right)=$ $C\\\\left(t^{3}-9 t^{2}+27 t-27\\\\right)$. Thus, $M$ must be one of the following block diagonal matrices:\\n\\n(a) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{rr}0 & -5 \\\\\\\\ 1 & 2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rr}0 & -5 \\\\\\\\ 1 & 2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rrr}0 & 0 & 27 \\\\\\\\ 1 & 0 & -27 \\\\\\\\ 0 & 1 & 9\\\\end{array}\\\\right]\\\\right)$,\\n\\n(b) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{rr}0 & -5 \\\\\\\\ 1 & 2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rrr}0 & 0 & 27 \\\\\\\\ 1 & 0 & -27 \\\\\\\\ 0 & 1 & 9\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rr}0 & -9 \\\\\\\\ 1 & 6\\\\end{array}\\\\right]\\\\right)$,\\n\\n(c) $\\\\operatorname{diag}\\\\left(\\\\left[\\\\begin{array}{rr}0 & -5 \\\\\\\\ 1 & 2\\\\end{array}\\\\right],\\\\left[\\\\begin{array}{rrr}0 & 0 & 27 \\\\\\\\ 1 & 0 & -27 \\\\\\\\ 0 & 1 & 9\\\\end{array}\\\\right],[3],[3]\\\\right)$\\n\\n\\n\\\\section*{Projections}\\n',\n",
       " '10.33. Suppose $V=W_{1} \\\\oplus \\\\cdots \\\\oplus W_{r}$. The projection of $V$ into its subspace $W_{k}$ is the mapping $E: V \\\\rightarrow V$ defined by $E(v)=w_{k}$, where $v=w_{1}+\\\\cdots+w_{r}, w_{i} \\\\in W_{i}$. Show that (a) $E$ is linear, (b) $E^{2}=E$.\\n\\n(a) Because the sum $v=w_{1}+\\\\cdots+w_{r}, w_{i} \\\\in W$ is uniquely determined by $v$, the mapping $E$ is well defined. Suppose, for $u \\\\in V, u=w_{1}^{\\\\prime}+\\\\cdots+w_{r}^{\\\\prime}$, $w_{i}^{\\\\prime} \\\\in W_{i}$. Then\\n\\n$$\\nv+u=\\\\left(w_{1}+w_{1}^{\\\\prime}\\\\right)+\\\\cdots+\\\\left(w_{r}+w_{r}^{\\\\prime}\\\\right) \\\\quad \\\\text { and } \\\\quad k v=k w_{1}+\\\\cdots+k w_{r}, \\\\quad k w_{i}, w_{i}+w_{i}^{\\\\prime} \\\\in W_{i}\\n$$\\n\\nare the unique sums corresponding to $v+u$ and $k v$. Hence,\\n\\n$$\\nE(v+u)=w_{k}+w_{k}^{\\\\prime}=E(v)+E(u) \\\\quad \\\\text { and } \\\\quad E(k v)=k w_{k}+k E(v)\\n$$\\n\\nand therefore $E$ is linear.\\\\\\\\\\n(b) We have that\\n\\n$$\\nw_{k}=0+\\\\cdots+0+w_{k}+0+\\\\cdots+0\\n$$\\n\\nis the unique sum corresponding to $w_{k} \\\\in W_{k}$; hence, $E\\\\left(w_{k}\\\\right)=w_{k}$. Then, for any $v \\\\in V$,\\n\\n$$\\nE^{2}(v)=E(E(v))=E\\\\left(w_{k}\\\\right)=w_{k}=E(v)\\n$$\\n\\nThus, $E^{2}=E$, as required.\\n',\n",
       " '\\n10.34. Suppose $E: V \\\\rightarrow V$ is linear and $E^{2}=E$. Show that (a) $E(u)=u$ for any $u \\\\in \\\\operatorname{Im} E$ (i.e., the restriction of $E$ to its image is the identity mapping); (b) $V$ is the direct sum of the image and kernel of $E: V=\\\\operatorname{Im} E \\\\oplus \\\\operatorname{Ker} E$; (c) $E$ is the projection of $V$ into $\\\\operatorname{Im} E$, its image. Thus, by the preceding problem, a linear mapping $T: V \\\\rightarrow V$ is a projection if and only if $T^{2}=T$; this characterization of a projection is frequently used as its definition.\\n\\n(a) If $u \\\\in \\\\operatorname{Im} E$, then there exists $v \\\\in V$ for which $E(v)=u$; hence, as required,\\n\\n$$\\nE(u)=E(E(v))=E^{2}(v)=E(v)=u\\n$$\\n\\n(b) Let $v \\\\in V$. We can write $v$ in the form $v=E(v)+v-E(v)$. Now $E(v) \\\\in \\\\operatorname{Im} E$ and, because\\n\\n$$\\nE(v-E(v))=E(v)-E^{2}(v)=E(v)-E(v)=0\\n$$\\n\\n$v-E(v) \\\\in \\\\operatorname{Ker} E$. Accordingly, $V=\\\\operatorname{Im} E+\\\\operatorname{Ker} E$.\\n\\nNow suppose $w \\\\in \\\\operatorname{Im} E \\\\cap \\\\operatorname{Ker} E$. By (i), $E(w)=w$ because $w \\\\in \\\\operatorname{Im} E$. On the other hand, $E(w)=0$ because $w \\\\in \\\\operatorname{Ker} E$. Thus, $w=0$, and so $\\\\operatorname{Im} E \\\\cap \\\\operatorname{Ker} E=\\\\{0\\\\}$. These two conditions imply that $V$ is the direct sum of the image and kernel of $E$.\\n\\n(c) Let $v \\\\in V$ and suppose $v=u+w$, where $u \\\\in \\\\operatorname{Im} E$ and $w \\\\in \\\\operatorname{Ker} E$. Note that $E(u)=u$ by (i), and $E(w)=0$ because $w \\\\in \\\\operatorname{Ker} E$. Hence,\\n\\n$$\\nE(v)=E(u+w)=E(u)+E(w)=u+0=u\\n$$\\n\\nThat is, $E$ is the projection of $V$ into its image.\\n',\n",
       " '\\n10.35. Suppose $V=U \\\\oplus W$ and suppose $T: V \\\\rightarrow V$ is linear. Show that $U$ and $W$ are both $T$-invariant if and only if $T E=E T$, where $E$ is the projection of $V$ into $U$.\\n\\nObserve that $E(v) \\\\in U$ for every $v \\\\in V$, and that (i) $E(v)=v$ iff $v \\\\in U$, (ii) $E(v)=0$ iff $v \\\\in W$.\\n\\nSuppose $E T=T E$. Let $u \\\\in U$. Because $E(u)=u$,\\n\\n$$\\nT(u)=T(E(u))=(T E)(u)=(E T)(u)=E(T(u)) \\\\in U\\n$$\\n\\nHence, $U$ is $T$-invariant. Now let $w \\\\in W$. Because $E(w)=0$,\\n\\n$$\\nE(T(w))=(E T)(w)=(T E)(w)=T(E(w))=T(0)=0, \\\\quad \\\\text { and so } \\\\quad T(w) \\\\in W\\n$$\\n\\nHence, $W$ is also $T$-invariant.\\n\\nConversely, suppose $U$ and $W$ are both $T$-invariant. Let $v \\\\in V$ and suppose $v=u+w$, where $u \\\\in T$ and $w \\\\in W$. Then $T(u) \\\\in U$ and $T(w) \\\\in W$; hence, $E(T(u))=T(u)$ and $E(T(w))=0$. Thus,\\n\\nand\\n\\n$$\\n(E T)(v)=(E T)(u+w)=(E T)(u)+(E T)(w)=E(T(u))+E(T(w))=T(u)\\n$$\\n\\n$$\\n(T E)(v)=(T E)(u+w)=T(E(u+w))=T(u)\\n$$\\n\\nThat is, $(E T)(v)=(T E)(v)$ for every $v \\\\in V$; therefore, $E T=T E$, as required.\\n\\n',\n",
       " '11.1. Find the basis $\\\\left\\\\{\\\\phi_{1}, \\\\phi_{2}, \\\\phi_{3}\\\\right\\\\}$ that is dual to the following basis of $\\\\mathbf{R}^{3}$ :\\n\\n$$\\n\\\\left\\\\{v_{1}=(1,-1,3), \\\\quad v_{2}=(0,1,-1), \\\\quad v_{3}=(0,3,-2)\\\\right\\\\}\\n$$\\n\\nThe linear functionals may be expressed in the form\\n\\n$$\\n\\\\phi_{1}(x, y, z)=a_{1} x+a_{2} y+a_{3} z, \\\\quad \\\\phi_{2}(x, y, z)=b_{1} x+b_{2} y+b_{3} z, \\\\quad \\\\phi_{3}(x, y, z)=c_{1} x+c_{2} y+c_{3} z\\n$$\\n\\nBy definition of the dual basis, $\\\\phi_{i}\\\\left(v_{j}\\\\right)=0$ for $i \\\\neq j$, but $\\\\phi_{i}\\\\left(v_{j}\\\\right)=1$ for $i=j$.\\n\\nWe find $\\\\phi_{1}$ by setting $\\\\phi_{1}\\\\left(v_{1}\\\\right)=1, \\\\phi_{1}\\\\left(v_{2}\\\\right)=0, \\\\phi_{1}\\\\left(v_{3}\\\\right)=0$. This yields\\n\\n$$\\n\\\\phi_{1}(1,-1,3)=a_{1}-a_{2}+3 a_{3}=1, \\\\quad \\\\phi_{1}(0,1,-1)=a_{2}-a_{3}=0, \\\\quad \\\\phi_{1}(0,3,-2)=3 a_{2}-2 a_{3}=0\\n$$\\n\\nSolving the system of equations yields $a_{1}=1, a_{2}=0, a_{3}=0$. Thus, $\\\\phi_{1}(x, y, z)=x$.\\n\\nWe find $\\\\phi_{2}$ by setting $\\\\phi_{2}\\\\left(v_{1}\\\\right)=0, \\\\phi_{2}\\\\left(v_{2}\\\\right)=1, \\\\phi_{2}\\\\left(v_{3}\\\\right)=0$. This yields\\n\\n$$\\n\\\\phi_{2}(1,-1,3)=b_{1}-b_{2}+3 b_{3}=0, \\\\quad \\\\phi_{2}(0,1,-1)=b_{2}-b_{3}=1, \\\\quad \\\\phi_{2}(0,3,-2)=3 b_{2}-2 b_{3}=0\\n$$\\n\\nSolving the system of equations yields $b_{1}=7, b_{2}=-2, a_{3}=-3$. Thus, $\\\\phi_{2}(x, y, z)=7 x-2 y-3 z$.\\n\\nWe find $\\\\phi_{3}$ by setting $\\\\phi_{3}\\\\left(v_{1}\\\\right)=0, \\\\phi_{3}\\\\left(v_{2}\\\\right)=0, \\\\phi_{3}\\\\left(v_{3}\\\\right)=1$. This yields\\n\\n$$\\n\\\\phi_{3}(1,-1,3)=c_{1}-c_{2}+3 c_{3}=0, \\\\quad \\\\phi_{3}(0,1,-1)=c_{2}-c_{3}=0, \\\\quad \\\\phi_{3}(0,3,-2)=3 c_{2}-2 c_{3}=1\\n$$\\n\\nSolving the system of equations yields $c_{1}=-2, c_{2}=1, c_{3}=1$. Thus, $\\\\phi_{3}(x, y, z)=-2 x+y+z$.\\n',\n",
       " '\\n11.2. Let $V=\\\\{a+b t: a, b \\\\in \\\\mathbf{R}\\\\}$, the vector space of real polynomials of degree $\\\\leq 1$. Find the basis $\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}$ of $V$ that is dual to the basis $\\\\left\\\\{\\\\phi_{1}, \\\\phi_{2}\\\\right\\\\}$ of $V^{*}$ defined by\\n\\n$$\\n\\\\phi_{1}(f(t))=\\\\int_{0}^{1} f(t) d t \\\\quad \\\\text { and } \\\\quad \\\\phi_{2}(f(t))=\\\\int_{0}^{2} f(t) d t\\n$$\\n\\nLet $v_{1}=a+b t$ and $v_{2}=c+d t$. By definition of the dual basis,\\n\\n$$\\n\\\\phi_{1}\\\\left(v_{1}\\\\right)=1, \\\\quad \\\\phi_{1}\\\\left(v_{2}\\\\right)=0 \\\\quad \\\\text { and } \\\\quad \\\\phi_{2}\\\\left(v_{1}\\\\right)=0, \\\\quad \\\\phi_{i}\\\\left(v_{j}\\\\right)=1\\n$$\\n\\nThus,\\n\\n$$\\n\\\\left.\\\\left.\\\\begin{array}{l}\\n\\\\phi_{1}\\\\left(v_{1}\\\\right)=\\\\int_{0}^{1}(a+b t) d t=a+\\\\frac{1}{2} b=1 \\\\\\\\\\n\\\\phi_{2}\\\\left(v_{1}\\\\right)=\\\\int_{0}^{2}(a+b t) d t=2 a+2 b=0\\n\\\\end{array}\\\\right\\\\} \\\\quad \\\\text { and } \\\\quad \\\\begin{array}{c}\\n\\\\phi_{1}\\\\left(v_{2}\\\\right)=\\\\int_{0}^{1}(c+d t) d t=c+\\\\frac{1}{2} d=0 \\\\\\\\\\n\\\\phi_{2}\\\\left(v_{2}\\\\right)=\\\\int_{0}^{2}(c+d t) d t=2 c+2 d=1\\n\\\\end{array}\\\\right\\\\}\\n$$\\n\\nSolving each system yields $a=2, \\\\quad b=-2$ and $c=-\\\\frac{1}{2}, \\\\quad d=1$. Thus, $\\\\left\\\\{v_{1}=2-2 t, \\\\quad v_{2}=-\\\\frac{1}{2}+t\\\\right\\\\}$ is the basis of $V$ that is dual to $\\\\left\\\\{\\\\phi_{1}, \\\\phi_{2}\\\\right\\\\}$.\\n',\n",
       " '\\n11.3. Prove Theorem 11.1: Suppose $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ is a basis of $V$ over $K$. Let $\\\\phi_{1}, \\\\ldots, \\\\phi_{n} \\\\in V^{*}$ be defined by $\\\\phi_{i}\\\\left(v_{j}\\\\right)=0$ for $i \\\\neq j$, but $\\\\phi_{i}\\\\left(v_{j}\\\\right)=1$ for $i=j$. Then $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ is a basis of $V^{*}$.\\n\\nWe first show that $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ spans $V^{*}$. Let $\\\\phi$ be an arbitrary element of $V^{*}$, and suppose\\n\\n$$\\n\\\\phi\\\\left(v_{1}\\\\right)=k_{1}, \\\\quad \\\\phi\\\\left(v_{2}\\\\right)=k_{2}, \\\\quad \\\\ldots, \\\\quad \\\\phi\\\\left(v_{n}\\\\right)=k_{n}\\n$$\\n\\nSet $\\\\sigma=k_{1} \\\\phi_{1}+\\\\cdots+k_{n} \\\\phi_{n}$. Then\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sigma\\\\left(v_{1}\\\\right) & =\\\\left(k_{1} \\\\phi_{1}+\\\\cdots+k_{n} \\\\phi_{n}\\\\right)\\\\left(v_{1}\\\\right)=k_{1} \\\\phi_{1}\\\\left(v_{1}\\\\right)+k_{2} \\\\phi_{2}\\\\left(v_{1}\\\\right)+\\\\cdots+k_{n} \\\\phi_{n}\\\\left(v_{1}\\\\right) \\\\\\\\\\n& =k_{1} \\\\cdot 1+k_{2} \\\\cdot 0+\\\\cdots+k_{n} \\\\cdot 0=k_{1}\\n\\\\end{aligned}\\n$$\\n\\nSimilarly, for $i=2, \\\\ldots, n$,\\n\\n$$\\n\\\\sigma\\\\left(v_{i}\\\\right)=\\\\left(k_{1} \\\\phi_{1}+\\\\cdots+k_{n} \\\\phi_{n}\\\\right)\\\\left(v_{i}\\\\right)=k_{1} \\\\phi_{1}\\\\left(v_{i}\\\\right)+\\\\cdots+k_{i} \\\\phi_{i}\\\\left(v_{i}\\\\right)+\\\\cdots+k_{n} \\\\phi_{n}\\\\left(v_{i}\\\\right)=k_{i}\\n$$\\n\\nThus, $\\\\phi\\\\left(v_{i}\\\\right)=\\\\sigma\\\\left(v_{i}\\\\right)$ for $i=1, \\\\ldots, n$. Because $\\\\phi$ and $\\\\sigma$ agree on the basis vectors, $\\\\phi=\\\\sigma=k_{1} \\\\phi_{1}+\\\\cdots+k_{n} \\\\phi_{n}$. Accordingly, $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ spans $V^{*}$.\\n\\nIt remains to be shown that $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ is linearly independent. Suppose\\n\\n$$\\na_{1} \\\\phi_{1}+a_{2} \\\\phi_{2}+\\\\cdots+a_{n} \\\\phi_{n}=\\\\mathbf{0}\\n$$\\n\\nApplying both sides to $v_{1}$, we obtain\\n\\n$$\\n\\\\begin{aligned}\\n0 & =\\\\mathbf{0}\\\\left(v_{1}\\\\right)=\\\\left(a_{1} \\\\phi_{1}+\\\\cdots+a_{n} \\\\phi_{n}\\\\right)\\\\left(v_{1}\\\\right)=a_{1} \\\\phi_{1}\\\\left(v_{1}\\\\right)+a_{2} \\\\phi_{2}\\\\left(v_{1}\\\\right)+\\\\cdots+a_{n} \\\\phi_{n}\\\\left(v_{1}\\\\right) \\\\\\\\\\n& =a_{1} \\\\cdot 1+a_{2} \\\\cdot 0+\\\\cdots+a_{n} \\\\cdot 0=a_{1}\\n\\\\end{aligned}\\n$$\\n\\nSimilarly, for $i=2, \\\\ldots, n$,\\n\\n$$\\n0=\\\\mathbf{0}\\\\left(v_{i}\\\\right)=\\\\left(a_{1} \\\\phi_{1}+\\\\cdots+a_{n} \\\\phi_{n}\\\\right)\\\\left(v_{i}\\\\right)=a_{1} \\\\phi_{1}\\\\left(v_{i}\\\\right)+\\\\cdots+a_{i} \\\\phi_{i}\\\\left(v_{i}\\\\right)+\\\\cdots+a_{n} \\\\phi_{n}\\\\left(v_{i}\\\\right)=a_{i}\\n$$\\n\\nThat is, $a_{1}=0, \\\\ldots, a_{n}=0$. Hence, $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ is linearly independent, and so it is a basis of $V^{*}$.\\n',\n",
       " '\\n11.4. Prove Theorem 11.2: Let $\\\\left\\\\{v_{1}, \\\\ldots, v_{n}\\\\right\\\\}$ be a basis of $V$ and let $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ be the dual basis in $V^{*}$. For any $u \\\\in V$ and any $\\\\sigma \\\\in V^{*}$, (i) $u=\\\\sum_{i} \\\\phi_{i}(u) v_{i}$. (ii) $\\\\sigma=\\\\sum_{i} \\\\phi\\\\left(v_{i}\\\\right) \\\\phi_{i}$.\\n\\nSuppose\\n\\n\\n\\\\begin{equation*}\\nu=a_{1} v_{1}+a_{2} v_{2}+\\\\cdots+a_{n} v_{n} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nThen\\n\\n$$\\n\\\\phi_{1}(u)=a_{1} \\\\phi_{1}\\\\left(v_{1}\\\\right)+a_{2} \\\\phi_{1}\\\\left(v_{2}\\\\right)+\\\\cdots+a_{n} \\\\phi_{1}\\\\left(v_{n}\\\\right)=a_{1} \\\\cdot 1+a_{2} \\\\cdot 0+\\\\cdots+a_{n} \\\\cdot 0=a_{1}\\n$$\\n\\nSimilarly, for $i=2, \\\\ldots, n$,\\n\\n$$\\n\\\\phi_{i}(u)=a_{1} \\\\phi_{i}\\\\left(v_{1}\\\\right)+\\\\cdots+a_{i} \\\\phi_{i}\\\\left(v_{i}\\\\right)+\\\\cdots+a_{n} \\\\phi_{i}\\\\left(v_{n}\\\\right)=a_{i}\\n$$\\n\\nThat is, $\\\\phi_{1}(u)=a_{1}, \\\\phi_{2}(u)=a_{2}, \\\\ldots, \\\\phi_{n}(u)=a_{n}$. Substituting these results into (1), we obtain (i).\\n\\nNext we prove (ii). Applying the linear functional $\\\\sigma$ to both sides of (i),\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sigma(u) & =\\\\phi_{1}(u) \\\\sigma\\\\left(v_{1}\\\\right)+\\\\phi_{2}(u) \\\\sigma\\\\left(v_{2}\\\\right)+\\\\cdots+\\\\phi_{n}(u) \\\\sigma\\\\left(v_{n}\\\\right) \\\\\\\\\\n& =\\\\sigma\\\\left(v_{1}\\\\right) \\\\phi_{1}(u)+\\\\sigma\\\\left(v_{2}\\\\right) \\\\phi_{2}(u)+\\\\cdots+\\\\sigma\\\\left(v_{n}\\\\right) \\\\phi_{n}(u) \\\\\\\\\\n& =\\\\left(\\\\sigma\\\\left(v_{1}\\\\right) \\\\phi_{1}+\\\\sigma\\\\left(v_{2}\\\\right) \\\\phi_{2}+\\\\cdots+\\\\sigma\\\\left(v_{n}\\\\right) \\\\phi_{n}\\\\right)(u)\\n\\\\end{aligned}\\n$$\\n\\nBecause the above holds for every $u \\\\in V, \\\\sigma=\\\\sigma\\\\left(v_{1}\\\\right) \\\\phi_{2}+\\\\sigma\\\\left(v_{2}\\\\right) \\\\phi_{2}+\\\\cdots+\\\\sigma\\\\left(v_{n}\\\\right) \\\\phi_{n}$, as claimed.\\n',\n",
       " '\\n11.5. Prove Theorem 11.3. Let $\\\\left\\\\{v_{i}\\\\right\\\\}$ and $\\\\left\\\\{w_{i}\\\\right\\\\}$ be bases of $V$ and let $\\\\left\\\\{\\\\phi_{i}\\\\right\\\\}$ and $\\\\left\\\\{\\\\sigma_{i}\\\\right\\\\}$ be the respective dual bases in $V^{*}$. Let $P$ be the change-of-basis matrix from $\\\\left\\\\{v_{i}\\\\right\\\\}$ to $\\\\left\\\\{w_{i}\\\\right\\\\}$. Then $\\\\left(P^{-1}\\\\right)^{T}$ is the change-of-basis matrix from $\\\\left\\\\{\\\\phi_{i}\\\\right\\\\}$ to $\\\\left\\\\{\\\\sigma_{i}\\\\right\\\\}$.\\n\\nSuppose, for $i=1, \\\\ldots, n$,\\n\\n$$\\nw_{i}=a_{i 1} v_{1}+a_{i 2} v_{2}+\\\\cdots+a_{i n} v_{n} \\\\quad \\\\text { and } \\\\quad \\\\sigma_{i}=b_{i 1} \\\\phi_{1}+b_{i 2} \\\\phi_{2}+\\\\cdots+a_{i n} v_{n}\\n$$\\n\\nThen $P=\\\\left[a_{i j}\\\\right]$ and $Q=\\\\left[b_{i j}\\\\right]$. We seek to prove that $Q=\\\\left(P^{-1}\\\\right)^{T}$.\\n\\nLet $R_{i}$ denote the $i$ th row of $Q$ and let $C_{j}$ denote the $j$ th column of $P^{T}$. Then\\n\\n$$\\nR_{i}=\\\\left(b_{i 1}, b_{i 2}, \\\\ldots, b_{i n}\\\\right) \\\\quad \\\\text { and } \\\\quad C_{j}=\\\\left(a_{j 1}, a_{j 2}, \\\\ldots, a_{j n}\\\\right)^{T}\\n$$\\n\\nBy definition of the dual basis,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sigma_{i}\\\\left(w_{j}\\\\right) & =\\\\left(b_{i 1} \\\\phi_{1}+b_{i 2} \\\\phi_{2}+\\\\cdots+b_{i n} \\\\phi_{n}\\\\right)\\\\left(a_{j 1} v_{1}+a_{j 2} v_{2}+\\\\cdots+a_{j n} v_{n}\\\\right) \\\\\\\\\\n& =b_{i 1} a_{j 1}+b_{i 2} a_{j 2}+\\\\cdots+b_{i n} a_{j n}=R_{i} C_{j}=\\\\delta_{i j}\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\delta_{i j}$ is the Kronecker delta. Thus,\\n\\n$$\\nQ P^{T}=\\\\left[R_{i} C_{j}\\\\right]=\\\\left[\\\\delta_{i j}\\\\right]=I\\n$$\\n\\nTherefore, $Q=\\\\left(P^{T}\\\\right)^{-1}=\\\\left(P^{-1}\\\\right)^{T}$, as claimed.\\n',\n",
       " '\\n11.6. Suppose $v \\\\in V, v \\\\neq 0$, and $\\\\operatorname{dim} V=n$. Show that there exists $\\\\phi \\\\in V^{*}$ such that $\\\\phi(v) \\\\neq 0$.\\n\\nWe extend $\\\\{v\\\\}$ to a basis $\\\\left\\\\{v, v_{2}, \\\\ldots, v_{n}\\\\right\\\\}$ of $V$. By Theorem 5.2, there exists a unique linear mapping $\\\\phi: V \\\\rightarrow K$ such that $\\\\phi(v)=1$ and $\\\\phi\\\\left(v_{i}\\\\right)=0, i=2, \\\\ldots, n$. Hence, $\\\\phi$ has the desired property.\\n',\n",
       " '\\n11.7. Prove Theorem 11.4: Suppose $\\\\operatorname{dim} V=n$. Then the natural mapping $v \\\\mapsto \\\\hat{v}$ is an isomorphism of $V$ onto $V^{* *}$.\\n\\nWe first prove that the map $v \\\\mapsto \\\\hat{v}$ is linear-that is, for any vectors $v, w \\\\in V$ and any scalars $a, b \\\\in K$, $\\\\widehat{a v+b w}=a \\\\hat{v}+b \\\\hat{w}$. For any linear functional $\\\\phi \\\\in V^{*}$,\\n\\n$$\\n\\\\widehat{a v+b w}(\\\\phi)=\\\\phi(a v+b w)=a \\\\phi(v)+b \\\\phi(w)=a \\\\hat{v}(\\\\phi)+b \\\\hat{w}(\\\\phi)=(a \\\\hat{v}+b \\\\hat{w})(\\\\phi)\\n$$\\n\\nBecause $\\\\widehat{a v+b w}(\\\\phi)=(a \\\\hat{v}+b \\\\hat{w})(\\\\phi)$ for every $\\\\phi \\\\in V^{*}$, we have $\\\\widehat{a v+b w}=a \\\\hat{v}+b \\\\hat{w}$. Thus, the map $v \\\\mapsto \\\\hat{v}$ is linear.\\n\\nNow suppose $v \\\\in V, v \\\\neq 0$. Then, by Problem 11.6, there exists $\\\\phi \\\\in V^{*}$ for which $\\\\phi(v) \\\\neq 0$. Hence, $\\\\hat{v}(\\\\phi)=\\\\phi(v) \\\\neq 0$, and thus $\\\\hat{v} \\\\neq 0$. Because $v \\\\neq 0$ implies $\\\\hat{v} \\\\neq 0$, the map $v \\\\mapsto \\\\hat{v}$ is nonsingular and hence an isomorphism (Theorem 5.64).\\n\\nNow $\\\\operatorname{dim} V=\\\\operatorname{dim} V^{*}=\\\\operatorname{dim} V^{* *}$, because $V$ has finite dimension. Accordingly, the mapping $v \\\\mapsto \\\\hat{v}$ is an isomorphism of $V$ onto $V^{* *}$.\\n\\n\\n\\\\section*{Annihilators}\\n',\n",
       " '11.8. Show that if $\\\\phi \\\\in V^{*}$ annihilates a subset $S$ of $V$, then $\\\\phi$ annihilates the linear span $L(S)$ of $S$. Hence, $S^{0}=[\\\\operatorname{span}(S)]^{0}$.\\n\\nSuppose $v \\\\in \\\\operatorname{span}(S)$. Then there exists $w_{1}, \\\\ldots, w_{r} \\\\in S$ for which $v=a_{1} w_{1}+a_{2} w_{2}+\\\\cdots+a_{r} w_{r}$.\\n\\n$$\\n\\\\phi(v)=a_{1} \\\\phi\\\\left(w_{1}\\\\right)+a_{2} \\\\phi\\\\left(w_{2}\\\\right)+\\\\cdots+a_{r} \\\\phi\\\\left(w_{r}\\\\right)=a_{1} 0+a_{2} 0+\\\\cdots+a_{r} 0=0\\n$$\\n\\nBecause $v$ was an arbitrary element of $\\\\operatorname{span}(S), \\\\phi$ annihilates $\\\\operatorname{span}(S)$, as claimed.\\n',\n",
       " '\\n11.9. Find a basis of the annihilator $W^{0}$ of the subspace $W$ of $\\\\mathbf{R}^{4}$ spanned by\\n\\n$$\\nv_{1}=(1,2,-3,4) \\\\quad \\\\text { and } \\\\quad v_{2}=(0,1,4,-1)\\n$$\\n\\nBy Problem 11.8, it suffices to find a basis of the set of linear functionals $\\\\phi$ such that $\\\\phi\\\\left(v_{1}\\\\right)=0$ and $\\\\phi\\\\left(v_{2}\\\\right)=0$, where $\\\\phi\\\\left(x_{1}, x_{2}, x_{3}, x_{4}\\\\right)=a x_{1}+b x_{2}+c x_{3}+d x_{4}$. Thus,\\n\\n$$\\n\\\\phi(1,2,-3,4)=a+2 b-3 c+4 d=0 \\\\quad \\\\text { and } \\\\quad \\\\phi(0,1,4,-1)=b+4 c-d=0\\n$$\\n\\nThe system of two equations in the unknowns $a, b, c, d$ is in echelon form with free variables $c$ and $d$.\\n\\n(1) Set $c=1, d=0$ to obtain the solution $a=11, b=-4, c=1, d=0$.\\n\\n(2) Set $c=0, d=1$ to obtain the solution $a=6, b=-1, c=0, d=1$.\\n\\nThe linear functions $\\\\phi_{1}\\\\left(x_{i}\\\\right)=11 x_{1}-4 x_{2}+x_{3}$ and $\\\\phi_{2}\\\\left(x_{i}\\\\right)=6 x_{1}-x_{2}+x_{4}$ form a basis of $W^{0}$.\\n',\n",
       " '\\n11.10. Show that (a) For any subset $S$ of $V, S \\\\subseteq S^{00}$. (b) If $S_{1} \\\\subseteq S_{2}$, then $S_{2}^{0} \\\\subseteq S_{1}^{0}$.\\n\\n(a) Let $v \\\\in S$. Then for every linear functional $\\\\phi \\\\in S^{0}, \\\\hat{v}(\\\\phi)=\\\\phi(v)=0$. Hence, $\\\\hat{v} \\\\in\\\\left(S^{0}\\\\right)^{0}$. Therefore, under the identification of $V$ and $V^{* *}, v \\\\in S^{00}$. Accordingly, $S \\\\subseteq S^{00}$.\\n\\n(b) Let $\\\\phi \\\\in S_{2}^{0}$. Then $\\\\phi(v)=0$ for every $v \\\\in S_{2}$. But $S_{1} \\\\subseteq S_{2}$; hence, $\\\\phi$ annihilates every element of $S_{1}$ (i.e., $\\\\phi \\\\in S_{1}^{0}$ ). Therefore, $S_{2}^{0} \\\\subseteq S_{1}^{0}$.\\n',\n",
       " \"\\n11.11. Prove Theorem 11.5: Suppose $V$ has finite dimension and $W$ is a subspace of $V$. Then\\n\\n(i) $\\\\operatorname{dim} W+\\\\operatorname{dim} W^{0}=\\\\operatorname{dim} V$, (ii) $W^{00}=W$.\\n\\n(i) Suppose $\\\\operatorname{dim} V=n$ and $\\\\operatorname{dim} W=r \\\\leq n$. We want to show that $\\\\operatorname{dim} W^{0}=n-r$. We choose a basis $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}\\\\right\\\\}$ of $W$ and extend it to a basis of $V$, say $\\\\left\\\\{w_{1}, \\\\ldots, w_{r}, v_{1}, \\\\ldots, v_{n-r}\\\\right\\\\}$. Consider the dual basis\\n\\n$$\\n\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{r}, \\\\sigma_{1}, \\\\ldots, \\\\sigma_{n-r}\\\\right\\\\}\\n$$\\n\\nBy definition of the dual basis, each of the above $\\\\sigma$ 's annihilates each $w_{i}$; hence, $\\\\sigma_{1}, \\\\ldots, \\\\sigma_{n-r} \\\\in W^{0}$. We claim that $\\\\left\\\\{\\\\sigma_{i}\\\\right\\\\}$ is a basis of $W^{0}$. Now $\\\\left\\\\{\\\\sigma_{j}\\\\right\\\\}$ is part of a basis of $V^{*}$, and so it is linearly independent.\\n\\nWe next show that $\\\\left\\\\{\\\\phi_{j}\\\\right\\\\}$ spans $W^{0}$. Let $\\\\sigma \\\\in W^{0}$. By Theorem 11.2,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sigma & =\\\\sigma\\\\left(w_{1}\\\\right) \\\\phi_{1}+\\\\cdots+\\\\sigma\\\\left(w_{r}\\\\right) \\\\phi_{r}+\\\\sigma\\\\left(v_{1}\\\\right) \\\\sigma_{1}+\\\\cdots+\\\\sigma\\\\left(v_{n-r}\\\\right) \\\\sigma_{n-r} \\\\\\\\\\n& =0 \\\\phi_{1}+\\\\cdots+0 \\\\phi_{r}+\\\\sigma\\\\left(v_{1}\\\\right) \\\\sigma_{1}+\\\\cdots+\\\\sigma\\\\left(v_{n-r}\\\\right) \\\\sigma_{n-r} \\\\\\\\\\n& =\\\\sigma\\\\left(v_{1}\\\\right) \\\\sigma_{1}+\\\\cdots+\\\\sigma\\\\left(v_{n-r}\\\\right) \\\\sigma_{n-r}\\n\\\\end{aligned}\\n$$\\n\\nConsequently, $\\\\left\\\\{\\\\sigma_{1}, \\\\ldots, \\\\sigma_{n-r}\\\\right\\\\}$ spans $W^{0}$ and so it is a basis of $W^{0}$. Accordingly, as required $\\\\operatorname{dim} W^{0}=n-r=\\\\operatorname{dim} V-\\\\operatorname{dim} W$.\\n\\n(ii) Suppose $\\\\operatorname{dim} V=n$ and $\\\\operatorname{dim} W=r$. Then $\\\\operatorname{dim} V^{*}=n$ and, by (i), $\\\\operatorname{dim} W^{0}=n-r$. Thus, by (i), $\\\\operatorname{dim} W^{00}=n-(n-r)=r$; therefore, $\\\\operatorname{dim} W=\\\\operatorname{dim} W^{00}$. By Problem 11.10, $W \\\\subseteq W^{00}$. Accordingly, $W=W^{00}$.\\n\",\n",
       " '\\n11.12. Let $U$ and $W$ be subspaces of $V$. Prove that $(U+W)^{0}=U^{0} \\\\cap W^{0}$.\\n\\nLet $\\\\phi \\\\in(U+W)^{0}$. Then $\\\\phi$ annihilates $U+W$, and so, in particular, $\\\\phi$ annihilates $U$ and $W$. That is, $\\\\phi \\\\in U^{0}$ and $\\\\phi \\\\in W^{0}$; hence, $\\\\phi \\\\in U^{0} \\\\cap W^{0}$. Thus, $(U+W)^{0} \\\\subseteq U^{0} \\\\cap W^{0}$.\\n\\nOn the other hand, suppose $\\\\sigma \\\\in U^{0} \\\\cap W^{0}$. Then $\\\\sigma$ annihilates $U$ and also $W$. If $v \\\\in U+W$, then $v=u+w$, where $u \\\\in U$ and $w \\\\in W$. Hence, $\\\\sigma(v)=\\\\sigma(u)+\\\\sigma(w)=0+0=0$. Thus, $\\\\sigma$ annihilates $U+W$; that is, $\\\\sigma \\\\in(U+W)^{0}$. Accordingly, $U^{0}+W^{0} \\\\subseteq(U+W)^{0}$.\\n\\nThe two inclusion relations together give us the desired equality.\\n\\nRemark: Observe that no dimension argument is employed in the proof; hence, the result holds for spaces of finite or infinite dimension.\\n\\n\\n\\\\section*{Transpose of a Linear Mapping}\\n',\n",
       " '11.13. Let $\\\\phi$ be the linear functional on $\\\\mathbf{R}^{2}$ defined by $\\\\phi(x, y)=x-2 y$. For each of the following linear operators $T$ on $\\\\mathbf{R}^{2}$, find $\\\\left(T^{t}(\\\\phi)\\\\right)(x, y)$ :\\\\\\\\\\n(a) $T(x, y)=(x, 0)$,\\\\\\\\\\n(b) $T(x, y)=(y, x+y)$,\\\\\\\\\\n(c) $T(x, y)=(2 x-3 y, 5 x+2 y)$\\n\\nBy definition, $T^{t}(\\\\phi)=\\\\phi \\\\circ T$; that is, $\\\\left(T^{t}(\\\\phi)\\\\right)(v)=\\\\phi(T(v))$ for every $v$. Hence,\\n\\n(a) $\\\\left(T^{t}(\\\\phi)\\\\right)(x, y)=\\\\phi(T(x, y))=\\\\phi(x, 0)=x$\\n\\n(b) $\\\\left(T^{t}(\\\\phi)\\\\right)(x, y)=\\\\phi(T(x, y))=\\\\phi(y, x+y)=y-2(x+y)=-2 x-y$\\n\\n(c) $\\\\left(T^{t}(\\\\phi)\\\\right)(x, y)=\\\\phi(T(x, y))=\\\\phi(2 x-3 y, 5 x+2 y)=(2 x-3 y)-2(5 x+2 y)=-8 x-7 y$\\n',\n",
       " '\\n11.14. Let $T: V \\\\rightarrow U$ be linear and let $T^{t}: U^{*} \\\\rightarrow V^{*}$ be its transpose. Show that the kernel of $T^{t}$ is the annihilator of the image of $T$-that is, $\\\\operatorname{Ker} T^{t}=(\\\\operatorname{Im} T)^{0}$.\\n\\nSuppose $\\\\phi \\\\in \\\\operatorname{Ker} T^{t}$; that is, $T^{t}(\\\\phi)=\\\\phi \\\\circ T=0$. If $u \\\\in \\\\operatorname{Im} T$, then $u=T(v)$ for some $v \\\\in V$; hence,\\n\\n$$\\n\\\\phi(u)=\\\\phi(T(v))=(\\\\phi \\\\circ T)(v)=\\\\mathbf{0}(v)=0\\n$$\\n\\nWe have that $\\\\phi(u)=0$ for every $u \\\\in \\\\operatorname{Im} T$; hence, $\\\\phi \\\\in(\\\\operatorname{Im} T)^{0}$. Thus, $\\\\operatorname{Ker} T^{t} \\\\subseteq(\\\\operatorname{Im} T)^{0}$.\\n\\nOn the other hand, suppose $\\\\sigma \\\\in(\\\\operatorname{Im} T)^{0}$; that is, $\\\\sigma(\\\\operatorname{Im} T)=\\\\{0\\\\}$. Then, for every $v \\\\in V$,\\n\\n$$\\n\\\\left(T^{t}(\\\\sigma)\\\\right)(v)=(\\\\sigma \\\\circ T)(v)=\\\\sigma(T(v))=0=\\\\mathbf{0}(v)\\n$$\\n\\nWe have $\\\\left(T^{t}(\\\\sigma)\\\\right)(v)=\\\\mathbf{0}(v)$ for every $v \\\\in V$; hence, $T^{t}(\\\\sigma)=0$. Thus, $\\\\sigma \\\\in \\\\operatorname{Ker} T^{t}$, and so $(\\\\operatorname{Im} T)^{0} \\\\subseteq \\\\operatorname{Ker} T^{t}$.\\n\\nThe two inclusion relations together give us the required equality.\\n',\n",
       " '\\n11.15. Suppose $V$ and $U$ have finite dimension and $T: V \\\\rightarrow U$ is linear. Prove $\\\\operatorname{rank}(T)=\\\\operatorname{rank}\\\\left(T^{t}\\\\right)$.\\n\\nSuppose $\\\\operatorname{dim} V=n$ and $\\\\operatorname{dim} U=m$, and suppose $\\\\operatorname{rank}(T)=r$. By Theorem 11.5,\\n\\n$$\\n\\\\operatorname{dim}(\\\\operatorname{Im} T)^{0}=\\\\operatorname{dim} u-\\\\operatorname{dim}(\\\\operatorname{Im} T)=m-\\\\operatorname{rank}(T)=m-r\\n$$\\n\\nBy Problem 11.14, $\\\\operatorname{Ker} T^{t}=(\\\\operatorname{Im} T)^{0}$. Hence, nullity $\\\\left(T^{t}\\\\right)=m-r$. It then follows that, as claimed,\\n\\n$$\\n\\\\operatorname{rank}\\\\left(T^{t}\\\\right)=\\\\operatorname{dim} U^{*}-\\\\operatorname{nullity}\\\\left(T^{t}\\\\right)=m-(m-r)=r=\\\\operatorname{rank}(T)\\n$$\\n',\n",
       " '\\n11.16. Prove Theorem 11.7: Let $T: V \\\\rightarrow U$ be linear and let $A$ be the matrix representation of $T$ in the bases $\\\\left\\\\{v_{j}\\\\right\\\\}$ of $V$ and $\\\\left\\\\{u_{i}\\\\right\\\\}$ of $U$. Then the transpose matrix $A^{T}$ is the matrix representation of $T^{t}: U^{*} \\\\rightarrow V^{*}$ in the bases dual to $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{v_{j}\\\\right\\\\}$.\\n\\nSuppose, for $j=1, \\\\ldots, m$,\\n\\n\\n\\\\begin{equation*}\\nT\\\\left(v_{j}\\\\right)=a_{j 1} u_{1}+a_{j 2} u_{2}+\\\\cdots+a_{j n} u_{n} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nWe want to prove that, for $i=1, \\\\ldots, n$,\\n\\n\\n\\\\begin{equation*}\\nT^{t}\\\\left(\\\\sigma_{i}\\\\right)=a_{1 i} \\\\phi_{1}+a_{2 i} \\\\phi_{2}+\\\\cdots+a_{m i} \\\\phi_{m} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nwhere $\\\\left\\\\{\\\\sigma_{i}\\\\right\\\\}$ and $\\\\left\\\\{\\\\phi_{j}\\\\right\\\\}$ are the bases dual to $\\\\left\\\\{u_{i}\\\\right\\\\}$ and $\\\\left\\\\{v_{j}\\\\right\\\\}$, respectively.\\n\\nLet $v \\\\in V$ and suppose $v=k_{1} v_{1}+k_{2} v_{2}+\\\\cdots+k_{m} v_{m}$. Then, by (1),\\n\\n$$\\n\\\\begin{aligned}\\nT(v) & =k_{1} T\\\\left(v_{1}\\\\right)+k_{2} T\\\\left(v_{2}\\\\right)+\\\\cdots+k_{m} T\\\\left(v_{m}\\\\right) \\\\\\\\\\n& =k_{1}\\\\left(a_{11} u_{1}+\\\\cdots+a_{1 n} u_{n}\\\\right)+k_{2}\\\\left(a_{21} u_{1}+\\\\cdots+a_{2 n} u_{n}\\\\right)+\\\\cdots+k_{m}\\\\left(a_{m 1} u_{1}+\\\\cdots+a_{m n} u_{n}\\\\right) \\\\\\\\\\n& =\\\\left(k_{1} a_{11}+k_{2} a_{21}+\\\\cdots+k_{m} a_{m 1}\\\\right) u_{1}+\\\\cdots+\\\\left(k_{1} a_{1 n}+k_{2} a_{2 n}+\\\\cdots+k_{m} a_{m n}\\\\right) u_{n} \\\\\\\\\\n& =\\\\sum_{i=1}^{n}\\\\left(k_{1} a_{1 i}+k_{2} a_{2 i}+\\\\cdots+k_{m} a_{m i}\\\\right) u_{i}\\n\\\\end{aligned}\\n$$\\n\\nHence, for $j=1, \\\\ldots, n$.\\n\\n\\n\\\\begin{align*}\\n\\\\left(T^{t}\\\\left(\\\\sigma_{j}\\\\right)(v)\\\\right) & =\\\\sigma_{j}(T(v))=\\\\sigma_{j}\\\\left(\\\\sum_{i=1}^{n}\\\\left(k_{1} a_{1 i}+k_{2} a_{2 i}+\\\\cdots+k_{m} a_{m i}\\\\right) u_{i}\\\\right) \\\\\\\\\\n& =k_{1} a_{1 j}+k_{2} a_{2 j}+\\\\cdots+k_{m} a_{m j} \\\\tag{3}\\n\\\\end{align*}\\n\\n\\nOn the other hand, for $j=1, \\\\ldots, n$,\\n\\n\\n\\\\begin{align*}\\n\\\\left(a_{1 j} \\\\phi_{1}+a_{2 j} \\\\phi_{2}+\\\\cdots+a_{m j} \\\\phi_{m}\\\\right)(v) & =\\\\left(a_{1 j} \\\\phi_{1}+a_{2 j} \\\\phi_{2}+\\\\cdots+a_{m j} \\\\phi_{m}\\\\right)\\\\left(k_{1} v_{1}+k_{2} v_{2}+\\\\cdots+k_{m} v_{m}\\\\right) \\\\\\\\\\n& =k_{1} a_{1 j}+k_{2} a_{2 j}+\\\\cdots+k_{m} a_{m j} \\\\tag{4}\\n\\\\end{align*}\\n\\n\\nBecause $v \\\\in V$ was arbitrary, (3) and (4) imply that\\n\\n$$\\nT^{t}\\\\left(\\\\sigma_{j}\\\\right)=a_{1 j} \\\\phi_{1}+a_{2 j} \\\\phi_{2}+\\\\cdots+a_{m j} \\\\phi_{m}, \\\\quad j=1, \\\\ldots, n\\n$$\\n\\nwhich is (2). Thus, the theorem is proved.\\n\\n',\n",
       " '12.1. Let $u=\\\\left(x_{1}, x_{2}, x_{3}\\\\right)$ and $v=\\\\left(y_{1}, y_{2}, y_{3}\\\\right)$. Express $f$ in matrix notation, where\\n\\n$$\\nf(u, v)=3 x_{1} y_{1}-2 x_{1} y_{3}+5 x_{2} y_{1}+7 x_{2} y_{2}-8 x_{2} y_{3}+4 x_{3} y_{2}-6 x_{3} y_{3}\\n$$\\n\\nLet $A=\\\\left[a_{i j}\\\\right]$, where $a_{i j}$ is the coefficient of $x_{i} y_{j}$. Then\\n\\n$$\\nf(u, v)=X^{T} A Y=\\\\left[x_{1}, x_{2}, x_{3}\\\\right]\\\\left[\\\\begin{array}{lll}\\n3 & 0 & -2 \\\\\\\\\\n5 & 7 & -8 \\\\\\\\\\n0 & 4 & -6\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\ny_{1} \\\\\\\\\\ny_{2} \\\\\\\\\\ny_{3}\\n\\\\end{array}\\\\right]\\n$$\\n',\n",
       " '\\n12.2. Let $A$ be an $n \\\\times n$ matrix over $K$. Show that the mapping $f$ defined by $f(X, Y)=X^{T} A Y$ is a bilinear form on $K^{n}$.\\n\\nFor any $a, b \\\\in K$ and any $X_{i}, Y_{i} \\\\in K^{n}$,\\n\\n$$\\n\\\\begin{aligned}\\nf\\\\left(a X_{1}+b X_{2}, Y\\\\right) & =\\\\left(a X_{1}+b X_{2}\\\\right)^{T} A Y=\\\\left(a X_{1}^{T}+b X_{2}^{T}\\\\right) A Y \\\\\\\\\\n& =a X_{1}^{T} A Y+b X_{2}^{T} A Y=a f\\\\left(X_{1}, Y\\\\right)+b f\\\\left(X_{2}, Y\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nHence, $f$ is linear in the first variable. Also,\\n\\n$$\\nf\\\\left(X, a Y_{1}+b Y_{2}\\\\right)=X^{T} A\\\\left(a Y_{1}+b Y_{2}\\\\right)=a X^{T} A Y_{1}+b X^{T} A Y_{2}=a f\\\\left(X, Y_{1}\\\\right)+b f\\\\left(X, Y_{2}\\\\right)\\n$$\\n\\nHence, $f$ is linear in the second variable, and so $f$ is a bilinear form on $K^{n}$.\\n',\n",
       " '\\n12.3. Let $f$ be the bilinear form on $\\\\mathbf{R}^{2}$ defined by\\n\\n$$\\nf\\\\left[\\\\left(x_{1}, x_{2}\\\\right), \\\\quad\\\\left(y_{1}, y_{2}\\\\right)\\\\right]=2 x_{1} y_{1}-3 x_{1} y_{2}+4 x_{2} y_{2}\\n$$\\n\\n(a) Find the matrix $A$ of $f$ in the basis $\\\\left\\\\{u_{1}=(1,0), u_{2}=(1,1)\\\\right\\\\}$.\\n\\n(b) Find the matrix $B$ of $f$ in the basis $\\\\left\\\\{v_{1}=(2,1), v_{2}=(1,-1)\\\\right\\\\}$.\\n\\n(c) Find the change-of-basis matrix $P$ from the basis $\\\\left\\\\{u_{i}\\\\right\\\\}$ to the basis $\\\\left\\\\{v_{i}\\\\right\\\\}$, and verify that $B=P^{T} A P$.\\n\\n(a) Set $A=\\\\left[a_{i j}\\\\right]$, where $a_{i j}=f\\\\left(u_{i}, u_{j}\\\\right)$. This yields\\n\\n$$\\n\\\\begin{aligned}\\n& a_{11}=f[(1,0), \\\\quad(1,0)]=2-0-0=2, \\\\quad a_{21}=f[(1,1), \\\\quad(1,0)]=2-0+0=2 \\\\\\\\\\n& a_{12}=f[(1,0), \\\\quad(1,1)]=2-3-0=-1, \\\\quad a_{22}=f[(1,1), \\\\quad(1,1)]=2-3+4=3\\n\\\\end{aligned}\\n$$\\n\\nThus, $A=\\\\left[\\\\begin{array}{rr}2 & -1 \\\\\\\\ 2 & 3\\\\end{array}\\\\right]$ is the matrix of $f$ in the basis $\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}$.\\n\\n(b) Set $B=\\\\left[b_{i j}\\\\right]$, where $b_{i j}=f\\\\left(v_{i}, v_{j}\\\\right)$. This yields\\n\\n\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2024_04_03_de2bde501961f6000cc6g-373}\\n\\\\end{center}\\n\\nThus, $B=\\\\left[\\\\begin{array}{rr}6 & 6 \\\\\\\\ -3 & 9\\\\end{array}\\\\right]$ is the matrix of $f$ in the basis $\\\\left\\\\{v_{1}, v_{2}\\\\right\\\\}$.\\n\\n(c) Writing $v_{1}$ and $v_{2}$ in terms of the $u_{i}$ yields $v_{1}=u_{1}+u_{2}$ and $v_{2}=2 u_{1}-u_{2}$. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n1 & -1\\n\\\\end{array}\\\\right], \\\\quad P^{T}=\\\\left[\\\\begin{array}{rr}\\n1 & 1 \\\\\\\\\\n2 & -1\\n\\\\end{array}\\\\right]\\n$$\\n\\nand\\n\\n$$\\nP^{T} A P=\\\\left[\\\\begin{array}{rr}\\n1 & 1 \\\\\\\\\\n2 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n2 & -1 \\\\\\\\\\n2 & 3\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n1 & -1\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}\\n6 & 6 \\\\\\\\\\n-3 & 9\\n\\\\end{array}\\\\right]=B\\n$$\\n',\n",
       " '\\n12.4. Prove Theorem 12.1: Let $V$ be an $n$-dimensional vector space over $K$. Let $\\\\left\\\\{\\\\phi_{1}, \\\\ldots, \\\\phi_{n}\\\\right\\\\}$ be any basis of the dual space $V^{*}$. Then $\\\\left\\\\{f_{i j}: i, j=1, \\\\ldots, n\\\\right\\\\}$ is a basis of $B(V)$, where $f_{i j}$ is defined by $f_{i j}(u, v)=\\\\phi_{i}(u) \\\\phi_{j}(v)$. Thus, $\\\\operatorname{dim} B(V)=n^{2}$.\\n\\nLet $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ be the basis of $V$ dual to $\\\\left\\\\{\\\\phi_{i}\\\\right\\\\}$. We first show that $\\\\left\\\\{f_{i j}\\\\right\\\\}$ spans $B(V)$. Let $f \\\\in B(V)$ and suppose $f\\\\left(u_{i}, u_{j}\\\\right)=a_{i j}$. We claim that $f=\\\\sum_{i, j} a_{i j} f_{i j}$. It suffices to show that\\n\\n$$\\nf\\\\left(u_{s}, u_{t}\\\\right)=\\\\left(\\\\sum a_{i j} f_{i j}\\\\right)\\\\left(u_{s}, u_{t}\\\\right) \\\\quad \\\\text { for } \\\\quad s, t=1, \\\\ldots, n\\n$$\\n\\nWe have\\n\\n$$\\n\\\\left(\\\\sum a_{i j} f_{i j}\\\\right)\\\\left(u_{s}, u_{t}\\\\right)=\\\\sum a_{i j} f_{i j}\\\\left(u_{s}, u_{t}\\\\right)=\\\\sum a_{i j} \\\\phi_{i}\\\\left(u_{s}\\\\right) \\\\phi_{j}\\\\left(u_{t}\\\\right)=\\\\sum a_{i j} \\\\delta_{i s} \\\\delta_{j t}=a_{s t}=f\\\\left(u_{s}, u_{t}\\\\right)\\n$$\\n\\nas required. Hence, $\\\\left\\\\{f_{i j}\\\\right\\\\}$ spans $B(V)$. Next, suppose $\\\\sum a_{i j} f_{i j}=\\\\mathbf{0}$. Then for $s, t=1, \\\\ldots, n$,\\n\\n$$\\n0=\\\\mathbf{0}\\\\left(u_{s}, u_{t}\\\\right)=\\\\left(\\\\sum a_{i j} f_{i j}\\\\right)\\\\left(u_{s}, u_{t}\\\\right)=a_{r s}\\n$$\\n\\nThe last step follows as above. Thus, $\\\\left\\\\{f_{i j}\\\\right\\\\}$ is independent, and hence is a basis of $B(V)$.\\n',\n",
       " '\\n12.5. Prove Theorem 12.2. Let $P$ be the change-of-basis matrix from a basis $S$ to a basis $S^{\\\\prime}$. Let $A$ be the matrix representing a bilinear form in the basis $S$. Then $B=P^{T} A P$ is the matrix representing $f$ in the basis $S^{\\\\prime}$.\\n\\nLet $u, v \\\\in V$. Because $P$ is the change-of-basis matrix from $S$ to $S^{\\\\prime}$, we have $P[u]_{S^{\\\\prime}}=[u]_{S}$ and also $P[v]_{S^{\\\\prime}}=[v]_{S}$; hence, $[u]_{S}^{T}=[u]_{S^{\\\\prime}}^{T} P^{T}$. Thus,\\n\\n$$\\nf(u, v)=[u]_{S}^{T} A[v]_{S}=[u]_{S^{\\\\prime}}^{T} P^{T} A P[v]_{S^{\\\\prime}}\\n$$\\n\\nBecause $u$ and $v$ are arbitrary elements of $V, P^{T} A P$ is the matrix of $f$ in the basis $S^{\\\\prime}$.\\n\\n\\n\\\\section*{Symmetric Bilinear Forms, Quadratic Forms}\\n',\n",
       " '12.6. Find the symmetric matrix that corresponds to each of the following quadratic forms:\\n\\n(a) $q(x, y, z)=3 x^{2}+4 x y-y^{2}+8 x z-6 y z+z^{2}$,\\n\\n(b) $q^{\\\\prime}(x, y, z)=3 x^{2}+x z-2 y z$, (c) $q^{\\\\prime \\\\prime}(x, y, z)=2 x^{2}-5 y^{2}-7 z^{2}$\\n\\nThe symmetric matrix $A=\\\\left[a_{i j}\\\\right]$ that represents $q\\\\left(x_{1}, \\\\ldots, x_{n}\\\\right)$ has the diagonal entry $a_{i i}$ equal to the coefficient of the square term $x_{i}^{2}$ and the nondiagonal entries $a_{i j}$ and $a_{j i}$ each equal to half of the coefficient of the cross-product term $x_{i} x_{j}$. Thus,\\n\\n(a) $A=\\\\left[\\\\begin{array}{rrr}3 & 2 & 4 \\\\\\\\ 2 & -1 & -3 \\\\\\\\ 4 & -3 & 1\\\\end{array}\\\\right]$, (b) $A^{\\\\prime}=\\\\left[\\\\begin{array}{rrr}3 & 0 & \\\\frac{1}{2} \\\\\\\\ 0 & 0 & -1 \\\\\\\\ \\\\frac{1}{2} & -1 & 0\\\\end{array}\\\\right]$, (c) $A^{\\\\prime \\\\prime}=\\\\left[\\\\begin{array}{rrr}2 & 0 & 0 \\\\\\\\ 0 & -5 & 0 \\\\\\\\ 0 & 0 & -7\\\\end{array}\\\\right]$\\n\\nThe third matrix $A^{\\\\prime \\\\prime}$ is diagonal, because the quadratic form $q^{\\\\prime \\\\prime}$ is diagonal; that is, $q^{\\\\prime \\\\prime}$ has no cross-product terms.\\n',\n",
       " '\\n12.7. Find the quadratic form $q(X)$ that corresponds to each of the following symmetric matrices:\\n\\n(a)\\n\\n$A=\\\\left[\\\\begin{array}{rr}5 & -3 \\\\\\\\ -3 & 8\\\\end{array}\\\\right]$\\n\\n(b) $B=\\\\left[\\\\begin{array}{rrr}4 & -5 & 7 \\\\\\\\ -5 & -6 & 8 \\\\\\\\ 7 & 8 & -9\\\\end{array}\\\\right]$,\\n\\n$C=\\\\left[\\\\begin{array}{rrrr}2 & 4 & -1 & 5 \\\\\\\\ 4 & -7 & -6 & 8 \\\\\\\\ -1 & -6 & 3 & 9 \\\\\\\\ 5 & 8 & 9 & 1\\\\end{array}\\\\right]$\\n\\nThe quadratic form $q(X)$ that corresponds to a symmetric matrix $M$ is defined by $q(X)=X^{T} M X$, where $X=\\\\left[x_{i}\\\\right]$ is the column vector of unknowns.\\n\\n(a) Compute as follows:\\n\\n$$\\n\\\\begin{aligned}\\nq(x, y) & =X^{T} A X=[x, y]\\\\left[\\\\begin{array}{rr}\\n5 & -3 \\\\\\\\\\n-3 & 8\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n5 x-3 y, & -3 x+8 y\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =5 x^{2}-3 x y-3 x y+8 y^{2}=5 x^{2}-6 x y+8 y^{2}\\n\\\\end{aligned}\\n$$\\n\\nAs expected, the coefficient 5 of the square term $x^{2}$ and the coefficient 8 of the square term $y^{2}$ are the diagonal elements of A, and the coefficient -6 of the cross-product term $x y$ is the sum of the nondiagonal elements -3 and -3 of $A$ (or twice the nondiagonal element -3 , because $A$ is symmetric).\\n\\n(b) Because $B$ is a three-square matrix, there are three unknowns, say $x, y, z$ or $x_{1}, x_{2}, x_{3}$. Then\\n\\nor\\n\\n$$\\n\\\\begin{gathered}\\nq(x, y, z)=4 x^{2}-10 x y-6 y^{2}+14 x z+16 y z-9 z^{2} \\\\\\\\\\nq\\\\left(x_{1}, x_{2}, x_{3}\\\\right)=4 x_{1}^{2}-10 x_{1} x_{2}-6 x_{2}^{2}+14 x_{1} x_{3}+16 x_{2} x_{3}-9 x_{3}^{2}\\n\\\\end{gathered}\\n$$\\n\\nHere we use the fact that the coefficients of the square terms $x_{1}^{2}, x_{2}^{2}, x_{3}^{2}\\\\left(\\\\right.$ or $\\\\left.x^{2}, y^{2}, z^{2}\\\\right)$ are the respective diagonal elements $4,-6,-9$ of $B$, and the coefficient of the cross-product term $x_{i} x_{j}$ is the sum of the nondiagonal elements $b_{i j}$ and $b_{j i}$ (or twice $b_{i j}$, because $b_{i j}=b_{j i}$ ).\\n\\n(c) Because $C$ is a four-square matrix, there are four unknowns. Hence,\\n\\n$$\\n\\\\begin{aligned}\\nq\\\\left(x_{1}, x_{2}, x_{3}, x_{4}\\\\right)= & 2 x_{1}^{2}-7 x_{2}^{2}+3 x_{3}^{2}+x_{4}^{2}+8 x_{1} x_{2}-2 x_{1} x_{3} \\\\\\\\\\n& +10 x_{1} x_{4}-12 x_{2} x_{3}+16 x_{2} x_{4}+18 x_{3} x_{4}\\n\\\\end{aligned}\\n$$\\n',\n",
       " '\\n12.8. Let $A=\\\\left[\\\\begin{array}{rrr}1 & -3 & 2 \\\\\\\\ -3 & 7 & -5 \\\\\\\\ 2 & -5 & 8\\\\end{array}\\\\right]$. Apply Algorithm 12.1 to find a nonsingular matrix $P$ such that $D=P^{T} A P$ is diagonal, and find $\\\\operatorname{sig}(A)$, the signature of $A$.\\n\\nFirst form the block matrix $M=[A, I]$ :\\n\\n$$\\nM=[A, I]=\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & -3 & 2 & 1 & 0 & 0 \\\\\\\\\\n-3 & 7 & -5 & 0 & 1 & 0 \\\\\\\\\\n2 & -5 & 8 & 0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nUsing $a_{11}=1$ as a pivot, apply the row operations \"Replace $R_{2}$ by $3 R_{1}+R_{2}$ \" and \"Replace $R_{3}$ by $-2 R_{1}+R_{3}$ \" to $M$ and then apply the corresponding column operations \"Replace $C_{2}$ by $3 C_{1}+C_{2}$ \" and \"Replace $C_{3}$ by $-2 C_{1}+C_{3}$ \" to $A$ to obtain\\n\\n$$\\n\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & -3 & 2 & 1 & 0 & 0 \\\\\\\\\\n0 & -2 & 1 & 3 & 1 & 0 \\\\\\\\\\n0 & 1 & 4 & -2 & 0 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad\\\\left[\\\\begin{array}{rrr:rrr}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & -2 & 1 & 3 & 1 & 0 \\\\\\\\\\n0 & 1 & 4 & -2 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nNext apply the row operation \"Replace $R_{3}$ by $R_{2}+2 R_{3}$ \" and then the corresponding column operation \"Replace $C_{3}$ by $C_{2}+2 C_{3}$ \" to obtain\\n\\n$$\\n\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & -2 & 1 & 3 & 1 & 0 \\\\\\\\\\n0 & 0 & 9 & -1 & 1 & 2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad\\\\left[\\\\begin{array}{rrrrrr}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & -2 & 0 & 3 & 1 & 0 \\\\\\\\\\n0 & 0 & 18 & -1 & 1 & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow $A$ has been diagonalized and the transpose of $P$ is in the right half of $M$. Thus, set\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rrr}\\n1 & 3 & -1 \\\\\\\\\\n0 & 1 & 1 \\\\\\\\\\n0 & 0 & 2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad D=P^{T} A P=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n0 & -2 & 0 \\\\\\\\\\n0 & 0 & 18\\n\\\\end{array}\\\\right]\\n$$\\n\\nNote $D$ has $\\\\mathbf{p}=2$ positive and $\\\\mathbf{n}=1$ negative diagonal elements. Thus, the signature of $A$ is $\\\\operatorname{sig}(A)=\\\\mathbf{p}-\\\\mathbf{n}=2-1=1$.\\n',\n",
       " '\\n12.9. Justify Algorithm 12.1, which diagonalizes (under congruence) a symmetric matrix $A$.\\n\\nConsider the block matrix $M=[A, I]$. The algorithm applies a sequence of elementary row operations and the corresponding column operations to the left side of $M$, which is the matrix $A$. This is equivalent to premultiplying $A$ by a sequence of elementary matrices, say, $E_{1}, E_{2}, \\\\ldots, E_{r}$, and postmultiplying $A$ by the transposes of the $E_{i}$. Thus, when the algorithm ends, the diagonal matrix $D$ on the left side of $M$ is equal to\\n\\n$$\\nD=E_{r} \\\\cdots E_{2} E_{1} A E_{1}^{T} E_{2}^{T} \\\\cdots E_{r}^{T}=Q A Q^{T}, \\\\quad \\\\text { where } \\\\quad Q=E_{r} \\\\cdots E_{2} E_{1}\\n$$\\n\\nOn the other hand, the algorithm only applies the elementary row operations to the identity matrix $I$ on the right side of $M$. Thus, when the algorithm ends, the matrix on the right side of $M$ is equal to\\n\\n$$\\nE_{r} \\\\cdots E_{2} E_{1} I=E_{r} \\\\cdots E_{2} E_{1}=Q\\n$$\\n\\nSetting $P=Q^{T}$, we get $D=P^{T} A P$, which is a diagonalization of $A$ under congruence.\\n',\n",
       " '\\n12.10. Prove Theorem 12.4: Let $f$ be a symmetric bilinear form on $V$ over $K$ (where $1+1 \\\\neq 0$ ). Then $V$ has a basis in which $f$ is represented by a diagonal matrix.\\n\\nAlgorithm 12.1 shows that every symmetric matrix over $K$ is congruent to a diagonal matrix. This is equivalent to the statement that $f$ has a diagonal representation.\\n',\n",
       " '\\n12.11. Let $q$ be the quadratic form associated with the symmetric bilinear form $f$. Verify the polar identity $f(u, v)=\\\\frac{1}{2}[q(u+v)-q(u)-q(v)]$. (Assume that $1+1 \\\\neq 0$.)\\n\\nWe have\\n\\n$$\\n\\\\begin{aligned}\\nq(u+v)-q(u)-q(v) & =f(u+v, u+v)-f(u, u)-f(v, v) \\\\\\\\\\n& =f(u, u)+f(u, v)+f(v, u)+f(v, v)-f(u, u)-f(v, v)=2 f(u, v)\\n\\\\end{aligned}\\n$$\\n\\nIf $1+1 \\\\neq 0$, we can divide by 2 to obtain the required identity.\\n',\n",
       " '\\n12.12. Consider the quadratic form $q(x, y)=3 x^{2}+2 x y-y^{2}$ and the linear substitution\\n\\n$$\\nx=s-3 t, \\\\quad y=2 s+t\\n$$\\n\\n(a) Rewrite $q(x, y)$ in matrix notation, and find the matrix $A$ representing $q(x, y)$.\\n\\n(b) Rewrite the linear substitution using matrix notation, and find the matrix $P$ corresponding to the substitution.\\n\\n(c) Find $q(s, t)$ using direct substitution.\\n\\n(d) Find $q(s, t)$ using matrix notation.\\n\\n(a) Here $q(x, y)=[x, y]\\\\left[\\\\begin{array}{rr}3 & 1 \\\\\\\\ 1 & -1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}x \\\\\\\\ y\\\\end{array}\\\\right]$. Thus, $A=\\\\left[\\\\begin{array}{rr}3 & 1 \\\\\\\\ 1 & -1\\\\end{array}\\\\right]$; and $q(X)=X^{T} A X$, where $X=[x, y]^{T}$.\\n\\n(b) Here $\\\\left[\\\\begin{array}{l}x \\\\\\\\ y\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}1 & -3 \\\\\\\\ 2 & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}s \\\\\\\\ t\\\\end{array}\\\\right]$. Thus, $P=\\\\left[\\\\begin{array}{rr}1 & -3 \\\\\\\\ 2 & 1\\\\end{array}\\\\right]$; and $X=\\\\left[\\\\begin{array}{l}x \\\\\\\\ y\\\\end{array}\\\\right], Y=\\\\left[\\\\begin{array}{l}s \\\\\\\\ t\\\\end{array}\\\\right]$ and $X=P Y$.\\n\\n(c) Substitute for $x$ and $y$ in $q$ to obtain\\n\\n$$\\n\\\\begin{aligned}\\nq(s, t) & =3(s-3 t)^{2}+2(s-3 t)(2 s+t)-(2 s+t)^{2} \\\\\\\\\\n& =3\\\\left(s^{2}-6 s t+9 t^{2}\\\\right)+2\\\\left(2 s^{2}-5 s t-3 t^{2}\\\\right)-\\\\left(4 s^{2}+4 s t+t^{2}\\\\right)=3 s^{2}-32 s t+20 t^{2}\\n\\\\end{aligned}\\n$$\\n\\n(d) Here $q(X)=X^{T} A X$ and $X=P Y$. Thus, $X^{T}=Y^{T} P^{T}$. Therefore,\\n\\n$$\\n\\\\begin{aligned}\\nq(s, t) & =q(Y)=Y^{T} P^{T} A P Y=[s, t]\\\\left[\\\\begin{array}{rr}\\n1 & 2 \\\\\\\\\\n-3 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n3 & 1 \\\\\\\\\\n1 & -1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}\\n1 & -3 \\\\\\\\\\n2 & 1\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\ns \\\\\\\\\\nt\\n\\\\end{array}\\\\right] \\\\\\\\\\n& =[s, t]\\\\left[\\\\begin{array}{rr}\\n3 & -16 \\\\\\\\\\n-16 & 20\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\ns \\\\\\\\\\nt\\n\\\\end{array}\\\\right]=3 s^{2}-32 s t+20 t^{2}\\n\\\\end{aligned}\\n$$\\n\\n[As expected, the results in parts (c) and (d) are equal.]\\n',\n",
       " '\\n12.13. Consider any diagonal matrix $A=\\\\operatorname{diag}\\\\left(a_{1}, \\\\ldots, a_{n}\\\\right)$ over $K$. Show that for any nonzero scalars $k_{1}, \\\\ldots, k_{n} \\\\in K, A$ is congruent to a diagonal matrix $D$ with diagonal entries $a_{1} k_{1}^{2}, \\\\ldots, a_{n} k_{n}^{2}$. Furthermore, show that\\n\\n(a) If $K=\\\\mathbf{C}$, then we can choose $D$ so that its diagonal entries are only l\\'s and 0 \\'s.\\n\\n(b) If $K=\\\\mathbf{R}$, then we can choose $D$ so that its diagonal entries are only 1 \\'s, -1 \\'s, and 0 \\'s.\\n\\nLet $P=\\\\operatorname{diag}\\\\left(k_{1}, \\\\ldots, k_{n}\\\\right)$. Then, as required,\\n\\n$$\\nD=P^{T} A P=\\\\operatorname{diag}\\\\left(k_{i}\\\\right) \\\\operatorname{diag}\\\\left(a_{i}\\\\right) \\\\operatorname{diag}\\\\left(k_{i}\\\\right)=\\\\operatorname{diag}\\\\left(a_{1} k_{1}^{2}, \\\\ldots, a_{n} k_{n}^{2}\\\\right)\\n$$\\n\\n(a) Let $P=\\\\operatorname{diag}\\\\left(b_{i}\\\\right)$, where $b_{i}=\\\\left\\\\{\\\\begin{array}{cll}1 / \\\\sqrt{a_{i}} & \\\\text { if } a_{i} \\\\neq 0 \\\\\\\\ 1 & \\\\text { if } & a_{i}=0\\\\end{array}\\\\right.$\\n\\nThen $P^{T} A P$ has the required form.\\n\\n(b) Let $P=\\\\operatorname{diag}\\\\left(b_{i}\\\\right)$, where $b_{i}=\\\\left\\\\{\\\\begin{array}{cl}1 / \\\\sqrt{\\\\left|a_{i}\\\\right|} & \\\\text { if } a_{i} \\\\neq 0 \\\\\\\\ 1 & \\\\text { if } a_{i}=0\\\\end{array}\\\\right.$\\n\\nThen $P^{T} A P$ has the required form.\\n\\nRemark: We emphasize that (b) is no longer true if \"congruence\" is replaced by \"Hermitian congruence.\"\\n',\n",
       " '\\n12.14. Prove Theorem 12.5: Let $f$ be a symmetric bilinear form on $V$ over $\\\\mathbf{R}$. Then there exists a basis of $V$ in which $f$ is represented by a diagonal matrix. Every other diagonal matrix representation of $f$ has the same number $\\\\mathbf{p}$ of positive entries and the same number $\\\\mathbf{n}$ of negative entries.\\n\\nBy Theorem 12.4, there is a basis $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ of $V$ in which $f$ is represented by a diagonal matrix with, say, $\\\\mathbf{p}$ positive and $\\\\mathbf{n}$ negative entries. Now suppose $\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$ is another basis of $V$, in which $f$ is represented by a diagonal matrix with $\\\\mathbf{p}^{\\\\prime}$ positive and $\\\\mathbf{n}^{\\\\prime}$ negative entries. We can assume without loss of generality that the positive entries in each matrix appear first. Because $\\\\operatorname{rank}(f)=\\\\mathbf{p}+\\\\mathbf{n}=\\\\mathbf{p}^{\\\\prime}+\\\\mathbf{n}^{\\\\prime}$, it suffices to prove that $\\\\mathbf{p}=\\\\mathbf{p}^{\\\\prime}$.\\n\\nLet $U$ be the linear span of $u_{1}, \\\\ldots, u_{\\\\mathbf{p}}$ and let $W$ be the linear span of $w_{\\\\mathbf{p}^{\\\\prime}+1}, \\\\ldots, w_{n}$. Then $f(v, v)>0$ for every nonzero $v \\\\in U$, and $f(v, v) \\\\leq 0$ for every nonzero $v \\\\in W$. Hence, $U \\\\cap W=\\\\{0\\\\}$. Note that $\\\\operatorname{dim} U=\\\\mathbf{p}$ and $\\\\operatorname{dim} W=n-\\\\mathbf{p}^{\\\\prime}$. Thus,\\n\\n$$\\n\\\\operatorname{dim}(U+W)=\\\\operatorname{dim} U+\\\\operatorname{dim} W-\\\\operatorname{dim}(U \\\\cap W)=\\\\mathbf{p}+\\\\left(n-\\\\mathbf{p}^{\\\\prime}\\\\right)-0=\\\\mathbf{p}-\\\\mathbf{p}^{\\\\prime}+n\\n$$\\n\\nBut $\\\\operatorname{dim}(U+W) \\\\leq \\\\operatorname{dim} V=n$; hence, $\\\\mathbf{p}-\\\\mathbf{p}^{\\\\prime}+n \\\\leq n$ or $\\\\mathbf{p} \\\\leq \\\\mathbf{p}^{\\\\prime}$. Similarly, $\\\\mathbf{p}^{\\\\prime} \\\\leq \\\\mathbf{p}$ and therefore $\\\\mathbf{p}=\\\\mathbf{p}^{\\\\prime}$, as required.\\n\\nRemark: The above theorem and proof depend only on the concept of positivity. Thus, the theorem is true for any subfield $K$ of the real field $\\\\mathbf{R}$ such as the rational field $\\\\mathbf{Q}$.\\n\\n\\n\\\\section*{Positive Definite Real Quadratic Forms}\\n',\n",
       " '12.15. Prove that the following definitions of a positive definite quadratic form $q$ are equivalent:\\n\\n(a) The diagonal entries are all positive in any diagonal representation of $q$.\\n\\n(b) $q(Y)>0$, for any nonzero vector $Y$ in $\\\\mathbf{R}^{n}$.\\n\\nSuppose $q(Y)=a_{1} y_{1}^{2}+a_{2} y_{2}^{2}+\\\\cdots+a_{n} y_{n}^{2}$. If all the coefficients are positive, then clearly $q(Y)>0$ whenever $Y \\\\neq 0$. Thus, (a) implies (b). Conversely, suppose (a) is not true; that is, suppose some diagonal entry $a_{k} \\\\leq 0$. Let $e_{k}=(0, \\\\ldots, 1, \\\\ldots 0)$ be the vector whose entries are all 0 except 1 in the $k$ th position. Then $q\\\\left(e_{k}\\\\right)=a_{k}$ is not positive, and so (b) is not true. That is, (b) implies (a). Accordingly, (a) and (b) are equivalent.\\n',\n",
       " '\\n12.16. Determine whether each of the following quadratic forms $q$ is positive definite:\\n\\n(a) $q(x, y, z)=x^{2}+2 y^{2}-4 x z-4 y z+7 z^{2}$\\n\\n(b) $q(x, y, z)=x^{2}+y^{2}+2 x z+4 y z+3 z^{2}$\\n\\nDiagonalize (under congruence) the symmetric matrix $A$ corresponding to $q$.\\n\\n(a) Apply the operations \"Replace $R_{3}$ by $2 R_{1}+R_{3}$ \" and \"Replace $C_{3}$ by $2 C_{1}+C_{3}$,\" and then \"Replace $R_{3}$ by $R_{2}+R_{3}$ \" and \"Replace $C_{3}$ by $C_{2}+C_{3}$.\" These yield\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & -2 \\\\\\\\\\n0 & 2 & -2 \\\\\\\\\\n-2 & -2 & 7\\n\\\\end{array}\\\\right] \\\\simeq\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n0 & 2 & -2 \\\\\\\\\\n0 & -2 & 3\\n\\\\end{array}\\\\right] \\\\simeq\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 2 & 0 \\\\\\\\\\n0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe diagonal representation of $q$ only contains positive entries, $1,2,1$, on the diagonal. Thus, $q$ is positive definite.\\n\\n(b) We have\\n\\n$$\\nA=\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 1 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n1 & 2 & 3\\n\\\\end{array}\\\\right] \\\\simeq\\\\left[\\\\begin{array}{lll}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 2 \\\\\\\\\\n0 & 2 & 2\\n\\\\end{array}\\\\right] \\\\simeq\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 \\\\\\\\\\n0 & 0 & -2\\n\\\\end{array}\\\\right]\\n$$\\n\\nThere is a negative entry -2 on the diagonal representation of $q$. Thus, $q$ is not positive definite.\\n',\n",
       " '\\n12.17. Show that $q(x, y)=a x^{2}+b x y+c y^{2}$ is positive definite if and only if $a>0$ and the discriminant $D=b^{2}-4 a c<0$.\\n\\nSuppose $v=(x, y) \\\\neq 0$. Then either $x \\\\neq 0$ or $y \\\\neq 0$; say, $y \\\\neq 0$. Let $t=x / y$. Then\\n\\n$$\\nq(v)=y^{2}\\\\left[a(x / y)^{2}+b(x / y)+c\\\\right]=y^{2}\\\\left(a t^{2}+b t+c\\\\right)\\n$$\\n\\nHowever, the following are equivalent:\\n\\n(i) $s=a t^{2}+b t+c$ is positive for every value of $t$.\\n\\n(ii) $s=a t^{2}+b t+c$ lies above the $t$-axis.\\n\\n(iii) $a>0$ and $D=b^{2}-4 a c<0$.\\n\\nThus, $q$ is positive definite if and only if $a>0$ and $D<0$. [Remark: $D<0$ is the same as $\\\\operatorname{det}(A)>0$, where $A$ is the symmetric matrix corresponding to $q$.]\\n',\n",
       " '\\n12.18. Determine whether or not each of the following quadratic forms $q$ is positive definite:\\n\\n(a) $q(x, y)=x^{2}-4 x y+7 y^{2}$, (b) $q(x, y)=x^{2}+8 x y+5 y^{2}$, (c) $q(x, y)=3 x^{2}+2 x y+y^{2}$\\n\\nCompute the discriminant $D=b^{2}-4 a c$, and then use Problem 12.17.\\n\\n(a) $D=16-28=-12$. Because $a=1>0$ and $D<0, q$ is positive definite.\\n\\n(b) $D=64-20=44$. Because $D>0, q$ is not positive definite.\\n\\n(c) $D=4-12=-8$. Because $a=3>0$ and $D<0, q$ is positive definite.\\n\\n\\n\\\\section*{Hermitian Forms}\\n',\n",
       " '12.19. Determine whether the following matrices are Hermitian:\\n\\n(a) $\\\\left[\\\\begin{array}{ccc}2 & 2+3 i & 4-5 i \\\\\\\\ 2-3 i & 5 & 6+2 i \\\\\\\\ 4+5 i & 6-2 i & -7\\\\end{array}\\\\right]$, (b) $\\\\left[\\\\begin{array}{ccc}3 & 2-i & 4+i \\\\\\\\ 2-i & 6 & i \\\\\\\\ 4+i & i & 7\\\\end{array}\\\\right]$, (c) $\\\\left[\\\\begin{array}{rrr}4 & -3 & 5 \\\\\\\\ -3 & 2 & 1 \\\\\\\\ 5 & 1 & -6\\\\end{array}\\\\right]$\\n\\nA complex matrix $A=\\\\left[a_{i j}\\\\right]$ is Hermitian if $A^{*}=A$-that is, if $a_{i j}=\\\\bar{a}_{j i}$.\\n\\n(a) Yes, because it is equal to its conjugate transpose.\\n\\n(b) No, even though it is symmetric.\\n\\n(c) Yes. In fact, a real matrix is Hermitian if and only if it is symmetric.\\n',\n",
       " '\\n12.20. Let $A$ be a Hermitian matrix. Show that $f$ is a Hermitian form on $\\\\mathbf{C}^{n}$ where $f$ is defined by $f(X, Y)=X^{T} A \\\\bar{Y}$.\\n\\nFor all $a, b \\\\in \\\\mathbf{C}$ and all $X_{1}, X_{2}, Y \\\\in \\\\mathbf{C}^{n}$,\\n\\n$$\\n\\\\begin{aligned}\\nf\\\\left(a X_{1}+b X_{2}, \\\\quad Y\\\\right) & =\\\\left(a X_{1}+b X_{2}\\\\right)^{T} A \\\\bar{Y}=\\\\left(a X_{1}^{T}+b X_{2}^{T}\\\\right) A \\\\bar{Y} \\\\\\\\\\n& =a X_{1}^{T} A \\\\bar{Y}+b X_{2}^{T} A \\\\bar{Y}=a f\\\\left(X_{1}, Y\\\\right)+b f\\\\left(X_{2}, Y\\\\right)\\n\\\\end{aligned}\\n$$\\n\\nHence, $f$ is linear in the first variable. Also,\\n\\n$$\\n\\\\overline{f(X, Y)}=\\\\overline{X^{T} A \\\\bar{Y}}=\\\\overline{\\\\left(X^{T} A \\\\bar{Y}\\\\right)^{T}}=\\\\overline{\\\\bar{Y}^{T} A^{T} X}=Y^{T} A^{*} \\\\bar{X}=Y^{T} A \\\\bar{X}=f(Y, X)\\n$$\\n\\nHence, $f$ is a Hermitian form on $\\\\mathbf{C}^{n}$.\\n\\nRemark: We use the fact that $X^{T} A \\\\bar{Y}$ is a scalar and so it is equal to its transpose.\\n',\n",
       " '\\n12.21. Let $f$ be a Hermitian form on $V$. Let $H$ be the matrix of $f$ in a basis $S=\\\\left\\\\{u_{i}\\\\right\\\\}$ of $V$. Prove the following:\\n\\n(a) $f(u, v)=[u]_{S}^{T} H \\\\overline{[v]_{S}}$ for all $u, v \\\\in V$.\\n\\n(b) If $P$ is the change-of-basis matrix from $S$ to a new basis $S^{\\\\prime}$ of $V$, then $B=P^{T} H \\\\bar{P}$ (or $B=Q^{*} H Q$, where $\\\\left.Q=\\\\bar{P}\\\\right)$ is the matrix of $f$ in the new basis $S^{\\\\prime}$.\\n\\nNote that (b) is the complex analog of Theorem 12.2.\\n\\n(a) Let $u, v \\\\in V$ and suppose $u=a_{1} u_{1}+\\\\cdots+a_{n} u_{n}$ and $v=b_{1} u_{1}+\\\\cdots+b_{n} u_{n}$. Then, as required,\\n\\n$$\\n\\\\begin{aligned}\\nf(u, v) & =f\\\\left(a_{1} u_{1}+\\\\cdots+a_{n} u_{n}, \\\\quad b_{1} u_{1}+\\\\cdots+b_{n} u_{n}\\\\right) \\\\\\\\\\n& =\\\\sum_{i, j} a_{i} \\\\bar{b}_{j} f\\\\left(u_{i}, v_{j}\\\\right)=\\\\left[a_{1}, \\\\ldots, a_{n}\\\\right] H\\\\left[\\\\bar{b}_{1}, \\\\ldots, \\\\bar{b}_{n}\\\\right]^{T}=[u]_{S}^{T} H \\\\overline{[v]}_{S}\\n\\\\end{aligned}\\n$$\\n\\n(b) Because $P$ is the change-of-basis matrix from $S$ to $S^{\\\\prime}$, we have $P[u]_{S^{\\\\prime}}=[u]_{S}$ and $P[v]_{S^{\\\\prime}}=[v]_{S}$; hence, $[u]_{S}^{T}=[u]_{S^{\\\\prime}}^{T} P^{T}$ and $\\\\overline{[v]_{S}}=\\\\bar{P} \\\\overline{[v]_{S^{\\\\prime}}}$. Thus, by (a),\\n\\n$$\\nf(u, v)=[u]_{S}^{T} H \\\\overline{[v]_{S}}=[u]_{S^{\\\\prime}}^{T} P^{T} H \\\\bar{P} \\\\overline{[v]_{S^{\\\\prime}}}\\n$$\\n\\nBut $u$ and $v$ are arbitrary elements of $V$; hence, $P^{T} H \\\\bar{P}$ is the matrix of $f$ in the basis $S^{\\\\prime}$.\\n',\n",
       " '\\n12.22. Let $H=\\\\left[\\\\begin{array}{ccc}1 & 1+i & 2 i \\\\\\\\ 1-i & 4 & 2-3 i \\\\\\\\ -2 i & 2+3 i & 7\\\\end{array}\\\\right]$, a Hermitian matrix.\\n\\nFind a nonsingular matrix $P$ such that $D=P^{T} H \\\\bar{P}$ is diagonal. Also, find the signature of $H$.\\n\\nUse the modified Algorithm 12.1 that applies the same row operations but the corresponding conjugate column operations. Thus, first form the block matrix $M=[H, I]$ :\\n\\n$$\\nM=\\\\left[\\\\begin{array}{cccccc}\\n1 & 1+i & 2 i & 1 & 0 & 0 \\\\\\\\\\n1-i & 4 & 2-3 i & 0 & 1 & 0 \\\\\\\\\\n-2 i & 2+3 i & 7 & 0 & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nApply the row operations \"Replace $R_{2}$ by $(-1+i) R_{1}+R_{2}$ \" and \"Replace $R_{3}$ by $2 i R_{1}+R_{3}$ \" and then the corresponding conjugate column operations \"Replace $C_{2}$ by $(-1-i) C_{1}+C_{2}$ \"\\' and \"Replace $C_{3}$ by $-2 i C_{1}+C_{3}$, to obtain\\n\\n$$\\n\\\\left[\\\\begin{array}{cccccc}\\n1 & 1+i & 2 i & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & -5 i & -1+i & 1 & 0 \\\\\\\\\\n0 & 5 i & 3 & 2 i & 0 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad\\\\left[\\\\begin{array}{cccccc}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & -5 i & -1+i & 1 & 0 \\\\\\\\\\n0 & 5 i & 3 & 2 i & 0 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nNext apply the row operation \"Replace $R_{3}$ by $-5 i R_{2}+2 R_{3}$ \" and the corresponding conjugate column operation \"Replace $C_{3}$ by $5 i C_{2}+2 C_{3}$ \" to obtain\\n\\n$$\\n\\\\left[\\\\begin{array}{cccccc}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & -5 i & -1+i & 1 & 0 \\\\\\\\\\n0 & 0 & -19 & 5+9 i & -5 i & 2\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad\\\\left[\\\\begin{array}{cccccc}\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\\n0 & 2 & 0 & -1+i & 1 & 0 \\\\\\\\\\n0 & 0 & -38 & 5+9 i & -5 i & 2\\n\\\\end{array}\\\\right]\\n$$\\n\\nNow $H$ has been diagonalized, and the transpose of the right half of $M$ is $P$. Thus, set\\n\\n$$\\nP=\\\\left[\\\\begin{array}{ccc}\\n1 & -1+i & 5+9 i \\\\\\\\\\n0 & 1 & -5 i \\\\\\\\\\n0 & 0 & 2\\n\\\\end{array}\\\\right], \\\\quad \\\\text { and then } \\\\quad D=P^{T} H \\\\bar{P}=\\\\left[\\\\begin{array}{rrr}\\n1 & 0 & 0 \\\\\\\\\\n0 & 2 & 0 \\\\\\\\\\n0 & 0 & -38\\n\\\\end{array}\\\\right]\\n$$\\n\\nNote $D$ has $\\\\mathbf{p}=2$ positive elements and $\\\\mathbf{n}=1$ negative elements. Thus, the signature of $H$ is $\\\\operatorname{sig}(H)=2-1=1$.\\n\\n\\n\\\\section*{Miscellaneous Problems}\\n',\n",
       " '12.23. Prove Theorem 12.3: Let $f$ be an alternating form on $V$. Then there exists a basis of $V$ in which $f$ is represented by a block diagonal matrix $M$ with blocks of the form $\\\\left[\\\\begin{array}{rr}0 & 1 \\\\\\\\ -1 & 0\\\\end{array}\\\\right]$ or 0 . The number of nonzero blocks is uniquely determined by $f$ [because it is equal to $\\\\frac{1}{2} \\\\operatorname{rank}(f)$ ].\\n\\nIf $f=0$, then the theorem is obviously true. Also, if $\\\\operatorname{dim} V=1$, then $f\\\\left(k_{1} u, k_{2} u\\\\right)=k_{1} k_{2} f(u, u)=0$ and so $f=0$. Accordingly, we can assume that $\\\\operatorname{dim} V>1$ and $f \\\\neq 0$.\\n\\nBecause $f \\\\neq 0$, there exist (nonzero) $u_{1}, u_{2} \\\\in V$ such that $f\\\\left(u_{1}, u_{2}\\\\right) \\\\neq 0$. In fact, multiplying $u_{1}$ by an appropriate factor, we can assume that $f\\\\left(u_{1}, u_{2}\\\\right)=1$ and so $f\\\\left(u_{2}, u_{1}\\\\right)=-1$. Now $u_{1}$ and $u_{2}$ are linearly independent; because if, say, $u_{2}=k u_{1}$, then $f\\\\left(u_{1}, u_{2}\\\\right)=f\\\\left(u_{1}, k u_{1}\\\\right)=k f\\\\left(u_{1}, u_{1}\\\\right)=0$. Let $U=\\\\operatorname{span}\\\\left(u_{1}, u_{2}\\\\right)$; then,\\\\\\\\\\n(i) The matrix representation of the restriction of $f$ to $U$ in the basis $\\\\left\\\\{u_{1}, u_{2}\\\\right\\\\}$ is $\\\\left[\\\\begin{array}{rr}0 & 1 \\\\\\\\ -1 & 0\\\\end{array}\\\\right]$,\\n\\n(ii) If $u \\\\in U$, say $u=a u_{1}+b u_{2}$, then\\n\\n$$\\nf\\\\left(u, u_{1}\\\\right)=f\\\\left(a u_{1}+b u_{2}, u_{1}\\\\right)=-b \\\\quad \\\\text { and } \\\\quad f\\\\left(u, u_{2}\\\\right)=f\\\\left(a u_{1}+b u_{2}, u_{2}\\\\right)=a\\n$$\\n\\nLet $W$ consists of those vectors $w \\\\in V$ such that $f\\\\left(w, u_{1}\\\\right)=0$ and $f\\\\left(w, u_{2}\\\\right)=0$. Equivalently,\\n\\n$$\\nW=\\\\{w \\\\in V: f(w, u)=0 \\\\text { for every } u \\\\in U\\\\}\\n$$\\n\\nWe claim that $V=U \\\\oplus W$. It is clear that $U \\\\cap W=\\\\{0\\\\}$, and so it remains to show that $V=U+W$. Let $v \\\\in V$. Set\\n\\n\\n\\\\begin{equation*}\\nu=f\\\\left(v, u_{2}\\\\right) u_{1}-f\\\\left(v, u_{1}\\\\right) u_{2} \\\\quad \\\\text { and } \\\\quad w=v-u \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause $u$ is a linear combination of $u_{1}$ and $u_{2}, u \\\\in U$.\\n\\nWe show next that $w \\\\in W$. By (1) and (ii), $f\\\\left(u, u_{1}\\\\right)=f\\\\left(v, u_{1}\\\\right)$; hence,\\n\\n$$\\nf\\\\left(w, u_{1}\\\\right)=f\\\\left(v-u, u_{1}\\\\right)=f\\\\left(v, u_{1}\\\\right)-f\\\\left(u, u_{1}\\\\right)=0\\n$$\\n\\nSimilarly, $f\\\\left(u, u_{2}\\\\right)=f\\\\left(v, u_{2}\\\\right)$ and so\\n\\n$$\\nf\\\\left(w, u_{2}\\\\right)+f\\\\left(v-u, u_{2}\\\\right)=f\\\\left(v, u_{2}\\\\right)-f\\\\left(u, u_{2}\\\\right)=0\\n$$\\n\\nThen $w \\\\in W$ and so, by (1), $v=u+w$, where $u \\\\in W$. This shows that $V=U+W$; therefore, $V=U \\\\oplus W$.\\n\\nNow the restriction of $f$ to $W$ is an alternating bilinear form on $W$. By induction, there exists a basis $u_{3}, \\\\ldots, u_{n}$ of $W$ in which the matrix representing $f$ restricted to $W$ has the desired form. Accordingly, $u_{1}, u_{2}, u_{3}, \\\\ldots, u_{n}$ is a basis of $V$ in which the matrix representing $f$ has the desired form.\\n\\n',\n",
       " '13.1. Find the adjoint of $F: \\\\mathbf{R}^{3} \\\\rightarrow \\\\mathbf{R}^{3}$ defined by\\n\\n$$\\nF(x, y, z)=(3 x+4 y-5 z, \\\\quad 2 x-6 y+7 z, \\\\quad 5 x-9 y+z)\\n$$\\n\\nFirst find the matrix $A$ that represents $F$ in the usual basis of $\\\\mathbf{R}^{3}$ - that is, the matrix $A$ whose rows are the coefficients of $x, y, z$ - and then form the transpose $A^{T}$ of $A$. This yields\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rrr}\\n3 & 4 & -5 \\\\\\\\\\n2 & -6 & 7 \\\\\\\\\\n5 & -9 & 1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad A^{T}=\\\\left[\\\\begin{array}{rrr}\\n3 & 2 & 5 \\\\\\\\\\n4 & -6 & -9 \\\\\\\\\\n-5 & 7 & 1\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe adjoint $F^{*}$ is represented by the transpose of $A$; hence,\\n\\n$$\\nF^{*}(x, y, z)=(3 x+2 y+5 z, \\\\quad 4 x-6 y-9 z, \\\\quad-5 x+7 y+z)\\n$$\\n',\n",
       " '\\n13.2. Find the adjoint of $G: \\\\mathbf{C}^{3} \\\\rightarrow \\\\mathbf{C}^{3}$ defined by\\n\\n$$\\nG(x, y, z)=[2 x+(1-i) y, \\\\quad(3+2 i) x-4 i z, \\\\quad 2 i x+(4-3 i) y-3 z]\\n$$\\n\\nFirst find the matrix $B$ that represents $G$ in the usual basis of $\\\\mathbf{C}^{3}$, and then form the conjugate transpose $B^{*}$ of $B$. This yields\\n\\n$$\\nB=\\\\left[\\\\begin{array}{ccc}\\n2 & 1-i & 0 \\\\\\\\\\n3+2 i & 0 & -4 i \\\\\\\\\\n2 i & 4-3 i & -3\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and then } \\\\quad B^{*}=\\\\left[\\\\begin{array}{ccc}\\n2 & 3-2 i & -2 i \\\\\\\\\\n1+i & 0 & 4+3 i \\\\\\\\\\n0 & 4 i & -3\\n\\\\end{array}\\\\right]\\n$$\\n\\nThen $G^{*}(x, y, z)=[2 x+(3-2 i) y-2 i z, \\\\quad(1+i) x+(4+3 i) z, 4 i y-3 z]$.\\n',\n",
       " '\\n13.3. Prove Theorem 13.3: Let $\\\\phi$ be a linear functional on an $n$-dimensional inner product space $V$. Then there exists a unique vector $u \\\\in V$ such that $\\\\phi(v)=\\\\langle v, u\\\\rangle$ for every $v \\\\in V$.\\n\\nLet $\\\\left\\\\{w_{1}, \\\\ldots, w_{n}\\\\right\\\\}$ be an orthonormal basis of $V$. Set\\n\\n$$\\nu=\\\\overline{\\\\phi\\\\left(w_{1}\\\\right)} w_{1}+\\\\overline{\\\\phi\\\\left(w_{2}\\\\right)} w_{2}+\\\\cdots+\\\\overline{\\\\phi\\\\left(w_{n}\\\\right)} w_{n}\\n$$\\n\\nLet $\\\\hat{u}$ be the linear functional on $V$ defined by $\\\\hat{u}(v)=\\\\langle v, u\\\\rangle$ for every $v \\\\in V$. Then, for $i=1, \\\\ldots, n$,\\n\\n$$\\n\\\\hat{u}\\\\left(w_{i}\\\\right)=\\\\left\\\\langle w_{i}, u\\\\right\\\\rangle=\\\\left\\\\langle w_{i}, \\\\overline{\\\\phi\\\\left(w_{1}\\\\right)} w_{1}+\\\\cdots+\\\\overline{\\\\phi\\\\left(w_{n}\\\\right)} w_{n}\\\\right\\\\rangle=\\\\phi\\\\left(w_{i}\\\\right)\\n$$\\n\\nBecause $\\\\hat{u}$ and $\\\\phi$ agree on each basis vector, $\\\\hat{u}=\\\\phi$.\\n\\nNow suppose $u^{\\\\prime}$ is another vector in $V$ for which $\\\\phi(v)=\\\\left\\\\langle v, u^{\\\\prime}\\\\right\\\\rangle$ for every $v \\\\in V$. Then $\\\\langle v, u\\\\rangle=\\\\left\\\\langle v, u^{\\\\prime}\\\\right\\\\rangle$ or $\\\\left\\\\langle v, u-u^{\\\\prime}\\\\right\\\\rangle=0$. In particular, this is true for $v=u-u^{\\\\prime}$, and so $\\\\left\\\\langle u-u^{\\\\prime}, u-u^{\\\\prime}\\\\right\\\\rangle=0$. This yields $u-u^{\\\\prime}=0$ and $u=u^{\\\\prime}$. Thus, such a vector $u$ is unique, as claimed.\\n',\n",
       " '\\n13.4. Prove Theorem 13.1: Let $T$ be a linear operator on an $n$-dimensional inner product space $V$. Then\\n\\n(a) There exists a unique linear operator $T^{*}$ on $V$ such that\\n\\n$$\\n\\\\langle T(u), v\\\\rangle=\\\\left\\\\langle u, T^{*}(v)\\\\right\\\\rangle \\\\quad \\\\text { for all } u, v \\\\in V .\\n$$\\n\\n(b) Let $A$ be the matrix that represents $T$ relative to an orthonormal basis $S=\\\\left\\\\{u_{i}\\\\right\\\\}$. Then the conjugate transpose $A^{*}$ of $A$ represents $T^{*}$ in the basis $S$.\\n\\n(a) We first define the mapping $T^{*}$. Let $v$ be an arbitrary but fixed element of $V$. The map $u \\\\mapsto\\\\langle T(u), v\\\\rangle$ is a linear functional on $V$. Hence, by Theorem 13.3, there exists a unique element $v^{\\\\prime} \\\\in V$ such that $\\\\langle T(u), v\\\\rangle=\\\\left\\\\langle u, v^{\\\\prime}\\\\right\\\\rangle$ for every $u \\\\in V$. We define $T^{*}: V \\\\rightarrow V$ by $T^{*}(v)=v^{\\\\prime}$. Then $\\\\langle T(u), v\\\\rangle=\\\\left\\\\langle u, T^{*}(v)\\\\right\\\\rangle$ for every $u, v \\\\in V$.\\n\\nWe next show that $T^{*}$ is linear. For any $u, v_{i} \\\\in V$, and any $a, b \\\\in K$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle u, \\\\quad T^{*}\\\\left(a v_{1}+b v_{2}\\\\right)\\\\right\\\\rangle & =\\\\left\\\\langle T(u), \\\\quad a v_{1}+b v_{2}\\\\right\\\\rangle=\\\\bar{a}\\\\left\\\\langle T(u), v_{1}\\\\right\\\\rangle+\\\\bar{b}\\\\left\\\\langle T(u), v_{2}\\\\right\\\\rangle \\\\\\\\\\n& =\\\\bar{a}\\\\left\\\\langle u, T^{*}\\\\left(v_{1}\\\\right)\\\\right\\\\rangle+\\\\bar{b}\\\\left\\\\langle u, T^{*}\\\\left(v_{2}\\\\right)\\\\right\\\\rangle=\\\\left\\\\langle u, a T^{*}\\\\left(v_{1}\\\\right)+b T^{*}\\\\left(v_{2}\\\\right)\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nBut this is true for every $u \\\\in V$; hence, $T^{*}\\\\left(a v_{1}+b v_{2}\\\\right)=a T^{*}\\\\left(v_{1}\\\\right)+b T^{*}\\\\left(v_{2}\\\\right)$. Thus, $T^{*}$ is linear.\\n\\n(b) The matrices $A=\\\\left[a_{i j}\\\\right]$ and $B=\\\\left[b_{i j}\\\\right]$ that represent $T$ and $T^{*}$, respectively, relative to the orthonormal basis $S$ are given by $a_{i j}=\\\\left\\\\langle T\\\\left(u_{j}\\\\right), u_{i}\\\\right\\\\rangle$ and $b_{i j}=\\\\left\\\\langle T^{*}\\\\left(u_{j}\\\\right), u_{i}\\\\right\\\\rangle$ (Problem 13.67). Hence,\\n\\n$$\\nb_{i j}=\\\\left\\\\langle T^{*}\\\\left(u_{j}\\\\right), u_{i}\\\\right\\\\rangle=\\\\overline{\\\\left\\\\langle u_{i}, T^{*}\\\\left(u_{j}\\\\right)\\\\right\\\\rangle}=\\\\overline{\\\\left\\\\langle T\\\\left(u_{i}\\\\right), u_{j}\\\\right\\\\rangle}=\\\\overline{a_{j i}}\\n$$\\n\\nThus, $B=A^{*}$, as claimed.\\n',\n",
       " '\\n13.5. Prove Theorem 13.2:\\n\\n(i) $\\\\left(T_{1}+T_{2}\\\\right)^{*}=T_{1}^{*}+T_{2}^{*}$,\\n\\n(iii) $\\\\left(T_{1} T_{2}\\\\right)^{*}=T_{2}^{*} T_{1}^{*}$,\\n\\n(ii) $(k T)^{*}=\\\\bar{k} T^{*}$,\\n\\n(iv) $\\\\left(T^{*}\\\\right)^{*}=T$.\\n\\n(i) For any $u, v \\\\in V$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle\\\\left(T_{1}+T_{2}\\\\right)(u), v\\\\right\\\\rangle & =\\\\left\\\\langle T_{1}(u)+T_{2}(u), \\\\quad v\\\\right\\\\rangle=\\\\left\\\\langle T_{1}(u), v\\\\right\\\\rangle+\\\\left\\\\langle T_{2}(u), v\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle u, T_{1}^{*}(v)\\\\right\\\\rangle+\\\\left\\\\langle u, T_{2}^{*}(v)\\\\right\\\\rangle=\\\\left\\\\langle u, \\\\quad T_{1}^{*}(v)+T_{2}^{*}(v)\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle u, \\\\quad\\\\left(T_{1}^{*}+T_{2}^{*}\\\\right)(v)\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThe uniqueness of the adjoint implies $\\\\left(T_{1}+T_{2}\\\\right)^{*}=T_{1}^{*}+T_{2}^{*}$.\\n\\n(ii) For any $u, v \\\\in V$,\\n\\n$$\\n\\\\langle(k T)(u), \\\\quad v\\\\rangle=\\\\langle k T(u), v\\\\rangle=k\\\\langle T(u), v\\\\rangle=k\\\\left\\\\langle u, T^{*}(v)\\\\right\\\\rangle=\\\\left\\\\langle u, \\\\bar{k} T^{*}(v)\\\\right\\\\rangle=\\\\left\\\\langle u,\\\\left(\\\\bar{k} T^{*}\\\\right)(v)\\\\right\\\\rangle\\n$$\\n\\nThe uniqueness of the adjoint implies $(k T)^{*}=\\\\bar{k} T^{*}$.\\n\\n(iii) For any $u, v \\\\in V$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left\\\\langle\\\\left(T_{1} T_{2}\\\\right)(u), v\\\\right\\\\rangle & =\\\\left\\\\langle T_{1}\\\\left(T_{2}(u)\\\\right), v\\\\right\\\\rangle=\\\\left\\\\langle T_{2}(u), T_{1}^{*}(v)\\\\right\\\\rangle \\\\\\\\\\n& =\\\\left\\\\langle u, T_{2}^{*}\\\\left(T_{1}^{*}(v)\\\\right)\\\\right\\\\rangle=\\\\left\\\\langle u,\\\\left(T_{2}^{*} T_{1}^{*}\\\\right)(v)\\\\right\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\nThe uniqueness of the adjoint implies $\\\\left(T_{1} T_{2}\\\\right)^{*}=T_{2}^{*} T_{1}^{*}$.\\n\\n(iv) For any $u, v \\\\in V$,\\n\\n$$\\n\\\\left\\\\langle T^{*}(u), v\\\\right\\\\rangle=\\\\overline{\\\\left\\\\langle v, T^{*}(u)\\\\right\\\\rangle}=\\\\overline{\\\\langle T(v), u\\\\rangle}=\\\\langle u, T(v)\\\\rangle\\n$$\\n\\nThe uniqueness of the adjoint implies $\\\\left(T^{*}\\\\right)^{*}=T$.\\n',\n",
       " '\\n13.6. Show that (a) $I^{*}=I$, and (b) $\\\\mathbf{0}^{*}=\\\\mathbf{0}$.\\n\\n(a) For every $u, v \\\\in V,\\\\langle I(u), v\\\\rangle=\\\\langle u, v\\\\rangle=\\\\langle u, I(v)\\\\rangle$; hence, $I^{*}=I$.\\n\\n(b) For every $u, v \\\\in V,\\\\langle\\\\mathbf{0}(u), v\\\\rangle=\\\\langle 0, v\\\\rangle=0=\\\\langle u, 0\\\\rangle=\\\\langle u, \\\\mathbf{0}(v)\\\\rangle$; hence, $\\\\mathbf{0}^{*}=\\\\mathbf{0}$.\\n',\n",
       " '\\n13.7. Suppose $T$ is invertible. Show that $\\\\left(T^{-1}\\\\right)^{*}=\\\\left(T^{*}\\\\right)^{-1}$.\\n\\n$$\\nI=I^{*}=\\\\left(T T^{-1}\\\\right)^{*}=\\\\left(T^{-1}\\\\right)^{*} T^{*} \\\\text {; hence, }\\\\left(T^{-1}\\\\right)^{*}=\\\\left(T^{*}\\\\right)^{-1} \\\\text {. }\\n$$\\n',\n",
       " '\\n13.8. Let $T$ be a linear operator on $V$, and let $W$ be a $T$-invariant subspace of $V$. Show that $W^{\\\\perp}$ is invariant under $T^{*}$.\\n\\nLet $u \\\\in W^{\\\\perp}$. If $w \\\\in W$, then $T(w) \\\\in W$ and so $\\\\left\\\\langle w, T^{*}(u)\\\\right\\\\rangle=\\\\langle T(w), u\\\\rangle=0$. Thus, $T^{*}(u) \\\\in W^{\\\\perp}$ because it is orthogonal to every $w \\\\in W$. Hence, $W^{\\\\perp}$ is invariant under $T^{*}$.\\n',\n",
       " '\\n13.9. Let $T$ be a linear operator on $V$. Show that each of the following conditions implies $T=\\\\mathbf{0}$ :\\n\\n(i) $\\\\langle T(u), v\\\\rangle=0$ for every $u, v \\\\in V$.\\n\\n(ii) $\\\\quad V$ is a complex space, and $\\\\langle T(u), u\\\\rangle=0$ for every $u \\\\in V$.\\n\\n(iii) $T$ is self-adjoint and $\\\\langle T(u), u\\\\rangle=0$ for every $u \\\\in V$.\\n\\nGive an example of an operator $T$ on a real space $V$ for which $\\\\langle T(u), u\\\\rangle=0$ for every $u \\\\in V$ but $T \\\\neq \\\\mathbf{0}$. [Thus, (ii) need not hold for a real space $V$.]\\n\\n(i) Set $v=T(u)$. Then $\\\\langle T(u), T(u)\\\\rangle=0$, and hence, $T(u)=0$, for every $u \\\\in V$. Accordingly, $T=\\\\mathbf{0}$.\\n\\n(ii) By hypothesis, $\\\\langle T(v+w), v+w\\\\rangle=0$ for any $v, w \\\\in V$. Expanding and setting $\\\\langle T(v), v\\\\rangle=0$ and $\\\\langle T(w), w\\\\rangle=0$, we find\\n\\n\\n\\\\begin{equation*}\\n\\\\langle T(v), w\\\\rangle+\\\\langle T(w), v\\\\rangle=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nNote $w$ is arbitrary in (1). Substituting $i w$ for $w$, and using $\\\\langle T(v), i w\\\\rangle=\\\\bar{i}\\\\langle T(v), w\\\\rangle=-i\\\\langle T(v), w\\\\rangle$ and $\\\\langle T(i w), v\\\\rangle=\\\\langle i T(w), v\\\\rangle=i\\\\langle T(w), v\\\\rangle$, we find\\n\\n$$\\n-i\\\\langle T(v), w\\\\rangle+i\\\\langle T(w), v\\\\rangle=0\\n$$\\n\\nDividing through by $i$ and adding to (1), we obtain $\\\\langle T(w), v\\\\rangle=0$ for any $v, w, \\\\in V$. By (i), $T=\\\\mathbf{0}$.\\n\\n(iii) By (ii), the result holds for the complex case; hence we need only consider the real case. Expanding $\\\\langle T(v+w), v+w\\\\rangle=0$, we again obtain (1). Because $T$ is self-adjoint and as it is a real space, we have $\\\\langle T(w), v\\\\rangle=\\\\langle w, T(v)\\\\rangle=\\\\langle T(v), w\\\\rangle$. Substituting this into (1), we obtain $\\\\langle T(v), w\\\\rangle=0$ for any $v, w \\\\in V$. By (i), $T=\\\\mathbf{0}$.\\n\\nFor an example, consider the linear operator $T$ on $\\\\mathbf{R}^{2}$ defined by $T(x, y)=(y,-x)$. Then $\\\\langle T(u), u\\\\rangle=0$ for every $u \\\\in V$, but $T \\\\neq \\\\mathbf{0}$.\\n\\n\\n\\\\section*{Orthogonal and Unitary Operators and Matrices}\\n',\n",
       " '13.10. Prove Theorem 13.6: The following conditions on an operator $U$ are equivalent:\\n\\n(i) $U^{*}=U^{-1}$; that is, $U$ is unitary. (ii) $\\\\langle U(v), U(w)\\\\rangle=\\\\langle u, w\\\\rangle$. (iii) $\\\\|U(v)\\\\|=\\\\|v\\\\|$. Suppose (i) holds. Then, for every $v, w, \\\\in V$,\\n\\n$$\\n\\\\langle U(v), U(w)\\\\rangle=\\\\left\\\\langle v, U^{*} U(w)\\\\right\\\\rangle=\\\\langle v, I(w)\\\\rangle=\\\\langle v, w\\\\rangle\\n$$\\n\\nThus, (i) implies (ii). Now if (ii) holds, then\\n\\n$$\\n\\\\|U(v)\\\\|=\\\\sqrt{\\\\langle U(v), U(v)\\\\rangle}=\\\\sqrt{\\\\langle v, v\\\\rangle}=\\\\|v\\\\|\\n$$\\n\\nHence, (ii) implies (iii). It remains to show that (iii) implies (i).\\n\\nSuppose (iii) holds. Then for every $v \\\\in V$,\\n\\n$$\\n\\\\left\\\\langle U^{*} U(v)\\\\right\\\\rangle=\\\\langle U(v), U(v)\\\\rangle=\\\\langle v, v\\\\rangle=\\\\langle I(v), v\\\\rangle\\n$$\\n\\nHence, $\\\\left\\\\langle\\\\left(U^{*} U-I\\\\right)(v), \\\\quad v\\\\right\\\\rangle=0$ for every $v \\\\in V$. But $U^{*} U-I$ is self-adjoint (Prove!); then, by Problem 13.9, we have $U^{*} U-I=0$ and so $U^{*} U=I$. Thus, $U^{*}=U^{-1}$, as claimed.\\n',\n",
       " '\\n13.11. Let $U$ be a unitary (orthogonal) operator on $V$, and let $W$ be a subspace invariant under $U$. Show that $W^{\\\\perp}$ is also invariant under $U$.\\n\\nBecause $U$ is nonsingular, $U(W)=W$; that is, for any $w \\\\in W$, there exists $w^{\\\\prime} \\\\in W$ such that $U\\\\left(w^{\\\\prime}\\\\right)=w$. Now let $v \\\\in W^{\\\\perp}$. Then, for any $w \\\\in W$,\\n\\n$$\\n\\\\langle U(v), w\\\\rangle=\\\\left\\\\langle U(v), U\\\\left(w^{\\\\prime}\\\\right)\\\\right\\\\rangle=\\\\left\\\\langle v, w^{\\\\prime}\\\\right\\\\rangle=0\\n$$\\n\\nThus, $U(v)$ belongs to $W^{\\\\perp}$. Therefore, $W^{\\\\perp}$ is invariant under $U$.\\n',\n",
       " '\\n13.12. Prove Theorem 13.9: The change-of-basis matrix from an orthonormal basis $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ into another orthonormal basis is unitary (orthogonal). Conversely, if $P=\\\\left[a_{i j}\\\\right]$ is a unitary (orthogonal) matrix, then the vectors $u_{i^{\\\\prime}}=\\\\sum_{j} a_{j i} u_{j}$ form an orthonormal basis.\\n\\nSuppose $\\\\left\\\\{v_{i}\\\\right\\\\}$ is another orthonormal basis and suppose\\n\\n\\n\\\\begin{equation*}\\nv_{i}=b_{i 1} u_{1}+b_{i 2} u_{2}+\\\\cdots+b_{i n} u_{n}, \\\\quad i=1, \\\\ldots, n \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nBecause $\\\\left\\\\{v_{i}\\\\right\\\\}$ is orthonormal,\\n\\n\\n\\\\begin{equation*}\\n\\\\delta_{i j}=\\\\left\\\\langle v_{i}, v_{j}\\\\right\\\\rangle=b_{i 1} \\\\overline{b_{j 1}}+b_{i 2} \\\\overline{b_{j 2}}+\\\\cdots+b_{i n} \\\\overline{b_{j n}} \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nLet $B=\\\\left[b_{i j}\\\\right]$ be the matrix of coefficients in (1). (Then $B^{T}$ is the change-of-basis matrix from $\\\\left\\\\{u_{i}\\\\right\\\\}$ to $\\\\left\\\\{v_{i}\\\\right\\\\}$.) Then $B B^{*}=\\\\left[c_{i j}\\\\right]$, where $c_{i j}=b_{i 1} \\\\overline{b_{j 1}}+b_{i 2} \\\\overline{b_{j 2}}+\\\\cdots+b_{i n} \\\\overline{b_{j n}}$. By (2), $c_{i j}=\\\\delta_{i j}$, and therefore $B B^{*}=I$. Accordingly, $B$, and hence, $B^{T}$, is unitary.\\n\\nIt remains to prove that $\\\\left\\\\{u_{i}^{\\\\prime}\\\\right\\\\}$ is orthonormal. By Problem 13.67,\\n\\n$$\\n\\\\left\\\\langle u_{i}^{\\\\prime}, u_{j}^{\\\\prime}\\\\right\\\\rangle=a_{1 i} \\\\overline{a_{1 j}}+a_{2 i} \\\\overline{a_{2 j}}+\\\\cdots+a_{n i} \\\\overline{a_{n j}}=\\\\left\\\\langle C_{i}, C_{j}\\\\right\\\\rangle\\n$$\\n\\nwhere $C_{i}$ denotes the $i$ th column of the unitary (orthogonal) matrix $P=\\\\left[a_{i j}\\\\right]$. Because $P$ is unitary (orthogonal), its columns are orthonormal; hence, $\\\\left\\\\langle u_{i}^{\\\\prime}, u_{j}^{\\\\prime}\\\\right\\\\rangle=\\\\left\\\\langle C_{i}, C_{j}\\\\right\\\\rangle=\\\\delta_{i j}$. Thus, $\\\\left\\\\{u_{i}^{\\\\prime}\\\\right\\\\}$ is an orthonormal basis.\\n\\n\\n\\\\section*{Symmetric Operators and Canonical Forms in Euclidean Spaces}\\n',\n",
       " '13.13. Let $T$ be a symmetric operator. Show that (a) The characteristic polynomial $\\\\Delta(t)$ of $T$ is a product of linear polynomials (over $\\\\mathbf{R}$ ); (b) $T$ has a nonzero eigenvector.\\n\\n(a) Let $A$ be a matrix representing $T$ relative to an orthonormal basis of $V$; then $A=A^{T}$. Let $\\\\Delta(t)$ be the characteristic polynomial of $A$. Viewing $A$ as a complex self-adjoint operator, $A$ has only real eigenvalues by Theorem 13.4. Thus,\\n\\n$$\\n\\\\Delta(t)=\\\\left(t-\\\\lambda_{1}\\\\right)\\\\left(t-\\\\lambda_{2}\\\\right) \\\\cdots\\\\left(t-\\\\lambda_{n}\\\\right)\\n$$\\n\\nwhere the $\\\\lambda_{i}$ are all real. In other words, $\\\\Delta(t)$ is a product of linear polynomials over $\\\\mathbf{R}$.\\n\\n(b) By (a), $T$ has at least one (real) eigenvalue. Hence, $T$ has a nonzero eigenvector.\\n',\n",
       " '\\n13.14. Prove Theorem 13.11: Let $T$ be a symmetric operator on a real $n$-dimensional inner product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$. (Hence, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.)\\n\\nThe proof is by induction on the dimension of $V$. If $\\\\operatorname{dim} V=1$, the theorem trivially holds. Now suppose $\\\\operatorname{dim} V=n>1$. By Problem 13.13, there exists a nonzero eigenvector $v_{1}$ of $T$. Let $W$ be the space spanned by $v_{1}$, and let $u_{1}$ be a unit vector in $W$, e.g., let $u_{1}=v_{1} /\\\\left\\\\|v_{1}\\\\right\\\\|$.\\n\\nBecause $v_{1}$ is an eigenvector of $T$, the subspace $W$ of $V$ is invariant under $T$. By Problem 13.8, $W^{\\\\perp}$ is invariant under $T^{*}=T$. Thus, the restriction $\\\\hat{T}$ of $T$ to $W^{\\\\perp}$ is a symmetric operator. By Theorem 7.4, $V=W \\\\oplus W^{\\\\perp}$. Hence, $\\\\operatorname{dim} W^{\\\\perp}=n-1$, because $\\\\operatorname{dim} W=1$. By induction, there exists an orthonormal basis $\\\\left\\\\{u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ of $W^{\\\\perp}$ consisting of eigenvectors of $\\\\hat{T}$ and hence of $T$. But $\\\\left\\\\langle u_{1}, u_{i}\\\\right\\\\rangle=0$ for $i=2, \\\\ldots, n$ because $u_{i} \\\\in W^{\\\\perp}$. Accordingly $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is an orthonormal set and consists of eigenvectors of $T$. Thus, the theorem is proved.\\n',\n",
       " '\\n13.15. Let $q(x, y)=3 x^{2}-6 x y+11 y^{2}$. Find an orthonormal change of coordinates (linear substitution) that diagonalizes the quadratic form $q$.\\n\\nFind the symmetric matrix $A$ representing $q$ and its characteristic polynomial $\\\\Delta(t)$. We have\\n\\n$$\\nA=\\\\left[\\\\begin{array}{rr}\\n3 & -3 \\\\\\\\\\n-3 & 11\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad \\\\Delta(t)=t^{2}-\\\\operatorname{tr}(A) t+|A|=t^{2}-14 t+24=(t-2)(t-12)\\n$$\\n\\nThe eigenvalues are $\\\\lambda=2$ and $\\\\lambda=12$. Hence, a diagonal form of $q$ is\\n\\n$$\\nq(s, t)=2 s^{2}+12 t^{2}\\n$$\\n\\n(where we use $s$ and $t$ as new variables). The corresponding orthogonal change of coordinates is obtained by finding an orthogonal set of eigenvectors of $A$.\\n\\nSubtract $\\\\lambda=2$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{rr}\\n1 & -3 \\\\\\\\\\n-3 & 9\\n\\\\end{array}\\\\right] \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{array}{r}\\nx-3 y=0 \\\\\\\\\\n-3 x+9 y=0\\n\\\\end{array} \\\\quad \\\\text { or } \\\\quad x-3 y=0\\n$$\\n\\nA nonzero solution is $u_{1}=(3,1)$. Next subtract $\\\\lambda=12$ down the diagonal of $A$ to obtain the matrix\\n\\n$$\\nM=\\\\left[\\\\begin{array}{ll}\\n-9 & -3 \\\\\\\\\\n-3 & -1\\n\\\\end{array}\\\\right] \\\\quad \\\\text { corresponding to } \\\\quad \\\\begin{array}{r}\\n-9 x-3 y=0 \\\\\\\\\\n-3 x-y=0\\n\\\\end{array} \\\\quad \\\\text { or } \\\\quad-3 x-y=0\\n$$\\n\\nA nonzero solution is $u_{2}=(-1,3)$. Normalize $u_{1}$ and $u_{2}$ to obtain the orthonormal basis\\n\\n$$\\n\\\\hat{u}_{1}=(3 / \\\\sqrt{10}, 1 / \\\\sqrt{10}), \\\\quad \\\\hat{u}_{2}=(-1 / \\\\sqrt{10}, 3 / \\\\sqrt{10})\\n$$\\n\\nNow let $P$ be the matrix whose columns are $\\\\hat{u}_{1}$ and $\\\\hat{u}_{2}$. Then\\n\\n$$\\nP=\\\\left[\\\\begin{array}{rr}\\n3 / \\\\sqrt{10} & -1 / \\\\sqrt{10} \\\\\\\\\\n1 / \\\\sqrt{10} & 3 / \\\\sqrt{10}\\n\\\\end{array}\\\\right] \\\\quad \\\\text { and } \\\\quad D=P^{-1} A P=P^{T} A P=\\\\left[\\\\begin{array}{rr}\\n2 & 0 \\\\\\\\\\n0 & 12\\n\\\\end{array}\\\\right]\\n$$\\n\\nThus, the required orthogonal change of coordinates is\\n\\n$$\\n\\\\left[\\\\begin{array}{l}\\nx \\\\\\\\\\ny\\n\\\\end{array}\\\\right]=P\\\\left[\\\\begin{array}{l}\\ns \\\\\\\\\\nt\\n\\\\end{array}\\\\right] \\\\quad \\\\text { or } \\\\quad x=\\\\frac{3 s-t}{\\\\sqrt{10}}, \\\\quad y=\\\\frac{s+3 t}{\\\\sqrt{10}}\\n$$\\n\\nOne can also express $s$ and $t$ in terms of $x$ and $y$ by using $P^{-1}=P^{T}$; that is,\\n\\n$$\\ns=\\\\frac{3 x+y}{\\\\sqrt{10}}, \\\\quad t=\\\\frac{-x+3 y}{\\\\sqrt{10}}\\n$$\\n',\n",
       " '\\n13.16. Prove Theorem 13.12: Let $T$ be an orthogonal operator on a real inner product space $V$. Then there exists an orthonormal basis of $V$ in which $T$ is represented by a block diagonal matrix $M$ of the form\\n\\n$$\\nM=\\\\operatorname{diag}\\\\left(1, \\\\ldots, 1,-1, \\\\ldots,-1,\\\\left[\\\\begin{array}{rr}\\n\\\\cos \\\\theta_{1} & -\\\\sin \\\\theta_{1} \\\\\\\\\\n\\\\sin \\\\theta_{1} & \\\\cos \\\\theta_{1}\\n\\\\end{array}\\\\right], \\\\ldots,\\\\left[\\\\begin{array}{rr}\\n\\\\cos \\\\theta_{r} & -\\\\sin \\\\theta_{r} \\\\\\\\\\n\\\\sin \\\\theta_{r} & \\\\cos \\\\theta_{r}\\n\\\\end{array}\\\\right]\\\\right)\\n$$\\n\\nLet $S=T+T^{-1}=T+T^{*}$. Then $S^{*}=\\\\left(T+T^{*}\\\\right)^{*}=T^{*}+T=S$. Thus, $S$ is a symmetric operator on $V$. By Theorem 13.11, there exists an orthonormal basis of $V$ consisting of eigenvectors of $S$. If $\\\\lambda_{1}, \\\\ldots, \\\\lambda_{m}$ denote the distinct eigenvalues of $S$, then $V$ can be decomposed into the direct sum $V=V_{1} \\\\oplus V_{2} \\\\oplus \\\\cdots \\\\oplus V_{m}$ where the $V_{i}$ consists of the eigenvectors of $S$ belonging to $\\\\lambda_{i}$. We claim that each $V_{i}$ is invariant under $T$. For suppose $v \\\\in V$; then $S(v)=\\\\lambda_{i} v$ and\\n\\n$$\\nS(T(v))=\\\\left(T+T^{-1}\\\\right) T(v)=T\\\\left(T+T^{-1}\\\\right)(v)=T S(v)=T\\\\left(\\\\lambda_{i} v\\\\right)=\\\\lambda_{i} T(v)\\n$$\\n\\nThat is, $T(v) \\\\in V_{i}$. Hence, $V_{i}$ is invariant under $T$. Because the $V_{i}$ are orthogonal to each other, we can restrict our investigation to the way that $T$ acts on each individual $V_{i}$.\\n\\nOn a given $V_{i}$, we have $\\\\left(T+T^{-1}\\\\right) v=S(v)=\\\\lambda_{i} v$. Multiplying by $T$, we get\\n\\n\\n\\\\begin{equation*}\\n\\\\left(T^{2}-\\\\lambda_{i} T+I\\\\right)(v)=0 \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nWe consider the cases $\\\\lambda_{i}= \\\\pm 2$ and $\\\\lambda_{i} \\\\neq \\\\pm 2$ separately. If $\\\\lambda_{i}= \\\\pm 2$, then $(T \\\\pm I)^{2}(v)=0$, which leads to $(T \\\\pm I)(v)=0$ or $T(v)= \\\\pm v$. Thus, $T$ restricted to this $V_{i}$ is either $I$ or $-I$.\\n\\nIf $\\\\lambda_{i} \\\\neq \\\\pm 2$, then $T$ has no eigenvectors in $V_{i}$, because, by Theorem 13.4, the only eigenvalues of $T$ are 1 or -1 . Accordingly, for $v \\\\neq 0$, the vectors $v$ and $T(v)$ are linearly independent. Let $W$ be the subspace spanned by $v$ and $T(v)$. Then $W$ is invariant under $T$, because using (1) we get\\n\\n$$\\nT(T(v))=T^{2}(v)=\\\\lambda_{i} T(v)-v \\\\in W\\n$$\\n\\nBy Theorem 7.4, $V_{i}=W \\\\oplus W^{\\\\perp}$. Furthermore, by Problem 13.8, $W^{\\\\perp}$ is also invariant under $T$. Thus, we can decompose $V_{i}$ into the direct sum of two-dimensional subspaces $W_{j}$ where the $W_{j}$ are orthogonal to each other and each $W_{j}$ is invariant under $T$. Thus, we can restrict our investigation to the way in which $T$ acts on each individual $W_{j}$.\\n\\nBecause $T^{2}-\\\\lambda_{i} T+I=0$, the characteristic polynomial $\\\\Delta(t)$ of $T$ acting on $W_{j}$ is $\\\\Delta(t)=t^{2}-\\\\lambda_{i} t+1$. Thus, the determinant of $T$ is 1 , the constant term in $\\\\Delta(t)$. By Theorem 2.7, the matrix $A$ representing $T$ acting on $W_{j}$ relative to any orthogonal basis of $W_{j}$ must be of the form\\n\\n$$\\n\\\\left[\\\\begin{array}{rr}\\n\\\\cos \\\\theta & -\\\\sin \\\\theta \\\\\\\\\\n\\\\sin \\\\theta & \\\\cos \\\\theta\\n\\\\end{array}\\\\right]\\n$$\\n\\nThe union of the bases of the $W_{j}$ gives an orthonormal basis of $V_{i}$, and the union of the bases of the $V_{i}$ gives an orthonormal basis of $V$ in which the matrix representing $T$ is of the desired form.\\n\\n\\n\\\\section*{Normal Operators and Canonical Forms in Unitary Spaces}\\n',\n",
       " '13.17. Determine which of the following matrices is normal:\\n\\n(a) $A=\\\\left[\\\\begin{array}{ll}1 & i \\\\\\\\ 0 & 1\\\\end{array}\\\\right]$, (b) $B=\\\\left[\\\\begin{array}{cc}1 & i \\\\\\\\ 1 & 2+i\\\\end{array}\\\\right]$\\n\\n(a) $A A^{*}=\\\\left[\\\\begin{array}{ll}1 & i \\\\\\\\ 0 & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{rr}1 & 0 \\\\\\\\ -i & 1\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}2 & i \\\\\\\\ -i & 1\\\\end{array}\\\\right], \\\\quad A^{*} A=\\\\left[\\\\begin{array}{rr}1 & 0 \\\\\\\\ -i & 1\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ll}1 & i \\\\\\\\ 0 & 1\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{rr}1 & i \\\\\\\\ -i & 2\\\\end{array}\\\\right]$\\n\\nBecause $A A^{*} \\\\neq A^{*} A$, the matrix $A$ is not normal.\\n\\n(b) $B B^{*}\\\\left[\\\\begin{array}{cc}1 & i \\\\\\\\ 1 & 2+i\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc}1 & 1 \\\\\\\\ -i & 2-i\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}2 & 2+2 i \\\\\\\\ 2-2 i & 6\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{cc}1 & 1 \\\\\\\\ -i & 2-i\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc}1 & i \\\\\\\\ 1 & 2+i\\\\end{array}\\\\right]=B^{*} B$\\n\\nBecause $B B^{*}=B^{*} B$, the matrix $B$ is normal.\\n',\n",
       " '\\n13.18. Let $T$ be a normal operator. Prove the following:\\n\\n(a) $T(v)=0$ if and only if $T^{*}(v)=0$. (b) $T-\\\\lambda I$ is normal.\\n\\n(c) If $T(v)=\\\\lambda v$, then $T^{*}(v)=\\\\bar{\\\\lambda} v$; hence, any eigenvector of $T$ is also an eigenvector of $T^{*}$.\\n\\n(d) If $T(v)=\\\\lambda_{1} v$ and $T(w)=\\\\lambda_{2} w$ where $\\\\lambda_{1} \\\\neq \\\\lambda_{2}$, then $\\\\langle v, w\\\\rangle=0$; that is, eigenvectors of $T$ belonging to distinct eigenvalues are orthogonal.\\n\\n(a) We show that $\\\\langle T(v), T(v)\\\\rangle=\\\\left\\\\langle T^{*}(v), T^{*}(v)\\\\right\\\\rangle$ :\\n\\n$$\\n\\\\langle T(v), T(v)\\\\rangle=\\\\left\\\\langle v, T^{*} T(v)\\\\right\\\\rangle=\\\\left\\\\langle v, T T^{*}(v)\\\\right\\\\rangle=\\\\left\\\\langle T^{*}(v), T^{*}(v)\\\\right\\\\rangle\\n$$\\n\\nHence, by $\\\\left[I_{3}\\\\right]$ in the definition of the inner product in Section 7.2, $T(v)=0$ if and only if $T^{*}(v)=0$.\\n\\n(b) We show that $T-\\\\lambda I$ commutes with its adjoint:\\n\\n$$\\n\\\\begin{aligned}\\n(T-\\\\lambda I)(T-\\\\lambda I)^{*} & =(T-\\\\lambda I)\\\\left(T^{*}-\\\\bar{\\\\lambda} I\\\\right)=T T^{*}-\\\\lambda T^{*}-\\\\bar{\\\\lambda} T+\\\\lambda \\\\bar{\\\\lambda} I \\\\\\\\\\n& =T^{*} T-\\\\bar{\\\\lambda} T-\\\\lambda T^{*}+\\\\bar{\\\\lambda} \\\\lambda I=\\\\left(T^{*}-\\\\bar{\\\\lambda} I\\\\right)(T-\\\\lambda I) \\\\\\\\\\n& =(T-\\\\lambda I)^{*}(T-\\\\lambda I)\\n\\\\end{aligned}\\n$$\\n\\nThus, $T-\\\\lambda I$ is normal.\\\\\\\\\\n(c) If $T(v)=\\\\lambda v$, then $(T-\\\\lambda I)(v)=0$. Now $T-\\\\lambda I$ is normal by (b); therefore, by (a), $(T-\\\\lambda I)^{*}(v)=0$. That is, $\\\\left(T^{*}-\\\\lambda I\\\\right)(v)=0$; hence, $T^{*}(v)=\\\\bar{\\\\lambda} v$.\\n\\n(d) We show that $\\\\lambda_{1}\\\\langle v, w\\\\rangle=\\\\lambda_{2}\\\\langle v, w\\\\rangle$ :\\n\\n$$\\n\\\\lambda_{1}\\\\langle v, w\\\\rangle=\\\\left\\\\langle\\\\lambda_{1} v, w\\\\right\\\\rangle=\\\\langle T(v), w\\\\rangle=\\\\left\\\\langle v, T^{*}(w)\\\\right\\\\rangle=\\\\left\\\\langle v, \\\\bar{\\\\lambda}_{2} w\\\\right\\\\rangle=\\\\lambda_{2}\\\\langle v, w\\\\rangle\\n$$\\n\\nBut $\\\\lambda_{1} \\\\neq \\\\lambda_{2}$; hence, $\\\\langle v, w\\\\rangle=0$.\\n',\n",
       " '\\n13.19. Prove Theorem 13.13: Let $T$ be a normal operator on a complex finite-dimensional inner product space $V$. Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$. (Thus, $T$ can be represented by a diagonal matrix relative to an orthonormal basis.)\\n\\nThe proof is by induction on the dimension of $V$. If $\\\\operatorname{dim} V=1$, then the theorem trivially holds. Now suppose $\\\\operatorname{dim} V=n>1$. Because $V$ is a complex vector space, $T$ has at least one eigenvalue and hence a nonzero eigenvector $v$. Let $W$ be the subspace of $V$ spanned by $v$, and let $u_{1}$ be a unit vector in $W$.\\n\\nBecause $v$ is an eigenvector of $T$, the subspace $W$ is invariant under $T$. However, $v$ is also an eigenvector of $T^{*}$ by Problem 13.18; hence, $W$ is also invariant under $T^{*}$. By Problem 13.8, $W^{\\\\perp}$ is invariant under $T^{* *}=T$. The remainder of the proof is identical with the latter part of the proof of Theorem 13.11 (Problem 13.14).\\n',\n",
       " '\\n13.20. Prove Theorem 13.14: Let $T$ be any operator on a complex finite-dimensional inner product space $V$. Then $T$ can be represented by a triangular matrix relative to an orthonormal basis of $V$.\\n\\nThe proof is by induction on the dimension of $V$. If $\\\\operatorname{dim} V=1$, then the theorem trivially holds. Now suppose $\\\\operatorname{dim} V=n>1$. Because $V$ is a complex vector space, $T$ has at least one eigenvalue and hence at least one nonzero eigenvector $v$. Let $W$ be the subspace of $V$ spanned by $v$, and let $u_{1}$ be a unit vector in $W$. Then $u_{1}$ is an eigenvector of $T$ and, say, $T\\\\left(u_{1}\\\\right)=a_{11} u_{1}$.\\n\\nBy Theorem 7.4, $V=W \\\\oplus W^{\\\\perp}$. Let $E$ denote the orthogonal projection $V$ into $W^{\\\\perp}$. Clearly $W^{\\\\perp}$ is invariant under the operator $E T$. By induction, there exists an orthonormal basis $\\\\left\\\\{u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ of $W^{\\\\perp}$ such that, for $i=2, \\\\ldots, n$,\\n\\n$$\\nE T\\\\left(u_{i}\\\\right)=a_{i 2} u_{2}+{ }_{i 3} u_{3}+\\\\cdots+a_{i i} u_{i}\\n$$\\n\\n(Note that $\\\\left\\\\{u_{1}, u_{2}, \\\\ldots, u_{n}\\\\right\\\\}$ is an orthonormal basis of $V$.) But $E$ is the orthogonal projection of $V$ onto $W^{\\\\perp}$; hence, we must have\\n\\n$$\\nT\\\\left(u_{i}\\\\right)=a_{i 1} u_{1}+a_{i 2} u_{2}+\\\\cdots+a_{i i} u_{i}\\n$$\\n\\nfor $i=2, \\\\ldots, n$. This with $T\\\\left(u_{1}\\\\right)=a_{11} u_{1}$ gives us the desired result.\\n\\n\\n\\\\section*{Miscellaneous Problems}\\n',\n",
       " '13.21. Prove Theorem 13.10B: The following are equivalent:\\n\\n(i) $P=T^{2}$ for some self-adjoint operator $T$.\\n\\n(ii) $P=S^{*} S$ for some operator $S$; that is, $P$ is positive.\\n\\n(iii) $P$ is self-adjoint and $\\\\langle P(u), u\\\\rangle \\\\geq 0$ for every $u \\\\in V$.\\n\\nSuppose (i) holds; that is, $P=T^{2}$ where $T=T^{*}$. Then $P=T T=T^{*} T$, and so (i) implies (ii). Now suppose (ii) holds. Then $P^{*}=\\\\left(S^{*} S\\\\right)^{*}=S^{*} S^{* *}=S^{*} S=P$, and so $P$ is self-adjoint. Furthermore,\\n\\n$$\\n\\\\langle P(u), u\\\\rangle=\\\\left\\\\langle S^{*} S(u), u\\\\right\\\\rangle=\\\\langle S(u), S(u)\\\\rangle \\\\geq 0\\n$$\\n\\nThus, (ii) implies (iii), and so it remains to prove that (iii) implies (i).\\n\\nNow suppose (iii) holds. Because $P$ is self-adjoint, there exists an orthonormal basis $\\\\left\\\\{u_{1}, \\\\ldots, u_{n}\\\\right\\\\}$ of $V$ consisting of eigenvectors of $P$; say, $P\\\\left(u_{i}\\\\right)=\\\\lambda_{i} u_{i}$. By Theorem 13.4, the $\\\\lambda_{i}$ are real. Using (iii), we show that the $\\\\lambda_{i}$ are nonnegative. We have, for each $i$,\\n\\n$$\\n0 \\\\leq\\\\left\\\\langle P\\\\left(u_{i}\\\\right), u_{i}\\\\right\\\\rangle=\\\\left\\\\langle\\\\lambda_{i} u_{i}, u_{i}\\\\right\\\\rangle=\\\\lambda_{i}\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle\\n$$\\n\\nThus, $\\\\left\\\\langle u_{i}, u_{i}\\\\right\\\\rangle \\\\geq 0$ forces $\\\\lambda_{i} \\\\geq 0$, as claimed. Accordingly, $\\\\sqrt{\\\\lambda_{i}}$ is a real number. Let $T$ be the linear operator defined by\\n\\n$$\\nT\\\\left(u_{i}\\\\right)=\\\\sqrt{\\\\lambda_{i}} u_{i} \\\\quad \\\\text { for } i=1, \\\\ldots, n\\n$$\\n\\nBecause $T$ is represented by a real diagonal matrix relative to the orthonormal basis $\\\\left\\\\{u_{i}\\\\right\\\\}, T$ is self-adjoint. Moreover, for each $i$,\\n\\n$$\\nT^{2}\\\\left(u_{i}\\\\right)=T\\\\left(\\\\sqrt{\\\\lambda_{i}} u_{i}\\\\right)=\\\\sqrt{\\\\lambda_{i}} T\\\\left(i_{i}\\\\right)=\\\\sqrt{\\\\lambda_{i}} \\\\sqrt{\\\\lambda_{i}} u_{i}=\\\\lambda_{i} u_{i}=P\\\\left(u_{i}\\\\right)\\n$$\\n\\nBecause $T^{2}$ and $P$ agree on a basis of $V, P=T^{2}$. Thus, the theorem is proved.\\n\\nRemark: The above operator $T$ is the unique positive operator such that $P=T^{2}$; it is called the positive square root of $P$.\\n',\n",
       " '\\n13.22. Show that any operator $T$ is the sum of a self-adjoint operator and a skew-adjoint operator.\\n\\nSet $S=\\\\frac{1}{2}\\\\left(T+T^{*}\\\\right)$ and $U=\\\\frac{1}{2}\\\\left(T-T^{*}\\\\right)$. Then $T=S+U$, where\\n\\nand\\n\\n$$\\n\\\\begin{aligned}\\nS^{*} & =\\\\left[\\\\frac{1}{2}\\\\left(T+T^{*}\\\\right)\\\\right]^{*}=\\\\frac{1}{2}\\\\left(T^{*}+T^{* *}\\\\right)=\\\\frac{1}{2}\\\\left(T^{*}+T\\\\right)=S \\\\\\\\\\nU^{*} & =\\\\left[\\\\frac{1}{2}\\\\left(T-T^{*}\\\\right)\\\\right]^{*}=\\\\frac{1}{2}\\\\left(T^{*}-T\\\\right)=-\\\\frac{1}{2}\\\\left(T-T^{*}\\\\right)=-U\\n\\\\end{aligned}\\n$$\\n\\nthat is, $S$ is self-adjoint and $U$ is skew-adjoint.\\n',\n",
       " '\\n13.23. Prove: Let $T$ be an arbitrary linear operator on a finite-dimensional inner product space $V$. Then $T$ is a product of a unitary (orthogonal) operator $U$ and a unique positive operator $P$; that is, $T=U P$. Furthermore, if $T$ is invertible, then $U$ is also uniquely determined.\\n\\nBy Theorem 13.10, $T^{*} T$ is a positive operator; hence, there exists a (unique) positive operator $P$ such that $P^{2}=T^{*} T$ (Problem 13.43). Observe that\\n\\n\\n\\\\begin{equation*}\\n\\\\|P(v)\\\\|^{2}=\\\\langle P(v), P(v)\\\\rangle=\\\\left\\\\langle P^{2}(v), v\\\\right\\\\rangle=\\\\left\\\\langle T^{*} T(v), v\\\\right\\\\rangle=\\\\langle T(v), T(v)\\\\rangle=\\\\|T(v)\\\\|^{2} \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nWe now consider separately the cases when $T$ is invertible and noninvertible.\\n\\nIf $T$ is invertible, then we set $\\\\hat{U}=P T^{-1}$. We show that $\\\\hat{U}$ is unitary:\\n\\n$$\\n\\\\hat{U}^{*}=\\\\left(P T^{-1}\\\\right)^{*}=T^{-1 *} P^{*}=\\\\left(T^{*}\\\\right)^{-1} P \\\\quad \\\\text { and } \\\\quad \\\\hat{U} * \\\\hat{U}=\\\\left(T^{*}\\\\right)^{-1} P P T^{-1}=\\\\left(T^{*}\\\\right)^{-1} T^{*} T T^{-1}=I\\n$$\\n\\nThus, $\\\\hat{U}$ is unitary. We next set $U=\\\\hat{U}^{-1}$. Then $U$ is also unitary, and $T=U P$ as required.\\n\\nTo prove uniqueness, we assume $T=U_{0} P_{0}$, where $U_{0}$ is unitary and $P_{0}$ is positive. Then\\n\\n$$\\nT^{*} T=P_{0}^{*} U_{0}^{*} U_{0} P_{0}=P_{0} I P_{0}=P_{0}^{2}\\n$$\\n\\nBut the positive square root of $T^{*} T$ is unique (Problem 13.43); hence, $P_{0}=P$. (Note that the invertibility of $T$ is not used to prove the uniqueness of $P$.) Now if $T$ is invertible, then $P$ is also invertible by (1). Multiplying $U_{0} P=U P$ on the right by $P^{-1}$ yields $U_{0}=U$. Thus, $U$ is also unique when $T$ is invertible.\\n\\nNow suppose $T$ is not invertible. Let $W$ be the image of $P$; that is, $W=\\\\operatorname{Im} P$. We define $U_{1}: W \\\\rightarrow V$ by\\n\\n\\n\\\\begin{equation*}\\nU_{1}(w)=T(v), \\\\quad \\\\text { where } \\\\quad P(v)=w \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nWe must show that $U_{1}$ is well defined; that is, that $P(v)=P\\\\left(v^{\\\\prime}\\\\right)$ implies $T(v)=T\\\\left(v^{\\\\prime}\\\\right)$. This follows from the fact that $P\\\\left(v-v^{\\\\prime}\\\\right)=0$ is equivalent to $\\\\left\\\\|P\\\\left(v-v^{\\\\prime}\\\\right)\\\\right\\\\|=0$, which forces $\\\\left\\\\|T\\\\left(v-v^{\\\\prime}\\\\right)\\\\right\\\\|=0$ by (1). Thus, $U_{1}$ is well defined. We next define $U_{2}: W \\\\rightarrow V$. Note that, by (1), $P$ and $T$ have the same kernels. Hence, the images of $P$ and $T$ have the same dimension; that is, $\\\\operatorname{dim}(\\\\operatorname{Im} P)=\\\\operatorname{dim} W=\\\\operatorname{dim}(\\\\operatorname{Im} T)$. Consequently, $W^{\\\\perp}$ and $(\\\\operatorname{Im} T)^{\\\\perp}$ also have the same dimension. We let $U_{2}$ be any isomorphism between $W^{\\\\perp}$ and $(\\\\operatorname{Im} T)^{\\\\perp}$.\\n\\nWe next set $U=U_{1} \\\\oplus U_{2}$. [Here $U$ is defined as follows: If $v \\\\in V$ and $v=w+w^{\\\\prime}$, where $w \\\\in W$, $w^{\\\\prime} \\\\in W^{\\\\perp}$, then $U(v)=U_{1}(w)+U_{2}\\\\left(w^{\\\\prime}\\\\right)$.] Now $U$ is linear (Problem 13.69), and, if $v \\\\in V$ and $P(v)=w$, then, by (2),\\n\\n$$\\nT(v)=U_{1}(w)=U(w)=U P(v)\\n$$\\n\\nThus, $T=U P$, as required.\\n\\nIt remains to show that $U$ is unitary. Now every vector $x \\\\in V$ can be written in the form $x=P(v)+w^{\\\\prime}$, where $w^{\\\\prime} \\\\in W^{\\\\perp}$. Then $U(x)=U P(v)+U_{2}\\\\left(w^{\\\\prime}\\\\right)=T(v)+U_{2}\\\\left(w^{\\\\prime}\\\\right)$, where $\\\\left\\\\langle T(v), U_{2}\\\\left(w^{\\\\prime}\\\\right)\\\\right\\\\rangle=0$ by definition\\\\\\\\\\nof $U_{2}$. Also, $\\\\langle T(v), T(v)\\\\rangle=\\\\langle P(v), P(v)\\\\rangle$ by (1). Thus,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\langle U(x), U(x)\\\\rangle & =\\\\left\\\\langle T(v)+U_{2}\\\\left(w^{\\\\prime}\\\\right), \\\\quad T(v)+U_{2}\\\\left(w^{\\\\prime}\\\\right)\\\\right\\\\rangle=\\\\langle T(v), T(v)\\\\rangle+\\\\left\\\\langle U_{2}\\\\left(w^{\\\\prime}\\\\right), U_{2}\\\\left(w^{\\\\prime}\\\\right)\\\\right\\\\rangle \\\\\\\\\\n& =\\\\langle P(v), P(v)\\\\rangle+\\\\left\\\\langle w^{\\\\prime}, w^{\\\\prime}\\\\right\\\\rangle=\\\\left\\\\langle P(v)+w^{\\\\prime}, \\\\quad P(v)+w^{\\\\prime}\\\\right)=\\\\langle x, x\\\\rangle\\n\\\\end{aligned}\\n$$\\n\\n[We also used the fact that $\\\\left\\\\langle P(v), w^{\\\\prime}\\\\right\\\\rangle=0$.] Thus, $U$ is unitary, and the theorem is proved.\\n',\n",
       " '\\n13.24. Let $V$ be the vector space of polynomials over $\\\\mathbf{R}$ with inner product defined by\\n\\n$$\\n\\\\langle f, g\\\\rangle=\\\\int_{0}^{1} f(t) g(t) d t\\n$$\\n\\nGive an example of a linear functional $\\\\phi$ on $V$ for which Theorem 13.3 does not hold - that is, for which there is no polynomial $h(t)$ such that $\\\\phi(f)=\\\\langle f, h\\\\rangle$ for every $f \\\\in V$.\\n\\nLet $\\\\phi: V \\\\rightarrow \\\\mathbf{R}$ be defined by $\\\\phi(f)=f(0)$; that is, $\\\\phi$ evaluates $f(t)$ at 0 , and hence maps $f(t)$ into its constant term. Suppose a polynomial $h(t)$ exists for which\\n\\n\\n\\\\begin{equation*}\\n\\\\phi(f)=f(0)=\\\\int_{0}^{1} f(t) h(t) d t \\\\tag{1}\\n\\\\end{equation*}\\n\\n\\nfor every polynomial $f(t)$. Observe that $\\\\phi$ maps the polynomial $t f(t)$ into 0 ; hence, by (1),\\n\\n\\n\\\\begin{equation*}\\n\\\\int_{0}^{1} t f(t) h(t) d t=0 \\\\tag{2}\\n\\\\end{equation*}\\n\\n\\nfor every polynomial $f(t)$. In particular (2) must hold for $f(t)=t h(t)$; that is,\\n\\n$$\\n\\\\int_{0}^{1} t^{2} h^{2}(t) d t=0\\n$$\\n\\nThis integral forces $h(t)$ to be the zero polynomial; hence, $\\\\phi(f)=\\\\langle f, h\\\\rangle=\\\\langle f, \\\\mathbf{0}\\\\rangle=0$ for every polynomial $f(t)$. This contradicts the fact that $\\\\phi$ is not the zero functional; hence, the polynomial $h(t)$ does not exist.\\n\\n']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solved_problems_this_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ccd0c273-7ea5-476b-b375-1464670d1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n"
     ]
    }
   ],
   "source": [
    "print(len(supplementary_problems_this_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a30c0e70-4e8d-4147-a630-3213be81ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_url, 'w') as file:\n",
    "    json.dump([contents, solved_problems_this_book, supplementary_problems_this_book] , file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
